FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Bentaleb, A
   Akcay, MN
   Lim, M
   Begen, AC
   Zimmermann, R
AF Bentaleb, Abdelhak
   Akcay, Mehmet N.
   Lim, May
   Begen, Ali C.
   Zimmermann, Roger
TI Catching the Moment With (LoL) (+) in Twitch-Like Low-Latency Live
   Streaming Platforms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HAS; ABR; DASH; CMAF; low latency; chunked transfer encoding; adaptive
   playback speed; SOM
AB Our earlier Low-on-Latency (dubbed as LoL) solution offered an accurate bandwidth prediction and rate adaptation algorithm tailored for live streaming applications that targeted an end-to-end latency of up to two seconds. While LoL was a significant step forward in multi-bitrate low-latency live streaming, further experimentation and testing showed that there was room for improvement in three areas. First, LoL used hardcoded parameters computed from an offline training process in the rate adaptation algorithm and this was seen as a significant barrier in LoL's wide deployment. Second, LoL's objective was to maximize a collective QoE function. Yet, certain use cases have specific objectives besides the singular QoE and this had to be accommodated. Third, the adaptive playback speed control failed to produce satisfying results in some scenarios. Our goal in this paper is to address these areas and make LoL sufficiently robust to deploy. We refer to the enhanced solution as LoL(+), which has been integrated to the official dash.js player in v3.2.0.
C1 [Bentaleb, Abdelhak; Lim, May; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Akcay, Mehmet N.; Begen, Ali C.] Ozyegin Univ, TR-34794 Istanbul, Turkey.
C3 National University of Singapore; Ozyegin University
RP Bentaleb, A (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM bentaleb@comp.nus.edu.sg; necmettin.akcay@ozu.edu.tr;
   maylim@comp.nus.edu.sg; ali.begen@ozyegin.edu.tr; rogerz@comp.nus.edu.sg
RI Begen, Ali C./R-5897-2016; Zimmermann, Roger/D-7944-2015
OI Begen, Ali C./0000-0002-0835-3017; Zimmermann,
   Roger/0000-0002-7410-2590; Bentaleb, Abdelhak/0000-0002-5382-6530
FU National Research Foundation, Singapore under its AI Singapore Programme
   AISG [AISG-100E2019-047]; Singapore Ministry of Education Academic
   Research Fund Tier 2 under MOE's official [MOE2018-T2-1-103]; UAE
   University [31T102-UPAR-1-2017]
FX This work was supported in part by National Research Foundation,
   Singapore under its AI Singapore Programme AISG under Grant
   AISG-100E2019-047, in part by the Singapore Ministry of Education
   Academic Research Fund Tier 2 under MOE's official under Grant
   MOE2018-T2-1-103, in part by under Grant 31T102-UPAR-1-2017 from UAE
   University. Any opinions, findings and conclusions or recommendations
   expressed in thismaterial are those of the author(s) and do not reflect
   the views of National Research Foundation, Singapore. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. ChonggangWang.
CR Adobe, REAL TIM MESS PROT R
   [Anonymous], 2018, J NETW SYST MANAGE
   [Anonymous], 23000192020 ISO IEC
   [Anonymous], 2020, P IEEE INFOCOM C COM
   [Anonymous], 2012, Self-Organizing Maps
   [Anonymous], 2016, IEEE COMMUN LETT
   Ben Yahia M, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3280854
   Bentaleb A, 2021, IEEE T MULTIMEDIA, V23, P2588, DOI 10.1109/TMM.2020.3013387
   Bentaleb A, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3371040
   Bentaleb A, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P7, DOI 10.1145/3304112.3325611
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Burkard R., 2012, Assignment problems: revised reprint, DOI [10.1137/1.9781611972238, DOI 10.1137/1.9781611972238]
   D'Aronco S, 2017, IEEE MULTIMEDIA, V24, P20, DOI 10.1109/MMUL.2017.41
   Dogan Yunus, 2013, Machine Learning and Data Mining in Pattern Recognition. 9th International Conference, MLDM 2013. Proceedings: LNCS 7988, P246, DOI 10.1007/978-3-642-39712-7_19
   Du KT, 2020, SIGCOMM '20: PROCEEDINGS OF THE 2020 ANNUAL CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION ON THE APPLICATIONS, TECHNOLOGIES, ARCHITECTURES, AND PROTOCOLS FOR COMPUTER COMMUNICATION, P557, DOI 10.1145/3387514.3405887
   ELESSAILI A, 2018, P IEEE INT S BROAD M, P1
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gutterman C, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P327, DOI 10.1145/3339825.3397044
   Houzé P, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511550
   Jiang J., 2012, P 8 INT C EM NETW EX, P97
   Jiang JC, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P393
   Jiang JC, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P137
   Karagkioules T, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P315, DOI 10.1145/3339825.3397042
   Kim J, 2020, SIGCOMM '20: PROCEEDINGS OF THE 2020 ANNUAL CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION ON THE APPLICATIONS, TECHNOLOGIES, ARCHITECTURES, AND PROTOCOLS FOR COMPUTER COMMUNICATION, P107, DOI 10.1145/3387514.3405856
   KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
   Lim M, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P321, DOI 10.1145/3339825.3397043
   Park S., 2019, J SUPERCOMPUT, V76, P1
   Peng H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2627, DOI 10.1145/3343031.3356049
   reference.dashif, DASH REFERENCE PLAYE
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shuai Y, 2018, 2018 15 IEEE ANN CON, P1, DOI [10.1109/CCNC.2018.8319262, DOI 10.1109/CCNC.2018.8319262]
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Swaminathan V, 2011, IEEE INT WORKSH MULT
   Twitch, 2020, ACM MULTIMEDIA SYST
   Yi G, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2622, DOI 10.1145/3343031.3356083
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
NR 38
TC 12
Z9 12
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2300
EP 2314
DI 10.1109/TMM.2021.3079288
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600007
OA hybrid
DA 2024-07-18
ER

PT J
AU Chen, XY
   Li, J
   Lan, XG
   Zheng, NN
AF Chen, Xingyu
   Li, Jin
   Lan, Xuguang
   Zheng, Nanning
TI Generalized Zero-Shot Learning Via Multi-Modal Aggregated Posterior
   Aligning Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Generative adversarial networks; Gallium
   nitride; Training; Task analysis; Gaussian distribution; Aggregated
   posterior distribution alignment; generalized zero-shot learning;
   multi-modal neural network
AB The visual-semantic gap between the visual space (visual features) and semantic space (semantic attributes) is one of the main problems in the Generalized Zero-Shot Learning (GZSL) task. The essence of this problem is that the structure of manifolds in these two spaces is inconsistent, which makes it difficult to learn embeddings that unify visual features and semantic attributes for similarity measurement. In this work, we tackle this problem by proposing a multi-modal aggregated posterior aligning neural network based on Wasserstein Auto-encoders (WAE) which learns a shared latent space for visual features and semantic attributes. The key to our approach is that the aggregated posterior distribution of the latent representations encoded from visual features of each class is encouraged to be aligned with a Gaussian distribution predicted by the corresponding semantic attribute in the latent space. On one hand, requiring the latent manifolds of visual features and semantic attributes to be consistent preserves the inter-class association between seen and unseen classes. On the other hand, the aggregated posterior of each class is directly defined as a Gaussian in the latent space, which provides a reliable way to synthesize latent features for training classification models. Using the AWA1, AWA2, CUB, aPY, FLO, and SUN benchmark datasets, we extensively conducted comparative evaluations to demonstrate the advantages of our method over state-of-the-art approaches.
C1 [Chen, Xingyu; Lan, Xuguang; Zheng, Nanning] Xi An Jiao Tong Univ, Coll Artificial Intelligence, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
   [Li, Jin] Tencent Inc, Interact Entertainment Grp, Xian, Peoples R China.
C3 Xi'an Jiaotong University; Tencent
RP Lan, XG (corresponding author), Xi An Jiao Tong Univ, Coll Artificial Intelligence, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
EM xingyuchen1990@gmail.com; j.lixjtu@gmail.com; xglan@mail.xjtu.edu.cn;
   nnzheng@mail.xjtu.edu.cn
OI Jin, Li/0000-0002-0260-3169; Chen, Xingyu/0000-0002-5226-963X
FU Trico-Robot plan of NSFC [91 748 208]; National Major Project
   [2018ZX01028-101]; Shaanxi Project [2018ZDCXLGY0607]; NSFC [61 973 246];
   Program of the Ministry of Education
FX This work was supported in part by Trico-Robot plan of NSFC under Grant
   91 748 208, in part by National Major Project under Grant
   2018ZX01028-101, in part by Shaanxi Project under Grant 2018ZDCXLGY0607,
   in part by NSFC under Grant 61 973 246, and in part by the Program of
   the Ministry of Education. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Marco Carli.
   (Corresponding author: Xuguang Lan.)
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu X., 2018, ARXIV180512352
   Han XT, 2017, IEEE T MULTIMEDIA, V19, P1583, DOI 10.1109/TMM.2017.2671414
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Liu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2490
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Makhzani A., 2015, ARXIV
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294
   Mukherjee Tanmoy, 2017, ARXIV171106047
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Rahman S, 2020, IEEE T MULTIMEDIA, V22, P242, DOI 10.1109/TMM.2019.2924511
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Rubenstein P. K., 2018, INT C LEARN REPR ICL
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Tolstikhin I., 2018, 6 INT C LEARNING REP
   Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386
   Tschannen M, 2018, P WORKSH BAYES DEEP
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang WL, 2018, AAAI CONF ARTIF INTE, P4211
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xu X, 2017, PROC CVPR IEEE, P2007, DOI 10.1109/CVPR.2017.217
   Zhang HG, 2018, PROC CVPR IEEE, P7670, DOI 10.1109/CVPR.2018.00800
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
NR 42
TC 10
Z9 10
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 177
EP 187
DI 10.1109/TMM.2020.3047546
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300014
DA 2024-07-18
ER

PT J
AU Huang, NAC
   Yang, Y
   Zhang, DW
   Zhang, Q
   Han, JG
AF Huang, Nianchang
   Yang, Yang
   Zhang, Dingwen
   Zhang, Qiang
   Han, Jungong
TI Employing Bilinear Fusion and Saliency Prior Information for RGB-D
   Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Saliency detection; Cognition; Task analysis; Object
   detection; Computational modeling; Visualization; Bilinear fusion
   strategy; RGB-D salient object detection; saliency prior information
   guided fusion; saliency refinement and prediction
ID MODEL; ATTENTION; NETWORK
AB Multi-modal feature fusion and saliency reasoning are two core sub-tasks of RGB-D salient object detection. However, most existing models employ linear fusion strategies (e.g., concatenation) for multi-modal feature fusion and use a simple coarse-to-fine structure for saliency reasoning. Despite their simpleness, they can neither fully capture the cross-modal complementary information nor exploit the multi-level complementary information among the cross-modal features at different levels. To address these issues, a novel RGB-D salient object detection model is presented, where we pay special attention to the aforementioned two sub-tasks. Concretely, a multi-modal feature interaction module is first presented to explore more interactions between the unimodal RGB and depth features. It helps to capture their cross-modal complementary information by jointly using some simple linear fusion strategies and bilinear fusion ones. Then, a saliency prior information guided fusion module is presented to exploit the multi-level complementary information among the fused cross-modal features at different levels. Instead of employing a simple convolutional layer for the final saliency prediction, a saliency refinement and prediction module is designed to better exploit those extracted multi-level cross-modal information for RGB-D saliency detection. Experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed framework over some state-of-the-art methods.
C1 [Huang, Nianchang; Yang, Yang; Zhang, Dingwen; Zhang, Qiang] Xidian Univ, Ctr Complex Syst, Sch Mechanoelect Engn, Xian 710071, Peoples R China.
   [Han, Jungong] Aberystwyth Univ, Comp Sci Dept, Aberystwyth SY23 3FL, Dyfed, Wales.
C3 Xidian University; Aberystwyth University
RP Zhang, Q (corresponding author), Xidian Univ, Ctr Complex Syst, Sch Mechanoelect Engn, Xian 710071, Peoples R China.; Han, JG (corresponding author), Aberystwyth Univ, Comp Sci Dept, Aberystwyth SY23 3FL, Dyfed, Wales.
EM nchuang@stu.xidian.edu.cn; yy18629370639@163.com;
   zhangdingwen2006yyy@gmail.com; qzhang@xidian.edu.cn;
   jungonghan77@gmail.com
RI zhang, dingwen/R-3463-2019
OI zhang, dingwen/0000-0001-8369-8886; Nianchang, Huang/0000-0001-9530-3490
FU National Natural Science Foundation of China [61773301, 61876140]; China
   Postdoctoral Support Scheme for Innovative Talents [BX20180236]
FX Date of publication March 31, 2021; date of current version March 29,
   2022. This work was supported in part by the National Natural Science
   Foundation of China under Grants 61773301 and 61876140, and in part by
   the China Postdoctoral Support Scheme for Innovative Talents under Grant
   BX20180236. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Mohammed Daoudi.
CR [Anonymous], 2018, IEEE T NEUR NET LEAR, DOI DOI 10.1109/TNNLS.2018.2817340
   Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, IEEE INT C INT ROBOT, P6821, DOI 10.1109/IROS.2018.8594373
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Huang NAC, 2021, IEEE T MULTIMEDIA, V23, P2428, DOI 10.1109/TMM.2020.3011327
   HUANG P, 2018, 2018 IEEE 23 INT C D, P1, DOI DOI 10.1109/ICDSP.2018.8631584
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kar P, 2012, ARTIFICIAL INTELLIGE, V22, P583
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Lao MR, 2018, IEEE ACCESS, V6, P57923, DOI 10.1109/ACCESS.2018.2873570
   Lei BY, 2015, PATTERN RECOGN, V48, P2567, DOI 10.1016/j.patcog.2015.02.004
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P360, DOI 10.1109/TIP.2019.2930906
   Liu Y, 2019, IEEE T CIRC SYST VID, V29, P1023, DOI 10.1109/TCSVT.2018.2823769
   Liu Y, 2018, IMAGE VISION COMPUT, V69, P155, DOI 10.1016/j.imavis.2017.10.002
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Paszke A, 2019, ADV NEUR IN, V32
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Simonyan K., 2014, 14091556 ARXIV
   Tang J., 2018, ARXIV181110763
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang Q, 2021, IEEE T CIRC SYST VID, V31, P1804, DOI 10.1109/TCSVT.2020.3014663
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
   Zhang Q, 2017, COMPUT VIS IMAGE UND, V161, P51, DOI 10.1016/j.cviu.2017.04.015
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
NR 62
TC 41
Z9 42
U1 6
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1651
EP 1664
DI 10.1109/TMM.2021.3069297
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200031
OA Green Published
DA 2024-07-18
ER

PT J
AU Liang, JX
   Wang, JW
   Quan, YH
   Chen, TY
   Liu, JY
   Ling, HB
   Xu, Y
AF Liang, Jinxiu
   Wang, Jingwen
   Quan, Yuhui
   Chen, Tianyi
   Liu, Jiaying
   Ling, Haibin
   Xu, Yong
TI Recurrent Exposure Generation for Low-Light Face Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face detection; Faces; Lighting; Detectors; Face recognition; Task
   analysis; Benchmark testing; Gated recurrent networks; low-light face
   detection; multi-exposure
ID RECOGNITION
AB Face detection from low-light images is challenging due to limited photons and inevitable noise, which, to make the task even harder, are often spatially unevenly distributed. A natural solution is to borrow the idea from multi-exposure, which captures multiple shots to obtain well-exposed images under challenging conditions. High-quality implementation/approximation of multi-exposure from a single image is however nontrivial. Fortunately, as shown in this paper, neither is such high-quality necessary since our task is face detection rather than image enhancement. Specifically, we propose a novel Recurrent Exposure Generation (REG) module and couple it seamlessly with a Multi-Exposure Detection (MED) module, and thus significantly improve face detection performance by effectively inhibiting non-uniform illumination and noise issues. REG produces progressively and efficiently intermediate images corresponding to various exposure settings, and such pseudo-exposures are then fused by MED to detect faces across different lighting conditions. The proposed method, named REGDet, is the first 'detection-with-enhancement' framework for low-light face detection. It not only encourages rich interaction and feature fusion across different illumination levels, but also enables effective end-to-end learning of the REG component to be better tailored for face detection. Moreover, as clearly shown in our experiments, REG can be flexibly coupled with different face detectors without extra low/normal-light image pairs for training. We tested REGDet on the DARK FACE low-light face benchmark with thorough ablation study, where REGDet outperforms previous state-of-the-arts by a significant margin, with only negligible extra parameters.
C1 [Liang, Jinxiu; Quan, Yuhui; Chen, Tianyi; Xu, Yong] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.
   [Wang, Jingwen] Tencent, Tencent AI Lab, Shenzhen 518057, Guangdong, Peoples R China.
   [Liu, Jiaying] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Strony Brook, NY 11794 USA.
   [Xu, Yong] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Xu, Yong] Commun & Comp Network Lab Guangdong, Guangzhou 510006, Peoples R China.
C3 South China University of Technology; Tencent; Peking University; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; Peng Cheng Laboratory
RP Xu, Y (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.; Xu, Y (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.
EM cssherryliang@mail.scut.edu.cn; jaywongjaywong@gmail.com;
   csyhquan@scut.edu.cn; csttychen@mail.scut.edu.cn; liujiaying@pku.edu.cn;
   hling@cs.stonybrook.edu; yxu@scut.edu.cn
RI wang, xu/IAN-4886-2023; wang, dan/JEF-0836-2023; Liang,
   Jinxiu/HKV-9220-2023; ZHOU, YUE/IZE-6277-2023; Wang, Jin/GYA-2019-2022;
   wang, jie/HTQ-4920-2023; wang, juan/IUO-6218-2023; Liu,
   JY/GYJ-0138-2022; wang, jian/HRB-9588-2023; Wang, Jing/IQW-3496-2023;
   wang, jing/GRS-7509-2022; wang, jing/HJA-5384-2022; wang,
   jiajun/JRW-6032-2023; wang, jing/GVT-8700-2022; wang,
   jiahui/IXD-1197-2023
OI Liang, Jinxiu/0000-0001-6302-0912; Wang, Jing/0000-0002-8296-2961; Liu,
   Jiaying/0000-0002-0468-9576; Ling, Haibin/0000-0003-4094-8413
FU National Natural Science Foundation of China [62072188, 61872151];
   Natural Science Foundation of Guangdong Province [2017A030313376,
   2020A1515011128]; Science and Technology Program of Guangdong Province
   [2019A050510010]; Science and Technology Program of Guangzhou
   [201802010055]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072188 and 61872151, in part by the
   Natural Science Foundation of Guangdong Province under Grants
   2017A030313376 and 2020A1515011128, in part by the Science and
   Technology Program of Guangdong Province under Grant 2019A050510010, and
   in part by the Science and Technology Program of Guangzhou under Grant
   201802010055. The associate editor coordinating the reviewof this
   manuscript and approving it for publicationwas Prof. M. Murshed.
CR Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Ballas N, 2016, P INT C LEARN REPR M
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen WL, 2006, IEEE T SYST MAN CY B, V36, P458, DOI 10.1109/TSMCB.2005.857353
   Chi C, 2019, AAAI CONF ARTIF INTE, P8231
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Farid H, 2001, IEEE T IMAGE PROCESS, V10, P1428, DOI 10.1109/83.951529
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Grossberg MD, 2004, IEEE T PATTERN ANAL, V26, P1272, DOI 10.1109/TPAMI.2004.88
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Han H, 2013, PATTERN RECOGN, V46, P1691, DOI 10.1016/j.patcog.2012.11.022
   Hao ZK, 2017, PROC CVPR IEEE, P1913, DOI 10.1109/CVPR.2017.207
   Howard AG, 2013, ARXIV13125402CS
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   Jain V., 2010, Fddb: A benchmark for face detection in unconstrained settings
   Jiang HZ, 2017, IEEE INT CONF AUTOMA, P650, DOI [10.1109/FG.2017.82, 10.1109/MWSYM.2017.8058653]
   Kingma D. P., 2014, arXiv
   Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803
   Levi K, 2004, PROC CVPR IEEE, P53
   Li J, 2019, PROC CVPR IEEE, P5055, DOI 10.1109/CVPR.2019.00520
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mann S, 2000, IEEE T IMAGE PROCESS, V9, P1389, DOI 10.1109/83.855434
   Nada H, 2018, INT CONF BIOMETR THE
   Najibi M, 2019, PROC CVPR IEEE, P7715, DOI 10.1109/CVPR.2019.00791
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren YR, 2019, IEEE T CIRC SYST VID, V29, P968, DOI 10.1109/TCSVT.2018.2828141
   Shan SG, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P157
   Shi XP, 2018, PROC CVPR IEEE, P2295, DOI 10.1109/CVPR.2018.00244
   Sun XD, 2018, NEUROCOMPUTING, V299, P42, DOI 10.1016/j.neucom.2018.03.030
   Tang X, 2018, LECT NOTES COMPUT SC, V11213, P812, DOI 10.1007/978-3-030-01240-3_49
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C., 2018, P BMVC, P1
   Yan SY, 2008, PROC CVPR IEEE, P3576
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419
   Yang WH, 2020, IEEE T IMAGE PROCESS, V29, P5737, DOI 10.1109/TIP.2020.2981922
   Ying Z., 2017, ARXIV171100591CS
   Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao J., 2018, Deep learning for human-centric image analysis
   Zhao J, 2020, INT J COMPUT VISION, V128, P460, DOI 10.1007/s11263-019-01252-7
   Zhao J, 2017, ADV NEUR IN, V30
   Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184
   Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235
   Zhao J, 2019, IEEE T PATTERN ANAL, V41, P2380, DOI 10.1109/TPAMI.2018.2858819
   Zhou YQ, 2018, IEEE INT CONF AUTOMA, P769, DOI 10.1109/FG.2018.00121
NR 64
TC 24
Z9 24
U1 3
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1609
EP 1621
DI 10.1109/TMM.2021.3068840
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, CX
   Mao, ZD
   Zhang, TZ
   Liu, AA
   Wang, B
   Zhang, YD
AF Liu, Chunxiao
   Mao, Zhendong
   Zhang, Tianzhu
   Liu, An-An
   Wang, Bin
   Zhang, Yongdong
TI Focus Your Attention: A Focal Attention for Multimodal Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Task analysis; Visualization; Interference; Stacking; Neural
   networks; Feature extraction; Focal attention; multimodal learning
ID IMAGE; TEXT
AB The key point in multimodal learning is to learn semantic alignment that finds the correspondence between sub-elements of instances from different modality data. Attention mechanism has shown its power in semantic alignment learning as it enables to densely associate sub-elements across different modalities. However, for each sub-element, existing attention aligns it with all the sub-elements from another modality, while most of them have no correspondence with it, i.e. irrelevant sub-elements. The irrelevant sub-elements will distract the semantic alignment if they are also attended. In this paper, we propose a novel focal attention mechanism to learn more accurate semantic alignment. The focal attention sparsely attends to a subset of sub-elements, which are identified as relevant ones according to their posterior probabilities given each sub-element from another modality. Based on the observation that relevant sub-elements mostly describe the same semantic, the posterior probability can precisely distinguish relevant and irrelevant ones by taking interactions within the same modality into consideration, such that relevant sub-elements get higher and closer posterior probabilities, while irrelevant ones get lower probabilities. Such a design learns better semantic alignment by preventing the interference of irrelevant sub-elements, and it facilitates subsequent multimodal tasks that demand semantic alignment. To validate the effectiveness of the focal attention, we conduct extensive experiments on image-text matching and text-to-image generation, and we propose a bidirectional and stacked version of focal attention for them, respectively. Experimental results on benchmarks show that the focal attention can significantly and consistently outperform state-of-the-arts.
C1 [Liu, Chunxiao; Mao, Zhendong; Zhang, Tianzhu; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
   [Liu, Chunxiao] Chinese Acad Sci, Univ Chinese Acad Sci, Sch Cyber Secur, Inst Informat Engn, Beijing 100093, Peoples R China.
   [Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Wang, Bin] Xiaomi AI Lab, Beijing 100085, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Information
   Engineering, CAS; University of Chinese Academy of Sciences, CAS;
   Tianjin University
RP Mao, ZD (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
EM liuchunxiao@iie.ac.cn; zdmao@ustc.edu.cn; tzzhang@ustc.edu.cn;
   anan0422@gmail.com; wangbin11@xiaomi.com; zhyd73@ustc.edu.cn
RI Zhang, Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106; zhang, tian zhu/0000-0003-1856-9564
FU National Key Research and Development Program [2020YFB1406603,
   U19A2057]; National Science Fund for Distinguished Young Scholars
   [61525206]; Fundamental Research Funds for the Central Universities
   [WK3480000008]
FX This work is supported by the National Key Research and Development
   Program under Grant 2020YFB1406603, in part by theNationalNatural
   Science Foundation of China, under Grant U19A2057, in part by the
   National Science Fund for Distinguished Young Scholars 61525206, in part
   by the Fundamental Research Funds for the Central Universities, under
   Grant WK3480000008. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. C. Spampinato.
CR Agrawal A, 2017, INT J COMPUT VISION, V123, P4, DOI 10.1007/s11263-016-0966-6
   Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374
   Cao QX, 2018, PROC CVPR IEEE, P7249, DOI 10.1109/CVPR.2018.00757
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Devlin J., 2018, BERT PRE TRAINING DE
   Dong L, 2019, ADV NEUR IN, V32
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Frome A., 2013, P ADV NEUR INF PROC
   Gao LL, 2019, AAAI CONF ARTIF INTE, P8312
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huang FR, 2019, IEEE T IMAGE PROCESS, V28, P2008, DOI 10.1109/TIP.2018.2882225
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Karpathy A, 2014, ADV NEUR IN, V27
   Kim JH, 2018, ADV NEUR IN, V31
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Liang JW, 2019, IEEE T PATTERN ANAL, V41, P1893, DOI 10.1109/TPAMI.2018.2890628
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1175, DOI 10.1145/3343031.3350993
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Lu JS, 2016, ADV NEUR IN, V29
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Ma S, 2018, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2018.00593
   Mansimov Elman, 2015, ARXIV151102793
   Mao J, 2014, CELL DEATH DIS, V5, DOI 10.1038/cddis.2013.515
   Mithun NC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1856, DOI 10.1145/3240508.3240712
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Nguyen TV, 2013, IEEE T MULTIMEDIA, V15, P1910, DOI 10.1109/TMM.2013.2272919
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Norcliffe-Brown W, 2018, ADV NEUR IN, V31
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian XM, 2011, IEEE T MULTIMEDIA, V13, P639, DOI 10.1109/TMM.2011.2111363
   Vendrov Ivan, 2015, Order-embeddings of images and language
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang SP, 2017, IEEE T MULTIMEDIA, V19, P1454, DOI 10.1109/TMM.2017.2663324
   Wu YL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P825, DOI 10.1145/3240508.3240521
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zheng HL, 2020, IEEE T IMAGE PROCESS, V29, P476, DOI 10.1109/TIP.2019.2921876
NR 62
TC 5
Z9 5
U1 7
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 103
EP 115
DI 10.1109/TMM.2020.3046855
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300008
DA 2024-07-18
ER

PT J
AU Luan, X
   Zhao, YY
   Ou, WH
   Liu, LH
   Li, WS
   Shu, YC
   Geng, HM
AF Luan, Xiao
   Zhao, Yuanyuan
   Ou, Weihua
   Liu, Linghui
   Li, Weisheng
   Shu, Yucheng
   Geng, Hongmin
TI Collaborative Learning With a Multi-Branch Framework for Feature
   Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative work; Computer architecture; Training; Task analysis;
   Visualization; Convolutional codes; Computer vision; BranchNet;
   collaborative learning; feature enhancement
AB Feature representation is highly important for many computer vision tasks. A broad range of prior studies have been proposed to strengthen representation ability of architectures via built-in blocks. However, during the forward propagation, the reduction in feature map scales still leads to the lack of representation ability. In this paper, we focus on boosting the representational power of a convolutional network by the multi-branch framework that we term the BranchNet. Each branch is directly supervised by label information to enrich the hierarchy features in BranchNet. Based on this framework, we further propose a collaborative learning loss and a soft target loss to transfer knowledge from deeper layers to shallow layers. BranchNet is an efficient training framework without extra parameters introduced in inference and can be integrated in existing networks, e.g., VGG, ResNet, and DenseNet. We evaluate BranchNet on all of these models and find that our method outperforms the baseline models on the widely-used CIFAR and ImageNet datasets. In particular, on the CIFAR-100 dataset, the classification error of ResNet-164 with BranchNet decreases by 4.51 percent. We also conduct experiments on the representative computer vision tasks of instance segmentation and class activation mapping, further verifying the superiority of BranchNet over the baseline models. Models and code are available at https://github.com/zyyupup/BranchNet/.
C1 [Luan, Xiao; Zhao, Yuanyuan; Li, Weisheng; Shu, Yucheng; Geng, Hongmin] Chongqing Univ Posts & Telecommun, Sch Comp Sci, Chongqing 400065, Peoples R China.
   [Ou, Weihua] Guizhou Normal Univ, Sch Big Data & Comp Sci, Guiyang 550025, Peoples R China.
   [Liu, Linghui] Chongqing Univ Posts & Telecommun, Sch Software Engn, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Guizhou Normal
   University; Chongqing University of Posts & Telecommunications
RP Luan, X (corresponding author), Chongqing Univ Posts & Telecommun, Sch Comp Sci, Chongqing 400065, Peoples R China.
EM luanxiao@cqupt.edu.cn; zyy31213121@gmail.com; ouweihuahust@gmail.com;
   liulh@cqupt.edu.cn; liws@cqupt.edu.cn; shyc@cqupt.edu.cn;
   genghm1994@gmail.com
RI zhao, yang/HTN-4320-2023; Ou, Weihua/T-9156-2019; ZHAO,
   YUAN/HCI-5831-2022
OI Ou, Weihua/0000-0001-5241-7703; Geng, Hongmin/0000-0003-0867-175X
FU National Natural Science Foundation of China [61801068, 61972060,
   U1713213, 61502067, 61906024, 61962010, 61762021]; Natural Science
   Foundation of Chongqing [cstc2015jcyjA40013, cstc2015jcyjA40034,
   cstc2016jcyjA0407, cstc2019jcyj-msxmX0461]; Science and Technology
   Research Program of Chongqing Municipal Education Commission
   [KJZD-K201900601]; Natural Science Foundation of Guizhou Province
   [[2017]1130, [2017]5726-32]; Excellent Young Scientific, and
   Technological Talent of Guizhou Province [[2019]-5670]; Graduate
   Scientific Research, and Innovation Foundation of Chongqing [CYS19262]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61801068, 61972060, U1713213, 61502067,
   61906024, 61962010, and 61762021, in part by the Natural Science
   Foundation of Chongqing under Grants cstc2015jcyjA40013,
   cstc2015jcyjA40034, cstc2016jcyjA0407, and cstc2019jcyj-msxmX0461, in
   part by the the Science and Technology Research Program of Chongqing
   Municipal EducationCommission (No. KJZD-K201900601), Natural Science
   Foundation ofGuizhou Province underGrant [2017]1130 and [2017]5726-32,
   in part by the Excellent Young Scientific, and Technological Talent of
   Guizhou Province ([2019]-5670), and Graduate Scientific Research, and
   Innovation Foundation of Chongqing under Grant CYS19262.
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Batra T., 2017, ARXIV170505512
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZR, 2019, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR.2019.00939
   Dean J., 2015, NIPS DEEP LEARNING R
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong L, 2018, IEEE T MULTIMEDIA, V20, P2012, DOI 10.1109/TMM.2017.2788205
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Duo Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7639, DOI 10.1109/CVPR42600.2020.00766
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fukui H, 2019, PROC CVPR IEEE, P10697, DOI 10.1109/CVPR.2019.01096
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   He D, 2016, ADV NEUR IN, V29
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffer E, 2017, ADV NEUR IN, V30
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Huang Gao, 2017, ARXIV170309844
   Huang Z., 2017, ARXIV170701219
   Ioffe S., 2015, P INT C LEARN REPR S
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Keskar N. S., 2017, ICLR, P1
   Komodakis N, 2017, P ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Masters D, 2018, ARXIV, DOI 10.48550/arXiv.1804.07612
   Pinheiro PO, 2014, PR MACH LEARN RES, V32
   Qiurui Wang, 2019, IEEE Transactions on Multimedia, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Redmon J., 2018, IEEE C COMPUTER VISI
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Romero A., 2015, ICLR, P1, DOI DOI 10.48550/ARXIV.1412.6550
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simonyan K., 2015, P ICLR
   Simonyan K, 2014, ADV NEUR IN, V27
   Smith SL, 2018, P 6 INT C LEARN REPR, P1
   Srivastava RK., 2015, P 28 INT C NEURAL IN, P2377, DOI DOI 10.48550/ARXIV.1507.06228
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhang YK, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4348
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao BN, 2019, NEUROCOMPUTING, V365, P273, DOI 10.1016/j.neucom.2019.07.078
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 61
TC 0
Z9 0
U1 3
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 929
EP 941
DI 10.1109/TMM.2021.3061810
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100032
DA 2024-07-18
ER

PT J
AU Mao, YD
   Jiang, QP
   Cong, RM
   Gao, W
   Shao, F
   Kwong, S
AF Mao, Yudong
   Jiang, Qiuping
   Cong, Runmin
   Gao, Wei
   Shao, Feng
   Kwong, Sam
TI Cross-Modality Fusion and Progressive Integration Network for Saliency
   Prediction on Stereoscopic 3D Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Fuses; Decoding; Three-dimensional displays;
   Predictive models; Pipelines; Visualization; Stereoscopic 3D image;
   saliency prediction; visual attention; cross-modality fusion;
   Progressive integration
ID COMPUTATIONAL MODEL; DEPTH
AB Traditional 2D image-based saliency prediction models suffer from unsatisfactory performance when dealing with stereoscopic 3D (S3D) images because eye movements in the case of freely viewing S3D images are demonstrated to be guided by both RGB and depth features. This paper studies the problem of saliency prediction on S3D images, where the interactions between RGB and depth modalities are both taken into account. Specifically, we design a novel deep neural network named Cross-modality Fusion and Progressive Integration Network (CFPI-Net) to address this problem. It consists of a Multi-level Cross-modality Feature Fusion (MCFF) module and a Multi-stage Progressive Feature Integration (MPFI) module. The MCFF module first captures hierarchical contexture features from each modality and then effectively fuses the hierarchical contexture features from different modalities at each level. The MPFI module involves multiple cascaded deeply supervised feature integration (DSFI) blocks in which the low-level and high-level cross-modality features are progressively integrated using the integrated features in the previous stage as a guidance. Our proposed CFPI-Net benefits from the advantages of multi-level feature representation, cross-modality feature fusion, and multi-stage progressive feature integration, which hereby fully boost the performance. Experimental results on two benchmark datasets demonstrate that CFPI-Net outperforms state-of-the-art saliency prediction methods both quantitatively and qualitatively. All the results and relevant codes will be made available to the public.
C1 [Mao, Yudong; Jiang, Qiuping; Shao, Feng] Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Cong, Runmin] Beijing Jiaotong Univ, Inst Informat Sci, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Gao, Wei] Peking Univ Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Gao, Wei] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong 518057, Peoples R China.
C3 Ningbo University; Beijing Jiaotong University; Peng Cheng Laboratory;
   City University of Hong Kong
RP Jiang, QP (corresponding author), Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 562809428@qq.com; jiangqiuping@nbu.edu.cn; rmcong@bjtu.edu.cn;
   gaowei262@pku.edu.cn; shaofeng@nbu.edu.cn; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012; Jiang, Qiuping/AAL-8273-2020
OI Kwong, Sam/0000-0001-7484-7261; Qiuping, Jiang/0000-0002-6025-9343; MAO,
   YUDONG/0000-0003-4718-8592
FU National Natural Science Foundation of China [61901236, 62071261,
   62002014]; Ningbo Natural Science Foundation [2019A610097]; Zhejiang
   Natural Science Foundation [R18F010008]; Fundamental Research Funds for
   the Provincial Universities of Zhejiang [SJLZ2020003]; Beijing Nova
   Program [Z201100006820016]; China Association for Science and Technology
   [2020QNRC001]; K.C. Wong Magna Fund at Ningbo University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61901236, 62071261, and 62002014, in
   part by the Ningbo Natural Science Foundation under Grant 2019A610097,
   in part by the Zhejiang Natural Science Foundation under Grant
   R18F010008, in part by the Fundamental Research Funds for the Provincial
   Universities of Zhejiang (SJLZ2020003), in part by the Beijing Nova
   Program (Z201100006820016), in part by Young Elite Scientist Sponsorship
   Program by the China Association for Science and Technology
   (2020QNRC001), and in part by the K.C. Wong Magna Fund at Ningbo
   University. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhu Liu.
CR Aslantas V, 2007, OPT EXPRESS, V15, P1011, DOI 10.1364/OE.15.001011
   Borji A., 2013, IEEE transactions on pattern analysis and machine intelligence, V35, P185
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Cheng H, 2019, IEEE T MULTIMEDIA, V21, P678, DOI 10.1109/TMM.2018.2864613
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Duan L., 2012, J COMPUTAT THEORETIC, V6, P385
   Fang YM, 2017, IEEE T IMAGE PROCESS, V26, P4684, DOI 10.1109/TIP.2017.2721112
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Farell B, 1998, NATURE, V395, P689, DOI 10.1038/27192
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Fu KR, 2019, NEUROCOMPUTING, V356, P69, DOI 10.1016/j.neucom.2019.04.062
   Guo F, 2020, IEEE T CIRC SYST VID, V30, P4887, DOI 10.1109/TCSVT.2019.2906226
   Guo F, 2018, IEEE T CYBERNETICS, V48, P3159, DOI 10.1109/TCYB.2017.2761361
   Hadizadeh H, 2016, PATTERN RECOGN LETT, V84, P49, DOI 10.1016/j.patrec.2016.08.011
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu B, 2016, VISION RES, V119, P42, DOI 10.1016/j.visres.2015.12.004
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Iatsun L., 2015, SIGNAL PROCESS-IMAGE, V38
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jetley S, 2016, PROC CVPR IEEE, P5753, DOI 10.1109/CVPR.2016.620
   Jiang QP, 2019, IEEE T IMAGE PROCESS, V28, P1866, DOI 10.1109/TIP.2018.2881828
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Jiang QP, 2015, SIGNAL PROCESS-IMAGE, V38, P57, DOI 10.1016/j.image.2015.04.007
   Kingma D. P., 2014, arXiv
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Liu C, 2019, IEEE T CYBERNETICS, V49, P3665, DOI 10.1109/TCYB.2018.2846361
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Ma CY, 2015, J VISION, V15, DOI 10.1167/15.6.19
   Pan J., 2017, PROC IEEE C COMPUT V
   Paszke A, 2019, ADV NEUR IN, V32
   Peng JT, 2016, IEEE T CYBERNETICS, V46, P1616, DOI 10.1109/TCYB.2015.2453091
   Wang WG, 2018, IEEE T CIRC SYST VID, V28, P1727, DOI 10.1109/TCSVT.2017.2701279
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhou T, 2020, IEEE T MED IMAGING, V39, P2772, DOI 10.1109/TMI.2020.2975344
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
NR 53
TC 11
Z9 12
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2435
EP 2448
DI 10.1109/TMM.2021.3081260
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600016
DA 2024-07-18
ER

PT J
AU Mei, YX
   Li, L
   Li, Z
   Li, F
AF Mei, Yixin
   Li, Li
   Li, Zhu
   Li, Fan
TI Learning-Based Scalable Image Compression With Latent-Feature Reuse and
   Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Standards; Redundancy; Streaming media; Scalability; Image
   reconstruction; Spatial resolution; Scalable image compression;
   end-to-end optimized auto-encoder; feature-domain prediction;
   convolutional neural network
ID VIDEO; EXTENSIONS; MULTICAST
AB Recently, learning-based image compression model has attracted much attention due to its impressive performance and ease of optimization, compared with traditional DCT and wavelet-based image compression standards. Most learning-based image compression models are trained to minimize joint rate-distortion (RD) loss on one single RD trade-off point. However, in many multimedia applications, due to communication constraints, or display adaptation needs for different spatial formats, bit rates or power, it is necessary to provide a variety of image versions for different client devices. To fulfill this requirement, typical end-to-end image compression methods have to compress an image into several bit streams independently by a number of pre-trained networks, which are resource-consuming because of redundancy among these streams. To address this problem, inspired by traditional scalable video coding framework, we propose a learning-based end-to-end quality and spatial scalable image compression (QSSIC) model in multi-layer structure, in which each layer could generate one bitstream corresponding to a specified resolution and image fidelity. This scalability is achieved by exploring the potential of feature-domain representation prediction and reuse. To be specific, firstly, bitstreams of previous layers are used to predict the current layer representations which contains the enhancement information, and then only prediction residuals need to be coded in enhancement layers. Secondly, previous bitstreams are reused in image reconstruction in higher layers to provide basic information. The proposed model could be optimized in an end-to-end manner. Extensive experiments show that our method outperforms state-of-art deep neural networks (DNN)-based auto-encoders in simulcast scenarios. In addition, our method has a better performance than the traditional scalable image compression method scalable extension of H.264/AVC (SVC) and is comparable to scalable extension of H.265/HEVC (SHVC).
C1 [Mei, Yixin; Li, Fan] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Li, Li] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Li, Zhu] Univ Missouri, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
C3 Xi'an Jiaotong University; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS; University of Missouri System;
   University of Missouri Kansas City
RP Li, F (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
EM xamichelle@stu.xjtu.edu.cn; lil1@ustc.edu.cn; lizhu@u.k.edu;
   lifan@mail.xjtu.edu.cn
RI Li, Zhu/AAD-8182-2021
OI Li, Zhu/0000-0002-8246-177X
FU National Science Foundation of China (NSFC) [U1903213]; National Science
   Foundation, United States (NSF) [1747751]
FX This work was supported in part by the National Science Foundation of
   China (NSFC) under Grant U1903213, and in part by National Science
   Foundation, United States (NSF) award under Grant 1747751.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Agustsson E, 2017, ADV NEUR IN, V30
   Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031
   [Anonymous], 2018, CVPR WORKSH
   [Anonymous], 2017, SHM-12.4 Software Package
   [Anonymous], FFMPEG tool
   [Anonymous], 2007, JSVM-9.19 Software Package
   [Anonymous], 2007, 14496102005FDAM3 ISO
   [Anonymous], 2019, Workshop and challenge on learned image compression
   Balle J., 2017, 5 INT C LEARN REPR
   Balle J., 2016, P INT C LEARN REPR
   Balle J, 2018, ICLR
   Ballé J, 2018, PICT COD SYMP, P248, DOI 10.1109/PCS.2018.8456272
   Balle Johannes, 2018, TENSORFLOW COMPRESSI
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Cai CL, 2019, IEEE T CIRC SYST VID, V29, P3687, DOI 10.1109/TCSVT.2018.2880492
   Chen T., 2019, arXiv
   Cheng ZX, 2020, IEEE T MULTIMEDIA, V22, P860, DOI 10.1109/TMM.2019.2938345
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HoangVan X, 2017, IEEE T CIRC SYST VID, V27, P1761, DOI 10.1109/TCSVT.2016.2543120
   Jia CM, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P431, DOI 10.1109/MIPR.2019.00087
   Jiang F, 2018, IEEE T CIRC SYST VID, V28, P3007, DOI 10.1109/TCSVT.2017.2734838
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kodak E, 1993, KODAK LOSSLESS TRUE
   Lee, 2019, INT C LEARN REPR
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P361, DOI 10.1109/TMM.2017.2745709
   Liu H., 2018, P IEEE C COMP VIS PA
   Ma HC, 2020, IEEE T MULTIMEDIA, V22, P1667, DOI 10.1109/TMM.2019.2957990
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Minnen D, 2018, ADV NEUR IN, V31
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   RISSANEN J, 1981, IEEE T INFORM THEORY, V27, P12, DOI 10.1109/TIT.1981.1056282
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shen Q, 2018, LECT NOTES COMPUT SC, V11164, P3, DOI 10.1007/978-3-030-00776-8_1
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Theis L., 2017, ICLR
   Toderici G., 2016, P INT C LEARN REPR I
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Yang J, 2018, IEEE T MULTIMEDIA, V20, P1260, DOI 10.1109/TMM.2017.2760630
   Yang SH, 2017, IEEE T CIRC SYST VID, V27, P1555, DOI 10.1109/TCSVT.2016.2539639
NR 45
TC 6
Z9 6
U1 3
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4143
EP 4157
DI 10.1109/TMM.2021.3114548
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400034
OA Bronze
DA 2024-07-18
ER

PT J
AU Perez, M
   Liu, J
   Kot, AC
AF Perez, Mauricio
   Liu, Jun
   Kot, Alex C.
TI Interaction Relational Network for Mutual Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognition; Videos; Task analysis; Feature extraction; Skeleton; Data
   mining; Support vector machines; Interaction recognition; pose
   information; relational reasoning; skeleton based
AB Person-person mutual action recognition (also referred to as interaction recognition) is an important research branch of human activity analysis. Current solutions in the field - mainly dominated by CNNs, GCNs and LSTMs - often consist of complicated architectures and mechanisms to embed the relationships between the two persons on the architecture itself, to ensure the interaction patterns can be properly learned. Our main contribution with this work is by proposing a simpler yet very powerful architecture, named Interaction Relational Network, which utilizes minimal prior knowledge about the structure of the human body. We drive the network to identify by itself how to relate the body parts from the individuals interacting. In order to better represent the interaction, we define two different relationships, leading to specialized architectures and models for each. These multiple relationship models will then be fused into a single and special architecture, in order to leverage both streams of information for further enhancing the relational reasoning capability. Furthermore we define important structured pair-wise operations to extract meaningful extra information from each pair of joints - distance and motion. Ultimately, with the coupling of an LSTM, our IRN is capable of paramount sequential relational reasoning. These important extensions we made to our network can also be valuable to other problems that require sophisticated relational reasoning. Our solution is able to achieve state-of-the-art performance on the traditional interaction recognition datasets SBU and UT, and also on the mutual actions from the large-scale dataset NTU RGB+D. Furthermore, it obtains competitive performance in the NTU RGB+D 120 dataset interactions subset.
C1 [Perez, Mauricio; Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Rapid Rich Object Search Lab, Singapore 639798, Singapore.
   [Liu, Jun] Singapore Univ Technol & Design, ISTD Pillar, Singapore 639798, Singapore.
C3 Nanyang Technological University; Singapore University of Technology &
   Design
RP Perez, M (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Rapid Rich Object Search Lab, Singapore 639798, Singapore.
EM mauricio001@ntu.edu.sg; jun_liu@sutd.edu.sg; eackot@ntu.edu.sg
RI Perez, Mauricio Lisboa/HSH-3396-2023
OI Perez, Mauricio Lisboa/0000-0002-4296-9202; Liu,
   Jun/0000-0002-4365-4165; Kot, Alex/0000-0001-6262-8125
FU Rapid-Rich Object Search (ROSE) Laboratory, Nanyang Technological
   University (NTU), Singapore; NTU College of Engineering [M4081746.D90];
   SUTD SGP-AI grant
FX This work was supported in part by the Rapid-Rich Object Search (ROSE)
   Laboratory, Nanyang Technological University (NTU), Singapore, in part
   by a grant from NTU College of Engineering (M4081746.D90), and in part
   by SUTD SGP-AI grant. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Xilin Chen.
   (Corresponding author: Mauricio Perez.)
CR Aliakbarian MS, 2017, IEEE I CONF COMP VIS, P280, DOI 10.1109/ICCV.2017.39
   Aydin R, 2014, IN C IND ENG ENG MAN, P1, DOI 10.1109/IEEM.2014.7058588
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chowdhury MIH, 2018, IEEE IMAGE PROC, P599, DOI 10.1109/ICIP.2018.8451103
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Ibrahim MS, 2018, LECT NOTES COMPUT SC, V11207, P742, DOI 10.1007/978-3-030-01219-9_44
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Ji YL, 2015, J VIS COMMUN IMAGE R, V33, P340, DOI 10.1016/j.jvcir.2015.10.001
   Ke QH, 2020, IEEE T IMAGE PROCESS, V29, P959, DOI 10.1109/TIP.2019.2937757
   Ke Q, 2018, IEEE T MULTIMEDIA, V20, P1712, DOI 10.1109/TMM.2017.2778559
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Kong Y, 2016, IEEE T IMAGE PROCESS, V25, P167, DOI 10.1109/TIP.2015.2498410
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Li WB, 2015, IEEE I CONF COMP VIS, P4444, DOI 10.1109/ICCV.2015.505
   Liu J, 2019, INT J COMPUT VISION, V127, P1545, DOI 10.1007/s11263-019-01192-2
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P1453, DOI 10.1109/TPAMI.2019.2898954
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Park S., 2018, BRIT MACH VIS C 2018, P1
   Perez Mauricio, 2020, Pattern Recognition. 5th Asian Conference, ACPR 2019. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12046), P268, DOI 10.1007/978-3-030-41404-7_19
   Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Santoro A, 2017, ADV NEUR IN, V30
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shi YG, 2018, LECT NOTES COMPUT SC, V11214, P305, DOI 10.1007/978-3-030-01249-6_19
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Simonyan K, 2014, ADV NEUR IN, V27
   Vahdat A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1729, DOI 10.1109/ICCVW.2011.6130458
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang XY, 2017, IEEE T PATTERN ANAL, V39, P1770, DOI 10.1109/TPAMI.2016.2616308
   Wu HM, 2018, IEEE T HUM-MACH SYST, V48, P304, DOI 10.1109/THMS.2017.2776211
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yu F., P INT C LEARN REPR, P1
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang YM, 2012, LECT NOTES COMPUT SC, V7574, P707, DOI 10.1007/978-3-642-33712-3_51
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 47
TC 28
Z9 28
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 366
EP 376
DI 10.1109/TMM.2021.3050642
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yi, J
   Chen, ZZ
AF Yi, Jing
   Chen, Zhenzhong
TI Multi-Modal Variational Graph Auto-Encoder for Recommendation Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Uncertainty; Feature extraction; Semantics; Collaboration;
   Visualization; Fuses; Convolution; Multi-modal analysis; variational
   auto-encoder; graph convolution network; recommendation systems
ID MATRIX FACTORIZATION TECHNIQUES
AB Graph embedding based methods have been used in recommendation systems recently, owing to their advances in modeling nodes as embeddings in a low-dimensional space. By effective neighborhood aggregation, graph convolutional networks can exploit high-order connections of neighbors such that the learned embeddings could be more informative thus improve the recommendation performance. However, user and item representations learned by graph aggregation inherently contain uncertainty due to sparsity of user-item interactions and noise of item features. To address these challenges, we propose a multi-modal variational graph auto-encoder (MVGAE) method. Specifically, we design modality-specific variational encoders that learn a Gaussian variable for each node whereas the mean vector represents semantic information and the variance vector denotes the noise level of the corresponding modality. Moreover, with the conditional independence assumption, the modality-specific Gaussian node embeddings are fused according to the product-of-experts principle, where the semantic information in each modality is weighted based on the estimated uncertainty level. Extensive experiments on three public datasets, Amazon Movies, Amazon Electronics and AliShop-7 C, demonstrate that our proposed method achieves competitive performance when compared with the state-of-the-art algorithms.
C1 [Yi, Jing; Chen, Zhenzhong] Wuhan Univ, Sch Comp Sci, Wuhan 430079, Hubei, Peoples R China.
   [Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Hubei, Peoples R China.
C3 Wuhan University; Wuhan University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430079, Hubei, Peoples R China.
EM yijing-v@whu.edu.cn; zzchen@ieee.org
RI Chen, Zhenzhong/C-2529-2015
FU National Natural Science Foundation of China [62036005, 61771348];
   Fundamental Research Funds for the Central Universities [2042020KF0205]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62036005 and 61771348 also in part by
   the Fundamental Research Funds for the Central Universities under Grant
   2042020KF0205. The Guest Editor coordinating the review of this
   manuscript and approving it for publication was Jian Zhang.
CR Arora S, 2019, 5 INT C LEARN REPR I
   Beck P. D., 2017, P C LABS EV FOR
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen QW, 2019, 1ST INTERNATIONAL WORKSHOP ON DEEP LEARNING PRACTICE FOR HIGH-DIMENSIONAL SPARSE DATA WITH KDD (DLP-KDD 2019), DOI 10.1145/3326937.3341261
   Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190
   Cui Q, 2020, IEEE T KNOWL DATA EN, V32, P317, DOI 10.1109/TKDE.2018.2881260
   de Souza Pereira Moreira Gabriel, 2019, CEUR WORKSHOP P, V2554, P18
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang H, 2019, LECT NOTES COMPUT SC, V11496, P574, DOI 10.1007/978-3-030-19274-7_47
   Hastie T, 2015, J MACH LEARN RES, V16, P3367
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   He XN, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P8, DOI 10.1145/3206025.3210497
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hidasi B., 2016, 4 INT C LEARN REPR I
   Huang ZH, 2019, IEEE INTERNET THINGS, V6, P10675, DOI 10.1109/JIOT.2019.2940709
   Kingma D. P., 2014, arXiv
   Kipf Thomas N., 2017, 5 INT C LEARN REPRES
   Kipf TN, 2016, ARXIV
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Li XP, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P305, DOI 10.1145/3097983.3098077
   Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1526, DOI 10.1145/3343031.3350953
   Liu J, 2010, IUI 2010, P31
   Ma JX, 2019, ADV NEUR IN, V32
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Nguyen J, 2013, STAT ANAL DATA MIN, V6, P286, DOI 10.1002/sam.11184
   Oramas S., 2017, P 2 WORKSH DEEP LEAR, P32, DOI DOI 10.1145/3125486.3125492
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Sarwar B, 2002, 5 INT C COMPUTER INF, P27
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Shen JL, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P1131
   Shi Y., 2019, P ADV NEURAL INFORM, VVolume 32
   Smith B, 2017, IEEE INTERNET COMPUT, V21, P12, DOI 10.1109/MIC.2017.72
   Sun R, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1405, DOI 10.1145/3340531.3411947
   Suzuki M., 2017, P INT C LEARN REPR W
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van der Merwe R, 2019, ARCH REC, V40, P239, DOI 10.1080/23257962.2017.1388224
   Wang H, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1235, DOI 10.1145/2783258.2783273
   Wang J., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P501, DOI 10.1145/1148170.1148257
   Wang JZ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P839, DOI 10.1145/3219819.3219869
   Wang MH, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2349, DOI 10.1145/3394486.3403284
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wu MK, 2018, ADV NEUR IN, V31
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu JX, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3356, DOI 10.1145/3394486.3403388
   Xu XR, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P482, DOI 10.1145/3240323.3241730
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Zhang L, 2022, IEEE T MULTIMEDIA, V24, P1830, DOI 10.1109/TMM.2021.3073267
   Zhang YF, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1449, DOI 10.1145/3132847.3132892
NR 54
TC 7
Z9 7
U1 2
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1067
EP 1079
DI 10.1109/TMM.2021.3111487
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800005
DA 2024-07-18
ER

PT J
AU Zhan, HJ
   Lin, J
   Ak, KE
   Shi, BX
   Duan, LY
   Kot, AC
AF Zhan, Huijing
   Lin, Jie
   Ak, Kenan Emir
   Shi, Boxin
   Duan, Ling-Yu
   Kot, Alex C.
TI A<SUP>3</SUP>-FKG: Attentive Attribute-Aware Fashion Knowledge Graph for
   Outfit Preference Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention; attribute-aware; knowledge graph; multi-modal; personalized
   preference prediction
ID HETEROGENEOUS INFORMATION
AB With the booming development of the online fashion industry, effective personalized recommender systems have become indispensable for the convenience they brought to the customers and the profits to the e-commercial platforms. Estimating the user's preference towards the outfit is at the core of a personalized recommendation system. Existing works on fashion recommendation are largely centering on modelling the clothing compatibility without considering the user factor or characterizing the user's preference over the single item. However, how to effectively model the outfits with either few or even none interactions, is yet under-explored. In this paper, we address the task of personalized outfit preference prediction via a novel Attentive Attribute-Aware Fashion Knowledge Graph (A(3)-FKG), which is incorporated to build the association between different outfits with both outfit- and item- level attributes. Additionally, a two-level attention mechanism is developed to capture the user's preference: 1) User-specific relation-aware attention layer, which captures the user's fine-grained preferences with different focus on relations for learning outfit representation; 2) Target-aware attention layer, which characterizes the user's latent diverse interests from his/her behavior sequences for learning user representation. Extensive experiments conducted on a large-scale fashion outfit dataset demonstrate significant improvements over other methods, which verify the excellence of our proposed framework.
C1 [Zhan, Huijing; Lin, Jie; Ak, Kenan Emir] Inst Infocomm Res I2R, Singapore 138634, Singapore.
   [Shi, Boxin; Duan, Ling-Yu] Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Peking University; Nanyang Technological
   University
RP Lin, J (corresponding author), Inst Infocomm Res I2R, Singapore 138634, Singapore.
EM zhan_huijing@i2r.a-star.edu.sg; lin-j@i2r.a-star.edu.sg;
   kenan_emir_ak@i2r.a-star.edu.sg; boxin.shi@gmail.com; lingyu@pku.edu.cn;
   eackot@ntu.edu.sg
OI Kot, Alex/0000-0001-6262-8125
FU Agency for Science, Technology and Research (A*STAR) under its AME
   Programmatic Funds Project [A1892b0026]
FX This research is supported by the Agency for Science, Technology and
   Research (A*STAR) under its AME Programmatic Funds Project A1892b0026.
CR [Anonymous], 2013, P ICML
   Bordes A., 2013, P 26 INT C NEURAL IN, P2787
   Charlton G., GLOBAL FASHION ECOMM
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen W, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2662, DOI 10.1145/3292500.3330652
   Chen X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P765, DOI 10.1145/3331184.3331254
   Cui ZY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P307, DOI 10.1145/3308558.3313444
   Dong X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P302, DOI 10.1145/3343031.3350905
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   GOLDBERG D, 1992, COMMUN ACM, V35, P61, DOI 10.1145/138859.138867
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   Hidayati SC, 2021, IEEE T MULTIMEDIA, V23, P365, DOI 10.1109/TMM.2020.2980195
   Hidayati SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P438, DOI 10.1145/3240508.3240546
   Hsiao WL, 2018, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR.2018.00748
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Letarte G., 2018, P EMNLP WORKSH BLACK, P267, DOI DOI 10.18653/V1/W18-5429
   Li JJ, 2019, AAAI CONF ARTIF INTE, P4189
   Li XC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P159, DOI 10.1145/3397271.3401080
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liang DW, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P59, DOI 10.1145/2959100.2959182
   Lin YK, 2015, AAAI CONF ARTIF INTE, P2181
   Lin YJ, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P1095, DOI 10.1145/3308558.3313614
   Lin YS, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P77, DOI 10.1145/3366423.3380096
   Liu SW, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P2029, DOI 10.1145/3397271.3401252
   Liu X, 2021, IEEE T MULTIMEDIA, V23, P2894, DOI 10.1109/TMM.2020.3018021
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lu Z, 2019, PROC CVPR IEEE, P10554, DOI 10.1109/CVPR.2019.01081
   Ma H., 2011, Proceedings of the 4th ACM International Conference on Web Search and Data Mining, P287
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Qin C., 2020, ASURVEY KNOWLEDGE GR
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.5555/1795114.1795167
   Schlichtkrull Michael, 2018, PROC EUR SEMANTIC WE, P593
   Shi C, 2016, KNOWL INF SYST, V49, P835, DOI 10.1007/s10115-016-0925-0
   Simo-Serra E, 2016, PROC CVPR IEEE, P298, DOI 10.1109/CVPR.2016.39
   Song XM, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P320, DOI 10.1145/3343031.3350956
   Tan RB, 2019, IEEE I CONF COMP VIS, P10372, DOI 10.1109/ICCV.2019.01047
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang HW, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P968, DOI 10.1145/3292500.3330836
   Wang HW, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3307, DOI 10.1145/3308558.3313417
   Wang HW, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P417, DOI 10.1145/3269206.3271739
   Wang HW, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P592, DOI 10.1145/3159652.3159666
   Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989
   Wang YY, 2017, MULTIMEDIA SYST, V23, P703, DOI 10.1007/s00530-015-0502-5
   Wang Z, 2014, AAAI CONF ARTIF INTE, P1112
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Xiong SW, 2019, LECT NOTES COMPUT SC, V11158, P106, DOI 10.1007/978-3-030-01391-2_18
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zhan HJ, 2021, IEEE T MULTIMEDIA, V23, P133, DOI 10.1109/TMM.2020.2978669
   Zhan HJ, 2019, IEEE IMAGE PROC, P280, DOI [10.1109/icip.2019.8802939, 10.1109/ICIP.2019.8802939]
   Zhan HJ, 2017, IEEE T IMAGE PROCESS, V26, P5867, DOI 10.1109/TIP.2017.2736346
   Zhang FZ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P353, DOI 10.1145/2939672.2939673
   Zhang H, 2019, PR MACH LEARN RES, V97
NR 59
TC 24
Z9 25
U1 7
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 819
EP 831
DI 10.1109/TMM.2021.3059514
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100024
DA 2024-07-18
ER

PT J
AU Zhang, LB
   Huang, SL
   Liu, W
AF Zhang, Lianbo
   Huang, Shaoli
   Liu, Wei
TI Enhancing Mixture-of-Experts by Leveraging Attention for Fine-Grained
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Training; Costs; Data models; Computational modeling;
   Location awareness; Image recognition; Deep learning; fine-grained
   recognition; image classification; attention; mixture of expert; data
   augmentation
ID PARTS
AB Differentiating subcategories of a common visual category is challenging because of the similar appearance shared among different classes in fine-grained recognition. Existing mixture-of-expert based methods divide the fine-grained space into some specific regions and solve the integrated problem by conquering subspace ones. However, it is not feasible to learn diverse experts directly through data partition strategy because of limited data available for fine-grained recognition problems. To address the issue, we leverage visual attention to learn an enhanced experts' mixture. Specifically, we introduce a gradually-enhanced learning strategy from model attention. The strategy promotes diversity among experts by feeding each expert with full-size data distinct in granularity. We further promote expert's learning by providing it with a larger data space, which is achieved by swapping attentive regions within positive pairs. Our method learns new experts on the dataset with the prior knowledge from former experts sequentially and enforces the experts to learn more diverse but discriminative representation. These enhanced experts are finally combined to make stronger predictions. We conduct extensive experiments on fine-grained benchmarks. The results show that our method consistently outperforms the state-of-the-art method in both weakly supervised localization and fine-grained image classification. Our code is publicly available at https://github.com/lbzhang/Enhanced-Expert-FGVC-Pytorch.git.
C1 [Zhang, Lianbo; Liu, Wei] Univ Technol Sydney, Sch Comp Sci, FEIT, Sydney, NSW 2007, Australia.
   [Huang, Shaoli] Univ Sydney, Sch Comp Sci, FEIT, Sydney, NSW 2008, Australia.
C3 University of Technology Sydney; University of Sydney
RP Liu, W (corresponding author), Univ Technol Sydney, Sch Comp Sci, FEIT, Sydney, NSW 2007, Australia.; Huang, SL (corresponding author), Univ Sydney, Sch Comp Sci, FEIT, Sydney, NSW 2008, Australia.
EM lbzhang.lianbo@gmail.com; shaoli.huang@sydney.edu.au; wei.liu@uts.edu.au
RI huang, shaoli/AAF-2431-2019
FU ARC [FL-170100117]
FX The work of Shaoli Huang was supported by ARC under Grant FL-170100117.
   The Associate Editor coordinating the review of this manuscript and
   approving it for publicationwas Prof. Jinhui Tang. Lianbo Zhang and
   Shaoli Huang contributed equally to this work.)(
CR Antoniou A, 2018, Arxiv, DOI arXiv:1711.04340
   Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Berthelot D, 2019, ADV NEUR IN, V32
   Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen TS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2023, DOI 10.1145/3240508.3240523
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Choe J, 2020, PROC CVPR IEEE, P3130, DOI 10.1109/CVPR42600.2020.00320
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   DeVries T, 2017, Arxiv, DOI arXiv:1702.05538
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Eigen D., 2014, PROC ICLR WORKSHOP
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Ge Z., 2016, 2016 IEEE WINT C APP, P1, DOI [10.1109/WACV.2016.7477700, DOI 10.1109/WACV.2016.7477700]
   Ge ZY, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301271
   Gross S, 2017, PROC CVPR IEEE, P5085, DOI 10.1109/CVPR.2017.540
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P627, DOI 10.1145/3123266.3123319
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Inoue H, 2018, Arxiv, DOI arXiv:1801.02929
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   Jaderberg M, 2015, ADV NEUR IN, V28
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Kaiser L, 2017, Arxiv, DOI arXiv:1706.05137
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Konno T, 2018, Arxiv, DOI arXiv:1807.06540
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2017, IEEE INT CONF COMP V, P1199, DOI 10.1109/ICCVW.2017.145
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shazeer N., 2017, INT C LEARNING REPRE
   Simonyan K., 2014, CORR
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Summers C, 2019, IEEE WINT CONF APPL, P1262, DOI 10.1109/WACV.2019.00139
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Takahashi R, 2020, IEEE T CIRC SYST VID, V30, P2917, DOI 10.1109/TCSVT.2019.2935128
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wah C, 2011, CALTECH UCSD BIRDS 2
   WANG QL, 2018, ADV NEUR IN, V31
   Wang X, 2019, AAAI CONF ARTIF INTE, P8965
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2019, IEEE T CYBERNETICS, V49, P1791, DOI 10.1109/TCYB.2018.2813971
   Wu L, 2018, COMPUT VIS IMAGE UND, V167, P63, DOI 10.1016/j.cviu.2017.11.009
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARN REPRESEN
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhang YB, 2020, IEEE T MULTIMEDIA, V22, P1345, DOI 10.1109/TMM.2019.2939747
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zheng XT, 2021, IEEE T MULTIMEDIA, V23, P1187, DOI 10.1109/TMM.2020.2993960
   Zhihui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9746, DOI 10.1109/CVPR42600.2020.00977
   Zhong Z, 2017, Arxiv, DOI arXiv:1708.04896
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 78
TC 6
Z9 6
U1 6
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4409
EP 4421
DI 10.1109/TMM.2021.3117064
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 6A4RT
UT WOS:000880645000003
DA 2024-07-18
ER

PT J
AU Zhou, H
   Zhou, WG
   Zhou, Y
   Li, HQ
AF Zhou, Hao
   Zhou, Wengang
   Zhou, Yun
   Li, Houqiang
TI Spatial-Temporal Multi-Cue Network for Sign Language Recognition and
   Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Assistive technology; Gesture recognition; Hidden Markov models; Shape;
   Optimization; Visualization; Tools; Multi-cue; pose estimation;
   segmented attention; sign language recognition; sign language
   translation
ID HAND GESTURE RECOGNITION; FRAMEWORK
AB Despite the recent success of deep learning in video-related tasks, deep models typically focus on the most discriminative features, ignoring other potentially non-trivial and informative contents. Such characteristic heavily constrains their capability to learn implicit visual grammars in sign videos behind the collaboration of different visual cues (i.e., hand shape, facial expression and body posture). To this end, we approach video-based sign language understanding with multi-cue learning and propose a spatial-temporal multi-cue (STMC) network to solve the vision-based sequence learning problem. Our STMC network consists of a spatial multi-cue (SMC) module and a temporal multi-cue (TMC) module. The SMC module learns to spatial representation of different cues with a self-contained pose estimation branch. The TMC module models temporal corrections from intra-cue and inter-cue perspectives to explore the collaboration of multiple cues. A joint optimization strategy and a segmented attention mechanism are designed to make the best of multi-cue sources for SL recognition and translation. To validate the effectiveness, we perform experiments on three large-scale sign language benchmarks: PHOENIX-2014, CSL and PHOENIX-2014-T. Experimental results demonstrate that the proposed method achieves new state-of-the-art performance on all three benchmarks.
C1 [Zhou, Hao; Zhou, Wengang; Zhou, Yun; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM zhouh156@mail.ustc.edu.cn; zhwg@ustc.edu.cn; zhouyun@ustc.edu.cn;
   lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
OI , Yun/0009-0000-3594-2151; Zhou, Hao/0000-0001-9764-1012
FU National Natural Science Foundation of China [U20A20183, 61836006,
   61836011]; Youth Innovation Promotion Association CAS [2018497]; GPU
   cluster built by MCC Laboratory of Information Science and Technology
   Institution, USTC
FX This work was supported in part by the National Natural Science
   Foundation of China under Contract U20A20183, 61836006, and 61836011, in
   part by the Youth Innovation Promotion Association CAS under Grant
   2018497, and in part by the GPU cluster built by MCC Laboratory of
   Information Science and Technology Institution, USTC.
CR [Anonymous], 2008, IEEE INT C AUTOMATIC, DOI [DOI 10.1109/AFGR.2008.4813472, 10.1109/AFGR.2008.4813472]
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/D15-1166, DOI 10.48550/ARXIV.1508.04025]
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bengio S, 2015, ADV NEUR IN, V28
   Bowden R, 2016, P 7 WORKSH REPR PROC, P121
   Bragg D, 2019, ASSETS'19: THE 21ST INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P16, DOI 10.1145/3308561.3353774
   Buehler P, 2009, PROC CVPR IEEE, P2953, DOI 10.1109/CVPRW.2009.5206523
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P216, DOI 10.1007/s10791-009-9110-3
   Cooper Helen, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2568, DOI 10.1109/CVPRW.2009.5206647
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P744
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   Hannun A, 2014, DEEP SPEECH SCALING, V1412, P5567
   Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Ko SK, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9132683
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2016, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR.2016.412
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Koller O, 2014, LECT NOTES COMPUT SC, V8689, P281, DOI 10.1007/978-3-319-10590-1_19
   Koller Oscar., 2016, P BMVC, P1, DOI 10.5244/C.30.136
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu Z, 2017, IEEE INT CONF COMP V, P3056, DOI 10.1109/ICCVW.2017.361
   Lopez-Paz D., 2016, International Conference on Learning Representations (ICLR), P1
   Ma WY, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON SMART CITY/SOCIALCOM/SUSTAINCOM (SMARTCITY), P1, DOI [10.1109/NEBEC.2015.7117114, 10.1109/SmartCity.2015.38]
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Ong SCW, 2005, IEEE T PATTERN ANAL, V27, P873, DOI 10.1109/TPAMI.2005.112
   Ong Y.S., 2017, Computer Vision and Pattern Recognition
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pfister T., 2013, P BRIT MACH VIS C
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Simonyan K, 2015, IEEE INT C ICLR
   Simonyan K, 2014, ADV NEUR IN, V27
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sutskever I, 2014, ADV NEUR IN, V27
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang H., 2016, ACM T ACCESS COMPUT, V8, P1, DOI DOI 10.1145/2897735
   Wang HJ, 2019, IEEE T MULTIMEDIA, V21, P2806, DOI 10.1109/TMM.2019.2915032
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
   Zhou H, 2019, IEEE INT CON MULTI, P1282, DOI 10.1109/ICME.2019.00223
NR 61
TC 31
Z9 33
U1 4
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 768
EP 779
DI 10.1109/TMM.2021.3059098
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100020
DA 2024-07-18
ER

PT J
AU Zhu, XY
   Guo, KH
   Fang, H
   Chen, L
   Ren, S
   Hu, B
AF Zhu, Xiangyuan
   Guo, Kehua
   Fang, Hui
   Chen, Liang
   Ren, Sheng
   Hu, Bin
TI Cross View Capture for Stereo Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Superresolution; Feature extraction; Image reconstruction; Spatial
   resolution; Task analysis; Visual perception; Training; Stereo image;
   super-resolution; cross view capture; spatial perception
AB Stereo image super-resolution exploits additional features from cross view image pairs for high resolution (HR) image reconstruction. Recently, several new methods have been proposed to investigate cross view features along epipolar lines to enhance the visual perception of recovered HR images. Despite the impressive performance of these methods, global contextual features from cross view images are left unexplored. In this paper, we propose a cross view capture network (CVCnet) for stereo image super-resolution by using both global contextual and local features extracted from both views. Specifically, we design a cross view block to capture diverse feature embeddings from the views in stereo vision. In addition, a cascaded spatial perception module is proposed to redistribute each location in feature maps according to the weight it occupies to make the extraction of features more effective. Extensive experiments demonstrate that our proposed CVCnet outperforms the state-of-the-art image super-resolution methods to achieve the best performance for stereo image super-resolution tasks. The source code is available at https://github.com/xyzhu1/CVCnet.
C1 [Zhu, Xiangyuan; Guo, Kehua; Chen, Liang; Ren, Sheng; Hu, Bin] Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
   [Fang, Hui] Loughborough Univ, Dept Comp Sci, Loughborough LE11 3TU, Leics, England.
C3 Central South University; Loughborough University
RP Guo, KH (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
EM zhuxiangyuan@csu.edu.cn; guokehua@csu.edu.cn; h.fang@lboro.ac.uk;
   chenliang1@csu.edu.cn; rensheng@csu.edu.cn; hubincsu@csu.edu.cn
OI Hu, Bin/0000-0002-2974-1166; Fang, Hui/0000-0001-9365-7420; Ren,
   Sheng/0000-0002-3597-7124
FU Natural Science Foundation of China [62076255]; Hunan Provincial Science
   and Technology Plan [2020SK2059]; National Science Foundation of Hunan
   Province, China [2019JJ20025, 2019JJ40406]; National Social Science Fund
   of China [20ZD120]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62076255, in part by the Hunan Provincial Science and
   Technology Plan under Project 2020SK2059, in part by the National
   Science Foundation of Hunan Province, China, under Grant 2019JJ20025 and
   Grant 2019JJ40406, and in part by the National Social Science Fund of
   China (No. 20&ZD120).
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen CQ, 2022, IEEE T MULTIMEDIA, V24, P202, DOI 10.1109/TMM.2021.3050092
   Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Islam MB, 2018, IEEE T MULTIMEDIA, V20, P2964, DOI 10.1109/TMM.2018.2820324
   Jeon DS, 2018, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2018.00185
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun GM, 2020, IEEE T MULTIMEDIA, V22, P2938, DOI 10.1109/TMM.2020.2965461
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Wang LG, 2022, IEEE T PATTERN ANAL, V44, P2108, DOI 10.1109/TPAMI.2020.3026899
   Wang LG, 2020, IEEE T IMAGE PROCESS, V29, P4323, DOI 10.1109/TIP.2020.2967596
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zheng H., 2017, BMVC, DOI [10.5244/C.31.138, DOI 10.5244/C.31.138]
   Zheng HT, 2018, LECT NOTES COMPUT SC, V11210, P87, DOI 10.1007/978-3-030-01231-1_6
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 49
TC 56
Z9 57
U1 14
U2 80
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3074
EP 3086
DI 10.1109/TMM.2021.3092571
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000031
OA Green Published
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zuo, YF
   Wang, H
   Fang, YM
   Huang, XS
   Shang, XW
   Wu, Q
AF Zuo, Yifan
   Wang, Hao
   Fang, Yuming
   Huang, Xiaoshui
   Shang, Xiwu
   Wu, Qiang
TI MIG-Net: Multi-Scale Network Alternatively Guided by Intensity and
   Gradient Features for Depth Map Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Superresolution; Image edge detection; Image color analysis; Image
   coding; Color; Noise reduction; Dictionaries; Deep convolutional neual
   network; depth gradient features; depth-guided gradient enhancement;
   gradient-guided depth enhancement; intensity-guided depth map
   super-resolution
ID RECOVERY
AB The studies of previous decades have shown that the quality of depth maps can be significantly lifted by introducing the guidance from intensity images describing the same scenes. With the rising of deep convolutional neural network, the performance of guided depth map super-resolution is further improved. The variants always consider deep structure, optimized gradient flow and feature reusing. Nevertheless, it is difficult to obtain sufficient and appropriate guidance from intensity features without any prior. In fact, features in the gradient domain, e.g., edges, present strong correlations between the intensity image and the corresponding depth map. Therefore, the guidance in the gradient domain can be more efficiently explored. In this paper, the depth features are iteratively upsampled by 2x. In each upsampling stage, the low-quality depth features and the corresponding gradient features are iteratively refined by the guidance from the intensity features via two parallel streams. Then, to make full use of depth features in the image and gradient domains, the depth features and gradient features are alternatively complemented with each other. Compared with state-of-the-art counterparts, the sufficient experimental results show improvements according to the objective and subjective assessments. The code is available at https://github.com/Yifan-Zuo/MIG-net-gradient_guided_depth_enhancement.
C1 [Zuo, Yifan; Wang, Hao; Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330013, Jiangxi, Peoples R China.
   [Huang, Xiaoshui] Univ Sydney, Sch Med & Hlth, Sydney, NSW 2006, Australia.
   [Shang, Xiwu] Shanghai Univ Engn Sci, Sch Elect & Elect Engn, Shanghai 201620, Peoples R China.
   [Wu, Qiang] Univ Technol Sydney, Sch Engn & Informat Technol, Sydney, NSW 2007, Australia.
C3 Jiangxi University of Finance & Economics; University of Sydney;
   Shanghai University of Engineering Science; University of Technology
   Sydney
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330013, Jiangxi, Peoples R China.
EM kenny0410@126.com; w_hao_97@163.com; fa0001ng@e.ntu.edu.sg;
   xiaoshui.huang@sydney.edu.au; dxsxw@126.com; Qiang.Wu@uts.edu.au
RI Zuo, Yifan/JVZ-3041-2024
OI Zuo, Yifan/0000-0003-4980-7211; Wu, Qiang/0000-0001-5641-2483; Huang,
   Xiaoshui/0000-0002-3579-538X
FU National Key R&D Program of China [2018AAA0100601]; National Natural
   Science Foundation of China [61901197, 62001283]; Natural Science
   Foundation of Jiangxi Province [20192BAB217005]; Funding of Postdoctoral
   Science Foundation of China [2020T130266, 2020M682105]; Double Thousand
   Plan of Jiangxi Province
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0100601, in part by the National Natural Science
   Foundation of China under Grants 61901197 and 62001283, in part by the
   Natural Science Foundation of Jiangxi Province under Grant
   20192BAB217005, in part by the Funding of Postdoctoral Science
   Foundation of China under Grants 2020T130266 and 2020M682105, and in
   part by the Double Thousand Plan of Jiangxi Province. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Dr. Lu Fang.
CR [Anonymous], TENSORFLOW ONLINE
   [Anonymous], Middlebury Datasets [Online] (n.d.)
   [Anonymous], 1999, GUIDE
   Bose NK, 2006, IEEE T IMAGE PROCESS, V15, P2239, DOI 10.1109/TIP.2006.877406
   Choi O, 2014, IEEE T IMAGE PROCESS, V23, P3321, DOI 10.1109/TIP.2014.2329766
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Diebel J., 2005, P 18 INT C NEUR INF, V18, P291
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Gu SH, 2017, PROC CVPR IEEE, P712, DOI 10.1109/CVPR.2017.83
   Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257
   Hao X., 2019, Proceedings of 2018 IEEE International Conference of Safety Produce Informatization, IICSPI 2018, P62, DOI [DOI 10.1109/IICSPI.2018.8690367, 10.1109/IICSPI.2018.8690367]
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hornácek M, 2013, PROC CVPR IEEE, P1123, DOI 10.1109/CVPR.2013.149
   Hu W, 2014, IEEE IMAGE PROC, P2056, DOI 10.1109/ICIP.2014.7025412
   Hu W, 2012, IEEE INT CONF MULTI, P605, DOI 10.1109/ICMEW.2012.111
   Hu W, 2013, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2013.6659254
   Huang LQ, 2019, IEEE SIGNAL PROC LET, V26, P1723, DOI 10.1109/LSP.2019.2944646
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Ioffe S., 2015, 32 INT C MACHINE LEA
   Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Kwon H, 2015, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2015.7298611
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Liu W, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2612826
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1636, DOI 10.1109/TIP.2018.2875506
   Lo KH, 2018, IEEE T CYBERNETICS, V48, P371, DOI 10.1109/TCYB.2016.2637661
   Lu T, 2020, NEUROCOMPUTING, V387, P309, DOI 10.1016/j.neucom.2020.01.015
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164
   Pang JH, 2018, PROC CVPR IEEE, P2070, DOI 10.1109/CVPR.2018.00221
   Park J, 2014, IEEE T IMAGE PROCESS, V23, P5559, DOI 10.1109/TIP.2014.2361034
   Riegler G., 2016, BRIT MACH VIS C
   Riegler G, 2016, LECT NOTES COMPUT SC, V9907, P268, DOI 10.1007/978-3-319-46487-9_17
   Song XB, 2019, IEEE T CIRC SYST VID, V29, P2323, DOI 10.1109/TCSVT.2018.2866399
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang J, 2020, IEEE T MULTIMEDIA, V22, P1470, DOI 10.1109/TMM.2019.2946075
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Wen Y, 2019, IEEE T IMAGE PROCESS, V28, P994, DOI 10.1109/TIP.2018.2874285
   Xie J, 2016, IEEE T IMAGE PROCESS, V25, P428, DOI 10.1109/TIP.2015.2501749
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Yang GR, 2019, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2019.00099
   Yang JY, 2019, IEEE T BROADCAST, V65, P123, DOI 10.1109/TBC.2018.2818405
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yang Q., 2007, P IEEE INT C COMP VI, P1
   Yang S, 2018, IEEE T CYBERNETICS, V48, P399, DOI 10.1109/TCYB.2016.2638856
   Yanjie Li, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P152, DOI 10.1109/ICME.2012.30
   Zhang YB, 2020, IEEE T CIRC SYST VID, V30, P320, DOI 10.1109/TCSVT.2018.2890574
   Zuo Y, 2017, IEEE INT CON MULTI, P211, DOI 10.1109/ICME.2017.8019366
   Zuo Y., 2016, IEEE INT CON MULTI, P1
   Zuo YF, 2021, IEEE T MULTIMEDIA, V23, P772, DOI 10.1109/TMM.2020.2987706
   Zuo YF, 2020, IEEE T CIRC SYST VID, V30, P4676, DOI 10.1109/TCSVT.2019.2962867
   Zuo YF, 2020, IEEE T CIRC SYST VID, V30, P297, DOI 10.1109/TCSVT.2018.2890271
   Zuo YF, 2019, INFORM SCIENCES, V495, P52, DOI 10.1016/j.ins.2019.05.003
   Zuo YF, 2018, IEEE T IMAGE PROCESS, V27, P4145, DOI 10.1109/TIP.2018.2828335
   Zuo YF, 2018, IEEE T CIRC SYST VID, V28, P439, DOI 10.1109/TCSVT.2016.2609438
NR 62
TC 10
Z9 11
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 29
PY 2021
VL 24
BP 3506
EP 3519
DI 10.1109/TMM.2021.3100766
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NJ
UT WOS:000824706800002
DA 2024-07-18
ER

PT J
AU Liu, CX
   Kong, DH
   Wang, SF
   Li, JH
   Yin, BC
AF Liu, Caixia
   Kong, Dehui
   Wang, Shaofan
   Li, Jinghua
   Yin, Baocai
TI DLGAN: Depth-Preserving Latent Generative Adversarial Network for 3D
   Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Image reconstruction; Shape; Gallium
   nitride; Generative adversarial networks; Two dimensional displays;
   Transforms; 3D reconstruction; depth loss; ELM; latent vector; monocular
   depth image
ID SHAPE
AB Although deep networks based methods outperform traditional 3D reconstruction methods which require multiocular images or class labels to recover the full 3D geometry, they may produce incomplete recovery and unfaithful reconstruction when facing occluded parts of 3D objects. To address these issues, we propose Depth-preserving Latent Generative Adversarial Network (DLGAN) which consists of 3D Encoder-Decoder based GAN (EDGAN, serving as a generator and a discriminator) and Extreme Learning Machine (ELM, serving as a classifier) for 3D reconstruction from a monocular depth image of an object. Firstly, EDGAN decodes a latent vector from the 2.5D voxel grid representation of an input image, and generates the initial 3D occupancy grid under common GAN losses, a latent vector loss and a depth loss. For the latent vector loss, we design 3D deep AutoEncoder (AE) to learn a target latent vector from ground truth 3D voxel grid and utilize the vector to penalize the latent vector encoded from the input 2.5D data. For the depth loss, we utilize the input 2.5D data to penalize the initial 3D voxel grid from 2.5D views. Afterwards, ELM transforms float values of the initial 3D voxel grid to binary values under a binary reconstruction loss. Experimental results show that DLGAN not only outperforms several state-of-the-art methods by a large margin on both a synthetic dataset and a real-world dataset, but also predicts more occluded parts of 3D objects accurately without class labels.
C1 [Liu, Caixia; Kong, Dehui; Wang, Shaofan; Li, Jinghua; Yin, Baocai] Beijing Univ Technol, Fac Informat Technol, Beijing Artificial Intelligence Inst, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Wang, SF (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing Artificial Intelligence Inst, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM lcxxib@emails.bjut.edu.cn; kdh@bjut.edu.cn; wangshaofan@bjut.edu.cn;
   lijinghua@bjut.edu.cn; ybc@dlut.edu.cn
OI WANG, SHAOFAN/0000-0002-3045-624X
FU National Natural Science Foundation of China [61772049, 61632006,
   61876012, U19B2039]; Beijing Natural Science Foundation [4202003];
   Beijing Outstanding Young Scientists Projects [BJJWZYJH01201910005018]
FX Manuscript received January 30, 2020; revised July 20, 2020; accepted
   August 3, 2020. Date of publication August 24, 2020; date of current
   version August 24, 2021. This work was supported in part by the National
   Natural Science Foundation of China under Grants 61772049, 61632006,
   61876012, and U19B2039, in part by the Beijing Natural Science
   Foundation under Grant 4202003, and in part by the Beijing Outstanding
   Young Scientists Projects BJJWZYJH01201910005018. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Marco Carli. (Corresponding author: Shaofan Wang.)
CR Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586
   Florea R, 2017, IEEE T MULTIMEDIA, V19, P236, DOI 10.1109/TMM.2016.2614483
   Furukawa Y, 2006, LECT NOTES COMPUT SC, V3951, P564
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Gwak J, 2017, INT CONF 3D VISION, P263, DOI 10.1109/3DV.2017.00038
   Hu RZ, 2020, IEEE T VIS COMPUT GR, V26, P739, DOI 10.1109/TVCG.2019.2934799
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23
   Kingma D. P., 2014, arXiv
   Kong C, 2017, PROC CVPR IEEE, P5603, DOI 10.1109/CVPR.2017.594
   Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44
   Li YY, 2015, COMPUT GRAPH FORUM, V34, P435, DOI 10.1111/cgf.12573
   Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Liu CX, 2018, ADV MECH ENG, V10, DOI 10.1177/1687814018772713
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Nash C, 2017, COMPUT GRAPH FORUM, V36, P1, DOI 10.1111/cgf.13240
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Pontes JK, 2019, LECT NOTES COMPUT SC, V11361, P365, DOI 10.1007/978-3-030-20887-5_23
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Rezende D.J., 2016, ADV NEURAL INFORM PR, P4996
   Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20
   Smith Edward J., 2017, P MACH LEARN RES, V78, P87
   Steinbrücker F, 2013, IEEE I CONF COMP VIS, P3264, DOI 10.1109/ICCV.2013.405
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   Varley J, 2017, IEEE INT C INT ROBOT, P2442, DOI 10.1109/IROS.2017.8206060
   Wang L., 2017, UNSUPERVISED 3D RECO, P1
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wu GC, 2019, IEEE T PATTERN ANAL, V41, P1681, DOI 10.1109/TPAMI.2018.2845393
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101
   Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278
   Xu Q., 2019, Advances in Neural Information Processing Systems (NeurIPS)
   Yan XC, 2016, ADV NEUR IN, V29
   Yang B, 2020, INT J COMPUT VISION, V128, P53, DOI 10.1007/s11263-019-01217-w
   Yang B, 2017, IEEE INT CONF COMP V, P679, DOI 10.1109/ICCVW.2017.86
   Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zhu H, 2017, IEEE T CIRC SYST VID, V27, P760, DOI 10.1109/TCSVT.2016.2596118
   Zou CH, 2017, IEEE I CONF COMP VIS, P900, DOI 10.1109/ICCV.2017.103
NR 57
TC 7
Z9 7
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2843
EP 2856
DI 10.1109/TMM.2020.3017924
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600024
DA 2024-07-18
ER

PT J
AU Xiao, J
   Li, L
   Xu, DJ
   Long, CJ
   Shao, J
   Zhang, SF
   Pu, SL
   Zhuang, YT
AF Xiao, Jun
   Li, Lin
   Xu, Dejing
   Long, Chengjiang
   Shao, Jian
   Zhang, Shifeng
   Pu, Shiliang
   Zhuang, Yueting
TI Explore Video Clip Order With Self-Supervised and Curriculum Learning
   for Video Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Three-dimensional displays; Feature extraction; Two
   dimensional displays; Training; Convolution; Knowledge discovery; Action
   recognition; curriculum learning; nearest neighbor retrieval;
   self-supervised learning; video question answering
ID ACTION RECOGNITION
AB We present a self-supervised spatiotemporal learning approach by exploring the temporal coherence of videos. The chronological order of shuffled clips from the video is used as the supervisory signal to guide the 3D Convolutional Neural Networks (CNNs) to learn meaningful visual knowledge. Unlike the existing approaches which use frames, we utilize dynamic video clips to reduce the uncertainty of order. We test three types of representative 3D CNNs, all of which benefit from the proposed approach. The learned 3D CNNs can be used either as a feature extractor or a pre-trained model for further fine-tuning on downstream tasks. We also propose two curriculum learning strategies to make the 3D CNNs easier to train and get the state-of-the-art results in nearest neighbor retrieval and action recognition tasks compared with other self-supervised learning methods. Meanwhile, it is further extended to the field of visual question answering application and has achieved promising results. Besides, comprehensive and extensive experimental results and analyses are provided for readers to better understand the video clip order we explore with self-supervised and curriculum learning for video application.
C1 [Xiao, Jun; Li, Lin; Xu, Dejing; Shao, Jian; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310000, Zhejiang, Peoples R China.
   [Long, Chengjiang] JD Digits AI Lab, Mountain View, CA 94043 USA.
   [Zhang, Shifeng; Pu, Shiliang] Hikvision, Hikvis Res Inst, Hangzhou 310000, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Xu, DJ (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310000, Zhejiang, Peoples R China.
EM junx@cs.zju.edu.cn; mukti@zju.edu.cn; xudejing@zju.edu.cn;
   cjfykx@gmail.com; jshao@cs.zju.edu.cn; zhangshifeng@hikvision.com;
   pushiliang.hri@hikvision.com; yzhuang@zju.edu.cn
RI zhang, Shifeng/HPH-0217-2023; Xu, Dejing/AAA-2793-2021
OI Xu, Dejing/0000-0003-3404-1305; Li, Lin/0000-0002-5678-4487; Pu,
   Shiliang/0000-0001-5269-7821; Long, Chengjiang/0000-0003-1584-7290
FU National Key Research and Development Project of China [2018AAA0101900];
   NationalNatural Science Foundation ofChina [U19B2043, 61976185];
   Zhejiang Natural Science Foundation [LR19F020002, LZ17F020001];
   Fundamental Research Funds for the CentralUniversities; Chinese
   Knowledge Center for Engineering Sciences, and Technology; Joint
   Research Program of ZJU; Hikvision Research Institute
FX This work was supported in part by the National Key Research and
   Development Project of China (No. 2018AAA0101900), in part by
   theNationalNatural Science Foundation ofChina (U19B2043, 61976185), in
   part by Zhejiang Natural Science Foundation (LR19F020002, LZ17F020001),
   and in part by the Fundamental Research Funds for the
   CentralUniversities, Chinese Knowledge Center for Engineering Sciences,
   and Technology, and Joint Research Program of ZJU, and Hikvision
   Research Institute. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Wanqing Li.
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Benaim S., 2020, CVPR, P9919
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Büchler U, 2018, LECT NOTES COMPUT SC, V11219, P797, DOI 10.1007/978-3-030-01267-0_47
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YX, 2017, IEEE DEVICE RES CONF
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C., 2016, P INT C NEUR INF PRO, P3468
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Fernando B, 2017, IEEE T PATTERN ANAL, V39, P773, DOI 10.1109/TPAMI.2016.2558148
   Gidaris S., 2018, P 6 INT C LEARNING R
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang L, 2015, AAAI CONF ARTIF INTE, P2694
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Klaser A., 2008, BMVC 2008 19 BRIT MA, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Luo DZ, 2020, AAAI CONF ARTIF INTE, V34, P11701
   Masters D, 2018, ARXIV, DOI 10.48550/arXiv.1804.07612
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Paszke A, 2019, ADV NEUR IN, V32
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Pawan Kumar M., 2010, NIPS
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pentina A, 2015, PROC CVPR IEEE, P5492, DOI 10.1109/CVPR.2015.7299188
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K, 2014, ADV NEUR IN, V27
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413
   Wang L., 2016, P ECCV
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XL, 2017, IEEE I CONF COMP VIS, P1338, DOI 10.1109/ICCV.2017.149
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658
   Yu Z, 2019, AAAI CONF ARTIF INTE, P9127
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang X, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1903
NR 59
TC 8
Z9 8
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3454
EP 3466
DI 10.1109/TMM.2020.3025661
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100002
DA 2024-07-18
ER

PT J
AU Xu, RT
   Xu, Y
   Quan, YH
AF Xu, Ruotao
   Xu, Yong
   Quan, Yuhui
TI Factorized Tensor Dictionary Learning for Visual Tensor Data Completion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensile stress; Dictionaries; Machine learning; Visualization;
   Convolutional codes; Encoding; Analytical models; Tensor dictionary
   learning; tensor completion; convolutional sparse coding; factorized
   dictionary learning
ID SPARSE; RANK; REPRESENTATION; DECOMPOSITION
AB This paper aims at developing a dictionary-learning-based method for completing the visual tensor data with missing elements. Traditional dictionary learning approaches suffer from very high computational costs when processing high-dimensional tensor data. Some existing approaches for acceleration impose orthogonality constraints or rank-one decompositions on dictionary atoms; however, the expressibility of the resulting dictionary is rather limited. To address such issues, we propose a convolutional analysis model for tensor dictionary learning, where the update of sparse coefficients during dictionary learning is simple and fast. Furthermore, we propose an orthogonality-constrained convolutional factorization scheme for dictionary construction, in which each tensor dictionary atom is factorized by the convolution of two atoms selected from two orthogonal factor dictionaries respectively. This factorization scheme enables us to efficiently learn an expressive dictionary with over-completeness and non-rank-one atoms. Based on our convolutional analysis model and factorization scheme, an effective yet efficient dictionary learning method is proposed for visual tensor completion. Extensive experiments show that, our method not only outperforms existing dictionary-based approaches with relatively-low time cost, but also outperforms recent low-rank approaches.
C1 [Xu, Ruotao; Xu, Yong; Quan, Yuhui] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Xu, Yong] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Xu, Yong] Commun & Comp Network Lab Guangdong, Guangzhou 510006, Peoples R China.
   [Quan, Yuhui] Guangdong Prov Key Lab Computat Intelligence & Cy, Guangzhou 510006, Peoples R China.
C3 South China University of Technology; Peng Cheng Laboratory
RP Quan, YH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM xu.ruotao@mail.scut.edu.cn; yxu@scut.edu.cn; csyhquan@scut.edu.cn
RI Xu, Ruotao/HTR-9465-2023
OI Xu, Ruotao/0000-0002-5277-9859
FU National Natural Science Foundation of China [61872151, 61672241,
   U1611461]; Natural Science Foundation of Guangdong Province
   [2017A030313376, 2016A030308013, 2020A1515011128]; Science and
   Technology Program of Guangdong Province [2019A050510010, 20140904-160];
   Science and Technology Program of Guangzhou [201802010055]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872151, 61672241, and U1611461, in
   part by the Natural Science Foundation of Guangdong Province under
   Grants 2017A030313376, 2016A030308013, and 2020A1515011128, in part by
   Science and Technology Program of Guangdong Province under Grants
   2019A050510010 and 20140904-160, and in part by Science and Technology
   Program of Guangzhou under Grant 201802010055.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Bao CL, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487966
   Bao CL, 2013, IEEE I CONF COMP VIS, P3384, DOI 10.1109/ICCV.2013.420
   Bibi A, 2017, IEEE I CONF COMP VIS, P1790, DOI 10.1109/ICCV.2017.197
   Cai JF, 2014, APPL COMPUT HARMON A, V37, P89, DOI 10.1016/j.acha.2013.10.001
   Dantas CF, 2018, LECT NOTES COMPUT SC, V10891, P456, DOI 10.1007/978-3-319-93764-9_42
   Dong WS, 2015, IEEE I CONF COMP VIS, P442, DOI 10.1109/ICCV.2015.58
   Du B, 2017, IEEE T MULTIMEDIA, V19, P67, DOI 10.1109/TMM.2016.2608780
   Du Y, 2019, IEEE T CYBERNETICS, V49, P3898, DOI 10.1109/TCYB.2018.2853122
   Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624
   Fu Y, 2014, IEEE IJCNN, P2957, DOI 10.1109/IJCNN.2014.6889490
   Han ZF, 2017, NEURAL PROCESS LETT, V45, P729, DOI 10.1007/s11063-016-9503-4
   Hao RR, 2018, J COMPUT APPL MATH, V329, P125, DOI 10.1016/j.cam.2017.01.022
   Hawe S, 2013, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2013.63
   Hawe S, 2013, IEEE T IMAGE PROCESS, V22, P2138, DOI 10.1109/TIP.2013.2246175
   Hazan T, 2005, IEEE I CONF COMP VIS, P50
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Hosono K, 2016, IEEE IMAGE PROC, P3081, DOI 10.1109/ICIP.2016.7532926
   Huang F., 2015, Feature Extraction: Modern Questions and Challenges, P116
   Ju FJ, 2016, NEUROCOMPUTING, V218, P120, DOI 10.1016/j.neucom.2016.08.064
   Kim D, 2019, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR.2019.00594
   Koniusz P, 2016, PROC CVPR IEEE, P5395, DOI 10.1109/CVPR.2016.582
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   LIU Y., 2014, P 2014 SIAM INT C DA, P866
   Liu YP, 2019, IEEE T MULTIMEDIA, V21, P338, DOI 10.1109/TMM.2018.2859026
   Liu YY, 2015, IEEE T CYBERNETICS, V45, P2437, DOI 10.1109/TCYB.2014.2374695
   Lu CY, 2016, PROC CVPR IEEE, P5249, DOI 10.1109/CVPR.2016.567
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Nie XS, 2017, IEEE T MULTIMEDIA, V19, P785, DOI 10.1109/TMM.2016.2629758
   Novikov A., 2015, P 29 INT C NEUR INF, P442, DOI DOI 10.5555/2969239.2969289
   Papyan V, 2017, IEEE I CONF COMP VIS, P5306, DOI 10.1109/ICCV.2017.566
   Peng Y, 2014, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2014.377
   Poot DHJ, 2015, IEEE T MED IMAGING, V34, P1164, DOI 10.1109/TMI.2014.2380830
   Preibisch JB, 2013, 2013 7TH INTERNATIONAL CONGRESS ON ADVANCED ELECTROMAGNETIC MATERIALS IN MICROWAVES AND OPTICS (METAMATERIALS 2013), P493, DOI 10.1109/MetaMaterials.2013.6809097
   Qi N, 2016, PROC CVPR IEEE, P5916, DOI 10.1109/CVPR.2016.637
   Quan Y., 2016, P IEEE INT C COMP VI, P73
   Quan YH, 2020, IEEE SIGNAL PROC LET, V27, P116, DOI 10.1109/LSP.2019.2959225
   Quan YH, 2016, PATTERN RECOGN, V55, P247, DOI 10.1016/j.patcog.2016.01.028
   Ren JN, 2016, CONF REC ASILOMAR C, P1744, DOI 10.1109/ACSSC.2016.7869681
   Rigamonti R, 2013, PROC CVPR IEEE, P2754, DOI 10.1109/CVPR.2013.355
   Roemer Florian, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3963, DOI 10.1109/ICASSP.2014.6854345
   Rubinstein R., 2008, CS08 TECHN
   Rubinstein R, 2013, IEEE T SIGNAL PROCES, V61, DOI 10.1109/TSP.2012.2226445
   Sivalingam R, 2015, IEEE T IMAGE PROCESS, V24, P4592, DOI 10.1109/TIP.2015.2440766
   Stevens A, 2017, PR MACH LEARN RES, V54, P121
   Sulam J, 2018, IEEE T SIGNAL PROCES, V66, P4090, DOI 10.1109/TSP.2018.2846226
   Tan X, 2015, IEEE T MULTIMEDIA, V17, P660, DOI 10.1109/TMM.2015.2410135
   Xie Q, 2018, IEEE T PATTERN ANAL, V40, P1888, DOI 10.1109/TPAMI.2017.2734888
   Xie Q, 2016, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2016.187
   Xiong BA, 2018, IEEE T MULTIMEDIA, V20, P2316, DOI 10.1109/TMM.2018.2806225
   Xu W., 2020, NEUROCOMPUTING
   Yaghoobi M, 2011, EUR SIGNAL PR CONF, P1470
   Yang JY, 2018, IEEE J-STSP, V12, P1420, DOI 10.1109/JSTSP.2018.2873990
   Yokota T, 2016, IEEE T SIGNAL PROCES, V64, P5423, DOI 10.1109/TSP.2016.2586759
   Zhang XY, 2018, PROC CVPR IEEE, P8232, DOI 10.1109/CVPR.2018.00859
   Zhang YMZ, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.83
   Zhang ZM, 2017, IEEE T SIGNAL PROCES, V65, P1511, DOI 10.1109/TSP.2016.2639466
   Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485
   Zhao QB, 2015, IEEE T PATTERN ANAL, V37, P1751, DOI 10.1109/TPAMI.2015.2392756
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhou XM, 2012, IEEE T MULTIMEDIA, V14, P1220, DOI 10.1109/TMM.2012.2194481
   Zubair S., 2013, 2013 18th International Conference on Digital Signal Processing (DSP) IEEE, P1, DOI DOI 10.1109/ICDSP.2013.6622725
   Zubair S, 2014, INT CONF DIGIT SIG, P361, DOI 10.1109/ICDSP.2014.6900687
NR 63
TC 8
Z9 8
U1 1
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1225
EP 1238
DI 10.1109/TMM.2020.2994512
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200005
DA 2024-07-18
ER

PT J
AU Li, J
   Liu, XL
   Zhang, WX
   Zhang, MY
   Song, JK
   Sebe, N
AF Li, Jun
   Liu, Xianglong
   Zhang, Wenxuan
   Zhang, Mingyuan
   Song, Jingkuan
   Sebe, Nicu
TI Spatio-Temporal Attention Networks for Action Recognition and Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Task analysis; Two
   dimensional displays; Computer architecture; Optical imaging;
   Visualization; 3D CNN; spatio-temporal attention; temporal attention;
   spatial attention; action recognition; action detection
ID REPRESENTATION; VIDEOS
AB Recently, 3D Convolutional Neural Network (3D CNN) models have been widely studied for video sequences and achieved satisfying performance in action recognition and detection tasks. However, most of the existing 3D CNNs treat all input video frames equally, thus ignoring the spatial and temporal differences across the video frames. To address the problem, we propose a spatio-temporal attention (STA) network that is able to learn the discriminative feature representation for actions, by respectively characterizing the beneficial information at both the frame level and the channel level. By simultaneously exploiting the differences in spatial and temporal dimensions, our STA module enhances the learning capability of the 3D convolutions when handling the complex videos. The proposed STA method can be wrapped as a generic module easily plugged into the state-of-the-art 3D CNN architectures for video action detection and recognition. We extensively evaluate our method on action recognition and detection tasks over three popular datasets (UCF-101, HMDB-51 and THUMOS 2014), and the experimental results demonstrate that adding our STA network module can obtain the state-of-the-art performance on UCF-101 and HMDB-51, which has the top-1 accuracies of 98.4% and 81.4% respectively, and achieve significant improvement on THUMOS 2014 dataset compared against original models.
C1 [Li, Jun; Liu, Xianglong; Zhang, Wenxuan; Zhang, Mingyuan] Beihang Univ, State Key Lab Software Dev Environm, Beijing 10000, Peoples R China.
   [Liu, Xianglong] Beihang Univ, Beijing Adv Innovat Ctr Big Data Based Precis Med, Beijing 10000, Peoples R China.
   [Song, Jingkuan] Univ Elect Sci & Technol China, Innovat Ctr, Chengdu 610051, Peoples R China.
   [Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38122 Trento, Italy.
C3 Beihang University; Beihang University; University of Electronic Science
   & Technology of China; University of Trento
RP Liu, XL (corresponding author), Beihang Univ, State Key Lab Software Dev Environm, Beijing 10000, Peoples R China.
EM junmuzi@gmail.com; xlliu@nlsde.buaa.edu.cn; zwx980624@gmail.com;
   zhangmy718@gmail.com; jingkuan.song@gmail.com; sebe@disi.unitn.it
RI Li, Jun/HJH-3122-2023; Sebe, Niculae/KEC-2000-2024
OI Sebe, Niculae/0000-0002-6597-7248; song, jingkuan/0000-0002-2549-8322;
   Li, Jun/0000-0002-5009-6536; Zhang, Wenxuan/0000-0002-3947-2991; Liu,
   Xianglong/0000-0002-7618-3275
FU National Natural Science Foundation of China [61872021, 61690202];
   Beijing Nova Program of Science and Technology [Z191100001119050]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61872021 and 61690202 and in part by Beijing Nova
   Program of Science and Technology under Grant Z191100001119050. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Chang-Su Kim.
CR [Anonymous], 2018, P 32 AAAI C ART INT
   [Anonymous], 2012, CoRR
   [Anonymous], 2015, CORR
   Bahdanau D., 2014, 3 INT C LEARN REPR
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Choo J, 2007, IEEE INT C BIOINFORM, P71, DOI 10.1109/BIBM.2007.51
   Chorowski J, 2015, ADV NEUR IN, V28
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan LJ, 2018, PROC CVPR IEEE, P6016, DOI 10.1109/CVPR.2018.00630
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Frintrop S, 2011, COMPUTER ANALYSIS OF HUMAN BEHAVIOR, P69, DOI 10.1007/978-0-85729-994-9_4
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao ZN, 2019, AAAI CONF ARTIF INTE, P8328
   Girdhar R, 2017, ADV NEUR IN, V30
   Gleason J, 2019, IEEE WINT CONF APPL, P141, DOI 10.1109/WACV.2019.00021
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Jiang Y., 2014, ECCV WORKSH
   Karaman S., 2014, ECCV THUMOS WORKSH, V1, P5
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751
   Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Oneata D., 2014, P WORKSH EUR C COMP
   Pan XJ, 2018, ADVANCED FUNCTIONAL MATERIALS (CMC 2017), P363, DOI 10.1007/978-981-13-0110-0_41
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Piergiovanni AJ, 2019, PR MACH LEARN RES, V97
   Piergiovanni AJ, 2019, PROC CVPR IEEE, P9937, DOI 10.1109/CVPR.2019.01018
   Piergiovanni AJ, 2017, AAAI CONF ARTIF INTE, P4247
   Pramono RRA, 2019, IEEE I CONF COMP VIS, P61, DOI 10.1109/ICCV.2019.00015
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Richard A, 2018, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2018.00627
   Rush AlexanderM., 2015, P 2015 C EMP METH NA
   Shah M., 2017, P BMVC, P7
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Soomro K, 2017, IEEE I CONF COMP VIS, P696, DOI 10.1109/ICCV.2017.82
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L, 2014, IEEE INT CONF VLSI
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang PC, 2018, COMPUT VIS IMAGE UND, V171, P118, DOI 10.1016/j.cviu.2018.04.007
   Wang WG, 2019, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2019.00318
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhang C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P500, DOI 10.1145/3343031.3350876
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2518, DOI 10.1109/TMM.2019.2907453
   Zhao H, 2019, IEEE I CONF COMP VIS, P8667, DOI 10.1109/ICCV.2019.00876
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
NR 76
TC 99
Z9 100
U1 3
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2990
EP 3001
DI 10.1109/TMM.2020.2965434
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900018
DA 2024-07-18
ER

PT J
AU Zhu, KJ
   Wang, RX
   Zhao, QS
   Cheng, J
   Tao, DP
AF Zhu, Kaijun
   Wang, Ruxin
   Zhao, Qingsong
   Cheng, Jun
   Tao, Dapeng
TI A Cuboid CNN Model With an Attention Mechanism for Skeleton-Based Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Skeleton; Sensors; Three-dimensional displays;
   Spatiotemporal phenomena; Hidden Markov models; Neural networks; CNN;
   action recognition; attention mechanism; feature cuboid
ID FEATURES
AB The introduction of depth sensors such as Microsoft Kinect have driven research in human action recognition. Human skeletal data collected from depth sensors convey a significant amount of information for action recognition. While there has been considerable progress in action recognition, most existing skeleton-based approaches neglect the fact that not all human body parts move during many actions, and they fail to consider the ordinal positions of body joints. Here, and motivated by the fact that an actions category is determined by local joint movements, we propose a cuboid model for skeleton-based action recognition. Specifically, a cuboid arranging strategy is developed to organize the pairwise displacements between all body joints to obtain a cuboid action representation. Such a representation is well structured and allows deep CNN models to focus analyses on actions. Moreover, an attention mechanism is exploited in the deep model, such that the most relevant features are extracted. Extensive experiments on our new Yunnan University-Chinese Academy of Sciences-Multimodal Human Action Dataset (CAS-YNU MHAD), the NTU RGB+D dataset, the UTD-MHAD dataset, and the UTKinect-Action3D dataset demonstrate the effectiveness of our method compared to the current state-of-the-art.
C1 [Zhu, Kaijun; Tao, Dapeng] Yunnan Univ, Sch Informat Sci & Engn, FIST LAB, Kunming 650091, Yunnan, Peoples R China.
   [Wang, Ruxin] Union Vis Innovat Technol, Shenzhen 518000, Peoples R China.
   [Zhao, Qingsong] Univ Chinese Acad Sci, Shenzhen Coll Adv Technol, Shenzhen 518055, Peoples R China.
   [Cheng, Jun] Chinese Acad Sci, Shenzhen Inst Adv Technol, CAS Key Lab Human Machine Intelligence Synergy Sy, Shenzhen 518055, Peoples R China.
   [Cheng, Jun] Chinese Univ Hong Kong, Hong Kong 999077, Peoples R China.
C3 Yunnan University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS; Chinese University of Hong Kong
RP Tao, DP (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, FIST LAB, Kunming 650091, Yunnan, Peoples R China.; Cheng, J (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, CAS Key Lab Human Machine Intelligence Synergy Sy, Shenzhen 518055, Peoples R China.
EM xunfeng.zkj@gmail.com; rosinwang@gmail.com; qs.zhao@siat.ac.cn;
   jun.cheng@siat.ac.cn; dapeng.tao@gmail.com
RI ; Tao, Dapeng/E-8649-2013
OI zhao, qingsong/0000-0002-7120-8170; Tao, Dapeng/0000-0003-0783-5273
FU National Key R&D Program of China [2018YFB1308000]; National Natural
   Science Foundation of China [61772455, 61572486, 61772508, U1713213];
   Yunnan Natural Science Funds [2018FY001(-013), 2019FA045]; Guangdong
   Technology Project [2017B010110007, 2016B010108010]; Program for
   Excellent Young Talents of National Natural Science Foundation of Yunnan
   University [2018YDJQ004]; Program for Excellent Young Talents of Yunnan
   University [WX069051]; Project of Innovative Research Team of Yunnan
   Province [2018HC019]; Shenzhen Technology Project
   [JCYJ20180507182610734, JCYJ20170413152535587]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFB1308000, in part by the National Natural Science
   Foundation of China under Grants 61772455, 61572486, 61772508, and
   U1713213, in part by the Yunnan Natural Science Funds under Grants
   2018FY001(-013) and 2019FA045, in part by the Guangdong Technology
   Project under Grants 2017B010110007 and 2016B010108010, in part by the
   Program for Excellent Young Talents of National Natural Science
   Foundation of Yunnan University under Grant 2018YDJQ004, in part by the
   Program for Excellent Young Talents of Yunnan University under Grant
   WX069051, in part by the Project of Innovative Research Team of Yunnan
   Province under Grant 2018HC019, and in part by the Shenzhen Technology
   Project underGrants JCYJ20180507182610734 and JCYJ20170413152535587. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Honggang Wang.
CR [Anonymous], 2016, ARXIV160403227
   [Anonymous], 2016, ARXIV160308199
   [Anonymous], 2017, arXiv
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Ding WW, 2015, J VIS COMMUN IMAGE R, V26, P329, DOI 10.1016/j.jvcir.2014.10.009
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Ellis C, 2013, INT J COMPUT VISION, V101, P420, DOI 10.1007/s11263-012-0550-7
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Hussein, 2013, INT JOINT C ART INT
   Ji XP, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P208, DOI 10.1109/ICACI.2016.7449827
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kerola Tommi., 2014, Asian Conference on Computer Vision, P417
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li CK, 2017, IEEE SIGNAL PROC LET, V24, P624, DOI 10.1109/LSP.2017.2678539
   Lin Weiyao, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1489, DOI 10.1109/TPAMI.2016.2608884
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu MY, 2017, IEEE IMAGE PROC, P3670, DOI 10.1109/ICIP.2017.8296967
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Luvizon DC, 2017, PATTERN RECOGN LETT, V99, P13, DOI 10.1016/j.patrec.2017.02.001
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Passalis N, 2018, IEEE T CYBERNETICS, V48, P52, DOI 10.1109/TCYB.2016.2623581
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sharma S., 2015, NEURAL INFORM PROCES
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Song Y, 2018, NEUROCOMPUTING
   Tao DP, 2016, IEEE T CYBERNETICS, V46, P756, DOI 10.1109/TCYB.2015.2414920
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang P., 2018, PROC ACM MM, P102
   Wang PC, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1119, DOI 10.1145/2733373.2806296
   Wang ZK, 2018, IEEE IMAGE PROC, P3458, DOI 10.1109/ICIP.2018.8451483
   Xiao JJ, 2018, IEEE T CYBERNETICS, V48, P2485, DOI 10.1109/TCYB.2017.2740952
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yang YH, 2017, IEEE T CYBERNETICS, V47, P439, DOI 10.1109/TCYB.2016.2519448
   Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y
   Zhang CJ, 2018, IEEE T CYBERNETICS, V48, P2012, DOI 10.1109/TCYB.2017.2726079
   Zhang SB, 2018, IEEE T CYBERNETICS, V48, P1827, DOI 10.1109/TCYB.2017.2715846
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhou Y, 2014, IEEE INT CONGR BIG, P1, DOI 10.1109/BigData.Congress.2014.11
NR 45
TC 36
Z9 40
U1 1
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2977
EP 2989
DI 10.1109/TMM.2019.2962304
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900017
DA 2024-07-18
ER

PT J
AU Peng, F
   Yin, LP
   Zhang, LB
   Long, M
AF Peng, Fei
   Yin, Li-Ping
   Zhang, Le-Bing
   Long, Min
TI CGR-GAN: CG Facial Image Regeneration for Antiforensics Based on
   Generative Adversarial Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image anti-forensics; generative adversarial network (GAN); natural
   images (NI); computer-generated image; computer-generated graphics (CG)
   detector
ID DISCRIMINATION; GRAPHICS
AB In this paper, a Computer-generated graphics (CG) facial image regeneration scheme for anti-forensics based on generative adversarial network (CGR-GAN) is proposed. The generator of CGR-GAN utilizes a deep U-Net structure, and its discriminator utilizes some stacked convolution layers. Besides, content loss and style loss are both designed to guarantee that the regenerated CG facial images (CGR) retain both the facial profile of the original CG and the characteristics of natural image (NI). Experimental results and analysis demonstrate that the CG facial images regenerated by the proposed anti-forensics scheme can achieve better visual quality comparedwith those of the existingCG facial image anti-forensics and domain adaptation methods, and it can strike a good balance between visual quality and deception ability.
C1 [Peng, Fei; Yin, Li-Ping; Zhang, Le-Bing] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Long, Min] Hunan Univ, Coll Comp & Commun Engn, Changsha Univ Sci & Technol, Changsha 410114, Peoples R China.
C3 Hunan University; Hunan University; Changsha University of Science &
   Technology
RP Peng, F (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
EM eepengf@gmail.com; yinqk@hnu.edu.cn; zhanglebing@hnu.edu.cn;
   caslongm@gmail.com
RI Zhang, Le-Bing/AHA-1060-2022; Long, Min/AGW-6059-2022; Peng,
   Fei/H-6951-2017
OI Peng, Fei/0000-0001-8053-4587; Zhang, Le-Bing/0000-0001-9651-0889
FU National Natural Science Foundation of China [U1936115, 61572182]
FX This work was supported in part by project supported by National Natural
   Science Foundation of China under Grants U1936115 and 61572182. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Lei Zhang.
CR Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   [Anonymous], CALTECH FRONTAL FACE
   [Anonymous], 2013, EMERGING DIGITAL FOR
   [Anonymous], 2016, DISTILL, DOI [10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   [Anonymous], 2013, P ICML
   Arjovsky M., 2017, ARXIV170107875
   Chen W, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1123
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   de Rezende ERS, 2017, SIBGRAPI, P71, DOI 10.1109/SIBGRAPI.2017.16
   Dang-Nguyen DT, 2012, EUR SIGNAL PR CONF, P1234
   Fei Peng, 2017, Security, Privacy, and Anonymity in Computation, Communication, and Storage. 10th International Conference, SpaCCS 2017. Proceedings: LNCS 10656, P368, DOI 10.1007/978-3-319-72389-1_30
   Feng XY, 2012, IEEE T MULTIMEDIA, V14, P536, DOI 10.1109/TMM.2012.2191946
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   H Nguyen Huy, 2015, PROC INT WORKSHOP DI, P39
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kakar P, 2011, IEEE T MULTIMEDIA, V13, P443, DOI 10.1109/TMM.2011.2121056
   Li ZG, 2013, INTERNATIONAL CONFERENCE ON EARTH AND ENVIRONMENTAL SCIENCE, P228
   Liu MY, 2017, ADV NEUR IN, V30
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lyu S, 2005, IEEE T SIGNAL PROCES, V53, P845, DOI 10.1109/TSP.2004.839896
   Nguyen HH, 2019, 13TH INTERNATIONAL CONFERENCE ON AVAILABILITY, RELIABILITY AND SECURITY (ARES 2018), DOI 10.1145/3230833.3230863
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Peng F, 2017, AEU-INT J ELECTRON C, V71, P72, DOI 10.1016/j.aeue.2016.11.009
   Quan WZ, 2018, IEEE T INF FOREN SEC, V13, P2772, DOI 10.1109/TIFS.2018.2834147
   Rahmouni N, 2017, IEEE INT WORKS INFOR
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruoyu Wu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P1933, DOI 10.1109/ICIP.2011.6115849
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sankar G, 2009, INT CONF ACOUST SPEE, P1513, DOI 10.1109/ICASSP.2009.4959883
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su LC, 2018, IEEE T MULTIMEDIA, V20, P825, DOI 10.1109/TMM.2017.2760098
   Tokuda E, 2013, J VIS COMMUN IMAGE R, V24, P1276, DOI 10.1016/j.jvcir.2013.08.009
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Yao Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041296
   Yongzhen Ke, 2013, Journal of Theoretical and Applied Information Technology, V49, P844
NR 38
TC 21
Z9 22
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2511
EP 2525
DI 10.1109/TMM.2019.2959443
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000003
DA 2024-07-18
ER

PT J
AU Jiang, SH
   Wang, ZW
   Hertzmann, A
   Jin, HL
   Fu, Y
AF Jiang, Shuhui
   Wang, Zhaowen
   Hertzmann, Aaron
   Jin, Hailin
   Fu, Yun
TI Visual Font Pairing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Font; pairing; recommendation; metric learning
ID TEXT DETECTION
AB This paper introduces the problem of automatic font pairing. Font pairing is an important design task that is difficult for novices. Given a font selection for one part of a document (e.g., header), our goal is to recommend a font to be used in another part (e.g., body) such that the two fonts used together look visually pleasing. There are three main challenges in font pairing. First, this is a fine-grained problem, in which the subtle distinctions between fonts may be important. Second, rules and conventions of font pairing given by human experts are difficult to formalize. Third, font pairing is an asymmetric problem in that the roles played by header and body fonts are not interchangeable. To address these challenges, we propose automatic font pairing through learning visual relationships from large-scale human-generated font pairs. We introduce a new database for font pairing constructed from millions of PDF documents available on the Internet. We propose two font pairing algorithms: dual-space k-NN and asymmetric similarity metric learning (ASML). These two methods automatically learn fine-grained relationships from large-scale data. We also investigate several baseline methods based on the rules from professional designers. Experiments and user studies demonstrate the effectiveness of our proposed dataset and methods.
C1 [Jiang, Shuhui] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
   [Wang, Zhaowen; Hertzmann, Aaron; Jin, Hailin] Adobe Syst Inc, San Jose, CA 95110 USA.
   [Fu, Yun] Northeastern Univ, Dept Elect & Comp Engn, Coll Engn, Boston, MA 02115 USA.
   [Fu, Yun] Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA.
C3 Northeastern University; Adobe Systems Inc.; Northeastern University;
   Northeastern University
RP Jiang, SH (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
EM shjiang@ece.neu.edu; zhawang@adobe.com; hertzman@dgp.toronto.edu;
   hljin@adobe.com; yunfu@ece.neu.edu
RI Jiang, Shuhui/W-6907-2019
OI Fu, Yun/0000-0002-5098-2853
FU Adobe
FX This work was supported in part by Adobe.
CR Ahn YY, 2011, SCI REP-UK, V1, DOI 10.1038/srep00196
   Aizawa A, 2003, INFORM PROCESS MANAG, V39, P45, DOI 10.1016/S0306-4573(02)00021-3
   [Anonymous], 2008, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2008.4587799, DOI 10.1109/CVPR.2008.4587799]
   [Anonymous], 2012, P ACM INT C MULT
   Bonneville D., 2010, BIG BOOK FONT COMBIN
   Cao Q, 2013, IEEE I CONF COMP VIS, P2408, DOI 10.1109/ICCV.2013.299
   Chen G, 2014, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2014.460
   Chu WT, 2018, IEEE T MULTIMEDIA, V20, P2491, DOI 10.1109/TMM.2018.2801718
   Dansana J, 2013, 2013 IEEE CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES (ICT 2013), P835
   Feng LN, 2016, IEEE T PATTERN ANAL, V38, P785, DOI 10.1109/TPAMI.2015.2469281
   Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822
   Gunawardana A, 2009, J MACH LEARN RES, V10, P2935
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   Huang SR, 2017, IEEE T MULTIMEDIA, V19, P1314, DOI 10.1109/TMM.2017.2652074
   Hunter DR, 2004, ANN STAT, V32, P384
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Jiang SH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3721
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kembhavi A, 2016, LECT NOTES COMPUT SC, V9908, P235, DOI 10.1007/978-3-319-46493-0_15
   Ladicky L, 2010, LECT NOTES COMPUT SC, V6315, P239, DOI 10.1007/978-3-642-15555-0_18
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu TQ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766898
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   McAuley J, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2783258.2783381
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   O'Donovan P, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601110
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Siegel N, 2016, LECT NOTES COMPUT SC, V9911, P664, DOI 10.1007/978-3-319-46478-7_41
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang Z., 2015, P WORKSH 3 INT C LEA
   Wang ZY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P813, DOI 10.1145/2733373.2807988
   Wang ZY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P451, DOI 10.1145/2733373.2806219
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Wu YL, 2019, IEEE T IMAGE PROCESS, V28, P4299, DOI 10.1109/TIP.2019.2908774
   Wu YL, 2017, IEEE INT CON MULTI, P823, DOI 10.1109/ICME.2017.8019528
   Wu YR, 2017, IEEE T MULTIMEDIA, V19, P183, DOI 10.1109/TMM.2016.2609407
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P220, DOI 10.1109/TITS.2017.2749977
   Yang XY, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818709
   Yu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366153
   Zhao NX, 2018, COMPUT GRAPH FORUM, V37, P385, DOI 10.1111/cgf.13576
NR 42
TC 5
Z9 5
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2086
EP 2097
DI 10.1109/TMM.2019.2952266
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500014
DA 2024-07-18
ER

PT J
AU Peng, YX
   Zhang, J
   Ye, ZD
AF Peng, Yuxin
   Zhang, Jian
   Ye, Zhaoda
TI Deep Reinforcement Learning for Image Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reinforcement learning; Binary codes; Image retrieval; Correlation;
   Learning systems; Optimization; Training; Deep reinforcement learning;
   image hashing; image retrieval
ID REPRESENTATION; SIMILARITY; RETRIEVAL; SCENE
AB Deep hashing methods have received much attention recently, which achieve promising results by taking advantage of the strong representation power of deep networks. However, most existing deep hashing methods learn a whole set of hashing functions independently, while ignore the correlations between different hashing functions that can promote the retrieval accuracy greatly. Inspired by the sequential decision ability of deep reinforcement learning, we propose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH). Our proposed DRLIH approach models the hashing learning problem as a sequential decision process, which learns each hashing function by correcting the errors imposed by previous ones and promotes retrieval accuracy. To the best of our knowledge, this is the first work to address hashing problem from deep reinforcement learning perspective. The main contributions of our proposed DRLIH approach can be summarized as follows: (1) We propose a deep reinforcement learning hashing network. In the proposed network, we utilize recurrent neural network (RNN) as agents to model the hashing functions, which take actions of projecting images into binary codes sequentially, so that the current hashing function learning can take previous hashing functions' error into account. (2) We propose a sequential learning strategy based on proposed DRLIH. We define the state as a tuple of internal features of RNN's hidden layers and image features, which can reflect history decisions made by the agents. We also propose an action group method to enhance the correlation of hash functions in the same group. Experiments on three widely-used datasets demonstrate the effectiveness of our proposed DRLIH approach.
C1 [Peng, Yuxin; Zhang, Jian; Ye, Zhaoda] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
FU National Natural Science Foundation of China [61925201, 61771025]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61925201 and Grant 61771025.
CR [Anonymous], ACM INT C IM VID RET
   [Anonymous], 2016, IJCAI
   [Anonymous], 2009, NIPS
   [Anonymous], 2009, NEURIPS
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Kafai M, 2014, IEEE T MULTIMEDIA, V16, P1090, DOI 10.1109/TMM.2014.2305633
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Li WJ, 2016, IJCAI, P1711
   Lin M., 2016, ARXIV13124400
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Lv Q., 2007, P 33 INT C VER LARG, P950
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ranzato MarcAurelio, 2015, CoRR
   Rao YM, 2017, IEEE I CONF COMP VIS, P3951, DOI 10.1109/ICCV.2017.424
   REN Z, 2017, PROC CVPR IEEE, P1151, DOI DOI 10.1109/CVPR.2017.128
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sablayrolles A, 2017, INT CONF ACOUST SPEE, P1732, DOI 10.1109/ICASSP.2017.7952453
   Schütt M, 2004, PEPTIDE REVOLUTION: GENOMICS, PROTEOMICS & THERAPEUTICS, P41
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen XB, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3178119
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang Jianfeng., 2013, ACM Multimedia, P133
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang Jun., 2010, ICML, P1127
   Wang QF, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3911
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   WILLIAMS N, 1992, DESIGN, P5
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P212, DOI 10.1109/TCSVT.2017.2771332
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang L, 2013, PROCEEDINGS OF THE EIGHTH INTERNATIONAL SYMPOSIUM ON VITICULTURE AND ENOLOGY (2013), P123
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang SF, 2018, IEEE T CIRC SYST VID, V28, P2716, DOI 10.1109/TCSVT.2017.2710345
   Zhang YD, 2014, IEEE T MULTIMEDIA, V16, P1127, DOI 10.1109/TMM.2014.2306392
   Zhao DB, 2017, IEEE T COGN DEV SYST, V9, P356, DOI 10.1109/TCDS.2016.2614675
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 64
TC 27
Z9 29
U1 1
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2061
EP 2073
DI 10.1109/TMM.2019.2951462
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, CB
   Xie, HT
   Zha, ZJ
   Yu, LY
   Chen, ZN
   Zhang, YD
AF Liu, Chuanbin
   Xie, Hongtao
   Zha, Zhengjun
   Yu, Lingyun
   Chen, Zhineng
   Zhang, Yongdong
TI Bidirectional Attention-Recognition Model for Fine-Grained Object
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fine-grained object classification; interpretable machine learning;
   visual attention; pattern recognition; data augmentation
AB Fine-grained object classification (FGOC) is a challenging research topic in multimedia computing with machine learning, which faces two pivotal conundrums: focusing attention on the discriminate part regions, and then processing recognition with the part-based features. Existing approaches generally adopt a unidirectional two-step structure, that first locate the discriminate parts and then recognize the part-based features. However, they neglect the truth that part localization and feature recognition can be reinforced in a bidirectional process. In this paper, we propose a novel bidirectional attention-recognition model (BARM) to actualize the bidirectional reinforcement for FGOC. The proposed BARM consists of one attention agent for discriminate part regions proposing and one recognition agent for feature extraction and recognition. Meanwhile, a feedback flow is creatively established to optimize the attention agent directly by recognition agent. Therefore, in BARM the attention agent and the recognition agent can reinforce each other in a bidirectional way and the overall framework can be trained end-to-end without neither object nor parts annotations. Moreover, a novel Multiple Random Erasing data augmentation is proposed, and it exhibits impressive pertinency and superiority for FGOC. Conducted on several extensive FGOC benchmarks, BARM outperforms the present state-of-the-art methods in classification accuracy. Furthermore, BARM exhibits a clear interpretability and keeps consistent with the human perception in visualization experiments.
C1 [Liu, Chuanbin; Xie, Hongtao; Zha, Zhengjun; Yu, Lingyun; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Chen, Zhineng] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Automation, CAS
RP Xie, HT; Zhang, YD (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM lcb592@mail.ustc.edu.cn; htxie@ustc.edu.cn; zhazj@ustc.edu.cn;
   yuly@mail.ustc.edu.cn; zhineng.chen@ia.ac.cn; zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020; Liu, Chuanbin/JLL-9341-2023; chen,
   zhineng/AAD-6723-2020
OI Liu, Chuanbin/0000-0002-2840-6235
FU National Key Research and Development Program of China [2017YFC0820600];
   National Nature Science Foundation of China [61525206, 61771468]; Youth
   Innovation Promotion Association Chinese Academy of Sciences [2017209]
FX This work was supported in part by the National Key Research and
   Development Program of China 2017YFC0820600, in part by the National
   Nature Science Foundation of China under Grants 61525206 and 61771468,
   and in part by the Youth Innovation Promotion Association Chinese
   Academy of Sciences 2017209.
CR [Anonymous], 2006, Proceedings of the 2006 Conference on Computer Vision and Pattern Recognition Workshop, DOI [10.1109/CVPRW.2006.211, DOI 10.1109/CVPRW.2006.211]
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2008, Proceedings of the 16th ACM international conference on Multimedia, Vancouver
   [Anonymous], MULTIMEDIA
   Branson S., 2014, BIRD SPECIES CATEGOR, V1, P7
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Chen W., 2009, ADV NEURAL INFORM PR, V22, P315, DOI [DOI 10.5555/2984093.2984129, 10.5555/2984093.2984129]
   Cubuk Ekin D, 2018, ARXIV180509501
   DeVries T, 2017, PREPRINT
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2016, IEEE T MULTIMEDIA, V18, P2115, DOI 10.1109/TMM.2016.2581483
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He XT, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P741
   He XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P627, DOI 10.1145/3123266.3123319
   Hu H., 2017, Proceedings_of_the_34th_International_Conference_on_Machine_Learning-Volume, P1568
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Inoue H., 2018, arXiv
   Khosla A., 2011, P IEEE 1 WORKSH FIN
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   PARKHI OM, 2012, PROC CVPR IEEE, P3498, DOI [DOI 10.1109/CVPR.2012.6248092, 10.1109/CVPR.2012.6248092]
   Qi GJ, 2016, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2016.249
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russell DW, 1997, IEEE INFOR VIS, P25, DOI 10.1109/IV.1997.626469
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Tang J., 2007, Proceedings of the 15th ACM international conference on Multimedia, P297
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Vedaldi A., 2013, Technical report
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Xiao LX, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511399
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie HT, 2019, IEEE T MULTIMEDIA, V21, P1248, DOI 10.1109/TMM.2018.2872898
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Xie N., 2019, ARXIV190106706
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Zha Z.-J., IEEE T PATTERN ANAL
   Zhang H., 2018, PROC 6TH INT CONF LE
   Zhang Jian, 2019, ARXIV190209941
   Zhang LH, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2141, DOI 10.1145/3097983.3098117
   Zhang LM, 2016, IEEE T IMAGE PROCESS, V25, P553, DOI 10.1109/TIP.2015.2502147
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng H., 2019, CVPR, P5012
   Zheng HL, 2020, IEEE T IMAGE PROCESS, V29, P476, DOI 10.1109/TIP.2019.2921876
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 67
TC 35
Z9 35
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1785
EP 1795
DI 10.1109/TMM.2019.2954747
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500011
DA 2024-07-18
ER

PT J
AU Han, XJ
   Yang, HY
   Xing, GY
   Liu, YL
AF Han, Xianjun
   Yang, Hongyu
   Xing, Guanyu
   Liu, Yanli
TI Asymmetric Joint GANs for Normalizing Face Illumination From a Single
   Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE face illumination normalization; face relighting; generative adversarial
   networks; face identity; image translation
ID EXPRESSION RECOGNITION; REFLECTANCE; DATABASE
AB Illumination normalization for face recognition is very important when a face is captured under harsh lighting conditions. Instead of designing hand-crafted features, in this paper we formulate face illumination normalization as an image-to-image translation task. A great challenge of face normalization is that human facial structures are particularly sensitive to image structure distortion, which frequently occurs in traditional image-to-image translation tasks. Unfortunately, sometimes even slight facial structure distortions may prohibit human eyes and machine face recognition methods from identifying face identities. To address this issue, a novel GAN-based network architecture called the asymmetric joint generative adversarial network (AJGAN) is developed to normalize face images under arbitrary illumination conditions, without known face geometry and albedo information. In addition, an illumination normalization GAN G(1) and an asymmetric relighting GAN G(2) that maps a frontal-illuminated image to images with various lighting conditions are incorporated in AJGAN to maintain personalized facial structures. To avoid image blurring caused by the under-constrained relighting mapping, we introduce a scheme of one-hot lighting labels into G(2) and enforce label classification loss. Furthermore, the number of training images starting from a very limited number of labels is dynamically extended by the combination of different lighting labels. Qualitative and quantitative experiments on three databases validate that AJGAN significantly outperforms the state-of-the-art methods.
C1 [Han, Xianjun; Yang, Hongyu; Liu, Yanli] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Xing, Guanyu] Sichuan Univ, Natl Key Lab Fundamental Sci Synthet Vis, Chengdu 610065, Peoples R China.
C3 Sichuan University; Sichuan University
RP Liu, YL (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
EM hxj@stu.scu.edu.cn; yanghongyu@scu.edu.cn; xingguanyu@scu.edu.cn;
   yanliliu@scu.edu.cn
RI yang, yun/IZE-1092-2023; Yang, Hongyu/JXM-2064-2024
OI Yang, Hongyu/0000-0002-5894-1693; Han, Xianjun/0000-0001-7674-1428; Liu,
   Yanli/0000-0002-3181-2699
FU National Natural Science Foundation of China [61972271, 61572333]
FX This research was supported by National Natural Science Foundation of
   China under Grants 61972271 and 61572333.
CR Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229
   Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374
   [Anonymous], 2016, TECH REP
   [Anonymous], 2015, P 21 ACM S VIRT REAL, DOI DOI 10.1145/2821592.2821601
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Belhumeur PN, 1998, INT J COMPUT VISION, V28, P245, DOI 10.1023/A:1008005721484
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen X, 2016, ADV NEUR IN, V29
   Chen XW, 2011, PROC CVPR IEEE, P281, DOI 10.1109/CVPR.2011.5995473
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Ferrari C, 2017, IEEE T MULTIMEDIA, V19, P2666, DOI 10.1109/TMM.2017.2707341
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gross R, 2008, IEEE INT CONF AUTOMA, P849
   Huang YZ, 2010, IEEE T MULTIMEDIA, V12, P536, DOI 10.1109/TMM.2010.2052792
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ma W, 2018, INT C PATT RECOG, P2558, DOI 10.1109/ICPR.2018.8545434
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Matsukawa T, 2012, INT C PATT RECOG, P1848
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Pei SC, 2017, IEEE T MULTIMEDIA, V19, P1956, DOI 10.1109/TMM.2017.2688924
   Qing LY, 2005, INT J PATTERN RECOGN, V19, P513, DOI 10.1142/S0218001405004186
   Radford A., 2015, ARXIV
   Ren PR, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766899
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1968, DOI 10.1109/TPAMI.2008.244
   Xie XH, 2011, IEEE T IMAGE PROCESS, V20, P1807, DOI 10.1109/TIP.2010.2097270
   Zhang L, 2005, PROC CVPR IEEE, P209
   Zhang Y, 2019, J VIS COMMUN IMAGE R, V59, P501, DOI 10.1016/j.jvcir.2019.02.007
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 38
TC 26
Z9 26
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1619
EP 1633
DI 10.1109/TMM.2019.2945197
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100020
DA 2024-07-18
ER

PT J
AU Wan, CQ
   Wu, Y
   Tian, XM
   Huang, JQ
   Hua, XS
AF Wan, Chaoqun
   Wu, Yue
   Tian, Xinmei
   Huang, Jianqiang
   Hua, Xian-Sheng
TI Concentrated Local Part Discovery With Fine-Grained Part Representation
   for Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Cameras; Convolutional neural
   networks; Head; Torso; Legged locomotion; Person re-identification;
   local part learning; constraint attention mechanism; fine-grained
   representation
ID IDENTIFICATION; NETWORK
AB The attention mechanism for person re-identification has been widely studied with deep convolutional neural networks. This mechanism works as a good complement to the global features extracted from an image of the entire human body. However, existing works mainly focus on discovering local parts with simple feature representations, such as global average pooling. Moreover, these works either require extra supervision, such as labeling of body joints, or pay little attention to the guidance of part learning, resulting in scattered activation of learned parts. Furthermore, existing works usually extract local features from different body parts via global average pooling and then concatenate them together as good global features. We find that local features acquired in this way contribute little to the overall performance. In this paper, we argue the significance of local part description and explore the attention mechanism from both local part discovery and local part representation aspects. For local part discovery, we propose a new constrained attention module to make the activated regions concentrated and meaningful without extra supervision. For local part representation, we propose a statistical-positional-relational descriptor to represent local parts from a fine-grained viewpoint. Extensive experiments are conducted to validate the overall performance, the effectiveness of each component, and the generalization ability. We achieve a rank-1 accuracy of 95.1% on Market1501, 64.7% on CUHK03, 87.1% on DukeMTMC-ReID, and 79.9% on MSMT17, outperforming state-of-the-art methods.
C1 [Wan, Chaoqun; Tian, Xinmei] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Wu, Yue; Huang, Jianqiang; Hua, Xian-Sheng] Alibaba Grp, Hangzhou 311121, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Alibaba Group
RP Tian, XM (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM wancq14@mail.ustc.edu.cn; matthew.wy@alibaba-inc.com;
   xinmei@ustc.edu.cn; jianqiang.hjq@alibaba-inc.com; hxs@alibaba-inc.com
FU National Key Research and Development Program of China [2017YFB1002203];
   National Natural Science Foundation of China [61872329, 61572451]
FX This work was supported in part by National Key Research and Development
   Program of China under Grant 2017YFB1002203 and in part by the National
   Natural Science Foundation of China under Grants 61872329 and 61572451.
CR [Anonymous], 2016, ARXIV
   [Anonymous], CORR
   [Anonymous], 2017, ARXIV 1708 04896
   [Anonymous], 2015, ARXIV150407889
   Bazzani L, 2013, COMPUT VIS IMAGE UND, V117, P130, DOI 10.1016/j.cviu.2012.10.008
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Fox NA, 2007, IEEE T MULTIMEDIA, V9, P701, DOI 10.1109/TMM.2007.893339
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hamdoun O, 2008, 2008 SECOND ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P140
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jaderberg M, 2015, ADV NEUR IN, V28
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Simonyan K, 2015, IEEE INT C ICLR
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yu J, 2018, IEEE T IND ELECTRON, V65, P5060, DOI 10.1109/TIE.2017.2739691
   Yu J, 2014, INFORM SCIENCES, V281, P674, DOI 10.1016/j.ins.2014.01.025
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 47
TC 30
Z9 30
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1605
EP 1618
DI 10.1109/TMM.2019.2946486
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100019
DA 2024-07-18
ER

PT J
AU Wang, M
   Zhou, WG
   Tian, Q
   Li, HQ
AF Wang, Min
   Zhou, Wengang
   Tian, Qi
   Li, Houqiang
TI Neighborhood Pyramid Preserving Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary codes; Quantization (signal); Hash functions; Semantics; Training
   data; Linear programming; Euclidean distance; Binary hashing;
   approximate nearest neighbour search; image retrieval
ID QUANTIZATION
AB In this paper, we devote our efforts to the approximate nearest neighbour (ANN) search problem and propose a new unsupervised binary hashing method, i.e., Neighbourhood Pyramid preserving Hashing (NPH). We represent the nearest neighbours of each data point in a pyramid, and as the learning objective, we impose that the pyramid neighbourhood in each level is consistently preserved across the original Euclidean space and the transformed Hamming space. The neighbourhood is quantitatively characterized by its size, defined as the average distance from the involved nearest neighbours to the referred data point. Our approach is consistent with the distance-preserving principle of binary hashing and achieves stricter neighbourhood structure preserving over previous graph hashing algorithms. The experiments on several large-scale benchmark datasets demonstrate that NPH achieves promising performances compared with those of the existing state-of-the-art unsupervised binary hashing methods.
C1 [Wang, Min; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Tian, Qi] Huawei Noahs Ark Lab, Comp Vis, Shenzhen 518129, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Huawei Technologies
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM wm123@mail.ustc.edu.cn; zhwg@ustc.edu.cn; tian.qi1@huawei.com;
   lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
OI Wang, Min/0000-0003-3048-6980
FU National Key R&D Program of China [2018YFB1402600]; National Natural
   Science Foundation of China [61822208, 61632019]; Youth Innovation
   Promotion Association [CAS 2018497]; NSFC [61836011]
FX The work ofW. Zhou was supported in part by the National Key R&D Program
   of China under contract 2018YFB1402600, in part by the National Natural
   Science Foundation of China under contract 61822208 and 61632019, and in
   part by the Youth Innovation Promotion Association CAS 2018497. The work
   of H. Li by NSFC under contract 61836011.
CR [Anonymous], 2016, P ACM INT C MULT
   [Anonymous], 2009, NIPS
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Brandt J, 2010, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2010.5539852
   Carreira-Perpiñán MA, 2015, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2015.7298654
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Dai B, 2017, PR MACH LEARN RES, V70
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hu QH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1584, DOI 10.1145/3123266.3123403
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Joly Alexis., 2008, PROCEEDING 16 ACM IN, P209
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Lin RS, 2010, PROC CVPR IEEE, P848, DOI 10.1109/CVPR.2010.5540129
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XL, 2013, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2013.206
   Mao XJ, 2017, IEEE T MULTIMEDIA, V19, P382, DOI 10.1109/TMM.2016.2614858
   Min Wang, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9916, P559, DOI 10.1007/978-3-319-48890-5_55
   Norouzi M.E., 2011, ICML
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Song DJ, 2015, IEEE I CONF COMP VIS, P1922, DOI 10.1109/ICCV.2015.223
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Do TT, 2016, LECT NOTES COMPUT SC, V9906, P802, DOI 10.1007/978-3-319-46475-6_49
   Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang Jianfeng., 2013, ACM Multimedia, P133
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang M, 2018, IEEE T IMAGE PROCESS, V27, P907, DOI 10.1109/TIP.2017.2751150
   Wang QF, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3911
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xia Y, 2015, PROC CVPR IEEE, P3332, DOI 10.1109/CVPR.2015.7298954
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yi SY, 2019, IEEE T MULTIMEDIA, V21, P1399, DOI 10.1109/TMM.2018.2877888
   Zhang L, 2013, PROCEEDINGS OF THE EIGHTH INTERNATIONAL SYMPOSIUM ON VITICULTURE AND ENOLOGY (2013), P123
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zheng F, 2018, AAAI CONF ARTIF INTE, P4539
   Zheng F, 2018, IEEE T PATTERN ANAL, V40, P1059, DOI 10.1109/TPAMI.2016.2645565
   Zhou WG, 2016, IEEE T PATTERN ANAL, V38, P159, DOI 10.1109/TPAMI.2015.2430329
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 57
TC 5
Z9 5
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1507
EP 1518
DI 10.1109/TMM.2019.2943778
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100011
DA 2024-07-18
ER

PT J
AU Zhang, YF
   Wang, P
   Bao, FX
   Yao, XX
   Zhang, CM
   Lin, HW
AF Zhang, Yunfeng
   Wang, Ping
   Bao, Fangxun
   Yao, Xunxiang
   Zhang, Caiming
   Lin, Hongwei
TI A Single-Image Super-Resolution Method Based on Progressive-Iterative
   Approximation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image edge detection; Interpolation; Surface reconstruction; Image
   reconstruction; Splines (mathematics); Surface topography; Surface
   texture; Single image super-resolution; regional division; surface
   geometric iteration; curve geometric iteration
ID SPARSE REPRESENTATION; INTERPOLATION; REGRESSION; DEBLOCKING
AB In this paper, a novel single image super-resolution (SR) method based on progressive-iterative approximation is proposed. To preserve textures and clear edges, the image SR reconstruction is treated as an image progressive-iterative fitting procedure and achieved by iterative interpolation. Due to different features in different regions, we first employ the nonsubsampled contourlet transform (NSCT) to divide the image into smooth regions, texture regions, and edges. Then, a hybrid interpolation scheme based on curves and surfaces is proposed, which differs from the traditional surface interpolation methods. Specifically, smooth regions are interpolated by the non-uniform rational basis spline (NURBS) surface geometric iteration. To retain textures, control points are increased, and the progressive-iterative approximation of the NURBS surface is employed to interpolate the texture regions. By considering edges in an image as curve segments that are connected by pixels with dramatic changes, we use NURBS curve progressive-iterative approximation to interpolate the edges, which sharpens the edges and can maintain the image edge structure without jaggy and block artifacts. The experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art methods in terms of both subjective and objective measures.
C1 [Zhang, Yunfeng] Shandong Univ Finance & Econ, Comp Sci & Technol, Jinan 250014, Peoples R China.
   [Wang, Ping] Ecole Technol Super, Software & IT Dept, Montreal, PQ H3C 1K3, Canada.
   [Bao, Fangxun] Shandong Univ, Dept Math, Jinan 250100, Peoples R China.
   [Yao, Xunxiang] Univ Technol, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Zhang, Caiming] Shandong Univ, Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
   [Lin, Hongwei] Zhejiang Univ, Dept Math Sci, Hangzhou 310058, Peoples R China.
C3 Shandong University of Finance & Economics; University of Quebec; Ecole
   de Technologie Superieure - Canada; Shandong University; University of
   Technology Sydney; Shandong University; Zhejiang University
RP Bao, FX (corresponding author), Shandong Univ, Dept Math, Jinan 250100, Peoples R China.
EM yfzhang@sdufe.edu.cn; wp_scdedu@163.com; fxbao@sdu.edu.cn;
   12967776@stu-dent.uts.edu.au; czhang@sdu.edu.cn; hwlin@zju.edu.cn
RI li, sixuan/KGR-3943-2024
OI Wang, Ping/0000-0002-0992-4010; zhang, caiming/0000-0003-0217-1543; Lin,
   Hongwei/0000-0002-9337-9624
FU National Natural Science Foundation of China [61373080, 61672018,
   61402261, 61772309]; National Natural Science Foundation
   ofChina-Zhejiang Two Integration Joint Fund Key Project [U1609218];
   Primary Research and Development Plan of Shandong Province
   [2016GSF120013, 2017GGX10109, 2018GGX101013, 2019GSF109112]; Fostering
   Project of Dominant Discipline and Talent Team of Shandong Province
   Higher Education Institutions; Shandong Provincial Natural Science
   Foundation for Excellent Youth [ZR2018JL022]; Shandong Provincial
   Natural Science Foundation [ZR2019MF051]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61373080, 61672018, 61402261, and
   61772309, in part by the National Natural Science Foundation
   ofChina-Zhejiang Two Integration Joint Fund Key Project under Grant
   U1609218, in part by the Primary Research and Development Plan of
   Shandong Province under Grants 2016GSF120013, 2017GGX10109,
   2018GGX101013, and 2019GSF109112, in part by the Fostering Project of
   Dominant Discipline and Talent Team of Shandong Province Higher
   Education Institutions, in part by the Shandong Provincial Natural
   Science Foundation for Excellent Youth under Grant ZR2018JL022, and in
   part by Shandong Provincial Natural Science Foundation under Grant
   ZR2019MF051. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Marco Carli.
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bevilacqua M, 2014, IEEE T IMAGE PROCESS, V23, P5334, DOI 10.1109/TIP.2014.2364116
   Chen HG, 2017, IEEE T MULTIMEDIA, V19, P1702, DOI 10.1109/TMM.2017.2688920
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   Franzen Rich, Kodak lossless true color image suite
   Giachetti A, 2011, IEEE T IMAGE PROCESS, V20, P2760, DOI 10.1109/TIP.2011.2136352
   Hu Shi-Min, 2009, Chinese Journal of Computers, V92, P1451, DOI 10.3724/SP.J.1016.2009.01451
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Li MD, 2015, IEEE T CIRC SYST VID, V25, P200, DOI 10.1109/TCSVT.2014.2347531
   Li T, 2018, IEEE T MULTIMEDIA, V20, P1305, DOI 10.1109/TMM.2017.2766889
   Li T, 2016, NEUROCOMPUTING, V216, P1, DOI 10.1016/j.neucom.2016.06.066
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Liu JY, 2017, IEEE T MULTIMEDIA, V19, P302, DOI 10.1109/TMM.2016.2614427
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   MILLARD JB, 1967, IEEE T INFORM THEORY, V13, P341, DOI 10.1109/TIT.1967.1053982
   Piegl L, 1995, NURBS BOOK
   Ren C, 2016, IEEE T IMAGE PROCESS, V25, P2168, DOI 10.1109/TIP.2016.2542442
   Shi F, 2015, IEEE T MED IMAGING, V34, P2459, DOI 10.1109/TMI.2015.2437894
   Song Q, 2018, IEEE T IMAGE PROCESS, V27, P1966, DOI 10.1109/TIP.2017.2789323
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tang Y, 2013, J VIS COMMUN IMAGE R, V24, P148, DOI 10.1016/j.jvcir.2012.02.003
   Thévenaz P, 2000, BIOMED EN S, P393
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang LF, 2013, IEEE T CIRC SYST VID, V23, P1289, DOI 10.1109/TCSVT.2013.2240915
   Xu HT, 2013, IEEE T CIRC SYST VID, V23, P1740, DOI 10.1109/TCSVT.2013.2248305
   Yan Q, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2414877
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang KB, 2015, IEEE T IMAGE PROCESS, V24, P846, DOI 10.1109/TIP.2015.2389629
   Zhang ML, 2019, IEEE T IMAGE PROCESS, V28, P868, DOI 10.1109/TIP.2018.2874284
   Zhang YB, 2016, IEEE T MULTIMEDIA, V18, P405, DOI 10.1109/TMM.2015.2512046
   Zhang YF, 2018, IEEE T IMAGE PROCESS, V27, P3782, DOI 10.1109/TIP.2018.2826139
   Zhou D, 2012, IET IMAGE PROCESS, V6, P627, DOI 10.1049/iet-ipr.2011.0534
   Zhu SY, 2016, IEEE T MULTIMEDIA, V18, P1707, DOI 10.1109/TMM.2016.2593039
NR 42
TC 17
Z9 25
U1 1
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1407
EP 1422
DI 10.1109/TMM.2019.2943750
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100003
DA 2024-07-18
ER

PT J
AU Jung, H
   Lee, HJ
   Rhee, CE
AF Jung, Hyunmin
   Lee, Hyuk-Jae
   Rhee, Chae Eun
TI Flexibly Connectable Light Field System For Free View Exploration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Scalability; Light fields; Three-dimensional displays; Two
   dimensional displays; Virtual reality; Google; Free view visualization;
   light field; 3D scanning; view navigation; virtual reality; virtual
   exploration
AB Conventional captured-image-based virtual reality (VR) systems have three degrees of freedom (DoFs), where only rotational user motion is tracked for view rendering. This is a major cause of the reduced sense of reality. To increase user immersion levels akin to the real world, 3-DoF+ VR systems that support not only rotational but also translational view changes have been proposed. The light-field (LF) approach is suitable for this type of 3-DoF+ VR because it renders a view from a free view position by simply combining lights. Many previous systems have limited scalability because they assume a single LF for the acquisition and representation of light. One recent work connects multiple LFs at a physical intersection to increase the scalability. However, these fixed connection points limit the renderable view range and the layout of multiple LFs. Furthermore, in conventional single- or multiple-LF systems, the representable ranges of the view positions are highly dependent on the input field-of-view (FOV) of the camera used. In order to realize a wide view exploration range, the above-mentioned limitations must be overcome. This paper proposes a flexible connection scheme for multiple-LF systems taking advantage of the constant radiance of rays in LF theory. The proposed flexibly connectable LF system is able to widen the range of the renderable view position under reasonable conditions of the camera FOV. A light-field unit (LFU) which uses the proposed flexible connection is implemented. The LFU has a square-shaped structure and is thus easily stackable. This offers the advantage of dramatically expanding the scope of view explorations. The proposed LFU achieves 3-DoF+ VR with good quality as well as high scalability. Its cost-performance outcome is also better compared to those in previous works.
C1 [Jung, Hyunmin; Lee, Hyuk-Jae] Seoul Natl Univ, Interuniv Semicond Res Ctr, Dept Elect & Comp Engn, Seoul 08226, South Korea.
   [Rhee, Chae Eun] Inha Univ, Dept Informat & Commun Engn, Incheon 22212, South Korea.
C3 Seoul National University (SNU); Inha University
RP Rhee, CE (corresponding author), Inha Univ, Dept Informat & Commun Engn, Incheon 22212, South Korea.
EM jhm6944@capp.snu.ac.kr; hyuk_jae_lee@capp.snu.ac.kr;
   chae.rhee@inha.ac.kr
OI Rhee, Chae Eun/0000-0002-7851-1703
FU Samsung Research Funding Center of Samsung Electronics [SRFC-IT1702-06]
FX This work was supported by Samsung Research Funding Center of Samsung
   Electronics under Project Number SRFC-IT1702-06.
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 2012, MITCSAILTR2012020
   [Anonymous], 2018, P IEEE INT S CIRC SY
   Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Dansereau DG, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2665074
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   Flint A, 2011, IEEE I CONF COMP VIS, P2228, DOI 10.1109/ICCV.2011.6126501
   Fujii T, 2002, PROC SPIE, V4864, P175, DOI 10.1117/12.454905
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Ge XH, 2017, IEEE T MULTIMEDIA, V19, P2345, DOI 10.1109/TMM.2017.2733461
   Geng J, 2011, ADV OPT PHOTONICS, V3, P128, DOI 10.1364/AOP.3.000128
   Vu HH, 2012, IEEE T PATTERN ANAL, V34, P889, DOI 10.1109/TPAMI.2011.172
   Ikehata S, 2015, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2015.156
   Jenke P, 2009, WSCG 2009, FULL PAPERS PROCEEDINGS, P17
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li JQ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440760
   Ma R, 2018, IEEE T MULTIMEDIA, V20, P1595, DOI 10.1109/TMM.2017.2779039
   Maesen S., 2016, 2016 18th European Conference on Power Electronics and Applications, EPE 2016 ECCE Europe, P1
   Maugey T, 2013, IEEE T MULTIMEDIA, V15, P1070, DOI 10.1109/TMM.2013.2246147
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Overbeck RS, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214811
   Shum HY, 2005, IEEE T MULTIMEDIA, V7, P85, DOI 10.1109/TMM.2004.840591
   Stankiewicz O, 2018, IEEE T MULTIMEDIA, V20, P2182, DOI 10.1109/TMM.2018.2790162
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Wang RS, 2013, INT J IMAGE DATA FUS, V4, P273, DOI 10.1080/19479832.2013.811124
   Wang YW, 2017, IEEE T VIS COMPUT GR, V23, P2357, DOI 10.1109/TVCG.2016.2628743
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wong TT, 2002, IEEE T MULTIMEDIA, V4, P361, DOI 10.1109/TMM.2002.802835
   Xiao JX, 2012, LECT NOTES COMPUT SC, V7572, P668, DOI [10.1007/s11263-014-0711-y, 10.1007/978-3-642-33718-5_48]
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang S, 2016, COMPUT VIS IMAGE UND, V145, P148, DOI 10.1016/j.cviu.2015.12.007
NR 41
TC 17
Z9 18
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 980
EP 991
DI 10.1109/TMM.2019.2934819
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400013
DA 2024-07-18
ER

PT J
AU Jang, W
AF Jang, Wooyoung
TI MLC STT-MRAM-Aware Memory Subsystem for Smart Image Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Resistance; Magnetic tunneling; Random access memory; Memory management;
   Multimedia systems; Switches; Streaming media; Multi-level cell;
   STT-MRAM; image; memory management
ID RAM
AB Next-generation memories with high storage capacity, high performance, and low power consumption are being researched due to the ever-growing demand for artificial intelligence and high-definition applications. Among such future memories, a multi-level cell (MLC) spin-transfer magnetic torque random access memory (STT-MRAM) attracts considerable attention as an alternative to static or dynamic random access memories. An MCL STT-MRAM has the advantages of capacity and non-volatility, but the disadvantages of performance, power consumption, and endurance resulting from complicated resistance state transition and detection processes. In particular, such issues are exacerbated in the latest smart image applications employing block-based processing algorithms. In this paper, we propose a memory subsystem that mitigates the MLC STT-MRAM disadvantages in smart image applications. Our main idea is threefold: MLC-aware image buffer composing, block-aware pixel-to-memory mapping, and prediction-aware image-to-buffer allocating techniques that all make multi-step resistance state transition and detection processes less required. Experimental results show that the proposed memory subsystem achieves 24.5% shorter application execution time, and 96.4% lower memory power consumption than the conventional memory subsystems for industrial smart image applications. In addition, our memory subsystem increases the lifetime of MLC STT-MRAMs via 93.8% fewer multi-step resistance state transition processes.
C1 [Jang, Wooyoung] Dankook Univ, Dept Elect & Elect Engn, Yongin 16890, South Korea.
C3 Dankook University
RP Jang, W (corresponding author), Dankook Univ, Dept Elect & Elect Engn, Yongin 16890, South Korea.
EM wyjang@dankook.ac.kr
RI Jang, Wooyoung/HTT-2559-2023
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education [NRF2017R1D1A1B03036353]
FX This research was supported by Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Education under Grant NRF2017R1D1A1B03036353.
CR [Anonymous], OP COR PROT SPEC REL
   [Anonymous], 2015, J APPL PHYS
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], HP CACT
   [Anonymous], INT CYCL 10 FPGA HIG
   Bojarski Mariusz, 2016, arXiv
   Chang MT, 2013, INT S HIGH PERF COMP, P143, DOI 10.1109/HPCA.2013.6522314
   Chappert C, 2007, NAT MATER, V6, P813, DOI 10.1038/nmat2024
   Chen QB, 2018, IEEE T MULTIMEDIA, V20, P1113, DOI 10.1109/TMM.2017.2762004
   Chen XC, 2017, IEEE T COMPUT, V66, P786, DOI 10.1109/TC.2016.2625245
   Chen YR, 2010, MIDWEST SYMP CIRCUIT, P1109, DOI 10.1109/MWSCAS.2010.5548848
   Chi P, 2015, INT SYM QUAL ELECT, P639
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fang YT, 2014, IEEE T VLSI SYST, V22, P1450, DOI 10.1109/TVLSI.2013.2266668
   Guo L, 2014, IEEE T MULTIMEDIA, V16, P2323, DOI 10.1109/TMM.2014.2350256
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ishigaki T, 2010, S VLSI TECH, P47, DOI 10.1109/VLSIT.2010.5556126
   Jang W, 2017, ELECTRON LETT, V53, P12, DOI 10.1049/el.2016.2435
   Jang WY, 2011, IEEE T COMPUT AID D, V30, P1521, DOI 10.1109/TCAD.2011.2160176
   Jang WY, 2010, IEEE T COMPUT AID D, V29, P1572, DOI 10.1109/TCAD.2010.2061251
   Jiang L, 2012, DES AUT CON, P907
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li YR, 2012, IEEE T MULTIMEDIA, V14, P303, DOI 10.1109/TMM.2011.2177079
   Lu N, 2014, IEEE INTERNET THINGS, V1, P289, DOI 10.1109/JIOT.2014.2327587
   Luo HZ, 2016, DES AUT CON, DOI 10.1145/2897937.2898106
   Ping Zhou, 2009, Proceedings of the 2009 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 2009), P264, DOI 10.1145/1687399.1687448
   Sampaio F, 2014, ICCAD-IEEE ACM INT, P132, DOI 10.1109/ICCAD.2014.7001343
   Stancu LC, 2012, IEEE SYM EMBED SYST, P2, DOI 10.1109/ESTIMedia.2012.6507022
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang JX, 2014, PR IEEE COMP DESIGN, P126
   Watkinson J., 2004, The MPEG Handbook MPEG-1, MPEG-2, MPEG-4, VSecond
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wong HSP, 2015, NAT NANOTECHNOL, V10, P191, DOI 10.1038/nnano.2015.29
   Wong HSP, 2012, P IEEE, V100, P1951, DOI 10.1109/JPROC.2012.2190369
   Wuttig M, 2007, NAT MATER, V6, P824, DOI 10.1038/nmat2009
   Yiran Chen, 2011, 2011 International Symposium on Low Power Electronics and Design (ISLPED 2011), P73, DOI 10.1109/ISLPED.2011.5993610
NR 37
TC 3
Z9 3
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 717
EP 729
DI 10.1109/TMM.2019.2930342
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700012
DA 2024-07-18
ER

PT J
AU Lu, LH
   Lu, Y
   Yu, RZ
   Di, HJ
   Zhang, L
   Wang, SZ
AF Lu, Lihua
   Lu, Yao
   Yu, Ruizhe
   Di, Huijun
   Zhang, Lin
   Wang, Shunzhou
TI GAIM: Graph Attention Interaction Model for Collective Activity
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Activity recognition; Recurrent neural networks; Task analysis;
   Graphical models; Convolutional neural networks; Laplace equations;
   Unbalanced and Two-Level Interactions; Graph Convolutional Networks;
   Attention Mechanisms; Collective Activity Recognition
ID NETWORKS
AB Unbalanced interaction relationships at personal and group levels play a pivotal role in collective activity recognition, which has not been adaptively and jointly explored by previous approaches. In this paper, we propose a graph attention interaction model (GAIM) embedded with the graph attention block (GAB) to explicitly and adaptively infer unbalanced interaction relations at personal and group levels in a unified architecture, and further to learn the spatial and temporal evolutions of the collective activity from these interactions to predict the activity labels. We first design the spatiotemporal graphs tailored to the collective activity where the concurrent person and group nodes, respectively, represent individuals' actions and the collective activity. The graphs provide both spatial structures and semantic appearance features for the collective activity. Then, GAB performs convolution-like filters on the graphs to infer unequal and two-level interaction relations in the collective activity by implementing graph convolutional networks with a shared attention mechanism. At the personal level, the GAB learns different levels of interactions for each person node from its neighbor person nodes under the guidance from the group node. At the group level, the GAB assesses various degrees of interactions to the group node contributed by person nodes. Equipped with the GRUs network, the GAIM learns the spatial and temporal evolutions of individuals' actions as well as the collective activity from the captured interactions, and finally predicts the label of the collective activity. Experiments on four publicly available datasets and ablation studies are conducted to evaluate the performance of our GAIM, and the improved performance demonstrates the effectiveness of our model.
C1 [Lu, Lihua; Lu, Yao; Yu, Ruizhe; Di, Huijun; Zhang, Lin; Wang, Shunzhou] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Lu, Y (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM lulihua@bit.edu.cn; vis_yl@bit.edu.cn; 2120171090@bit.edu.cn;
   ajon@bit.edu.cn; zhanglin@bit.edu.cn; wangshunzhou@bit.edu.cn
RI WANG, SHUNZHOU/GWQ-9444-2022
OI Lu, Lihua/0000-0003-4328-8465
FU National Nature Science Foundation of China [61273273]; National Key
   Research and Development Plan [2017YFC0112001]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61273273 and in part by the National Key
   Research and Development Plan (No. 2017YFC0112001). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Engin. Erzin.
CR Amer MR, 2014, LECT NOTES COMPUT SC, V8694, P572, DOI 10.1007/978-3-319-10599-4_37
   [Anonymous], 2015, DEEP CONVOLUTIONAL N
   [Anonymous], 2018, ARXIV180209834
   [Anonymous], 2014, ABS14091259 CORR
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], 2016, ARXIV160106759
   [Anonymous], 2016, ARXIV160702643
   [Anonymous], 2015, ARXIV150300075
   Atwood J, 2016, NIPS, P2001
   Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365
   Bahdanau Dzmitry, 2014, NEURAL MACHINE TRANS
   Baradel F., 2017, Pose-conditioned Spatio-Temporal Attention for Human Action Recognition
   Biswas S, 2018, IEEE WINT CONF APPL, P1615, DOI 10.1109/WACV.2018.00180
   Bruna J., 2013, INT C LEARNING REPRE
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang XB, 2015, IEEE T IMAGE PROCESS, V24, P1905, DOI 10.1109/TIP.2015.2409564
   Choi W, 2014, IEEE T PATTERN ANAL, V36, P1242, DOI 10.1109/TPAMI.2013.220
   Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16
   Choi W, 2009, IEEE ANTENNAS PROP, P1280
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng Z, 2015, ARXIV150604191
   Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Gehring J, 2017, PR MACH LEARN RES, V70
   Gómez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572
   Hajimirsadeghi H, 2015, PROC CVPR IEEE, P2596, DOI 10.1109/CVPR.2015.7298875
   Hamilton WL, 2017, ADV NEUR IN, V30
   Ibrahim MS, 2018, LECT NOTES COMPUT SC, V11207, P742, DOI 10.1007/978-3-030-01219-9_44
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Jiang B, 2019, PROC CVPR IEEE, P11305, DOI 10.1109/CVPR.2019.01157
   Kipf TN, 2016, ARXIV
   Lan T, 2012, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2012.6247821
   Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228
   Li CL, 2018, IEEE T IMAGE PROCESS, V27, P3657, DOI 10.1109/TIP.2018.2815744
   Li X, 2017, IEEE I CONF COMP VIS, P2895, DOI 10.1109/ICCV.2017.313
   Li YH, 2015, 2015 INTERNATIONAL WORKSHOP ON PATTERN RECOGNITION IN NEUROIMAGING (PRNI) 2015, P41, DOI 10.1109/PRNI.2015.18
   Lu LH, 2018, NEUROCOMPUTING, V322, P195, DOI 10.1016/j.neucom.2018.09.060
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Niepert M, 2016, PR MACH LEARN RES, V48
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Ramanathan V, 2016, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR.2016.332
   Sharma S., 2015, NEURAL INFORM PROCES
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Shu TM, 2017, PROC CVPR IEEE, P4255, DOI 10.1109/CVPR.2017.453
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang YY, 2018, PEER PEER NETW APPL, V11, P711, DOI 10.1007/s12083-017-0554-8
   Thekumparampil Kiran K., 2018, CoRR
   Vaswani A., 2017, P ADV NEUR INF PROC, V2017, P5998
   Velikovic P., 2017, Graph attention networks, P1
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang MS, 2017, PROC CVPR IEEE, P7408, DOI 10.1109/CVPR.2017.783
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang Xianwang., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM'11, P1353
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wongun Choi, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3273, DOI 10.1109/CVPR.2011.5995707
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y
   Zheng ZL, 2019, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2019.00683
   Zhu YY, 2013, PROC CVPR IEEE, P2491, DOI 10.1109/CVPR.2013.322
NR 68
TC 37
Z9 45
U1 1
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 524
EP 539
DI 10.1109/TMM.2019.2930344
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300019
DA 2024-07-18
ER

PT J
AU Li, J
   Krasula, L
   Baveye, Y
   Li, Z
   Le Callet, P
AF Li, Jing
   Krasula, Lukas
   Baveye, Yoann
   Li, Zhi
   Le Callet, Patrick
TI AccAnn: A New Subjective Assessment Methodology for Measuring
   Acceptability and Annoyance of Quality of Experience
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acceptability; annoyance; video quality; subjective quality assessment;
   statistical results processing; bench marking
ID VIDEO; ERROR
AB User expectations have a crucial impact on the levels of quality of experience (QoE) that they consider acceptable or satisfying. Measuring acceptability and annoyance has mainly been performed in separate or multi-step experiments without any control over participants' expectations. This paper introduces a simple methodology to obtain the information about both of the entities in a single step and compares several data processing strategies useful for results interpretation. A specifically designed subjective experiment, conducted on compressed videos, has shown that the multi-step procedures could be replaced by our proposed single-step approach, regardless of the viewing conditions, while the novel approach is significantly preferred by observers for its low time requirements and higher intuitiveness. The test has simultaneously proven that user expectations can be altered by the instructions and it is, therefore, possible to simulate different user profiles regardless of the participants' real habits. The acceptability/annoyance experimental results are also used to benchmark the state-of-the-art objective video quality metrics in predicting acceptability/annoyance of QoE. A case study on the determination of the threshold of acceptability/annoyance for objective quality metrics is conducted, which can be served as a guideline for video streaming service providers.
C1 [Li, Jing; Krasula, Lukas; Le Callet, Patrick] Univ Nantes, Polytech Nantes, IPI LS2N Lab, F-44300 Nantes, France.
   [Baveye, Yoann] Capacites SAS, F-44200 Nantes, France.
   [Li, Zhi] Netflix, Los Gatos, CA 95032 USA.
C3 Nantes Universite; Netflix, Inc.
RP Li, J (corresponding author), Univ Nantes, Polytech Nantes, IPI LS2N Lab, F-44300 Nantes, France.
EM jing.li.univ@gmail.com; l.krasula@gmail.com; yoann.baveye@capacites.fr;
   zli@netflix.com; patrick.lecallet@univ-nantes.fr
RI Le Callet, Patrick/F-5772-2010
FU Netflix
FX This work was supported by Netflix via a donation to Fondation de
   l'Universite de Nantes.
CR [Anonymous], 2004, P 2004 C HUM FACT CO
   [Anonymous], 2008, SUBJ VID QUAL ASS ME
   [Anonymous], 2012, METH SUBJ ASS QUAL T
   [Anonymous], 2012, P1401 ITUT
   [Anonymous], 2016, P IEEE INT C QUAL MU
   [Anonymous], P NORD C HUM COMP IN
   Atzori L, 2014, J VIS COMMUN IMAGE R, V25, P586, DOI 10.1016/j.jvcir.2013.08.013
   BARNARD GA, 1945, NATURE, V156, P177, DOI 10.1038/156177a0
   Carroll J., 2000, Making Use: Scenario-based Design of Human-Computer Interactions
   Catellier A, 2012, INT WORK QUAL MULTIM, P39, DOI 10.1109/QoMEX.2012.6263834
   Chan A, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P221
   De Pessemier T, 2012, IEEE T BROADCAST, V58, P580, DOI 10.1109/TBC.2012.2199590
   Eichhorn A., 2009, IEEE INT C COMM, P1, DOI [10.1109/ICC.2009.5305948, DOI 10.1109/ICC.2009.5305948]
   Fisher RA, 1922, J R STAT SOC, V85, P87, DOI 10.2307/2340521
   Goldmann L, 2010, PROC SPIE, V7526, DOI 10.1117/12.839438
   Himawan I, 2017, MULTIMED TOOLS APPL, V76, P785, DOI 10.1007/s11042-015-3054-y
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Jumisko-Pyykko Satu., 2008, P 10 INT C HUMAN COM, P63, DOI DOI 10.1145/1409240.1409248
   Khaustova D., 2015, THESIS
   Knoche H, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1556134.1556137
   Knoche H, 2008, MULTIMED TOOLS APPL, V36, P145, DOI 10.1007/s11042-006-0076-5
   Li JY, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON ENERGY, P1
   Li Z., 2016, NETFLIX TECH BLOG
   Li Z, 2017, IEEE DATA COMPR CONF, P52, DOI 10.1109/DCC.2017.26
   Liao Y., 2013, P 7 INT WORKSH VID P, P91
   McDonald J. H., 2014, HDB BIOL STAT
   McNemar Q, 1947, PSYCHOMETRIKA, V12, P153, DOI 10.1007/BF02295996
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Narwaria M, 2018, IEEE T MULTIMEDIA, V20, P2063, DOI 10.1109/TMM.2018.2794266
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Ponomarenko N., 2007, INT WORKSH VID PROC
   Rehman A, 2015, PROC SPIE, V9394, DOI 10.1117/12.2077917
   Ries Michal, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P159
   Ronca D., 2015, NETFLIX TECH BLOG
   SASSE MA, 2006, P 2 ISCA DEGA TUT RE
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Song W, 2010, INT J SOFTW ENG KNOW, V20, P1045, DOI 10.1142/S0218194010005067
   Song Wei., 2011, Proceedings_of_the_19th_ACM international_conference_on_Multimedia, P403
   Staelens N, 2014, IEEE T BROADCAST, V60, P707, DOI 10.1109/TBC.2014.2359255
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WELCH BL, 1947, BIOMETRIKA, V34, P28, DOI 10.2307/2332510
   Wolf S., 2011, NTIA Technical Memorandum TM-11-482
   Zeithaml V.A., 1990, DELIVERING QUALITY S
NR 47
TC 12
Z9 13
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2589
EP 2602
DI 10.1109/TMM.2019.2903722
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400013
DA 2024-07-18
ER

PT J
AU Zheng, J
   Wang, Y
   Wang, HZ
   Li, B
   Hu, HM
AF Zheng, Jin
   Wang, Yue
   Wang, Hanzi
   Li, Bo
   Hu, Hai-Miao
TI A Novel Projective-Consistent Plane Based Image Stitching Method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image alignment; image stitching; projective-consistent plane; normal
   vector; scene depth
ID SEGMENTATION; SHAPE
AB When different target surfaces, in three-dimensional space, are mapped onto an image plane, they have different projections. These projections vary with the viewpoint. These local differences have influence on the accuracy of image stitching. Most of the existing image stitching methods divide an input image into a number of fixed-size cells, and the pixels within the same cell are then warped using the same local transformation model for the alignment. These methods are based on the hypothesis that the transformation models in one cell are consistent. However, this hypothesis does not hold in general. In this paper, we propose a novel projective-consistent plane based image stitching method (termed PCPS). It divides the overlapping regions of an input image into some projective-consistent planes according to the normal vectors' orientations of local regions and the reprojection errors of aligned images. The local projective transformation model is estimated for each projective-consistent plane. And then, a hybrid warping model is estimated. For the pixels in overlapping regions, the local projective transformation models are adopted to achieve a better alignment. While for the pixels in non-overlapping regions, a global projective transformation model is estimated by using the inliers uniformly distributed in the projective consistent planes to avoid distortion. Compared with the state-of-the-art image stitching methods, the experimental results on a number of challenging image sequences show that the projective transformation model estimated by the proposed PCPS method for each projective-consistent plane is more accurate, and the achieved stitching results have less seams and projective distortion.
C1 [Zheng, Jin] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Yue; Li, Bo; Hu, Hai-Miao] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Wang, Hanzi] Xiamen Univ, Sch Informat Sci & Engn, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.
C3 Beihang University; Beihang University; Xiamen University
RP Wang, Y (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM JinZheng@buaa.edu.cn; wangyue64@126.com; hanzi_wang@163.com;
   boli@buaa.edu.cn; frank0139@163.com
RI Li, bo/IWL-9318-2023; Wang, Han/GPW-9809-2022; Li, Bo/AAA-8968-2020;
   wang, hao/HSE-7975-2023; wang, handong/HLH-5739-2023; , 郑锦/ABD-4196-2020
OI Li, Bo/0000-0002-7294-6888; , 郑锦/0000-0002-5218-9250
FU National Key Research and Development Plan [2016YFC0801002]; NSFC
   [61876014, 61772054, U1605252]; Army Equipment Research [301020203]
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFC0801002, in part by the NSFC under
   Grants 61876014, 61772054 and U1605252, and in part Army Equipment
   Research under Project 301020203.
CR Battiato S, 2010, IEEE T MULTIMEDIA, V12, P622, DOI 10.1109/TMM.2010.2060474
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chang XJ, 2017, IEEE T IMAGE PROCESS, V26, P3911, DOI 10.1109/TIP.2017.2708506
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Duplaquet ML, 1998, P SOC PHOTO-OPT INS, V3387, P369, DOI 10.1117/12.316427
   Edelsbrunner H, 2017, COMBINATORICA, V37, P887, DOI 10.1007/s00493-016-3308-y
   Erdil E, 2017, IEEE T IMAGE PROCESS, V26, P5312, DOI 10.1109/TIP.2017.2728185
   Gao JH, 2011, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2011.5995433
   García-Lamont F, 2016, ING INVEST, V36, P78, DOI 10.15446/ing.investig.v36n2.55746
   Haines O, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.31
   Heshmati A, 2016, IET IMAGE PROCESS, V10, P464, DOI 10.1049/iet-ipr.2015.0738
   Jung IL, 2013, IEEE T MULTIMEDIA, V15, P56, DOI 10.1109/TMM.2012.2225041
   Khelifi L, 2017, INFORM FUSION, V38, P104, DOI 10.1016/j.inffus.2017.03.001
   Lee SY, 2013, IEEE T MULTIMEDIA, V15, P1719, DOI 10.1109/TMM.2013.2271747
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu ZG, 2016, IEEE T VIS COMPUT GR, V22, P2437, DOI 10.1109/TVCG.2015.2510000
   Lou ZY, 2014, IEEE T MULTIMEDIA, V16, P2052, DOI 10.1109/TMM.2014.2346476
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Medeiros RS, 2016, COMPUT VIS IMAGE UND, V142, P23, DOI 10.1016/j.cviu.2015.06.001
   Mozerov MG, 2015, IEEE T IMAGE PROCESS, V24, P1153, DOI 10.1109/TIP.2015.2395820
   Shi CB, 2015, IEEE T IMAGE PROCESS, V24, P1412, DOI 10.1109/TIP.2015.2393054
   Su TY, 2016, COMPUT GRAPH-UK, V54, P65, DOI 10.1016/j.cag.2015.07.019
   Summa B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185579
   Vedaldi A., 2017, P INT C MULT, P1469
   Vetrivel A, 2015, INT ARCH PHOTOGRAMM, V40-3, P261, DOI 10.5194/isprsarchives-XL-3-W2-261-2015
   Vu DT, 2014, IEEE T IMAGE PROCESS, V23, P3428, DOI 10.1109/TIP.2014.2329389
   Wang HZ, 2012, IEEE T PATTERN ANAL, V34, P1177, DOI 10.1109/TPAMI.2011.216
   Wang Y, 2016, J VIS COMMUN IMAGE R, V40, P751, DOI 10.1016/j.jvcir.2016.08.019
   Xiang TZ, 2018, PATTERN RECOGN, V83, P481, DOI 10.1016/j.patcog.2018.06.013
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52
NR 34
TC 29
Z9 31
U1 5
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2561
EP 2575
DI 10.1109/TMM.2019.2905692
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400011
DA 2024-07-18
ER

PT J
AU Song, G
   Wang, D
   Tan, XY
AF Song, Ge
   Wang, Dong
   Tan, Xiaoyang
TI Deep Memory Network for Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; memory network; deep learning
AB With the explosive growth of multimedia data on the Internet, cross-modal retrieval has attracted a great deal of attention in both computer vision and multimedia communities. However, this task is challenging due to the heterogeneity gap between different modalities. Current approaches typically involve a common representation learning process that maps data from different modalities into a common space by linear or nonlinear embedding. Yet, most of them only handle the dual-modal situation and generalize poorly to complex cases that involve multiple modalities. In addition, they often require expensive fine-grained alignment of training data among diverse modalities. In this paper, we address these with a novel cross-modal memory network (CMMN), in which memory contents across modalities are simultaneously learned from end to end without the need of exact alignment. We further account for the diversity across multiple modalities using the strategy of adversarial learning. Extensive experimental results on several large-scale datasets demonstrate that the proposed CMMN approach achieves state-of-the-art performance in the task of cross-modal retrieval.
C1 [Song, Ge; Wang, Dong; Tan, Xiaoyang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Jiangsu, Peoples R China.
   [Song, Ge; Wang, Dong; Tan, Xiaoyang] Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 211106, Jiangsu, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Tan, XY (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Jiangsu, Peoples R China.
EM sunge@nuaa.edu.cn; dongwang@nuaa.edu.cn; x.tan@nuaa.edu.cn
RI , G.SONG/AAQ-8545-2020
OI Tan, Xiaoyang/0000-0002-2683-8667; Song, Ge/0000-0002-2159-8203
FU National Natural Science Foundation of China [61672280, 61373060,
   61732006]; Pre-research fund of Equipments of China; Jiangsu 333 Project
   [BRA2017377]; Qing Lan Project; Jiangsu Innovation Program for Graduate
   Education [KYLX15_0320]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672280, 61373060, and 61732006, in
   part by the Pre-research fund of Equipments of China, in part by the
   Jiangsu 333 Project BRA2017377, in part by the Qing Lan Project, and in
   part by the Jiangsu Innovation Program for Graduate Education under
   Grant KYLX15_0320. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xavier Giro-i-Nieto.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2011, P ICML
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], PROC 30TH AAAI CONF
   [Anonymous], 2017, INT C LEARNING REPRE
   [Anonymous], P ACM INT C IM VID R
   [Anonymous], 2016, EMNLP 2016
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321
   Chen JZ, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P551, DOI [10.1109/CIS.2016.133, 10.1109/CIS.2016.0134]
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodfellow I. J., 2014, arXiv, 1406. 2661
   Graves A., 2014, ABS14105401 CORR
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jiang XY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P69, DOI 10.1145/2733373.2806240
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros Ryan., 2015, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, P3294
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Lin L, 2017, IEEE T PATTERN ANAL, V39, P1089, DOI 10.1109/TPAMI.2016.2567386
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Liu L, 2017, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR.2017.247
   Miller A., 2016, P 2016 C EMP METH NA, P1400, DOI DOI 10.18653/V1/D16-1147
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Song G, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P64, DOI 10.1145/3206025.3206027
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Sukhbaatar Sainbayar, 2015, ADV NEURAL INFORM PR, P2440
   Wang D, 2019, PATTERN RECOGN, V86, P134, DOI 10.1016/j.patcog.2018.09.006
   Wang K., 2016, ABS160706215 CORR
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Weston J, 2014, ARXIV14103916
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhen Y, 2015, AAAI CONF ARTIF INTE, P3203
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zhou JH, 2014, AER ADV ENG RES, V7, P1
   Zhu YK, 2017, PROC CVPR IEEE, P6146, DOI 10.1109/CVPR.2017.651
NR 57
TC 30
Z9 31
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1261
EP 1275
DI 10.1109/TMM.2018.2877122
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600015
DA 2024-07-18
ER

PT J
AU Ji, RR
   Chen, FH
   Cao, LJ
   Gao, Y
AF Ji, Rongrong
   Chen, Fuhai
   Cao, Liujuan
   Gao, Yue
TI Cross-Modality Microblog Sentiment Prediction via Bi-Layer Multimodal
   Hypergraph Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sentiment prediction; microblog; hypergraph learning; multimodal
   classification
AB Microblog sentiment prediction has attracted extensive research focus with wide application prospects. With the increasing proportion of multimodal tweets consisting of images, texts, and emoticons, new challenges have been raised to the existing sentiment prediction schemes. More crucially, it remains an open problem to model the dependency among multiple modalities, where one or more modalities may be missing. In this paper, we present a novel Bi-layer Multimodal Hypergraph learning (Bi-MHG) toward robust sentiment prediction of multimodal tweets to tackle the above challenges. In particular, we design a two-layer structure for the proposed Bi-MHG model: The first layer, that is, a tweet-level hypergraph, learns the tweet-feature correlation and the tweet relevance to predict the sentiments of unlabeled tweets. The second layer, that is, a feature-level hypergraph learns the relevance among different feature modalities (including the midlevel visual features in Sentibank [1]) by leveraging prior multimodal sentiment dictionaries. These two layers are connected by sharing the relevance of multimodal features in a unified bilayer learning scheme. In such a way, Bi-MHG explicitly models the modality relevance rather than implicitly weighting multimodal features adopted in the existing Multimodal Hypergraph learning [2]. Finally, a nested alternating optimization is further proposed for Bi-MHG parameter learning. We have carried out extensive evaluations on a real-world microblog dataset crawled from Sina Weibo. For the task of multimodal sentiment prediction, superior performance is reported over several state-of-the-art and alternative approaches, which demonstrates the merits of the proposed scheme.
C1 [Ji, Rongrong; Chen, Fuhai; Cao, Liujuan] Xiamen Univ, Sch Informat Sci & Engn, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.
   [Gao, Yue] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
C3 Xiamen University; Tsinghua University
RP Cao, LJ (corresponding author), Xiamen Univ, Sch Informat Sci & Engn, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.; Gao, Y (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM jirongrong@gmail.com; cfh3c@qq.com; caoliujuan@xmu.edu.cn;
   kevin.gaoy@gmail.com
RI Gao, Yue/B-3376-2012; ARSLAN, Okan/AAA-3232-2020
OI Chen, Fuhai/0000-0001-5441-5998; Cao, Liujuan/0000-0002-7645-9606
FU National Key RD Program [2017YFC0113000, 2016YFB1001503]; Nature Science
   Foundation of China [U1705262, 61772443, 61572410]; Postdoctoral
   Innovative Talent Support Program [BX201600094]; China Postdoctoral
   Science Foundation [2017M612134]; Scientific Research Project of
   National Language Committee of China [YB135-49]; Nature Science
   Foundation of Fujian Province, China [2017J01125, 2018J01106]
FX This work was supported in part by the National Key R&D Program under
   Grants 2017YFC0113000 and 2016YFB1001503, in part by Nature Science
   Foundation of China under Grants U1705262, 61772443, and 61572410, in
   part by Postdoctoral Innovative Talent Support Program under Grant
   BX201600094, in part by China Postdoctoral Science Foundation under
   Grant 2017M612134, in part by Scientific Research Project of National
   Language Committee of China under Grant YB135-49, and in part by the
   Nature Science Foundation of Fujian Province, China under Grants
   2017J01125 and 2018J01106. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Hatice
   Gunes. (Corresponding authors: Yue Gao and Liujuan Cao.)
CR [Anonymous], 2012, PROC 20 ACM INT C MU
   [Anonymous], 2013, ACL
   [Anonymous], 2013, Ph.D. thesis
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2012, P 20 ACM MULT C MMM
   [Anonymous], ARXIV150805357
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Ceron A, 2015, SOC SCI COMPUT REV, V33, P3, DOI 10.1177/0894439314521983
   Chen F, 2015, 2015 IEEE 6TH INTERNATIONAL SYMPOSIUM ON MICROWAVE, ANTENNA, PROPAGATION, AND EMC TECHNOLOGIES (MAPE), P1, DOI 10.1109/MAPE.2015.7510253
   Chen RC, 2006, EXPERT SYST APPL, V31, P427, DOI 10.1016/j.eswa.2005.09.079
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dong L, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P49
   Esuli Andrea., 2006, LREC 2006 Proceedings, 2006, S, P417
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Gao Y, 2016, IEEE T MULTIMEDIA, V18, P2115, DOI 10.1109/TMM.2016.2581483
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Go A., 2009, Twitter sentiment classification using distant supervision, V150, DOI DOI 10.1016/J.SEDGEO.2006.07.004
   Hanjalic Alan., 2012, P 20 ACM INT C MULT, P1239
   Hu X., 2013, WSDM
   Huang YC, 2010, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2010.5540012
   Huang YC, 2009, PROC CVPR IEEE, P1738, DOI 10.1109/CVPRW.2009.5206795
   Irsoy O, 2014, P 2014 C EMPIRICAL M, P720, DOI DOI 10.3115/V1/D14-1080
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Jou B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P159, DOI 10.1145/2733373.2806246
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Li B, 2012, P 20 ACM INT C MULT, P1365
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Medhat W, 2014, AIN SHAMS ENG J, V5, P1093, DOI 10.1016/j.asej.2014.04.011
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Plutchik R., 1980, EMOTION PSYCHOEVOLUT
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P1231, DOI 10.1109/TMM.2013.2261481
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Stockl Hartmut., 2004, PERSPECTIVES MULTIMO, P9
   Tang DY, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1014
   Thelwall M, 2010, J AM SOC INF SCI TEC, V61, P2544, DOI 10.1002/asi.21416
   Nguyen TH, 2015, EXPERT SYST APPL, V42, P9603, DOI 10.1016/j.eswa.2015.07.052
   Turney PD, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P417
   Wang H., 2012, P ACL 2012 SYST DEM, P115, DOI DOI 10.1145/1935826.1935854
   Wang Jingwen., 2016, IJCAI, P3484
   Wang M., 2014, P INT C INT MULT COM, P76, DOI [10.1145/2632856.2632912, DOI 10.1145/2632856.2632912]
   Whissell C, 2009, PSYCHOL REP, V105, P509, DOI 10.2466/PR0.105.2.509-521
   Wong C., 2015, J MED INTERNET RES, V17
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yanulevskaya V, 2012, P 20 ACM INT C MULT, P349, DOI [10.1145/2393347.2393399, DOI 10.1145/2393347.2393399]
   You QZ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1071, DOI 10.1145/2733373.2806284
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1008, DOI 10.1145/2964284.2964288
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
   Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008
   Zhang H.-P., 2003, Proc. of the second SIGHAN workshop on Chinese language process, P184, DOI [10.3115/1119250.1119280, DOI 10.3115/1119250.1119280]
   Zhang XY, 2009, IEEE T MULTIMEDIA, V11, P272, DOI 10.1109/TMM.2008.2009689
   Zhou Dengyong, 2006, 19 INT C NEURAL INFO, V19, P1601
NR 54
TC 50
Z9 57
U1 9
U2 72
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 1062
EP 1075
DI 10.1109/TMM.2018.2867718
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700020
DA 2024-07-18
ER

PT J
AU Liu, JY
   Yang, YH
   Jeng, SK
AF Liu, Jen-Yu
   Yang, Yi-Hsuan
   Jeng, Shyh-Kang
TI Weakly-Supervised Visual Instrument-Playing Action Detection in Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action detection; weakly supervised learning; instrument detection;
   video understanding; object localization
ID RECOGNITION
AB Music videos are one of the most popular types of video streaming services, and instrument playing is among the most common scenes in such videos. In order to understand the instrument-playing scenes in the videos, it is important to know what instruments are played, when they are played, and where the playing actions occur in the scene. While audio-based recognition of instruments has been widely studied, the visual aspect of music instrument playing remains largely unaddressed in the literature. One of the main obstacles is the difficulty in collecting annotated data of the action locations for training-based methods. To address this issue, we propose a weakly supervised framework to find when and where the instruments are played in the videos. We propose using two auxiliary models: 1) a sound model and 2) an object model to provide supervision for training the instrument-playing action model. The sound model provides temporal supervisions, while the object model provides spatial supervisions. They together can simultaneously provide temporal and spatial supervisions. The resulting model only needs to analyze the visual part of a music video to deduce which, when, and where instruments are played. We found that the proposed method significantly improves localization accuracy. We evaluate the result of the proposed method temporally and spatially on a small dataset (a total of 5400 frames) that we manually annotated.
C1 [Liu, Jen-Yu; Jeng, Shyh-Kang] Natl Taiwan Univ, Grad Inst Elect Engn, Taipei 10617, Taiwan.
   [Liu, Jen-Yu; Yang, Yi-Hsuan] Acad Sinica, CITI, Taipei 11529, Taiwan.
C3 National Taiwan University; Academia Sinica - Taiwan
RP Liu, JY (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, Taipei 10617, Taiwan.
EM ciaua@citi.sinica.edu.tw; yang@citi.sinica.edu.tw; skjeng@ntu.edu.tw
OI Jeng, Shyh-Kang/0000-0001-8960-9995
CR Abu-El-Haija Sami, 2016, arXiv
   [Anonymous], 2016, P 24 ACM INT C MULTI
   [Anonymous], 2016, ISMIR
   Arandjelovic R., 2017, ARXIV170508168
   Aytar Y, 2016, ADV NEUR IN, V29
   Aytar Yusuf, 2017, ARXIV170600932
   Bittner R. M., 2014, P 15 INT SOC MUS INF, V14, P155
   Bojanowski P, 2014, LECT NOTES COMPUT SC, V8693, P628, DOI 10.1007/978-3-319-10602-1_41
   Canziani A., 2017, ARXIV170602735
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cricri F, 2014, IEEE T MULTIMEDIA, V16, P917, DOI 10.1109/TMM.2014.2307552
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Essid S, 2006, IEEE T AUDIO SPEECH, V14, P68, DOI 10.1109/TSA.2005.860351
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Giannoulis Dimitrios, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5222, DOI 10.1109/ICASSP.2014.6854599
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Han Y, 2017, IEEE-ACM T AUDIO SPE, V25, P208, DOI 10.1109/TASLP.2016.2632307
   Hartmann G, 2012, LECT NOTES COMPUT SC, V7583, P198, DOI 10.1007/978-3-642-33863-2_20
   Hu WM, 2016, IEEE T MULTIMEDIA, V18, P76, DOI 10.1109/TMM.2015.2496372
   Huang DA, 2016, LECT NOTES COMPUT SC, V9908, P137, DOI 10.1007/978-3-319-46493-0_9
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jang S, 2016, IEEE T MULTIMEDIA, V18, P1808, DOI 10.1109/TMM.2016.2581582
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LAW E, 2009, ISMIR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li BC, 2017, INT CONF ACOUST SPEE, P2906, DOI 10.1109/ICASSP.2017.7952688
   Liu JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1048, DOI 10.1145/2964284.2964292
   Liu X, 2014, PROC CVPR IEEE, P57, DOI 10.1109/CVPR.2014.15
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mosabbeb EA, 2015, LECT NOTES COMPUT SC, V9007, P241, DOI 10.1007/978-3-319-16814-2_16
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oquab M., 2015, PROC CVPR IEEE, P685
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Reboursiere  L., 2012, P NEW INT MUS EXPR A, P30
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rizzi A, 2017, IEEE T MULTIMEDIA, V19, P1405, DOI 10.1109/TMM.2017.2674603
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schindler A., 2016, ACM T INTEL SYST TEC, V8, P1
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Slizovskaia O., 2017, P ACM INT C MULTIMED, P226, DOI DOI 10.1145/3078971.3079002
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 50
TC 7
Z9 7
U1 2
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 887
EP 901
DI 10.1109/TMM.2018.2871418
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, GS
   Lei, XJ
   Qian, XM
   Mei, T
AF Zhao, Guoshuai
   Lei, Xiaojiang
   Qian, Xueming
   Mei, Tao
TI Exploring Users' Internal Influence from Reviews for Social
   Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Review
DE Data mining; interpersonal influence; recommender system; review
   sentiment; social network
ID RATING PREDICTION
AB In recent years, we have witnessed a flourish of social review websites. Internet users can easily share their experiences on some products and services with their friends. Therefore, measuring interpersonal influence becomes a popular method for recommender systems. However, traditional works are all based on external tangible activities, such as following, retweeting, mentioning, etc. In this paper, we explore user internal factors to measure his/her influence on a specific domain, namely, the social network on local businesses. The proposed user internal factors include user sentimental deviations and the review's reliability. The internal factors are not from explicit behavior but could help us to understand users. In addition, we utilize an attention mechanism that could auto-learn the weights of different factors. Through a case study on the Yelp dataset, we found that the proposed user internal factors on influence, that is, the proposed user sentimental deviations and the review's reliability, are effective in improving the accuracy of rating predictions.
C1 [Zhao, Guoshuai; Qian, Xueming] Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Shaanxi, Peoples R China.
   [Zhao, Guoshuai; Lei, Xiaojiang; Qian, Xueming] Xi An Jiao Tong Univ, SMILES Lab, Xian 710049, Shaanxi, Peoples R China.
   [Qian, Xueming] Zhibian Technol Co Ltd, Taizhou 317000, Peoples R China.
   [Mei, Tao] JD AI Res, Beijing 100105, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Shaanxi, Peoples R China.
EM zgs2012@stu.xjtu.edu.cn; xiaolei3439@stu.xjtu.edu.cn;
   qianxm@mail.xjtu.edu.cn; tmei@live.com
RI Mei, Tao/GQZ-0596-2022; Zhao, Guoshuai/AAN-1271-2020
OI Mei, Tao/0000-0002-5990-7307; Zhao, Guoshuai/0000-0003-4392-8450
FU NSFC [61732008, 61772407, 61332018, u1531141]; National Key RAMP;D
   Program of China [2017YFF0107700]
FX This work was supported in part by the NSFC under Grants 61732008,
   61772407, 61332018, and u1531141, and in part by the National Key R&D
   Program of China under Grant 2017YFF0107700. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Benoit Huet. (Corresponding author: Xueming Qian.)
CR [Anonymous], 2006, P 5 INT C LANG RES E
   Bao SH, 2012, IEEE T KNOWL DATA EN, V24, P1658, DOI 10.1109/TKDE.2011.188
   Bedi P, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2677
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cai Y, 2014, IEEE T KNOWL DATA EN, V26, P766, DOI 10.1109/TKDE.2013.7
   Chen L, 2015, USER MODEL USER-ADAP, V25, P99, DOI 10.1007/s11257-015-9155-5
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Ding X., 2008, P 2008 INT C WEB SEA, P231, DOI [DOI 10.1145/1341531.1341561, 10.1145/1341531.1341561]
   Forsati R, 2014, ACM T INFORM SYST, V32, DOI 10.1145/2641564
   Ganu G, 2012, INFORM SYST, V38, P1, DOI 10.1016/j.is.2012.03.001
   Guoshuai Zhao, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P181, DOI 10.1007/978-3-319-04117-9_17
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Jamali M., 2010, P 4 ACM C REC SYST, P135, DOI DOI 10.1145/1864708.1864736
   Jiang M, 2014, IEEE T KNOWL DATA EN, V26, P2789, DOI 10.1109/TKDE.2014.2300487
   Jiang Meng, 2012, P 21 ACM INT C INFOR, P45
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Konstan J., 2001, P 10 INT C WORLD WID, P285
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Kumar H, 2014, INFORM SCIENCES, V281, P399, DOI 10.1016/j.ins.2014.05.008
   Lee D D, 2000, Adv. Neural Inf. Process., P535, DOI DOI 10.1186/GB-2013-14-4-R39
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Li F., 2011, Proceedings of the Twenty-Second International joint Conference on Artificial Intelligence, V3, P1820
   Li L, 2014, EXPERT SYST APPL, V41, P3168, DOI 10.1016/j.eswa.2013.11.020
   Lin HJ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P507
   Ling G, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P105, DOI 10.1145/2645710.2645728
   Liu Y, 2010, 2ND IEEE INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER CONTROL (ICACC 2010), VOL. 2, P63, DOI 10.1109/ICACC.2010.5487185
   Lou PL, 2016, 2016 IEEE SECOND INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P33, DOI 10.1109/BigMM.2016.38
   Lu F, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE AND STORAGE (NAS), P167, DOI 10.1109/NAS.2015.7255214
   Lu Yue., 2011, WWW, DOI [DOI 10.1145/1963405.1963456, 10.1145/1963405.1963456]
   Ma H, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P203, DOI 10.1145/1571941.1571978
   Ma X, 2018, MULTIMED TOOLS APPL, V77, P6425, DOI 10.1007/s11042-017-4550-z
   Mikolov T., 2013, P 26 INT C NEURAL IN, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mnih A., 2007, ADV NEURAL INF PROCE, V20, P1257
   Nakagawa T., 2010, HUMAN LANGUAGE TECHN, P786
   Ngoc PT, 2014, 2014 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN 2014), P444, DOI 10.1109/ICOIN.2014.6799721
   Pan Y, 2011, J RETAILING, V87, P598, DOI 10.1016/j.jretai.2011.05.002
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Pang B., 2004, ANN M ASS COMP LING, P271, DOI [10.3115/1218955.1218990, DOI 10.3115/1218955.1218990]
   Peng C.H., 2014, Proceedings of Thirty Fifth International Conference on Information Systems, P1
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Rennie Jasson D. M., 2005, P 22 INT C MACH LEAR, P713, DOI DOI 10.1145/1102351.1102441
   Sato T, 2013, 2013 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM), P1345
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Sun Y, 2012, IEEE SIGNAL PROC MAG, V29, P87, DOI 10.1109/MSP.2011.942344
   Tang DY, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1014
   Venkatasubramanian Suresh., 2011, Proceedings of the 20th international conference companion on World wide web, P137
   Wang SQ, 2012, 2012 2ND INTERNATIONAL CONFERENCE ON UNCERTAINTY REASONING AND KNOWLEDGE ENGINEERING (URKE), P90, DOI 10.1109/URKE.2012.6319592
   Wang T., 2014, P 23 ACM INT C C INF, P281, DOI DOI 10.1145/2661829.2662043
   Wang W.J., 2012, METAL MINE, V10, P1
   Wang XY, 2015, IEEE T MULTIMEDIA, V17, P409, DOI 10.1109/TMM.2014.2385473
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wilson TS, 2005, PHIL EDUC, P347, DOI 10.3115/1220575.1220619
   Xiang L., 2010, P 16 ACM SIGKDD INT, P723, DOI 10.1145/1835804.1835896
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Xiwang., 2012, P 18 ACM SIGKDD INT, P1267, DOI [10.1145/2339530.2339728, DOI 10.1145/2339530.2339728]
   Zeng Y., 2014, STCSN E-Letter, V2
   Zeng YB, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P846, DOI 10.1109/ChinaSIP.2015.7230524
   Zhang WS, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2414425.2414434
   Zhang YF, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P83, DOI 10.1145/2600428.2609579
   Zhang Yongfeng, 2013, P 22 INT C WORLD WID, P1511, DOI DOI 10.1145/2488388.2488520
   Zhao GS, 2020, IEEE T SERV COMPUT, V13, P1115, DOI 10.1109/TSC.2017.2747538
   Zhao GS, 2017, IEEE T BIG DATA, V3, P67, DOI 10.1109/TBDATA.2016.2552541
   Zhao GS, 2016, IEEE T KNOWL DATA EN, V28, P3382, DOI 10.1109/TKDE.2016.2607172
   Zhao GS, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P228, DOI 10.1109/BigMM.2015.67
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhou P, 2016, IEEE T MULTIMEDIA, V18, P1217, DOI 10.1109/TMM.2016.2537216
NR 67
TC 48
Z9 49
U1 2
U2 50
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 771
EP 781
DI 10.1109/TMM.2018.2863598
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800020
DA 2024-07-18
ER

PT J
AU Cappallo, S
   Svetlichnaya, S
   Garrigues, P
   Mensink, T
   Snoek, CGM
AF Cappallo, Spencer
   Svetlichnaya, Stacey
   Garrigues, Pierre
   Mensink, Thomas
   Snoek, Cees G. M.
TI New Modality: Emoji Challenges in Prediction, Anticipation, and
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based retrieval; image classification; machine learning; social
   computing
AB Over the past decade, emoji have emerged as a new and widespread form of digital communication, spanning diverse social networks and spoken languages. We propose treating these ideograms as a new modality in their own right, distinct in their semantic structure from both the text in which they are often embedded as well as the images which they resemble. As a new modality, emoji present rich novel possibilities for representation and interaction. In this paper, we explore the challenges that arise naturally from considering the emoji modality through the lens of multimedia research, specifically the ways in which emoji can be related to other common modalities such as text and images. To do so, we first present a large-scale data set of real-world emoji usage collected from Twitter. This data set contains examples of both text-emoji and image-emoji relationships within tweets. We present baseline results on the challenge of predicting emoji from both text and images, using state-of-the-art neural networks. Further, we offer a first consideration into the problem of how to account for new, unseen emoji-a relevant issue as the emoji vocabulary continues to expand on a yearly basis. Finally, we present results for multimedia retrieval using emoji as queries.
C1 [Cappallo, Spencer] Univ Amsterdam, Comp Vis & Machine Learning, NL-1012 WX Amsterdam, Netherlands.
   [Mensink, Thomas] Univ Amsterdam, Artificial Intelligence, NL-1012 WX Amsterdam, Netherlands.
   [Mensink, Thomas] Univ Amsterdam, 3 D Deep Learning, NL-1012 WX Amsterdam, Netherlands.
   [Snoek, Cees G. M.] Univ Amsterdam, Business Informat Syst, NL-1012 WX Amsterdam, Netherlands.
   [Snoek, Cees G. M.] Univ Amsterdam, Comp Sci, NL-1012 WX Amsterdam, Netherlands.
   [Snoek, Cees G. M.] Univ Amsterdam, Intelligent Sensory Informat Syst Lab, NL-1012 WX Amsterdam, Netherlands.
   [Snoek, Cees G. M.] Univ Amsterdam, Deep Learning & Comp Vis, NL-1012 WX Amsterdam, Netherlands.
   [Snoek, Cees G. M.] Univ Amsterdam, NL-1012 WX Amsterdam, Netherlands.
   [Svetlichnaya, Stacey; Garrigues, Pierre] Yahoo Res, San Francisco, CA 94111 USA.
C3 University of Amsterdam; University of Amsterdam; University of
   Amsterdam; University of Amsterdam; University of Amsterdam; University
   of Amsterdam; University of Amsterdam; University of Amsterdam; Yahoo!
   Inc; Yahoo! Inc United States
RP Cappallo, S (corresponding author), Univ Amsterdam, Comp Vis & Machine Learning, NL-1012 WX Amsterdam, Netherlands.
EM astucity@gmail.com; stacey.svet@gmail.com; pierre.garrigues@gmail.com;
   tmensink@uva.nl; cgmsnoek@uva.nl
OI Svetlichnaya, Anastasia/0000-0001-8923-8063; Mensink,
   Thomas/0000-0002-5730-713X; Snoek, Cees/0000-0001-9092-1556
FU STW Story project
FX This work was supported by the STW Story project. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Lei Zhang.
CR Ai W., 2017, Proceedings of the Eleventh International AAAI Conference on Web and Social Media-ICWSM'17, P2
   Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   [Anonymous], 2017, P 2017 C EMPIRICAL M
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], ARXIV171000888
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], P ASS COMP LING
   [Anonymous], 2017, 8 WORKSH COMP APPR S
   [Anonymous], 2015, ARXIV150301817
   [Anonymous], P LANG RES EV C
   [Anonymous], 2016, 54 ANN M ASS COMPUTA, DOI DOI 10.18653/V1/W16-2610
   [Anonymous], P IT CONV SEC
   Barbieri F., 2016, P 2016 ACM MULT C MM, P531, DOI [DOI 10.1145/2964284.2967278, https://doi.org/10.1145/2964284.2967278]
   Barbieri F., 2017, PROC 15 C EUROPEAN C, V2, P105
   Cappallo S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1311, DOI 10.1145/2733373.2806335
   Cappallo S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P735, DOI 10.1145/2733373.2807961
   Chen Jiawei., 2014, Proceedings of International Conference on Multimedia Retrieval, P1
   Chen Z., 2017, ARXIV170505546
   Donato G., 2017, P 8 WORKSH COMP APPR, P118
   El Ali Abdallah., 2017, Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems CHI EA'17, P1577
   Gao Y, 2014, IEEE MULTIMEDIA, V21, P52, DOI 10.1109/MMUL.2014.20
   Guthier B, 2017, IEEE SYS MAN CYBERN, P2105, DOI 10.1109/SMC.2017.8122930
   Lee J, 2010, IEEE T MULTIMEDIA, V12, P552, DOI 10.1109/TMM.2010.2051874
   Li X, 2017, LECT NOTES COMPUT SC, V10367, P48, DOI 10.1007/978-3-319-63564-4_4
   Liu J, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS IEEE-ROBIO 2014, P297, DOI 10.1109/ROBIO.2014.7090346
   Lu X, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P770, DOI 10.1145/2971648.2971724
   Aiello LM, 2013, IEEE T MULTIMEDIA, V15, P1268, DOI 10.1109/TMM.2013.2265080
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mikolov Tomas, 2013, Preprints
   Miller H., 2017, P 11 VIS COMM Q INT, P152, DOI DOI 10.1609/ICWSM.V11I1.14901
   Miller H., 2016, INT AAAI C WEB SOC M, P259, DOI [DOI 10.1089/CYBER.2011.0179, https://doi.org/10.1089/cyber.2011.0179]
   Novak PK, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0144296
   Pohl H, 2017, ACM T COMPUT-HUM INT, V24, DOI 10.1145/3039685
   Rathan M, 2018, APPL SOFT COMPUT, V68, P765, DOI 10.1016/j.asoc.2017.07.056
   Riordan MA, 2017, COMPUT HUM BEHAV, V76, P75, DOI 10.1016/j.chb.2017.07.009
   Rodrigues D, 2017, TELEMAT INFORM, V34, P1532, DOI 10.1016/j.tele.2017.07.001
   Shiha Mohammed O., 2017, International Journal of Computer and Electrical Engineering, V9, P360, DOI 10.17706/ijcee.2017.9.1.360-369
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tigwell GW, 2016, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI 2016), P859, DOI 10.1145/2957265.2961844
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Wijeratne S, 2017, 2017 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE (WI 2017), P646, DOI 10.1145/3106426.3106490
   Wijeratne Sanjaya, 2016, Proc Int Workshop Soc Inform, V10046, P527, DOI 10.1007/978-3-319-47880-7_33
   Zhou R, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P748, DOI 10.1145/3025453.3025800
NR 46
TC 24
Z9 26
U1 2
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 402
EP 415
DI 10.1109/TMM.2018.2862363
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400011
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, HM
   Xu, M
AF Zhang, Haimin
   Xu, Min
TI Recognition of Emotions in User-Generated Videos With Kernelized
   Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video emotion recognition; space transformation; kernel method
AB Recognition of emotions in user-generated videos has attracted increasing research attention. Most existing approaches are based on spatial features extracted from video frames. However, due to the broad affective gap between spatial features of images and high-level emotions, the performance of existing approaches is restricted. To bridge the affective gap, we propose recognizing emotions in user-generated videos with kernelized features. We reformulate the equation of the discrete Fourier transform as a linear kernel function and construct a polynomial kernel function based on the linear kernel. The polynomial kernel is applied to spatial features of video frames to generate kernelized features. Compared with spatial features, kernelized features show superior discriminative capability. Moreover, we are the first to apply the sparse representation method to reduce the impact of noise contained in videos; this method helps contribute to performance improvement. Extensive experiments are conducted on two challenging benchmark datasets, that is, VideoEmotion-8 and Ekman-6. The experimental results demonstrate that the proposed method achieves state-of-the-art performance.
C1 [Zhang, Haimin; Xu, Min] Univ Technol Sydney, Fac Engn & IT, Ultimo, NSW 2007, Australia.
   [Zhang, Haimin; Xu, Min] Northwestern Polytech Univ, Sch Microelect & Software, Xian 710129, Shaanxi, Peoples R China.
C3 University of Technology Sydney; Northwestern Polytechnical University
RP Xu, M (corresponding author), Univ Technol Sydney, Fac Engn & IT, Ultimo, NSW 2007, Australia.
EM Haimin.Zhang@student.uts.edu.au; Min.Xu@uts.edu.au
OI Xu, Min/0000-0001-9581-8849; Zhang, Haimin/0000-0002-0021-3634
CR AHARON M., 2006, OVERCOMPLETE DICT SP
   [Anonymous], 1992, Fourier analysis and its applications
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 1986, EMOTION THEORY RES E
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2010, P NIPS
   [Anonymous], IEEE T AFFECTI UNPUB
   [Anonymous], REPRODUCING KERNEL S
   [Anonymous], 2016, Proceedings of the 24th ACM international conference on Multimedia, DOI DOI 10.1145/2964284.2967196
   [Anonymous], 2014, P 31 INT C INT C MAC
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Ekman P., 1982, EMOTION HUMAN FACE
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2014, AAAI CONF ARTIF INTE, P73
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Krause Andreas, 2012, ARXIV12071394
   Li Y, 2016, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2016.215
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Perronnin F., 2007, PROC IEEE C COMPUT V, P1
   Rasheed Z, 2005, IEEE T CIRC SYST VID, V15, P52, DOI 10.1109/TCSVT.2004.839993
   SCHLOSBERG H, 1954, PSYCHOL REV, V61, P81, DOI 10.1037/h0054570
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Scholkopf B., 2002, Encyclopedia of Biostatistics
   Simonyan K., 2014, 14091556 ARXIV
   Solli M, 2009, LECT NOTES COMPUT SC, V5702, P573, DOI 10.1007/978-3-642-03767-2_70
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Xu M, 2013, SIGNAL PROCESS, V93, P2140, DOI 10.1016/j.sigpro.2012.06.026
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhu LC, 2017, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2017.147
NR 45
TC 24
Z9 24
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2824
EP 2835
DI 10.1109/TMM.2018.2808760
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000023
DA 2024-07-18
ER

PT J
AU Yang, JF
   She, DY
   Sun, M
   Cheng, MM
   Rosin, PL
   Wang, L
AF Yang, Jufeng
   She, Dongyu
   Sun, Ming
   Cheng, Ming-Ming
   Rosin, Paul L.
   Wang, Liang
TI Visual Sentiment Prediction Based on Automatic Discovery of Affective
   Regions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective region; convolutional neural networks; sentiment
   classification; visual sentiment analysis
ID IMAGES; CNNS
AB Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions via images and videos online. This paper investigates the problem of visual sentiment analysis, which involves a high-level abstraction in the recognition process. While most of the current methods focus on improving holistic representations, we aim to utilize the local information, which is inspired by the observation that both the whole image and local regions convey significant sentiment information. We propose a framework to leverage affective regions, where we first use an off-the-shelf objectness tool to generate the candidates, and employ a candidate selection method to remove redundant and noisy proposals. Then, a convolutional neural network (CNN) is connected with each candidate to compute the sentiment scores, and the affective regions are automatically discovered, taking the objectness score as well as the sentiment score into consideration. Finally, the CNN outputs from local regions are aggregated with the whole images to produce the final predictions. Our framework only requires image-level labels, thereby significantly reducing the annotation burden otherwise required for training. This is especially important for sentiment analysis since sentiment can be abstract, and labeling affective regions is too subjective and labor-consuming. Extensive experiments show that the proposed algorithm outperforms the state-of-the-art approaches on eight popular benchmark datasets.
C1 [Yang, Jufeng; She, Dongyu; Sun, Ming; Cheng, Ming-Ming] Nankai Univ, Sch Comp Sci & Control Engn, Tianjin 300350, Peoples R China.
   [Rosin, Paul L.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, S Glam, Wales.
   [Wang, Liang] Chinese Acad Sci, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Nankai University; Cardiff University; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Yang, JF (corresponding author), Nankai Univ, Sch Comp Sci & Control Engn, Tianjin 300350, Peoples R China.
EM yangjufeng@nankai.edu.cn; sherry6656@163.com; msunming@foxmail.com;
   cmm@nankai.edu.cn; Paul.Rosin@cs.cf.ac.uk; wangliang@nlpr.ia.ac.cn
RI Cheng, Ming-Ming/A-2527-2009
OI Cheng, Ming-Ming/0000-0001-5550-8758; Rosin, Paul/0000-0002-4965-3884
FU National Natural Science Foundation of China [61620106008, 61572264,
   61633021, 61525306, 61301238, 61201424]; Open Project Program of the
   National Laboratory of Pattern Recognition; Huawei Innovation Research
   Program; CAST YESS Program; IBM Global SUR award
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61620106008, 61572264, 61633021,
   61525306, 61301238, and 61201424; in part by the Open Project Program of
   the National Laboratory of Pattern Recognition; in part by Huawei
   Innovation Research Program; in part by CAST YESS Program; and in part
   by the IBM Global SUR award.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2014, BMVC
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P 2 ACM INT WORKSH I
   [Anonymous], PROC ACM INT CONF MU
   [Anonymous], 2016, 2016 IEEE International Conference on Multimedia and Expo
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, P 1 INT WORKSH AFF S, DOI 10.1145/2813524.2813530
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2014, P BRIT MACH VIS C BM
   [Anonymous], P 26 INT J C ART INT
   [Anonymous], 2014, Comput. Sci.
   [Anonymous], IEEE T AFFECT COMPUT
   [Anonymous], IEEE T CYBERN
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Bo Pang, 2008, Foundations and Trends in Information Retrieval, V2, P1, DOI 10.1561/1500000001
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Chen TH, 2014, PR IEEE COMP DESIGN, P367, DOI 10.1109/ICCD.2014.6974707
   Chen YY, 2015, IEEE T AFFECT COMPUT, V6, P298, DOI 10.1109/TAFFC.2014.2388370
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Hosang Jan, 2016, IEEE Trans Pattern Anal Mach Intell, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Jou B, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P213, DOI 10.1145/2647868.2656408
   Lang P. J., 2005, A6 U FLOR CTR RES PS
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li ZH, 2018, MULTIMED TOOLS APPL, V77, P1115, DOI 10.1007/s11042-016-4310-5
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Nicolaou M.A., 2011, Proceedings of the 19th ACM international conference on Multimedia, P933
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Peng KC, 2016, IEEE IMAGE PROC, P614, DOI 10.1109/ICIP.2016.7532430
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Siersdorfer S., 2010, ACM MM, P715
   Simonyan K., 2014, 14091556 ARXIV
   Solli M, 2009, LECT NOTES COMPUT SC, V5702, P573, DOI 10.1007/978-3-642-03767-2_70
   van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Wei-Ning W, 2006, IEEE SYS MAN CYBERN, P3534, DOI 10.1109/ICSMC.2006.384667
   Wu H, 2015, IEEE SYS MAN CYBERN, P2956, DOI 10.1109/SMC.2015.514
   Yang JF, 2017, AAAI CONF ARTIF INTE, P224
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhao SC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4669
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhao SC, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P879, DOI 10.1145/2733373.2806354
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385, DOI DOI 10.1145/2964284.2964289
   Zhou BL, 2014, ADV NEUR IN, V27
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 74
TC 123
Z9 135
U1 0
U2 52
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2513
EP 2525
DI 10.1109/TMM.2018.2803520
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200022
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Qian, SS
   Zhang, TZ
   Xu, CS
AF Qian, Shengsheng
   Zhang, Tianzhu
   Xu, Changsheng
TI Cross-Domain Collaborative Learning via Discriminative Nonparametric
   Bayesian Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social media; discriminative non-parametric Bayesian model;
   multi-modality
ID K-SVD; DICTIONARY; REPRESENTATION; SEARCH
AB Cross-domain data analysis has been becoming more and more important, and can he effectively adopted for many applications. However, it is difficult to propose a unified cross-domain collaborative learning framework for cross-domain analysis in social multimedia, because cross-domain data have multidomain, multimodal, sparse, and supervised properties. In this paper, we propose a generic cross-domain collaborative learning (CDCL) framework via a discriminative non-parametric Bayesian dictionary learning model for cross-domain data analysis. Compared with existing cross-domain learning methods, our proposed model mainly has four advantages: First, to address the domain discrepancy, we utilize the shared domain priors among multiple domains to make them share a common feature space. Second, to exploit the multimodal property, we use the shared modality priors to model the relationship between different modalities. Third, to deal with the sparse property of media data in one domain, our goal is to learn a shared dictionary to bridge different domains and complement each other. Finally, to make use of the supervised property, we exploit class label information to learn the shared discriminative dictionary, and utilize a latent probability vector to select different dictionary elements for representation of each class. Therefore, the proposed model can investigate the superiorities of different sources to supplement and improve each other effectively. In experiments, we have evaluated our model for two important applications including cross-platform event recognition and cross-network video recommendation. The experimental results have showed the effectiveness of our CDCL model for cross-domain analysis.
C1 [Qian, Shengsheng; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Qian, Shengsheng; Zhang, Tianzhu; Xu, Changsheng] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Qian, SS (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM sheng.qian@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI ARSLAN, Okan/AAA-3232-2020; xu, cj/HJZ-3488-2023; Zhang,
   Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106; xu, chang sheng/0000-0001-8343-9665;
   zhang, tian zhu/0000-0003-1856-9564
FU National Natural Science Foundation of China [61720106006, 61432019,
   61572498, 61532009, 61572296]; Key Research Program of Frontier
   Sciences, CAS [QYZDJ-SSW-JSC039]; Beijing Natural Science Foundation
   [4172062]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61720106006, 61432019, 61572498,
   61532009, and 61572296; in part by the Key Research Program of Frontier
   Sciences, CAS, under Grant QYZDJ-SSW-JSC039; and in part by Beijing
   Natural Science Foundation (4172062). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Marco Bertini.
CR Abel F, 2013, USER MODEL USER-ADAP, V23, P169, DOI 10.1007/s11257-012-9131-2
   Abel F, 2011, LECT NOTES COMPUT SC, V6757, P28, DOI 10.1007/978-3-642-22233-7_3
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Akhtar N, 2016, IEEE T PATTERN ANAL, V38, P2374, DOI 10.1109/TPAMI.2016.2527652
   [Anonymous], 2012, ACM MM'12'
   Bao BK, 2013, IEEE T IMAGE PROCESS, V22, P860, DOI 10.1109/TIP.2012.2219543
   Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Blitzer J., 2006, PROC C EMPIRICAL MET, P120, DOI DOI 10.3115/1610075.1610094
   Cha YC, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P565, DOI 10.1145/2348283.2348360
   Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546
   Chang XJ, 2016, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2016.208
   Chen X, 2012, IEEE T MULTIMEDIA, V14, P3, DOI 10.1109/TMM.2011.2167223
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Li HQ, 2013, IEEE T MULTIMEDIA, V15, P594, DOI 10.1109/TMM.2012.2234730
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu W, 2014, IEEE T MULTIMEDIA, V16, P2242, DOI 10.1109/TMM.2014.2359332
   Paisley J., 2009, P 26 ANN INT C MACH, P777
   Pan SJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1187
   Qian SS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P99, DOI 10.1145/2733373.2806234
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qian SS, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659521
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Reuter T., 2013, P MED EV MULT BENCHM, P18
   Tang Jie, 2012, P 18 ACM SIGKDD INT, P1285, DOI DOI 10.1145/2339530.2339730
   Wang HR, 2012, PATTERN RECOGN, V45, P3902, DOI 10.1016/j.patcog.2012.04.024
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Yan M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P557, DOI 10.1145/2647868.2654920
   Yan M, 2015, IEEE T MULTIMEDIA, V17, P1248, DOI 10.1109/TMM.2015.2446949
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yang XS, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700286
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yang Y, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457456
   Yuan CF, 2013, PROC CVPR IEEE, P423, DOI 10.1109/CVPR.2013.61
   Zhang H, 2017, J CHEM-NY, V2017, DOI 10.1155/2017/4513410
   Zhang TZ, 2013, IEEE I CONF COMP VIS, P281, DOI 10.1109/ICCV.2013.42
   Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0
   Zhang TZ, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2602633
   Zhao HY, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P200, DOI 10.1109/GSIS.2013.6714773
   Zhou M., 2009, NIPS
NR 48
TC 13
Z9 15
U1 3
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2086
EP 2099
DI 10.1109/TMM.2017.2785227
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600014
DA 2024-07-18
ER

PT J
AU Muñoz-Romero, S
   Arenas-García, J
   Gómez-Verdejo, V
AF Munoz-Romero, Sergio
   Arenas-Garcia, Jeronimo
   Gomez-Verdejo, Vanessa
TI Nonnegative OPLS for Supervised Design of Filter Banks: Application to
   Image and Audio Feature Extraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Orthonormalized partial least squares (OPLS); nonnegative solution;
   Gabor filters; filter design; texture classification; music genre
   classification
ID MULTIVARIATE-ANALYSIS FRAMEWORK; PARTIAL LEAST-SQUARES; MATRIX
   FACTORIZATION; ROTATION-INVARIANT; ALGORITHMS; SPARSE; SCALE;
   CLASSIFICATION
AB Audio or visual data analysis tasks usually have to deal with high-dimensional and nonnegative signals. However, most data analysis methods suffer from overfitting and numerical problems when data have more than a few dimensions needing a dimensionality reduction preprocessing. Moreover, interpretability about how and why filters work for audio or visual applications is a desired property, especially when energy or spectral signals are involved. In these cases, due to the nature of these signals, the nonnegativity of the filter weights is a desired property to better understand its working. Because of these two necessities, we propose different methods to reduce the dimensionality of data while the nonnegativity and interpretability of the solution are assured. In particular, we propose a generalized methodology to design filter banks in a supervised way for applications dealing with nonnegative data, and we explore different ways of solving the proposed objective function consisting of a nonnegative version of the orthonormalized partial least-squares method. We analyze the discriminative power of the features obtained with the proposed methods for two different and widely studied applications: texture and music genre classification. Furthermore, we compare the filter banks achieved by our methods with other state-of-the-art methods specifically designed for feature extraction.
C1 [Munoz-Romero, Sergio] Univ Rey Juan Carlos, Dept Signal Theory & Commun, Fuenlabrada 28943, Spain.
   [Munoz-Romero, Sergio] Univ Politecn Madrid, Ctr Computat Simulat, E-28040 Madrid, Spain.
   [Arenas-Garcia, Jeronimo; Gomez-Verdejo, Vanessa] Univ Carlos III Madrid, Dept Signal Theory & Commun, Leganes 28911, Spain.
C3 Universidad Rey Juan Carlos; Universidad Politecnica de Madrid;
   Universidad Carlos III de Madrid
RP Muñoz-Romero, S (corresponding author), Univ Rey Juan Carlos, Dept Signal Theory & Commun, Fuenlabrada 28943, Spain.; Muñoz-Romero, S (corresponding author), Univ Politecn Madrid, Ctr Computat Simulat, E-28040 Madrid, Spain.
EM sergio.munoz@urjc.es; jarenas@tsc.uc3m.es; vanessa@tsc.uc3m.es
RI Gómez-Verdejo, Vanessa/N-4688-2016; Muñoz-Romero, Sergio/Y-6827-2019;
   Arenas-Garcia, Jeronimo/A-6237-2008; GOMEZ VERDEJO,
   VANESSA/KSM-5291-2024
OI Muñoz-Romero, Sergio/0000-0003-1356-2646; Arenas-Garcia,
   Jeronimo/0000-0003-4071-7068; GOMEZ VERDEJO, VANESSA/0000-0001-7702-8747
FU MINECO [TEC2013-48439-C4-1-R, TEC2014-52289-R, TEC2016-75161-C2-1-R,
   TEC2016-75161-C2-2-R, TEC2016-81900-REDT/AEI]; PRICAM [S2013/ICE-2933]
FX This work was supported in parts by the MINECO projects
   TEC2013-48439-C4-1-R, TEC2014-52289-R, TEC2016-75161-C2-1-R,
   TEC2016-75161-C2-2-R, TEC2016-81900-REDT/AEI, and PRICAM
   (S2013/ICE-2933). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Nicu Sebe.
CR Allen GI, 2013, STAT ANAL DATA MIN, V6, P302, DOI 10.1002/sam.11169
   [Anonymous], 2013, 2013 IEEE INT C MULT
   [Anonymous], 2009, KERNEL METHODS REMOT
   [Anonymous], 1980, MULTIVARIATE ANAL
   [Anonymous], 1966, Research papers in statistics
   [Anonymous], 1999002 GATSB COMP N
   [Anonymous], 2003, P 4 INT S IND COMP A
   Arenas-Garcia J., 2007, ADV NEURAL INFORM PR, V19, P33
   Arenas-Garcia J., 2006, P 7 INT C MUS INF RE, P290
   Arenas-García J, 2008, IEEE T GEOSCI REMOTE, V46, P2872, DOI 10.1109/TGRS.2008.918765
   Arenas-García J, 2013, IEEE SIGNAL PROC MAG, V30, P16, DOI 10.1109/MSP.2013.2250591
   Aucouturier JJ, 2005, IEEE T MULTIMEDIA, V7, P1028, DOI 10.1109/TMM.2005.858380
   Barker M, 2003, J CHEMOMETR, V17, P166, DOI 10.1002/cem.785
   Bianconi F, 2007, PATTERN RECOGN, V40, P3325, DOI 10.1016/j.patcog.2007.04.023
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Borga M., 1997, LITHISYR1992 LINK U
   Boutsidis C, 2008, PATTERN RECOGN, V41, P1350, DOI 10.1016/j.patcog.2007.09.010
   Brodatz P., 1966, Textures: a photographic album for artists and designers, V66
   Choi S, 2008, IEEE IJCNN, P1828, DOI 10.1109/IJCNN.2008.4634046
   De la Torre F, 2012, IEEE T PATTERN ANAL, V34, P1041, DOI 10.1109/TPAMI.2011.184
   Dhanjal C, 2009, IEEE T PATTERN ANAL, V31, P1347, DOI 10.1109/TPAMI.2008.171
   FOGEL I, 1989, BIOL CYBERN, V61, P103, DOI 10.1007/BF00204594
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   Gillis N, 2012, NEURAL COMPUT, V24, P1085, DOI 10.1162/NECO_a_00256
   Han J, 2007, IMAGE VISION COMPUT, V25, P1474, DOI 10.1016/j.imavis.2006.12.015
   Hansen LK, 2007, BRAIN LANG, V102, P186, DOI 10.1016/j.bandl.2006.12.004
   Hardoon DR, 2011, MACH LEARN, V83, P331, DOI 10.1007/s10994-010-5222-7
   Hastie T, 2007, ELECTRON J STAT, V1, P1, DOI 10.1214/07-EJS004
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Kim J, 2008, IEEE DATA MINING, P353, DOI 10.1109/ICDM.2008.149
   Kounades-Bastian D, 2016, IEEE-ACM T AUDIO SPE, V24, P1408, DOI 10.1109/TASLP.2016.2554286
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li WT, 2010, I C CONT AUTOMAT ROB, P1193, DOI 10.1109/ICARCV.2010.5707806
   Lin CJ, 2007, IEEE T NEURAL NETWOR, V18, P1589, DOI 10.1109/TNN.2007.895831
   Mackey L., 2008, P ADV NEUR INF PROC, P1
   Mandel MI, 2006, MULTIMEDIA SYST, V12, P3, DOI 10.1007/s00530-006-0032-2
   McKinney M.F., 2003, Proc. ISMIR, V3, P151
   Meng A., 2005, Proceedings of the International Conference on Music Information Retrieval, P604
   Meng A, 2007, IEEE T AUDIO SPEECH, V15, P1654, DOI 10.1109/TASL.2007.899293
   Muñoz-Romero S, 2016, IEEE COMPUT INTELL M, V11, P24, DOI 10.1109/MCI.2016.2601701
   Muñoz-Romero S, 2015, PATTERN RECOGN, V48, P1797, DOI 10.1016/j.patcog.2014.12.002
   Muñoz-Romero S, 2013, INT CONF ACOUST SPEE, P3387, DOI 10.1109/ICASSP.2013.6638286
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pauca VP, 2006, LINEAR ALGEBRA APPL, V416, P29, DOI 10.1016/j.laa.2005.06.025
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Reinsel G.C., 1998, MULTIVARIATE REDUCED, V136, DOI DOI 10.1007/978-1-4757-2853-8
   Scaringella N, 2006, IEEE SIGNAL PROC MAG, V23, P133, DOI 10.1109/MSP.2006.1598089
   SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Sigg Christian, 2007, 2007 IEEE Workshop on Machine Learning for Signal Processing, P253, DOI 10.1109/MLSP.2007.4414315
   Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860
   Sun LA, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1230
   TURNER MR, 1986, BIOL CYBERN, V55, P71
   Van Benthem MH, 2004, J CHEMOMETR, V18, P441, DOI 10.1002/cem.889
   van Gerven MAJ, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/2/026017
   Worsley KJ, 1997, NEUROIMAGE, V6, P305, DOI 10.1006/nimg.1997.0294
   Yuan ZJ, 2005, LECT NOTES COMPUT SC, V3540, P333
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 59
TC 5
Z9 5
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1751
EP 1766
DI 10.1109/TMM.2017.2778568
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100013
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Hanh, NQ
   Reju, VG
   Khong, AWH
AF Nguyen Quang Hanh
   Reju, Vaninirappuputhenpurayil Gopalan
   Khong, Andy W. H.
TI Impact Localization on Rigid Surfaces Using Hermitian Angle Distribution
   for Human-Computer Interface Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human-computer interface; tangible interface; source localization on
   solids; TOA estimation
ID ACOUSTIC-EMISSION; SOURCE LOCATION; IDENTIFICATION; TRANSFORM
AB We propose an algorithm to localize impacts on rigid surfaces using induced vibration signals. This allows for the conversion of daily objects, such as tabletops and glass panels, into human-computer touch interfaces using low-cost piezoelectric sensors. Impact localization is achieved by estimating the time-of-arrivals and subsequently time-difference-of-arrivals of the sensor-received signals. Time-of-arrival estimation is highly challenging with increasing source-sensor distance due to the occurrence of a gradual noise-to-signal transition at the sensor output. We address this problem by first converting the signal into Hermitian angle distributions. The time-varying probability contributions of the background noise and vibration signal in each of the distributions are subsequently monitored to identify the instant when the signal begins to dominate the noise, signifying the signals arrival. The proposed framework also allows simultaneous time-of-arrival estimation across all the sensors to minimize errors in the resultant time-difference-of-arrival estimates. Experimental results show that the proposed algorithm outperforms existing techniques for source localization on solid surfaces of different materials.
C1 [Nguyen Quang Hanh; Reju, Vaninirappuputhenpurayil Gopalan; Khong, Andy W. H.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Khong, AWH (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM quanghan001@e.ntu.edu.sg; reju@ntu.edu.sg; AndyKhong@ntu.edu.sg
RI Reju, Vaninirappuputhenpurayil Gopalan/C-3100-2017; Khong,
   Andy/A-5169-2011
OI Reju, Vaninirappuputhenpurayil Gopalan/0000-0002-6234-965X; Khong,
   Andy/0000-0002-0708-4791
CR [Anonymous], 2011, P INT C MECH ENG 201
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2012, An introduction to statistical concepts
   [Anonymous], P 2013 IEEE INT C MU
   Arun KR, 2011, IEEE T MULTIMEDIA, V13, P487, DOI 10.1109/TMM.2011.2123084
   Bian ZP, 2016, IEEE J BIOMED HEALTH, V20, P915, DOI 10.1109/JBHI.2015.2412125
   Chan YT, 2010, IEEE T SIGNAL PROCES, V58, P1433, DOI 10.1109/TSP.2009.2035987
   CHAN YT, 1994, IEEE T SIGNAL PROCES, V42, P1905, DOI 10.1109/78.301830
   Chen Ke-Yu., 2013, Proceedings of the 2013 CHI Conference on Human Factors in Computing Systems. CHI 2013, P2581
   Ciampa F, 2010, COMPOS PART A-APPL S, V41, P1777, DOI 10.1016/j.compositesa.2010.08.013
   Crevoisier A., 2005, P 2005 INT C NEW INT, P97
   Foroozan F, 2011, IEEE T SIGNAL PROCES, V59, P2655, DOI 10.1109/TSP.2011.2128317
   Gaul L, 1998, MECH SYST SIGNAL PR, V12, P783, DOI 10.1006/mssp.1998.0163
   Hanh N.Q., 2015, P 10 INT C INF COMM, P1
   Ing RK, 2005, APPL PHYS LETT, V87, DOI 10.1063/1.2130720
   Kaphle M, 2012, STRUCT CONTROL HLTH, V19, P187, DOI 10.1002/stc.413
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Kosel T, 2003, AIRCR ENG AEROSP TEC, V75, P11, DOI 10.1108/00022660310457248
   Launius R., 2005, Arkham Horror
   Lee Y.-T., 2010, P INT C CONS EL JAN, P475
   Lui KWK, 2009, DIGIT SIGNAL PROCESS, V19, P650, DOI 10.1016/j.dsp.2009.01.002
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Miedl F, 2016, IEEE-ASME T MECH, V21, P787, DOI 10.1109/TMECH.2015.2466455
   Minotto VP, 2015, IEEE T MULTIMEDIA, V17, P1694, DOI 10.1109/TMM.2015.2463722
   Hanh NQ, 2016, INT CONF ACOUST SPEE, P2867, DOI 10.1109/ICASSP.2016.7472201
   Peck R., 2016, INTRO STAT DATA ANAL, V5th
   Pham D.T., 2005, Proceedings of IPROMS 2005 Virtual Conference, P497
   Poletkin K, 2010, IEEE INT CON MULTI, P286, DOI 10.1109/ICME.2010.5582570
   Reju VG, 2013, IEEE T MULTIMEDIA, V15, P1365, DOI 10.1109/TMM.2013.2264656
   Rong Chang, 2010, Proceedings of the 2010 Asia-Pacific Conference on Wearable Computing Systems (APWC 2010), P363, DOI 10.1109/APWCS.2010.99
   Soon Nyean C., 2010, SCI SOCIAL RES CSSR, P680
   Su Z., IDENTIFICATION DAMAG
   Varshney S, 2013, INT CONF CONTEMP, P219, DOI 10.1109/IC3.2013.6612194
   Viktrov I., 1967, Rayleigh and Lamb Waves: Physical Theory and Applications
   Walker G., 2014, FUNDAMENTALS TOUCH T
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang Q, 2001, J SOUND VIB, V248, P91, DOI 10.1006/jsvi.2001.3676
   Wu GL, 2016, IEEE T MULTIMEDIA, V18, P978, DOI 10.1109/TMM.2016.2545401
   Yoshida S, 2016, IEEE COMPUT GRAPH, V36, P62, DOI 10.1109/MCG.2015.1
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   ZIOLA SM, 1991, J ACOUST SOC AM, V90, P2551, DOI 10.1121/1.402348
NR 41
TC 3
Z9 4
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1448
EP 1461
DI 10.1109/TMM.2017.2772441
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400013
DA 2024-07-18
ER

PT J
AU Lee, WY
   Hsu, WH
   Satoh, S
AF Lee, Wen-Yu
   Hsu, Winston H.
   Satoh, Shin'ichi
TI Learning From Cross-Domain Media Streams for Event-of-Interest Discovery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-domain media mining; event discovery; graph-based data fusion;
   social network analysis
AB Every clay, vast amounts of data are uploaded to various social-sharing websites. Each social-sharing website has its own media dataset. Recently, mining media datasets has shown great potential for our daily lives, e.g., earthquake detection. Generally, different datasets have different characteristics. Combining different datasets is capable of achieving better performance than using any dataset independently, particularly if the datasets can compensate for each other. The resulting performance, however, depends on the fusion method. Effectively combining different datasets is challenging. As a solution to this challenge, this paper presents a generic two-stage framework for events of interest. Specifically, the first stage normalizes the contents of different datasets to make them comparable; then, the second stage combines the normalized contents for a ranked event list using graph-based algorithms. Practically, this paper unifies a low-based media dataset and a check-in-based media dataset. Based on the precision for the top n events, the experimental results demonstrate that the proposed framework can achieve better performance in finding events associated with sports, local festivals, concerts, and exhibitions compared with a state-of-the-art approach that uses one dataset alone.
C1 [Lee, Wen-Yu] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
   [Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
   [Satoh, Shin'ichi] Natl Inst Informat, Tokyo 1018430, Japan.
C3 National Taiwan University; National Taiwan University; Research
   Organization of Information & Systems (ROIS); National Institute of
   Informatics (NII) - Japan
RP Lee, WY (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
EM major-rei@cmlab.csie.ntu.edu.tw; winston@csie.ntu.edu.tw;
   satoh@nii.ac.jp
OI HSU, WINSTON/0000-0002-3330-0638
FU Ministry of Science and Technology, Taiwan [MOST 103-2221-E-002-105-MY3,
   MOST 105-2917-I-564-054]; Microsoft Research Asia
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grant MOST 103-2221-E-002-105-MY3 and Grant
   MOST 105-2917-I-564-054 and sponsored by Microsoft Research Asia. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Marco Bertini.
CR Al Bawab Ziad., 2012, Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12, P397
   [Anonymous], 2015, P INT AAAI C WEB SOC
   [Anonymous], 2009, P 17 ACM SIGSPATIAL, DOI DOI 10.1145/1653771.1653781
   [Anonymous], 2016, P 2016 ACM WORKSH MU
   Becker Hila., 2012, Proceedings of the fifth ACM international conference on Web search and data mining, P533, DOI [DOI 10.1145/2124295.2124360, 10.1145/2124295.212436017, DOI 10.1145/2124295.212436017]
   Chang XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2234
   Dao MS, 2014, MULTIMED TOOLS APPL, V70, P25, DOI 10.1007/s11042-012-1153-6
   FUJISAKA T, 2010, P 4 INT C UB INF MAN, P246
   Ho KH, 2015, IEEE T COMPUT AID D, V34, P161, DOI 10.1109/TCAD.2014.2379656
   Kuo YH, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P201, DOI 10.1145/2647868.2656406
   Lee WY, 2016, J VIS COMMUN IMAGE R, V41, P200, DOI 10.1016/j.jvcir.2016.09.017
   Long JY, 2008, ISPD'08: PROCEEDINGS OF THE 2008 ACM INTERNATIONAL SYMPOSIUM ON PHYSICAL DESIGN, P126
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Aiello LM, 2013, IEEE T MULTIMEDIA, V15, P1268, DOI 10.1109/TMM.2013.2265080
   Mazloom M, 2016, IEEE T MULTIMEDIA, V18, P1378, DOI 10.1109/TMM.2016.2559947
   Ruocco M, 2014, MULTIMED TOOLS APPL, V70, P55, DOI 10.1007/s11042-012-1087-z
   Sakaki T., 2010, P 19 INT C WORLD WID, P851
   Shah RR, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P185, DOI 10.1145/2733373.2809932
   Shah RR, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P607, DOI 10.1145/2647868.2654919
   Shao-Lun Huang, 2011, 2011 16th Asia and South Pacific Design Automation Conference, ASP-DAC 2011, P382, DOI 10.1109/ASPDAC.2011.5722218
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Weng J., 2011, HPL201198
   Wu CC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P727, DOI 10.1145/2600428.2609569
   Yan Y, 2015, AAAI CONF ARTIF INTE, P3841
   YAO ACC, 1982, SIAM J COMPUT, V11, P721, DOI 10.1137/0211059
   Zhang WS, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2700478
   Zhou H, 2002, INFORM PROCESS LETT, V81, P271, DOI 10.1016/S0020-0190(01)00232-0
NR 27
TC 9
Z9 10
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 142
EP 154
DI 10.1109/TMM.2017.2726184
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700012
DA 2024-07-18
ER

PT J
AU Tan, B
   Wu, J
   Li, Y
   Cui, H
   Yu, W
   Chen, CW
AF Tan, Bin
   Wu, Jun
   Li, Ying
   Cui, Hao
   Yu, Wei
   Chen, Chang Wen
TI Analog Coded SoftCast: A Network Slice Design for Multimedia
   Broadcast/Multicast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Analog coding; network slice; rateless code; SoftCast
ID SEAMLESS RATE ADAPTATION; TRANSMISSION
AB This paper presents a network slice design for ultra high definition (UHD) video broadcast/multicast to achieve higher network efficiency and improved quality of experience (QoE). The proposed network slice design consists of a rateless source compression scheme and an analog-coded SoftCast scheme. The rateless Spinal code is adopted to compress the video source at content server and the compressed source is transmitted from content server across wireless core network to the base station. An a priori information-assisted Spinal decoder is designed to utilize the sparsity of bit planes for compression. In the analog-coded SoftCast scheme, we design a new chaotic function-based analog code with negligible power penalty for the generalized Gaussian-distributed source in SoftCast because the existing chaotic functions designed for uniformly distributed sources suffer from serious power penalty in SoftCast. We also design a maximum a posteriori probability decoding algorithm for the proposed analog code in order to exploit the statistics of video source as a priori information to improve the performance. The experimental results show that the proposed rateless code-based compression scheme achieves efficient compression and approaches the bound of binary erasure channel. In particular, the 1/2 analog-coded SoftCast has almost 2 dB gain over conventional SoftCast with two repetitions, and the 1/3 analog-coded SoftCast has almost 3 dB gain over conventional SoftCast with three repetitions. The system simulations for the broadcast system show higher network capacity and improved QoE in the proposed UHD slice, because the reconstructed video quality of each user is commensurate with its channel condition.
C1 [Tan, Bin] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Tan, Bin] Jinggangshan Univ, Coll Elect & Informat Engn, Jian 343009, Jiangxi, Peoples R China.
   [Wu, Jun] Tongji Univ, Coll Elect & Informat Engn, Key Lab, Minist Educ Embedded Syst, Shanghai 201804, Peoples R China.
   [Wu, Jun] Tongji Univ, Coll Elect & Informat Engn, Serv Comp, Shanghai 201804, Peoples R China.
   [Li, Ying; Cui, Hao; Yu, Wei] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14228 USA.
C3 Tongji University; Jinggangshan University; Tongji University; Tongji
   University; Tongji University; State University of New York (SUNY)
   System; State University of New York (SUNY) Buffalo
RP Wu, J (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Key Lab, Minist Educ Embedded Syst, Shanghai 201804, Peoples R China.; Wu, J (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Serv Comp, Shanghai 201804, Peoples R China.
EM 1310499@tongji.edu.cn; wujun@tongji.edu.cn; 1610477@tongji.edu.cn;
   hao.cui@live.com; 2014yuwei@tongji.edu.cn; chencw@buffalo.edu
RI Li, Ying/KDP-2725-2024
OI Chen, Chang Wen/0000-0002-6720-234X
FU National Science Foundation China [91538203, 61631017, 61390513,
   61502341]; Foundation of Jiangxi Educational Committee [GJJ160757]
FX This work was supported in part by the National Science Foundation China
   under Grant 91538203, Grant 61631017, Grant 61390513, and Grant
   61502341, and in part by the Foundation of Jiangxi Educational Committee
   GJJ160757. The guest editor coordinating the review of this manuscript
   and approving it for publication was Dr. Shiwen Mao. (Corresponding
   author: Jun Wu.)
CR Aditya Siripuram., 2011, ACM MOBICOM, P277
   Almowuena S, 2016, IEEE T MULTIMEDIA, V18, P102, DOI 10.1109/TMM.2015.2502067
   Anjum MDN, 2016, DIGIT COMMUN NETW, V2, P130, DOI 10.1016/j.dcan.2016.06.004
   [Anonymous], 2011, P 4 CJK INT WORKSH T, P1
   [Anonymous], 2012, Network Functions Virtualisation - Introductory White Paper
   [Anonymous], IBM J RES DEV
   [Anonymous], 2011, INT HET NETW SOL BRI
   Baldemair R, 2013, IEEE VEH TECHNOL MAG, V8, P24, DOI 10.1109/MVT.2012.2234051
   C. M. R. Institute, 2011, CISC VIS NETW IND GL
   Chen B, 1998, IEEE T COMMUN, V46, P881, DOI 10.1109/26.701312
   Cui H, 2013, IEEE T WIREL COMMUN, V12, P4892, DOI 10.1109/TWC.2013.090413.121308
   Cui H, 2011, ACM S MODEL ANAL SIM, P437
   D. Standards, 2010, A111 DVB
   Gamal A. E., 2012, Network Information Theory
   Gastpar M, 2003, IEEE T INFORM THEORY, V49, P1147, DOI 10.1109/TIT.2003.810631
   GOBLICK TJ, 1965, IEEE T INFORM THEORY, V11, P558, DOI 10.1109/TIT.1965.1053821
   Gong M.X., 2010, GLOBECOM, P1
   Gong MX, 2005, IEEE ICC, P3401
   Guan T. K. H., 2010, DISCOVERY CLOUD RAN
   Gudipati A, 2011, ACM SIGCOMM COMP COM, V41, P158, DOI 10.1145/2043164.2018455
   He DL, 2017, IEEE T MULTIMEDIA, V19, P1894, DOI 10.1109/TMM.2017.2686703
   Hu DL, 2010, IEEE T WIREL COMMUN, V9, P3501, DOI 10.1109/TWC.2010.092810.100098
   Jakubczak S., 2009, TECH REP
   Jakubczak S., 2011, PROC MOBICOM, P289
   Jin YC, 2016, IEEE T MULTIMEDIA, V18, P807, DOI 10.1109/TMM.2016.2537199
   Jing Li, 2012, IEEE International Conference on Communications (ICC 2012), P3769, DOI 10.1109/ICC.2012.6364541
   Lecompte D, 2012, IEEE COMMUN MAG, V50, P68, DOI 10.1109/MCOM.2012.6353684
   LEE KH, 1976, IEEE T COMMUN, V24, P1283
   Liu Y., 2011, J BEIJING U POSTS TE, V13, P1
   Liu Y, 2015, EURASIP J ADV SIG PR, DOI 10.1186/s13634-015-0243-9
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Marshall T. G. J.  Jr., 1984, IEEE Journal on Selected Areas in Communications, VSAC-2, P381, DOI 10.1109/JSAC.1984.1146063
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Mitzenmacher M., 2005, PROBABILITY COMPUTIN, P321
   PAPADOPOULOS HC, 1995, IEEE T INFORM THEORY, V41, P312, DOI 10.1109/18.370091
   Peng MG, 2016, IEEE T MULTIMEDIA, V18, P879, DOI 10.1109/TMM.2016.2535722
   Perry J., 2011, P 10 ACM WORKSH HOT
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Segel J., 2011, 3 ALC LUC BOUL BILL
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Wang SW, 2015, DIGIT COMMUN NETW, V1, P161, DOI 10.1016/j.dcan.2015.09.004
   WOLF JK, 1983, IEEE T COMMUN, V31, P458, DOI 10.1109/TCOM.1983.1095820
   Wu J, 2016, IEEE T MULTIMEDIA, V18, P893, DOI 10.1109/TMM.2016.2535727
   Xie K, 2009, IEEE INT SYMP INFO, P894, DOI 10.1109/ISIT.2009.5205624
   Zhao ZY, 2015, DIGIT COMMUN NETW, V1, P57, DOI 10.1016/j.dcan.2015.02.005
NR 46
TC 24
Z9 24
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2293
EP 2306
DI 10.1109/TMM.2017.2733303
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600013
DA 2024-07-18
ER

PT J
AU Lei, JJ
   Wu, M
   Zhang, CQ
   Wu, F
   Ling, N
   Hou, CP
AF Lei, Jianjun
   Wu, Min
   Zhang, Changqing
   Wu, Feng
   Ling, Nam
   Hou, Chunping
TI Depth-Preserving Stereo Image Retargeting Based on Pixel Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereo image retargeting; pixel fusion; 3D image; depth; disparity;
   matching map
ID VISUAL-ATTENTION; QUALITY; COHERENCE
AB In this paper, we propose a pixel fusion-based stereo image retargeting method, which could adaptively retarget stereo images with flexible aspect ratios, simultaneously preserving the depth. Retargeting each image independently by the pixel fusion method ignores the disparity relationship between pixels in the image pair and hence will introduce the distortion of disparity. To address this issue, we advocate to extend the single pixel fusion-based way to be applicable for stereo image pair. First, seams are selected based on the energy function, which simultaneously considers the seam selecting and seam matching. Second, a seam-matching-based matching map is proposed to preserve the disparity relationship between image pair. Then, the scaling factors for the left image are assigned considering both the important object and depth preservation. Subsequently, the scaling factors for the right image are obtained according to the proposed matching map. Based on these scaling factors, the stereo image pair is retargeted with pixel fusion. In contrast to removing pixels to resize image, the way of pixel fusion can obtain more smooth results with less depth distortion. Experimental results demonstrate that our method achieves more preferable qualities in both depth and shape preservation for stereo image retargeting.
C1 [Lei, Jianjun; Wu, Min; Hou, Chunping] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Zhang, Changqing] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Ling, Nam] Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
C3 Tianjin University; Tianjin University; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; Santa Clara University
RP Lei, JJ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jjlei@tju.edu.cn; wumin_tju@tju.edu.cn; zhangchangqing@tju.edu.cn;
   fengwu@ustc.edu.cn; nling@scu.edu; hcp@tju.edu.cn
RI Lei, Jianjun/P-2539-2018; Wu, Min/HNI-5320-2023; Wu, Feng/KCY-3017-2024;
   Zhang, Chang/HTO-2939-2023
OI Wu, Min/0000-0002-3956-0107; 
FU Natural Science Foundation of China [61271324, 61520106002, 61602337,
   61471262]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61271324, Grant 61520106002, Grant 61602337, and Grant
   61471262. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Qi Tian.
CR [Anonymous], 2010, 2010 3DTV C TRUE VIS
   [Anonymous], IEEE T IMAGE PROCESS
   [Anonymous], 2015, P IEEE INT C MULT EX, DOI DOI 10.1109/ICME.2015.7177529
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Chang CH, 2012, PROC CVPR IEEE, P1075, DOI 10.1109/CVPR.2012.6247786
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Dekel T, 2013, IEEE T PATTERN ANAL, V35, P2513, DOI 10.1109/TPAMI.2013.46
   Fang YM, 2016, INFORM SCIENCES, V372, P347, DOI 10.1016/j.ins.2016.08.062
   Grundmann M, 2010, PROC CVPR IEEE, P569, DOI 10.1109/CVPR.2010.5540165
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Junle Wang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P669, DOI 10.1109/ICASSP.2014.6853680
   Kim H, 2015, IEEE T MULTIMEDIA, V17, P2198, DOI 10.1109/TMM.2015.2493367
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Lee KY, 2012, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2012.6247657
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   [李保松 Li Baosong], 2011, [高分子通报, Polymer Bulletin], P1
   Li B, 2011, LECT NOTES COMPUT SC, V6524, P12
   Lin SS, 2014, IEEE T CIRC SYST VID, V24, P759, DOI 10.1109/TCSVT.2013.2291282
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Mansfield A, 2010, LECT NOTES COMPUT SC, V6311, P143, DOI 10.1007/978-3-642-15549-9_11
   Niu N, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0047362
   Niu YZ, 2012, IEEE T MULTIMEDIA, V14, P783, DOI 10.1109/TMM.2012.2186122
   Pang YW, 2016, IEEE T CYBERNETICS, V46, P2220, DOI 10.1109/TCYB.2015.2472478
   Pang YW, 2014, IEEE T NEUR NET LEAR, V25, P2191, DOI 10.1109/TNNLS.2014.2306844
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Son JY, 2010, J DISP TECHNOL, V6, P394, DOI 10.1109/JDT.2010.2045636
   Wang C, 2008, PROC SPIE, V6803, DOI 10.1117/12.767702
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Yan B, 2015, BMC CANCER, V15, DOI 10.1186/s12885-015-1394-7
   Yen TC, 2011, IEEE T IMAGE PROCESS, V20, P2339, DOI 10.1109/TIP.2011.2114357
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
NR 40
TC 39
Z9 41
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1442
EP 1453
DI 10.1109/TMM.2017.2660440
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800004
DA 2024-07-18
ER

PT J
AU Li, Y
   Yang, GB
   Zhu, YP
   Ding, XL
   Sun, XM
AF Li, Yue
   Yang, Gaobo
   Zhu, Yapei
   Ding, Xiangling
   Sun, Xingming
TI Unimodal Stopping Model-Based Early SKIP Mode Decision for
   High-Efficiency Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficiency video coding (HEVC); mode decision; SKIP mode; unimodal
   stopping model (USM)
ID MOTION ESTIMATION; HEVC ENCODER; PREDICTION; OPTIMIZATION; ALGORITHM;
   SELECTION
AB High-efficiency video coding (HEVC) can greatly improve coding efficiency compared with the prior video coding standard H.264/AVC by adopting advanced hierarchical coding structures such as coding unit (CU), prediction unit (PU), and transform unit. For each CU, an exhaustive mode decision strategy is adopted to achieve the best rate distortion (RD) cost, which simultaneously results in enormous computational complexity. In this paper, an early SKIP mode decision algorithm is proposed for the HEVC encoder to speed up the process of mode decision. Each CU size is categorized into either rare used or frequent used by exploiting the correlation of CU depth, which is estimated from the temporally colocated CUs. For the rare-used CU size, the SKIP mode is directly selected as the optimal mode and the remaining mode decision process is early terminated. For the frequent-used CU size, a unimodal stopping model is designed for its early SKIP mode decision by exploiting both hierarchical mode structure and RD cost property. Experimental results show that the proposed early SKIP mode decision method achieves average 58.5% and 54.8% encoding time savings, while the Bjontegaard Delta bit rate only increases average 0.8% and 0.8% for various test sequences under the random access and the low delay B conditions, respectively.
C1 [Li, Yue; Yang, Gaobo; Ding, Xiangling] Hunan Univ, Sch Informat Sci & Engn, Changsha 410082, Hunan, Peoples R China.
   [Zhu, Yapei] Hengyang Normal Univ, Fac Phys & Elect Informat Sci, Hengyang 421002, Peoples R China.
   [Sun, Xingming] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
C3 Hunan University; Hengyang Normal University; Nanjing University of
   Information Science & Technology
RP Li, Y (corresponding author), Hunan Univ, Sch Informat Sci & Engn, Changsha 410082, Hunan, Peoples R China.
EM yueli@hnu.edu.cn; yanggaobo@hnu.edu.cn; zyp1016@hynu.edu.cn;
   xianglingding@163.com; sunnudt@163.com
RI xiangling, Ding/T-7175-2019; Sun, Xingming/AAD-1866-2019
OI ding, xiangling/0000-0002-6581-4633; Yang, Gaobo/0000-0003-2734-659X
FU National Natural Science Foundation of China [61572183, 61379143,
   61672222]; Specialized Research Fund for the Doctoral Program of Higher
   Education [20120161110014]; Priority Academic Program Development of
   Jiangsu Higer Education Institutions; Jiangsu Collaborative Innovation
   Center on Atmospheric Environment and Equipment Technology
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61572183, Grant 61379143, and Grant
   61672222, in part by the Specialized Research Fund for the Doctoral
   Program of Higher Education under Grant 20120161110014, in part by the
   Priority Academic Program Development of Jiangsu Higer Education
   Institutions, and in part by Jiangsu Collaborative Innovation Center on
   Atmospheric Environment and Equipment Technology. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ivan V. Bajic.
CR Ahn S, 2015, IEEE T CIRC SYST VID, V25, P422, DOI 10.1109/TCSVT.2014.2360031
   [Anonymous], [No title captured]
   [Anonymous], 2013, Technical Report JCTVC-L1100
   [Anonymous], 2001, SC16Q6 ITUT
   Choi K., 2011, document JCTVC-F092 of JCT-VC
   Ferguson T.S., 2012, Optimal stopping and applications
   Gweon R., 2011, JCTVCF045
   Hu Q, 2016, IEEE T MULTIMEDIA, V18, P379, DOI 10.1109/TMM.2015.2512799
   Jung SH, 2016, IEEE T CIRC SYST VID, V26, P1846, DOI 10.1109/TCSVT.2015.2473303
   Kim BG, 2008, IEEE T CIRC SYST VID, V18, P273, DOI 10.1109/TCSVT.2008.918121
   Lee H, 2015, IEEE T BROADCAST, V61, P388, DOI 10.1109/TBC.2015.2419172
   Lee J, 2015, IEEE T CIRC SYST VID, V25, P411, DOI 10.1109/TCSVT.2014.2339612
   Li Y, 2016, J REAL-TIME IMAGE PR, V12, P575, DOI 10.1007/s11554-015-0527-1
   Li Y, 2016, IEEE T BROADCAST, V62, P700, DOI 10.1109/TBC.2016.2570018
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Pan ZQ, 2016, J VIS COMMUN IMAGE R, V40, P516, DOI 10.1016/j.jvcir.2016.07.018
   Pan ZQ, 2016, IEEE T BROADCAST, V62, P675, DOI 10.1109/TBC.2016.2580920
   Pan ZQ, 2016, IET IMAGE PROCESS, V10, P9, DOI 10.1049/iet-ipr.2014.1018
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Pan ZQ, 2014, IEEE T BROADCAST, V60, P405, DOI 10.1109/TBC.2014.2321682
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen LQ, 2011, IEEE T CIRC SYST VID, V21, P837, DOI 10.1109/TCSVT.2011.2130310
   Shen LQ, 2010, SIGNAL PROCESS-IMAGE, V25, P88, DOI 10.1016/j.image.2009.11.003
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sung YH, 2012, IEEE T MULTIMEDIA, V14, P693, DOI 10.1109/TMM.2012.2186793
   Vanne J, 2014, IEEE T CIRC SYST VID, V24, P1579, DOI 10.1109/TCSVT.2014.2308453
   Wang FS, 2013, SIGNAL PROCESS-IMAGE, V28, P736, DOI 10.1016/j.image.2013.05.003
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Yang J., 2011, JCTVCG543
   Yeh CH, 2014, IEEE T IND INFORM, V10, P594, DOI 10.1109/TII.2013.2273308
   Zeng HQ, 2014, IEEE T CIRC SYST VID, V24, P1566, DOI 10.1109/TCSVT.2014.2310143
   Zhang Y, 2013, IEEE T BROADCAST, V59, P390, DOI 10.1109/TBC.2013.2253033
   Zhao TS, 2013, IEEE J-STSP, V7, P1135, DOI 10.1109/JSTSP.2013.2271421
   Zhao TS, 2012, IEEE T IMAGE PROCESS, V21, P2607, DOI 10.1109/TIP.2012.2186148
   Zhao WJ, 2015, IEEE T CIRC SYST VID, V25, P1651, DOI 10.1109/TCSVT.2015.2395751
   Zhu C, 2002, IEEE T CIRC SYST VID, V12, P349, DOI 10.1109/TCSVT.2002.1003474
NR 39
TC 31
Z9 32
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1431
EP 1441
DI 10.1109/TMM.2017.2669863
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800003
DA 2024-07-18
ER

PT J
AU Kyaw, Z
   Qi, SH
   Gao, K
   Zhang, HW
   Zhang, LM
   Xiao, J
   Wang, X
   Chua, TS
AF Kyaw, Zawlin
   Qi, Shuhan
   Gao, Ke
   Zhang, Hanwang
   Zhang, Luming
   Xiao, Jun
   Wang, Xuan
   Chua, Tat-Seng
TI Matryoshka Peek: Toward Learning Fine-Grained, Robust, Discriminative
   Features for Product Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; image representation; robust learning; image
   retrieval
ID RETRIEVAL; IMAGES
AB In sharp contrast to the traditional category/subcategory level image retrieval, product image search aims to find the images containing the exact same product. This is a challenging problem because in addition to being robust under different imaging conditions such as varying viewpoints and illumination changes, the features should also be able to distinguish the specific product among many similar products. Consequently, it is important to utilize a large dataset, containing many product classes, to learn a strongly discriminative representation. Building such a dataset requires laborious manual annotation. Toward learning fine-grained, robust, discriminative features for product image search, we present a novel paradigm that can construct the required dataset without any human annotation. Unlike other fine-grained recognition works that rely on high-quality annotated datasets and are very narrowly focused on a specific object category, our method handles multiple object classes and requires minimum human effort. First, an ImageNet pretrained model is used to generate product clusters. As the original features from ImageNet are not discriminative, the clusters generated by this unsupervised procedure contain much noise. We alleviate noise by explicitly modeling noise distribution and automatically detecting errors during learning. The proposed paradigm is general, requires minimum human efforts, and is applicable to any deep learning task where fine-grained discriminative features are desired. Extensive experiments on the ALISC dataset have demonstrated that our approach is sound and effective, surpassing the baseline GoogleNet model by 15.09%.
C1 [Kyaw, Zawlin; Zhang, Hanwang; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Qi, Shuhan] Harbin Inst Technol, ShenZhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Gao, Ke] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Zhang, Luming] Hefei Univ Technol, Hefei 132312, Peoples R China.
   [Xiao, Jun] Zhejiang Univ, Hangzhou 132312, Zhejiang, Peoples R China.
   [Wang, Xuan] Harbin Inst Technol, Comp Applicat Res Ctr, ShenZhen Grad Sch, Shenzhen 518055, Peoples R China.
C3 National University of Singapore; Harbin Institute of Technology;
   Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Hefei University of Technology; Zhejiang University; Harbin Institute of
   Technology
RP Qi, SH (corresponding author), Harbin Inst Technol, ShenZhen Grad Sch, Shenzhen 518055, Peoples R China.
EM dcskzl@nus.edu.sg; shuhanqi@cs.hitsz.edu.cn; kegao@ict.ac.cn;
   hanwangzhang@gmail.com; zglumg@gmail.com; junx@cs.zju.edu.cn;
   wangxuan@cs.hitsz.edu.cn; chuats@comp.nus.edu.sg
RI Lei, Ming/JAD-1050-2023; wang, xuan/GXF-3679-2022; zhang,
   lu/GRO-2969-2022; wang, xuan/JBJ-6948-2023
OI Zhang, Hanwang/0000-0001-7374-8739; shuhan, qi/0000-0002-6903-145X
FU National Research Foundation, Prime Minister's Office, Singapore, under
   its IRC@SG Funding Initiative; National Nature Science Foundation of
   China [61525206, 61271428, 61572169, 61472266]; International Exchange
   and Cooperation Foundation of Shenzhen City [GJHZ20150312114149569];
   National University of Singapore (Suzhou) Research Institute;
   Fundamental Research Funds for the Central Universities
FX This work was supported by the National Research Foundation, Prime
   Minister's Office, Singapore, under its IRC@SG Funding Initiative, in
   part by the National Nature Science Foundation of China under Grant
   61525206, Grant 61271428, Grant 61572169, and Grant 61472266, in part by
   the International Exchange and Cooperation Foundation of Shenzhen City
   under Grant GJHZ20150312114149569, in part by the National University of
   Singapore (Suzhou) Research Institute, and in part by the Fundamental
   Research Funds for the Central Universities. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Shu-Ching Chen. (Corresponding author: Shuhan Qi.)
CR Ankerst M, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P49
   [Anonymous], AUTOMATICA
   [Anonymous], 2011, Technical Report CNS-TR-2011-001
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, SIMILARITY BASED PAT
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2016, CORR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2013, COMPUTER SCI
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], CORR
   [Anonymous], 2016, P INT C LEARN REPR
   Branson Steve, 2014, CORR
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Frénay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   George M, 2014, LECT NOTES COMPUT SC, V8690, P440, DOI 10.1007/978-3-319-10605-2_29
   George Marian., 2015, Proceedings of the IEEE International Conference on Computer Vision Workshops, P154
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang JZ, 2014, J FUNCT SPACE, V2014, DOI 10.1155/2014/326940
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Khosla A., 2011, P IEEE C COMP VIS PA, V2
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Nair V., 2015, P INT C MACH LEARN, P807
   Nilsback M.E., 2006, IEEE C COMP VIS PATT, P1447, DOI DOI 10.1109/CVPR.2006.42
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen XH, 2012, LECT NOTES COMPUT SC, V7575, P114, DOI 10.1007/978-3-642-33765-9_9
   Simonyan K., 2014, 14091556 ARXIV
   Sukhbaatar S., 2015, Training convolutional networks with noisy labels, P1
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vedaldi A, 2014, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2014.463
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang W, 2016, DECIS SUPPORT SYST, V87, P80, DOI 10.1016/j.dss.2016.05.002
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wang Y, 2016, IEEE T MULTIMEDIA, V18, P1869, DOI 10.1109/TMM.2016.2581580
   Wang Y, 2015, IEEE T MULTIMEDIA, V17, P2072, DOI 10.1109/TMM.2015.2480228
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Xie SN, 2015, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2015.7298880
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Zhang HF, 2013, KEY ENG MATER, V562-565, P33, DOI 10.4028/www.scientific.net/KEM.562-565.33
   Zhang HW, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2978656
   Zhang HW, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P325, DOI 10.1145/2911451.2911502
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang LM, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886775
   Zhao B, 2016, IEEE T MULTIMEDIA, V18, P1111, DOI 10.1109/TMM.2016.2537783
NR 54
TC 3
Z9 3
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1272
EP 1284
DI 10.1109/TMM.2017.2655422
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400013
DA 2024-07-18
ER

PT J
AU Qian, XM
   Wang, H
   Zhao, YS
   Hou, XS
   Hong, RC
   Wang, M
   Tang, YY
AF Qian, Xueming
   Wang, Huan
   Zhao, Yisi
   Hou, Xingsong
   Hong, Richang
   Wang, Meng
   Tang, Yuan Yan
TI Image Location Inference by Multisaliency Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Location estimation; region of interest (ROI); visual phrase; salient
   map; salient region; spatial constraint
ID RETRIEVAL; FEATURES; SEARCH; MODEL; DEEP
AB Locations of images have been widely used in many application scenarios for large geotagged image corpora. As to images that are not geographically tagged, we estimate their locations with the help of the large geotagged image set by content-based image retrieval. Bag-of-words image representation has been utilized widely. However, the individual visual word-based image retrieval approach is not effective in expressing the salient relationships of image region. In this paper, we present an image location estimation approach by multisaliency enhancement. We first extract region-of-interests (ROIs) by mean-shift clustering on the visual words and salient map of the image based on which we further determine the importance of the ROI. Then, we describe each ROI by the spatial descriptors of visualwords. Finally, regionbased visual phrases are generated to further enhance the saliency in image location estimation. Experiments show the effectiveness of our proposed approach.
C1 [Qian, Xueming; Wang, Huan; Zhao, Yisi; Hou, Xingsong] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Smiles Lab, Xian 710049, Peoples R China.
   [Hong, Richang; Wang, Meng] Hefei Univ Technol, Hefei 230009, Peoples R China.
   [Tang, Yuan Yan] Univ Macau, Fac Sci & Technol, Macau, Peoples R China.
C3 Xi'an Jiaotong University; Hefei University of Technology; University of
   Macau
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Smiles Lab, Xian 710049, Peoples R China.
EM qianxm@mail.xjtu.edu.cn; email_wanghuan@163.com;
   zys2012@stu.xjtu.edu.cn; houxs@mail.xjtu.edu.cn; hongrc.hfut@gmail.com;
   eric.mengwang@gmail.com; yytang@umac.edu.cn
RI Wang, Meng/ITR-8699-2023
FU Program 973 [2012CB316400]; NSFC [60903121, 61173109, 61332018];
   Microsoft Research Asia; Fundamental Research Funds for the Central
   Universities
FX This work was supported in part by the Program 973 under Grant
   2012CB316400, in part by NSFC under Grant 60903121, Grant 61173109, and
   Grant 61332018, in part by Microsoft Research Asia, and in part by the
   Fundamental Research Funds for the Central Universities. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. K. Selcuk Candan. (Corresponding author: Xueming
   Qian.)
CR [Anonymous], P MED EV WORKSH
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], 2008, 2008 IEEE C COMPUTER
   [Anonymous], ADV MULTIMEDIA MODEL
   Baatz G, 2012, INT J COMPUT VISION, V96, P315, DOI 10.1007/s11263-011-0458-7
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Cao XC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P997, DOI 10.1145/2647868.2655007
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Changchang Wu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563037
   Chen J., 2010, P 19 IEEE ICIP SEP O, P1909
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Donoser M, 2014, PROC CVPR IEEE, P516, DOI 10.1109/CVPR.2014.73
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Gavves E, 2012, COMPUT VIS IMAGE UND, V116, P238, DOI 10.1016/j.cviu.2011.10.004
   Han JW, 2014, MULTIMED TOOLS APPL, V72, P2275, DOI 10.1007/s11042-013-1509-6
   Hauff C, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P691, DOI 10.1145/2348283.2348376
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji RR, 2012, PROC CVPR IEEE, P2925, DOI 10.1109/CVPR.2012.6248020
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kalogerakis E, 2009, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2009.5459259
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laere O., 2010, P MEDIAEVAL WORKSH, P1
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li HQ, 2013, IEEE T MULTIMEDIA, V15, P594, DOI 10.1109/TMM.2012.2234730
   Li J, 2015, SIGNAL PROCESS-IMAGE, V38, P141, DOI 10.1016/j.image.2015.07.007
   Li J, 2013, IEEE T MULTIMEDIA, V15, P2058, DOI 10.1109/TMM.2013.2280127
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P740, DOI 10.1109/TIP.2015.2507942
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu D, 2016, IEEE T MULTIMEDIA, V18, P1628, DOI 10.1109/TMM.2016.2568099
   Lu XQ, 2017, IEEE T CYBERNETICS, V47, P884, DOI 10.1109/TCYB.2016.2531179
   Lu XQ, 2017, IEEE T IMAGE PROCESS, V26, P355, DOI 10.1109/TIP.2016.2627801
   Lu XQ, 2015, IEEE T CYBERNETICS, V45, P1967, DOI 10.1109/TCYB.2014.2362959
   Lu XQ, 2014, IEEE T CYBERNETICS, V44, P366, DOI 10.1109/TCYB.2013.2256347
   Lu XQ, 2014, IEEE T CYBERNETICS, V44, P149, DOI 10.1109/TCYB.2013.2286496
   Lu XQ, 2013, IEEE T CIRC SYST VID, V23, P2022, DOI 10.1109/TCSVT.2013.2244798
   Min WQ, 2014, IEEE MULTIMEDIA, V21, P20, DOI 10.1109/MMUL.2014.1
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Park M., 2010, Proceedings of the International Conference on Multimedia, Firenze, Italy, P631
   Penatti OAB, 2014, PATTERN RECOGN, V47, P705, DOI 10.1016/j.patcog.2013.08.012
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Qian XM, 2016, IEEE T CIRC SYST VID, V26, P1746, DOI 10.1109/TCSVT.2015.2475815
   Qian XM, 2016, IEEE T IMAGE PROCESS, V25, P195, DOI 10.1109/TIP.2015.2497145
   Qian XM, 2015, IEEE T CIRC SYST VID, V25, P1857, DOI 10.1109/TCSVT.2014.2369731
   Qian XM, 2015, IEEE T IMAGE PROCESS, V24, P4348, DOI 10.1109/TIP.2015.2462131
   Qian XM, 2014, MULTIMED TOOLS APPL, V69, P897, DOI 10.1007/s11042-012-1151-8
   Quack Till., 2008, P 2008 INT C CONTENT, P47
   Sang JT, 2013, IEEE T MULTIMEDIA, V15, P1665, DOI 10.1109/TMM.2013.2268052
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P1359, DOI 10.1109/TNNLS.2013.2293418
   Shuhui Jiang, 2016, IEEE Transactions on Big Data, V2, P43, DOI 10.1109/TBDATA.2016.2541160
   Tang W., 2011, P 19 ACM INT C MULT, P503
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xue Y, 2012, IEEE IMAGE PROC, P2873, DOI 10.1109/ICIP.2012.6467499
   Yang CL, 2013, PATTERN RECOGN, V46, P948, DOI 10.1016/j.patcog.2012.07.011
   Yang XY, 2015, PATTERN RECOGN, V48, P3093, DOI 10.1016/j.patcog.2014.12.017
   Yang XY, 2015, IEEE T IMAGE PROCESS, V24, P1709, DOI 10.1109/TIP.2015.2411433
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yibin Li, 2009, 2009 IEEE International Conference on Automation and Logistics (ICAL), P1957, DOI 10.1109/ICAL.2009.5262626
   Yisi Zhao, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P436, DOI 10.1007/978-3-319-14442-9_49
   Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222
   Yuan Y, 2015, IEEE T NEUR NET LEAR, V26, P2222, DOI 10.1109/TNNLS.2014.2359471
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhao GS, 2017, IEEE T BIG DATA, V3, P67, DOI 10.1109/TBDATA.2016.2552541
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
NR 78
TC 27
Z9 28
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 813
EP 821
DI 10.1109/TMM.2016.2638207
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500012
DA 2024-07-18
ER

PT J
AU Mao, XJ
   Yang, YB
   Li, N
AF Mao, Xiao-Jiao
   Yang, Yu-Bin
   Li, Ning
TI Hashing With Pairwise Correlation Learning and Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary codes; correlation learning; correlation reconstruction; hashing
ID BINARY-CODES; QUANTIZATION; SEARCH; GRAPH
AB Existing hashing methods normally define certain specific forms of hash functions, after which an objective function can be formulated to optimize the loss on training set to learn the parameters. However, in this way, the hash function will be tightly coupled with the generated objective inmost cases. Moreover, since the objectives are generally formulated with binary quantization, most of them are nonconvex, which makes the optimization difficult and consequently decreases the similarity preserving performance of hashing. To solve this problem, we propose a novel pairwise correlation preserving framework to learn compact binary codes for hashing. First, we project each data into a metric space and represent it as a vector encoding the underlying local and global structure by pairwise correlation learning. Afterwards, pairwise correlation reconstruction (PCR), is further proposed to preserve the correlations of data between themetric space and the hamming space to learn binary codes. The PCR model is convex. Moreover, no specific hash functions are needed to be predefined and the steps of correlation learning and reconstruction are independent. The above characteristicsmake the optimization of PCR easily and efficiently, and thus leads to better preservation of data similarity in hamming space.
C1 [Mao, Xiao-Jiao; Yang, Yu-Bin; Li, Ning] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210093, Jiangsu, Peoples R China.
C3 Nanjing University
RP Mao, XJ (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210093, Jiangsu, Peoples R China.
EM xjmgl.nju@gmail.com; yangyubin@nju.edu.cn; ln@nju.edu.cn
FU Natural Science Foundation of China [61673204, 61273257, 61321491];
   Program for Distinguished Talents of Jiangsu Province, China
   [2013-XXRJ-018]; Fundamental Research Funds for the Central Universities
   [020214380026]
FX This work was supported by the Natural Science Foundation of China under
   Grant 61673204, Grant 61273257, and Grant 61321491, by the Program for
   Distinguished Talents of Jiangsu Province, China under Grant
   2013-XXRJ-018, and by the Fundamental Research Funds for the Central
   Universities under Grant 020214380026.
CR [Anonymous], 2009, NIPS
   [Anonymous], 2009, NEURIPS
   Babenko A, 2015, IEEE T PATTERN ANAL, V37, P1247, DOI 10.1109/TPAMI.2014.2361319
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Ge TZ, 2014, LECT NOTES COMPUT SC, V8695, P250, DOI 10.1007/978-3-319-10584-0_17
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Grauman K., 2013, Machine Learning for Computer Vision, V411, P49, DOI 10.1007/978-3-642-28661-2_3
   Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Jégou H, 2011, INT CONF ACOUST SPEE, P861
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jin ZM, 2013, IEEE I CONF COMP VIS, P257, DOI 10.1109/ICCV.2013.39
   Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lin GS, 2015, IEEE T PATTERN ANAL, V37, P2317, DOI 10.1109/TPAMI.2015.2404776
   Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317
   Lin GS, 2014, LECT NOTES COMPUT SC, V8691, P613, DOI 10.1007/978-3-319-10578-9_40
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu WJ, 2011, E-POLYMERS
   Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26
   Norouzi M.E., 2011, ICML
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Peng X, 2013, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2013.62
   SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451
   Sha CF, 2016, FRONT COMPUT SCI-CHI, V10, P477, DOI 10.1007/s11704-015-5222-7
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Y., 2008, ADV NEURAL INFORM PR, V21, P1753
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yan CC, 2015, FRONT COMPUT SCI-CHI, V9, P741, DOI 10.1007/s11704-015-4192-0
   Zhuang LS, 2012, PROC CVPR IEEE, P2328, DOI 10.1109/CVPR.2012.6247944
NR 43
TC 12
Z9 12
U1 3
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 382
EP 392
DI 10.1109/TMM.2016.2614858
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800013
DA 2024-07-18
ER

PT J
AU Pang, JB
   Tao, F
   Zhang, CJ
   Zhang, WG
   Huang, QM
   Yin, BC
AF Pang, Junbiao
   Tao, Fei
   Zhang, Chunjie
   Zhang, Weigang
   Huang, Qingming
   Yin, Baocai
TI Robust Latent Poisson Deconvolution From Multiple Features for Web Topic
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE K-nearest neighbor similarity graph; latent poisson deconvolution (LPD);
   multi-view learning (MVL); user-generated content (UGC); web topic
   detection
AB Detecting "hot" topics from the enormous user-generated content (UGC) data on web poses two main difficulties that the conventional approaches can barely handle: 1) poor feature representations from noisy images or short texts, and 2) uncertain roles of modalities where the visual content is either highly or weakly relevant to the textual cues due to the less-constrained UGC. In this paper, following the detection-by-ranking approach, we address above challenges by learning a robust latent representation from multiple, noisy and a high probability of the complementary features. Both the textual features and the visual ones are encoded into a k-nearest neighbor hybrid similarity graph (HSG), where nonnegative matrix factorization using random walk is introduced to generate topic candidates. An efficient fusion of multiple HSGs is then done by a latent poisson deconvolution, which consists of a poisson deconvolution with sparse basis similarity for each edge. Experiments show significantly improved accuracy of the proposed approach in comparison with the state-of-the-art methods on two public datasets.
C1 [Pang, Junbiao] Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
   [Tao, Fei; Zhang, Chunjie] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 100049, Peoples R China.
   [Zhang, Weigang] Harbin Inst Technol, Sch Comp Sci & Technol, Weihai 264209, Peoples R China.
   [Zhang, Weigang; Huang, Qingming] Univ Chinese Acad Sci, Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Adv Invocat Ctr Future Internet Technol, Dalian 116024, Peoples R China.
   [Yin, Baocai] Beijing Univ Technol, Beijing 100124, Peoples R China.
C3 Beijing University of Technology; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Harbin Institute of
   Technology; Chinese Academy of Sciences; University of Chinese Academy
   of Sciences, CAS; Chinese Academy of Sciences; Institute of Computing
   Technology, CAS; Dalian University of Technology; Beijing University of
   Technology
RP Zhang, WG (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Weihai 264209, Peoples R China.
EM junbiao_pang@bjut.edu.cn; ftao@jdl.ac.cn; cjzhang@jdl.ac.cn;
   wgzhang@hit.edu.cn; qmhuang@jdl.ac.cn; ybc@dlut.edu.cn
RI zhang, chunjie/Z-3035-2019; Zhang, Weigang/GZA-9095-2022
OI zhang, chunjie/0000-0002-1161-8995; Zhang, Weigang/0000-0003-0042-7074;
   pang, Junbiao/0000-0001-8153-7229
FU National Basic Research Program of China (973 Program) [2012CB316400,
   2015CB3351800]; Natural Science Foundation of China [61332016, 61472387,
   61303153, 61390510, 61303154]; Beijing Post-Doctoral Research
   Foundation; Beijing Municipal Commission of Education [KM201610005034];
   Funding Project for Academic Human Resources Development in Institutions
   of Higher Learning Under the Jurisdiction of Beijing Municipality (PHR)`
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2012CB316400 and Grant 2015CB3351800,
   in part by the Natural Science Foundation of China under Grant 61332016,
   Grant 61472387, Grant 61303153, Grant 61390510, and Grant 61303154, in
   part by the Beijing Post-Doctoral Research Foundation, in part the
   Beijing Municipal Commission of Education under Grant KM201610005034,
   and in part by the Funding Project for Academic Human Resources
   Development in Institutions of Higher Learning Under the Jurisdiction of
   Beijing Municipality (PHR). The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Jing-Ming
   Guo. (Corresponding author: Weigang Zhang.)
CR Allan J., 1998, P DARPA BROADC NEWS
   [Anonymous], 2010, P 16 ACM SIGKDD INT, DOI DOI 10.1145/1835804.1835884
   [Anonymous], 2007, P 15 ACM INT C MULT
   [Anonymous], 2003, P 26 ANN INT ACM SIG, DOI DOI 10.1145/860435.860495
   Banerjee A, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P431
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bickel S, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P19, DOI 10.1109/ICDM.2004.10095
   Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Cao J, 2011, IEEE T CIRC SYST VID, V21, P1835, DOI 10.1109/TCSVT.2011.2148470
   Chaudhuri K., 2009, P 26 ANN INT C MACH, P129
   Chen J, 2009, J MACH LEARN RES, V10, P1989
   Chen T., 2012, P 20 ACM INT C MULT, P781
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Han Bo., 2012, Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, P421
   He Q, 2010, IEEE T PATTERN ANAL, V32, P1795, DOI 10.1109/TPAMI.2009.203
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Kludas J, 2008, LECT NOTES COMPUT SC, V4918, P147, DOI 10.1007/978-3-540-79860-6_12
   Kumar A., 2011, P NEUR INF PROC SYST, P1412
   Lin Z., 2013, CORR
   Liu Y, 2009, PROCEEDINGS OF INTERNATIONAL FORUM ON TECHNOLOGICAL INNOVATION AND COMPETITIVE TECHNICAL INTELLIGENCE 2008, P338, DOI 10.1109/ICNC.2009.485
   Luo Z.-Q., 2012, CORR
   Aiello LM, 2013, IEEE T MULTIMEDIA, V15, P1268, DOI 10.1109/TMM.2013.2265080
   Newman D, 2011, Advances in Neural Information Processing Systems, P496, DOI DOI 10.5555/2986459.2986515
   Pan Y., 2013, P AAAI C ART INT, P3021
   Pang JB, 2015, IEEE T MULTIMEDIA, V17, P843, DOI 10.1109/TMM.2015.2425143
   Papadopoulos S, 2011, IEEE MULTIMEDIA, V18, P52, DOI 10.1109/MMUL.2010.68
   PUTTHIVIDHYA D, 2010, PROC CVPR IEEE, P3408, DOI DOI 10.1109/CVPR.2010.5540000
   Qi He, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P207
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Sun AX, 2011, IEEE T SYST MAN CY A, V41, P834, DOI 10.1109/TSMCA.2011.2157129
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Wang C., 2008, Proceedings of the 17th International Conference on World Wide Web, WWW'08, P457, DOI DOI 10.1145/1367497.1367560.
   Wu X, 2007, P INT C ACM MULT, P168
   Xu C, 2013, ARXIV PREPRINT ARXIV
   Xu W., 2003, P 26 ANN INT ACM SIG, P267
   Yang Y., 1998, P 21 ANN INT ACM SIG, P28, DOI DOI 10.1145/290941.290953
   Yang Z., 2012, ADV NEURAL INFORM PR, P1079
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Zhang Y, 2013, J MATER CHEM B, V1, P132, DOI 10.1039/c2tb00071g
   Zhao WNX, 2011, LECT NOTES COMPUT SC, V6611, P338, DOI 10.1007/978-3-642-20161-5_34
   Zhou C. J., 2007, P 24 INT C MACH LEAR, P1159, DOI DOI 10.1145/1273496.1273642
   Zhou Y, 2014, IEEE INT CONGR BIG, P1, DOI 10.1109/BigData.Congress.2014.11
   Zien A., 2007, P INT C MACHINE LEAR, V227, P1191, DOI [10.1145/1273496.1273646, DOI 10.1145/1273496.1273646]
NR 46
TC 8
Z9 8
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2482
EP 2493
DI 10.1109/TMM.2016.2598439
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200014
DA 2024-07-18
ER

PT J
AU Kurzhals, K
   John, M
   Heimerl, F
   Kuznecov, P
   Weiskopf, D
AF Kurzhals, Kuno
   John, Markus
   Heimerl, Florian
   Kuznecov, Paul
   Weiskopf, Daniel
TI Visual Movie Analytics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Movie analysis; video visualization; visual analytics
ID SHOT-BOUNDARY DETECTION; VIDEO; RETRIEVAL; IMAGE
AB The analysis of inherent structures of movies plays an important role in studying stylistic devices and specific, content-related questions. Examples are the analysis of personal constellations in movie scenes, dialogue-based content analysis, or the investigation of image-based features. We provide a visual analytics approach that supports the analytical reasoning process to derive higher level insights about the content on a semantic level. Combining automatic methods for semantic scene analysis based on script and subtitle text, we perform a low-level analysis of the data automatically. Our approach features an interactive visualization that allows a multilayer interpretation of descriptive features to characterize movie content. For semantic analysis, we extract scene information from movie scripts and match them with the corresponding subtitles. With text-and image-based query techniques, we facilitate an interactive comparison of different movie scenes on an image and on a semantic level. We demonstrate how our approach can be applied for content analysis on a popular Hollywood movie.
C1 [Kurzhals, Kuno; Weiskopf, Daniel] Univ Stuttgart, Visualizat Res Ctr VISUS, D-70569 Stuttgart, Germany.
   [John, Markus; Heimerl, Florian; Kuznecov, Paul] Univ Stuttgart, Inst Visualizat & Interact Syst, D-70569 Stuttgart, Germany.
C3 University of Stuttgart; University of Stuttgart
RP Kurzhals, K (corresponding author), Univ Stuttgart, Visualizat Res Ctr VISUS, D-70569 Stuttgart, Germany.
EM kurzhals@visus.uni-stuttgart.de; markus.john@vis.uni-stuttgart.de;
   florian.heimerl@vis.unistuttgart.de; paul.kuznecov@gmx.de;
   Daniel.Weiskopf@visus.uni-stuttgart.de
RI Heimerl, Florian/AAY-9917-2020
FU German Research Foundation (DFG) within Cluster of Excellence in
   Simulation Technology [EXC 310]; German Federal Ministry of Education
   and Research (BMBF) as of the Center for Reflected Text Analysis CRETA
   at University of Stuttgart
FX This work was supported in part by the German Research Foundation (DFG)
   within the Cluster of Excellence in Simulation Technology (EXC 310) and
   in part by the German Federal Ministry of Education and Research (BMBF)
   as of the Center for Reflected Text Analysis CRETA at University of
   Stuttgart. The guest editor coordinating the review of this manuscript
   and approving it for publication was Prof. Yingcai Wu.
CR Andrienko N, 2003, J VISUAL LANG COMPUT, V14, P503, DOI 10.1016/S1045-926X(03)00046-6
   [Anonymous], 2013, VIDEO CONTENT ANAL U
   [Anonymous], 2010, P ACM C MULTIMEDIA S
   [Anonymous], 2010, P 18 ACM INT C MULTI
   [Anonymous], P 6 ACM SIGMM INT WO, DOI [10.1145/1026711.1026752, DOI 10.1145/1026711.1026752]
   Boreczky J., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P185, DOI 10.1145/332040.332428
   Boreczky JS, 1996, J ELECTRON IMAGING, V5, P122, DOI 10.1117/12.238675
   Borgo R, 2012, COMPUT GRAPH FORUM, V31, P2450, DOI 10.1111/j.1467-8659.2012.03158.x
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Chinchor NA, 2010, IEEE COMPUT GRAPH, V30, P52, DOI 10.1109/MCG.2010.92
   Cour T, 2008, LECT NOTES COMPUT SC, V5305, P158, DOI 10.1007/978-3-540-88693-8_12
   Das D, 2014, COMPUT LINGUIST, V40, P9, DOI 10.1162/COLI_a_00163
   Del Fabro M, 2013, MULTIMEDIA SYST, V19, P427, DOI 10.1007/s00530-013-0306-4
   Ellouze M, 2010, J VIS COMMUN IMAGE R, V21, P283, DOI 10.1016/j.jvcir.2010.01.007
   Escamilla G, 2000, AM J PUBLIC HEALTH, V90, P412, DOI 10.2105/AJPH.90.3.412
   Fillmore C. J., 1982, Linguistics in the Morning Calm: Vol. Selected Papers from SICOL-1981, P111
   Fouse Adam., 2011, PROC EXTENDED ABSTRA, P299, DOI 10.1145/1979742.1979706
   Furini M, 2007, P 6 ACM INT C IM VID, P635
   Furini M, 2010, MULTIMED TOOLS APPL, V46, P47, DOI 10.1007/s11042-009-0307-7
   Hagedorn J., 2008, P WORK C ADV VIS INT, P317, DOI DOI 10.1145/1385569.1385622
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Jänicke H, 2010, COMPUT GRAPH FORUM, V29, P357, DOI 10.1111/j.1467-8659.2009.01605.x
   Kipp M., 2014, The Oxford handbook of Corpus phonology (chapter 21), P420
   Li Y, 2006, IEEE SIGNAL PROC MAG, V23, P79
   Li Y., 2011, P 19 ACM INT C MULT, P1573
   Lienhart R, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P314, DOI 10.1109/MMCS.1996.534993
   LIU A, 2010, INT J DIGITAL CONTEN, V4, P23, DOI DOI 10.4156/JDCTA.VOL4.ISSUE5
   Liu AL, 2008, PROCEEDINGS OF THE 6TH CONFERENCE OF BIOMATHEMATICS, VOLS I AND II, P1
   Liu SX, 2013, IEEE T VIS COMPUT GR, V19, P2436, DOI 10.1109/TVCG.2013.196
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Nam J., 1999, PROC 7 ACM INT C MUL, P53
   Ponceleon D., 2001, P 34 ANN HAW INT C S, P1
   Rollins PeterC., 2011, Hollywood's Indian: The Portrayal of the Native American in Film
   Ruppenhofer Josef, 2006, FrameNet II: Extended theory and practice
   Ryan MichaelMelissa Lenos., 2012, An Introduction to Film Analysis: Technique and Meaning in Narrative Film
   Schoeffmann K, 2015, ACM COMPUT SURV, V48, DOI 10.1145/2808796
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith MA, 1997, PROC CVPR IEEE, P775, DOI 10.1109/CVPR.1997.609414
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Vasconcelos N, 2000, IEEE T IMAGE PROCESS, V9, P3, DOI 10.1109/83.817595
   Wactlar HD, 1996, COMPUTER, V29, P46, DOI 10.1109/2.493456
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P12, DOI 10.1109/79.888862
   Weng CY, 2009, IEEE T MULTIMEDIA, V11, P256, DOI 10.1109/TMM.2008.2009684
   Worring M, 2012, IEEE MULTIMEDIA, V19, P6, DOI 10.1109/MMUL.2012.53
   Yoshitaka A, 1999, IEEE T KNOWL DATA EN, V11, P81, DOI 10.1109/69.755617
NR 48
TC 31
Z9 34
U1 2
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2149
EP 2160
DI 10.1109/TMM.2016.2614184
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900003
DA 2024-07-18
ER

PT J
AU Li, L
   Li, B
   Liu, D
   Li, HQ
AF Li, Li
   Li, Bin
   Liu, Dong
   Li, Houqiang
TI λ-Domain Rate Control Algorithm for HEVC Scalable Extension
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficiency video coding (HEVC); rate control; R-lambda model; R-Q
   model; scalable HEVC (SHVC); scalable video coding (SVC)
ID DELAY RATE CONTROL; BIT ALLOCATION; RHO-DOMAIN; H.264 VIDEO;
   OPTIMIZATION; SCALABILITY
AB In this paper, we propose a lambda-domain rate control algorithm for high efficiency video coding (HEVC) scalable extension. All the commonly used scalabilities including temporal, spatial, and quality scalability are taken into consideration. The proposed algorithm mainly has three key contributions. First, we propose an optimal initial target bits and initial encoding parameters determination algorithm for the first frame of each layer to achieve the best rate-distortion (R-D) performance. Second, an optimal bit allocation algorithm taking both the intra and inter layer dependence into consideration is proposed for the inter frames under spatial and quality scalability cases. Third, since the coding scheme of HEVC scalable extension with multiple layers is even more flexible than HEVC, an adaptive updating algorithm for R-lambda model is proposed to control the bits per frame even more precisely. The experimental results demonstrate that the proposed lambda-domain rate control algorithm can bring both more precise bitrate accuracy and better R-D performance compared with the previous rate control algorithms for HEVC scalable extension.
C1 [Li, Li; Liu, Dong; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Li, Bin] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Li, L (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM lilimao@mail.ustc.edu.cn; libin@microsoft.com; dongeliu@ustc.edu.cn;
   lihq@ustc.edu.cn
RI Li, Li/T-2232-2019; liu, dong/GRJ-9115-2022; liu,
   dongsheng/IWM-1597-2023; Li, Houqiang Li/B-6259-2013
OI Liu, Dong/0000-0001-9100-2906
FU 973 Program [2013CB329004]; National Key Research and Development Plan
   [2016YFC0801001]; Natural Science Foundation of China [61325009,
   61390510, 61272316]
FX This work was supported in part by 973 Program under Contract
   2013CB329004, in part by the National Key Research and Development Plan
   under Grant 2016YFC0801001, and in part by the Natural Science
   Foundation of China under Contract 61325009, Contract 61390510, and
   Contract 61272316. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shahram Shirani.
CR [Anonymous], 2012, P 11 JOINT COLL TEAM
   [Anonymous], HEVC TEST MOD
   [Anonymous], 2012, JCTVCH0213
   [Anonymous], SCAL HEVC TEST MOD
   [Anonymous], 2003, JVTG012
   [Anonymous], 2015, JCTVCU0132
   [Anonymous], 2005, JVTN046
   [Anonymous], JCTVCL1009
   [Anonymous], JVTW043
   [Anonymous], PICT COD S LISB PORT
   [Anonymous], P SPIE VIS COMMUN IM
   Bossen F., 2012, JCTVCJ1100
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P840, DOI 10.1109/TCSVT.2002.804883
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P928, DOI 10.1109/76.937431
   He ZH, 2008, IEEE T MULTIMEDIA, V10, P1237, DOI 10.1109/TMM.2008.2004903
   Hsiang ST, 2001, SIGNAL PROCESS-IMAGE, V16, P705, DOI 10.1016/S0923-5965(01)00002-9
   Hu SD, 2011, IEEE T CIRC SYST VID, V21, P1152, DOI 10.1109/TCSVT.2011.2138810
   Jiang MQ, 2006, IEEE T MULTIMEDIA, V8, P467, DOI 10.1109/TMM.2006.870713
   Kim BJ, 2000, IEEE T CIRC SYST VID, V10, P1374, DOI 10.1109/76.889025
   Kwon DK, 2007, IEEE T CIRC SYST VID, V17, P517, DOI 10.1109/TCSVT.2007.894053
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li L, 2015, IEEE INT SYMP CIRC S, P2788, DOI 10.1109/ISCAS.2015.7169265
   Liu JY, 2011, IEEE IMAGE PROC, P1641, DOI 10.1109/ICIP.2011.6115767
   Liu JY, 2010, IEEE T CIRC SYST VID, V20, P967, DOI 10.1109/TCSVT.2010.2045924
   Liu M, 2010, IEEE IMAGE PROC, P1277, DOI 10.1109/ICIP.2010.5653340
   Liu Y, 2007, IEEE INT SYMP CIRC S, P1746, DOI 10.1109/ISCAS.2007.378009
   Liu Y, 2008, IEEE T CIRC SYST VID, V18, P116, DOI 10.1109/TCSVT.2007.903325
   Luo JC, 2010, IEEE T MULTIMEDIA, V12, P97, DOI 10.1109/TMM.2009.2037385
   Mansour H, 2011, IEEE T MULTIMEDIA, V13, P165, DOI 10.1109/TMM.2010.2099648
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   Pitrey Y, 2008, IEEE INT SYM MULTIM, P89, DOI 10.1109/ISM.2008.71
   Reichel J., 2007, JOINT SCALABLE VIDEO
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Seo CW, 2010, IEEE T CIRC SYST VID, V20, P1210, DOI 10.1109/TCSVT.2010.2057011
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Binh VP, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTING, MANAGEMENT AND TELECOMMUNICATIONS (COMMANTEL), P225, DOI 10.1109/ComManTel.2013.6482395
   Wang HL, 2008, IEEE T CIRC SYST VID, V18, P140, DOI 10.1109/TCSVT.2007.913757
   Wu W, 2009, IEEE T CONSUM ELECTR, V55, P665, DOI 10.1109/TCE.2009.5174437
   Xu J., 2012, JCT-VC, document JCTVC-I0426
   Yang J, 2010, GLOB TELECOMM CONF
NR 43
TC 31
Z9 33
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2023
EP 2039
DI 10.1109/TMM.2016.2595264
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800010
DA 2024-07-18
ER

PT J
AU Weng, RL
   Lu, JW
   Tan, YP
   Zhou, J
AF Weng, Renliang
   Lu, Jiwen
   Tan, Yap-Peng
   Zhou, Jie
TI Learning Cascaded Deep Auto-Encoder Networks for Face Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Auto-encoder; deep learning; face alignment
ID REPRESENTATION; MODELS
AB In this paper, we propose a new cascaded deep auto-encoder networks (CDAN) approach for face alignment. Our framework consists of a global exemplar-based deep auto-encoder network (GEDAN) and a series of localized deep auto-encoder networks (LDAN) in a cascaded fashion. The global network takes a low-resolution holistic facial image as input and generates a preliminary facial landmark configuration. The following localized networks sample pose-indexed local features around current landmark positions, and refine the landmark positions with increasingly higher image resolutions. Our network architectures are designed to achieve greater robustness against pose variations as well as higher landmark estimation accuracy. Experimental results on three datasets show that the proposed approach achieves superior alignment accuracy with real-time speed.
C1 [Weng, Renliang; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Lu, Jiwen; Zhou, Jie] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Nanyang Technological University; Tsinghua University
RP Lu, JW (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
EM wrlkyle@gmail.com; lujiwen@tsinghua.edu.cn; eyptan@ntu.edu.sg;
   jzhou@tsinghua.edu.cn
RI Tan, Yap-Peng/A-5158-2011; Lu, Jiwen/C-5291-2009
OI Lu, Jiwen/0000-0002-6121-5529
FU National Key Research and Development Program of China [2016YFB1001001];
   National 1000 Young Talents Plan Program; Natural Science Foundation of
   China [61225008, 61572271, 61527808, 61373074, 61373090]; National Basic
   Research Program of China [2014CB349304]; Ministry of Education of China
   [20120002110033]; Tsinghua University Initiative Scientific Research
   Program; National Research Foundation, Prime Minister's Office,
   Singapore under its IDM Futures Funding Initiative
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2016YFB1001001, by the National 1000 Young
   Talents Plan Program, by the Natural Science Foundation of China under
   Grant 61225008, Grant 61572271, Grant 61527808, Grant 61373074, and
   Grant 61373090, by the National Basic Research Program of China under
   Grant 2014CB349304, by the Ministry of Education of China under Grant
   20120002110033, by the Tsinghua University Initiative Scientific
   Research Program, and by a research grant from the National Research
   Foundation, Prime Minister's Office, Singapore, under its IDM Futures
   Funding Initiative and administered by the IDM Programme Office at the
   Rapid-Rich Object Search Laboratory, Nanyang Technological University,
   Singapore. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Guo-Jun Qi.
   (Corresponding author: Jiwen Lu.)
CR [Anonymous], 2008, Advances in neural information processing systems
   [Anonymous], CORR
   [Anonymous], 1995, Backpropagation: theory, architectures, and applications
   Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240
   Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao X., 2012, INT J COMPUT VISION, V107, P177
   Duong N, 2015, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2015.7299111
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D., 2005, P BRIT MACH VIS C, P929
   Dantone M., 2006, P IEEE C COMP VIS PA, P2578
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI [DOI 10.1109/CVPR.2014.241, 10.1109/CVPR.2014.241]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACHI, V382, P609, DOI 10.1145/1553374.1553453
   Luo P, 2012, PROC CVPR IEEE, P2480, DOI 10.1109/CVPR.2012.6247963
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Nair P, 2009, IEEE T MULTIMEDIA, V11, P611, DOI 10.1109/TMM.2009.2017629
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Shan SG, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P314
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tzimiropoulos G, 2014, IEEE T INF FOREN SEC, V9, P2024, DOI 10.1109/TIFS.2014.2361018
   Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vincent Pascal, 2008, P 25 INT C MACHINE L, DOI DOI 10.1145/1390156.1390294
   Wang N, 2013, P ADV NEURAL INFORM
   Wu Y, 2015, INT J COMPUT VISION, V113, P37, DOI 10.1007/s11263-014-0775-8
   Wu Y, 2013, PROC CVPR IEEE, P3452, DOI 10.1109/CVPR.2013.443
   Xing JL, 2014, PROC CVPR IEEE, P1829, DOI 10.1109/CVPR.2014.236
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhou F, 2013, IEEE I CONF COMP VIS, P1025, DOI 10.1109/ICCV.2013.131
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 51
TC 40
Z9 44
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2066
EP 2077
DI 10.1109/TMM.2016.2591508
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800013
DA 2024-07-18
ER

PT J
AU Zhang, XM
   Hu, X
   Wang, SZ
   Yang, Y
   Li, ZJ
   Zhou, JS
AF Zhang, Xiaoming
   Hu, Xia
   Wang, Senzhang
   Yang, Yang
   Li, Zhoujun
   Zhou, Jianshe
TI Learning Geographical Hierarchy Features via a Compositional Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hierarchical features; image location; image topic; multi-modal deep
   model; multi-modal feature
ID COMPLEX
AB Image location prediction is used to estimate the geolocation where an image is taken, which is important for many image applications, such as image retrieval, image browsing, and organization. Since a social image contains heterogeneous contents, such as visual content and textual content, effectively incorporating these contents to predict location is nontrivial. Moreover, it is observed that image content patterns and the locations where they may appear correlate hierarchically. Traditional image location prediction methods mainly adopt a single-level architecture and assume images are independently distributed in geographical space, which is not directly adaptable to the hierarchical correlation. In this paper, we propose a geographically hierarchical bi-modal deep belief network (GH-BDBN) model, which is a compositional learning architecture that integrates multi-modal deep learning model with a non-parametric hierarchical prior model. GH-BDBN learns a joint representation capturing the correlations among different types of image content using a bi-modal DBN, with a geographically hierarchical prior over the joint representation to model the hierarchical correlation between image content and location. Then, an efficient inference algorithm is proposed to learn the parameters and the geographical hierarchical structure of geographical locations. Experimental results demonstrate the superiority of our model for image location prediction.
C1 [Zhang, Xiaoming; Yang, Yang; Li, Zhoujun] Beihang Univ, Beijing Key Lab Network Technol, Beijing 100191, Peoples R China.
   [Zhang, Xiaoming; Yang, Yang; Li, Zhoujun] Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Hu, Xia] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77840 USA.
   [Wang, Senzhang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 210016, Peoples R China.
   [Zhou, Jianshe] Capital Normal Univ, Beijing Adv Innovat Ctr Imaging Technol, Beijing 100048, Peoples R China.
C3 Beihang University; Beihang University; Texas A&M University System;
   Texas A&M University College Station; Nanjing University of Aeronautics
   & Astronautics; Capital Normal University
RP Zhang, XM (corresponding author), Beihang Univ, Beijing Key Lab Network Technol, Beijing 100191, Peoples R China.
EM yolixs@buaa.edu.cn; xiahu@gmail.com; senzhangwang@gmail.com;
   yangyangfu-ture@gmail.com; lizj@buaa.edu.cn; jszhou@cnu.edu.cn
RI Li, Kexin/KAO-2519-2024; hu, xia hong/GQP-8544-2022
OI Li, Zhoujun/0000-0002-9603-9713
FU National Natural Science Foundation of China [61202239, 61170189];
   Fundamental Research Funds for the Central Universities
   [YWF-14-JSJXY-16]; Fund of the State Key Laboratory of Software
   Development Environment [SKLSDE-2015ZX-11]; Beijing Advanced Innovation
   Center for Imaging Technology [BAICIT-2016001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61202239 and Grant 61170189, in part by
   the Fundamental Research Funds for the Central Universities under Grant
   YWF-14-JSJXY-16, in part by the Fund of the State Key Laboratory of
   Software Development Environment under Grant SKLSDE-2015ZX-11, and in
   part by the Beijing Advanced Innovation Center for Imaging Technology
   under Grant BAICIT-2016001. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Winston
   Hsu.
CR [Anonymous], 2010, BAYESIAN NONPARAMETR
   [Anonymous], 2012, P ICML REPR LEARN WO
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], MEDIAEVAL WORKSH PIS
   [Anonymous], 2013, Proceedings of the 21st ACM international conference on Multimedia
   [Anonymous], 2011, PROC ACM INT C MULTI
   [Anonymous], P 29 C UNC ART INT
   [Anonymous], SURVEYING LAND INFOR
   [Anonymous], 2011, Proceedings of the 20th International Conference on World Wide Web-WWW'11, DOI [DOI 10.1145/1963405.1963443, 10.1145/1963405.1963443]
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blei DM, 2010, J ACM, V57, DOI 10.1145/1667053.1667056
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Nguyen CT, 2013, ACM T WEB, V7, DOI 10.1145/2516633.2516634
   Chen F, 2013, PROC CVPR IEEE, P1870, DOI 10.1109/CVPR.2013.244
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Daniels MJ, 1999, J AM STAT ASSOC, V94, P1254, DOI 10.2307/2669939
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Hauff C, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P691, DOI 10.1145/2348283.2348376
   Hays J, 2008, PROC CVPR IEEE, P3436
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Hou B, 2013, IEEE J-STARS, V6, P1602, DOI 10.1109/JSTARS.2013.2259470
   Hu PF, 2012, COMM COM INF SC, V321, P556
   Kalantidis Y, 2011, MULTIMED TOOLS APPL, V51, P555, DOI 10.1007/s11042-010-0651-7
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kling CC, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P603, DOI 10.1145/2556195.2556218
   Larson Martha., 2011, Proceedings of the 1st ACM international conference on multimedia retrieval, P51, DOI DOI 10.1145/1991996
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Liu J, 2014, IEEE T MULTIMEDIA, V16, P588, DOI 10.1109/TMM.2014.2302732
   Liu Y, 2011, PATTERN RECOGN, V44, P2287, DOI 10.1016/j.patcog.2010.12.012
   Luo JB, 2011, MULTIMED TOOLS APPL, V51, P187, DOI 10.1007/s11042-010-0623-y
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Mikolov T., 2013, INT C LEARN REPR SCO, DOI 10.48550/ARXIV.1301.3781
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Serdyukov P, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P484, DOI 10.1145/1571941.1572025
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tang JH, 2010, IEEE T MULTIMEDIA, V12, P131, DOI 10.1109/TMM.2009.2037373
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Wang H, 2013, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2013.398
   Wang YF, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P307, DOI 10.1145/2647868.2654901
   Wu JG, 2002, ECOL MODEL, V153, P7, DOI 10.1016/S0304-3800(01)00499-9
   Zhang XM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2401
   Zhang XM, 2012, WORLD WIDE WEB, V15, P233, DOI 10.1007/s11280-011-0132-6
   Zhu L, 2015, IEEE T CYBERNETICS, V45, P2756, DOI 10.1109/TCYB.2014.2383389
NR 46
TC 4
Z9 4
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1855
EP 1868
DI 10.1109/TMM.2016.2574122
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800015
DA 2024-07-18
ER

PT J
AU Gao, ZN
   Xue, JR
   Zhou, WG
   Pang, SM
   Tian, Q
AF Gao, Zhanning
   Xue, Jianru
   Zhou, Wengang
   Pang, Shanmin
   Tian, Qi
TI Democratic Diffusion Aggregation for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Democratic diffusion aggregation (DDA); image retrieval; query fusion
ID SCALE; DESCRIPTORS; GRAPH; QUANTIZATION; CODEBOOK; GEOMETRY
AB Content-based image retrieval is an important research topic in the multimedia field. In large-scale image search using local features, image features are encoded and aggregated into a compact vector to avoid indexing each feature individually. In the aggregation step, sum-aggregation is wildly used in many existing works and demonstrates promising performance. However, it is based on a strong and implicit assumption that the local descriptors of an image are identically and independently distributed in descriptor space and image plane. To address this problem, we propose a new aggregation method named democratic diffusion aggregation (DDA) with weak spatial context embedded. The main idea of our aggregation method is to re-weight the embedded vectors before sum-aggregation by considering the relevance among local descriptors. Different from previous work, by conducting a diffusion process on the improved kernel matrix, we calculate the weighting coefficients more efficiently without any iterative optimization. Besides considering the relevance of local descriptors from different images, we also discuss an efficient query fusion strategy which uses the initial top-ranked image vectors to enhance the retrieval performance. Experimental results show that our aggregation method exhibits much higher efficiency (about x 14 faster) and better retrieval accuracy compared with previous methods, and the query fusion strategy consistently improves the retrieval quality.
C1 [Gao, Zhanning; Xue, Jianru; Pang, Shanmin] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
   [Zhou, Wengang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Xi'an Jiaotong University; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS; University of Texas System;
   University of Texas at San Antonio (UTSA)
RP Gao, ZN (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
EM gaozn1990@stu.xjtu.edu.cn; jrxue@mail.xjtu.edu.cn; zhwg@ustc.edu.cn;
   pangshan-minn@sina.com; qitian@cs.utsa.edu
RI Pang, Shanmin/KBQ-6978-2024
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China (NSFC) [61273252]; NSFC
   [61472378]; Anhui Provincial Natural Science Fundation [1508085MF109];
   NSFC da [61429201]; ARO [W911NF-15-1-0290, W911NF-12-1-0057]; Faculty
   Research Gift Awards by NEC Laboratories of America; Blippar
FX This work was supported by National Basic Research Program of China (973
   Program) Project 2012CB316400 and by the National Natural Science
   Foundation of China (NSFC) under Contract 61273252. The work of W. Zhou
   was supported in part by the NSFC under Contract 61472378 and in part by
   the Anhui Provincial Natural Science Fundation under Contract
   1508085MF109. The work of Q. Tian was supported in part by the NSFC
   under Contract 61429201, in part by the ARO under Grant W911NF-15-1-0290
   and Grant W911NF-12-1-0057, and in part by the Faculty Research Gift
   Awards by NEC Laboratories of America and Blippar. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Adrian Munteanu.
CR [Anonymous], 2009, Advances in neural information processing systems
   [Anonymous], 2010, P 27 INT C MACHINE L
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2010, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2010.5539997
   Cinbis RG, 2012, PROC CVPR IEEE, P2184, DOI 10.1109/CVPR.2012.6247926
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Douze M, 2011, PROC CVPR IEEE, P745, DOI 10.1109/CVPR.2011.5995595
   Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Gao ZN, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P35, DOI 10.1145/2671188.2749293
   Ge TZ, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.132
   Ge TZ, 2014, PROC CVPR IEEE, P939, DOI 10.1109/CVPR.2014.125
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Knight PA, 2008, SIAM J MATRIX ANAL A, V30, P261, DOI 10.1137/060659624
   Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1393, DOI 10.1109/TPAMI.2006.184
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Lin J, 2014, IEEE SIGNAL PROC LET, V21, P195, DOI 10.1109/LSP.2013.2296532
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Murray N, 2014, PROC CVPR IEEE, P2473, DOI 10.1109/CVPR.2014.317
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Philbin J., 2008, P CVPR, P1
   Safadi B, 2013, INT WORK CONTENT MUL, P65, DOI 10.1109/CBMI.2013.6576554
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tian XM, 2015, IEEE T MULTIMEDIA, V17, P79, DOI 10.1109/TMM.2014.2368714
   Tolias G, 2014, LECT NOTES COMPUT SC, V8694, P382, DOI 10.1007/978-3-319-10599-4_25
   Tolias G, 2014, PATTERN RECOGN, V47, P3466, DOI 10.1016/j.patcog.2014.04.007
   Wang S, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700292
   Winder SAJ, 2007, PROC CVPR IEEE, P17
   Yang XW, 2013, IEEE T PATTERN ANAL, V35, P28, DOI 10.1109/TPAMI.2012.60
   Zhao WL, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.99
   Zheng L, 2013, PROC CVPR IEEE, P1626, DOI 10.1109/CVPR.2013.213
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhou WG, 2016, IEEE T PATTERN ANAL, V38, P159, DOI 10.1109/TPAMI.2015.2430329
   Zhou WG, 2015, IEEE T IMAGE PROCESS, V24, P967, DOI 10.1109/TIP.2015.2389624
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
NR 51
TC 26
Z9 27
U1 0
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1661
EP 1674
DI 10.1109/TMM.2016.2568748
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000018
DA 2024-07-18
ER

PT J
AU He, ZF
   Mao, SW
   Kompella, S
AF He, Zhifeng
   Mao, Shiwen
   Kompella, Sastry
TI Quality of Experience Driven Multi-User Video Streaming in Cellular
   Cognitive Radio Networks With Single Channel Access
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience (QoE); cognitive radio network (CRN);
   decomposition; multi-user video streaming; optimization
ID SPECTRUM ACCESS; ALLOCATION
AB We investigate the problem of streaming multi-user videos over the downlink of a cognitive radio network (CRN), where each cognitive user (CU) can access one channel at a time. We first consider the case where each CU can sense one channel at a time slot at most. To make the problem tractable, we tackle the optimal spectrum sensing and access problems separately and develop matching-based optimal algorithms to the subproblems, which yield an overall suboptimal solution. We then consider the case where each CU can sense multiple channels. We show that under the assumption that all the spectrum sensors work on the same operating point, a two-step approach can derive the optimal spectrum sensing and access policies that maximize the quality of experience (QoE) of the streaming videos. The superior performance of the proposed approaches is validated with simulations and comparisons with benchmark schemes, where a performance gain from 25% to 30% is demonstrated.
C1 [He, Zhifeng; Mao, Shiwen] Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
   [Kompella, Sastry] Naval Res Lab, Div Informat Technol, Washington, DC 20375 USA.
C3 Auburn University System; Auburn University; United States Department of
   Defense; United States Navy; Naval Research Laboratory
RP He, ZF (corresponding author), Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
EM zzh0008@tigermail.auburn.edu; smao@ieee.org; sk@ieee.org
RI Mao, Shiwen/AAY-4471-2020
OI Kompella, Sastry/0000-0001-7696-6213; Mao, Shiwen/0000-0002-7052-0007
FU U.S. National Science Foundation [CNS-0953513]; Wireless Engineering
   Research and Education Center at Auburn University; Division Of Computer
   and Network Systems; Direct For Computer & Info Scie & Enginr [0953513]
   Funding Source: National Science Foundation
FX This work was supported in part by the U.S. National Science Foundation
   under Grant CNS-0953513, and in part by the Wireless Engineering
   Research and Education Center at Auburn University. This work was
   presented in part at IEEE GLOBECOM 2014, Austin, TX, USA, December 2014.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Maria Martini.
CR [Anonymous], 2009, Linear Programming and Network Flows
   [Anonymous], IEEE T WIRELESS COMM
   [Anonymous], IEEE 72 VEH TECH C F
   [Anonymous], P IEEE GLOB TEL C DE
   [Anonymous], NTT TECH REV
   [Anonymous], VIS NETW IND VNI
   [Anonymous], P 5 INT ICST MOB MUL
   Chen Y, 2010, IEEE T COMMUN, V58, P2381, DOI 10.1109/TCOMM.2010.08.090528
   Chen Y, 2008, IEEE T INFORM THEORY, V54, P2053, DOI 10.1109/TIT.2008.920248
   Eden A, 2008, IEEE T BROADCAST, V54, P691, DOI 10.1109/TBC.2008.2001718
   Garfinkel R.S., 1972, INTEGER PROGRAMMING
   Griva I, 2009, OTHER TITL APPL MATH, V108, P1, DOI 10.1137/1.9780898717730
   Hu D., 2012, EAI ENDORSED T MOBIL, V12, pe1, DOI DOI 10.4108/MCA.2012.07-09.E1
   Huynh-Thu Q, 2008, IEEE T BROADCAST, V54, P641, DOI 10.1109/TBC.2008.2001246
   Jiang TG, 2012, IEEE J SEL AREA COMM, V30, P1215, DOI 10.1109/JSAC.2012.120807
   Kar K, 2008, IEEE T WIREL COMMUN, V7, P2619, DOI 10.1109/TWC.2008.061044
   Khan A., 2011, IEEE International Conference on Communications (ICC), P1
   Khan A., 2009, PROC IEEE INT C COMM, P1
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Liang YC, 2008, IEEE T WIREL COMMUN, V7, P1326, DOI 10.1109/TWC.2008.060869
   Min AW, 2013, IEEE T MOBILE COMPUT, V12, P1532, DOI 10.1109/TMC.2012.116
   Mitola J, 2001, MOBILE NETW APPL, V6, P435, DOI 10.1023/A:1011426600077
   Ojanperä T, 2015, WIRELESS PERS COMMUN, V84, P1739, DOI 10.1007/s11277-015-2519-7
   Saki H, 2015, IEEE ICC, P7564, DOI 10.1109/ICC.2015.7249536
   Son IK, 2015, MOBILE NETW APPL, V20, P763, DOI 10.1007/s11036-014-0565-0
   Son IK, 2012, IEEE INFOCOM SER, P2149, DOI 10.1109/INFCOM.2012.6195598
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Su H, 2008, IEEE J SEL AREA COMM, V26, P118, DOI 10.1109/JSAC.2008.080111
   Wang BB, 2010, IEEE T COMMUN, V58, P890, DOI 10.1109/TCOMM.2010.03.090084
   Wang W., 2011, 2011 IEEE 54 INT MID, P1, DOI DOI 10.1080/01431161.2010.489073
   Wu YQ, 2014, IEEE J SEL AREA COMM, V32, P2134, DOI 10.1109/JSAC.2014.141115
   Xu Y, 2017, IEEE SYST J, V11, P7, DOI 10.1109/JSYST.2015.2475702
   Yamagishi K, 2006, IEICE T COMMUN, VE89B, P281, DOI 10.1093/ietcom/e89-b.2.281
   You JY, 2010, SIGNAL PROCESS-IMAGE, V25, P482, DOI 10.1016/j.image.2010.02.002
   Zhang SQ, 2009, IEEE T WIREL COMMUN, V8, P5575, DOI 10.1109/TWC.2009.081106
   Zhao YP, 2009, P IEEE, V97, P642, DOI 10.1109/JPROC.2009.2013017
   Zhou L, 2013, IEEE T WIREL COMMUN, V12, P3733, DOI 10.1109/TWC.2013.051413.120597
NR 38
TC 17
Z9 22
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1401
EP 1413
DI 10.1109/TMM.2016.2564104
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600015
DA 2024-07-18
ER

PT J
AU Qin, M
   Lu, Y
   Di, HJ
   Huang, W
AF Qin, Ming
   Lu, Yao
   Di, Huijun
   Huang, Wei
TI A Background Basis Selection-Based Foreground Detection Method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Basis matrix construction; basis matrix update; foreground detection;
   multiple clustering evaluation
ID MOVING OBJECT DETECTION; SUBTRACTION; MODEL; TRACKING
AB Foreground detection plays a fundamental role in video analysis. Frames with only background information are usually beneficial for many foreground detection algorithms, especially for regression-based methods where the background is recovered from a background basis matrix. However, many regression-based methods ignore the basis selection process or select bases by simple sampling, which may limit their performance. In this paper, a regression-based foreground detection method with a novel background basis selection process is proposed. The proposed basis selection method, which includes basis matrix construction and basis matrix update processes, aims to build an effective background basis matrix which helps to boost the performance of our foreground detection method. In our algorithm, the basis matrix construction process first builds the basis matrix locally with a multiple clustering evaluation process. With the locally constructed basis matrix, a modified linear regression-based foreground detection method is proposed for separating foreground and background globally. To further increase the representativeness and the adaptiveness of the background basis matrix, a basis matrix update algorithm is designed to incrementally replace the ineffective bases with new selected ones. Extensive experiments on challenging sequences demonstrate the effectiveness and the advantages of our method.
C1 [Qin, Ming; Lu, Yao; Di, Huijun; Huang, Wei] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Qin, M (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM q_inming@163.com; vis_yl@bit.edu.cn; ajon@bit.edu.cn;
   whuang.dev@gmail.com
FU National Natural Science Foundation of China [61273273, 61175096,
   61271374]; Research Fund for Doctoral Program of Higher Education of
   China [2012110110034]; Special Fund for Joint Building Program of
   Beijing Municipal Education Commission
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61273273, Grant 61175096, and Grant
   61271374, in part by the Research Fund for Doctoral Program of Higher
   Education of China under Grant 2012110110034, and in part by the Special
   Fund for Joint Building Program of Beijing Municipal Education
   Commission. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Yap-Peng Tan.
CR [Anonymous], CORR
   [Anonymous], 2009, MARKOV RANDOM FIELD
   [Anonymous], IEEE T INF THEORY
   Boyd S., 2004, CONVEX OPTIMIZATION
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen BH, 2014, IEEE T MULTIMEDIA, V16, P837, DOI 10.1109/TMM.2014.2298377
   Cheng FC, 2015, ACM T INTEL SYST TEC, V7, DOI 10.1145/2746409
   Cheng FC, 2015, ENG APPL ARTIF INTEL, V38, P138, DOI 10.1016/j.engappai.2014.10.023
   Dikmen M, 2008, INT C PATT RECOG, P2527
   Gengjian Xue, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3269, DOI 10.1109/ICIP.2011.6116368
   Graciela R., 2015, NEUROCOMPUTING, V175, P990
   Guo JM, 2013, IEEE T CIRC SYST VID, V23, P1809, DOI 10.1109/TCSVT.2013.2269011
   Guo X., 2014, Proceedings of the 51st Annual Design Automation Conference, P1, DOI 10.1109/WHISPERS.2014.8077627
   Guo XJ, 2014, LECT NOTES COMPUT SC, V8695, P535, DOI 10.1007/978-3-319-10584-0_35
   Gutchess D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P733, DOI 10.1109/ICCV.2001.937598
   Haines TSF, 2014, IEEE T PATTERN ANAL, V36, P670, DOI 10.1109/TPAMI.2013.239
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   Huang JZ, 2009, IEEE I CONF COMP VIS, P64, DOI 10.1109/ICCV.2009.5459202
   Huang SC, 2014, IEEE T CYBERNETICS, V44, P114, DOI 10.1109/TCYB.2013.2248057
   Huang SC, 2013, IEEE T NEUR NET LEAR, V24, P1920, DOI 10.1109/TNNLS.2013.2270314
   Huang SC, 2014, IEEE T IND ELECTRON, V61, P2099, DOI 10.1109/TIE.2013.2262764
   Huang SC, 2011, IEEE T CIRC SYST VID, V21, P1, DOI 10.1109/TCSVT.2010.2087812
   Jung CR, 2009, IEEE T MULTIMEDIA, V11, P571, DOI 10.1109/TMM.2009.2012924
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Liang D, 2015, PATTERN RECOGN, V48, P1374, DOI 10.1016/j.patcog.2014.10.020
   Maddalena L, 2008, IEEE T IMAGE PROCESS, V17, P1168, DOI 10.1109/TIP.2008.924285
   Mairal J., 2010, NIPS
   Morde A., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on, P15
   Oreifej O, 2013, IEEE T PATTERN ANAL, V35, P450, DOI 10.1109/TPAMI.2012.97
   Qin M., 2015, P INT C MULT EXP JUN, P1
   Seidel F, 2014, MACH VISION APPL, V25, P1227, DOI 10.1007/s00138-013-0555-4
   Seo JW, 2014, IEEE T MULTIMEDIA, V16, P2333, DOI 10.1109/TMM.2014.2353772
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wan YL, 2014, IEEE T MULTIMEDIA, V16, P637, DOI 10.1109/TMM.2014.2299515
   Wang D. S. H., 2006, NOVEL ROBUST STAT ME
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Xue G., 2013, P IET INT RAD C, P1
   Xue GJ, 2013, IEEE T CIRC SYST VID, V23, P1346, DOI 10.1109/TCSVT.2013.2243053
   Yang AY, 2013, IEEE T IMAGE PROCESS, V22, P3234, DOI 10.1109/TIP.2013.2262292
   Zhang W, 2007, IEEE T MULTIMEDIA, V9, P1202, DOI 10.1109/TMM.2007.902842
   Zhang XG, 2013, IEEE T MULTIMEDIA, V15, P1769, DOI 10.1109/TMM.2013.2280117
   Zhou T., 2011, P ICML, P33
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
NR 48
TC 11
Z9 12
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1283
EP 1296
DI 10.1109/TMM.2016.2557729
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600005
DA 2024-07-18
ER

PT J
AU Wu, GL
   Kang, WX
AF Wu, Guile
   Kang, Wenxiong
TI Robust Fingertip Detection in a Complex Environment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fingertip detection; hand appearance model; hand region segmentation;
   human-computer interaction (HCI)
ID RECOGNITION
AB Fingertip detection has a broad application in gesture recognition and finger tracking. It is also an important foundation of human-computer interaction systems. However, most algorithms are suitable for simple conditions with low accuracy because the hand is a nonrigid object, and its appearance model is complex. To address the challenging problem of accurately detecting fingertips in a complex environment, we propose a novel and robust fingertip detection algorithm in this paper. Unlike existing methods, our study requires no special device or mark, and users are free to move their hands. Via dense optical flow and a skin filter, we perform complete hand region segmentation in a complex environment. We find the maximum value of the local centroid distance outside the centroid circles and identify fingertips. Our algorithm performs favorably compared with common hand region segmentation and fingertip detection methods. Thorough experimentation proves that our proposed algorithm is effective and robust.
C1 [Wu, Guile; Kang, Wenxiong] S China Univ Technol, Guangzhou Key Lab Brain Comp Interact & Applicat, Sch Automat Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Wu, Guile; Kang, Wenxiong] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
C3 South China University of Technology; Nanjing University
RP Kang, WX (corresponding author), S China Univ Technol, Guangzhou Key Lab Brain Comp Interact & Applicat, Sch Automat Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.; Kang, WX (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
EM Guile.Wu.CN@ieee.org; auwxkang@scut.edu.cn
FU National Natural Science Foundation of China [61573151, 61105019];
   Science and Technology Program of Guangzhou [201510010088]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61573151 and Grant 61105019, and in part
   by the Science and Technology Program of Guangzhou under Grant
   201510010088. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Sen-Ching Samson
   Cheung. (Corresponding author: Wenxiong Kang.)
CR [Anonymous], ACTA TECHNICA NAPOCE
   Argyros AA, 2006, INT C PATT RECOG, P207
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Basilio Jorge Alberto Marcial, 2011, Applications of Mathematics and Computer Engineering. American Conference on Applied Mathematics (AMERICAN-MATH'11). 5th WSEAS International Conference on Computer Engineering and Applications (CEA'11), P123
   Bradski G., 2008, LEARNING OPENCV, P246
   Chaudhary A, 2012, I C MECH MACH VIS PR, P26
   Crowley J., 1995, International Workshop on Gesture and Face Recognition, P195
   Dorfmüller-Ulhaas K, 2001, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDINGS, P55, DOI 10.1109/ISAR.2001.970515
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kulshreshth A, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P187, DOI 10.1109/3DUI.2013.6550241
   Lee D, 2011, ETRI J, V33, P415, DOI 10.4218/etrij.11.0110.0313
   Li KP, 2014, 2014 7TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP 2014), P457, DOI 10.1109/CISP.2014.7003824
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Meier T, 1999, IEEE T CIRC SYST VID, V9, P1190, DOI 10.1109/76.809155
   Nakamura T, 2008, LECT NOTES COMPUT SC, V5068, P292
   Oikonomidis I., 2010, Asian Conference on Computer Vision, P744, DOI DOI 10.1007/978-3-642-19318-7_58
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Raheja J.L., 2011, INT J EMBEDDED SYSTE, V3, P85
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Ren Z, 2011, PROC FL STATE HORTIC, V124, P1
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Sajid H., 2015, Information Forensics and Security (WIFS), 2015 IEEE International Workshop on, P1
   Shah K.N., 2014, INT J COMPUT TRENDS, V7, P174, DOI [10.14445/22312803/IJCTT-V7P148, DOI 10.14445/22312803/IJCTT-V7P148]
   Singh S.K., 2003, TAMKANG J SCI ENG, V6, P227
   Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305
   Suau X, 2012, IEEE T MULTIMEDIA, V14, P575, DOI 10.1109/TMM.2012.2189853
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   [张睿 Zhang Rui], 2005, [计算机工程, Computer Engineering], V31, P223
NR 29
TC 22
Z9 25
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 978
EP 987
DI 10.1109/TMM.2016.2545401
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100003
DA 2024-07-18
ER

PT J
AU Parseihian, G
   Gondre, C
   Aramaki, M
   Ystad, S
   Kronland-Martinet, R
AF Parseihian, Gaetan
   Gondre, Charles
   Aramaki, Mitsuko
   Ystad, Solvi
   Kronland-Martinet, Richard
TI Comparison and Evaluation of Sonification Strategies for Guidance Tasks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Auditory feedback; guidance; parameter mapping sonification;
   sonification
ID PERCEPTION; DISCRIMINATION; NAVIGATION; MUSIC
AB This paper aims to reveal the efficiency of sonification strategies in terms of rapidity, precision, and overshooting in the case of a one-dimensional guidance task. The sonification strategies are based on the four main perceptual attributes of a sound (pitch, loudness, duration/tempo, and timbre) and classified with respect to the presence or not of one or several auditory references. Perceptual evaluations are used to display the strategies in a precision/rapidity space and enable prediction of user behavior for a chosen sonification strategy. The evaluation of sonification strategies constitutes a first step toward general guidelines for sound design in interactive multimedia systems that involve guidance issues.
C1 [Parseihian, Gaetan; Gondre, Charles; Aramaki, Mitsuko; Ystad, Solvi; Kronland-Martinet, Richard] Aix Marseille Univ, Lab Mcan & Acoust, CNRS, F-13402 Marseille, France.
C3 Aix-Marseille Universite; Centre National de la Recherche Scientifique
   (CNRS)
RP Parseihian, G; Gondre, C; Aramaki, M; Ystad, S; Kronland-Martinet, R (corresponding author), Aix Marseille Univ, Lab Mcan & Acoust, CNRS, F-13402 Marseille, France.
EM parseihian@lma.cnrs-mrs.fr; gondre@lma.cnrs-mrs.fr;
   aramaki@lma.cnrs-mrs.fr; ystad@lma.cnrs-mrs.fr; kronland@lma.cnrs-mrs.fr
RI Kronland-Martinet, Richard/M-3095-2016; Ystad, Sølvi/AAN-4517-2020
OI Kronland-Martinet, Richard/0000-0002-7325-4920; Ystad,
   Sølvi/0000-0001-9022-9690
FU French National Research Agency (ANR) under the SoniMove: Inform, Guide,
   and Learn Actions by Sounds project [ANR-14-CE24-0018-01]
FX This work was supported by the French National Research Agency (ANR)
   under the SoniMove: Inform, Guide, and Learn Actions by Sounds project
   ANR-14-CE24-0018-01. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Sheng-Wei Chen.
CR Alonso-Arevalo MA, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2355598.2355600
   [Anonymous], 1999, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI DOI 10.7551/MITPRESS/1486.001.0001
   [Anonymous], 2003, ISO 226:2003
   [Anonymous], 2011, SONIFICATION HDB
   [Anonymous], 1993, Thinking in sound: the cognitive psychology of human audition
   Aramaki M, 2006, COMPUT MUSIC J, V30, P32, DOI 10.1162/comj.2006.30.2.32
   Barrass S., 1998, THESIS AUST NAT U CA
   BEAUCHAMP JW, 1982, J AUDIO ENG SOC, V30, P396
   Begault DurandR., 1994, 3-D sound for virtual reality and multimedia
   Blauert J., 1997, SPATIAL HEARING PSYC
   Cho B, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/769659
   Dozza M, 2004, P ANN INT IEEE EMBS, V26, P4799
   Dubus G, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082491
   Effenberg AO, 2005, IEEE MULTIMEDIA, V12, P53, DOI 10.1109/MMUL.2005.31
   Eslambolchilar P., 2004, INT WORKSH INT SON O
   Fastl H, 2006, PSYCHOACOUSTICS FACT
   Ferguson S., 2006, 12 INT C AUD DISPL L
   FLETCHER H, 1962, J ACOUST SOC AM, V34, P749, DOI 10.1121/1.1918192
   Gibet S., 2009, MUSICAL GESTURES SOU, P212
   Godbout A., 2010, 16 INT C AUD DISPL W
   Hansen C, 2013, INT J MED ROBOT COMP, V9, P36, DOI 10.1002/rcs.1466
   Hermann T, 2005, IEEE MULTIMEDIA, V12, P20, DOI 10.1109/MMUL.2005.26
   Hermann T., 2008, 14 INT C AUD DISPL P
   Kramer K., 1999, SONIFICATION REPORT
   Kronland-Martinet R, 2012, AI SOC, V27, P245, DOI 10.1007/s00146-011-0340-8
   Lartillot O, 2008, ST CLASS DAT ANAL, P261, DOI 10.1007/978-3-540-78246-9_31
   Lokki T, 2005, IEEE MULTIMEDIA, V12, P80, DOI 10.1109/MMUL.2005.33
   Loomis JM, 1998, PRESENCE-TELEOP VIRT, V7, P193, DOI 10.1162/105474698565677
   MICHON JA, 1964, ACTA PSYCHOL, V22, P441, DOI 10.1016/0001-6918(64)90032-0
   Moore B. C. J., 1997, INTRO PSYCHOL HEARIN
   Nyman A., 2005, SEM ALT ACC FEEL GAM
   Parseihian G., 2015, WI J MOBILE MEDIA, V9
   Parseihian G., 2013, 10 INT S COMP MUS MU
   Parseihian G, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00269
   Parseihian G, 2012, J AUDIO ENG SOC, V60, P409
   Scholz DS, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00332
   STEVENS SS, 1955, J ACOUST SOC AM, V27, P815, DOI 10.1121/1.1908048
   Terhardt E., 1974, Facts and models in hearing, P353
   Trainor LJ, 2002, MUSIC PERCEPT, V20, P187, DOI 10.1525/mp.2002.20.2.187
   Wegner K., 1998, INT C AUD DISPL GLAS, V6
   WIER CC, 1977, J ACOUST SOC AM, V61, P178, DOI 10.1121/1.381251
   Wilson J, 2007, ELEVENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P91
   Wright BA, 1997, J NEUROSCI, V17, P3956
   YOUNG RW, 1952, J ACOUST SOC AM, V24, P267, DOI 10.1121/1.1906888
   Zatorre RJ, 2007, NAT REV NEUROSCI, V8, P547, DOI 10.1038/nrn2152
NR 45
TC 34
Z9 34
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 674
EP 686
DI 10.1109/TMM.2016.2531978
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, C
   Lin, CW
   Guo, ZM
AF Zhou, Chao
   Lin, Chia-Wen
   Guo, Zongming
TI <i>m</i>DASH: A Markov Decision-Based Rate Adaptation Approach for
   Dynamic HTTP Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic adaptive streaming over HTTP (DASH); Markov decision; quality of
   experience; rate adaptation
ID QUALITY; ALGORITHM
AB Dynamic adaptive streaming over HTTP (DASH) has recently been widely deployed in the Internet. It, however, does not impose any adaptation logic for selecting the quality of video fragments requested by clients. In this paper, we propose a novel Markov decision-based rate adaptation scheme for DASH aiming to maximize the quality of user experience under time-varying channel conditions. To this end, our proposed method takes into account those key factors that make a critical impact on visual quality, including video playback quality, video rate switching frequency and amplitude, buffer overflow/underflow, and buffer occupancy. Besides, to reduce computational complexity, we propose a low-complexity sub-optimal greedy algorithm which is suitable for real-time video streaming. Our experiments in network test-bed and real-world Internet all demonstrate the good performance of the proposed method in both objective and subjective visual quality.
C1 [Zhou, Chao; Guo, Zongming] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
C3 Peking University; National Tsing Hua University; National Tsing Hua
   University
RP Lin, CW (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.; Lin, CW (corresponding author), Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
EM zhouchaoyf@gmail.com; cwlin@ee.nthu.edu.tw; guozongming@pku.edu.cn
RI Zhou, Chao/A-9803-2012; Lin, Chia-Wen/ABH-6075-2020; Lin,
   Chia-Wen/M-4571-2013
OI Lin, Chia-Wen/0000-0002-9097-2318
FU Ministry of Science and Technology, Taiwan [MOST 100-2628-E-007-026-MY3,
   MOST 103-2221-E-007-046-MY3]
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan under Grant MOST 100-2628-E-007-026-MY3 and Grant
   MOST 103-2221-E-007-046-MY3. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Christian Timmerer.
CR Akhshabi S., 2011, P ACM MMSYS11 FEB, P169
   Andelin T., 2012, Proceedings of the 3rd ACM Multimedia Systems Conference, P149
   [Anonymous], COM12LS62E
   Begen AC, 2011, IEEE INTERNET COMPUT, V15, P54, DOI 10.1109/MIC.2010.155
   Bing Zhou, 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P734, DOI 10.1109/ICCNC.2012.6167520
   Cranley N, 2006, INT J HUM-COMPUT ST, V64, P637, DOI 10.1016/j.ijhcs.2005.12.002
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   De Cicco L., 2011, P 2 ANN ACM C MULTIM, P145
   García S, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2331912
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Huang T.Y., 2013, P 2013 ACM SIGCOMM W, P9, DOI [DOI 10.1145/2491172, DOI 10.1145/2491172.2491179]
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Liu Y., 2003, P IEEE INT C COMM JU, P682
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Oyman O, 2012, IEEE COMMUN MAG, V50, P20, DOI 10.1109/MCOM.2012.6178830
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Suh D, 2014, 2014 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN 2014), P497, DOI 10.1109/ICOIN.2014.6799731
   Tavakoli S., 2014, P SOC PHOTO-OPT INS
   Tavakoli S, 2014, IEEE J SEL AREA COMM, V32, P684, DOI 10.1109/JSAC.2014.140402
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   Watson M., 2011, ACM MULT SYST C SAN
   Wei DX, 2006, IEEE ACM T NETWORK, V14, P1246, DOI 10.1109/TNET.2006.886335
   Xiang S., 2012, ACM MMSys '12, P167
   Xing M, 2014, IEEE J SEL AREA COMM, V32, P795, DOI 10.1109/JSAC.2014.140411
   Xing M, 2012, IEEE GLOB COMM CONF, P5745, DOI 10.1109/GLOCOM.2012.6504037
   Zambelli A., 2010, MS SSTR MICROSOFT SM
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhou C, 2014, IEEE T CIRC SYST VID, V24, P681, DOI 10.1109/TCSVT.2013.2290580
   Zhou CF, 2015, ADV INTELL SYST, V355, P1, DOI [10.1109/VCIP.2015.7457843, 10.1007/978-3-319-17398-6_1]
NR 30
TC 104
Z9 112
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 738
EP 751
DI 10.1109/TMM.2016.2522650
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300015
DA 2024-07-18
ER

PT J
AU Sang, JT
   Deng, ZY
   Lu, DY
   Xu, CS
AF Sang, Jitao
   Deng, Zhengyu
   Lu, Dongyuan
   Xu, Changsheng
TI Cross-OSN User Modeling by Homogeneous Behavior Quantification and Local
   Social Regularization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Behavior fusion; cross-OSN user modeling; local social regularization;
   personalization; video recommendation
AB In the context of social media services, data shortage has severally hindered accurate user modeling and practical personalized applications. This paper is motivated to leverage the user data distributed in disparate online social networks (OSN) to make up for the data shortage in user modeling, which we refer to as "cross-OSN user modeling." Generally, the data that the same user distributes in different OSNs consist of both behavior data (i.e., interaction with multimedia items) and social data (i.e., interaction between users). This paper focuses on the following two challenges: 1) how to aggregate the users' cross-OSN interactions with multimedia items of the same modality, which we call cross-OSN homogeneous behaviors, and 2) how to integrate users' cross-OSN social data with behavior data. Our proposed solution to address the challenges consist of two corresponding components as follows. 1) Homogeneous behavior quantification, where homogeneous user behaviors are quantified based on their importance in reflecting user preferences. After quantification, the examined cross-OSN user behaviors are aggregated to construct a unified user-item interaction matrix. 2) Local social regularization, where the cross-OSN social data is integrated as regularization in matrix factorization-based user modeling at local topic level. The proposed cross-OSN user modeling solution is evaluated in the application of personalized video recommendation. Carefully designed experiments on self-collected Google+ and YouTube datasets have validated its effectiveness and the advantage over single-OSN-based methods.
C1 [Sang, Jitao; Deng, Zhengyu; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Lu, Dongyuan] Univ Int Business & Econ, Sch Informat Technol & Management, Beijing 100029, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; University of
   International Business & Economics
RP Sang, JT (corresponding author), CASIA, Natl Lab Pattern Recognit, Wageningen, Netherlands.
EM jtsang@nlpr.ia.ac.cn; zydeng@nlpr.ia.ac.cn; ludy@uibe.edu.cn;
   csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61332016, 61303176];
   Beijing Natural Science Foundation [4131004]
FX This work was supported in part by National Basic Research Program of
   China under Grant 2012CB316304, in part by the National Natural Science
   Foundation of China under Grant 61225009, Grant 61332016, and Grant
   61303176, and in part by the Beijing Natural Science Foundation under
   Grant 4131004. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Cees Snoek.
CR Abel F, 2013, USER MODEL USER-ADAP, V23, P169, DOI 10.1007/s11257-012-9131-2
   Abel F, 2011, LECT NOTES COMPUT SC, V6757, P28, DOI 10.1007/978-3-642-22233-7_3
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cortes C., 2010, Proceedings of the 27th International Conference on Machine Learning, P239
   Cristianini N, 2002, ADV NEUR IN, V14, P367
   Deng ZY, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2637285
   Deng Zhengyu., 2013, IEEE INT C MULTIMEDI, P1
   Gao Y, 2014, IEEE T IMAGE PROCESS, V23, P5400, DOI 10.1109/TIP.2014.2364536
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Hofmann T, 2004, ACM T INFORM SYST, V22, P89, DOI 10.1145/963770.963774
   Jamali M., 2010, P 4 ACM C REC SYST, P135, DOI DOI 10.1145/1864708.1864736
   Jamali M, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P397
   Jiang Meng., 2012, PROC 21 ACM INT C IN, P1422
   Jinlong W., 2008, P 2 KDD WORKSH LARG, P1
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Ma H., 2011, Proceedings of the 4th ACM International Conference on Web Search and Data Mining, P287
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Resnick P., 1994, Transcending Boundaries, CSCW '94. Proceedings of the Conference on Computer Supported Cooperative Work, P175, DOI 10.1145/192844.192905
   Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1
   Sang JT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1333, DOI 10.1145/2733373.2807423
   Szomszor M, 2008, LECT NOTES COMPUT SC, V5318, P632, DOI 10.1007/978-3-540-88564-1_40
   Yan M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/2671188.2749344
   Yan M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P557, DOI 10.1145/2647868.2654920
   Yuan NicholasJing., 2013, P 1 ACM C ONLINE SOC, P3
   Zhao HY, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P200, DOI 10.1109/GSIS.2013.6714773
   Zhao Xiaojian, 2011, P 19 ACM INT C MULTI, P1521
NR 27
TC 16
Z9 17
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2259
EP 2270
DI 10.1109/TMM.2015.2486524
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500013
DA 2024-07-18
ER

PT J
AU Zhao, L
   Hu, QH
   Wang, WW
AF Zhao, Lei
   Hu, Qinghua
   Wang, Wenwu
TI Heterogeneous Feature Selection With Multi-Modal Deep Neural Networks
   and Sparse Group LASSO
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; feature selection; heterogeneous data; multi-modal;
   sparse representation
ID FEATURE FUSION; REGRESSION; DIMENSIONALITY; RECOGNITION; INFORMATION
AB Heterogeneous feature representations are widely used in machine learning and pattern recognition, especially for multimedia analysis. The multi-modal, often also high-dimensional, features may contain redundant and irrelevant information that can deteriorate the performance of modeling in classification. It is a challenging problem to select the informative features for a given task from the redundant and heterogeneous feature groups. In this paper, we propose a novel framework to address this problem. This framework is composed of two modules, namely, multi-modal deep neural networks and feature selection with sparse group LASSO. Given diverse groups of discriminative features, the proposed technique first converts the multi-modal data into a unified representation with different branches of the multi-modal deep neural networks. Then, through solving a sparse group LASSO problem, the feature selection component is used to derive a weight vector to indicate the importance of the feature groups. Finally, the feature groups with large weights are considered more relevant and hence are selected. We evaluate our framework on three image classification datasets. Experimental results show that the proposed approach is effective in selecting the relevant feature groups and achieves competitive classification performance as compared with several recent baseline methods.
C1 [Zhao, Lei; Hu, Qinghua] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Wang, Wenwu] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 Tianjin University; University of Surrey
RP Hu, QH (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM huqinghua@tju.edu.cn; w.wang@surrey.ac.uk
RI Hu, Qinghua/B-8857-2008; wang, wenwu/HOF-4371-2023
FU 973 Program [2013CB329304]; National Natural Foundation of China
   [61222210, 61432011]
FX This work was supported in part by the 973 Program under Grant
   2013CB329304 and by the National Natural Foundation of China under Grant
   61222210 and Grant 61432011. The guest editor coordinating the review of
   this manuscript and approving it for publication was Dr. Guo-Jun Qi.
   (Corresponding author: Qinghua Hu.)
CR [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], 2011, P ICML
   [Anonymous], 2000, Em: Proceedings of the 2000 ACM workshops on Multimedia, DOI DOI 10.1145/357744.357758
   [Anonymous], 2007, PROC IEEE INT C COMP
   [Anonymous], 2007, P 24 INT C MACH LEAR, DOI DOI 10.1145/1273496.1273594
   [Anonymous], 2004, P 21 INT C MACH LEAR
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Berg AC, 2005, PROC CVPR IEEE, P26
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Donahue J, 2014, PR MACH LEARN RES, V32
   Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110
   Friedman J., 2010, CORR
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Han YH, 2010, AAAI CONF ARTIF INTE, P469
   He XF, 2004, ADV NEUR IN, V16, P153
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu QH, 2008, INFORM SCIENCES, V178, P3577, DOI 10.1016/j.ins.2008.05.024
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lin Y., 2007, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1
   Lin YY, 2011, IEEE T PATTERN ANAL, V33, P1147, DOI 10.1109/TPAMI.2010.183
   Liu J., 2009, P 25 C UNCERTAINTY A, P339, DOI DOI 10.5555/1795114.1795154
   Liu J., 2010, ADV NEURAL INFORM PR, P1459
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma Z., 2011, Proc. 19th ACM Int'l Conf. Multimedia, P283, DOI DOI 10.1145/2072298.2072336
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x
   Ouyang WL, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.299
   Peng J, 2010, ANN APPL STAT, V4, P53, DOI 10.1214/09-AOAS271
   Qi GJ, 2015, IEEE T KNOWL DATA EN, V27, P1741, DOI 10.1109/TKDE.2014.2313871
   Qi GJ, 2012, P IEEE, V100, P2688, DOI 10.1109/JPROC.2012.2201909
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Shekhar S, 2014, IEEE T PATTERN ANAL, V36, P113, DOI 10.1109/TPAMI.2013.109
   Shen HF, 2010, IEEE INT CON MULTI, P980, DOI 10.1109/ICME.2010.5583900
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang H, 2013, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2013.398
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu F., 2010, Proceedings of the International Conference on Multimedia (MM'10), P15, DOI DOI 10.1145/1873951.1873957
   Wu P., 2013, P 21 ACM INT C MULT, P153, DOI DOI 10.1145/2502081.2502112
   Yang J, 2003, PATTERN RECOGN, V36, P1369, DOI 10.1016/S0031-3203(02)00262-5
   Yang L, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P99, DOI 10.1145/2505515.2505720
   Yang Y., 2011, P 22 INT JOINT C ART, P1589
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yuan M., 2006, Journal of the Royal Statistical Society, Series B, V70, P53
   Zhou YC, 2015, NEUROCOMPUTING, V168, P408, DOI 10.1016/j.neucom.2015.05.086
NR 55
TC 85
Z9 92
U1 3
U2 58
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1936
EP 1948
DI 10.1109/TMM.2015.2477058
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400007
OA Green Published
DA 2024-07-18
ER

PT J
AU Min, WQ
   Bao, BK
   Xu, CS
   Hossain, MS
AF Min, Weiqing
   Bao, Bing-Kun
   Xu, Changsheng
   Hossain, M. Shamim
TI Cross-Platform Multi-Modal Topic Modeling for Personalized
   Inter-Platform Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-platform; topic model; recommendation
AB In this paper, we investigate a novel cross-platform multimedia problem: given two platforms, Flickr and Foursquare, we conduct the recommendation between these two platforms, namely the photo recommendation from Flickr to Foursquare users and the venue recommendation from Foursquare to Flickr users. Such inter-platform recommendations enable users from one single platform to enjoy different recommendation services effectively. To solve the problem, we propose a cross-platform multi-modal topic model ((CMTM)-T-3), which is capable of: 1) differentiating between two kinds of topics, i.e., platform-specific topics only relevant to a certain platform and shared topics characterizing the knowledge shared by different platforms and 2) aligning multiple modalities from different platforms. Specifically, (CMTM)-T-3 can not only split the topic space into the shared topic space and platform-specific topic space and learn them simultaneously, but also enable the alignment among different modalities through the learned topic space. Given the location information, we applied the proposed (CMTM)-T-3 into two inter-platform recommendation applications: 1) personalized venue recommendation from Foursquare to Flickr users and 2) personalized image recommendation from Flickr to Foursquare users. We have conducted experiments on the collected large-scale real-world dataset from Flickr and Foursquare. Qualitative and quantitative evaluation results validate the effectiveness of our method and demonstrate the advantage of connecting different platforms with different modalities for the inter-platform recommendation.
C1 [Min, Weiqing; Bao, Bing-Kun; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Hossain, M. Shamim] King Saud Univ, SWE Dept, Coll Comp & Informat Sci, Riyadh 11543, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Saud
   University
RP Min, WQ (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM wqmin@nlpr.ia.ac.cn; bingkunbao@gmail.com; csxu@nlpr.ia.ac.cn;
   mshossain@ksu.edu.sa
RI xu, cj/HJZ-3488-2023; Hossain, M. Shamim/K-1362-2014; Guizani,
   Mohsen/AAX-4534-2021
OI Hossain, M. Shamim/0000-0001-5906-9422; Guizani,
   Mohsen/0000-0002-8972-8094
FU National Program on Key Basic Research Project (973 Program)
   [2012CB316304]; National Natural Science Foundation of China [61225009,
   61201374, 61432019]; Beijing Natural Science Foundation [4131004,
   4152053]; State Key Laboratory for Novel Software Technology of Nanjing
   University; Deanship of Scientific Research at King Saud University
   under International Research Group Program IRG [14-18]
FX This work was supported in part by the National Program on Key Basic
   Research Project (973 Program, Project 2012CB316304), by the National
   Natural Science Foundation of China under Grant 61225009, Grant
   61201374, and Grant 61432019, by the Beijing Natural Science Foundation
   under Grant 4131004 and Grant 4152053, by the open project from the
   State Key Laboratory for Novel Software Technology of Nanjing
   University, and by the Deanship of Scientific Research at King Saud
   University under International Research Group Program IRG 14-18. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cees Snoek.
CR [Anonymous], 2012, ACM MM'12'
   [Anonymous], 2012, P 18 ACM SIGKDD INT
   [Anonymous], 2012, P 2 SPAN C INF RETR
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2012, P 20 ACM INT C MULT
   Bao B.-K., 2012, P 20 ACM INT C MULT, P1357
   Bao BK, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2730889
   Bao J., 2012, P 20 INT C ADV GEOGR, P199, DOI [DOI 10.1145/2424321.2424348, 10.1145/2424321.2424348]
   Blei D, 2010, IEEE SIGNAL PROC MAG, V27, P55, DOI 10.1109/MSP.2010.938079
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Chen X., 2009, the 18th ACM conference on Information and knowledge management, P495
   Donoser M., 2006, COMPUTER VISION PATT, V1, P553, DOI DOI 10.1109/CVPR.2006.107
   Fan JP, 2009, IEEE T CIRC SYST VID, V19, P273, DOI 10.1109/TCSVT.2008.2009258
   Gao Y., 2010, Proceedings of Descriptional Complexity of Formal Systems 12th Workshop (DCFS 2010), P123
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hoffman M., 2008, Proc. ISMIR, P349
   Jiang Meng, 2012, P 21 ACM INT C INFOR, P45
   Joseph K, 2012, UBICOMP'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P919
   Li B., 2009, P 26 ANN INT C MACH, P617, DOI DOI 10.1145/1553374.1553454
   Li B, 2011, PROC INT C TOOLS ART, P1085, DOI 10.1109/ICTAI.2011.184
   Li B, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P2052
   Li Lianghao, 2012, P 26 AAAI C ART INT, P998
   Li T, 2011, IEEE T IMAGE PROCESS, V20, P2301, DOI 10.1109/TIP.2010.2103081
   Liu B, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1043
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Newman D, 2009, J MACH LEARN RES, V10, P1801
   Pan WK, 2010, AAAI CONF ARTIF INTE, P230
   Pan Weike, 2011, P 22 INT JOINT C ART, P2318
   Qi GJ, 2013, PROC INT CONF DATA, P793, DOI 10.1109/ICDE.2013.6544875
   Shi Y, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2483669.2483680
   Wang Chong, 2011, P 17 ACM SIGKDD INT, P448, DOI [DOI 10.1145/2020408.2020480, 10.1145/2020408.2020480]
   Yao T., 2011, Proceedings of the 19th ACM International Conference on Multimedia', MM'11, P945
   Ye M, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P325
   Yin HZ, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P221
   Ying Josh Jia-Ching, 2012, P ACM SIGKDD INT WOR, P63, DOI DOI 10.1145/2346496.2346507
   Yuan NicholasJing., 2013, P 1 ACM C ONLINE SOC, P3
   Yuan Q, 2011, P 5 ACM C REC SYST, DOI DOI 10.1145/2043932.2043975
   Zahálka J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P205, DOI 10.1145/2647868.2656403
   Zhang FZ, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P373, DOI 10.1145/2566486.2567976
   Zhang J, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P303, DOI 10.1145/2556195.2559894
   Zhang JW, 2013, IEEE DATA MINING, P1289, DOI 10.1109/ICDM.2013.134
   Zhang Y., 2012, CoRR
   Zhao HY, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P200, DOI 10.1109/GSIS.2013.6714773
   Zhao Y.-L., 2014, ACM T INTELL SYST TE, V5, P50
   Zheng VW, 2010, P 19 INT C WORLD WID, P1029
   Zheng X., 2014, CORR
   Zhong E., 2014, CORR
NR 48
TC 52
Z9 54
U1 3
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1787
EP 1801
DI 10.1109/TMM.2015.2463226
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400009
DA 2024-07-18
ER

PT J
AU Chen, F
   Zhang, C
   Wang, F
   Liu, JC
   Wang, XF
   Liu, Y
AF Chen, Fei
   Zhang, Cong
   Wang, Feng
   Liu, Jiangchuan
   Wang, Xiaofeng
   Liu, Yuan
TI Cloud-Assisted Live Streaming for Crowdsourced Multimedia Content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud computing; live streaming; crowdsourced multimedia content
ID PEER-TO-PEER; SPANNING-TREES; OPPORTUNITIES
AB Empowered by today's rich tools for media generation and distribution, and the convenient Internet access, streaming crowdsourced multimedia content (crowdsourced streaming, in brief) generalizes the single-source streaming paradigm by including massive contributors for a video/data channel. It calls a joint optimization along the path from crowdsourcers, through streaming servers, to the end-users to minimize the overall latency. The dynamics of the video sources, together with the globalized request demands and the high computation demand from each sourcer, make crowdsourced live streaming challenging even with powerful support from modern cloud computing. In this paper, we present a generic framework that facilitates a cost-effective cloud service for crowdsourced live streaming. Through adaptively leasing, the cloud servers can be provisioned in a fine granularity to accommodate geo-distributed video crowdsourcers. We present an optimal solution to deal with service migration among cloud instances of diverse lease prices. It also addresses the location impact to the streaming quality. To understand the performance of the proposed strategies in the real world, we have built a prototype system running over the planetlab and the Amazon/Microsoft Cloud. Our extensive experiments demonstrate that the effectiveness of our solution in terms of deployment cost and streaming quality.
C1 [Chen, Fei; Liu, Yuan] Jiangnan Univ, Sch Digital Media, Wuxi 214122, Peoples R China.
   [Zhang, Cong; Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, Vancouver, BC V5A 1S6, Canada.
   [Wang, Feng] Univ Mississippi, Dept Comp & Informat Sci, Oxford, MS 38677 USA.
   [Wang, Xiaofeng] Jiangnan Univ, Sch Internet Things Engn, Wuxi 214122, Peoples R China.
C3 Jiangnan University; Simon Fraser University; University of Mississippi;
   Jiangnan University
RP Liu, JC (corresponding author), Simon Fraser Univ, Sch Comp Sci, Vancouver, BC V5A 1S6, Canada.
EM chenf@jiangnan.edu.cn; congz@sfu.ca; fwang@cs.olemiss.edu;
   jcliu@cs.sfu.ca; wangxf@jiangnan.edu.cn; lyuan1800@sina.com
RI Li, Ye/JBS-2949-2023; Hu, Shaolin/N-1791-2018; li, ye/GWN-2672-2022
FU Natural Sciences and Engineering Research Council of Canada (NSERC);
   University of Mississippi; Jiangnan University; National Natural Science
   Foundation of China [61103223]; Natural Science Foundation of Jiangsu
   Province [BK2011003]
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada (NSERC), by the University of Mississippi under a
   Start-Up Grant, by Jiangnan University under a Start-Up Grant, by the
   National Natural Science Foundation of China under Grant 61103223, and
   by the Natural Science Foundation of Jiangsu Province under Grant
   BK2011003. An early version of this paper was presented at IEEE INFOCOM
   2015, Hong Kong, China, April-May 2015. The guest editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Haohong Wang. (Corresponding author: Jiangchuan Liu.)
CR Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Aggarwal V, 2013, IEEE T MULTIMEDIA, V15, P789, DOI 10.1109/TMM.2013.2240287
   Biel JI, 2014, IEEE T MULTIMEDIA, V16, P2062, DOI 10.1109/TMM.2014.2346471
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Feng Y, 2012, IEEE INFOCOM SER, P1134, DOI 10.1109/INFCOM.2012.6195472
   GABOW HN, 1978, SIAM J COMPUT, V7, P280, DOI 10.1137/0207024
   Kapoor S, 2000, ALGORITHMICA, V27, P120, DOI 10.1007/s004530010008
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   Liu JC, 2008, P IEEE, V96, P11, DOI 10.1109/JPROC.2007.909921
   Ma HD, 2014, IEEE COMMUN MAG, V52, P29, DOI 10.1109/MCOM.2014.6871666
   Niu D, 2011, IEEE INFOCOM SER, P421, DOI 10.1109/INFCOM.2011.5935196
   Ou ZH, 2015, IEEE T MOBILE COMPUT, V14, P194, DOI 10.1109/TMC.2014.2316517
   Simoens P., 2013, Proceeding of the 11th annual international conference on Mobile systems, applications, and services, P139
   Wang F., P IEEE INFO IN PRESS
   Wang F, 2008, IEEE INFOCOM SER, P2038
   Wang Z, 2014, IEEE INFOCOM SER, P91, DOI 10.1109/INFOCOM.2014.6847928
   Wu CS, 2015, IEEE T MOBILE COMPUT, V14, P444, DOI 10.1109/TMC.2014.2320254
   Wu Y, 2012, IEEE INFOCOM SER, P684, DOI 10.1109/INFCOM.2012.6195813
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Zhenyun Zhuang, 2011, 2011 IEEE 9th International Symposium on Parallel and Distributed Processing with Applications (ISPA), P183, DOI 10.1109/ISPA.2011.44
NR 20
TC 26
Z9 27
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1471
EP 1483
DI 10.1109/TMM.2015.2460193
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000008
DA 2024-07-18
ER

PT J
AU Go, Y
   Kwon, OC
   Song, H
AF Go, Yunmin
   Kwon, Oh Chan
   Song, Hwangjun
TI An Energy-Efficient HTTP Adaptive Video Streaming With Networking Cost
   Constraint Over Heterogeneous Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Heterogeneous wireless networks; HTTP adaptive streaming; multipath
ID BANDWIDTH AGGREGATION; SYSTEM
AB This paper presents a seamless high-quality HTTP adaptive streaming algorithm that considers wireless network conditions and the energy consumption of a mobile device with networking cost constraints over heterogeneous wireless networks. In the proposed algorithm, the requested video segments are concurrently delivered through multiple wireless networks to overcome the limitations of a single network. The segment quality level, number of requested segments, network inactive interval, and amount of requested data through each wireless network are determined adaptively in order to provide a seamless high-quality video with low energy consumption and networking cost constraints at the mobile device. The proposed algorithm is fully implemented in an Android-based mobile device and tested in an actual wireless network environment.
C1 [Go, Yunmin] Pohang Univ Sci & Technol, Div IT Convergence Engn, Pohang 790784, South Korea.
   [Kwon, Oh Chan] Samsung Elect, Software R&D Ctr, Cloud Platform Team, Suwon 443742, South Korea.
   [Song, Hwangjun] Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Pohang 790784, South Korea.
C3 Pohang University of Science & Technology (POSTECH); Samsung; Samsung
   Electronics; Pohang University of Science & Technology (POSTECH)
RP Go, Y (corresponding author), Pohang Univ Sci & Technol, Div IT Convergence Engn, Pohang 790784, South Korea.
EM gnfservant@postech.ac.kr; ochan.kwon@samsung.com; hwangjun@postech.ac.kr
FU ICT R&D program of MSIP/IITP [13-911-04-005]; Basic Science Research
   Program through the National Research Foundation of Korea (NRF) -
   Ministry of Education [NRF-2013R1A1A2006732]
FX This work was supported in part by the ICT R&D program of MSIP/IITP
   under Grant 13-911-04-005, Research and Development of 5G Mobile
   Communications Technologies Using CCN-Cased Multi-Dimensional
   Scalability, and by the Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Education under Grant NRF-2013R1A1A2006732. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Tommaso Melodia.
CR [Anonymous], RFC7320 IETF
   [Anonymous], 2013, YOUTUBE ADV ENC SETT
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], 2013, PROC 5 WORKSHOP MOBI, DOI DOI 10.1145/2457413.2457417
   [Anonymous], 2011, 6182 RFC
   [Anonymous], 2012, 230091 ISOIEC
   [Anonymous], P 4 WORKSH MOB VID C
   [Anonymous], 2003, 1449614 ISOIEC
   [Anonymous], 2014, 230011 ISOIEC JTC1SC
   [Anonymous], CISC VIS NETW IND GL
   Becke M., 2012, IEEE International Conference on Communications (ICC 2012), P2666, DOI 10.1109/ICC.2012.6363695
   Bui DH, 2013, REAL TIM SYST SYMP P, P57, DOI 10.1109/RTSS.2013.14
   Chen SY, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P1291
   Dan J., 2006, Journal of Zhejiang University (Science), V7, P713, DOI 10.1631/jzus.2006.A0713
   Ding RQ, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P4588
   Evensen Kristian., 2011, Proceedings of the second annual ACM conference on Multimedia systems, P57
   Han SC, 2011, IEEE J SEL AREA COMM, V29, P1032, DOI 10.1109/JSAC.2011.110513
   Ho D, 2013, J VIS COMMUN IMAGE R, V24, P1293, DOI 10.1016/j.jvcir.2013.08.012
   Hoque M. A., 2011, 2011 IEEE Consumer Communications and Networking Conference (CCNC 2011), P891, DOI 10.1109/CCNC.2011.5766635
   Hoque M. A., 2013, P 23 ACM WORKSH NETW, P13
   Huang J., 2012, P 10 INT C MOB SYST, P225, DOI DOI 10.1145/2307636.2307658
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P1227, DOI 10.1109/TMM.2007.902852
   Kaspar Dominik., 2010, P ICC, P1
   Kennedy M, 2010, C LOCAL COMPUT NETW, P843, DOI 10.1109/LCN.2010.5735821
   Kuschnig R., 2011, MMSYS, P245
   Kwon C., IEEE T MOBI IN PRESS
   Lederer S., 2012, P 3 MULT SYST C, P89
   Lim Y.-s., 2014, Proceedings of the 4th Workshop on All Things Cellular: Operations, Applications, #38; Challenges, AllThingsCellular'14, P3, DOI DOI 10.1145/2627585.2627596
   Moldovan AN, 2012, IEEE T BROADCAST, V58, P480, DOI 10.1109/TBC.2012.2191688
   R, 1999, HYPERTEXT TRANSFER P
   Ramaboli AL, 2012, J NETW COMPUT APPL, V35, P1674, DOI 10.1016/j.jnca.2012.05.015
   Rizzo L., 1997, Computer Communication Review, V27, P31, DOI 10.1145/251007.251012
   Shen H, 2013, DES AUT TEST EUROPE, P258
   Singh V., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, P190, DOI DOI 10.1145/2483977.2484002
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Trestian R, 2013, IEEE T BROADCAST, V59, P340, DOI 10.1109/TBC.2013.2244790
   Le TA, 2012, IEEE COMMUN LETT, V16, P275, DOI 10.1109/LCOMM.2011.120211.111818
   Yao J, 2011, LECT NOTES COMPUT SC, V6640, P92, DOI 10.1007/978-3-642-20757-0_8
   Zhang X., 2009, J LIGHTNING RES, V1, P1, DOI DOI 10.1109/CAS-ICTD.2009.4960900
NR 39
TC 50
Z9 52
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1646
EP 1657
DI 10.1109/TMM.2015.2451951
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000022
DA 2024-07-18
ER

PT J
AU Tsai, TJ
   Stolcke, A
   Slaney, M
AF Tsai, T. J.
   Stolcke, Andreas
   Slaney, Malcolm
TI A Study of Multimodal Addressee Detection in Human-Human-Computer
   Interaction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Addressee detection; beamforming; dialogue system; head pose;
   human-human-computer; multimodal; multiparty; prosody; speech
   recognition
ID FUSION
AB The goal of addressee detection is to answer the question, "Are you talking to me?" When a dialogue system interacts with multiple users, it is crucial to detect when a user is speaking to the system as opposed to another person. We study this problem in a multimodal scenario, using lexical, acoustic, visual, dialogue state, and beamforming information. Using data from a multiparty dialogue system, we quantify the benefits of using multiple modalities over using a single modality. We also assess the relative importance of the various modalities, as well as of key individual features, in estimating the addressee. We find that energy-based acoustic features are by far the most important, that information from speech recognition and system state is useful as well, and that visual and beamforming features provide little additional benefit. While we find that head pose is affected by whom the speaker is addressing, it yields little nonredundant information due to the system acting as a situational attractor. Our findings would be relevant to multiparty, open-world dialogue systems in which the agent plays an active, conversational role, such as an interactive assistant deployed in a public, open space. For these scenarios, our study suggests that acoustic, lexical, and system-state information is an effective and practical combination of modalities to use for addressee detection. We also consider how our analyses might be affected by the ongoing development of more realistic, natural dialogue systems.
C1 [Tsai, T. J.; Stolcke, Andreas; Slaney, Malcolm] Microsoft Res, Mountain View, CA 94043 USA.
C3 Microsoft
RP Tsai, TJ (corresponding author), Univ Calif Berkeley, Berkeley, CA 94704 USA.
EM tjtsai@icsi.berkeley.edu; stolcke@icsi.berkeley.edu; malcolm@ieee.org
OI Slaney, Malcolm/0000-0001-9733-4864
CR Amer MR, 2014, IEEE WINT CONF APPL, P556, DOI 10.1109/WACV.2014.6836053
   [Anonymous], 2014, P 16 INT C MULTIMODA
   [Anonymous], 2006, PROC 8 INT C MULTIMO, DOI DOI 10.1145/1180995.1181002
   [Anonymous], 2014, TOB REX TECHN SPEC F
   ARGYLE M, 1976, ENVIRON PSYCH NONVER, V1, P6, DOI 10.1007/BF01115461
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Baba Naoya, 2012, P 4 WORKSH EYE GAZ I
   Bakx I., 2003, P INTERACT 2003 ZUR, P163
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   Bohus D., 2011, P SIGDIAL 2011 C, P98
   Bohus D., 2010, P INT C MULT INT WOR
   Bohus D, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P637
   Bohus Dan, 2009, P 2009 INT C MULT IN, P31, DOI DOI 10.1145/1647314.1647323
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Brumitt B, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P375
   DeVault D, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1061
   Dowding J., 2006, P AAAI FALL S AURALL, P22
   Ferrer L., 2008, THESIS STANFORD U ST
   Freedman D. A., 2009, STAT MODELS THEORY P
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Huang H.-H., 2011, P 13 INT C MULT INT, P401
   Katzenmaier M., 2004, Human Factors, P144, DOI DOI 10.1145/1027933.(ICMI'1027959
   Knott A, 2008, ARTIF INTELL, V172, P69, DOI 10.1016/j.artint.2007.06.001
   Lalanne D., 2009, Proceedings of the 2009 international conference on Multimodal interfaces, P153, DOI 10.1145/1647314.1647343
   Lee H., 2013, NAACL HLT, P221
   Lee MK, 2010, 2010 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, P31
   Maglio PP, 2000, LECT NOTES COMPUT SC, V1948, P1
   Martin A., 1997, Tech. Rep. ADA530509
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Paek T., 2000, P ICSLP, P138
   Perez-Rosas V., 2013, P ANN M ASS COMP LIN, P973
   Shriberg E, 2013, INTERSPEECH, P2558
   Shriberg E, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P334
   Skantze Gabriel, 2009, P SIGDIAL C, P310
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Suk HI, 2014, NEUROIMAGE, V101, P569, DOI 10.1016/j.neuroimage.2014.06.077
   Terken J, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P94
   Tsai T. J., 2015, P ICASSP, P2314
   van Turnhout K., 2005, Proceedings of the 7th International Conference on Multimodal interfaces (ICMI '05), P175, DOI DOI 10.1145/1088463.1088495
   Vinyals O, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P417
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang Q, 2006, IEEE T CIRC SYST VID, V16, P1533, DOI 10.1109/TCSVT.2006.885727
   Zhang C, 2006, INT C PATT RECOG, P37
NR 44
TC 21
Z9 23
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1550
EP 1561
DI 10.1109/TMM.2015.2454332
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000015
DA 2024-07-18
ER

PT J
AU Rosani, A
   Boato, G
   De Natale, FGB
AF Rosani, Andrea
   Boato, Giulia
   De Natale, Francesco G. B.
TI EventMask: A Game-Based Framework for Event-Saliency Identification in
   Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event detection; gaming; photo galleries; saliency
AB The concept of "event" emerged in recent years as a key feature to efficiently index and retrieve media. Several approaches have been proposed to analyze the relationship between events and media, enable event discovery, and perform event-based media tagging, indexing, and retrieval. Despite the outstanding work done in this area, a major problem that remains open is how to infer the link between visual concepts and events. In particular, the possibility of understanding which perceptual elements allow a human recognizing the event depicted by an image would open new directions in event media discovery. In this paper we introduce the concept of event saliency to define the above event-revealing perceptual elements, and we propose an original method to detect it by exploiting crowd knowledge through gamification. We propose an adversarial game with a hidden purpose, where users are engaged in competitive roles: masking photos to prevent competitors recognizing the related event, and discovering events in photos masked by other players. Rules and incentives are defined to minimize cheating and force players to focus on details that really matter. A suitable algorithm composes the masks created by different players on the same media, thus producing a saliency map that, different from the traditional concept of saliency, does not focus on perceptual prominence but rather on event-related semantics of media. A thorough validation on public datasets is presented, and initial experiments to apply event saliency in detection tasks are proposed. Furthermore, an event-saliency dataset is disclosed to allow further research.
C1 [Rosani, Andrea; Boato, Giulia; De Natale, Francesco G. B.] Univ Trento, Dept Informat Engn Comp Sci, I-38123 Trento, Italy.
C3 University of Trento
RP Rosani, A (corresponding author), Univ Trento, Dept Informat Engn Comp Sci, I-38123 Trento, Italy.
EM andrea.rosani@unitn.it; giulia.boato@unitn.it;
   francesco.denatale@unitn.it
OI Rosani, Andrea/0009-0008-2622-6776
CR Abramson G., 2001, PHYS REV E, V63
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], ICMR 2013 P 3 ACM C
   [Anonymous], P 6 INT C SEM SYST I
   [Anonymous], 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1
   [Anonymous], 2011, P 20 ACM INT C INF K
   [Anonymous], 2011, P 3 ACM SIGMM INT WO
   [Anonymous], 2010, P 3 ACM INT C WEB SE, DOI DOI 10.1145/1718487.1718524
   [Anonymous], 2012, P 2 ACM INT C MULTIM, DOI DOI 10.1145/2324796.2324825
   Ballan L, 2011, MULTIMED TOOLS APPL, V51, P279, DOI 10.1007/s11042-010-0643-7
   Barkhuus L, 2005, LECT NOTES COMPUT SC, V3660, P358
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Becker H., 2011, ICWSM, V5, P438, DOI DOI 10.7916/D81V5NVX
   Bernstein MS, 2010, ACM T COMPUT-HUM INT, V17, DOI 10.1145/1746259.1746260
   Bhattacharya S., 2014, P ACM INT C MULT RET, P105
   Boato G., 2015, MULTIMED TOOLS APPL, V2015, P1
   Brenner M., 2012, P 2 ACM INT C MULT R
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen L., 2009, Proceedings of the 18th ACM conference on Information and knowledge management, P523
   Cooper Matthew., 2005, ACM T MULTIM COMPUT, V1, P269, DOI [DOI 10.1145/1083314.1083317, 10.1145/1083314.1083317]
   Dao M.-S., 2012, MULTIMED TOOLS APPL, V70, P1
   Dao M.-S., 2012, P 2 ACM INT C MULT R
   Duncan K, 2012, IET COMPUT VIS, V6, P514, DOI 10.1049/iet-cvi.2012.0032
   Firan C.S., 2010, Proceedings of the 19th ACM international conference on Information and knowledge management, P189
   Galli Luca, 2014, P 1 INT WORKSH GAM I, P7, DOI [10.1145/2594776.2594778, DOI 10.1145/2594776.2594778]
   Gkalelis N., 2011, 2011 9th International Workshop on Content-Based Multimedia Indexing (CBMI), P85, DOI 10.1109/CBMI.2011.5972525
   Guy I, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1373
   Jacobs M., 2013, P ACM CHI 2013 WORKS
   Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81
   Jiang Y.-G., 2010, TRECVID, V20, P21
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Law E, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1197
   Ma ZG, 2014, IEEE T PATTERN ANAL, V36, P1789, DOI 10.1109/TPAMI.2014.2306419
   Maltzahn C., 2014, P 1 INT WORKSH GAM I, P33
   Markus B., 2014, P WORKSH SOC EV WEB
   Mattivi R, 2011, INFOCOMMUNICATIONS J, V3, P9
   Matyas S., 2008, ACE 08 P 2008 INT C, P244, DOI 10.1145/1501750.1501806
   Mezaris V, 2014, MULTIMED TOOLS APPL, V70, P1, DOI 10.1007/s11042-013-1426-8
   Muratov O, 2011, INT CONF ACOUST SPEE, P1217
   Papadopoulos S, 2011, IEEE MULTIMEDIA, V18, P52, DOI 10.1109/MMUL.2010.68
   Pessoa L, 1998, BEHAV BRAIN SCI, V21, P781, DOI 10.1017/S0140525X98591753
   Rattenbury Tye, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P103, DOI 10.1145/1277741.1277762
   Reuter T., 2013, P MEDIAEVAL 2013 WOR
   Rosani A., 2014, DISI14009 U TRENT
   Siorpaes K, 2010, WORLD WIDE WEB, V13, P33, DOI 10.1007/s11280-009-0078-0
   Truc-Vien N., 2014, P WORKSH SOC EV WEB
   Tsampoulatidis I., 2011, P ACM INT C MULT RET
   van Zwol R., 2008, P WORLD WID WEB C
   Von Ahn L, 2004, P SIGCHI C HUM FACT, DOI DOI 10.1145/985692.985733
   von Ahn L, 2008, COMMUN ACM, V51, P58, DOI 10.1145/1378704.1378719
   VONAHN L, 2006, P SIGCHI C HUM FACT, P55, DOI DOI 10.1145/1124772.1124782
   Waitelonis J, 2011, INTERACT TECHNOL SMA, V8, P236, DOI 10.1108/17415651111189478
   Wang Y., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P865, DOI [DOI 10.1145/2393347.239633216, 10.1145/2393347.239633216]
   Westermann U, 2007, IEEE MULTIMEDIA, V14, P19, DOI 10.1109/MMUL.2007.23
   Zaharieva M., 2013, P 3 ACM C INT C MULT, P167
NR 55
TC 21
Z9 21
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1359
EP 1371
DI 10.1109/TMM.2015.2441003
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000020
DA 2024-07-18
ER

PT J
AU Chen, DM
   Girod, B
AF Chen, David M.
   Girod, Bernd
TI A Hybrid Mobile Visual Search System With Compact Global Signatures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compact signatures; image retrieval; interframe compression; mobile
   visual search
ID FISHER CODES; FEATURES; SIFT
AB Mobile visual search systems typically compare a query image against a database of annotated images for accurate object recognition. On-server database matching can search a large database hosted in the cloud, but the query latency could suffer with slow network transmissions or server congestion. On-device database matching can ensure fast recognition responses regardless of network or server conditions, but a small amount of memory on the mobile device can severely limit the number of images that can be stored in an on-device database. This paper presents a new hybrid system that combines the advantages of on-device and on-server database matching. At the core of this system, we first develop a compact and discriminative global signature to characterize each image. Our global signature uses an optimized local feature count that is derived from a statistical analysis of the retrieval performance. We additionally create two extensions that exploit color information within images and relationships between similar database images to improve retrieval accuracy. Then, we propose methods for efficient interframe coding of a sequence of global signatures which are extracted from the viewfinder frames on the mobile device. A low bitrate stream of global signatures can be sent to the server at an uplink bitrate of less than 2 kbps to broaden the search range of the current query and to update the on-device database to help future queries.
C1 [Chen, David M.; Girod, Bernd] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
C3 Stanford University
RP Chen, DM (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM dmchen@alumni.stanford.edu; bgirod@stanford.edu
CR Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   [Anonymous], 2009, P 5 INT C MOB MULT C
   [Anonymous], IEEE T MULT IN PRESS
   [Anonymous], 2012, P 20 ACM MULTIMEDIA
   [Anonymous], 2010, P 18 ACM INT C MULT
   [Anonymous], N14095 JTCISC29WG11
   [Anonymous], P SPIE
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], N12202 ISOIEC JTCISC
   [Anonymous], P IEEE DAT COMPR C M
   [Anonymous], MPEG2013M28061 ISOIE
   Arandjelovic R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.92
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733
   Chandrasekhar V, 2012, INT J COMPUT VISION, V96, P384, DOI 10.1007/s11263-011-0453-z
   Chao JS, 2011, IEEE IMAGE PROC, P301, DOI 10.1109/ICIP.2011.6116299
   Chen D, 2013, SIGNAL PROCESS, V93, P2316, DOI 10.1016/j.sigpro.2012.06.005
   Chen D, 2011, CONF REC ASILOMAR C, P850, DOI 10.1109/ACSSC.2011.6190128
   Chen DM, 2014, IEEE DATA COMPR CONF, P33, DOI 10.1109/DCC.2014.8
   Chen DM, 2014, IEEE MULTIMEDIA, V21, P14, DOI 10.1109/MMUL.2013.46
   Chen DM, 2008, IEEE DATA COMPR CONF, P143, DOI 10.1109/DCC.2009.33
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   DREZNER Z, 1993, COMMUN STAT THEORY, V22, P3051, DOI 10.1080/03610929308831202
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fockler P., 2005, Proceedings of the 4th international conference on Mobile and ubiquitous multimedia, P3
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2009, IEEE I CONF COMP VIS, P2357, DOI 10.1109/ICCV.2009.5459419
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   Lin J, 2013, INT CONF ACOUST SPEE, P1513, DOI 10.1109/ICASSP.2013.6637904
   Lin J, 2014, IEEE SIGNAL PROC LET, V21, P195, DOI 10.1109/LSP.2013.2296532
   Liu Wu, 2013, P ACM INT C MULT, P887
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Makar M, 2013, INT J SEMANT COMPUT, V7, P5, DOI 10.1142/S1793351X13400011
   Nister David, 2006, CVPR
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Philbin J., 2008, PROC IEEE C COMPUT V, P1
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schindler G., 2007, 2007 IEEE Conference on Computer Vision and Pattern Recognition, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Takacs G., 2008, MIR 08, P427, DOI DOI 10.1145/1460096.1460165
   Takacs G, 2010, PROC CVPR IEEE, P934, DOI 10.1109/CVPR.2010.5540116
   Tolias G, 2014, PATTERN RECOGN, V47, P3466, DOI 10.1016/j.patcog.2014.04.007
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Yeo C, 2008, IEEE IMAGE PROC, P217, DOI 10.1109/ICIP.2008.4711730
NR 52
TC 14
Z9 17
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1019
EP 1030
DI 10.1109/TMM.2015.2427744
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300009
DA 2024-07-18
ER

PT J
AU Wang, S
   Zhang, J
   Han, TX
   Miao, ZJ
AF Wang, Shu
   Zhang, Jian
   Han, Tony X.
   Miao, Zhenjiang
TI Sketch-Based Image Retrieval Through Hypothesis-Driven Object Boundary
   Selection With HLR Descriptor
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Large-scale sketch retrieval; line segment-based descriptor; object
   boundary selection
AB The appearance gap between sketches and photo-realistic images is a fundamental challenge in sketch-based image retrieval (SBIR) systems. The existence of noisy edges on photo-realistic images is a key factor in the enlargement of the appearance gap and significantly degrades retrieval performance. To bridge the gap, we propose a framework consisting of a new line segment-based descriptor named histogram of line relationship (HLR) and a new noise impact reduction algorithm known as object boundary selection. HLR treats sketches and extracted edges of photo-realistic images as a series of piece-wise line segments and captures the relationship between them. Based on the HLR, the object boundary selection algorithm aims to reduce the impact of noisy edges by selecting the shaping edges that best correspond to the object boundaries. Multiple hypotheses are generated for descriptors by hypothetical edge selection. The selection algorithm is formulated to find the best combination of hypotheses to maximize the retrieval score; a fast method is also proposed. To reduce the distraction of false matches in the scoring process, two constraints on spatial and coherent aspects are introduced. We tested the HLR descriptor and the proposed framework on public datasets and a new image dataset of three million images, which we recently collected for SBIR evaluation purposes. We compared the proposed HLR with state-of-the-art descriptors (SHoG, GF-HOG). The experimental results show that our HLR descriptor outperforms them. Combined with the object boundary selection algorithm, our framework significantly improves SBIR performance.
C1 [Wang, Shu; Miao, Zhenjiang] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 10044, Peoples R China.
   [Zhang, Jian] Univ Technol Sydney, Fac Engn & IT, Sydney, NSW 2007, Australia.
   [Han, Tony X.] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
C3 Beijing Jiaotong University; University of Technology Sydney; University
   of Missouri System; University of Missouri Columbia
RP Wang, S (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 10044, Peoples R China.
EM 09112083@bjtu.edu.cn; jian.zhang@uts.edu.au; hantx@missouri.edu;
   zjmiao@bjtu.edu.cn
OI Zhang, Jian/0000-0002-7240-3541
FU NSFC [61273274, 61370127]; 973 Program [2011CB302203]; National Key
   Technology RAMP;D Program of China [2012BAH01F03, NSFB4123104, FRFCU
   2014JBZ004, Z131110001913143]; Tsinghua-Tencent Joint Lab for IIT
FX This work was supported by the NSFC under Grant 61273274 and Grant
   61370127, by the 973 Program 2011CB302203, by the National Key
   Technology R&D Program of China under Grant 2012BAH01F03, Grant
   NSFB4123104, Grant FRFCU 2014JBZ004, and Grant Z131110001913143, and by
   the Tsinghua-Tencent Joint Lab for IIT. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Cees Snoek.
CR [Anonymous], 2007, CALTECH 256 OBJECT C
   [Anonymous], 2013, MULTIFEATURE CANONIC
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2010, PROC BRIT MACH VIS C
   [Anonymous], 2002, Introduction to MPEG-7: Multimedia Content Description Interface
   Bozas K, 2012, LECT NOTES COMPUT SC, V7431, P210, DOI 10.1007/978-3-642-33179-4_21
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao XC, 2013, IEEE I CONF COMP VIS, P313, DOI 10.1109/ICCV.2013.46
   Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460
   Cao Yang, 2010, P 18 ACM INT C MULT, P1605
   Chalechale A, 2005, IEEE T SYST MAN CY A, V35, P28, DOI 10.1109/TSMCA.2004.838464
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DelBimbo A, 1997, IEEE T PATTERN ANAL, V19, P121, DOI 10.1109/34.574790
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266
   Eitz Mathias., 2009, P 6 EUR S SKETCH BAS, P29
   Furuya Takahiko, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P37, DOI 10.1007/978-3-319-04114-8_4
   Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005
   Lee Y. J., 2011, P ACM SIGGRAPH, P27
   Liang Z., 2014, 2014 IEEE Workshop on Wide Bandgap Power Devices and Applications, P1
   Lin YL, 2013, IEEE I CONF COMP VIS, P3495, DOI 10.1109/ICCV.2013.434
   Ma C, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.65
   Mäenpää T, 2003, LECT NOTES COMPUT SC, V2749, P885
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Parui S, 2014, LECT NOTES COMPUT SC, V8694, P398, DOI 10.1007/978-3-319-10599-4_26
   Philbin J., 2008, P CVPR, P1
   Philbin J, 2010, LECT NOTES COMPUT SC, V6313, P677
   Robertson S, 2004, J DOC, V60, P503, DOI 10.1108/00220410410560582
   Rui Hu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3661, DOI 10.1109/ICIP.2011.6116513
   Saavedra JM, 2014, MULTIMED TOOLS APPL, V73, P2033, DOI 10.1007/s11042-013-1689-0
   Saavedra JM, 2010, LECT NOTES COMPUT SC, V6376, P432
   Salve SG, 2010, INT CONF COMP SCI, P471, DOI 10.1109/ICCSIT.2010.5565098
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Shrivastava A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024188
   Sousa P, 2010, J VISUAL LANG COMPUT, V21, P69, DOI 10.1016/j.jvlc.2009.12.001
   Sun Xinghai., 2013, Proceedings of the International Conference On Multimedia (MM), P233
   Xie HT, 2011, IEEE T MULTIMEDIA, V13, P1319, DOI 10.1109/TMM.2011.2167224
   Yao J., 2005, P IEEE INT C MULT EX, P1198
   Zitnick CL, 2010, LECT NOTES COMPUT SC, V6312, P170, DOI 10.1007/978-3-642-15552-9_13
   Zobel J, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132956.1132959
NR 45
TC 31
Z9 34
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1045
EP 1057
DI 10.1109/TMM.2015.2431492
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300011
DA 2024-07-18
ER

PT J
AU Pang, L
   Ngo, CW
AF Pang, Lei
   Ngo, Chong-Wah
TI Unsupervised Celebrity Face Naming in Web Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Celebrity face naming; social network; unconstrained web videos;
   unsupervised
ID ANNOTATION; RECOGNITION
AB This paper investigates the problem of celebrity face naming in unconstrained videos with user-provided metadata. Instead of relying on accurate face labels for supervised learning, a rich set of relationships automatically derived from video content and knowledge from image domain and social cues is leveraged for unsupervised face labeling. The relationships refer to the appearances of faces under different spatio-temporal contexts and their visual similarities. The knowledge includes Web images weakly tagged with celebrity names and the celebrity social networks. The relationships and knowledge are elegantly encoded using conditional random field (CRF) for label inference. Two versions of face annotation are considered: within-video and between-video face labeling. The former addresses the problem of incomplete and noisy labels in metadata, where null assignment of names is allowed-a problem seldom been considered in the literature. The latter further rectifies the errors in metadata, specifically to correct false labels and annotate faces with missing names in the metadata of a video, by considering a group of socially connected videos for joint label inference. Experimental results on a large archive of Web videos show the robustness of the proposed approach in dealing with the problems of missing and false labels, leading to higher accuracy in face labeling than several existing approaches but with minor degradation in speed efficiency.
C1 [Pang, Lei; Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Pang, L (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM leipang3-c@my.cityu.edu.hk; cwngo@cs.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [11210514]; City University of Hong Kong [7008178]; National
   Natural Science Foundation of China [61272290]
FX This work was supported by the Research Grants Council of the Hong Kong
   Special Administrative Region, China under CityU Grant 11210514, the
   City University of Hong Kong under Project 7008178, and the National
   Natural Science Foundation of China under Project 61272290. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Xiao-Ping Zhang.
CR [Anonymous], 2005, SPRINGER TEXTS STAT
   [Anonymous], 2012, P 20 ACM INT C MULT, DOI DOI 10.1145/2393347.2393439
   [Anonymous], ICTMCG09001 CHIN AC
   Bu J., 2012, P 20 ACM INT C MULT, P219
   Chen B. C., IEEE T CIRCUIT UNPUB
   Chen ZN, 2014, J COMPUT SCI TECH-CH, V29, P785, DOI 10.1007/s11390-014-1468-z
   Choi JY, 2011, IEEE T MULTIMEDIA, V13, P14, DOI 10.1109/TMM.2010.2087320
   Everingham M., 2006, P BRIT MACH VIS C, P899, DOI [DOI 10.5244/C.20.92, 10.5244/C.20.92]
   Forsyth, 2005, ADV NEURAL INFORM PR, V17, P137
   Guillaumin M., 2008, Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, P1
   Lafferty John, 2001, INT C MACH LEARN ICM
   Li W, 2006, LECT NOTES COMPUT SC, V4071, P463
   Özcan M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.29
   Paul G, 2014, IEEE IMAGE PROC, P318, DOI 10.1109/ICIP.2014.7025063
   Pons P, 2005, LECT NOTES COMPUT SC, V3733, P284
   ROMANO JP, 1990, J AM STAT ASSOC, V85, P686, DOI 10.2307/2290003
   Satoh S, 1999, IEEE MULTIMEDIA, V6, P22, DOI 10.1109/93.752960
   Smoliar S. W., 1994, IEEE Multimedia, V1, P62, DOI 10.1109/93.311653
   Stone Z, 2010, P IEEE, V98, P1408, DOI 10.1109/JPROC.2010.2044551
   Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tapaswi M, 2012, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2012.6247986
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang DY, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P443
   Wang DY, 2014, IEEE T PATTERN ANAL, V36, P550, DOI 10.1109/TPAMI.2013.145
   Wang DY, 2014, IEEE T KNOWL DATA EN, V26, P166, DOI 10.1109/TKDE.2012.240
   Yang J, 2005, Proceedings of the 2005 International Conference on Active Media Technology (AMT 2005), P28
   Yang J., 2004, P 12 ANN ACM INT C M, P580
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
   Zhang L., 2013, Proceedings of the 3rd ACM conference on International conference on multimedia retrieval, P9
   Zhang YF, 2009, IEEE T MULTIMEDIA, V11, P1276, DOI 10.1109/TMM.2009.2030629
   Zhao M, 2008, IEEE INT CONF AUTOMA, P901
   Zhu JK, 2008, IEEE T MULTIMEDIA, V10, P86, DOI 10.1109/TMM.2007.911245
NR 33
TC 7
Z9 8
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 854
EP 866
DI 10.1109/TMM.2015.2419452
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500008
OA Green Published
DA 2024-07-18
ER

PT J
AU Lei, JJ
   Zhang, CC
   Fang, YM
   Gu, ZY
   Ling, N
   Hou, CP
AF Lei, Jianjun
   Zhang, Cuicui
   Fang, Yuming
   Gu, Zhouye
   Ling, Nam
   Hou, Chunping
TI Depth Sensation Enhancement for Multiple Virtual View Rendering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth image-based rendering (DIBR); depth sensation enhancement; just
   noticeable depth difference (JNDD); multiple virtual view rendering
ID MULTIVIEW-VIDEO; SALIENCY DETECTION; TEXTURE; MAPS; DIBR
AB Depth information is an indispensable element in depth image-based rendering (DIBR) for three-dimensional (3-D) display. In this paper, we propose a novel depth sensation enhancement method to address the problems in multiple virtual view rendering. First, as the depth sensation is decreased when rendering intermediate multiple virtual views, the basic principle of depth sensation enhancement is derived according to the number of rendering views. Second, with the increase of the scene complexity, it is difficult to ensure the depth sensation of all neighboring objects. The saliency analysis is adopted to give preferred guarantee to the depth sensation between the salient object and its neighbors. Then, the depth sensation enhancement for multiple virtual view rendering is performed based on a defined energy function built by the number of rendering views and the saliency analysis. Finally, considering the temporal consistency between adjacent frames, the depth sensation enhancement is extended to video applications with a newly designed energy function with energy term of temporal consistency preservation. Experimental results on a public database demonstrate that the proposed method can obtain promising performance in depth sensation.
C1 [Lei, Jianjun; Zhang, Cuicui; Hou, Chunping] Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
   [Gu, Zhouye; Ling, Nam] Santa Clara Univ, Santa Clara, CA 95053 USA.
C3 Tianjin University; Jiangxi University of Finance & Economics; Santa
   Clara University
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
EM jjlei@tju.edu.cn; fa0001ng@e.ntu.edu.sg; guzh0001@gmail.com;
   nling@scu.edu
RI Lei, Jianjun/P-2539-2018
FU Natural Science Foundation of China [61271324, 61471262, 91320201,
   61471260, 61202266]; Natural Science Foundation of Tianjin
   [12JCYBJC10400]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61271324, Grant 61471262, Grant 91320201, Grant
   61471260, and Grant 61202266, and by the Natural Science Foundation of
   Tianjin under Grant 12JCYBJC10400. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Cha
   Zhang.
CR [Anonymous], IET ELECT LETT
   [Anonymous], P SPIE STEREOSCOPIC
   [Anonymous], P IEEE 3DTV C TRU VI
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], P SPIE STEREOSCOPIC
   Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603
   De Silva DVSX, 2011, IEEE J-STSP, V5, P335, DOI 10.1109/JSTSP.2011.2108113
   De Silva DVSX, 2010, ELECTRON LETT, V46, P1546, DOI 10.1049/el.2010.2320
   De Silva DVSX, 2010, IEEE INT CON MULTI, P1219, DOI 10.1109/ICME.2010.5582582
   Dellen B., 2011, 2011 IEEE Workshop on Applications of Computer Vision (WACV), P591
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Gao P, 2014, IEEE T MULTIMEDIA, V16, P1797, DOI 10.1109/TMM.2014.2331013
   Hannuksela MM, 2013, IEEE T IMAGE PROCESS, V22, P3449, DOI 10.1109/TIP.2013.2269274
   Heinzle S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964989
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Holliman NS, 2011, IEEE T BROADCAST, V57, P362, DOI 10.1109/TBC.2011.2130930
   Jo NY, 2013, OPT EXPRESS, V21, P29628, DOI 10.1364/OE.21.029628
   Jun Zhou, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4440, DOI 10.1109/ICPR.2010.1078
   Jung SW, 2013, IEEE T IMAGE PROCESS, V22, P3892, DOI 10.1109/TIP.2013.2263150
   Jung SW, 2012, IEEE T IMAGE PROCESS, V21, P3624, DOI 10.1109/TIP.2012.2191569
   Jung SW, 2012, IEICE T FUND ELECTR, VE95A, P673, DOI 10.1587/transfun.E95.A.673
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Lei JJ, 2013, NEUROCOMPUTING, V120, P24, DOI 10.1016/j.neucom.2012.08.057
   Li S, 2014, IEEE SIGNAL PROC LET, V21, P74, DOI 10.1109/LSP.2013.2291941
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Näsänen R, 2011, OPT EXPRESS, V19, P16075, DOI 10.1364/OE.19.016075
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D., 2007, PROC IEEE C COMPUT V, P1
   Seitz S. M., 1995, Proceedings IEEE Workshop on Representation of Visual Scenes (In Conjunction with ICCV'95) (Cat. No.95TB8126), P18, DOI 10.1109/WVRS.1995.476848
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Sun WX, 2014, IEEE T IMAGE PROCESS, V23, P342, DOI 10.1109/TIP.2013.2289994
   Wang S, 2003, IEEE T PATTERN ANAL, V25, P675, DOI 10.1109/TPAMI.2003.1201819
   Xu XY, 2012, INT CONF ACOUST SPEE, P805, DOI 10.1109/ICASSP.2012.6288006
   Yao C, 2014, IEEE T BROADCAST, V60, P394, DOI 10.1109/TBC.2014.2321671
NR 39
TC 51
Z9 55
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2015
VL 17
IS 4
BP 457
EP 469
DI 10.1109/TMM.2015.2400823
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QH
UT WOS:000351586300001
DA 2024-07-18
ER

PT J
AU Smith, JBL
   Chuan, CH
   Chew, E
AF Smith, Jordan B. L.
   Chuan, Ching-Hua
   Chew, Elaine
TI Audio Properties of Perceived Boundaries in Music
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Boundaries; corpus analysis; music analysis; music information retrieval
ID GENERATIVE THEORY; SEGMENTATION; LERDAHL; MELODY
AB Data mining tasks such as music indexing, information retrieval, and similarity search, require an understanding of how listeners process music internally. Many algorithms for automatically analyzing the structure of recorded music assume that a large change in one or another musical feature suggests a section boundary. However, this assumption has not been tested: while our understanding of how listeners segment melodies has advanced greatly in the past decades, little is known about how this process works with more complex, full-textured pieces of music, or how stable this process is across genres. Knowing how these factors affect how boundaries are perceived will help researchers to judge the viability of algorithmic approaches with different corpora of music. We present a statistical analysis of a large corpus of recordings whose formal structure was annotated by expert listeners. We find that the acoustic properties of boundaries in these recordings corroborate findings of previous perceptual experiments. Nearly all boundaries correspond to peaks in novelty functions, which measure the rate of change of a musical feature at a particular time scale. Moreover, most of these boundaries match peaks in novelty for several features at several time scales. We observe that the boundary-novelty relationship can vary with listener, time scale, genre, and musical feature. Finally, we show that a boundary profile derived from a collection of novelty functions correlates with the estimated salience of boundaries indicated by listeners.
C1 [Smith, Jordan B. L.; Chew, Elaine] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
   [Chuan, Ching-Hua] Univ N Florida, Coll Comp Engn & Construct, Jacksonville, FL 32224 USA.
C3 University of London; Queen Mary University London; State University
   System of Florida; University of North Florida
RP Smith, JBL (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
EM jblsmith@eecs.qmul.ac.uk; c.chuan@unf.edu; elaine.chew@eecs.qmul.ac.uk
RI Smith, Jordan B. L./M-7049-2016; Smith, Jordan/GVU-3480-2022
OI Smith, Jordan B. L./0000-0002-0316-1235; Chew,
   Elaine/0000-0002-8342-1024
FU Social Sciences and Humanities Research Council of Canada; National
   Science Foundation of the United States; JISC of the United Kingdom
FX We thank Ashley Burgoyne, Ichiro Fujinaga, David De Roure, and Stephen
   J. Downie, who with the first author assembled the SALAMI data set. The
   SALAMI project was supported by the Social Sciences and Humanities
   Research Council of Canada, the National Science Foundation of the
   United States and JISC of the United Kingdom.
CR Aucouturier JJ, 2005, IEEE T MULTIMEDIA, V7, P1028, DOI 10.1109/TMM.2005.858380
   Bruderer M., 2008, P C INT MUS THESS GR
   Bruderer MJ, 2009, MUSIC SCI, V13, P273, DOI 10.1177/102986490901300204
   Cambouropoulos E., 2001, P ICMC HAV CUB
   Chew E, 2005, J NEW MUSIC RES, V34, P341, DOI 10.1080/09298210600578147
   Chew E., 2000, THESIS MIT CAMBRIDGE
   Chuan CH, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/56561
   CLARKE EF, 1990, MUSIC PERCEPT, V7, P213
   DELIEGE I, 1987, MUSIC PERCEPT, V4, P325
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P452, DOI 10.1109/ICME.2000.869637
   Frankland BW, 2004, MUSIC PERCEPT, V21, P499, DOI 10.1525/mp.2004.21.4.499
   Goto M, 2006, IEEE T AUDIO SPEECH, V14, P1783, DOI 10.1109/TSA.2005.863204
   Grosche P., 2012, Proc of the 13th International Society of Music Information Retrieval, number Ismir, P55
   Hamanaka M, 2006, J NEW MUSIC RES, V35, P249, DOI 10.1080/09298210701563238
   Landone C., QMVAMP PLUGINS 2011
   Lerdahl Fred., 1983, A Generative Theory of Tonal Music
   Margulis EH, 2012, MUSIC PERCEPT, V29, P377, DOI 10.1525/MP.2012.29.4.377
   Müller M, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/89686
   Pampalk E, 2004, COMPUT MUSIC J, V28, P49, DOI 10.1162/014892604323112248
   Pampalk E., 2002, Proceedings of the tenth ACM international conference on Multimedia, P570, DOI DOI 10.1145/641007.641121
   Pampalk E., 2004, INT SOC MUS INF RETR, P254
   Paulus J., 2008, P 11 INT C DIG AUD E, P309
   Paulus J., 2010, Ismir, P625
   Peeters G, 2004, LECT NOTES COMPUT SC, V2771, P143
   Sanden C, 2012, J NEW MUSIC RES, V41, P277, DOI 10.1080/09298215.2012.666556
   Smith J. B. L., 2011, P 12 INT SOC MUS INF, P555, DOI DOI 10.5281/ZENODO.1416884
   Temperley David., 2001, COGNITION BASIC MUSI
   Turnbull D., 2007, ISMIR, P51
NR 28
TC 8
Z9 8
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1219
EP 1228
DI 10.1109/TMM.2014.2310706
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600005
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhu, WJ
   Ding, WP
   Xu, JZ
   Shi, YH
   Yin, BC
AF Zhu, Weijia
   Ding, Wenpeng
   Xu, Jizheng
   Shi, Yunhui
   Yin, Baocai
TI Screen Content Coding Based on HEVC Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Directional correlation; base color representation; high efficiency
   video coding; screen content coding; temporal correlation
ID IMAGE COMPRESSION; SEGMENTATION
AB Screen content like cartoons, captures of typical computer screens or video with text overlay or news ticker is an important category of video, which needs new techniques beyond the existing video coding techniques. In this paper, we analyze the characteristics of screen content and coding efficiency of HEVC on screen content. We propose a new coding scheme, which adopts a non-transform representation, separating screen content into color component and structure component. Based on the proposed representation, two coding modes are designed for screen content to exploit the directional correlation and non-translational changes in screen video sequences. The proposed scheme is then seamlessly incorporated into the HEVC structure and implemented into HEVC range extension reference software HM9.0. Experimental results show that the proposed scheme achieves up to 52.6% bitrate saving compared with HM9.0. On average, 35.1%, 29.2% and 23.6% bitrate saving are achieved with intra, random-access and low-delay configurations, respectively. The visual quality of the decoded video sequences is also significantly improved by reducing ringing artifacts around sharp edges and reserving the shape of text without blur.
C1 [Zhu, Weijia; Ding, Wenpeng; Shi, Yunhui; Yin, Baocai] Beijing Univ Technol, Beijing Key Lab Multimedia & Intelligent Software, Coll Metropolitan Transportat, Beijing 100124, Peoples R China.
   [Xu, Jizheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Beijing University of Technology; Microsoft Research Asia; Microsoft
RP Zhu, WJ (corresponding author), Beijing Univ Technol, Beijing Key Lab Multimedia & Intelligent Software, Coll Metropolitan Transportat, Beijing 100124, Peoples R China.
EM sparkjj@bjut.edu.cn; wpding@bjut.edu.cn; jzxu@microsoft.com;
   syhzm@bjut.edu.cnand; ybc@bjut.edu.cn
RI Zhu, Weijia/A-1598-2019; Xu, Jizheng/JDD-5152-2023; Zhu,
   Weijia/C-3087-2017
OI Zhu, Weijia/0000-0001-9202-1260
FU NSFC [61390510, 61033004, 61170103, 61370118, 61133003]
FX This work was supported by NSFC (No. 61390510; 61033004; 61170103;
   61370118; 61133003). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Jing-Ming
   Guo.
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   [Anonymous], 2012, JCTVCJ0237
   [Anonymous], 2001, VCEGM33
   Bottou L, 1998, J ELECTRON IMAGING, V7, P410, DOI 10.1117/1.482609
   Budagavi M., 2013, document JCTVC-M0350
   Cheng H, 2001, J ELECTRON IMAGING, V10, P460, DOI 10.1117/1.1344590
   de Queiroz R., 1999, Proc. IST/SPIE Symp. on Electronic Imaging, V3653, P1106
   de Queiroz RL, 2000, IEEE T IMAGE PROCESS, V9, P1461, DOI 10.1109/83.862619
   Ding W., 2013, JCTVCM0431
   Ding WP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P809, DOI 10.1109/ICME.2006.262624
   Flynn D., 2013, JCTVCL006
   Hampel H., 1992, Signal Processing: Image Communication, V4, P103, DOI 10.1016/0923-5965(92)90017-A
   Ikeda M., 2012, JCTVCH0275
   Lan C., 2011, JCTVCE145, P16
   Lan C., 2012, JCTVC10408
   Lan CL, 2010, IEEE T IMAGE PROCESS, V19, P946, DOI 10.1109/TIP.2009.2038636
   Lin T, 2005, IEEE T IMAGE PROCESS, V14, P993, DOI 10.1109/TIP.2005.849776
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   Pennebaker W. B., 1993, JPEG: Still image data compression standard
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Wang S., 2012, JCTVCK0207
   Wenpeng Ding, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P337
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yu H., 2013, document JCTVC-L0301
   Zaghetto A., 2007, P 25 S BRAS TEL REC
   Zaghetto A, 2007, IEEE T IMAGE PROCESS, V16, P1755, DOI 10.1109/TIP.2007.899036
   Zhu W., 2013, JCTVCM0330
   Zhu W, 2012, 2012 VISUAL COMMUNIC, P1
NR 31
TC 52
Z9 59
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1316
EP 1326
DI 10.1109/TMM.2014.2315782
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600013
DA 2024-07-18
ER

PT J
AU Chen, BH
   Huang, SC
AF Chen, Bo-Hao
   Huang, Shih-Chia
TI An Advanced Moving Object Detection Algorithm for Automatic Traffic
   Monitoring in Real-World Limited Bandwidth Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Intelligent transportation systems; moving object detection; neural
   network; principal component analysis; variable bit-rate
ID MOTION DETECTION ALGORITHM; OPTICAL-FLOW; MODEL
AB Automated motion detection technology is an integral component of intelligent transportation systems, and is particularly essential for management of traffic and maintenance of traffic surveillance systems. Traffic surveillance systems using video communication over real-world networks with limited bandwidth often encounter difficulties due to network congestion and/or unstable bandwidth. This is especially problematic in wireless video communication. This has necessitated the development of a rate control scheme which alters the bit-rate to match the obtainable network bandwidth, thereby producing variable bit-rate video streams. However, complete and accurate detection of moving objects under variable bit-rate video streams is a very difficult task. In this paper, we propose an approach for motion detection which utilizes an analysis-based radial basis function network as its principal component. This approach is applicable not only in high bit-rate video streams, but in low bit-rate video streams, as well. The proposed approach consists of a various background generation stage and a moving object detection stage. During the various background generation stage, the lower-dimensional Eigen-patterns and the adaptive background model are established in variable bit-rate video streams by using the proposed approach in order to accommodate the properties of variable bit-rate video streams. During the moving object detection stage, moving objects are extracted via the proposed approach in both low bit-rate and high bit-rate video streams; detection results are then generated through the output value of the proposed approach. The detection results produced through our approach indicate it to be highly effective in variable bit-rate video streams over real-world limited bandwidth networks. In addition, the proposed method can be easily achieved for real-time application. Quantitative and qualitative evaluations demonstrate that it offers advantages over other state-of-the-art methods. For instance, and Similarity and F-1 accuracy rates produced via the proposed approach were up to 86.38% and 89.88% higher than those produced via other compared methods, respectively.
C1 [Chen, Bo-Hao; Huang, Shih-Chia] Natl Taipei Univ Technol, Dept Elect Engn, Taipei 106, Taiwan.
C3 National Taipei University of Technology
RP Huang, SC (corresponding author), Natl Taipei Univ Technol, Dept Elect Engn, Taipei 106, Taiwan.
EM schuang@ntut.edu.tw
FU National Science Council, Taiwan [NSC 100-2628-E-027-012-MY3, NSC
   102-2221-E-027-065]
FX This work was supported by the National Science Council, Taiwan under
   the Grant NSC 100-2628-E-027-012-MY3 and NSC 102-2221-E-027-065..
   (Corresponding author: S.-C. Huang.) The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Ali Begen.
CR Buch N, 2011, IEEE T INTELL TRANSP, V12, P920, DOI 10.1109/TITS.2011.2119372
   Cheng FC, 2011, IEEE T BROADCAST, V57, P794, DOI 10.1109/TBC.2011.2160106
   Cheng FC, 2011, IEEE T SYST MAN CY C, V41, P589, DOI 10.1109/TSMCC.2010.2092425
   Czarlinska A, 2008, IEEE T MULTIMEDIA, V10, P675, DOI 10.1109/TMM.2008.922775
   Gibson D, 2003, IEEE T IMAGE PROCESS, V12, P431, DOI 10.1109/TIP.2003.811628
   Ha JE, 2010, OPT ENG, V49, DOI 10.1117/1.3374043
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Huang SC, 2012, ENG APPL ARTIF INTEL, V25, P1338, DOI 10.1016/j.engappai.2012.02.002
   Huang SC, 2011, IEEE T CIRC SYST VID, V21, P1, DOI 10.1109/TCSVT.2010.2087812
   Joint Video Team (JVT) of ISO/IEC MPEG and ITU-T VCEG, 2003, 1449610 JVT ISOIEC M
   Li NX, 2013, IEEE T MULTIMEDIA, V15, P1213, DOI 10.1109/TMM.2013.2241416
   Li Z., 2003, JVT 7 M
   Lipton AJ, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P8, DOI 10.1109/ACV.1998.732851
   Lo Presti L, 2012, IEEE T MULTIMEDIA, V14, P346, DOI 10.1109/TMM.2011.2173323
   Ma LY, 2005, IEEE T NEURAL NETWOR, V16, P821, DOI 10.1109/TNN.2005.851786
   Manzanera A., 2004, P IND C COMP VIS GRA, P46
   Manzanera A, 2007, PATTERN RECOGN LETT, V28, P320, DOI 10.1016/j.patrec.2006.04.007
   Oral M, 2007, IMAGE VISION COMPUT, V25, P1365, DOI 10.1016/j.imavis.2006.10.001
   Saini M, 2012, IEEE T MULTIMEDIA, V14, P555, DOI 10.1109/TMM.2012.2186957
   Shyu ML, 2008, IEEE T MULTIMEDIA, V10, P252, DOI 10.1109/TMM.2007.911830
   SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhou DX, 2005, IEEE SYS MAN CYBERN, P2224
NR 23
TC 51
Z9 55
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 837
EP 847
DI 10.1109/TMM.2014.2298377
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500022
DA 2024-07-18
ER

PT J
AU Cicalò, S
   Tralli, V
AF Cicalo, Sergio
   Tralli, Velio
TI Distortion-Fair Cross-Layer Resource Allocation for Scalable Video
   Transmission in OFDMA Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer; fairness; OFDMA; resource allocation; scalable video coding
   (SVC); wireless networks
ID OPTIMIZATION; COMPLEXITY; EXTENSION
AB The design of optimized video delivery to multiple users over a wireless channel is a challenging task, especially when the objectives of maximizing the spectral efficiency and providing a fair video quality have to be jointly considered. In this paper we propose a novel cross-layer optimization framework for scalable video delivery over OFDMA wireless networks. It jointly addresses rate adaptation and resource allocation with the aim of maximizing the sum of the achievable rates while minimizing the distortion difference among multiple videos. After having discussed the feasibility of the optimization problem, we consider a "vertical" decomposition of it and propose the iterative local approximation (ILA) algorithm to derive the optimal solution. The ILA algorithm requires a limited information exchange between the application and the MAC layers, which independently run algorithms that handle parameters and constraints characteristic of a single layer. In order to reduce the overall complexity and the latency of the optimal algorithm, we also propose suboptimal strategies based on the first-step of the ILA algorithm and on the use of stochastic approximations at the MAC layer. Our numerical evaluations show the fast convergence of the ILA algorithm and the resulting small gap in terms of efficiency and video quality fairness between optimal and suboptimal strategies. Moreover, significant individual PSNR gains, up to 7 dB for high-complexity videos in the investigated scenario, are obtained with respect to other state-of-the-art frameworks with similar complexity.
C1 [Cicalo, Sergio; Tralli, Velio] Univ Ferrara ENDIF, Dept Engn, I-44100 Ferrara, Italy.
   [Cicalo, Sergio; Tralli, Velio] CNIT, I-44100 Ferrara, Italy.
C3 University of Ferrara
RP Cicalò, S (corresponding author), Univ Ferrara ENDIF, Dept Engn, I-44100 Ferrara, Italy.
EM cclsrg@unife.it; trv@unife.it
FU European Commission under the FP7 ICT (CONCERTO) project [288502]
FX This work was supported in part by the European Commission under the FP7
   ICT (CONCERTO) project, Grant no. 288502. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shahram Shirani.
CR Amonou I, 2007, IEEE T CIRC SYST VID, V17, P1186, DOI 10.1109/TCSVT.2007.906870
   [Anonymous], 2006, Simulation
   [Anonymous], 2012, P ACM INT C MULT MM
   [Anonymous], 2011, JSVM 9 19 11 REFEREN
   Brehmer J, 2007, 2007 41ST ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1 AND 2, P437, DOI 10.1109/CISS.2007.4298344
   Cicalo S., 2011, Vehicular Technology Conference (VTC Spring), 15-18 May 2011, P1
   Cicalò S, 2012, SIGNAL PROCESS-IMAGE, V27, P800, DOI 10.1016/j.image.2012.01.005
   Fallah YP, 2008, IEEE T CIRC SYST VID, V18, P875, DOI 10.1109/TCSVT.2008.920745
   Guan Z., 2009, EURASIP J WIRELESS C, V2009
   Haseeb A., 2012, 2012 Wireless Advanced (WiAd) (Formerly known as SPWC), P85, DOI 10.1109/WiAd.2012.6296574
   Henarejos P, 2012, SIGNAL PROCESS, V92, P2975, DOI 10.1016/j.sigpro.2012.05.031
   Ji X, 2009, IEEE T CIRC SYST VID, V19, P1549, DOI 10.1109/TCSVT.2009.2026812
   Maani E, 2010, IEEE T CIRC SYST VID, V20, P407, DOI 10.1109/TCSVT.2009.2035846
   Mansour H, 2008, IEEE T MULTIMEDIA, V10, P1366, DOI 10.1109/TMM.2008.2004915
   Mansour H, 2009, IEEE T MULTIMEDIA, V11, P1478, DOI 10.1109/TMM.2009.2032682
   Mazzotti M, 2012, IEEE T COMMUN, V60, P2915, DOI 10.1109/TCOMM.2012.081412.110113
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Schierl T, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P885
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Seeling P, 2012, IEEE COMMUN SURV TUT, V14, P1142, DOI 10.1109/SURV.2011.082911.00067
   Shen ZK, 2005, IEEE T WIREL COMMUN, V4, P2726, DOI 10.1109/TWC.2005.858010
   Song GC, 2005, IEEE T WIREL COMMUN, V4, P625, DOI 10.1109/TWC.2004.843067
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Su GM, 2006, IEEE T CIRC SYST VID, V16, P1217, DOI 10.1109/TCSVT.2006.883513
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Vukadinovic V, 2010, IEEE J SEL AREA COMM, V28, P399, DOI 10.1109/JSAC.2010.100411
   Wang X, 2011, IEEE T INFORM THEORY, V57, P4359, DOI 10.1109/TIT.2011.2145770
   Wang X, 2010, IEEE T INFORM THEORY, V56, P2382, DOI 10.1109/TIT.2010.2040968
   Wong IC, 2008, IEEE T WIREL COMMUN, V7, P962, DOI 10.1109/TWC.2008.060718
   Wong IC, 2008, CONF REC ASILOMAR C, P2203, DOI 10.1109/ACSSC.2008.5074826
   Zhang HH, 2010, IEEE J SEL AREA COMM, V28, P344, DOI 10.1109/JSAC.2010.100406
NR 31
TC 50
Z9 52
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 848
EP 863
DI 10.1109/TMM.2014.2300442
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500023
DA 2024-07-18
ER

PT J
AU Wan, YL
   Miao, ZJ
   Zhang, XP
   Tang, Z
   Wang, ZF
AF Wan, Yanli
   Miao, Zhenjiang
   Zhang, Xiao-Ping
   Tang, Zhen
   Wang, Zhifei
TI Illumination Robust Video Foreground Prediction Based on Color
   Recovering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color recovering; foreground prediction; illumination changes; opacity
   propagation; optical flow estimation
ID SEGMENTATION; IMAGE
AB Video foreground prediction is a technique to estimate the probability of each pixel being foreground in current frame based on a foreground segmentation result of its previous frame. Existing foreground prediction algorithms usually assume that the illumination conditions are constant for consecutive frames. Therefore, they cannot predict foreground accurately when the illumination condition changes sharply between video frames. In this paper, a new robust video foreground prediction algorithm is proposed based on color recovering, which is derived based on an observation that the illumination changes are locally smooth. By integrating color recovering with an optical flow estimation algorithm and an opacity propagation algorithm, the negative impact of the illumination changes could be removed. Experimental results show that the proposed algorithm can get more accurate results for videos with illumination changes compared with the existing foreground prediction algorithms.
C1 [Wan, Yanli] Beijing Jiaotong Univ, Inst Syst Engn & Control, Beijing 100044, Peoples R China.
   [Miao, Zhenjiang; Tang, Zhen; Wang, Zhifei] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Toronto
   Metropolitan University
RP Wan, YL (corresponding author), Beijing Jiaotong Univ, Inst Syst Engn & Control, Beijing 100044, Peoples R China.
EM 07112067@bjtu.edu.cn; zjmiao@bjtu.edu.cn; xzhang@ee.ry-erson.ca;
   06112059@bjtu.edu.cn; 06112059@bjtu.edu.cn
RI Zhang, Xiao-Ping (Steven)/B-1436-2016
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069
FU China Postdoctoral Science Foundation [2013M530020]; NSFC [61273274];
   973 Program [2011CB302203]; Ph.D. Programs Foundation of Ministry of
   Education of China [20100009110004]; National Key Technology R&D Program
   of China [2012BAH01F03]; Natural Sciences and Engineering Research
   Council of Canada (NSERC) [RGPIN239031]
FX This work was supported by the China Postdoctoral Science Foundation
   (No.2013M530020), NSFC (No. 61273274), 973 Program (No.2011CB302203),
   Ph.D. Programs Foundation of Ministry of Education of China (No.
   20100009110004), National Key Technology R&D Program of China
   (No.2012BAH01F03), Natural Sciences and Engineering Research Council of
   Canada (NSERC, No.RGPIN239031). The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Chia-Wen Lin.
CR Ahn JK, 2008, IEEE IMAGE PROC, P1544, DOI 10.1109/ICIP.2008.4712062
   [Anonymous], 2008, 5 EUR C VIS MED PROD
   [Anonymous], 2006, CVPR, DOI DOI 10.1109/CVPR.2006.69
   Apostoloff N, 2004, PROC CVPR IEEE, P407
   Bai X, 2010, LECT NOTES COMPUT SC, V6315, P617
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   Bai X, 2009, INT J COMPUT VISION, V82, P113, DOI 10.1007/s11263-008-0191-z
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Chan SC, 2009, IEEE T CIRC SYST VID, V19, P821, DOI 10.1109/TCSVT.2009.2017302
   Chuang Y., 2002, P SIGGRAPH
   Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572
   Chung CY, 2010, IEEE T CIRC SYST VID, V20, P149, DOI 10.1109/TCSVT.2009.2026823
   Crow F. C., 1984, Computers & Graphics, V18, P207
   Fan J., 2010, P ECCV
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136
   Hsiao HH, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-12
   Hunt R.W.G., 1995, The Reproduction of Color
   Jain A, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P25, DOI 10.1109/VECIMS.2008.4592747
   Lee SY, 2010, GRAPH MODELS, V72, P25, DOI 10.1016/j.gmod.2010.03.001
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li J, 2012, IET IMAGE PROCESS, V6, P606, DOI 10.1049/iet-ipr.2012.0025
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Ng KK, 2011, INT SYMP IMAGE SIG, P236
   Pham VQ, 2010, LECT NOTES COMPUT SC, V5995, P489
   Price BL, 2009, IEEE I CONF COMP VIS, P779, DOI 10.1109/ICCV.2009.5459293
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Tang Z, 2010, IEEE INT CON MULTI, P370, DOI [10.1109/ICCDA.2010.5541074, 10.1109/ICME.2010.5583894]
   Tang Z, 2011, PATTERN RECOGN LETT, V32, P1720, DOI 10.1016/j.patrec.2011.07.025
   Vosters L. P. J., 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P384, DOI 10.1109/AVSS.2010.72
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
   Wang J., 2007, 2007 IEEE C COMP VIS, P1
   Yin Pei., 2007, P IEEE C COMPUTER VI, P1, DOI DOI 10.1109/CVPR.2007.383008
   Zhao Xian-rui, 2007, Instrument Techniques and Sensor, P1
   Zhong F, 2010, PROC CVPR IEEE, P2189, DOI 10.1109/CVPR.2010.5539899
NR 36
TC 11
Z9 12
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 637
EP 652
DI 10.1109/TMM.2014.2299515
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500006
DA 2024-07-18
ER

PT J
AU Liu, Y
   Wei, Q
   Guo, L
   Shen, B
   Chen, SQ
   Lan, YJ
AF Liu, Yao
   Wei, Qi
   Guo, Lei
   Shen, Bo
   Chen, Songqing
   Lan, Yingjie
TI Investigating Redundant Internet Video Streaming Traffic on iOS Devices:
   Causes and Solutions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Internet mobile streaming; iOS; redundant traffic
AB The Internet has witnessed rapidly increasing streaming traffic to various mobile devices. In this paper, through analysis of a server-side workload and experiments in a controlled lab environment, we find that current practice has introduced a significant amount of redundant traffic. In particular, for the popular iOS based mobile devices, accessing popular Internet streaming services typically involves about 10%-70% redundant traffic. Such a practice not only over-utilizes and wastes resources on the server side and the network (cellular or Internet), but also consumes additional battery power on user's mobile devices and leads to possible monetary cost. To alleviate such a situation without changing the server side or the client side, we design and implement CStreamer that can transparently work between existing mobile clients and servers. We have implemented a prototype and installed on Amazon EC2. Experiments conducted based on this prototype show that CStreamer can completely eliminate the redundant traffic without degrading user's QoS.
C1 [Liu, Yao] SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA.
   [Wei, Qi] George Mason Univ, Dept Bioengn, Fairfax, VA 22030 USA.
   [Guo, Lei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   [Shen, Bo] XinLab Inc, Vuclip, Milpitas, CA 95035 USA.
   [Chen, Songqing] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   [Lan, Yingjie] Peking Univ, Guanghua Sch Management, Beijing 100871, Peoples R China.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Binghamton; George Mason University; University System of Ohio;
   Ohio State University; George Mason University; Peking University
RP Liu, Y (corresponding author), SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA.
EM yaoliu@cs.binghamton.edu; qwei2@gmu.edu; lguo@cse.ohio-state.edu;
   bshen@vuclip.com; sqchen@cs.gmu.edu; ylan@gsm.pku.edu.cn
RI Wei, Qi/JJF-3393-2023
FU NSF [CNS-0746649, CNS-1117300]; Division Of Computer and Network
   Systems; Direct For Computer & Info Scie & Enginr [0746649] Funding
   Source: National Science Foundation
FX The work was supported in part by NSF under grants CNS-0746649 and
   CNS-1117300. An earlier version [36] of this manuscript is published in
   the Proceedings of INFOCOM2013 mini-conference. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Yiannis Andreopoulos.
CR Ali S., 2006, P WORKSH REC ADV PEE
   [Anonymous], P ACM IMC
   [Anonymous], P ACM IMC
   Cha M., 2007, P ACM IMC
   Cheng X., 2008, P EEE IWQOS
   Cisco, 2012, CISC VIS NETW IND GL
   Finamore A., 2011, P ACM IMC
   Guo L., 2005, P ACM WWW
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Huang Y., 2008, P ACM SIGCOMM
   iOS, ANDR MARK SHAR MOB T
   Krishnappa D. K., 2011, P PAM
   Li Y., 2011, P ACM IMC
   Liu Y., 2013, P IEEE INFOCOM MIN
   Liu Y., 2013, P PAM
   Liu Y., IEEE T PARA IN PRESS
   Liu Y., 2011, P ACM NOSSDAV
   Rao A., 2011, P ACM CONEXT
   Siekkinen M., 2013, P MOVID
   Wu C, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1386109.1386112
   Xiao Y., 2008, P NGMAST
   Yin H., 2009, P ACM IMC
   YU H, 2006, P EUROSYS
NR 23
TC 1
Z9 4
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 510
EP 520
DI 10.1109/TMM.2013.2293312
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, DA
   Kang, LW
   Wang, YCF
   Lin, CW
AF Huang, De-An
   Kang, Li-Wei
   Wang, Yu-Chiang Frank
   Lin, Chia-Wen
TI Self-Learning Based Image Decomposition With Applications to Single
   Image Denoising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Denoising; image decomposition; rain removal; self-learning; sparse
   representation
ID SPARSE; REPRESENTATION
AB Decomposition of an image into multiple semantic components has been an effective research topic for various image processing applications such as image denoising, enhancement, and inpainting. In this paper, we present a novel self-learning based image decomposition framework. Based on the recent success of sparse representation, the proposed framework first learns an over-complete dictionary from the high spatial frequency parts of the input image for reconstruction purposes. We perform unsupervised clustering on the observed dictionary atoms (and their corresponding reconstructed image versions) via affinity propagation, which allows us to identify image-dependent components with similar context information. While applying the proposed method for the applications of image denoising, we are able to automatically determine the undesirable patterns (e. g., rain streaks or Gaussian noise) from the derived image components directly from the input image, so that the task of single-image denoising can be addressed. Different from prior image processing works with sparse representation, our method does not need to collect training image data in advance, nor do we assume image priors such as the relationship between input and output image dictionaries. We conduct experiments on two denoising problems: single-image denoising with Gaussian noise and rain removal. Our empirical results confirm the effectiveness and robustness of our approach, which is shown to outperform state-of-the-art image denoising algorithms.
C1 [Huang, De-An; Wang, Yu-Chiang Frank] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Kang, Li-Wei] Natl Yunlin Univ Sci & Technol, Grad Sch Engn Sci, Yunlin, Taiwan.
   [Kang, Li-Wei] Natl Yunlin Univ Sci & Technol, Technol Doctoral Program, Yunlin, Taiwan.
   [Kang, Li-Wei] Natl Yunlin Univ Sci & Technol, Dept Comp Sci & Informat Engn, Yunlin, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu, Taiwan.
C3 Academia Sinica - Taiwan; National Yunlin University Science &
   Technology; National Yunlin University Science & Technology; National
   Yunlin University Science & Technology; National Tsing Hua University;
   National Tsing Hua University
RP Huang, DA (corresponding author), Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
EM drew800619@gmail.com; lwkang@yuntech.edu.tw; ycwang@citi.sinica.edu.tw;
   cwlin@ee.nthu.edu.tw
RI Lin, Chia-Wen/ABH-6075-2020; Lin, Chia-Wen/M-4571-2013
OI Lin, Chia-Wen/0000-0002-9097-2318
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   Blu T, 2007, IEEE T IMAGE PROCESS, V16, P2778, DOI 10.1109/TIP.2007.906002
   Bobin J, 2007, IEEE T IMAGE PROCESS, V16, P2675, DOI 10.1109/TIP.2007.907073
   Bobin J, 2007, IEEE T IMAGE PROCESS, V16, P2662, DOI 10.1109/TIP.2007.906256
   Bossu J, 2011, INT J COMPUT VISION, V93, P348, DOI 10.1007/s11263-011-0421-7
   Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   De-An Huang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P164, DOI 10.1109/ICME.2012.92
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Fadili JM, 2010, COMPUT SCI ENG, V12, P44, DOI 10.1109/MCSE.2010.14
   Fadili MJ, 2010, P IEEE, V98, P983, DOI 10.1109/JPROC.2009.2024776
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Garg K, 2005, IEEE I CONF COMP VIS, P1067
   Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Meyer FG, 2002, IEEE T IMAGE PROCESS, V11, P1072, DOI 10.1109/TIP.2002.802527
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
NR 26
TC 161
Z9 178
U1 1
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 83
EP 93
DI 10.1109/TMM.2013.2284759
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100008
DA 2024-07-18
ER

PT J
AU Meng, FM
   Li, HL
   Liu, GH
   Ngan, KN
AF Meng, Fanman
   Li, Hongliang
   Liu, Guanghui
   Ngan, King Ngi
TI From Logo to Object Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Specific object segmentation; logo detection
ID SHAPE PRIOR; IMAGE; COSEGMENTATION
AB This paper proposes a method to segment object from the web images using logo detection. The method consists of three steps. In the first step, the logos are located from the original images by SIFT matching. Based on the logo location and the object shape model, the second step extracts the object boundary from the image. In the third step, we use the object boundary to model the object appearance, which is then used in the MRF based segmentation method to finally achieve the object segmentation. The key of our method is the object boundary extraction, which is achieved by searching a variation of the shape model that best fits the local edge of the image. Affine transform is used to consider the variations among the objects. Meanwhile, the Nelder-Mead simplex method with a simple initial rough search is used to run the boundary search. To verify the proposed method, we collect a LogoSeg dataset from the web such as Flickr and Google. TheMOMI dataset is also used for the verification. The experimental results demonstrate that the proposed logo detection based segmentation method can improve the performance of the object segmentation.
C1 [Meng, Fanman; Li, Hongliang; Liu, Guanghui; Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610073, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong
RP Meng, FM (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610073, Peoples R China.
EM fanmanmeng@gmail.com; hlli@uestc.edu.cn; guanghuiliu@uestc.edu.cn;
   knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014; Liu, Guanghui/C-3658-2012
OI Ngan, N/0000-0003-1946-3235; Liu, Guanghui/0000-0002-4170-4552; Li,
   Hongliang/0000-0002-7481-095X
CR [Anonymous], PATTERN RECOGNIT
   [Anonymous], P VIS COMM IM PROC V
   Arbeláez P, 2012, PROC CVPR IEEE, P3378, DOI 10.1109/CVPR.2012.6248077
   Arbeláez P, 2009, PROC CVPR IEEE, P2294, DOI 10.1109/CVPRW.2009.5206707
   Bagon S, 2010, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2010.5540233
   Batra D, 2009, IEEE IMAGE PROC, P2393, DOI 10.1109/ICIP.2009.5414482
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chu W.-S., 2010, Proc. of the Asian Conference on Computer Vision (ACCV), P355, DOI DOI 10.1007/978-3-642-19315-6_28
   Collins MD, 2012, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2012.6247859
   Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5
   Daliri MR, 2008, PATTERN RECOGN, V41, P1782, DOI 10.1016/j.patcog.2007.10.020
   Das P, 2009, IMAGE VISION COMPUT, V27, P206, DOI 10.1016/j.imavis.2008.02.006
   Ferrari V., 2007, PROC IEEE C COMPUTER, P1
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Jiang TT, 2009, PROC CVPR IEEE, P848, DOI 10.1109/CVPRW.2009.5206568
   Jing F, 2004, IEEE T CIRC SYST VID, V14, P672, DOI 10.1109/TCSVT.2004.826775
   Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Kim G, 2012, PROC CVPR IEEE, P837, DOI 10.1109/CVPR.2012.6247756
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Klodt M, 2011, IEEE I CONF COMP VIS, P2236, DOI 10.1109/ICCV.2011.6126502
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Larochelle H., 2007, An empirical evaluation of deep architectures on problems with many factors of variation, V227, P473
   Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262
   Ma TY, 2011, PROC CVPR IEEE, P1441, DOI 10.1109/CVPR.2011.5995591
   Meng FM, 2013, IEEE T CYBERNETICS, V43, P725, DOI 10.1109/TSMCB.2012.2215316
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Mukherjee Lopamudra, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P1881, DOI 10.1109/CVPR.2011.5995420
   Mukherjee L, 2009, PROC CVPR IEEE, P2028, DOI 10.1109/CVPRW.2009.5206652
   Parkhi OM, 2011, IEEE I CONF COMP VIS, P1427, DOI 10.1109/ICCV.2011.6126398
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubio JC, 2012, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2012.6247745
   Schoenemann Thomas., 2007, Computer Vision, IEEE International Conference on, P1
   Shechtman E., 2007, PROC IEEE C COMPUTER
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Trinh NH, 2011, INT J COMPUT VISION, V94, P215, DOI 10.1007/s11263-010-0412-0
   Veksler O, 2008, LECT NOTES COMPUT SC, V5304, P454, DOI 10.1007/978-3-540-88690-7_34
   Vezhnevets A, 2012, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2012.6247757
   Vezhnevets A, 2012, PROC CVPR IEEE, P3162, DOI 10.1109/CVPR.2012.6248050
   Vezhnevets A, 2011, IEEE I CONF COMP VIS, P643, DOI 10.1109/ICCV.2011.6126299
   Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530
   Vicente S, 2010, LECT NOTES COMPUT SC, V6312, P465, DOI 10.1007/978-3-642-15552-9_34
   Wu YN, 2010, INT J COMPUT VISION, V90, P198, DOI 10.1007/s11263-009-0287-0
   Zhang JY, 2010, PROC CVPR IEEE, P2125, DOI 10.1109/CVPR.2010.5539891
NR 49
TC 15
Z9 17
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2186
EP 2197
DI 10.1109/TMM.2013.2280893
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900037
DA 2024-07-18
ER

PT J
AU Zhang, WW
   Wen, YG
   Chen, ZZ
   Khisti, A
AF Zhang, Weiwen
   Wen, Yonggang
   Chen, Zhenzhong
   Khisti, Ashish
TI QoE-Driven Cache Management for HTTP Adaptive Bit Rate Streaming Over
   Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive bit rate streaming; content cache management; optimization;
   quality of experience
ID VIDEO
AB In this paper, we investigate the problem of optimal content cache management for HTTP adaptive bit rate (ABR) streaming over wireless networks. Specifically, in the media cloud, each content is transcoded into a set of media files with diverse playback rates, and appropriate files will be dynamically chosen in response to channel conditions and screen forms. Our design objective is to maximize the quality of experience (QoE) of an individual content for the end users, under a limited storage budget. Deriving a logarithmic QoE model from our experimental results, we formulate the individual content cache management for HTTP ABR streaming over wireless network as a constrained convex optimization problem. We adopt a two-step process to solve the snapshot problem. First, using the Lagrange multiplier method, we obtain the numerical solution of the set of playback rates for a fixed number of cache copies and characterize the optimal solution analytically. Our investigation reveals a fundamental phase change in the optimal solution as the number of cached files increases. Second, we develop three alternative search algorithms to find the optimal number of cached files, and compare their scalability under average and worst complexity metrics. Our numerical results suggest that, under optimal cache schemes, the maximum QoE measurement, i.e., mean-opinion-score (MOS), is a concave function of the allowable storage size. Our cache management can provide high expected QoE with low complexity, shedding light on the design of HTTP ABR streaming services over wireless networks.
C1 [Zhang, Weiwen; Wen, Yonggang] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Chen, Zhenzhong] MediaTek USA Inc, San Jose, CA 95134 USA.
   [Khisti, Ashish] Univ Toronto, Sch Elect & Comp Engn, Toronto, ON M5S 1A1, Canada.
C3 Nanyang Technological University; Mediatek Incorporated; University of
   Toronto
RP Zhang, WW (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Nanyang Ave, Singapore 639798, Singapore.
EM wzhang9@ntu.edu.sg; ygwen@ntu.edu.sg; zzchen@ieee.org;
   akhisti@comm.utoronto.ca
RI 陈, 震中/C-6857-2014; Wen, Yonggang/B-8848-2011; Wen, Yonggang/P-9406-2017;
   Khisti, Ashish J/F-9908-2010; zhang, weijun/AAX-3743-2020; Chen,
   Zhenzhong/C-2529-2015
OI Wen, Yonggang/0000-0002-2751-5114; zhang, weijun/0000-0002-9432-7404;
   Zhang, Weiguo/0000-0002-8850-5235
CR Adzic V., 2011, P SPIE, V8135
   [Anonymous], 1999, document P.910, DOI 11.1002/1000/4751
   [Anonymous], 2011, P IEEE GLOB TEL C HO
   [Anonymous], 2010, P 2010 INFOCOM IEEE
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], 2011, 2011 IEEE INT S WORL
   [Anonymous], 2011, CISC VIS NETW IND FO
   [Anonymous], 2002, METHODOLOGY SUBJECTI
   [Anonymous], 1980, CODING INFORM THEORY
   Begen AC, 2011, IEEE INTERNET COMPUT, V15, P54, DOI 10.1109/MIC.2010.155
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   Cisco, 2007, CISC CDS INT STREAM
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Hofmann Ingo., 2011, IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB), P1
   JSVM, 2011, JSVM SOFTW MAN
   Khan A, 2010, IET COMMUN, V4, P1337, DOI 10.1049/iet-com.2009.0422
   Khan A., 2011, IEEE International Conference on Communications (ICC), P1
   Kim HL, 2010, INT CONF ADV COMMUN, P1377
   Lei ZJ, 2005, J SYST SOFTWARE, V75, P253, DOI 10.1016/j.jss.2003.09.029
   Li Q, 2007, IEEE T VEH TECHNOL, V56, P3533, DOI 10.1109/TVT.2007.901927
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548
   Mok R. K. P., 2011, 2011 IFIP/IEEE International Symposium on Integrated Network Management (IM 2011), P485, DOI 10.1109/INM.2011.5990550
   Nocedal J., 1999, NUMERICAL OPTIMIZATI
   Reichl P., 2011, TELECOMMUN SYST, P1
   Thakolsri S., 2010, P 18 ACM INT C MULT, P783
   Venkata M.G., 2009, IPDPS 2009. IEEE International Symposium on Parallel Distributed Processing, P1
   Vetro A, 2002, IEEE IMAGE PROC, P29
NR 29
TC 148
Z9 157
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1431
EP 1445
DI 10.1109/TMM.2013.2247583
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400018
DA 2024-07-18
ER

PT J
AU Bhatnagar, G
   Wu, QMJ
   Liu, Z
AF Bhatnagar, Gaurav
   Wu, Q. M. Jonathan
   Liu, Zheng
TI Directive Contrast Based Multimodal Medical Image Fusion in NSCT Domain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal medical image fusion; non-subsampled contour transform; phase
   congruency; directive contrast
ID NONSUBSAMPLED CONTOURLET TRANSFORM; FRAMEWORK
AB Multimodal medical image fusion, as a powerful tool for the clinical applications, has developed with the advent of various imaging modalities in medical imaging. The main motivation is to capture most relevant information from sources into a single output, which plays an important role in medical diagnosis. In this paper, a novel fusion framework is proposed for multimodal medical images based on non-subsampled contourlet transform (NSCT). The source medical images are first transformed by NSCT followed by combining low-and high-frequency components. Two different fusion rules based on phase congruency and directive contrast are proposed and used to fuse low-and high-frequency coefficients. Finally, the fused image is constructed by the inverse NSCT with all composite coefficients. Experimental results and comparative study show that the proposed fusion framework provides an effective way to enable more accurate analysis of multimodality images. Further, the applicability of the proposed framework is carried out by the three clinical examples of persons affected with Alzheimer, subacute stroke and recurrent tumor.
C1 [Bhatnagar, Gaurav; Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
   [Liu, Zheng] Toyota Technol Inst, Intelligent Informat Proc Lab, Nagoya, Aichi 468, Japan.
C3 University of Windsor; Toyota Technological Institute
RP Bhatnagar, G (corresponding author), Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
EM goravb@uwindsor.ca; jwu@uwindsor.ca; zheng.liu@ieee.org
RI Bhatnagar, Gaurav/O-5817-2019; Wu, Q.M.Jonathan/O-3234-2017; Liu,
   Zheng/D-8678-2016
OI Bhatnagar, Gaurav/0000-0002-0282-3372; Liu, Zheng/0000-0002-7241-3483
FU Canada Research Chair program, the Natural Sciences and Engineering
   Research Council of Canada (NSERC) Discovery Grant
FX This work was supported by the Canada Research Chair program, the
   Natural Sciences and Engineering Research Council of Canada (NSERC)
   Discovery Grant. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Dimitri Van De
   Ville.
CR Ali F. E., 2008, Progress In Electromagnetics Research C, V3, P215, DOI 10.2528/PIERC08041305
   Barra V, 2001, NEUROIMAGE, V13, P410, DOI 10.1006/nimg.2000.0707
   Bhatnagar G., 2009, ELECT LETT COMPUT VI, V8, P18
   Bhatnagar G, 2012, INT J WAVELETS MULTI, V10, DOI 10.1142/S0219691311004444
   Bhatnagar G, 2010, LECT NOTES COMPUT SC, V6134, P71, DOI 10.1007/978-3-642-13681-8_9
   Boussion N, 2008, COMPUT METH PROG BIO, V90, P191, DOI 10.1016/j.cmpb.2007.12.009
   Cardinali A, 2005, 2005 7TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), VOLS 1 AND 2, P475
   Chai Y, 2012, OPTIK, V123, P569, DOI 10.1016/j.ijleo.2011.02.034
   CHAVEZ PS, 1989, PHOTOGRAMM ENG REM S, V55, P339
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Daneshvar S, 2010, INFORM FUSION, V11, P114, DOI 10.1016/j.inffus.2009.05.003
   Das S., 2011, Progress In Electromagnetics Research B, V30, P355
   Wei H, 2007, PATTERN RECOGN LETT, V28, P493, DOI 10.1016/j.patrec.2006.09.005
   Kovesi P, 2000, PSYCHOL RES-PSYCH FO, V64, P136, DOI 10.1007/s004260000024
   Kovesi P.D., 1999, Videre: Journal of Computer Vision Research, V1
   LI H, 1995, GRAPH MODEL IM PROC, V57, P235, DOI 10.1006/gmip.1995.1022
   Li ST, 2011, INFORM FUSION, V12, P74, DOI 10.1016/j.inffus.2010.03.002
   Li TJ, 2011, INFORM FUSION, V12, P85, DOI 10.1016/j.inffus.2010.03.007
   Maes F, 2003, P IEEE, V91, P1699, DOI 10.1109/JPROC.2003.817864
   Miao QG, 2011, OPT COMMUN, V284, P1540, DOI 10.1016/j.optcom.2010.11.048
   Petrovic VS, 2004, IEEE T IMAGE PROCESS, V13, P228, DOI 10.1109/tip.2004.823821
   Qu GH, 2001, OPT EXPRESS, V9, P184, DOI 10.1364/OE.9.000184
   Qu Xiao-Bo, 2008, Acta Automatica Sinica, V34, P1508, DOI 10.3724/SP.J.1004.2008.01508
   Redondo R, 2009, INFORM FUSION, V10, P163, DOI 10.1016/j.inffus.2008.08.006
   Ruderman DL, 1998, J OPT SOC AM A, V15, P2036, DOI 10.1364/JOSAA.15.002036
   TOET A, 1989, OPT ENG, V28, P789, DOI 10.1117/12.7977034
   Toet A., 1990, Machine Vision and Applications, V3, P1, DOI 10.1007/BF01211447
   WATSON AB, 1987, J OPT SOC AM A, V4, P2401, DOI 10.1364/JOSAA.4.002401
   Yang C, 2008, INFORM FUSION, V9, P156, DOI 10.1016/j.inffus.2006.09.001
   Yang L, 2008, NEUROCOMPUTING, V72, P203, DOI 10.1016/j.neucom.2008.02.025
   Yang SY, 2010, INFORM FUSION, V11, P78, DOI 10.1016/j.inffus.2009.05.001
   Yang SY, 2009, SIGNAL PROCESS, V89, P2596, DOI 10.1016/j.sigpro.2009.04.027
   Yang Y, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/579341
   Zhang Q, 2009, SIGNAL PROCESS, V89, P1334, DOI 10.1016/j.sigpro.2009.01.012
NR 34
TC 330
Z9 356
U1 3
U2 109
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1014
EP 1024
DI 10.1109/TMM.2013.2244870
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600005
DA 2024-07-18
ER

PT J
AU Höferlin, M
   Höferlin, B
   Heidemann, G
   Weiskopf, D
AF Hoeferlin, Markus
   Hoeferlin, Benjamin
   Heidemann, Gunther
   Weiskopf, Daniel
TI Interactive Schematic Summaries for Faceted Exploration of Surveillance
   Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interactive schematic summary; scatter/gather browsing; trajectory
   bundling; video exploration
AB We present a scalable technique to explore surveillance videos by scatter/gather browsing of trajectories of moving objects. Trajectories are clustered according to a variety of properties, such as location, orientation, and velocity that can be selected by the users. These properties allow for faceted video exploration and refinement of previous browsing steps. The proposed approach facilitates interactive clustering of trajectories by an effective way of cluster visualization that we term schematic summaries. This novel visualization illustrates cluster summaries in a schematic, nonphotorealistic style. To reduce visual clutter, we introduce the trajectory bundling technique. Further, schematic summaries include a timeline view and a showcase view to represent the facets present in a cluster. The fusion of schematic summaries, a variety of facets, and user interaction lead to efficient hierarchical exploration of video data. Examples of different browsing scenarios and initial user feedback demonstrate the potentials of our method.
C1 [Hoeferlin, Markus; Weiskopf, Daniel] Univ Stuttgart, Visualizat Res Ctr, D-70569 Stuttgart, Germany.
   [Hoeferlin, Benjamin; Heidemann, Gunther] Univ Osnabruck, Comp Vis Grp, Inst Cognit Sci, D-70569 Stuttgart, Germany.
C3 University of Stuttgart; University Osnabruck
RP Höferlin, M (corresponding author), Univ Stuttgart, Visualizat Res Ctr, D-70569 Stuttgart, Germany.
EM Markus.Hoeferlin@vis.uni-stuttgart.de;
   Benjamin.Hoeferlin@vis.uni-stuttgart.de; gheidema@uni-osnabrueck.de;
   Daniel.Weiskopf@visus.uni-stuttgart.de
FU German Research Foundation (DFG) [SPP 1335]
FX This work was supported by German Research Foundation (DFG) as part of
   the Priority Program "Scalable Visual Analytics" (SPP 1335). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Nicu Sebe.
CR Aravecchia M, 2010, P ACM WORKSH MULT FO, P37
   Bleicher A, 2010, IEEE SPECTRUM, V47, P16, DOI 10.1109/MSPEC.2010.5583451
   Broilo M, 2010, STUD COMPUT INTELL, V287, P3
   Chao GC, 2010, J VIS COMMUN IMAGE R, V21, P682, DOI 10.1016/j.jvcir.2010.05.002
   CUTTING DR, 1992, SIGIR 92 : PROCEEDINGS OF THE FIFTEENTH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P318
   Goldman DB, 2006, ACM T GRAPHIC, V25, P862, DOI 10.1145/1141911.1141967
   Höferlin B, 2011, MULTIMED TOOLS APPL, V55, P127, DOI 10.1007/s11042-010-0606-z
   Höferlin M, 2011, J SPAT INT SCI, P87, DOI 10.5311/JOSIS.2010.2.1
   Hoferlin M., 2011, P ACM INT C MULT RET, P9
   Holten D, 2009, COMPUT GRAPH FORUM, V28, P983, DOI 10.1111/j.1467-8659.2009.01450.x
   Ke WM, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P19, DOI 10.1145/1571941.1571947
   Laube P, 2007, COMPUT ENVIRON URBAN, V31, P481, DOI 10.1016/j.compenvurbsys.2007.08.002
   Lee F, 2011, LECT NOTES COMPUT SC, V6524, P219
   Liao TW, 2005, PATTERN RECOGN, V38, P1857, DOI 10.1016/j.patcog.2005.01.025
   Liu Y., 2007, TR200706 U N CAR SCH
   Majecka B., 2009, Statistical models of pedestrian behaviour in the forum
   Nilsson F., 2009, INTELLIGENT NETWORK
   Pritch Y, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P195, DOI 10.1109/AVSS.2009.53
   Schoeffmann K., 2010, SPIE Reviews, V1, P018004, DOI DOI 10.1117/6.0000005
   Schoeffmann K, 2009, INT WORK CONTENT MUL, P243, DOI 10.1109/CBMI.2009.40
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Talyor A., 2000, LIB SCI TEXT
   Vasiliev I.R., 1997, CARTOGRAPHICA, V34, P1, DOI DOI 10.3138/D357-234G-2M62-4373
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
NR 26
TC 17
Z9 21
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 908
EP 920
DI 10.1109/TMM.2013.2238521
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500018
DA 2024-07-18
ER

PT J
AU Zhou, L
   Chen, M
   Qian, Y
   Chen, HH
AF Zhou, Liang
   Chen, Min
   Qian, Yi
   Chen, Hsiao-Hwa
TI Fairness Resource Allocation in Blind Wireless Multimedia Communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE alpha-Fairness; blind communication; multimedia application; resource
   allocation
ID SCHEDULING SCHEME; ADAPTATION; STRATEGIES; MULTICAST; QUALITY; GAMES
AB Traditional alpha-fairness resource allocation in wireless multimedia communications assumes that the quality of experience (QoE) model (or utility function) of each user is available to the base station (BS), which may not be valid in many practical cases. In this paper, we consider a blind scenario where the BS has no knowledge of the underlying QoE model. Generally, this consideration raises two fundamental questions. Is it possible to set the fairness parameter alpha in a precisely mathematical manner? If so, is it possible to implement a specific alpha-fairness resource allocation scheme online? In this work, we will give positive answers to both questions. First, we characterize the tradeoff between the performance and fairness by providing an upper bound of the performance loss resulting from employing alpha-fairness scheme. Then, we decompose the alpha-fairness problem into two subproblems that describe the behaviors of the users and BS and design a bidding game for the reconciliation between the two subproblems. We demonstrate that, although all users behave selfishly, the equilibrium point of the game can realize the alpha-fairness efficiently, and the convergence time is reasonably short. Furthermore, we present numerical simulation results that confirm the validity of the analytical results.
C1 [Zhou, Liang] Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Nanjing 210046, Jiangsu, Peoples R China.
   [Chen, Min] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Qian, Yi] Univ Nebraska Lincoln, Dept Comp & Elect Engn, Omaha, NE 68101 USA.
   [Chen, Hsiao-Hwa] Natl Cheng Kung Univ, Dept Engn Sci, Tainan 70101, Taiwan.
C3 Nanjing University of Posts & Telecommunications; Huazhong University of
   Science & Technology; University of Nebraska System; University of
   Nebraska Lincoln; National Cheng Kung University
RP Zhou, L (corresponding author), Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Nanjing 210046, Jiangsu, Peoples R China.
EM liang.zhou@ieee.org; minchen@ieee.org; yqian2@unl.edu;
   hshwchen@mail.ncku.edu.tw
RI qian, yi/HZH-4175-2023; Chen, Min/N-9350-2015; Qian, Yi/KEI-0952-2024
OI Chen, Min/0000-0002-0960-4447; 
FU State Key Development Program of Basic Research of China [2013CB329005];
   National Natural Science Foundation of China [61201165, 61271240];
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions; Nanjing University of Posts and Telecommunications
   Foundation [NY211032]; National Science Council of Taiwan
   [NSC99-2221-E-006-016-MY3]
FX This work was supported in part by the State Key Development Program of
   Basic Research of China (2013CB329005), the National Natural Science
   Foundation of China under Grants 61201165 and 61271240, the Priority
   Academic Program Development of Jiangsu Higher Education Institutions,
   Nanjing University of Posts and Telecommunications Foundation under
   Grant NY211032, and the National Science Council of Taiwan under Grant
   NSC99-2221-E-006-016-MY3. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Monica
   Aguilar.
CR [Anonymous], 2009, LECT NOTES COMPUTER, V5894, P219
   Bertsimas D, 2012, MANAGE SCI, V58, P2234, DOI 10.1287/mnsc.1120.1549
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Chen Y, 2009, IEEE T MULTIMEDIA, V11, P1170, DOI 10.1109/TMM.2009.2026101
   Du HF, 2009, IEEE WIREL COMMUN, V16, P60, DOI 10.1109/MWC.2009.5109465
   Han Z, 2005, IEEE T COMMUN, V53, P1366, DOI 10.1109/TCOMM.2005.852826
   Han Z, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/980304
   Hou F, 2009, IEEE T WIREL COMMUN, V8, P1508, DOI 10.1109/TWC.2009.080417
   Hou IH, 2010, IEEE INFOCOM SER
   Hu DL, 2010, IEEE J SEL AREA COMM, V28, P434, DOI 10.1109/JSAC.2010.100414
   Hu H, 2011, IEEE T CIRC SYST VID, V21, P1013, DOI 10.1109/TCSVT.2011.2129290
   Jurca D, 2007, IEEE INT SYM MULTIM, P229, DOI 10.1109/ISM.2007.31
   Kaplow L., 2002, FAIRNESS VERSUS WELF
   Kelly F, 1997, EUR T TELECOMMUN, V8, P33, DOI 10.1002/ett.4460080106
   Khan A, 2010, IET COMMUN, V4, P1337, DOI 10.1049/iet-com.2009.0422
   Lan T, 2009, AXIOMATIC THEORY FAI
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Luo HY, 2010, IEEE COMMUN MAG, V48, P102, DOI 10.1109/MCOM.2010.5402671
   Maciel TF, 2010, IEEE T VEH TECHNOL, V59, P406, DOI 10.1109/TVT.2009.2029438
   Nguyen T, 2008, IEEE T MULTIMEDIA, V10, P523, DOI 10.1109/TMM.2008.917351
   Park H, 2007, IEEE T SIGNAL PROCES, V55, P3496, DOI 10.1109/TSP.2007.893755
   Park H, 2010, IEEE T CIRC SYST VID, V20, P297, DOI 10.1109/TCSVT.2009.2031767
   Rawls J., 2009, THEORY JUSTICE, DOI DOI 10.4159/9780674042605
   Shiang HP, 2010, IEEE T CIRC SYST VID, V20, P505, DOI 10.1109/TCSVT.2009.2035837
   Tang A, 2004, IEEE INFOCOM SER, P35
   Wang P, 2009, IEEE T WIREL COMMUN, V8, P3577, DOI 10.1109/TWC.2009.071406
   Zhang Q, 2011, IET COMMUN, V5, P396, DOI 10.1049/iet-com.2010.0173
   Zhang YS, 2010, IEEE T MULTIMEDIA, V12, P886, DOI 10.1109/TMM.2010.2065217
   Zhou LA, 2011, IEEE T VEH TECHNOL, V60, P692, DOI 10.1109/TVT.2010.2102782
   Zhou L, 2010, IEEE J SEL AREA COMM, V28, P409, DOI 10.1109/JSAC.2010.100412
   Zhu XQ, 2010, IEEE T CIRC SYST VID, V20, P1462, DOI 10.1109/TCSVT.2010.2077492
NR 31
TC 32
Z9 32
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 946
EP 956
DI 10.1109/TMM.2013.2237895
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500021
DA 2024-07-18
ER

PT J
AU Ma, ZG
   Nie, FP
   Yang, Y
   Uijlings, JRR
   Sebe, N
   Hauptmann, AG
AF Ma, Zhigang
   Nie, Feiping
   Yang, Yi
   Uijlings, Jasper R. R.
   Sebe, Nicu
   Hauptmann, Alexander G.
TI Discriminating Joint Feature Analysis for Multimedia Data Understanding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature analysis; image annotation; semi-supervised learning; sparsity;
   video concept detection; 3-D motion data analysis
AB In this paper, we propose a novel semi-supervised feature analyzing framework for multimedia data understanding and apply it to three different applications: image annotation, video concept detection and 3-D motion data analysis. Our method is built upon two advancements of the state of the art: 1) l(2,1)-norm regularized feature selection which can jointly select the most relevant features from all the data points. This feature selection approach was shown to be robust and efficient in literature as it considers the correlation between different features jointly when conducting feature selection; 2) manifold learning which analyzes the feature space by exploiting both labeled and unlabeled data. It is a widely used technique to extend many algorithms to semi-supervised scenarios for its capability of leveraging the manifold structure of multimedia data. The proposed method is able to learn a classifier for different applications by selecting the discriminating features closely related to the semantic concepts. The objective function of our method is non-smooth and difficult to solve, so we design an efficient iterative algorithm with fast convergence, thus making it applicable to practical applications. Extensive experiments on image annotation, video concept detection and 3-D motion data analysis are performed on different real-world data sets to demonstrate the effectiveness of our algorithm.
C1 [Ma, Zhigang; Uijlings, Jasper R. R.; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, Trento, Italy.
   [Nie, Feiping] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
   [Yang, Yi; Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
C3 University of Trento; University of Texas System; University of Texas
   Arlington; Carnegie Mellon University
RP Ma, ZG (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, Trento, Italy.
EM ma@disi.unitn.it; feip-ingnie@gmail.com; yiyang@cs.cmu.edu;
   uijlings@disi.unitn.it; sebe@disi.unitn.it; alex@cs.cmu.edu
RI Lang, Ming/HIK-0758-2022; Sebe, Niculae/KEC-2000-2024; Nie,
   Feiping/B-3039-2012; Ma, Zhigang/H-3543-2015; yang, yang/GWB-9426-2022;
   yang, yang/GVT-5210-2022; yang, yang/HGT-7999-2022; Yang, Yi/B-9273-2017
OI Sebe, Niculae/0000-0002-6597-7248; Yang, Yi/0000-0002-0512-880X
FU European Commission [FP7-248984 GLOCAL]; National Science Foundation
   [IIS-0917072]; National Institutes of Health (NIH) [1RC1MH090021-01];
   Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [0917072] Funding Source: National Science
   Foundation
FX The work of Z. Ma, J. R. R. Uijlings, and N. Sebe was supported in part
   by the European Commission under the contract FP7-248984 GLOCAL. The
   work of Y. Yang and A. G. Hauptmann was supported in part by the
   National Science Foundation under Grant No. IIS-0917072, and by the
   National Institutes of Health (NIH) Grant No. 1RC1MH090021-01. Any
   opinions, findings, and conclusions or recommendations expressed in this
   material are those of the author(s) and do not necessarily reflect the
   views of the National Science Foundation or the National Institutes of
   Health. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Alan Hanjalic.
CR [Anonymous], P CIVR
   [Anonymous], 2006, CS0608 BROWN U
   [Anonymous], P WORKSH LEARN PART
   [Anonymous], P SIAM INT C DAT MIN
   [Anonymous], P AAAI
   Argyriou A., 2007, P NIPS
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Cawley G., 2006, P NIPS
   Chen M.-Y., 2009, Tech. Rep. CMU-CS- 09-161
   Cohen I, 2004, IEEE T PATTERN ANAL, V26, P1553, DOI 10.1109/TPAMI.2004.127
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Hoi SCH, 2006, IEEE T KNOWL DATA EN, V18, P509, DOI 10.1109/TKDE.2006.1599389
   Hoi SCH, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823752
   Li H., 2006, P IEEE INT C DAT MIN
   Lin Y., 2005, P ACM MULT
   Loui A., 2007, P INT WORKSH MULT IN
   Ma Z., 2011, PROC ACM MULTIMEDIA
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Nie F., 2010, P NIPS, P1
   Nie FP, 2010, IEEE T IMAGE PROCESS, V19, P1921, DOI 10.1109/TIP.2010.2044958
   Ning H., 2008, P CVPR
   Wu F., 2010, P ACM MULT
   Yang Y., 2009, P ACM MULT
   Yang Y., 2011, P IJCAI
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yang Y, 2010, IEEE T IMAGE PROCESS, V19, P2761, DOI 10.1109/TIP.2010.2049235
   Zhao JD, 2008, NEUROCOMPUTING, V71, P1842, DOI 10.1016/j.neucom.2007.06.014
   Zhu X, 2003, ICML
   Zhu Xiaojin, 2007, 1530 U WISC
NR 30
TC 122
Z9 135
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1662
EP 1672
DI 10.1109/TMM.2012.2199293
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400015
DA 2024-07-18
ER

PT J
AU Wang, JJ
   Gong, YH
AF Wang, Jinjun
   Gong, Yihong
TI Discovering Image Semantics in Codebook Derivative Space
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codebook derivative; multiclass image classification; sparse-coding
ID CLASSIFICATION
AB The sparse coding based approaches for image recognition have recently shown improved performance than traditional bag-of-features technique. Due to high dimensionality of the image descriptor space, existing systems usually require very large codebook size to minimize coding error in order to get satisfactory accuracy. While most research efforts try to address the problem by constructing a relatively smaller codebook with stronger discriminative power, in this paper, we introduce an alternative solution by enhancing the quality of coding. Particularly, we apply the idea similar to Fisher kernel to the coding framework, where we use the image-dependent codebook derivative to represent the image. The proposed idea is generic across multiple coding criteria, and in this paper, it is applied to enhance the locality-constraint linear coding (LLC). Experiments show that, the extracted new feature, called "LLC+," achieved significantly improved accuracy on several challenging datasets even with a small codebook of 1/20 the reported size used by LLC. This obviously adds to LLC+ the modeling accuracy, processing speed and codebook training advantages.
C1 [Wang, Jinjun] Epson Res & Dev Inc, San Jose, CA 95131 USA.
   [Gong, Yihong] NEC Labs China, Beijing 100084, Peoples R China.
RP Wang, JJ (corresponding author), Epson Res & Dev Inc, San Jose, CA 95131 USA.
EM jwang@erd.epson.com; ygong@sv.nec-labs.com
CR [Anonymous], P NIPS VANC CAN
   [Anonymous], ICML HAIF ISR
   [Anonymous], P ICML HAIF ISR
   [Anonymous], NIPS VANC CAN
   [Anonymous], P CVPR
   [Anonymous], NIPS VANC CAN
   [Anonymous], 2006, 2006 C COMP VIS PATT
   [Anonymous], 2008, P CVPR
   [Anonymous], NIPS VANC CAN
   [Anonymous], 2004, Workshop on Generative Model Based Vision
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], ECCV CRET GREEC
   [Anonymous], P NIPS VANC CAN
   [Anonymous], NIPS VANC CAN
   Berg AC, 2005, PROC CVPR IEEE, P26
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Griffin Gregory, 2007, CALTECH 256 OBJECT C
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jurie F, 2005, IEEE I CONF COMP VIS, P604
   Lazebnik S, 2005, IEEE I CONF COMP VIS, P832, DOI 10.1109/ICCV.2005.10
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li Y., 2008, 4 INT C WIRELESS COM, P1
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Nister David, 2006, CVPR
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Perronnin F., 2007, P IEEE CVPR, P1
   Philbin J., 2008, P CVPR, P1
   Rigamonti R, 2011, PROC CVPR IEEE, P1545, DOI 10.1109/CVPR.2011.5995313
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Uijlings JRR, 2009, PROC CVPR IEEE, P770, DOI 10.1109/CVPRW.2009.5206663
   van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang XG, 2011, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2011.5995696
   Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhou X, 2009, IEEE I CONF COMP VIS, P1971, DOI 10.1109/ICCV.2009.5459435
NR 44
TC 9
Z9 9
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 986
EP 994
DI 10.1109/TMM.2012.2186120
PN 1
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300005
DA 2024-07-18
ER

PT J
AU Xiao, X
   Xu, CS
   Wang, JQ
   Xu, M
AF Xiao, Xian
   Xu, Changsheng
   Wang, Jinqiao
   Xu, Min
TI Enhanced 3-D Modeling for Landmark Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention analysis; attention-based 3-D reconstruction; landmark image
   classification; 3-D model enhancement
ID VISUAL-ATTENTION; SCENE; COLLECTIONS; FEATURES
AB Landmark image classification is a challenging task due to the various circumstances, e. g., illumination, viewpoint, zoom in/out and occlusion under which landmark images are taken. Most existing approaches utilize features extracted from the whole image including both landmark and non-landmark areas. However, non-landmark areas introduce redundant and noisy information. In this paper, we propose a novel approach to improve landmark image classification consisting of three steps. First, an attention-based 3-D reconstruction method is proposed to reconstruct sparse 3-D landmark models. Second, the sparse 3-D models are projected onto iconic images in order to identify images of the hot regions. For a landmark, hot regions are parts of a landmark which attract photographers' attention and are popularly captured in photos. These hot region images are later used to enhance reconstructed sparse 3-D models. Third, the landmark regions are obtained through mapping the enhanced 3-D models to landmark images. A k-dimensional tree (kd-tree) is then constructed for each landmark based on scale invariant feature transform (SIFT) features [5] extracted from the landmark area to classify unlabeled images into pre-defined landmark categories. The proposed method is evaluated using 291 661 images of 51 landmarks. Experiments of comparison indicate that our method outperforms bag-of-words (BoW) based approach [25] 18.5% and method of spatial-pyramid-matching using sparse-coding (ScSPM) [3] 8.4%.
C1 [Xiao, Xian; Xu, Changsheng; Wang, Jinqiao; Xu, Min] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China.
   [Xiao, Xian; Xu, Changsheng; Wang, Jinqiao; Xu, Min] China Singapore Inst Digital Media, Singapore, Singapore.
   [Xu, Min] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; University of
   Technology Sydney
RP Xiao, X (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China.
EM xxiao@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; jqwang@nlpr.ia.ac.cn;
   min.xu@uts.edu.au
RI xu, cj/HJZ-3488-2023
OI Xu, Min/0000-0001-9581-8849
FU National Program on Key Basic Research Project (973 Program)
   [2012CB316304]; National Natural Science Foundation of China [60970092,
   61003161, 60905008]
FX This work was supported in part by the National Program on Key Basic
   Research Project (973 Program, Project No. 2012CB316304) and National
   Natural Science Foundation of China (Grant No. 60970092, 61003161,
   60905008). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Samson Cheung.
CR [Anonymous], 2007, P SIGGRAPH
   [Anonymous], P CVPR
   [Anonymous], P WWW
   [Anonymous], P CVPR
   [Anonymous], P CVIU
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2008, P 16 ACM INT C MULT, DOI DOI 10.1145/1459359.1459410
   Bruce NDB, 2005, NEUROCOMPUTING, V65, P125, DOI 10.1016/j.neucom.2004.10.065
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Li XW, 2008, LECT NOTES COMPUT SC, V5302, P427
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Simon I, 2007, IEEE I CONF COMP VIS, P274
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Xiao X., 2010, Proceedings of the International Conference on Multimedia, Firenze, Italy, P719
   Xiao XA, 2010, IEEE INT CON MULTI, P1091, DOI 10.1109/ICME.2010.5582982
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
NR 26
TC 18
Z9 20
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1246
EP 1258
DI 10.1109/TMM.2012.2190384
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400011
DA 2024-07-18
ER

PT J
AU Ewert, S
   Müller, M
   Konz, V
   Muellensiefen, D
   Wiggins, GA
AF Ewert, Sebastian
   Mueller, Meinard
   Konz, Verena
   Muellensiefen, Daniel
   Wiggins, Geraint A.
TI Towards Cross-Version Harmonic Analysis of Music
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Alignment; chord recognition; music information retrieval; music
   synchronization
ID SYNCHRONIZATION; AUDIO; IDENTIFICATION; ACCOMPANIMENT; TRANSCRIPTION;
   ALIGNMENT; CHORDS
AB For a given piece of music, there often exist multiple versions belonging to the symbolic (e. g., MIDI representations), acoustic (audio recordings), or visual (sheet music) domain. Each type of information allows for applying specialized, domain-specific approaches to music analysis tasks. In this paper, we formulate the idea of a cross-version analysis for comparing and/or combining analysis results from different representations. As an example, we realize this idea in the context of harmonic analysis to automatically evaluate MIDI-based chord labeling procedures using annotations given for corresponding audio recordings. To this end, one needs reliable synchronization procedures that automatically establish the musical relationship between the multiple versions of a given piece. This becomes a hard problem when there are significant local deviations in these versions. We introduce a novel late-fusion approach that combines different alignment procedures in order to identify reliable parts in synchronization results. Then, the cross-version comparison of the various chord labeling results is performed only on the basis of the reliable parts. Finally, we show how inconsistencies in these results across the different versions allow for a quantitative and qualitative evaluation, which not only indicates limitations of the employed chord labeling strategies but also deepens the understanding of the underlying music material.
C1 [Ewert, Sebastian] Univ Bonn, Multimedia Signal Proc Grp, Dept Comp Sci 3, Bonn, Germany.
   [Mueller, Meinard; Konz, Verena] Univ Saarland, D-6600 Saarbrucken, Germany.
   [Mueller, Meinard; Konz, Verena] Max Planck Inst Informat, Saarbrucken, Germany.
   [Muellensiefen, Daniel] Univ London, Dept Psychol, London, England.
   [Wiggins, Geraint A.] Univ London, Ctr Digital Mus, Sch Elect Engn & Comp Sci, London, England.
C3 University of Bonn; Saarland University; Max Planck Society; University
   of London; University of London
RP Ewert, S (corresponding author), Univ Bonn, Multimedia Signal Proc Grp, Dept Comp Sci 3, Bonn, Germany.
EM ewerts@iai.uni-bonn.de; meinard@mpi-inf.mpg.de; vkonz@mpi-inf.mpg.de;
   d.mullensiefen@gold.ac.uk; geraint.wiggins@eecs.qmul.ac.uk
RI Mueller, Meinard/U-2097-2019; Wiggins, Geraint/K-9443-2016; Ewert,
   Sebastian/G-8412-2016
OI Mueller, Meinard/0000-0001-6062-7524; Wiggins,
   Geraint/0000-0002-1587-112X; Ewert, Sebastian/0000-0002-0718-0476
FU German Research Foundation (DFG) [CL 64/6-1]; Cluster of Excellence on
   Multimodal Computing and Interaction (MMCI)
FX The work of S. Ewert was supported by the German Research Foundation
   (DFG CL 64/6-1). The work of M. Muller and V. Konz was supported by
   Cluster of Excellence on Multimodal Computing and Interaction (MMCI).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Svetha Venkatesh.
CR [Anonymous], 2005, P INT C MUS INF RETR
   [Anonymous], 2007, INFORM RETRIEVAL MUS
   [Anonymous], P INT C MUS INF RETR
   [Anonymous], 2010, MIREX AUDIO CHORD ES
   [Anonymous], 1993, PRENTICE HALL SIGNAL
   [Anonymous], P IEEE INT C AC SPEE
   [Anonymous], P IEEE WORKSH APPL S
   [Anonymous], 2003, P INT C MUS INF RETR
   Arifi V., 2004, Computing in Musicology, V13, P9
   Cont A, 2010, IEEE T PATTERN ANAL, V32, P974, DOI 10.1109/TPAMI.2009.106
   Dannenberg R, 1984, P 1984 INT COMP MUS, P193
   Dannenberg RB, 2006, COMMUN ACM, V49, P38, DOI 10.1145/1145287.1145311
   Ewert S., 2009, LECT NOTES COMPUTER, V6535, P35
   Ewert S, 2009, INT CONF ACOUST SPEE, P1869, DOI 10.1109/ICASSP.2009.4959972
   Foote J, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P77, DOI 10.1145/319463.319472
   Fujihara H, 2011, IEEE J-STSP, V5, P1252, DOI 10.1109/JSTSP.2011.2159577
   Gomez E., 2006, THESIS UPF BARCELONA
   Goto M., 2002, P ISMIR, V2
   Han Y., 2010, P INT SOC MUS INF RE, P315
   Hasler N, 2009, PROC CVPR IEEE, P224, DOI 10.1109/CVPRW.2009.5206859
   Itoyama Katsutoshi., 2008, ISMIR, P133
   Kan MY, 2008, IEEE T AUDIO SPEECH, V16, P338, DOI 10.1109/TASL.2007.911559
   Konz V., 2010, 11 INT SOC MUSIC INF, P9
   Lee K, 2008, IEEE T AUDIO SPEECH, V16, P291, DOI 10.1109/TASL.2007.914399
   Lerdahl Fred., 1983, A Generative Theory of Tonal Music
   Maddage NC, 2006, IEEE MULTIMEDIA, V13, P65, DOI 10.1109/MMUL.2006.3
   Mauch M, 2010, IEEE T AUDIO SPEECH, V18, P1280, DOI 10.1109/TASL.2009.2032947
   Maxwell J. H., 1992, EXPERT SYSTEM HARMON, P335
   Mayer Rudolf., 2011, P INT C MUSIC INFORM, P675
   McKay Cory., 2010, Proceedings of the international conference on Multimedia information retrieval, P257
   Müller M, 2008, INT CONF ACOUST SPEE, P65, DOI 10.1109/ICASSP.2008.4517547
   Muller M., 2006, Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, P437
   Muller M, 2005, P 6 INT C MUSIC INFO, P288
   Orio N., 2003, Proc. International Conference on New Interfaces for Musical Expression (NIME), P36, DOI DOI 10.5281/ZENODO.1176547.URL
   Pardo B., 2001, CSETR43901 U MICH DE
   Pevzner P.A., 2000, COMPUTATIONAL MOL BI
   Raphael C, 2004, COMPUT MUSIC J, V28, P45, DOI 10.1162/0148926041790676
   Raphael C, 2001, J COMPUT GRAPH STAT, V10, P487, DOI 10.1198/106186001317115081
   RAPHAEL C, 2004, P 5 INT C MUS INF RE, P387
   Rhodes C., 2009, P INT C MATH COMP MU, P107
   Ryynänen MP, 2008, COMPUT MUSIC J, V32, P72, DOI 10.1162/comj.2008.32.3.72
   Scholz Ricardo., 2008, Proceedings of the 9th International Conference on Music Information Retrieval, P27
   Serrà J, 2008, IEEE T AUDIO SPEECH, V16, P1138, DOI 10.1109/TASL.2008.924595
   Sleator D., 2003, MELISMA MUSIC ANAL
   SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5
   Temperley D., 2007, Music and probability
   Temperley David., 2001, COGNITION BASIC MUSI
   WIGGINS G, 1993, COMPUT MUSIC J, V17, P31, DOI 10.2307/3680941
   WINOGRAD T, 1968, J MUSIC THEORY, V12, P2, DOI DOI 10.2307/842885
NR 49
TC 4
Z9 7
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 770
EP 782
DI 10.1109/TMM.2012.2190047
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Daras, P
   Axenopoulos, A
   Litos, G
AF Daras, Petros
   Axenopoulos, Apostolos
   Litos, Georgios
TI Investigating the Effects of Multiple Factors Towards More Accurate 3-D
   Object Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D object retrieval; descriptor extraction; feature selection; manifold
   learning; rotation estimation
AB This paper proposes a novel framework for 3-D object retrieval, taking into account most of the factors that may affect the retrieval performance. Initially, a novel 3-D model alignment method is introduced, which achieves accurate rotation estimation through the combination of two intuitive criteria, plane reflection symmetry and rectilinearity. After the pose normalization stage, a low-level descriptor extraction procedure follows, using three different types of descriptors, which have been proven to be effective. Then, a novel combination procedure of the above descriptors takes place, which achieves higher retrieval performance than each descriptor does separately. The paper provides also an in-depth study of the factors that can further improve the 3-D object retrieval accuracy. These include selection of the appropriate dissimilarity metric, feature selection/dimensionality reduction on the initial low-level descriptors, as well as manifold learning for re-ranking of the search results. Experiments performed on two 3-D model benchmark datasets confirm our assumption that future research in 3-D object retrieval should focus more on the efficient combination of low-level descriptors as well as on the selection of the best features and matching metrics, than on the investigation of the optimal 3-D object descriptor.
C1 [Daras, Petros; Axenopoulos, Apostolos; Litos, Georgios] Ctr Res & Technol Hellas, Informat & Telemat Inst, GR-57001 Thessaloniki, Greece.
C3 Centre for Research & Technology Hellas
RP Daras, P (corresponding author), Ctr Res & Technol Hellas, Informat & Telemat Inst, GR-57001 Thessaloniki, Greece.
EM daras@iti.gr; axenop@iti.gr; gl@iti.gr
RI Daras, Petros/F-5284-2012
OI Daras, Petros/0000-0003-3814-6710
FU EC
FX Manuscript received March 27, 2011; revised August 23, 2011; accepted
   November 05, 2011. Date of publication November 15, 2011; date of
   current version March 21, 2012. This work was supported by the EC-funded
   project I-SEARCH. The associate editor coordinating the review of this
   manuscript and approving it for publication was Christophe De
   Vleeschouwer.
CR Akgul C., 2009, P 30 INT C EUROGRAPH
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Axenopoulos A., 2011, P 1 ACM INT C MULT R
   Bonev B., 2010, LECT NOTES COMPUTER
   Chang MC, 2011, COMPUT VIS IMAGE UND, V115, P707, DOI 10.1016/j.cviu.2010.10.013
   Chaouch M., 2007, P IEEE INT C MULT EX
   Chaouch M, 2009, GRAPH MODELS, V71, P63, DOI 10.1016/j.gmod.2008.12.006
   Daras P., 2009, SPRINGER INT J C JUL
   Dash M., 1997, Intelligent Data Analysis, V1
   Dutagaci H., 2011, P 4 EUR WORKSH 3 D O
   Funkhouser T., 2006, P 4 EUROGRAPHICS S G, P131
   Goodall S., 2005, STORAGE RETRIEV METH
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Hall M. A., 2000, P 17 INT C MACH LEAR, P359, DOI DOI 10.5555/645529.657793
   He J. R., 2004, P ACM MM NEW YORK
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Johnson R.A., 2007, Applied Multivariate Statistical Analysis, V47, DOI DOI 10.1198/TECH.2005.S319
   JOLLIFFE IT, 1986, PRINCIPLAL COMPONENT
   Jonhson N. L., 1994, CONTINUOUS UNIVARIAT, V1
   KAZHDAN M, 2004, P S GEOM PROC
   KRIEGEL HP, 2003, P IEEE 8 INT C DAT S
   Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539
   Lian Z., INT J COMPUT VISI, V89, P130
   LING, 2006, P IEEE C CVPR NEW YO, V1, P246
   Liu H., 1996, P 13 INT C MACHINE L, P319
   Lupatini G, 1998, EIGHTH INTERNATIONAL WORKSHOP ON RESEARCH ISSUES IN DATA ENGINEERING - CONTINUOUS-MEDIA DATABASES AND APPLICATIONS, PROCEEDINGS, P34, DOI 10.1109/RIDE.1998.658276
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   Maes C., 2010, P BTAS 10
   Marini S., 2010, P EUR WORKSH 3D OBJ
   MARKO RS, 1997, P 14 INT C MACH LEAR, P296
   Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462
   MINOVIC P, 1993, IEEE T PATTERN ANAL, V15, P507, DOI 10.1109/34.211472
   NOVOTNI M, 2005, CG20052 FRIEDR WILH
   Ohbuchi R., 2006, P ACM MIR SANT BAR C
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Paquet E, 1999, IMAGE VISION COMPUT, V17, P157, DOI 10.1016/S0262-8856(98)00119-X
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   PU JT, 2005, P ASME IDETC CIE 200, P301
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Saul L. K., 2003, J MACH LEARN RES
   Smeets D, 2009, LECT NOTES COMPUT SC, V5702, P757, DOI 10.1007/978-3-642-03767-2_92
   Sun CM, 1997, IEEE T PATTERN ANAL, V19, P164, DOI 10.1109/34.574800
   Tal A, 2006, GRAPP 2006: PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P142
   ThiruvadandamPorethi V, 2010, P EUR WORKSH 3D OBJ
   Tung T., 2005, International Journal of Shape Modeling, V11, P91, DOI 10.1142/S0218654305000748
   Vranic DV, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P963
   Vranic DV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P757
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   Yang F., 2007, P IEEE INT C COMP AI
   Yang Y., 2009, P ACM MM BEIJ CHIN
   ZARPALAS D, 2007, EURASIP J ADV SIG PR, V2007, P14
   [No title captured]
NR 53
TC 25
Z9 26
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 374
EP 388
DI 10.1109/TMM.2011.2176111
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500012
DA 2024-07-18
ER

PT J
AU Mohanta, PP
   Saha, SK
   Chanda, B
AF Mohanta, Partha Pratim
   Saha, Sanjoy Kumar
   Chanda, Bhabatosh
TI A Model-Based Shot Boundary Detection Technique Using Frame Transition
   Parameters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Abrupt change; frame estimation; gradual change; shot detection;
   transition model
ID SEGMENTATION
AB We have presented a unified model for detecting different types of video shot transitions. Based on the proposed model, we formulate frame estimation scheme using the previous and the next frames. Unlike other shot boundary detection algorithms, instead of properties of frames, frame transition parameters and frame estimation errors based on global and local features are used for boundary detection and classification. Local features include scatter matrix of edge strength and motion matrix. Finally, the frames are classified as no change (within shot frame), abrupt change, or gradual change frames using a multilayer perceptron network. The proposed method is relatively less dependent on user defined thresholds and is free from sliding window size as widely used by various schemes found in the literature. Moreover, handling both abrupt and gradual transitions along with non-transition frames under a single framework using model guided visual feature is another unique aspect of the work.
C1 [Mohanta, Partha Pratim; Chanda, Bhabatosh] Indian Stat Inst, Elect & Commun Sci Unit, Kolkata, India.
   [Saha, Sanjoy Kumar] Jadavpur Univ, Comp Sci & Engn Dept, Kolkata, W Bengal, India.
C3 Indian Statistical Institute; Indian Statistical Institute Kolkata;
   Jadavpur University
RP Mohanta, PP (corresponding author), Indian Stat Inst, Elect & Commun Sci Unit, Kolkata, India.
EM ppmo-hanta@isical.ac.in; sks_ju@yahoo.co.in; chanda@isical.ac.in
CR Adjeroh D, 2009, EURASIP J IMAGE VIDE, DOI 10.1155/2009/859371
   [Anonymous], 1995, P ACM MULT, DOI DOI 10.1145/217279.215266
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Bescós J, 2005, IEEE T MULTIMEDIA, V7, P293, DOI 10.1109/TMM.2004.840598
   Bhattacharyya Anil, 1943, B CALCUTTA MATH SOC, V35, P99
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Cerneková Z, 2006, IEEE T CIRC SYST VID, V16, P82, DOI 10.1109/TCSVT.2005.856896
   Chau LP, 2003, SIGNAL PROCESS, V83, P671, DOI 10.1016/S0165-1684(02)00451-6
   Chen LH, 2008, PATTERN RECOGN, V41, P1056, DOI 10.1016/j.patcog.2007.07.024
   Cooper M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P502, DOI 10.1109/ICME.2005.1521470
   Cooper M, 2007, IEEE T MULTIMEDIA, V9, P610, DOI 10.1109/TMM.2006.888015
   Gonzalez R.C., 1987, DIGITAL IMAGE PROCES, VSecond
   Grana C, 2007, IEEE T CIRC SYST VID, V17, P483, DOI 10.1109/TCSVT.2006.888818
   Hampapur A., 1995, MULTIMED TOOLS APPL, V1, P1
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   Haoran Y., 2006, INFORM SYST, V31, P638
   Huang CL, 2001, IEEE T CIRC SYST VID, V11, P1281, DOI 10.1109/76.974682
   Jaffre G., 2004, P TRECVID WORKSH
   Jinhui Yuan, 2005, 13th Annual ACM International Conference on Multimedia, P539, DOI 10.1145/1101149.1101271
   Lee MH, 2006, EXPERT SYST APPL, V31, P13, DOI 10.1016/j.eswa.2005.09.031
   Lienhart R, 2001, PROC SPIE, V4315, P219, DOI 10.1117/12.410931
   Ling X, 2008, MUE: 2008 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND UBIQUITOUS ENGINEERING, PROCEEDINGS, P394, DOI 10.1109/MUE.2008.102
   Liu X., 2003, P ICASSP
   Mas J., 2003, TRECVID2003
   Mohanta PP, 2007, LECT NOTES COMPUT SC, V4815, P641
   Murai Y., 2008, P ICPR
   Patel NV, 1997, PATTERN RECOGN, V30, P583, DOI 10.1016/S0031-3203(96)00114-8
   Porter S., 2001, BRIT MACH VIS C BMVC, P73
   Qing LY, 2003, LECT NOTES COMPUT SC, V2690, P1097
   Tsamoura E, 2008, IEEE IMAGE PROC, P45, DOI 10.1109/ICIP.2008.4711687
   Won JU, 2003, ITRE2003: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: RESEARCH AND EDUCATION, P104
   Xiang-Wei L., 2009, RES J INF TECHNOL, V1, P70
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   Yoo HW, 2006, MULTIMED TOOLS APPL, V28, P283, DOI 10.1007/s11042-006-7715-8
   Yuan JH, 2007, IEEE T CIRC SYST VID, V17, P168, DOI 10.1109/TCSVT.2006.888023
   Zhao Huan, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P1124, DOI 10.1109/CSSE.2008.939
NR 36
TC 57
Z9 63
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 223
EP 233
DI 10.1109/TMM.2011.2170963
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100021
DA 2024-07-18
ER

PT J
AU Wang, F
   Ngo, CW
AF Wang, Feng
   Ngo, Chong-Wah
TI Summarizing Rushes Videos by Motion, Object, and Event Understanding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion analysis; object and event understanding; rushes video
   structuring; video summarization
AB Rushes footages are considered as cheap gold mine with the potential for reuse in broadcasting and filmmaking industries. However, mining "gold" from unedited videos such as rushes is challenging as the reusable segments are buried in a large set of redundant information. In this paper, we propose a unified framework for stock footage classification and summarization to support video editors in navigating and organizing rushes videos. Our approach is composed of two steps. First, we employ motion features to filter the undesired camera motion and locate the stock footage. A hierarchical hidden Markov model (HHMM) is proposed to model the motion feature distribution and classify video segments into different categories to decide their potential for reuse. Second, we generate a short video summary to facilitate quick browsing of the stock footages by including the objects and events that are important for storytelling. For objects, we detect the presence of persons and moving objects. For events, we extract a set of features to detect and describe visual (motion activities and scene changes) and audio events (speech clips). A representability measure is then proposed to select the most representative video clips for video summarization. Our experiments show that the proposed HHMM significantly outperforms other methods based on SVM, FSM, and HMM. The automatically generated rushes summaries are also demonstrated to be easy-to-understand, containing little redundancy, and capable of including ground-truth objects and events with shorter durations and relatively pleasant rhythm based on the TRECVID 2007, 2008, and our subjective evaluations.
C1 [Wang, Feng] E China Normal Univ, Dept Comp Sci & Technol, Shanghai 200241, Peoples R China.
   [Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 East China Normal University; City University of Hong Kong
RP Wang, F (corresponding author), E China Normal Univ, Dept Comp Sci & Technol, Shanghai 200241, Peoples R China.
EM fwang@cs.ecnu.edu.cn; cwngo@cs.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 119610]; National Natural Science Foundation of China
   [61103127]
FX This work was supported in part by a grant from the Research Grants
   Council of the Hong Kong Special Administrative Region, China (CityU
   119610) and in part by a grant from the National Natural Science
   Foundation of China (No. 61103127). The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Changsheng Xu.
CR Allen B. P., 2006, P NIST TRECVID WORKS
   [Anonymous], TRECVID WORKSH
   [Anonymous], LEARNING HIERARCHICA
   Bailer W., 2008, P TRECVID WORKSH BBC
   Bailer W., 2007, P ACM C IM VID RETR
   Byrne D., 2007, P TRECVID WORKSH BBC
   Chen F., 2007, P TRECVID WORKSH BBC
   Christel M. G., 2008, P TRECVID WORKSH BBC
   Dumont E., 2007, P TRECVID WORKSH BBC
   Ferman AM, 2003, IEEE T MULTIMEDIA, V5, P244, DOI 10.1109/TMM.2003.811617
   Hauptmann A. G., 2007, P TRECVID WORKSH BBC
   Hua X. S., 2006, P NIST TRECVID WORKS
   Ishihara K., 2008, P NIST TRECVID WORKS
   Kim C, 2002, IEEE T CIRC SYST VID, V12, P1128, DOI 10.1109/TCSVT.2002.806813
   Koskela M., 2008, P NIST TRECVID WORKS
   Liu Y., 2008, P TRECVID WORKSH BBC
   Liu Z., 2008, P TRECVID WORKSH BBC
   MA YF, 2002, P ACM MULT C
   Mei T, 2007, IEEE T CIRC SYST VID, V17, P699, DOI 10.1109/TCSVT.2007.896640
   Naci S., 2008, P TRECVID WORKSH BBC
   Nam J., 1999, P ACM MULT C
   Ngo C. W., 2006, P ACM MULT C
   Ngo C. W., 2006, P ACM INT C IM VID R
   Ngo C. W., 2005, P TRECVID WORKSH
   Ngo CW, 2001, IEEE T CIRC SYST VID, V11, P941, DOI 10.1109/76.937435
   Over P., 2007, P TRECVID WORKSH BBC
   Over P., 2008, P TRECVID WORKSH BBC
   Pan Z., 2004, P ACM SIGMM INT WORK
   Pan ZL, 2007, IEEE T MULTIMEDIA, V9, P268, DOI 10.1109/TMM.2006.887992
   Ren J., 2008, P TRECVID WORKSH BBC
   Ren JC, 2009, IEEE T MULTIMEDIA, V11, P906, DOI 10.1109/TMM.2009.2021782
   Rousseeuw P.J., 1987, ROBUST REGRESSION OU
   Sasongko J., 2008, P TRECVID WORKSH BBC
   Tang L., 2009, P ACM MULT
   Tang S., 2006, P NIST TRECVID WORKS
   Taskiran CM, 2006, IEEE T MULTIMEDIA, V8, P775, DOI 10.1109/TMM.2006.876282
   Truong B. T., 2006, P NIST TRECVID WORKS
   Truong B. T., 2006, ACM T MULTIM COMPUT, V3, P1
   Truong B. T., 2007, P TRECVID WORKSH RUS
   Wang F., 2007, P TRECVID WORKSH RUS
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wang T., 2008, P TRECVID WORKSH BBC
   You JY, 2007, IEEE T CIRC SYST VID, V17, P273, DOI 10.1109/TCSVT.2007.890857
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
NR 45
TC 33
Z9 33
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 76
EP 87
DI 10.1109/TMM.2011.2165531
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ntalampiras, S
   Potamitis, I
   Fakotakis, N
AF Ntalampiras, Stavros
   Potamitis, Ilyas
   Fakotakis, Nikos
TI Probabilistic Novelty Detection for Acoustic Surveillance Under
   Real-World Conditions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio signal processing; MPEG-7 standard; probabilistic novelty
   detection; public safety; wavelet packets
AB Novelty detection in the machine learning context refers to identifying unknown/novel data, i.e., data which vary greatly from the ones that the system was trained with. This paper explores this technique as applied to acoustic surveillance of abnormal situations. The ultimate goal of the system is to help an authorized person towards taking the appropriate actions for preventing life/property loss. A wide variety of acoustic parameters is employed towards forming a multidomain feature vector, which captures diverse characteristics of the audio signals. Subsequently the feature coefficients are fed to three probabilistic novelty detection methodologies. Their performance is computed using two measures which take into account misdetections and false alarms. Out dataset was recorded under real-world conditions including three different locations where various types of normal and abnormal sound events were captured. A smart-home environment, an open public space, and an office corridor were used. The results indicate that probabilistic novelty detection can provide an accurate analysis of the audio scene to identify abnormal events.
C1 [Ntalampiras, Stavros; Fakotakis, Nikos] Univ Patras, Dept Elect & Comp Engn, Patras 26500, Greece.
   [Potamitis, Ilyas] Technol Educ Inst Crete, Dept Mus Technol & Acoust, Daskalaki Perivolia 74100, Greece.
C3 University of Patras; Hellenic Mediterranean University
RP Ntalampiras, S (corresponding author), Univ Patras, Dept Elect & Comp Engn, Patras 26500, Greece.
EM snta-lampiras@upatras.gr; potamitis@staff.teicrete.gr;
   fakotaki@upatras.gr
RI Ntalampiras, Stavros/W-5636-2019
OI Ntalampiras, Stavros/0000-0003-3482-9215; Potamitis,
   Ilyas/0000-0002-7111-3331
FU EC
FX This work was supported by the EC FP 7th PROMETHEUS project. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Gerald Schuller.
CR Clavel C, 2008, SPEECH COMMUN, V50, P487, DOI 10.1016/j.specom.2008.03.012
   Clavel C, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P1307
   Davy M, 2006, SIGNAL PROCESS, V86, P2009, DOI 10.1016/j.sigpro.2005.09.027
   EMAMIAN V, 2000, P ICASSP, P3891
   Fan W, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P123, DOI 10.1109/ICDM.2001.989509
   FLEXER A, 2005, P 6 INT C MUS INF RE, P252
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P452, DOI 10.1109/ICME.2000.869637
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Kim HG, 2005, MPEG-7 AUDIO AND BEYOND: AUDIO CONTENT INDEXING AND RETRIEVAL, P1, DOI 10.1002/0470093366
   Kwan C, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/96706
   LINARES G, 1997, P ICASSP, P3365
   Markou M, 2003, SIGNAL PROCESS, V83, P2481, DOI 10.1016/j.sigpro.2003.07.018
   NTALAMPIRAS S, 2009, P 16 DIG SIGN PROC C, P1
   Ntalampiras S, 2009, EURASIP J AUDIO SPEE, DOI 10.1155/2009/594103
   Ntalampiras S, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/807162
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Richard G, 2007, INT CONF ACOUST SPEE, P461
   Tarassenko L, 1995, P 4 IEE INT C ART NE, V4, P442
   Tax D. M. J., 1998, Advances in Pattern Recognition. Joint IAPR International Workshops SSPR'98 and SPR'98. Proceedings, P593, DOI 10.1007/BFb0033283
   Valenzise G, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P21, DOI 10.1109/AVSS.2007.4425280
   Zhou GJ, 2001, IEEE T SPEECH AUDI P, V9, P201, DOI 10.1109/89.905995
NR 21
TC 91
Z9 95
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 713
EP 719
DI 10.1109/TMM.2011.2122247
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300011
DA 2024-07-18
ER

PT J
AU Okwechime, D
   Ong, EJ
   Bowden, R
AF Okwechime, Dumebi
   Ong, Eng-Jon
   Bowden, Richard
TI MIMiC: Multimodal Interactive Motion Controller
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Animation; human-computer interaction; motion; probability density
   function; synthesis
AB We introduce a new algorithm for real-time interactive motion control and demonstrate its application to motion captured data, prerecorded videos, and HCI. Firstly, a data set of frames are projected into a lower dimensional space. An appearance model is learnt using a multivariate probability distribution. A novel approach to determining transition points is presented based on k-medoids, whereby appropriate points of intersection in the motion trajectory are derived as cluster centers. These points are used to segment the data into smaller subsequences. A transition matrix combined with a kernel density estimation is used to determine suitable transitions between the subsequences to develop novel motion. To facilitate real-time interactive control, conditional probabilities are used to derive motion given user commands. The user commands can come from any modality including auditory, touch, and gesture. The system is also extended to HCI using audio signals of speech in a conversation to trigger nonverbal responses from a synthetic listener in real-time. We demonstrate the flexibility of the model by presenting results ranging from data sets composed of vectorized images, 2-D, and 3-D point representations. Results show real-time interaction and plausible motion generation between different types of movement.
C1 [Okwechime, Dumebi; Ong, Eng-Jon; Bowden, Richard] Univ Surrey, Dept Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 University of Surrey
RP Okwechime, D (corresponding author), Univ Surrey, Dept Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
EM d.okwechime@surrey.ac.uk; e.ong@surrey.ac.uk; r.bowden@surrey.ac.uk
RI Bowden, Richard/AAF-8283-2019
OI Bowden, Richard/0000-0003-3285-8020
FU EPSRC [EP/E027946/1]; European Community [231135-DictaSign]; EPSRC
   [EP/E027946/1] Funding Source: UKRI
FX This work was supported by the EPSRC project LILiR (EP/E027946/1) and
   the European Community's Seventh Framework Programme (FP7/2007-2013)
   under grant agreement number 231135-DictaSign. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Sheila S. Hemami.
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Balci K, 2009, 2009 24TH INTERNATIONAL SYMPOSIUM ON COMPUTER AND INFORMATION SCIENCES, P435
   BEAUDOIN P, 2008, P 2008 ACM SIGGRAPH, P117
   BHAT K, P SIGGRAPH 2004
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   FLAGG M, 2009, P 2009 S INT 3D GRAP, P199
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   JEBARA T, 1998, P WORKSH INT VIS MOT
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   KWATRA V, P SIGGRAPH 2003, P277
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   MERTINS A, P 2006 IEEE INT C AC, V5
   Moore A., 1991, THESIS U CAMBRIDGE C
   OKWECHIME D, 2008, P 5 C ART MOT DEF OB
   OKWECHIME D, 2009, P IEEE INT WORKSH HU
   ONG EJ, 2009, P C COMP VIS ICCV 20
   PULLEN K, 2002, ACM T GRAPH TOG
   RACHEL H, 2007, P 24 INT S INT 3D IN, P129
   Sahouria E, 1999, IEEE T CIRC SYST VID, V9, P1290, DOI 10.1109/76.809163
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   SHIN H, 2006, P 2006 ACM SIGGRAPH, P298
   Stone M, 2004, ACM T GRAPHIC, V23, P506, DOI 10.1145/1015706.1015753
   Szummer M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P823, DOI 10.1109/ICIP.1996.560871
   TANCO LM, 2000, P IEE WORKSH HUM MOT
   Treuille A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239458
   Troje NF, 2002, J VISION, V2, P371, DOI 10.1167/2.5.2
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
NR 32
TC 2
Z9 2
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 255
EP 265
DI 10.1109/TMM.2010.2096410
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800008
OA Green Published
DA 2024-07-18
ER

PT J
AU Chew, BS
   Chau, LP
   Yap, KH
AF Chew, Boon-Seng
   Chau, Lap-Pui
   Yap, Kim-Hui
TI A Fuzzy Clustering Algorithm for Virtual Character Animation
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compression; fuzzy c-mean clustering and realism; virtual character
   animation
ID COMPRESSION
AB The use of realistic humanoid animations generated through motion capture (MoCap) technology is widespread across various 3-D applications and industries. However, the existing compression techniques for such representation often do not consider the implicit coherence within the anatomical structure of a human skeletal model and lacks portability for transmission consideration. In this paper, a novel concept virtual character animation image (VCAI) is proposed. Built upon a fuzzy clustering algorithm, the data similarity within the anatomy structure of a virtual character (VC) model is jointly considered with the temporal coherence within the motion data to achieve efficient data compression. Since the VCA is mapped as an image, the use of image processing tool is possible for efficient compression and delivery of such content across dynamic network. A modified motion filter (MMF) is proposed to minimize the visual discontinuity in VCA's motion due to the quantization and transmission error. The MMF helps to remove high frequency noise components and smoothen the motion signal providing perceptually improved VCA with lessened distortion. Simulation results show that the proposed algorithm is competitive in compression efficiency and decoded VCA quality against the state-of-the-art VCA compression methods, making it suitable for providing quality VCA animation to low-powered mobile devices.
C1 [Chew, Boon-Seng; Chau, Lap-Pui; Yap, Kim-Hui] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Chew, BS (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM bschew@pmail.ntu.edu.sg; elpchau@ntu.edu.sg; ekhyap@ntu.edu.sg
RI Chau, Lap-Pui/A-5149-2011; Yap, Kim-Hui/A-5157-2011
OI Chau, Lap-Pui/0000-0003-4932-0593; Yap, Kim-Hui/0000-0003-1933-4986
FU NSF [EIA-0196217]
FX Manuscript received January 14, 2010; revised May 17, 2010; accepted
   September 21, 2010. Date of publication September 30, 2010; date of
   current version January 19, 2011. The data used in this project was
   obtained from mocap.cs.cmu.edu. The database was created with funding
   from NSF EIA-0196217. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Nadia
   Magnenat-Thalmann.
CR ALLIEZ P, 2003, P S MULT GEOM MOD
   [Anonymous], CMU GRAPH LAB MOT CA
   ARIKAN O, 2006, P ACM SIGGRAPH 06, P890
   Chattopadhyay S, 2007, IEEE T MULTIMEDIA, V9, P1, DOI 10.1109/TMM.2006.886326
   Chattopadhyay S, 2007, IEEE T VIS COMPUT GR, V13, P5, DOI 10.1109/TVCG.2007.13
   Chew BS, 2009, IEEE INT SYMP CIRC S, P1461, DOI 10.1109/ISCAS.2009.5118042
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Giacomo T. D., 2004, COMPUT GRAPH, V28, P65
   Grassia F. S., 1998, J. Graph. Tools, V6, DOI [10.1080/10867651.1998.10487493, DOI 10.1080/10867651.1998.10487493]
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   He Y., 2009, Proceeding of ACM international conference on Multimedia, P431
   *HUM AN WORK GROUP, 2001, H AN 1 1
   *ISO, 2002, 1449616 ISOIEC
   *JPEG COMM, 2004, 1544412004 ISOIEC JE
   Kalra P, 1998, IEEE COMPUT GRAPH, V18, P42, DOI 10.1109/38.708560
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Lin NH, 2007, IEEE T CONSUM ELECTR, V53, P182, DOI 10.1109/TCE.2007.339523
   Liu Guodong., 2006, SCA 06, P127
   Magnenat-Thalmann Nadia, 2006, IJVR, V5, P15
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Payan F, 2007, COMPUT GRAPH-UK, V31, P77, DOI 10.1016/j.cag.2006.09.009
   Preda M, 2004, IEEE T CIRC SYST VID, V14, P975, DOI 10.1109/TCSVT.2004.830661
   Preda M, 2002, SIGNAL PROCESS-IMAGE, V17, P717, DOI 10.1016/S0923-5965(02)00077-2
   Preda M, 2007, WEB3D 2007 - 12TH INTERNATIONAL CONFERENCE ON 3D WEB TECHNOLOGY, PROCEEDINGS, P181
   PRETEUX F, 2005, ENCY INFORM SCI TECH, V1, P123
   Tseng VS, 2007, IEEE T FUZZY SYST, V15, P1188, DOI 10.1109/TFUZZ.2006.890673
NR 26
TC 14
Z9 17
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 40
EP 49
DI 10.1109/TMM.2010.2082512
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900005
DA 2024-07-18
ER

PT J
AU Lee, MJ
   Kim, KS
   Lee, HK
AF Lee, Min-Jeong
   Kim, Kyung-Su
   Lee, Heung-Kyu
TI Digital Cinema Watermarking for Estimating the Position of the Pirate
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Digital cinema; in-theater piracy; local auto-correlation function;
   video watermarking
ID IMAGE WATERMARKING; ROBUST; VIDEO; DISTORTIONS; ROTATION; TRACKING;
   THEATER; SCALE
AB Many illegal copies of digital video productions for cinema release can be found on the Internet before their official release. During the illegal copying of cinema footage, composite geometric distortions commonly occur due to the angle of the camcorder relative to the screen. We propose a novel video watermarking based on spread spectrum way that satisfies the requirements for protecting digital cinema. It enables the detector to not only extract the embedded message but also estimate the position where the camcorder recording is made. It is sure that the proposed position estimating model (PEM) can judge the seat in a theater with a mean absolute error (MAE) of (33.84, 9.53, 50.38) cm. Experimental results using various types of films show that the presented method provides the mathematical model for detecting and investigating the position of the pirate.
C1 [Lee, Min-Jeong; Lee, Heung-Kyu] Korea Adv Inst Sci & Technol, Dept Comp Sci, Taejon 305701, South Korea.
   [Kim, Kyung-Su] KT Network R&D Lab, Network Secur Res Team, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Lee, MJ (corresponding author), Korea Adv Inst Sci & Technol, Dept Comp Sci, Taejon 305701, South Korea.
EM hklee@mmc.kaist.ac.kr
RI Lee, Heung Kyu/C-1941-2011
FU Ministry of Education, Science and Technology [R0A-2007-000- 20023-0]
FX Manuscript received September 30, 2009; revised March 02, 2010 and May
   10, 2010; accepted July 10, 2010. Date of publication August 05, 2010;
   date of current version October 15, 2010. This work was supported by NRL
   (National Research Lab) program through the National Research Foundation
   of Korea funded by the Ministry of Education, Science and Technology
   (No. R0A-2007-000-20023-0). The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Alex C. Kot.
CR Alghoniemy M, 2000, PROC SPIE, V3971, P82, DOI 10.1117/12.385011
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Bas P, 2002, IEEE T IMAGE PROCESS, V11, P1014, DOI 10.1109/TIP.2002.801587
   Bogumil D, 2004, LECT NOTES COMPUT SC, V3200, P25
   CHUPEAU B, 2008, P SPIE SEC FOR STEG, V6819
   DELANNAY D, 2003, P SOC PHOTO-OPT INS, V4314, P149
   *DIG CIN IN LLC, 2008, DIG CIN SYST SPEC VE
   Karybali IG, 2006, IEEE T INF FOREN SEC, V1, P256, DOI 10.1109/TIFS.2006.873652
   Kim KS, 2008, IEICE T INF SYST, VE91D, P1359, DOI 10.1093/ietisy/e91-d.5.1359
   Kutter M., 1998, Proceedings of SPIE, V3628, P423
   Lee MJ, 2009, LECT NOTES COMPUT SC, V5806, P117
   LEE MJ, 2008, P ICIP, P425
   LEEST A, 2001, P SOC PHOTO-OPT INS, V5020, P526
   Lin CY, 2001, IEEE T IMAGE PROCESS, V10, P767, DOI 10.1109/83.918569
   Lubin J, 2003, P SOC PHOTO-OPT INS, V5020, P536, DOI 10.1117/12.477336
   Maes M, 2000, IEEE SIGNAL PROC MAG, V17, P47, DOI 10.1109/79.879338
   *MOT PICT ASS AM, 2005, US PIR FACT SHEET
   Nakashima Y, 2009, IEEE T MULTIMEDIA, V11, P443, DOI 10.1109/TMM.2009.2012938
   Pereira S, 2000, IEEE T IMAGE PROCESS, V9, P1123, DOI 10.1109/83.846253
   Ruanaidh JJKO, 1998, SIGNAL PROCESS, V66, P303, DOI 10.1016/S0165-1684(98)00012-7
   Voloshynovskiy S, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P999, DOI 10.1109/ICIP.2001.958294
   Wang WH, 2008, LECT NOTES COMPUT SC, V5284, P72
NR 22
TC 27
Z9 28
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 605
EP 621
DI 10.1109/TMM.2010.2061221
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500001
DA 2024-07-18
ER

PT J
AU Huang, YZ
   Li, Y
   Fan, N
AF Huang, Yizhen
   Li, Ying
   Fan, Na
TI Robust Symbolic Dual-View Facial Expression Recognition With Skin
   Wrinkles: Local Versus Global Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Expression intensities; facial expression recognition; side-view
   profile; skin wrinkles
ID AUTOMATIC-ANALYSIS; FACE RECOGNITION
AB Simple cartoon facial expressions can be represented by emoticons, that is, a special sequence of symbols. This inspires us that a sketch of facial feature contour may be adequate to recognize expressions. Metrics of such sketches are easier to be calibrated under varying illumination and head pose. While skin wrinkles such as nasolabial folds, eye pouches, dimples, forehead, and chin furrows are not salient facial features, they may convey crucial subtle signals about an individual's emotion. Our experiments have shown that the side-view profile plus skin wrinkles can correctly differentiate nearly 70% expressions, and it contributes to the increase of overall recognition rate. Finally, we compare the accuracy and robustness of various local and global processing schemes, especially under the condition of partial occlusion.
C1 [Huang, Yizhen] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
   [Li, Ying] IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
   [Fan, Na] E China Normal Univ, Dept Elect Engn, Shanghai 200241, Peoples R China.
C3 University of Wisconsin System; University of Wisconsin Madison;
   International Business Machines (IBM); East China Normal University
RP Huang, YZ (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
EM huang.yizhen@gmail.com; yingli@us.ibm.com; fanna.cn@gmail.com
RI Moureng, Huang/AAH-8485-2020
CR [Anonymous], 1996, An Introduction to the Visual System
   BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061
   BUENAPOSADA JM, P CVPR 2004 WORKSH A
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Ekman P, 1978, FACIAL ACTION CODING
   Ekman P., 1975, UNMASKING FACE
   Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232
   Fan N, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P125
   Fan ZG, 2005, IEEE I CONF COMP VIS, P76
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   HAGER JC, 2002, FACS INVESTIGATORS G
   Hietanen JK, 1999, NEUROREPORT, V10, P3443, DOI 10.1097/00001756-199911080-00033
   HU C, 2003, P BRIT MACH VIS C
   HUANG CL, 1992, PATTERN RECOGN, V25, P1435, DOI 10.1016/0031-3203(92)90118-3
   Huang Y., 2008, Computer Vision and Pattern Recognition, P1
   Li SZ, 2005, IEEE T IMAGE PROCESS, V14, P705, DOI 10.1109/TIP.2005.847295
   Long YJ, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P419, DOI 10.1109/MMSP.2006.285343
   Lu ZM, 2010, PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON ASIAN AND PACIFIC COASTS, VOL 3, P1
   MANJUNATH BS, 1993, IEEE T NEURAL NETWOR, V4, P96, DOI 10.1109/72.182699
   MARAGOS P, 1987, OPT ENG, V26, P623, DOI 10.1117/12.7974127
   Martínez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Padgett C, 1997, ADV NEUR IN, V9, P894
   Pantic M, 2004, IEEE T SYST MAN CY B, V34, P1449, DOI 10.1109/TSMCB.2004.825931
   Pantic M, 2003, P IEEE, V91, P1370, DOI 10.1109/JPROC.2003.817122
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Pantic M, 2000, IMAGE VISION COMPUT, V18, P881, DOI 10.1016/S0262-8856(00)00034-2
   PENTLAND A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P84, DOI 10.1109/CVPR.1994.323814
   SARBAJIT P, 2004, P IEEE INT C INF TEC, V1, P45
   Scassellati B, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P969
   SINHA P, 1996, THESIS MIT CAMBRIDGE
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Wang JZ, 2007, MOB INF SYST, V3, P1
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 35
TC 22
Z9 22
U1 4
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 536
EP 543
DI 10.1109/TMM.2010.2052792
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900007
DA 2024-07-18
ER

PT J
AU Kim, JY
   Choi, HK
AF Kim, Jung-Yoon
   Choi, Hyoung-Kee
TI Improvements on Sun <i>et al</i>.'s Conditional Access System in Pay-TV
   Broadcasting Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Access key management; conditional access system; group key
   distribution; pay-TV
AB A conditional access system (CAS) proposed by Sun et al. has a critical security weakness in its inability to preserve backward secrecy; a former subscriber can still access programs despite his or her change in status. This weakness in Sun et al.'s CAS originates because 1) no change is made to a group key after a new member arrives, and 2) updates of group keys are done in an insecure manner. We show how simple protocol changes can fix these weaknesses and thus render Sun et al.'s CAS capable of preserving backward secrecy.
C1 [Kim, Jung-Yoon] Sungkyunkwan Univ, Dept Mobile Syst Engn, Suwon 440746, South Korea.
   [Choi, Hyoung-Kee] Sungkyunkwan Univ, Sch Informat & Commun Engn, Suwon 440746, South Korea.
C3 Sungkyunkwan University (SKKU); Sungkyunkwan University (SKKU)
RP Kim, JY (corresponding author), Sungkyunkwan Univ, Dept Mobile Syst Engn, Suwon 440746, South Korea.
EM steal83@ece.skku.ac.kr; hkchoi@ece.skku.ac.kr
RI Kim, Jung Yoon/AAC-5343-2020
OI Kim, Jung Yoon/0000-0002-2396-9514
CR Huang YL, 2004, IEEE T MULTIMEDIA, V6, P760, DOI 10.1109/TMM.2004.834861
   Jiang TP, 2004, IEEE T CONSUM ELECTR, V50, P225, DOI 10.1109/TCE.2004.1277866
   Liu BF, 2004, IEEE T CONSUM ELECTR, V50, P632, DOI 10.1109/TCE.2004.1309442
   MACQ BM, 1995, P IEEE, V83, P944, DOI 10.1109/5.387094
   Sakarindr P, 2007, IEEE WIREL COMMUN, V14, P8, DOI 10.1109/MWC.2007.4396938
   Sun HM, 2008, IEEE T MULTIMEDIA, V10, P1109, DOI 10.1109/TMM.2008.2001381
   Wang R, 2008, ASIAN J ANDROL, V10, P3, DOI 10.1111/j.1745-7262.2008.00381.x
NR 7
TC 22
Z9 22
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 337
EP 340
DI 10.1109/TMM.2010.2046362
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100010
DA 2024-07-18
ER

PT J
AU Jiang, YG
   Yang, J
   Ngo, CW
   Hauptmann, AG
AF Jiang, Yu-Gang
   Yang, Jun
   Ngo, Chong-Wah
   Hauptmann, Alexander G.
TI Representations of Keypoint-Based Semantic Concept Detection: A
   Comprehensive Study
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag-of-visual-words; representation choice; semantic concept detection
ID KERNELS
AB Based on the local keypoints extracted as salient image patches, an image can be described as a "bag-of-visual-words (BoW)" and this representation has appeared promising for object and scene classification. The performance of BoW features in semantic concept detection for large-scale multimedia databases is subject to various representation choices. In this paper, we conduct a comprehensive study on the representation choices of BoW, including vocabulary size, weighting scheme, stop word removal, feature selection, spatial information, and visual bi-gram. We offer practical insights in how to optimize the performance of BoW by choosing appropriate representation choices. For the weighting scheme, we elaborate a soft-weighting method to assess the significance of a visual word to an image. We experimentally show that the soft-weighting outperforms other popular weighting schemes such as TF-IDF with a large margin. Our extensive experiments on TRECVID data sets also indicate that BoW feature alone, with appropriate representation choices, already produces highly competitive concept detection performance. Based on our empirical findings, we further apply our method to detect a large set of 374 semantic concepts. The detectors, as well as the features and detection scores on several recent benchmark data sets, are released to the multimedia community.
C1 [Jiang, Yu-Gang; Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Jiang, Yu-Gang] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
   [Yang, Jun] Google Inc, Mountain View, CA 94043 USA.
   [Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
C3 City University of Hong Kong; Columbia University; Google Incorporated;
   Carnegie Mellon University
RP Jiang, YG (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM yjiang@ee.columbia.edu; yangjun@google.com; cwngo@cs.cityu.edu.hk;
   alex@cs.cmu.edu
OI Ngo, Chong Wah/0000-0003-4182-8261
FU Hong Kong Special Administrative Region, China [CityU 118906]; City
   University of Hong Kong [7002241]
FX This work was fully supported by a grant from the Research Grants
   Council of the Hong Kong Special Administrative Region, China (Project
   No. CityU 118906), and a grant from City University of Hong Kong
   (Project No. 7002241). The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Marcel Worring.
CR ALY R, 2008, P ACM CIVR
   [Anonymous], P ACM MULT
   [Anonymous], TREC VIDEO RETRIEVAL
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], P ACM MULT
   [Anonymous], 2007, COLUMBIA U BASELINE
   [Anonymous], P TRECVID WORKSH
   [Anonymous], 1994, N-gram-based text categorization
   Berg A., 2001, Proc. CVPR
   CAMPBELL M, 2006, P TRECVID WORKSH
   Cao J., 2006, P TRECVID WORKSH
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CHANG SF, 2008, P TRECVID WORKSH
   Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646
   Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Grauman K., 2005, P ICCV
   HAUPTMANN AG, 2006, P TRECVID WORKSH
   JIANG YG, 2007, P ACM CIVR
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   LAZEBNIK S, 2005, P IEEE ICCV
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Nowak E., 2006, Proc. ECCV
   Nowozin S., 2007, P CVPR
   Odone F, 2005, IEEE T IMAGE PROCESS, V14, P169, DOI 10.1109/TIP.2004.840701
   PETROV S, 2006, P TRECVID WORKSH
   Philbin J., 2008, P CVPR, P1
   Platt JC, 2000, ADV NEUR IN, P61
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   SMEATON AF, 2006, P ACM MIR
   SNOEK CGM, 2008, P TRECVID WORKSH
   SNOEK CGM, 2006, P TRECVID WORKSH
   Sun J., 2009, Proc. of CVPR
   Tan CM, 2002, INFORM PROCESS MANAG, V38, P529, DOI 10.1016/S0306-4573(01)00045-0
   TANG S, 2008, P TRECVID WORKSH
   VANDESANDE KE, 2008, P ACM CIVR
   VANDEWEIJER J, 2006, P ECCV
   WEI XY, 2008, P ACM MULT
   Williams C.K.I., PASCAL VISUAL OBJECT
   YANG J, 2007, P ACM MIR
   Yang Y, 1997, PROCESSING INT C MAC
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   2006, DTO CHALL WORKSH LAR
NR 46
TC 162
Z9 170
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2010
VL 12
IS 1
BP 42
EP 53
DI 10.1109/TMM.2009.2036235
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 533SJ
UT WOS:000272844800004
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Chia, AYS
   Rahardja, S
   Rajan, D
   Leung, MKH
AF Chia, Alex Yong-Sang
   Rahardja, Susanto
   Rajan, Deepu
   Leung, Maylor K. H.
TI Structural Descriptors for Category Level Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Category modeling; object detection; structural representation
ID RECOGNITION
AB We propose a new class of descriptors which exhibits the ability to yield meaningful structural descriptions of objects. These descriptors are constructed from two types of image primitives: quadrangles and ellipses. The primitives are extracted from an image based on human cognitive psychology and model local parts of objects. Experiments reveal that these primitives densely cover objects in images. In this regard, structural information of an object can be comprehensively described by these primitives. It is found that a combination of simple spatial relationships between primitives plus a small set of geometrical attributes provide rich and accurate local structural descriptions of objects. Category level object detection of four-legged animals, bicycles, and cars images is demonstrated under scaling, moderate viewpoint variations, and background clutter. Promising results are achieved.
C1 [Chia, Alex Yong-Sang; Rajan, Deepu; Leung, Maylor K. H.] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Chia, Alex Yong-Sang; Rahardja, Susanto] Inst Infocomm Res, Singapore 138632, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Chia, AYS (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM alex_chia@scholars.a-star.edu.sg; rsusanto@i2r.a-star.edu.sg;
   asdrajan@ntu.edu.sg; asmkleung@ntu.edu.sg
RI Rajan, Deepu/A-3666-2011
FU Academic Research Fund Tier 1 [RG17/08 M52020088]; Agency for Science,
   Technology and Research
FX This work was supported in part by the Academic Research Fund Tier 1
   (RG17/08 M52020088) and in part by the Agency for Science, Technology
   and Research (A*STAR).
CR [Anonymous], P 5 C NEUR NETW THEI
   [Anonymous], 2002, P BRIT MACH VIS C
   ARKIN EM, 1991, IEEE T PATTERN ANAL, V13, P209, DOI 10.1109/34.75509
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BLUM H, 1973, THEORETICAL BIOL, P205
   BRUCE V, 1992, APPL COGNITIVE PSYCH, V6, P619, DOI 10.1002/acp.2350060705
   Chi YL, 2007, IEEE T PATTERN ANAL, V29, P890, DOI 10.1109/TPAMI.2007.1026
   Chuang JH, 2004, COMPUT GRAPH-UK, V28, P907, DOI 10.1016/j.cag.2004.08.004
   Cosgriff R.L., 1960, IDENTIFICATION SHAPE
   CRIMINISI A, 2004, MICR RES CAMBR OBJ R
   David P, 2005, IEEE I CONF COMP VIS, P1581
   Fergus R, 2004, LECT NOTES COMPUT SC, V3021, P242
   Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658
   Gao YS, 2002, IEEE T PATTERN ANAL, V24, P764, DOI 10.1109/TPAMI.2002.1008383
   Helson H, 1932, AM J PSYCHOL, V44, P79, DOI 10.2307/1414957
   IRVING B, 1988, COGNITIVE PSYCHOL, P38
   Jurie F, 2004, PROC CVPR IEEE, P90
   Koffka Kurt, 2013, PRINCIPLES GESTALT P
   Leibe B, 2003, PROC CVPR IEEE, P409
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mikolajczyk K, 2005, IEEE I CONF COMP VIS, P1792
   Mikolajczyk Krystian., 2003, BRIT MACHINE VISION, V2, P779
   NALWA VS, 1988, IEEE T PATTERN ANAL, V10, P514, DOI 10.1109/34.3914
   Opelt A, 2006, IEEE T PATTERN ANAL, V28, P416, DOI 10.1109/TPAMI.2006.54
   Shotton J, 2008, IEEE T PATTERN ANAL, V30, P1270, DOI 10.1109/TPAMI.2007.70772
NR 27
TC 10
Z9 10
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2009
VL 11
IS 8
BP 1407
EP 1421
DI 10.1109/TMM.2009.2032683
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 520KL
UT WOS:000271843000002
DA 2024-07-18
ER

PT J
AU Guan, T
   Wang, C
AF Guan, T.
   Wang, C.
TI Registration Based on Scene Recognition and Natural Features Tracking
   Techniques for Wide-Area Augmented Reality Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Augmented reality; natural features; registration; scene recognition;
   wide-area
AB This research focuses on designing a robust and flexible registration method for wide-area augmented reality applications using scene recognition and natural features tracking techniques. Instead of building a global map of the wide-area scene, we propose to partition the whole scene into several sub-maps according to the user's preference or the requirements of the augmented reality (AR) applications. Random classification trees are used to learn and recognize the reconstructed scenes because they naturally handle multi-class problems, while being both robust and fast. The result is a system that can deal with large scale scene that previous methods cannot cope with. We also propose a hybrid natural features tracking strategy combining both wide and narrow baseline techniques. While providing seamless registration, our system can recover from registration failures and switch between different sub-maps automatically. Experimental results demonstrate the validity of the proposed method for wide-area augmented reality applications.
C1 [Guan, T.; Wang, C.] HuaZhong Univ Sci & Technol, Digital Engn & Simulat Ctr, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Guan, T (corresponding author), HuaZhong Univ Sci & Technol, Digital Engn & Simulat Ctr, Wuhan 430074, Peoples R China.
EM qd_gt@126.com
FU National Natural Science Foundation of China (NSFC) [60903095]; Science
   Foundation Funded Project of China [20080440941]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant No. 60903095 and in part by the
   Postdoctoral Science Foundation Funded Project of China under Grant No.
   20080440941. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Nadia
   Magnenat-Thalmann.
CR Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545
   [Anonymous], P 10 EUR C COMP VIS
   CHEKHLOV D, 2007, P IEEE INT C COMP VI
   Davison AJ, 2003, P IEEE ACM INT S MIX
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Fung J., 2005, ACM MULTIMEDIA, P849
   Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547
   HSU CY, 2008, IEEE T MULTIMEDIA, V10, P585
   Klein G., 2007, P IEEE ACM INT S MIX
   Lee T, 2009, IEEE T VIS COMPUT GR, V15, P355, DOI 10.1109/TVCG.2008.190
   Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188
   Li I, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P182, DOI 10.1109/ISMAR.2004.42
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P837, DOI 10.1016/j.imavis.2004.02.009
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Neumann U, 1999, IEEE T MULTIMEDIA, V1, P53, DOI 10.1109/6046.748171
   Pupilli M., 2005, P BRIT MACH VIS C BM
   Simon G, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P120, DOI 10.1109/ISAR.2000.880935
   Skrypnyk I, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P110, DOI 10.1109/ISMAR.2004.53
   Stewénius H, 2006, ISPRS J PHOTOGRAMM, V60, P284, DOI 10.1016/j.isprsjprs.2006.03.005
   Vacchetti L, 2004, IEEE T PATTERN ANAL, V26, P1385, DOI 10.1109/TPAMI.2004.92
   WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779
   WILLIAMS B, 2007, P INT C ROB AUT
   Williams B., 2007, P INT C COMP VIS
   Yuan ML, 2006, IEEE T VIS COMPUT GR, V12, P569, DOI 10.1109/TVCG.2006.79
NR 25
TC 30
Z9 34
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2009
VL 11
IS 8
BP 1393
EP 1406
DI 10.1109/TMM.2009.2032684
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 520KL
UT WOS:000271843000001
DA 2024-07-18
ER

PT J
AU Hong, CP
   Lee, EH
   Weems, CC
   Kim, SD
AF Hong, Chung-Pyo
   Lee, Eo-Hyung
   Weems, Charles C.
   Kim, Shin-Dug
TI A Profile-Based Multimedia Sharing Scheme With Virtual Community, Based
   on Personal Space in a Ubiquitous Computing Environment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Community computing; P2P network; personal space; ubiquitous computing;
   user/community profile
AB For ubiquitous computing environments, an important parameter is whether all the components in the specific environment can connect with one another. Given this capability, we can share various kinds of content across mobile terminals. This paper introduces an effective scheme to manage multimedia sharing based on specially designed profiles and a virtual community. A virtual community is defined as any specific group of users connected for a common interest. Specifically the proposed scheme consists of two layers, i.e., a community construction layer and a multimedia sharing layer, based on personal spaces, which are responsible for constructing and managing the multimedia sharing community. The community construction layer, which is designed to be run on the mobile terminals, provides an effective way to find community members simultaneously, based on specially designed profiles, such as a user profile and an abstract profile. The multimedia sharing layer is responsible for sharing multimedia content, and is constructed as a specially designed scheme based on locality. The proposed scheme provides an effective multimedia sharing mechanism within a community. Simulation results show that the number of messages and the time required for community member discovery is reduced by 32% and 73%, respectively, in comparison with the conventional DHT-based scheme. The approach also reduces the time to exchange content by 50% with respect to the same baseline.
C1 [Hong, Chung-Pyo; Lee, Eo-Hyung; Kim, Shin-Dug] Yonsei Univ, Dept Comp Sci, Sch Engn, Seoul 120749, South Korea.
   [Weems, Charles C.] Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
C3 Yonsei University; University of Massachusetts System; University of
   Massachusetts Amherst
RP Hong, CP (corresponding author), Yonsei Univ, Dept Comp Sci, Sch Engn, Seoul 120749, South Korea.
EM hulkboy@yonsei.ac.kr; liquid@yonsei.ac.kr; weems@cs.umass.edu;
   sdkim@yonsei.ac.kr
CR Amoretti M., 2006, P 1 INT C PERF EV ME
   Androutsellis-Theotokis S, 2004, ACM COMPUT SURV, V36, P335, DOI 10.1145/1041680.1041681
   [Anonymous], PROJECT JXTA LOOSELY
   [Anonymous], P SIGCOMM 01, DOI DOI 10.1145/383059.383071
   Antony I., 2001, LECT NOTES COMPUTER, P329, DOI DOI 10.1007/3-540-45518-3_18
   Benevenuto F, 2004, 2004 INTERNATIONAL WORKSHOP ON HOT TOPICS IN PEER-TO-PEER SYSTEMS, PROCEEDINGS, P56, DOI 10.1109/PTPSYS.2004.17
   El-Khatib K, 2004, WIREL COMMUN MOB COM, V4, P595, DOI 10.1002/wcm.231
   Halepovic E, 2003, 2003 IEEE PACIFIC RIM CONFERENCE ON COMMUNICATIONS, COMPUTERS, AND SIGNAL PROCESSING, VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P149
   Park KL, 2008, IEEE T SYST MAN CY A, V38, P1295, DOI 10.1109/TSMCA.2008.2003468
   Portmann M, 2002, IEEE SYMP COMP COMMU, P941, DOI 10.1109/ISCC.2002.1021785
   Roman M., 2002, IEEE Pervasive Computing, V1, P74, DOI 10.1109/MPRV.2002.1158281
   Sousa J.P., 2002, SOFTWARE ARCHITECTUR, P29
   Yang DG, 2006, LECT NOTES ARTIF INT, V4088, P398
   Zhao BY, 2004, IEEE J SEL AREA COMM, V22, P41, DOI 10.1109/JSAC.2003.818784
   SHAREPROJECT
   RFC GNUTELLA 0 6
NR 16
TC 2
Z9 2
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1353
EP 1361
DI 10.1109/TMM.2009.2030616
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300013
DA 2024-07-18
ER

PT J
AU Piotto, N
   Conci, N
   De Natale, FGB
AF Piotto, Nicola
   Conci, Nicola
   De Natale, Francesco G. B.
TI Syntactic Matching of Trajectories for Ambient Intelligence Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ambient intelligence; trajectory analysis; trajectory matching;
   trajectory representation; visual surveillance
ID SURVEILLANCE; RECOGNITION; MOTION
AB In this paper we propose a novel approach for syntactic description and matching of object trajectories in digital video, suitable for classification and recognition purposes. Trajectories are first segmented by detecting the meaningful discontinuities in time and space, and are successively expressed through an ad-hoc syntax. A suitable metric is then proposed, which allows determining the similarity among trajectories, based on the so-called inexact or approximate matching. The metric mimics the algorithms used in bio-informatics to match DNA sequences, and returns a score, which allows identifying the analogies among different trajectories on both global and local basis. The tool can therefore be adopted for the analysis, classification, and learning of motion patterns, in activity detection or behavioral understanding.
C1 [Piotto, Nicola; De Natale, Francesco G. B.] Univ Trent, Dept Engn & Comp Sci, I-38100 Trento, Italy.
   [Conci, Nicola] Queen Mary Univ London, Multimedia & Vis Res Grp, London E1 4NS, England.
C3 University of Trento; University of London; Queen Mary University London
RP Piotto, N (corresponding author), Univ Trent, Dept Engn & Comp Sci, I-38100 Trento, Italy.
EM pi-otto@disi.unitn.it; nicola.conci@elec.qmul.ac.uk;
   denatale@ing.unitn.it
RI Conci, Nicola/AAH-4671-2020
OI Conci, Nicola/0000-0002-7858-0928
FU Provincia Autonoma di Trento ( Italy)
FX Manuscript received December 23, 2008; revised July 27, 2009. First
   published August 21, 2009; current version published October 16, 2009.
   This work was developed under the A-Cube project, funded by the
   Provincia Autonoma di Trento ( Italy). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Shrikanth Narayanan.
CR Anjum N., 2007, P IEEE INT C IM PROC, V3
   [Anonymous], 1966, SOVIET PHYS DOKL
   [Anonymous], P ICIP
   [Anonymous], P IEEE 7 WORKSH MULT
   Brandle N., 2006, P IEEE INT TRANSP SY, P115
   CALDERARA S, 2007, P IEEE INT C IM AN P, P137
   Chen LCL, 2004, PACLIC 18: Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation, P227, DOI 10.1145/1026711.1026749
   Cucchiara R., 2002, P 10 ACM INT C MULTI, P223
   De Natale FGB, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/91730
   Foresti GL, 1999, IEEE T CIRC SYST VID, V9, P1045, DOI 10.1109/76.795058
   Hsieh JW, 2006, IEEE T CIRC SYST VID, V16, P396, DOI 10.1109/TCSVT.2006.869965
   Hu WM, 2007, IEEE T IMAGE PROCESS, V16, P1168, DOI 10.1109/TIP.2006.891352
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   IMRAN N, 2004, P 17 INT C PATT REC, P716
   Johnson N, 1996, IMAGE VISION COMPUT, V14, P609, DOI 10.1016/0262-8856(96)01101-8
   Li X, 2006, INT C PATT RECOG, P591
   NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4
   Piciarelli C, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P40
   PIOTTO N, 2008, P IEEE MULT SIGN PRO
   PORIKLY FM, 2004, P EUR C COMP VIS
   SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5
   Vlachos M, 2002, PROC INT CONF DATA, P673, DOI 10.1109/ICDE.2002.994784
   Wachs JP, 2005, IEEE T SYST MAN CY A, V35, P932, DOI 10.1109/TSMCA.2005.851332
   Wang L, 2004, IEEE T CIRC SYST VID, V14, P149, DOI 10.1109/TCSVT.2003.821972
   Wang Q, 2006, IEEE SYS MAN CYBERN, P4268, DOI 10.1109/ICSMC.2006.384805
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
NR 26
TC 21
Z9 23
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1266
EP 1275
DI 10.1109/TMM.2009.2030746
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Salamin, H
   Favre, S
   Vinciarelli, A
AF Salamin, Hugues
   Favre, Sarah
   Vinciarelli, Alessandro
TI Automatic Role Recognition in Multiparty Recordings: Using Social
   Affiliation Networks for Feature Extraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Broadcast data; meeting recordings; role recognition; social network
   analysis
AB Automatic analysis of social interactions attracts increasing attention in the multimedia community. This letter considers one of the most important aspects of the problem, namely the roles played by individuals interacting in different settings. In particular, this work proposes an automatic approach for the recognition of roles in both production environment contexts (e.g., news and talk-shows) and spontaneous situations (e.g., meetings). The experiments are performed over roughly 90 h of material (one of the largest databases used for role recognition in the literature) and show that the recognition effectiveness depends on how much the roles influence the behavior of people. Furthermore, this work proposes the first approach for modeling mutual dependences between roles and assesses its effect on role recognition performance.
C1 [Salamin, Hugues; Vinciarelli, Alessandro] Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
   [Favre, Sarah] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
   [Salamin, Hugues; Favre, Sarah; Vinciarelli, Alessandro] Idiap Res Inst, CH-1920 Martigny, Switzerland.
C3 University of Glasgow; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Salamin, H (corresponding author), Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
EM hugues.salamin@dcs.gla.ac.uk; sfavre@idiap.ch;
   alessandro.vinciarelli@dcs.gla.ac.uk
RI Vinciarelli, Alessandro/HZI-8274-2023; Vinciarelli,
   Alessandro/C-1651-2012
OI Vinciarelli, Alessandro/0000-0002-9048-0524
FU Swiss National Science Foundation; European Community's Seventh
   Framework Programme [FP7/2007-2013]; SSPNet [231287]
FX Manuscript received June 08, 2009; revised July 19, 2009. First
   published August 21, 2009; current version published October 16, 2009.
   This work was supported in part by the Swiss National Science Foundation
   ( under the National Centre of Competence in Research on Interactive
   Multimodal Information Management), and in part by the European
   Community's Seventh Framework Programme (FP7/2007-2013), under grant
   agreement no. 231287 (SSPNet). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Xian-Sheng Hua.
CR AJMERA J, 2004, THESIS ECOLE POLYTEC
   [Anonymous], INT C LANG RES EV LR
   [Anonymous], P 10 INT C MULT INT
   [Anonymous], P INT
   Barzilay R, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P679
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Garg N. P., 2008, P ACM INT C MULT, P693
   GATICAPEREZ D, 2003, IMAGE VIS C IN PRESS
   JAYAGOPI D, 2008, P ACM INT C MULT, P809
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Laskowski Kornel., 2008, P ISCA ACL SIGDIAL W, P148
   Levine J.M., 1998, HDB SOCIAL PSYCHOL, V2, P415
   LIU Y, 2006, P HUM LANG TECHN C N, P81
   Maskey S., 2003, Proceedings of Interspeech, P1173
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   Tischler H.L., 1990, Introduction to Sociology, V3rd
   Vinciarelli A., 2008, P 16 ACM INT C MULT, P1061, DOI DOI 10.1145/1459359.1459573
   VINCIARELLI A, 2009, IMAGE VIS COMPUT, V27
   Vinciarelli A, 2007, IEEE T MULTIMEDIA, V9, P1215, DOI 10.1109/TMM.2007.902882
   Vinciarelli A, 2006, INT C PATT RECOG, P1154
   Vinciarelli Alessandro., 2007, Proceedings of the 15th international conference on Multimedia, MULTIMEDIA '07, P261, DOI DOI 10.1145/1291233.1291287
   Wasserman S., 1994, Social network analysis: Methods and applications'
   Weng CY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1403
   Weng CY, 2009, IEEE T MULTIMEDIA, V11, P256, DOI 10.1109/TMM.2008.2009684
NR 25
TC 30
Z9 30
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1373
EP 1380
DI 10.1109/TMM.2009.2030740
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300015
OA Green Published
DA 2024-07-18
ER

PT J
AU Oh, BJ
   Chen, CW
AF Oh, Byung Joon
   Chen, Chang Wen
TI A Cross-Layer Approach to Multichannel MAC Protocol Design for Video
   Streaming Over Wireless Ad Hoc Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer design; H264/AVC; maximum latency rate (MLR); multichannel
   MAC; network allocation vector (NAV); TDMA MAC
AB This paper presents a cross-layer design for a reliable video transmission over wireless ad hoc networks based on multichannel MAC protocol with TDMA. First, we conduct a study of the multichannel MAC protocol through Markov chain model. Based on this study, two novel cross-layer modules are adopted for the design of multichannel MAC protocol. First, we adopt maximum latency rate (MLR) as the channel quality metric. Unlike the traditional MAC design based on network allocation vector (NAV), MLR is implemented to provide differentiated traffic so that the channel with smaller MLR time is initiated for higher priority traffic. Second, we adopt two congestion-aware metrics, namely MAC utilization and queue length of MAC layer, to improve the congestion-aware routing protocols with AODV and DSR. These two novel modules allow the proposed MAC protocol design to achieve high performance video transmission over wireless ad hoc networks. Experimental results show that the proposed scheme outperforms the state-of-the-art schemes under multichannel environments in wireless ad hoc networks for as much as 3.6 dB in PSNR. Such significant performance enhancement confirms that the cross-layer approach is very effective for multichannel MAC protocol design.
C1 [Oh, Byung Joon] Florida Inst Technol, Melbourne, FL 32901 USA.
   [Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 Florida Institute of Technology; State University of New York (SUNY)
   System; State University of New York (SUNY) Buffalo
RP Oh, BJ (corresponding author), Link Commun Ltd, Annapolis Jct, MD 20701 USA.
EM byungjoonoh@lnkcom.com; chencw@buffalo.edu
CR [Anonymous], 1999, WIR LAN MED ACC CONT
   [Anonymous], NETWORK SIMULATOR NS
   BERNARD S, 2005, DIGITAL COMMUNICATIO
   Bianchi G, 2000, IEEE J SEL AREA COMM, V18, P535, DOI 10.1109/49.840210
   Butala A, 2005, EURASIP J APPL SIG P, V2005, P129, DOI 10.1155/ASP.2005.129
   Chiou HJ, 2005, J VIS COMMUN IMAGE R, V16, P563, DOI 10.1016/j.jvcir.2004.11.009
   Foh CH, 2007, IEEE T CIRC SYST VID, V17, P1665, DOI 10.1109/TCSVT.2007.903808
   Gong MX, 2007, MOBILE NETW APPL, V12, P43, DOI 10.1007/s11036-006-0005-x
   HUA G, 2007, P SPIE COMMUN ITCOM, V6777, P677
   Mo JH, 2008, IEEE T MOBILE COMPUT, V7, P50, DOI 10.1109/TMC.2007.1075
   Oh BJ, 2008, IEEE INT SYMP CIRC S, P3510
   Oh BJ, 2007, IEEE IC COMP COM NET, P464
   Oh BJ, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1389, DOI 10.1109/ICME.2008.4607703
   Sadeghi B, 2005, WIREL NETW, V11, P39, DOI 10.1007/s11276-004-4745-x
   So H.S. W., 2007, Proc. IEEE WCNC'2007, P334
   So J., 2004, MOBIHOC 04, P222
   Stockhammer T, 2005, IEEE WIREL COMMUN, V12, P6, DOI 10.1109/MWC.2005.1497853
   Wu SL, 2002, COMPUT J, V45, P101, DOI 10.1093/comjnl/45.1.101
   YANG N, 2005, P IEEE WIMOB 05 AUG, V3, P284
   Zhang JB, 2007, IEEE ICC, P3554, DOI 10.1109/ICC.2007.587
   Zhang Q, 2008, P IEEE, V96, P64, DOI 10.1109/JPROC.2007.909930
   Zhao JM., 2006, PERMIAN CATHAYSIAN F, P1, DOI DOI 10.1109/ICARCV.2006.345212
NR 22
TC 21
Z9 23
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1052
EP 1061
DI 10.1109/TMM.2009.2026083
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700002
DA 2024-07-18
ER

PT J
AU O'Hare, N
   Smeaton, AF
AF O'Hare, Neil
   Smeaton, Alan F.
TI Context-Aware Person Identification in Personal Photo Collections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context and content; person identification; personal photo management
ID FACE RECOGNITION
AB Identifying the people in photos is an important need for users of photo management systems. We present MediAssist, one such system which facilitates browsing, searching and semi-automatic annotation of personal photos, using analysis of both image content and the context in which the photo is captured. This semiautomatic annotation includes annotation of the identity of people in photos. In this paper, we focus on such person annotation, and propose person identification techniques based on a combination of context and content. We propose language modelling and nearest neighbor approaches to context-based person identification, in addition to novel face color and image color content-based features (used alongside face recognition and body patch features). We conduct a comprehensive empirical study of these techniques using the real private photo collections of a number of users, and show that combining context- and content-based analysis improves performance over content or context alone.
C1 [O'Hare, Neil; Smeaton, Alan F.] Dublin City Univ, Ctr Digital Video Proc, Dublin 9, Ireland.
   [Smeaton, Alan F.] Dublin City Univ, CLAR Ctr Sensor Web Technol, Dublin 9, Ireland.
C3 Dublin City University; Dublin City University
RP O'Hare, N (corresponding author), Dublin City Univ, Ctr Digital Video Proc, Dublin 9, Ireland.
EM nohare@computing.dcu.ie; alan.smeaton@computing.dcu.ie
OI Smeaton, Alan F./0000-0003-1028-8389
CR ANGUELOV D, 2007, P INT C COMP VIS PAT
   [Anonymous], CIVR
   [Anonymous], 11 TEXT RETRIEVAL C
   [Anonymous], IET INT C VIS INF EN
   [Anonymous], P IS T SPIE 18 ANN S
   [Anonymous], MULTIMEDIA 03
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Boutell M, 2005, PATTERN RECOGN, V38, P935, DOI 10.1016/j.patcog.2004.11.013
   Chen Longbin., 2003, IJIG, V3, P81
   Cooray S, 2005, AVSS 2005: Advanced Video and Signal Based Surveillance, Proceedings, P253
   Croft W.B., 2002, Combining approaches to information retrieval, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Devijver P. A., 1982, PATTERN RECOGNITION, V265
   DUDA TO, 2000, PATTERN CLASSIFICATI
   FOX EA, 1994, P 2 TEXT RETR C TREC, P243
   GALLAGHER AC, 2007, P INT C COMP VIS PAT
   Girgensohn Andreas., 2004, MIR 04, P99
   Graham A., 2002, JCDL 2002. Proceedings of the Second ACM/IEEE-CS Joint Conference on Digital Libraries, P326, DOI 10.1145/544220.544301
   KEMPTHOR.O, 1969, BIOMETRIKA, V56, P231
   KUCHINSKY A, 2001, CHI 99, P496
   Lee JH, 1997, PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P267, DOI 10.1145/258525.258587
   MALOBABIC J, 2005, P CBMI2005 4 INT WOR
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Naaman M, 2005, ACM-IEEE J CONF DIG, P178, DOI 10.1145/1065385.1065430
   Naaman M., 2004, P 12 ANN ACM INT C M, P196
   OHARE N, 2007, THESIS DUBLIN CITY U
   OHARE N, 2006, CIVR, P529
   Ponte JM, 1998, P 21 ANN INT ACM SIG, P275, DOI DOI 10.1145/290941.291008
   RODDEN K, 2003, CHI 03, P409
   Sivic Josef., 2006, British Machine Vision Conference, P909
   Suh B, 2007, INTERACT COMPUT, V19, P524, DOI 10.1016/j.intcom.2007.02.002
   Tian Yuandong, 2007, IEEE COMP SOC C COMP
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   ZHANG L, 2003, MULTIMEDIA 03, P355
   ZHAO M, 2006, 5 INT C IM VID RETR, P163
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 37
TC 49
Z9 54
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 220
EP 228
DI 10.1109/TMM.2008.2009679
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800004
OA Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Kobayashi, M
   Nakayama, H
   Ansari, N
   Kato, N
AF Kobayashi, Masahiro
   Nakayama, Hidehisa
   Ansari, Nirwan
   Kato, Nei
TI Robust and Efficient Stream Delivery for Application Layer Multicasting
   in Heterogeneous Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Application layer multicast; content delivery; multiple description
   coding; streaming contents
ID AVAILABLE BANDWIDTH
AB Application Layer Multicast (ALM) is highly expected to replace IP multicasting as the new technological choice for content delivery. Depending on the streaming application, ALM nodes will construct a multicast tree and deliver the stream through this tree. However, if a node resides in the tree leaves, it cannot deliver the stream to its descendant nodes. In this case, Quality of Service (QoS) will be compromised dramatically. To overcome this problem, Topology-aware Hierarchical Arrangement Graph (TRAG) was proposed. By employing Multiple Description Coding (MDC), THAG first splits the stream into a number of descriptions, and then uses Arrangement Graph (AG) to construct node-disjoint multicast trees for each description. However, using a constant AG size in THAG creates difficulty in delivering descriptions appropriately across a heterogeneous network. In this paper, we propose a method, referred to as Network-aware Hierarchical Arrangement Graph (NHAG), to change the AG size dynamically to enhance THAG performance, even in heterogeneous networks. Finally, we evaluate the proposed scheme by experiments using the network simulator ns-2. By comparing our proposed method to THAG and SplitStream, we show that our method provides better performance in terms of throughput and QoS. The results indicate that our approach is more reliable than other methods in heterogeneous networks.
C1 [Kobayashi, Masahiro; Kato, Nei] Tohoku Univ, Grad Sch Informat Sci, Sendai, Miyagi 9808579, Japan.
   [Nakayama, Hidehisa] Tohoku Inst Technol, Fac Engn, Dept Elect & Intelligent Syst, Sendai, Miyagi 9828577, Japan.
   [Ansari, Nirwan] New Jersey Inst Technol, Dept Elect & Comp Engn, Adv Networking Lab, Newark, NJ 07102 USA.
C3 Tohoku University; Tohoku Institute Technology; New Jersey Institute of
   Technology
RP Kobayashi, M (corresponding author), Tohoku Univ, Grad Sch Informat Sci, Sendai, Miyagi 9808579, Japan.
EM kobayashi@it.ecei.tohoku.ac.jp; hidehisa@m.ieice.org;
   nirwan.ansari@njit.edu; kato@it.ecei.tohoku.ac.jp
RI Ansari, Nirwan/N-1264-2019; KATO, NEI/T-5892-2019
OI Ansari, Nirwan/0000-0001-8541-3565; KATO, NEI/0000-0001-8769-302X;
   Kobayashi, Masahiro/0000-0002-6501-7095
FU Japan Science and Technology Agency; National Science Foundation Cyber
   Trust [0726549]; Direct For Computer & Info Scie & Enginr; Division Of
   Computer and Network Systems [0726549] Funding Source: National Science
   Foundation
FX Manuscript received June 04, 2008; revised September 15, 2008. First
   published December 16, 2008 current version published January 09, 2009.
   This work was supported in part through the strategic international
   cooperative program between the Japan Science and Technology Agency and
   the National Science Foundation Cyber Trust under Grant 0726549. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Christophe De Vleeschouwer.
CR Alasti M, 2001, IEEE T INFORM THEORY, V47, P891, DOI 10.1109/18.915641
   [Anonymous], NETWORK SIMULATOR NS
   [Anonymous], 2001, Pastry: Scalable, decentralized object location, and routing for large-scale peer-to-peer systems, DOI DOI 10.1007/3-540-45518-3_18
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   Campana O, 2006, P INT C WIR REC TERM, P217
   Carnpana O, 2008, IEEE T CIRC SYST VID, V18, P268, DOI 10.1109/TCSVT.2008.918113
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Castro M, 2002, IEEE J SEL AREA COMM, V20, P1489, DOI 10.1109/JSAC.2002.803069
   Chen YS, 2001, J SYST ARCHITECT, V47, P73, DOI 10.1016/S1383-7621(00)00041-2
   Chu YH, 2002, IEEE J SEL AREA COMM, V20, P1456, DOI 10.1109/JSAC.2002.803066
   DABEK F, 2004, P 2004 C APPL TECHN, P15
   DAY K, 1991, TR9143 U MINN
   DESHPANDE H, 2001, CS200131
   Francis P., 2000, Yoid: Extending the Internet Multicast Architecture
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Hamad AM, 2002, IEEE NETWORK, V16, P36, DOI 10.1109/MNET.2002.1020234
   Hosseini M, 2007, IEEE COMMUN SURV TUT, V9, P58, DOI 10.1109/COMST.2007.4317616
   Hossfeld F, 2004, ADV PARALLEL COMPUT, V13, P3
   Hu NN, 2003, IEEE J SEL AREA COMM, V21, P879, DOI 10.1109/JSAC.2003.814505
   Huang YC, 2007, IEEE T MULTIMEDIA, V9, P798, DOI 10.1109/TMM.2007.893343
   Jain M, 2003, IEEE ACM T NETWORK, V11, P537, DOI 10.1109/TNET.2003.815304
   Ng ISE, 2002, IEEE INFOCOM SER, P170, DOI 10.1109/INFCOM.2002.1019258
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Padmanabhan VN, 2003, 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P16, DOI 10.1109/ICNP.2003.1249753
   Pendarakis D, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P49
   Roca V, 2001, LECT NOTES COMPUT SC, V2093, P610
   Tian RX, 2005, IEEE T CIRC SYST VID, V15, P961, DOI 10.1109/TCSVT.2005.852416
   Tran DA, 2004, IEEE J SEL AREA COMM, V22, P121, DOI 10.1109/JSAC.2003.818803
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zegura EW, 1996, IEEE INFOCOM SER, P594, DOI 10.1109/INFCOM.1996.493353
NR 30
TC 23
Z9 29
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 166
EP 176
DI 10.1109/TMM.2008.2008933
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, HL
   Ngan, KN
   Liu, Q
AF Li, Hongliang
   Ngan, King N.
   Liu, Qiang
TI FaceSeg: Automatic Face Segmentation for Real-Time Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE AdaBoost; face detection; graph cut; matting; segmentation
ID COLOR; TRACKING; MODEL
AB Segmenting human faces automatically is very important for face recognition and verification, security system, and computer vision. In this paper, we present an accurate segmentation system for cutting human faces out from video sequences in real-time. First, a learning based face detector is developed to rapidly find human faces. To speed up the detection process, a face rejection cascade is constructed to remove most of negative samples while retaining all the face samples. Then, we develop a coarse-to-fine segmentation approach to extract the faces based on a min-cut optimization. Finally, a new matting algorithm is proposed to estimate the alpha-matte based on an adaptive trimap generation method. Experimental results demonstrate the effectiveness and robustness of our proposed method that can compete with the well-known interactive methods in real-time.
C1 [Li, Hongliang] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610054, Peoples R China.
   [Ngan, King N.; Liu, Qiang] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong
RP Li, HL (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610054, Peoples R China.
EM hlli@ee.uestc.edu.co; knngan@ee.cuhk.edu.hk; aliu@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014
OI Ngan, N/0000-0003-1946-3235; Li, Hongliang/0000-0002-7481-095X
FU Shun Hing Institute of Advanced Engineering; Research Grants Council of
   the Hong Kong SAR [CUHK415505]; University of Electronic Science and
   Technology of China
FX Manuscript received September 06, 2007; revised July 27, 2008. First
   published December 16, 2008; current version published January 09, 2009.
   This work was performed at the Chinese University of Hong Kong, and
   supported in part by the Shun Hing Institute of Advanced Engineering,
   the Research Grants Council of the Hong Kong SAR (Project CUHK415505),
   and the Scientific Research Fund for Introduction of Talents of
   University of Electronic Science and Technology of China. The associate
   editor coordinates the review of this manuscript and approving it for
   publication was Dr. Wen Gao.
CR [Anonymous], 2001, Interactive graph cuts for optimal boundary & region segmentation of objects in nd images, DOI DOI 10.1109/ICCV.2001.937505
   [Anonymous], IEEE CVPR 2004 WORKS
   AVIDAN S, 2004, NEURAL INFORM PR DEC
   Baluja S, 2007, INT J COMPUT VISION, V71, P111, DOI 10.1007/s11263-006-8910-9
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122
   CHUANG YY, P CVPR 2001, V2, P264
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Garcia C, 1999, IEEE T MULTIMEDIA, V1, P264, DOI 10.1109/6046.784465
   Greenspan H, 2001, PATTERN RECOGN LETT, V22, P1525, DOI 10.1016/S0167-8655(01)00086-1
   Habili N, 2004, IEEE T CIRC SYST VID, V14, P1086, DOI 10.1109/TCSVT.2004.831970
   HIMANSHU A, 2007, P CVPR2007, P1
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   JOSHI N, P SIGGRAPH 2006
   LAURA G, 2007, P CVPR2007
   Li HL, 2008, J VIS COMMUN IMAGE R, V19, P320, DOI 10.1016/j.jvcir.2008.04.001
   Li HL, 2007, IEEE T CIRC SYST VID, V17, P1742, DOI 10.1109/TCSVT.2007.903326
   Li HL, 2006, IEEE INT SYMP CIRC S, P2681
   Li HL, 2007, IEEE COMMUN MAG, V45, P27, DOI 10.1109/MCOM.2007.284535
   Li SZ, 2004, IEEE T PATTERN ANAL, V26, P1112, DOI 10.1109/TPAMI.2004.68
   LI Y, 2005, P SIGGRA PH 2005
   LI Y, 2004, P SIGGRAPH 2004
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Liévin M, 2004, IEEE T IMAGE PROCESS, V13, P63, DOI 10.1109/TIP.2003.818013
   Liu C, 2003, PROC CVPR IEEE, P587
   Luo HT, 2003, IEEE T MULTIMEDIA, V5, P379, DOI 10.1109/TMM.2003.813285
   MCGUIRE M, P SIGGRAPH 2005
   Mita T, 2005, IEEE I CONF COMP VIS, P1619
   Phung SL, 2005, IEEE T PATTERN ANAL, V27, P148, DOI 10.1109/TPAMI.2005.17
   ROTHER C, 2004, P SIGGRAPH 2004
   Ruzon MA, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.855793
   SUN J, P SIGGRA PH 2004
   SUN J, P SIGGRAPH 2006
   SUN J, 2006, P ECCV 2006
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   WANG J, 2005, P SIGGRAPH 2005
   Wang P, 2005, PROC CVPR IEEE, P373
   WU B, 2007, P CVPR2007
   YANG MH, 2000, P NEUR INF PROC SYST, P855
   Zhao T, 2003, PROC CVPR IEEE, P459, DOI 10.1109/NSSMIC.2003.1352083
NR 41
TC 45
Z9 51
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 77
EP 88
DI 10.1109/TMM.2008.2008922
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700007
DA 2024-07-18
ER

PT J
AU Chen, DY
   Cannons, K
   Tyan, HR
   Shih, SW
   Liao, HYM
AF Chen, Duan-Yu
   Cannons, Kevin
   Tyan, Hsiao-Rong
   Shih, Sheng-Wen
   Liao, Hong-Yuan Mark
TI Spatiotemporal Motion Analysis for the Detection and Classification of
   Moving Targets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object classification; spatiotemporal analysis; video surveillance
ID SURVEILLANCE; ALGORITHMS
AB This paper presents a video surveillance system in the environment of a stationary camera that can extract moving targets from a video stream in real time and classify them into predefined categories according to their spatiotemporal properties. Targets are detected by computing the pixel-wise difference between consecutive frames, and then classified with a temporally boosted classifier and "spatiotemporal-oriented energy" analysis. We demonstrate that the proposed classifier can successfully recognize five types of objects: a person, a bicycle, a motorcycle, a vehicle, and a person with an umbrella. In addition, we process targets that do not match any of the AdaBoost-based classifier's categories by using a secondary classification module that categorizes such targets as crowds of individuals or non-crowds. We show that the above classification task can be performed effectively by analyzing a target's spatiotemporal-oriented energies, which provide a rich description of the target's spatial and dynamic features. Our experiment results demonstrate that the proposed system is extremely effective in recognizing all predefined object classes.
C1 [Chen, Duan-Yu] Yuan Ze Univ, Dept Elect Engn, Chungli, Taiwan.
   [Cannons, Kevin] York Univ, Dept Comp Sci & Engn, N York, ON M3J 1P3, Canada.
   [Tyan, Hsiao-Rong] Chung Yuan Christian Univ, Dept Informat & Comp Engn, Chungli, Taiwan.
   [Liao, Hong-Yuan Mark] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
C3 Yuan Ze University; York University - Canada; Chung Yuan Christian
   University; Academia Sinica - Taiwan
RP Chen, DY (corresponding author), Yuan Ze Univ, Dept Elect Engn, Chungli, Taiwan.
EM dychen@iis.sinica.edu.tw; kcannons@cse.yorku.ca; tyan@ice.cycu.edu.tw;
   stone@csie.ncnu.edu.tw; liao@iis.sinica.edu.tw
RI Liao, Hong-Yuan Mark/AAQ-5514-2021
FU Ministry of Economic Affairs [96-EC-17-A-02-S1-032]; National Digital
   Archives [NSC 96-2422-H-001-001]
FX Manuscript received October 02, 2007: revised July 01, 2008. First
   published November 07, 2008. This work was supported in part by the
   Ministry of Economic Affairs under Contract 96-EC-17-A-02-S1-032, and
   National Digital Archives Program under Contract NSC 96-2422-H-001-001.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Ling Guan.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   [Anonymous], 2006, PETS
   [Anonymous], PROCEEDINGS OF THE I
   [Anonymous], P ICIP
   [Anonymous], 2007, PETS
   Bendat J.S., 2013, Engineering applications of correlation and spectral analysis
   CANNONS K, 2007, P AS C COMP VIS NOV, V1, P532
   Collins RT, 2001, P IEEE, V89, P1456, DOI 10.1109/5.959341
   DJOUADI A, 1990, IEEE T PATTERN ANAL, V12, P92, DOI 10.1109/34.41388
   Enzweiler M., 2005, P IEEE WORKSH MOT VI, V2, P66
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gryn JM, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P202
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Haritaoglu I., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P280, DOI 10.1109/ICIAP.1999.797608
   Heeger D., 1988, INT J COMPUT VISION, V1, P297
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Javed O., 2002, P EUR C COMP VIS, P439
   KAILATH T, 1967, IEEE T COMMUN TECHN, VCO15, P52, DOI 10.1109/TCOM.1967.1089532
   Lipton AJ, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P8, DOI 10.1109/ACV.1998.732851
   Liu X, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P306
   Marana AN, 1998, SAFETY SCI, V28, P165, DOI 10.1016/S0925-7535(97)00081-7
   Masoud O, 2001, IEEE T VEH TECHNOL, V50, P1267, DOI 10.1109/25.950328
   Reisman P, 2004, 2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P66
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Schneiderman H, 2000, PROC CVPR IEEE, P746, DOI 10.1109/CVPR.2000.855895
   Shah M, 2007, IEEE MULTIMEDIA, V14, P30, DOI 10.1109/MMUL.2007.3
   STAUFFER C, 1999, P IEEE INT C COMP VI, V19, P23
   Thirde David., 2006, PROC 9 IEEE INT WORK, P47
   Tsuchiya M, 2006, INT C PATT RECOG, P978
   Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734
   Wildes R., 2000, P IEEE EUR C COMP VI, P784
   Zhao T, 2003, PROC CVPR IEEE, P459, DOI 10.1109/NSSMIC.2003.1352083
NR 33
TC 15
Z9 18
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1578
EP 1591
DI 10.1109/TMM.2008.2007289
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600013
DA 2024-07-18
ER

PT J
AU Zafeiriou, S
   Pitas, I
AF Zafeiriou, Stefanos
   Pitas, Ioannis
TI Discriminant Graph Structures for Facial Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Elastic graph matching; expandable graphs; Fisher's linear discriminant
   analysis; Kernel techniques
ID ROBUST FEATURE-EXTRACTION; OBJECT RECOGNITION; AUTOMATIC-ANALYSIS; FACE
   VERIFICATION; SEQUENCES; EFFICIENT; SPACE
AB In this paper, a series of advances in elastic graph matching for facial expression recognition are proposed. More specifically, a new technique for the selection of the most discriminant facial landmarks for every facial expression (discriminant expression-specific graphs) is applied. Furthermore, a novel kernel-based technique for discriminant feature extraction from graphs is presented. This feature extraction technique remedies some of the limitations of the typical kernel Fisher discriminant analysis (KFDA) which provides a subspace of very limited dimensionality (i.e., one or two dimensions) in two-class problems. The proposed methods have been applied to the Cohn-Kanade database in which very good performance has been achieved in a fully automatic manner.
C1 [Zafeiriou, Stefanos] Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, London, England.
   [Pitas, Ioannis] Aristotle Univ Thessaloniki, Thessaloniki 54124, Greece.
C3 Imperial College London; Aristotle University of Thessaloniki
RP Zafeiriou, S (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, London, England.
EM dralbert@aiia.csd.auth.gr; pitas@aiia.csd.auth.gr
FU European Union [03ED849]; Greek Secretariat of Research and Technology
   (Hellenic Ministry of Development)
FX Manuscript received October 10, 2007 revised July 07. 2008. Current
   version published December 10, 2008. This work was supported by the
   project 03ED849 co-funded by the European Union and the Greek
   Secretariat of Research and Technology (Hellenic Ministry of
   Development) of the Operational Program for Competitiveness within the
   3rd Community Support Framework. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Wen
   Gao.
CR Aleksic PS, 2006, IEEE T INF FOREN SEC, V1, P3, DOI 10.1109/TIFS.2005.863510
   [Anonymous], 1990, STAT PATTERN RECOGNI
   [Anonymous], P IEEE INT C IM PROC
   Bartlett M., 2003, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Workshop on Computer Vision and Pattern Recognition for Human-Computer Interaction, V5, P53, DOI DOI 10.1109/CVPRW.2003.10057
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   DUE B, 1999, IEEE T IMAGE PROCESS, V4, P504
   Ekman P., 1975, EMOTION HUMAN FACE
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Guo GD, 2005, IEEE T SYST MAN CY B, V35, P477, DOI 10.1109/TSMCB.2005.846658
   Hong H, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P354, DOI 10.1109/AFGR.1998.670974
   Jackway PT, 1996, IEEE T PATTERN ANAL, V18, P38, DOI 10.1109/34.476009
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Kotropoulos C, 2000, IEEE T IMAGE PROCESS, V9, P555, DOI 10.1109/83.841933
   Kotropoulos C, 2000, PATTERN RECOGN, V33, P1935, DOI 10.1016/S0031-3203(99)00185-5
   Kotropoulos CL, 2000, IEEE T MULTIMEDIA, V2, P14, DOI 10.1109/6046.825791
   Kotsia I, 2007, IEEE T IMAGE PROCESS, V16, P172, DOI 10.1109/TIP.2006.884954
   Kruger N, 1997, IEEE T PATTERN ANAL, V19, P764, DOI 10.1109/34.598233
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Li BX, 2001, J OPT SOC AM A, V18, P2969, DOI 10.1364/JOSAA.18.002969
   Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852
   LITTLEWORT G, 2004, P IEEE C COMP VIS PA
   Liu J, 2007, IEEE T NEURAL NETWOR, V18, P1862, DOI 10.1109/TNN.2007.900813
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P117, DOI 10.1109/TNN.2002.806629
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P195, DOI 10.1109/TNN.2002.806647
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Lyons M. J., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P202, DOI 10.1109/AFGR.2000.840635
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   Mitra S, 2006, IEEE T INF FOREN SEC, V1, P350, DOI 10.1109/TIFS.2006.879301
   Müller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   Pantic M, 2003, P IEEE, V91, P1370, DOI 10.1109/JPROC.2003.817122
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Pantic M, 2000, IMAGE VISION COMPUT, V18, P881, DOI 10.1016/S0262-8856(00)00034-2
   Pentland A, 2000, COMPUTER, V33, P50, DOI 10.1109/2.820039
   RYDFALK M, 1978, CANDIDE PARAMETERIZE
   Schölkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Shin HC, 2007, IEEE T MULTIMEDIA, V9, P1125, DOI 10.1109/TMM.2007.898933
   Shin H, 2007, PATTERN RECOGN LETT, V28, P1077, DOI 10.1016/j.patrec.2007.01.003
   STAMOU G, 2005, P IEEE INT C IM PROC
   Tefas A, 2002, SIGNAL PROCESS, V82, P833, DOI 10.1016/S0165-1684(02)00157-3
   Tefas A, 2001, IEEE T PATTERN ANAL, V23, P735, DOI 10.1109/34.935847
   Tian Y., 2004, P IEEE WORKSH FAC PR
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Wiskott L, 1997, PATTERN RECOGN, V30, P837, DOI 10.1016/S0031-3203(96)00132-X
   Wurtz RP, 1997, IEEE T PATTERN ANAL, V19, P769, DOI 10.1109/34.598234
   Yang J, 2005, IEEE T PATTERN ANAL, V27, P230, DOI 10.1109/TPAMI.2005.33
   YANG P, 2004, P 6 IEEE INT C AUT F
   Yeasin M, 2006, IEEE T MULTIMEDIA, V8, P500, DOI 10.1109/TMM.2006.870737
   ZAFEIRIOU S, 2005, P INT WORKSH NONL SI
   ZAFEIRIOU S, 2005, P IEEE INT C IM PROC
   Zafeiriou S, 2007, IEEE T INF FOREN SEC, V2, P55, DOI 10.1109/TIFS.2006.890308
   ZENG Z, IEEE T PATT IN PRESS
   Zhang J, 1997, P IEEE, V85, P1423, DOI 10.1109/5.628712
   Zhang YM, 2005, IEEE T PATTERN ANAL, V27, P699, DOI 10.1109/TPAMI.2005.93
   Zhang ZY, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P454, DOI 10.1109/AFGR.1998.670990
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 56
TC 39
Z9 42
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1528
EP 1540
DI 10.1109/TMM.2008.2007292
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600009
DA 2024-07-18
ER

PT J
AU Mansour, H
   Krishnamurthy, V
   Nasiopoulos, P
AF Mansour, Hassan
   Krishnamurthy, Vikram
   Nasiopoulos, Panos
TI Channel Aware Multiuser Scalable Video Streaming Over Lossy
   Under-Provisioned Channels: Modeling and Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Loss-distortion modeling; scalable video coding (SVC); streaming delay
   analysis; unequal erasure protection (UXP); wireless video streaming
ID RESOURCE-ALLOCATION; TRANSMISSION; NETWORKS
AB In this paper, we analyze the performance of media-aware multiuser video streaming strategies in capacity limited wireless channels suffering from latency problems and packet losses. Wireless video streaming applications are characterized by their bandwidth-intensity, delay-sensitivity, and loss-tolerance. Our main contributions include i) a rate-minimized unequal erasure protection (UXP) scheme, ii) an analytical expression for packet delay and play-out deadline of UXP protected scalable video, iii) a loss-distortion model for hierarchical predictive video coders with picture copy concealment, iv) an analysis of the performance and complexity of delay-aware, capacity-aware, and optimized UXP streaming scenarios, and v) we show that the use of unequal error protection causes a rate-constrained optimization problem to be nonconvex. Performance evaluations using a 3GPP network simulator show that, for different channel capacities and packet loss rates, delay-aware nonstationary rate-allocation streaming policies deliver significant gains which range between 1.65 dB to 2 dB in average Y-PSNR of the received video streams over delay-unaware strategies. These gains come at a cost of increased offline computation which is performed prior to the start of the streaming session or in batches during transmission and therefore, do not affect the run-time performance of the streaming system.
C1 [Mansour, Hassan; Krishnamurthy, Vikram; Nasiopoulos, Panos] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia
RP Mansour, H (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
EM hassanm@ece.ubc.ca; vikramk@ece.ubc.ca; panos@ece.ubc.ca
CR *3GPP 3GPP2, 2003, S4040803 3GPP 3GPP2
   [Anonymous], MSRTR200135
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   Cabrera J, 2002, IEEE T CIRC SYST VID, V12, P496, DOI 10.1109/TCSVT.2002.800306
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Farrokh A, 2006, IEEE T MULTIMEDIA, V8, P844, DOI 10.1109/TMM.2006.876227
   Fu FW, 2007, IEEE J-STSP, V1, P264, DOI 10.1109/JSTSP.2007.901519
   GUO Y, 2005, JTC1SC29WG11 ISOIEC
   *ISO IEC, 2006, 1SC29WG11 ISOIEC JTC
   Lee Y, 2006, ADV ANAT PATHOL, V13, P1, DOI 10.1097/01.pap.0000201826.46978.e5
   Liebl G, 2005, TELECOMMUN SYST, V30, P255, DOI 10.1007/s11235-005-4328-x
   Liebl G, 2006, IEEE INT C MULT EXP
   LIEBL G, 2004, IETF             OCT
   MANSOUR H, 2000, IEEE INT S SIGN PROC, P536
   MANSOUR H, 2007, INT WORKSH IM AN MUL
   Pahalawatta P, 2007, IEEE J SEL AREA COMM, V25, P749, DOI 10.1109/JSAC.2007.070511
   REICHEL J, 2007, 1SC29WG11 ISOIEC JTC
   SCHIERL T, 2005, IEEE INT MULT EXP IC
   Schwarz H., 2005, JTC1SC29WG11 ISOIEC
   Setton E, 2005, IEEE WIREL COMMUN, V12, P59, DOI 10.1109/MWC.2005.1497859
   Shiang HP, 2007, IEEE T MULTIMEDIA, V9, P1299, DOI 10.1109/TMM.2007.902845
   Shiang HP, 2007, IEEE J SEL AREA COMM, V25, P770, DOI 10.1109/JSAC.2007.070513
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   TAO S, 2005, INT WORKSH NETW OP S, P129
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Varsa V., 2001, Common test conditions for RTP/IP over 3GPP/3GPP2
   WANG YK, 2005, COMMON CONDITIONS SV
   WENGER S, 2005, RFC3984 IETF
   Zhang Q, 2004, IEEE T CIRC SYST VID, V14, P1049, DOI 10.1109/TCSVT.2004.831966
   ZHU X, 2006, IEEE INT C MULT EXP
   Zhu XQ, 2005, SIGNAL PROCESS-IMAGE, V20, P773, DOI 10.1016/j.image.2005.05.005
   ZORZI M, 1995, P IEEE ICUPC 95 NOV, P211
NR 32
TC 19
Z9 21
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1366
EP 1381
DI 10.1109/TMM.2008.2004915
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700013
DA 2024-07-18
ER

PT J
AU Pallavi, V
   Mukherjee, J
   Majumdar, AK
   Sural, S
AF Pallavi, V.
   Mukherjee, Jayanta
   Majumdar, Arun K.
   Sural, Shamik
TI Graph-based multiplayer detection and tracking in broadcast soccer
   videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dynamic programming; player detection; soccer analysis; tracking;
   trajectory
AB In this paper, we propose a graph-based approach for detecting and tracking multiple players in broadcast soccer videos. In the first stage, the position of the players in each frame is determined by removing the non player regions. The remaining pixels are then grouped using a region growing algorithm to identify probable player candidates. A directed weighted graph is constructed, where probable player candidates correspond to the nodes of the graph while each edge links candidates in a frame with the candidates in next two consecutive frames. Finally, dynamic programming is applied to find the trajectory of each player. Experiments with several sequences from broadcasted videos of international soccer matches indicate that the proposed approach is able to track the players reasonably well even under varied illumination and ground conditions.
C1 [Pallavi, V.; Mukherjee, Jayanta; Majumdar, Arun K.] Philips Res Asia, Bangalore, Karnataka, India.
   [Sural, Shamik] Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal, India.
   [Pallavi, V.; Mukherjee, Jayanta; Majumdar, Arun K.] Indian Inst Technol, Dept Comp Sci & Engn, Kharagpur 721302, W Bengal, India.
C3 Philips; Philips Research; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Kharagpur; Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur
RP Pallavi, V (corresponding author), Philips Res Asia, Bangalore, Karnataka, India.
EM pallavi@cse.jitkgp.ernet.in; jay@cse.iitkgp.ernet.in;
   akmj@cse.iitkgp.ernet.in; shamik@sit.iitkgp.ernet.in
RI Sural, Shamik/C-1394-2011
OI Sural, Shamik/0000-0002-4315-7329
CR ALSUWAIYEL MH, 1999, ALGORITHMS DESIGN TE, V7, P205
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Figueroa P, 2004, INT C PATT RECOG, P787, DOI 10.1109/ICPR.2004.1333890
   Iwase S, 2004, INT C PATT RECOG, P751, DOI 10.1109/ICPR.2004.1333881
   KARMANN KP, 1990, SIGNAL PROCESSING V : THEORIES AND APPLICATIONS, VOLS 1-3, P951
   Keren D, 2001, IEEE T PATTERN ANAL, V23, P747, DOI 10.1109/34.935848
   Kim H, 2003, P IM VIS COMP PALM N, P159
   Li BX, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P132, DOI 10.1109/IVL.2001.990867
   Misu T, 2002, INT C PATT RECOG, P556, DOI 10.1109/ICPR.2002.1044792
   Mukherjee J, 2002, PATTERN RECOGN LETT, V23, P917, DOI 10.1016/S0167-8655(02)00022-3
   Nguyen HT, 2007, IEEE T PATTERN ANAL, V29, P52, DOI 10.1109/TPAMI.2007.250599
   OHTA Y, 1980, COMPUT VISION GRAPH, V13, P222, DOI 10.1016/0146-664X(80)90047-7
   PALLAVI V, 2006, P REC TRENDS INF SYS, V1, P216
   PETKOVIC M, 2001, P IASTED INT C VIS I, P512
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   SEO Y, 1997, P 9 INT C IM AN PROC, V2, P196
   Tang JS, 2003, J ELECTRON IMAGING, V12, P423, DOI 10.1117/1.1579485
   Utsumi O, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P45, DOI 10.1109/ICME.2002.1035714
   WANG L, 2004, P IEEE INT C AC SPEE, V3, P617
   Xu G, 2002, INT C PATT RECOG, P831, DOI 10.1109/ICPR.2002.1048431
   Yamada A, 2002, INT C PATT RECOG, P303, DOI 10.1109/ICPR.2002.1044697
   Yu XG, 2006, IEEE T MULTIMEDIA, V8, P1164, DOI 10.1109/TMM.2006.884621
   Zhu GY, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1629, DOI 10.1109/ICME.2006.262859
NR 23
TC 38
Z9 47
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 794
EP 805
DI 10.1109/TMM.2008.922869
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800012
DA 2024-07-18
ER

PT J
AU Nikseresht, A
   Gelgon, M
AF Nikseresht, Afshin
   Gelgon, Marc
TI Gossip-based computation of a Gaussian mixture model for distributed
   multimedia indexing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE classification; distributed computing; multimedia indexing; probability
   density estimation
ID ALGORITHMS; IDENTIFICATION; RETRIEVAL
AB This paper deals with pattern recognition in a distributed computing context of the peer-to-peer type, that should be more and more interesting tor multimedia data indexing and retrieval. Our goal is estimating of class-conditional probability densities, that take the form of Gaussian mixture models (GMM). Originally, we propagate GMMs in a decentralized fashion (gossip) in a network, and aggregate GMMs from various sources, through a technique that only involves little computation and that makes parsimonious usage of the network resource, as model parameters rather than data are transmitted. The aggregation is based on iterative optimization of an approximation of a KL. divergence allowing closed-form computation between mixture models. Experimental results demonstrate the scheme to the case of speaker recognition.
C1 [Nikseresht, Afshin; Gelgon, Marc] Univ Nantes, Polytech Nantes, LINA Lab, INRIA Atlas Project Team,CNRS,UMR 6241, F-44035 Nantes, France.
C3 Nantes Universite; Centre National de la Recherche Scientifique (CNRS)
RP Nikseresht, A (corresponding author), Univ Nantes, Polytech Nantes, LINA Lab, INRIA Atlas Project Team,CNRS,UMR 6241, F-44035 Nantes, France.
EM marc.gelgon@univ-nantes.fr
FU Region Pays-de-la-Loire; SFERE
FX This work was supported in part by the Region Pays-de-la-Loire under the
   MILES Project and by SFERE. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Kiyoharu
   Aizawa.
CR AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   [Anonymous], 2006, Toward Category-Level Object Recognition
   ATTIAS H, 1999, P NEUR INF PROC SYST
   BERRANI SA, 2003, P ACM INT WORKSH MUL, P70
   Biernacki C, 2003, COMPUT STAT DATA AN, V41, P561, DOI 10.1016/S0167-9473(02)00163-9
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516
   Cohen I, 2004, IEEE T PATTERN ANAL, V26, P1553, DOI 10.1109/TPAMI.2004.127
   EUGSTER PT, 2003, IEEE COMPUT, V37
   Fablet R, 2002, IEEE T IMAGE PROCESS, V11, P393, DOI 10.1109/TIP.2002.999674
   GOLDBERGER J, 2004, P NEUR INF PROC SYST, P505
   Hammoud R, 2000, INT C PATT RECOG, P71, DOI 10.1109/ICPR.2000.906020
   KOWALCZYK W, 2005, P NEUR INF PROC SYST, V17
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI [10.1162/neco.1992.4.3.415, 10.1162/neco.1992.4.3.448]
   MILOJICIC D, 2002, PEER PEER COMPUTING
   Muller W., 2003, Proceedings of the SPIE - The International Society for Optical Engineering, V5304, P57, DOI 10.1117/12.531184
   Nowak RD, 2003, IEEE T SIGNAL PROCES, V51, P2245, DOI 10.1109/TSP.2003.814623
   REYNOLDS DA, 1995, SPEECH COMMUN, V17, P91, DOI 10.1016/0167-6393(95)00009-D
   Schmid C, 2004, INT J COMPUT VISION, V56, P7, DOI 10.1023/B:VISI.0000004829.38247.b0
   TANG C, 2004, P ACM SIGIR SIG INT, P145
   VERBEEK JJ, DATA MINING IN PRESS
   XIE L, 2004, P IEEE WORKSH LEARN
   [No title captured]
NR 23
TC 13
Z9 13
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 385
EP 392
DI 10.1109/TMM.2008.917343
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Amatriain, X
AF Amatriain, Xavier
TI A domain-specific metamodel for multimedia processing systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dataflow graphs; modeling; multimedia systems; object-oriented methods;
   systems engineering; visual languages
AB In this paper, we introduce 4MPS, a Metamodel for Multimedia Processing Systems. The goal of 4MPS is to offer a generic system metamodel that can be instantiated to describe any multimedia processing design. The metamodel combines the advantages of the object-oriented paradigm and metamodeling techniques with system engineering principles and graphical models of computation.
   4MPS is based on the classification of multimedia processing objects into two main categories: Processing objects that operate on data and controls, and Data objects that passively hold media content. Processing objects encapsulate a method or algorithm. They also include support for synchronous data processing and asynchronous event-driven Controls as well as a configuration mechanism and an explicit life cycle state model. Data input to and output from Processing objects is done through Ports. Data objects offer a homogeneous interface to media data, and support for metaobject-like facilities such as reflection and serialization.
   The metamodel can be expressed in the language of graphical models of computation such as the Dataflow Networks and presents a comprehensive conceptual framework for media signal processing applications. 4MPS has its practical validation in several existing environments, including the author's CLAM framework.
C1 Univ Calif Santa Barbara, Media Arts & Technol Program, Santa Barbara, CA 93111 USA.
C3 University of California System; University of California Santa Barbara
RP Amatriain, X (corresponding author), Telefon I D, Barcelona, Spain.
EM xar@tid.es
CR ACKERMANN P, 1994, P ACM MULT C
   *ACM OBJ MAN GROUP, 2006, UN MOD LANG UML SPEC
   Amatriain X, 2002, DAFX - DIGITAL AUDIO EFFECTS, P373
   AMATRIAIN X, 2005, P 6 INT C MUS INF RE
   AMATRIAIN X, 2006, P ACM MULTIMED
   AMATRIAIN X, 2005, P INT COMP MUS C
   AMATRIAIN X, 2005, THESIS U POMPEU FABR
   Amatriain X, 2007, IEEE SOFTWARE, V24, P82, DOI 10.1109/MS.2007.8
   [Anonymous], 1995, THESIS U CALIFORNIA
   [Anonymous], ADV TOPICS DATAFLOW
   [Anonymous], 1974, PROC IFIP C 74
   ARUMI P, 2006, P PATT LANG PROGR PL
   Bray S., 2005, P INT COMP MUS C ICM
   BUEDA DM, 1999, ENG DESIGN SYSTEMS
   BURROUGHS N, 2006, P INT COMP MUS C, P79
   CASTELLANOS J, 2006, THESIS U CALIFORNIA
   CHAUDHARY A, 1999, P AUD ENG SOC 107 CO, P1
   COOK P, 1996, P 1996 SIGGRAPH
   COOK S, 2004, MDA J            JAN, P2
   Devedzic V., 2002, COMMUN ACM, V45
   EARNSHAW R, 1995, MULTIMEDIA SYSTEMS A
   ENGELS G, 2002, HDB SOFTWARE ENG KNO, V2, P21
   Frances A., 2001, 3rd International Conference on Future Groundwater Resources at Risk, CVRM/IST, Lisboa, P35
   Francois A. R. J., 2000, Proceedings ACM Multimedia 2000, P371, DOI 10.1145/354384.354536
   Gamma E., 1994, Design patterns: Elements of reusable object-oriented software
   GARCIA D, 2007, P 2007 LIN AUD C
   Geilen M, 2003, LECT NOTES COMPUT SC, V2618, P319
   Green TRG, 1996, J VISUAL LANG COMPUT, V7, P131, DOI 10.1006/jvlc.1996.0009
   Grotker T, 1997, INT CONF ACOUST SPEE, P271, DOI 10.1109/ICASSP.1997.599621
   HALL A, 1956, YB SOC ADV GEN SYSTE
   HASHIMOTO S, 1995, MULTIMEDIA MODELING
   Hylands C., 2003, Overview of the Ptolemy Project
   Kleppe A.G., 2003, MDA Explained; The Model Driven Architecture: Practice and Promise
   Knublauch H., 2004, P INT WORKSH MOD DRI
   LAZZARINI V, 2001, P 4 INT C DIG AUD EF
   LEE EA, 1987, IEEE T COMPUT, V36, P24, DOI 10.1109/TC.1987.5009446
   LEE EA, 1995, P IEEE, V83, P773, DOI 10.1109/5.381846
   LEVIA O, 1999, P FDL 99 LYON FRANC, P548
   Lindblad CJ, 1996, IEEE J SEL AREA COMM, V14, P1298, DOI 10.1109/49.536481
   LOURENS T, 2004, ANN INT COMP SOFTW A, P10
   Mandal M.K., 2002, Multimedia Signals and Systems
   MANOLESCU DA, 1997, P 4 PATT LANG PROGR
   MayerPatel K, 1997, P SOC PHOTO-OPT INS, V3020, P194, DOI 10.1117/12.264292
   MELLER SJ, 2003, IEEE SOFTW
   MELLINGER DK, 1991, WELL TEMPERED OBJECT, P188
   *OBJ MAN GROUP, 2005, MET OBJ FAC MOF 2 0
   POPE ST, 2006, P INT COMP MUS C ICM
   POPE ST, 2003, P INT COMP MUS C ICM
   PRATS J, 2006, THESIS U POMPEU FABR
   Rao K.R., 2002, Multimedia Communication Systems
   ROBERTS D, 1996, P 3 INT C PATT LANG
   ROWE WD, 1965, IEEE T SYST SCI CYB, VSSC1, P2, DOI 10.1109/TSSC.1965.300051
   Sauer S., 1999, Proceedings 1999 IEEE Symposium on Visual Languages, P80, DOI 10.1109/VL.1999.795878
   Seidewitz E, 2003, IEEE SOFTWARE, V20, P26, DOI 10.1109/MS.2003.1231147
   Störrle H, 2005, ELECTRON NOTES THEOR, V127, P35, DOI 10.1016/j.entcs.2004.08.046
   Sztipanovits J, 1998, COMMUN ACM, V41, P66, DOI 10.1145/274946.274958
   TZANETAKIS G, 2001, P INT C AUD DISPL IC
   UPSON C, 1989, IEEE COMPUT GRAPH, V9, P30, DOI 10.1109/38.31462
   VANDIJK HW, 2002, P INT S MOB MULT APP
   Willrich R, 2002, MULTIMED TOOLS APPL, V16, P7, DOI 10.1023/A:1013233517612
   YOUNG M, 1995, COMPUT GRAPH, V296, P22
   Zachariadis S, 2006, IEEE T SOFTWARE ENG, V32, P910, DOI 10.1109/TSE.2006.115
   ZELKOWITZ M, 1997, INFORM SOFTW TECHNOL, V39
   Ziegenbein D, 2002, IEEE T VLSI SYST, V10, P379, DOI 10.1109/TVLSI.2002.807767
   Zivkovic VD, 2002, LECT NOTES COMPUT SC, V2268, P74
NR 65
TC 6
Z9 6
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1284
EP 1298
DI 10.1109/TMM.2007.902885
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000017
DA 2024-07-18
ER

PT J
AU Inamoto, N
   Saito, H
AF Inamoto, Naho
   Saito, Hideo
TI Virtual viewpoint replay for a soccer match by view interpolation from
   multiple cameras
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dynamic event; multiple cameras; projective geometry; soccer match; view
   interpolation; virtual view synthesis
ID SYSTEM
AB This paper presents a novel method for virtual view synthesis that allows viewers to virtually fly through real soccer scenes, which are captured by multiple cameras in a stadium. The proposed method generates images of arbitrary viewpoints by view interpolation of real camera images near the chosen viewpoints. In this method, cameras do not need to be strongly calibrated since projective geometry between cameras is employed for the interpolation. For avoiding the complex and unreliable process of 3-D recovery, object scenes are segmented into several regions according to the geometric property of the scene. Dense correspondence between real views, which is necessary for intermediate view generation, is automatically obtained by applying projective geometry to each region. By superimposing intermediate images for all regions, virtual views for the entire soccer scene are generated. The efforts for camera calibration are reduced and correspondence matching requires no manual operation; hence, the proposed method can be easily applied to dynamic events in a large space. An application for fly-through observations of soccer match replays is introduced along with the algorithm of view synthesis and experimental results. This is a new approach for providing arbitrary views of an entire dynamic event.
C1 Keio Univ, Dept Informat & Comp Sci, Yokohama, Kanagawa 2238522, Japan.
C3 Keio University
RP Inamoto, N (corresponding author), Keio Univ, Dept Informat & Comp Sci, Yokohama, Kanagawa 2238522, Japan.
EM na-hotty@ozawa.ics.keio.ac.jp; saito@ozawa.ics.keio.ac.jp
RI Saito, Hideo/D-6223-2014
OI Saito, Hideo/0000-0002-2421-9862
CR [Anonymous], P 23 ANN C COMP GRAP
   Avidan S, 1998, IEEE T VIS COMPUT GR, V4, P293, DOI 10.1109/2945.765324
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153
   CONNOR K, 2003, P BRIT MACH VIS C BM
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Inamoto N, 2003, PROC SPIE, V5150, P1143, DOI 10.1117/12.502968
   Inamoto N, 2002, INT C PATT RECOG, P713, DOI 10.1109/ICPR.2002.1048401
   Inamoto N., 2002, INT WORKSH ENT COMP, P94
   Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394
   Kitahara I, 2003, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VR.2003.1191120
   Koyama T, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P178, DOI 10.1109/ISMAR.2003.1240701
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Manning R.A., 1999, Cvpr, P1388
   Pollard S, 2000, IMAGE VISION COMPUT, V18, P749, DOI 10.1016/S0262-8856(99)00078-5
   Saito H, 2003, IEEE T MULTIMEDIA, V5, P303, DOI 10.1109/TMM.2003.813283
   Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462
   Shum HY, 2005, IEEE T MULTIMEDIA, V7, P85, DOI 10.1109/TMM.2004.840591
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   WEXLER Y, 2000, P COMPUTER VISION PA, P1576
   WHEELER MD, 1997, P IMAG UND WORKSH, P911
   XIAO J, 2002, P EUR 2002
   Yaguchi S, 2004, IEEE T SYST MAN CY B, V34, P430, DOI 10.1109/TSMCB.2003.817108
NR 24
TC 32
Z9 40
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1155
EP 1166
DI 10.1109/TMM.2007.902832
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000007
OA Green Published
DA 2024-07-18
ER

PT J
AU Fu, FW
   van der Schaar, M
AF Fu, Fangwen
   van der Schaar, Mihaela
TI Noncollaborative resource management for wireless multimedia
   applications using mechanism design
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cross-layer optimization; game theory; mechanism design; multiuser
   wireless transmission; resource management; wireless multimedia
   streaming
ID PROPORTIONAL FAIRNESS; NETWORKS
AB We propose to add a new dimension to existing wireless multimedia communications systems by enabling competing stations to proactively engage in the resource management game by adapting their cross-layer transmission strategies. For this, we model wireless stations (WSTAs) as rational and selfish players competing for available wireless resources in a dynamic game. We focus on polling-based wireless LAN (WLAN) networks, where developing an efficient solution for managing the available transmission opportunities is of paramount importance. The resource allocation game is coordinated by a network moderator, which deploys a novel resource management based on the Vickrey-Clarke-Groves (VCG) mechanism to determine a) the amount of time to be allocated to the various users and b) the transmission cost associated to the allocated resources. The transmission cost is referred to in the VCG mechanism as "transfer" and depends not on the used resources, but rather on the inconvenience (in terms of utility impact) that it causes to other WSTAs. The transfer is introduced in order to discourage WSTAs from lying about their resource requirements. Importantly, this proposed dynamic resource management approach for wireless multimedia applications changes the passive way stations are currently adapting their cross-layer strategies by enabling them to selfishly influence the wireless systems dynamics by proactively adapting their packet scheduling strategies, error protection strategies, etc. Hence, each wireless station can play the resource management game by adapting its multimedia transmission strategy depending on the experienced channel conditions, derived video quality, attitude towards risk, willingness to pay for resources and available information about the wireless network. Our simulations show that using the VCG mechanism the WSTAs do not have any incentives to lie about their resource requirements as otherwise they will be severely penalized by a high transfer. We also show that deploying advanced cross-layer strategies for playing the resource management game significantly benefit the WSTAs' received video quality. The willingness-to-pay for resources is introduced to provide WSTAs a tool to gather additional resources whenever they need to transmit an important (part of a) video sequence by agreeing to pay for resources an increased cost. A novel risk-aware scheduling scheme is also proposed that provides WSTAs the ability to dynamically avoid network congestion and hence, reduce their incurred transfer.
C1 Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Fu, FW (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM fwfu@ee.ucla.edu; mi-haela@ee.ucla.edu
CR Albanese A, 1996, HIGH-SPEED NETWORKING FOR MULTIMEDIA APPLICATIONS, P247
   [Anonymous], 2001, THESIS
   Badia L, 2003, GLOB TELECOMM CONF, P4116, DOI 10.1109/GLOCOM.2003.1259002
   BERRY R, 2004, IEEE SIGNAL PROCESSI, V21
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cocchi R, 1993, IEEE ACM T NETWORK, V1, P614, DOI 10.1109/90.266050
   Curescu C, 2005, IEEE T PARALL DISTR, V16, P624, DOI 10.1109/TPDS.2005.87
   Fudenberg D., 1991, GAME THEORY
   HAJEK B, UNPUB STRATEGIC BUYE
   *IEEE, 2003, 80211ED50 IEEE
   Jackson M.O., 2003, ENCY LIFE SUPPORT SY
   Jiang LB, 2005, IEEE WCNC, P1551
   Kelly F, 1997, EUR T TELECOMMUN, V8, P33, DOI 10.1002/ett.4460080106
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   KRISHNASWAMY D, INTEL TECHNOL J
   krishnaswamy D., 2002, PROC 3G WIRELESS C, P165
   Lucky RW, 2006, IEEE SPECTRUM, V43, P88, DOI 10.1109/MSPEC.2006.1572368
   MACKIEMASON JK, 1995, IEEE J SEL AREA COMM, V13, P1141, DOI 10.1109/49.414634
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   OHM JR, 1994, IEEE T IMAGE PROCESS, V3, P559, DOI 10.1109/83.334985
   PAREKH AK, 1993, P IEEE ACM T NET JUN, V1
   SCAGLIONE A, 2005, P ICASSP 2005 MAR
   Semret N, 2000, IEEE J SEL AREA COMM, V18, P2499, DOI 10.1109/49.898733
   Setton E, 2005, IEEE WIREL COMMUN, V12, P59, DOI 10.1109/MWC.2005.1497859
   SHANKAR S, IN PRESS IEEE T VEHI
   Vaidya N, 2005, IEEE T MOBILE COMPUT, V4, P616, DOI 10.1109/TMC.2005.87
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   VANDERSCHAAR M, IN PRESS IEEE T MULT
NR 29
TC 26
Z9 30
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 851
EP 868
DI 10.1109/TMM.2007.895676
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200015
DA 2024-07-18
ER

PT J
AU Li, Z
   Sun, QB
   Lian, Y
   Chen, CW
AF Li, Zhi
   Sun, Qibin
   Lian, Yong
   Chen, Chang Wen
TI Joint source-channel-authentication resource allocation and unequal
   authenticity protection for multimedia over wireless networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE authentication; digital signatures; joint source-channel coding;
   rate-distortion optimization; unequal authenticity protection
ID RATE-DISTORTION ANALYSIS; IMAGE AUTHENTICATION; DIGITAL SIGNATURE
AB There have been increasing concerns about the security issues of wireless transmission of multimedia in recent years. Wireless networks, by their nature, are more vulnerable to external intrusions than wired ones. Many applications demand authenticating the integrity of multimedia content delivered wirelessly. In this work, we describe a framework for jointly coding and authenticating multimedia to be delivered over heterogeneous wireless networks. We firstly introduce a novel concept called unequal authenticity protection (UAP), which unequally allocate resources to achieve an optimal authentication result. We then consider integrating UAP with specific source and channel-coding models, to obtain optimal end-to-end quality by the means of joint source-channel-authentication analysis. Lastly, we present an implementation of the proposed joint coding and authentication system on a progressive JPEG coder. Experimental results demonstrate that the proposed approach is indeed able to achieve the desired authentication of multimedia over wireless networks.
C1 Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
   ASTAR, Inst Infocomm Res, Singapore 119613, Singapore.
   Florida Inst Technol, Dept Elect & Comp Engn, Melbourne, FL 32901 USA.
C3 National University of Singapore; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R);
   Florida Institute of Technology
RP Li, Z (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
EM lizhi@nus.edu.sg; qibin@i2r.a-star.edu.sg; eleliany@nus.edu.sg;
   cchen@fit.edu
RI Lian, Yong/B-9859-2008; Sun, Qibin/Q-5360-2017; Li, Zhi/J-6464-2013
OI Sun, Qibin/0000-0001-5715-0497; Lian, Yong/0000-0002-5289-5219
CR [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P SPI EINTERNATIONAL
   BERGER T, 1984, RATE DISTORTION THEO
   Cai J., 2005, ACM T MULTIM COMPUT, V1, P338
   Cai JF, 2001, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P10, DOI 10.1109/ITCC.2001.918757
   Gennaro R, 1997, LECT NOTES COMPUT SC, V1294, P180
   Golle P., 2001, Proceedings of the 8th Annual Network and Distributed Systems Security Symposium (NDSS), P13
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P1221, DOI 10.1109/76.974677
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P840, DOI 10.1109/TCSVT.2002.804883
   *ISO IEC, 2000, 154441 ISO IEC
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Lu CS, 2003, IEEE T MULTIMEDIA, V5, P161, DOI 10.1109/TMM.2003.811621
   Miner S, 2001, P IEEE S SECUR PRIV, P232, DOI 10.1109/SECPRI.2001.924301
   Perrig A, 2000, P IEEE S SECUR PRIV, P56, DOI 10.1109/SECPRI.2000.848446
   Shannon C.E., 1948, BELL SYST TECH J
   Sun QB, 2005, IEEE T MULTIMEDIA, V7, P480, DOI 10.1109/TMM.2005.846776
   Sun QB, 2005, INT J IMAGE GRAPH, V5, P135, DOI 10.1142/S0219467805001690
   Wu CW, 2002, IEEE T MULTIMEDIA, V4, P385, DOI 10.1109/TMM.2002.802018
   YE SM, IN PRESS SPRINGER LN
   Zhang ZF, 2006, LECT NOTES COMPUT SC, V3989, P293
NR 21
TC 24
Z9 29
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 837
EP 850
DI 10.1109/TMM.2007.893338
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200014
OA Green Published
DA 2024-07-18
ER

PT J
AU Ansary, TF
   Daoudi, M
   Vandeborre, JP
AF Ansary, Tarik Filali
   Daoudi, Mohamed
   Vandeborre, Jean-Philippe
TI A Bayesian 3-D search engine using adaptive views clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian approach; clustering; 3-D indexing; 3-D retrieval; views
AB In this paper, we propose a method for three-dimensional (3-D)-model indexing based on two-dimensional (2-D) views, which we call adaptive views clustering (AVC). The goal of this method is to provide an "optimal" selection of 2-D views from a 3-D model, and a probabilistic Bayesian method for 3-D-model retrieval from these views. The characteristic view selection algorithm is based on an adaptive clustering algorithm and uses statistical model distribution scores to select the optimal number of views. Starting from the fact that all views do not have equal importance, we also introduce a novel Bayesian approach to improve the retrieval. Finally, we present our results and compare our method to some state-of-the-art 3-D retrieval descriptors on the Princeton 3-D Shape Benchmark database and a 3-D-CAD-models database supplied by the car manufacturer Renault.
C1 GET INT Telecom, USTL, CNRS, UMR 8022,LIFL,FOX MIITE Res Grp, Lille 1, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite de Lille
RP Ansary, TF (corresponding author), GET INT Telecom, USTL, CNRS, UMR 8022,LIFL,FOX MIITE Res Grp, Lille 1, France.
EM filali@enic.fr; daoudi@enic.fr; vandeborre@enic.fr
RI Daoudi, Mohammed/H-5935-2013
OI Daoudi, Mohammed/0000-0003-4219-7860
CR Akaike H., 1973, 2 INT S INFORM THEOR, P267, DOI [10.1007/978-1-4612-1694-015, DOI 10.1007/978-1-4612-1694-0_15]
   ANKERST M, 1999, INT SOC BEHAV MED
   [Anonymous], 2002, MODEL SELECTION MULT
   ANSARY TF, 2004, P IEEE INT C PATT RE
   ANSARY TF, 2004, P IEEE INT S 3D DAT
   ANTINI G, 2005, P IEEE INT C MULT EX
   Assfalg J, 2004, INT C PATT RECOG, P906, DOI 10.1109/ICPR.2004.1334675
   BERGER JO, 1994, J AM STAT ASSOC, P109
   BOZDOGAN H, 1987, PSYCHOMETRIKA, V52, P354
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cyr CM, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P254, DOI 10.1109/ICCV.2001.937526
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073
   Ivanko E, 2005, LECT NOTES COMPUT SC, V3687, P360
   Kang S. B., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P580, DOI 10.1109/CVPR.1991.139757
   KAZHDAN M, 2003, P S GEOM PROC AACH G
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Kim WY, 2000, SIGNAL PROCESS-IMAGE, V16, P95, DOI 10.1016/S0923-5965(00)00019-9
   LEBARBIER E, 2004, RR5315
   Leifman G, 2003, 4th Israel Korea Bi-National Conference on Geometric Modeling and Computer Graphics, P159
   Macrini D, 2002, INT C PATT RECOG, P24, DOI 10.1109/ICPR.2002.1047786
   OSADA R, 2001, P IEEE SHAP MOD INT
   Pelleg D., 2000, P 17 INT C MACH LEAR, DOI DOI 10.1007/3-540-44491-2_3
   SAUPE D, 2001, GERMAN ASS PATTERN R, P392
   SCHIFFENBAUER RD, 2001, TRCIS200101
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   SHILANE, 2004, IEEE SHAPE MODELING
   SUNDAR H, 2003, P IEEE SHAP MOD INT
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   Vandeborre JP, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P644, DOI 10.1109/TDPVT.2002.1024132
   Vranic DV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P757
   FOX MIIRE 3D SEARCH
NR 33
TC 166
Z9 180
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 78
EP 88
DI 10.1109/TMM.2006.886359
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500009
OA Green Published
DA 2024-07-18
ER

PT J
AU Kiranyaz, S
   Gabbouj, M
AF Kiranyaz, Serkan
   Gabbouj, Moncef
TI Hierarchical cellular tree: An efficient indexing scheme for
   content-based retrieval on multimedia databases
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based retrieval; metric access methods; multimedia databases;
   similarity-based indexing
ID IMAGE RETRIEVAL
AB One of the challenges in the development of a content-based multimedia indexing and retrieval application is to achieve an efficient indexing scheme. The developers and users who are accustomed to making queries to retrieve a particular multimedia item from a large scale database can be frustrated by the long query times. Conventional indexing structures cannot usually cope with the requirements of a multimedia database, such as dynamic indexing or the presence of high-dimensional audiovisual features. Such structures do not scale well with the ever increasing size of multimedia databases whilst inducing corruption and resulting in an over-crowded indexing structure. This paper addresses such problems and presents a novel indexing technique, Hierarchical Cellular Tree (HCT), which is designed to bring an effective solution especially for indexing large multimedia databases. Furthermore it provides an enhanced browsing capability, which enables user to make a guided tour within the database. A pre-emptive cell-search mechanism is introduced in order to prevent corruption, which may occur due to erroneous item insertions. Among the hierarchical levels that are built in a bottom-up fashion, similar items are collected into appropriate cellular structures at some level. Cells are subject to mitosis operations when the dissimilarity exceeds a required level. By mitosis operations, cells are kept focused and compact and yet, they can grow into any dimension as long as the compactness is maintained. The proposed indexing scheme is then used along with a recently introduced query method, the progressive query, in order to achieve the ultimate goal, from the user point of view that is retrieval of the most relevant items in the earliest possible time regardless of the database size. Experimental results show that the speed of retrievals is significantly improved and the indexing structure shows no sign of degradations when the database size is increased. Furthermore, HCT indexing body can conveniently be used for efficient browsing and navigation operations among the multimedia database items.
C1 Tampere Univ Technol, Inst Signal Proc, FIN-33101 Tampere, Finland.
C3 Tampere University
RP Kiranyaz, S (corresponding author), Tampere Univ Technol, Inst Signal Proc, FIN-33101 Tampere, Finland.
EM serkan@cs.tut.fi; moncef.gabbouj@tut.fi
RI Gabbouj, Moncef/G-4293-2014; Kiranyaz, Serkan/AAK-1416-2021
OI Gabbouj, Moncef/0000-0002-9788-2323; kiranyaz,
   serkan/0000-0003-1551-3397
CR [Anonymous], 1994, The VLDB Journal, DOI DOI 10.1007/BF01231606
   [Anonymous], 1993, FUNDAMENTAL SPEECH R
   [Anonymous], 2000, P 26 INT C VER LARG
   [Anonymous], P ACM SIGMOD INT C M
   [Anonymous], P 21 INT C VER LARG
   BECKMANN N, 1990, SIGMOD REC, V19, P322, DOI 10.1145/93605.98741
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Berchtold S., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P577, DOI 10.1109/ICDE.2000.839456
   BERCHTOLD S, 1996, P 22 INT C VER LARG
   BOZKAYA T, 1997, P ACM SIGMOD INT C M, P357
   Chakrabarti K, 1999, PROC INT CONF DATA, P440, DOI 10.1109/ICDE.1999.754960
   CHANG SF, 1997, P ACM MULT SEATTL WA
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   Fonseca MJ, 2003, EIGHTH INTERNATIONAL CONFERENCE ON DATABASE SYSTEMS FOR ADVANCED APPLICATIONS, PROCEEDINGS, P267, DOI 10.1109/DASFAA.2003.1192391
   Guttman A., 1984, ACM SIGMOD INT C MAN, P47, DOI DOI 10.1145/602259.602266
   Johnson DB, 1996, ALGORITHMICA, V16, P633, DOI 10.1007/BF01944354
   Katayama N., 1997, P ACM SIGMOD, P369
   Kiranyaz S, 2005, IEE P-VIS IMAGE SIGN, V152, P356, DOI 10.1049/ip-vis:20045061
   Kiranyaz S, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P1, DOI 10.1109/ISSPA.2003.1224626
   KIRANYAZ S, 2003, P 3 INT WORKSH CONT
   KIRANYAZ S, 2005, P IEEE INT C IM PROC
   KIRANYAZ S, 2005, P EUR SIGN PROC C EU
   KOIKKALAINEN P, 1990, P INT JOINT C NEUR N
   KRUSKAL JR, 1956, P AMS, V71
   Laaksonen J, 2000, PATTERN RECOGN LETT, V21, P1199, DOI 10.1016/S0167-8655(00)00082-9
   MA WY, 1995, P IEEE INT C IM PROC
   Partio M., 2002, P 5 NORD SIGN PROC S
   PENTLAND A, 1994, P SOC PHOTO-OPT INS, V2185, P34, DOI 10.1117/12.171786
   PRIM RC, 1957, AT&T TECH J, V36, P1389, DOI 10.1002/j.1538-7305.1957.tb01515.x
   Sellis T., 1987, Proceedings of the Thirteenth International Conference on Very Large Data Bases: 1987 13th VLDB, P507
   Sethi IK, 1999, PATTERN RECOGN LETT, V20, P1337, DOI 10.1016/S0167-8655(99)00103-8
   SMITH JR, 1996, P ACM MULT BOST MA N
   Traina C, 2000, LECT NOTES COMPUT SC, V1777, P51
   Wang H, 2001, P 5 PAC AS C KNOWL D
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   White DA, 1996, PROC INT CONF DATA, P516, DOI 10.1109/ICDE.1996.492202
   YIANILOS PN, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P311
   ZHANG H, 1995, P SOC PHOTO-OPT INS, V2420, P36
   ZHOU X, 2003, P 14 AUSTR DAT C DAT, P161
NR 39
TC 23
Z9 25
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 102
EP 119
DI 10.1109/TMM.2006.886362
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500011
DA 2024-07-18
ER

PT J
AU Liu, Z
   Wang, Y
AF Liu, Zhu
   Wang, Yao
TI Major cast detection in video using both speaker and face information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based multimedia indexing; face detection; major cast detection;
   media integration; speaker segmentation; video browsing; video summary
ID SEGMENTATION
AB Major casts, for example, the anchor persons or reporters in news broadcast programs and the principle characters in movies, play an important role in video, and their occurrences provide meaningful indices for organizing and presenting video content. This paper describes a new approach for automatically generating a list of major casts in a video sequence based on multiple modalities, specifically, speaker information in audio track and face information in video track. The core algorithm is composed of three steps. First, speaker boundaries are detected and speaker segments are clustered in audio stream. Second, face appearances are tracked and face tracks are clustered in video stream. Finally, correspondences between speakers and faces are determined based on their temporal co-occurrence. A list of major casts is constructed and ranked in an order that reflects each cast's importance, which is determined by the accumulative temporal and spatial presence of the cast. The proposed algorithm has been integrated in a major cast based video browsing system, which presents the face icon and marks the speech locations in time stream for each detected major cast. The system provides a semantically meaningful summary of the video content, which helps the user to effectively digest the theme of the video.
C1 AT&T Labs Res, Middletown, NJ 07748 USA.
   Polytech Univ, Dept Elect Engn, Brooklyn, NY 11201 USA.
C3 AT&T; New York University
RP Liu, Z (corresponding author), AT&T Labs Res, Middletown, NJ 07748 USA.
EM zliu@research.att.com; yao@poly.edu
CR [Anonymous], NATURE STATISTI810
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Boreczky JS, 1998, INT CONF ACOUST SPEE, P3741, DOI 10.1109/ICASSP.1998.679697
   CHELLAPPA R, 1995, P IEEE, V83, P705, DOI 10.1109/5.381842
   Cover T. M., 1991, ELEMENTS INFORM THEO
   CUTLER R, 2000, P ICME NEW YORK NOV
   Ferman AM, 1998, J VIS COMMUN IMAGE R, V9, P336, DOI 10.1006/jvci.1998.0402
   FISHER WFJ, 2000, ADV NEUR INF PROC SY
   FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146
   Huang JC, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P526, DOI 10.1109/ICIP.1998.727252
   Iyengar G, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA369
   Jain A.K., 1998, ALGORITHMS CLUSTERIN
   Lienhart R, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P685, DOI 10.1109/MMCS.1999.779282
   Liu Z, 1998, J VLSI SIG PROC SYST, V20, P61, DOI 10.1023/A:1008066223044
   LIU Z, 2001, P ICASSP 2001 SALT L
   LIU Z, 2000, P ICIP 2000 VANC BC
   LIU Z, 2000, P ICASSP 2000 IST TU
   Lupatini G, 1998, EIGHTH INTERNATIONAL WORKSHOP ON RESEARCH ISSUES IN DATA ENGINEERING - CONTINUOUS-MEDIA DATABASES AND APPLICATIONS, PROCEEDINGS, P34, DOI 10.1109/RIDE.1998.658276
   Martinez A. M., 1998, THE AR FACE DATABASE
   REYNOLDS DA, 1995, IEEE T SPEECH AUDI P, V3, P72, DOI 10.1109/89.365379
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   SAMAL A, 1992, PATTERN RECOGN, V25, P65, DOI 10.1016/0031-3203(92)90007-6
   SARACENO C, 1998, P INT C IM PROC CHIC, V1, P363
   Satoh S, 1999, IEEE MULTIMEDIA, V6, P22, DOI 10.1109/93.752960
   Saunders J, 1996, INT CONF ACOUST SPEE, P993, DOI 10.1109/ICASSP.1996.543290
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   Zhang T, 1999, INT CONF ACOUST SPEE, P3001, DOI 10.1109/ICASSP.1999.757472
NR 29
TC 10
Z9 10
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 89
EP 101
DI 10.1109/TMM.2006.886360
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500010
DA 2024-07-18
ER

PT J
AU Farrokh, A
   Krishnamurthy, V
AF Farrokh, Arsalan
   Krishnamurthy, Vikram
TI Opportunistic scheduling for streaming multimedia users in high-speed
   downlink packet access (HSDPA)
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive modulation and coding (AMC); Hybrid ARQ (H-ARQ); high-speed
   downlink packet access (HSDPA); Markov Fading Channel; opportunistic
   scheduling; Quality-of-Service (QoS); stability; stochastic
   approximation; UMTS
AB High-speed downlink packet access (HSDPA) achieves high data rates and high spectral efficiency by using adaptive modulation and coding schemes and employing multicode CDMA. In this paper, we present opportunistic algorithms for scheduling HSDPA users and selecting modulation/coding and multicode schemes that exploit channel and buffer variations to increase the probability of uninterrupted media play-out. First, we introduce a stochastic discrete event model for a HSDPA system. By employing the discrete event model, we transform the scheduling problem of providing uninterrupted play-out to a feasibility problem that considers two sets of stochastic Quality-of-Service (QoS) constraints: stability constraints and robustness constraints. A methodology for obtaining a feasible solution is then proposed by starting with a so-called stable algorithm that satisfies the stability QoS constraints. Next, we present Stochastic Approximation algorithms that adapt the parameters of the stable algorithm in a way that a feasible point for the robustness QoS is reached within the feasibility region of the stability QoS.
C1 Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia
RP Farrokh, A (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
EM ar-salanf@ece.ubc.ca; vikramk@ece.ubc.ca
CR Andrews M, 2001, IEEE COMMUN MAG, V39, P150, DOI 10.1109/35.900644
   ANDREWS M, 2000, BELL LABS TECH MEMO
   [Anonymous], 1996, WIRELESS COMMUNICATI
   [Anonymous], 2001, Probability and Random Processes
   Berggren F, 2002, IEEE VTS VEH TECHNOL, P1934, DOI 10.1109/VETECF.2002.1040555
   Berggren F, 2001, IEEE J SEL AREA COMM, V19, P1860, DOI 10.1109/49.957302
   Bertsekas D. P., 2000, Dynamic programming and optimal control, V1
   Blomer F., 2003, THESIS MUNICH U TECH
   Frederiksen F, 2002, IEEE VTS VEH TECHNOL, P472, DOI 10.1109/VETECF.2002.1040388
   Kolding TE, 2002, IEEE VTS VEH TECHNOL, P477, DOI 10.1109/VETECF.2002.1040389
   Kushner H.J., 1997, STOCHASTIC APPROXIMA
   LIU ECX, 2001, P IEEE VTC FALL 2001, P824
   Liu X, 2001, IEEE J SEL AREA COMM, V19, P2053, DOI 10.1109/49.957318
   Love R, 2001, IEEE VTS VEH TECHNOL, P2234, DOI 10.1109/VETECS.2001.945093
   Pflug GC, 1996, KLUWER INT SERIES EN, V373
   Wang C. Y., 2001, Int. J. Struct. Stab. Dyn., V1, P163
   Zhang QQ, 1999, IEEE T COMMUN, V47, P1688, DOI 10.1109/26.803503
NR 17
TC 22
Z9 24
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 844
EP 855
DI 10.1109/TMM.2006.876227
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300017
DA 2024-07-18
ER

PT J
AU Das, TK
   Maitra, S
   Zhou, JY
AF Das, Tanmoy Kanti
   Maitra, Subhamoy
   Zhou, Jianying
TI Cryptanalysis of Chu's DCT based watermarking scheme
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cryptanalysis; digital watermarking; single copy attacks; subsampling
AB In 2003, Chu proposed an oblivious watermarking algorithm by modifying the CKLS scheme proposed by Cox, Kilian, Leighton, and Shamoon in 1997, known as the CKLS scheme. In this correspondence, we report that the modification presented by Chu is susceptible to a suitably modified attack devised by Das and Maitra in 2004. In fact, the experimental results show that Chu's scheme is even weaker than the CKLS scheme in terms of our attack.
C1 Infocomm Secur Dept, Singapore 119613, Singapore.
   Indian Stat Inst, Appl Stat Unit, Kolkata 700108, W Bengal, India.
C3 Indian Statistical Institute; Indian Statistical Institute Kolkata
RP Das, TK (corresponding author), Infocomm Secur Dept, Singapore 119613, Singapore.
EM tkdas@i2r.a-star.edu.sg; subho@isical.ac.in; jyzhou@i2r.a-star.edu.sg
CR Anderson RJ, 1998, IEEE J SEL AREA COMM, V16, P474, DOI 10.1109/49.668971
   BOEUF J, 2001, LECT NOTES COMPUTER, V2137, P395
   Chu WC, 2003, IEEE T MULTIMEDIA, V5, P34, DOI 10.1109/TMM.2003.808816
   Cox Ingemar J., 1997, IEEE T IMAGE PROCESS, V12, P1673
   Das TK, 2004, IEEE SIGNAL PROC LET, V11, P446, DOI 10.1109/LSP.2004.824028
   Kerckhoffs A., 1883, J. des Sci. militaires, V1, P5
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
NR 7
TC 5
Z9 5
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 629
EP 632
DI 10.1109/TMM.2006.870718
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000020
DA 2024-07-18
ER

PT J
AU Chang, MMY
   Wong, KH
AF Chang, MMY
   Wong, KH
TI Model reconstruction and pose acquisition using extended Lowe's method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D structure acquisition; bundle adjustment; Lowe's method; pose
   estimation; structure from motion
ID MOTION; SHAPE
AB Finding the pose and structure of an unknown object from an image sequence has many applications in graphics, virtual reality and multimedia processing. In this paper, we address this problem by using a two-stage iterative method. Starting from an initial guess of the structure, the first stage estimates the pose of the object. The second stage uses the estimated pose information to refine the structure. This process is repeated until the difference between the observed data and data re-projected from the estimated model is minimized. This method is a variation of the classical bundle adjustment method, but is faster in execution and is simpler to implement. We used the Kanade-Lucas-Tomasi feature tracker for obtaining the image features. Synthetic and real data have been tested with good results.
C1 Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
EM mchang@ie.cuhk.edu.hk; khwong@cse.cuhk.edu.hk
CR Beare MargaretE., 1997, TRANSNATIONAL ORG CR, V3, P11
   BIRCK D, KLT IMPLEMENTATION K
   BOUGUET JY, CAMEA CALIBRATION TO
   Dellaert F, 2000, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2000.854916
   Faugeras O., 2001, The geometry of multiple images: the laws that govern the formation of multiple images of a scene and some of their applications
   Hartley R., 2002, MULTIPLE VIEW GEOMET
   ISARD M, 1999, P IEEE INT WORKSH RE
   Jebara T, 1999, IEEE SIGNAL PROC MAG, V16, P66, DOI 10.1109/79.768574
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Lee KS, 2001, REAL-TIME IMAGING, V7, P173, DOI 10.1006/rtim.2000.0241
   Liu ML, 1999, PATTERN RECOGN LETT, V20, P69, DOI 10.1016/S0167-8655(98)00128-7
   LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043
   Morita T, 1997, IEEE T PATTERN ANAL, V19, P858, DOI 10.1109/34.608289
   Or SH, 1998, IMAGE VISION COMPUT, V16, P353, DOI 10.1016/S0262-8856(97)00073-5
   OR SH, 1999, P INT C IM SCI SYST, P596
   Pollefeys M., 1999, Ph.D. Thesis
   STRUM P, 1996, P EUR C COMP VIS EEC, V1065, P709
   Szeliski R, 1997, IEEE T PATTERN ANAL, V19, P506, DOI 10.1109/34.589211
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Triggs B., 2000, VISION ALGORITHMS TH, P298, DOI DOI 10.1007/3-540-44480-7_21THISWORKWASSUPPORTEDINPARTBYTHEEUROPEAN
   TRUCCO E, 1998, INTRO TECHNIUES 3 D
   WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074
   WONG KH, 2003, RECENT DEV THEORIES
   ZHANG Z, 1996, EPIPOLAR GEOMETRY ST
NR 25
TC 9
Z9 12
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 253
EP 260
DI 10.1109/TMM.2005.843344
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400007
DA 2024-07-18
ER

PT J
AU Cheung, CH
   Po, LM
AF Cheung, CH
   Po, LM
TI Novel cross-diamond-hexagonal search algorithms for fast block motion
   estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cross-center-biased property; cross-diamond-hexagonal search; motion
   estimation
AB In this paper, we propose two cross-diamond-hexagonal search (CDHS) algorithms, which differ from each other by their sizes of hexagonal search patterns. These algorithms basically employ two cross-shaped search patterns consecutively in the very beginning steps and switch using diamond-shaped patterns. To further reduce the checking points, two pairs of hexagonal search patterns are proposed in conjunction with candidates found located at diamond corners. Experimental results show that the proposed CDHSs perform faster than the diamond search (DS) by about 144 % and the cross-diamond search (CDS) by about 73 %, whereas similar prediction quality is still maintained.
C1 City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
EM terence@ieee.org; eelmpo@cityu.edu.hk
RI ; CHEUNG, Terence C.H./N-6914-2015
OI Po, Lai Man/0000-0002-5185-1492; CHEUNG, Terence
   C.H./0000-0003-1983-5393
CR [Anonymous], 1981, P NAT TEL C NEW ORL
   [Anonymous], P 1997 INT C INF COM
   Cheung CH, 2002, IEEE T CIRC SYST VID, V12, P1168, DOI 10.1109/TCSVT.2002.806815
   JAIN JR, 1981, IEEE T COMMUN, V29, P1799, DOI 10.1109/TCOM.1981.1094950
   Liu LK, 1996, IEEE T CIRC SYST VID, V6, P419, DOI 10.1109/76.510936
   MPEG1, 1993, 111722 JTC1SC29WG11
   MPEG4, 2000, 144692 JTC1SC29WG11
   Nie Y, 2002, IEEE T IMAGE PROCESS, V11, P1442, DOI 10.1109/TIP.2002.806251
   Po LM, 1996, IEEE T CIRC SYST VID, V6, P313, DOI 10.1109/76.499840
   Puri A., 1987, Proceedings: ICASSP 87. 1987 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.87CH2396-0), P1063
   SCHWARZ H, 2002, P IBC 2002
   THAN JY, 1998, IEEE T CIRCUITS SYST, V8, P369
   ZENG B, 1994, IEEE T CIRCUITS SYST, V4, P438
   ZHU C, 2001, P IEEE INT C AC SPEE
   Zhu S, 2000, IEEE T IMAGE PROCESS, V9, P287, DOI 10.1109/83.821744
   1989, 525 SPEC GROUP COD V
NR 16
TC 115
Z9 151
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 16
EP 22
DI 10.1109/TMM.2004.840609
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300002
DA 2024-07-18
ER

PT J
AU Ghandeharizadeh, S
   Krishnamachari, L
   Song, S
AF Ghandeharizadeh, S
   Krishnamachari, L
   Song, S
TI Placement of continuous media in wireless peer-to-peer networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE continuous display; data placement; peer-to-peer networks; replication
ID VIDEO
AB This paper investigates a novel streaming architecture consisting of home-to-home online (H2O) devices that collaborate with one another to provide on-demand access to large repositories of continuous media such as audio and video clips. An H2O device is configured with a high bandwidth wireless communication component, a powerful processor, and gigabytes of storage. A key challenge of this environment is how to place data across H2O devices in order to enhance startup latency, defined as the delay observed from when a user requests a clip, to the onset of its display. Our primary contribution is a novel replication technique that enhances startup latency, while minimizing the total storage space required from an environment consisting of N H2O devices. This technique is based on the following intuition: The first few blocks of a clip are required more urgently than its last few blocks, and should be replicated more frequently in order to minimize startup latency. We develop analytical models to quantify the number of replicas required for each block. In addition, we describe two alternative distributed implementation of our replication strategy. When compared with full replication, our technique provides on average greater than 97% (i.e., several orders of magnitude) savings in storage space, while ensuring zero startup latency and a hiccup-free reception.
C1 Univ So Calif, Dept Elect Engn Syst, Los Angeles, CA 90007 USA.
   Univ So Calif, Dept Comp Sci, Los Angeles, CA 90007 USA.
C3 University of Southern California; University of Southern California
RP Ghandeharizadeh, S (corresponding author), Univ So Calif, Dept Elect Engn Syst, Los Angeles, CA 90007 USA.
EM shahram@usc.edu; bkrishna@usc.edu; shanshas@usc.edu
CR Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P118, DOI 10.1109/MMCS.1996.534963
   ALMEIDA J, 2001, P ACM SPIE MULT COMP, P200
   [Anonymous], P ACM SIGCOMM 02 REP
   BERSON S, 1994, P ACM SIGMOD, P79
   Chen H. J., 1993, P 4 INT C FDN DAT OR, P19
   EAER D, 1999, P MMCN, P301
   GEMMELL DJ, 1995, COMPUTER, V28, P40, DOI 10.1109/2.384117
   GHANDEHARIZADEH S, 1995, COMPUT COMMUN, V18, P170, DOI 10.1016/0140-3664(95)98540-L
   GHANDEHARIZADEH S, 2003, 3 INT ACM WORKSH DAT, P77
   Ghandeharizadeh SA, 1998, PARALLEL COMPUT, V24, P91, DOI 10.1016/S0167-8191(97)00118-X
   GHANDELHARIZADE.S, 1997, P CABL MAR
   GHANDELHARIZADE.S, 2003, IEEE PAC RIM C MULT
   GUO Y, 2002, P ICC, V4, P2606
   Hua K., 1997, PROC SIGCOMM, P89
   Li J., 2001, P 7 ANN INT C MOB CO, P61, DOI DOI 10.1145/381677.381684
   Lv Qin., 2002, Proceedings of the 16th international conference on Supercomputing, P84, DOI DOI 10.1145/514191.514206
   LYNCH N, 1996, DISTRIBUTED ALGORITH, pCH4
   NG R, 1994, P 20 VLDB C, P451
   NUSSBAUMER JP, 1995, IEEE J SEL AREA COMM, V13, P779, DOI 10.1109/49.391753
   POLIMENIS V, 1991, TR91020 ICSI
   Rodriguez P, 2002, IEEE ACM T NETWORK, V10, P455, DOI 10.1109/TNET.2002.801413
   SEN S, 1999, P IEEE, V87, P1310
   SHAHABI C, 2002, IEEE T PARALL DISTR, V13, P455
   Tewari R., 1998, P SPIE ACM C MULT CO, P191
   Tobagi F. A., 1993, Proceedings ACM Multimedia 93, P393, DOI 10.1145/166266.168435
   VISWANATHAN S, 1995, P SOC PHOTO-OPT INS, V2417, P66, DOI 10.1117/12.206080
   WANG B, 2002, P IEEE, V90, P1726
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
NR 28
TC 18
Z9 27
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 335
EP 342
DI 10.1109/TMM.2003.822787
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ghandeharizadeh, S
   Huang, LG
   Kamel, I
AF Ghandeharizadeh, S
   Huang, LG
   Kamel, I
TI A cost driven disk scheduling algorithm for multimedia object retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID CONTINUOUS MEDIA SERVERS; REAL-TIME SYSTEMS; PERFORMANCE EVALUATION
AB This paper describes a novel cost-driven disk scheduling algorithm for environments consisting of multipriority requests. An example application is a video-on-demand (VOD) system that provides high and low quality services, termed priority 2 and 1, respectively. Customers ordering a high quality (priority 2) service pay a higher fee and are assigned a higher priority by the underlying system. Our proposed algorithm minimizes costs by maintaining one-queue And managing requests intelligently in order to meet the deadline of as many priority 1 requests as possible while maximizing the number of priority 2 requests that meet their deadline. Our algorithm is general enough to accommodate an arbitrary number of priority levels. Prior schemes, collectively termed "multiqueue" schemes maintain a separate queue for each priority level in order to optimize the performance of the high priority requests only. When compared with our proposed scheme, in certain cases, our technique provides more than one order of magnitude improvement in total cost.
C1 Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
   Panason Informat & Networking Technol Lab, Princeton, NJ 08540 USA.
C3 University of Southern California; Panasonic
RP Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM shahram@usc.edu; liguoh@usc.edu; ibrahim@research.panasonic.com
OI Kamel, Ibrahim/0000-0001-5546-939X; Huang, LiGuo/0000-0001-7790-0195
CR ABBOTT RK, 1992, ACM T DATABASE SYST, V17, P513, DOI 10.1145/132271.132276
   Andrews M, 1996, AN S FDN CO, P550, DOI 10.1109/SFCS.1996.548514
   Aref WG, 2001, IEEE T KNOWL DATA EN, V13, P933, DOI 10.1109/69.971188
   Carey M. J., 1989, Proceedings of the Fifteenth International Conference on Very Large Data Bases, P397
   CAREY MJ, 1988, ACM SIGMOD REC, V17, P51
   Chang E, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P87, DOI 10.1145/266180.266339
   CHEN M, 1993, P 1 ACM INT C MULT A
   CHEN SZ, 1991, REAL-TIME SYST, V3, P307, DOI 10.1007/BF00364960
   CHENG RMK, 1996, P 8 EUR WORKSH REAL
   DENNING PJ, 1968, COMMUN ACM, V11, P323, DOI 10.1145/363095.363141
   GEMMELL J, 1992, ACM T INFORM SYST, V10, P51, DOI 10.1145/128756.128758
   GHANDEHARIZADEH.S, 2000, MULTIMEDIA TOOLS MAY
   Ghandeharizadeh SA, 1998, PARALLEL COMPUT, V24, P91, DOI 10.1016/S0167-8191(97)00118-X
   GROCHOWSKI E, 2000, MAGNETIC DISK TRENDS
   GROSS D, 1998, FUNDAMENTALS QUEUE T, pCH2
   Haritsa J.R., 1993, VLDB J, V2, P117
   KAMEL I, 2000, P IEEE DATA ENG C 20
   LIN TH, 1991, P SIMM C, P31
   RAMAMRITHAM K, 1994, P IEEE, V82, P55, DOI 10.1109/5.259426
   REDDY ALN, 1994, COMPUTER, V27, P69, DOI 10.1109/2.268888
   Santos JR, 2000, PERF E R SI, V28, P44, DOI 10.1145/345063.339352
   To TPJ, 1999, MULTIMEDIA SYST, V7, P91, DOI 10.1007/s005300050113
   WIJAYARATNE R, 1999, P IEEE INT C MULT CO
NR 23
TC 4
Z9 5
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 186
EP 196
DI 10.1109/TMM.2003.811623
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100004
DA 2024-07-18
ER

PT J
AU Berretti, S
   Del Bimbo, A
   Vicario, E
AF Berretti, S
   Del Bimbo, A
   Vicario, E
TI Weighted walkthroughs between extended entities for retrieval by spatial
   arrangement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image databases; retrieval by visual content; spatial relationships
ID KNOWLEDGE
AB In the access to image databases, queries based on the appearing visual features of searched data reduce the gap between the user and the engineering representation. To support this access modality, image content can be modeled in terms of different types of features such as shape, texture, color, and spatial arrangement.
   An original framework is presented which supports quantitative nonsymbolic representation and comparison of the mutual positioning of extended nonrectangular spatial entities. Properties of the model are expounded to develop an efficient computation technique and to motivate and assess a metric of similarity for quantitative comparison of spatial relationships. Representation and comparison of binary relationships between entities is then embedded into a graph-theoretical framework supporting representation and comparison of the spatial arrangements of a picture. Two prototype applications are described.
C1 Univ Florence, Dipartimento Sistemi & Informat, I-50139 Florence, Italy.
C3 University of Florence
RP Berretti, S (corresponding author), Univ Florence, Dipartimento Sistemi & Informat, I-50139 Florence, Italy.
RI vicario, enrico/ABG-4344-2020; Berretti, Stefano/U-9004-2019
OI vicario, enrico/0000-0002-4983-4386; Berretti,
   Stefano/0000-0003-1219-4386; DEL BIMBO, ALBERTO/0000-0002-1052-8322
CR ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434
   [Anonymous], 1993, NETWORK FLOWS THEORY
   Berretti S, 2001, PATTERN ANAL APPL, V4, P83, DOI 10.1007/s100440170009
   Berretti S, 2001, IEEE T PATTERN ANAL, V23, P1089, DOI 10.1109/34.954600
   BERRETTI S, 2000, P 15 ICPR BARC SPAIN
   BERRETTI S, 1999, P IEEE ICMCS FIR IT
   BERRETTI S, 2000, P IEEE INT WORKSH CB
   CHANG S, 1989, SPIE P VISUAL COMMUN, V1199, P1360
   CHANG SK, 1987, IEEE T PATTERN ANAL, V9, P413, DOI 10.1109/TPAMI.1987.4767923
   CHANG SK, 1991, J VISUAL LANG COMPUT, V2, P195
   CORRIDONI JM, 1998, J AM SOC INF SYST
   Del Bimbo A, 1998, PATTERN RECOGN, V31, P1241, DOI 10.1016/S0031-3203(97)00164-7
   DELBIMBO A, 1999, VISUAL INFORMATION R
   DELBIMBO A, 1998, IEEE WORKSH CONT BAS
   DELBIMBO A, 1995, IEEE T VIS COMP GRAP, V1
   DELBIMBO A, 1998, IMAGE DESCRIPTION RE
   EGENHOFER MJ, 1991, INT J GEOGR INF SYST, V5, P161, DOI 10.1080/02693799108927841
   EGENHOFER MJ, 1992, INT J GEOGRAPH INF S, V9
   Flickner M., 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146
   FRANKLIN J, 1992, J VEG SCI, V3, P3, DOI 10.2307/3235991
   FREKSA C, 1992, LECT NOTES COMPUT SC, V639, P162
   GUDIVADA VN, 1995, INT C MULT COMP SYST
   GUDIVADA VN, 1995, COMPUTER, V28
   GUDIVADA VN, 1995, ACM T INF SYST, V13
   Gupta A, 1997, COMMUN ACM, V40, P70, DOI 10.1145/253769.253798
   HERNANDEZ D, FK113590 TU MUENCH
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   JUNGERT E, 1993, IEEE INT WORKSH VIS, P83
   LEE SY, 1992, PATTERN RECOGN, V25, P305, DOI 10.1016/0031-3203(92)90112-V
   LEE SY, 1992, J VISUAL LANG COMPUT, V3, P373
   PAPADIAS D, 1995, J VISUAL LANG COMPUT, V6, P53, DOI 10.1006/jvlc.1995.1004
   PAPADIAS D, 1993, P EUR C SPAT INF THE, P234
   SMITH J, 1998, P IEEE CVPR 98 WORKS
   SMITH JR, 1996, P ACM MULT BOST MA N
   SMITH JR, 1995, 4149520 TR COL U
   SOFFER A, 1996, P INT WORKSH IDB MMS
   TAO Y, 1999, P IEEE ICMCS FIR IT
   VICARIO E, 1997, P ICIAP SEPT
NR 38
TC 39
Z9 43
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017-2394 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 52
EP 70
DI 10.1109/TMM.2002.802833
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200006
DA 2024-07-18
ER

PT J
AU Cui, YW
   Deng, WX
   Xu, X
   Liu, Z
   Liu, Z
   Pietikainen, M
   Liu, L
AF Cui, Yawen
   Deng, Wanxia
   Xu, Xin
   Liu, Zhen
   Liu, Zhong
   Pietikainen, Matti
   Liu, Li
TI Uncertainty-Guided Semi-Supervised Few-Shot Class-Incremental Learning
   With Knowledge Distillation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Power capacitors; Training; Semisupervised learning; Data
   models; Uncertainty; Deep learning; Few-shot learning; class-incremental
   learning; object classification; computer vision; semi-supervised
   learning; deep learning; knowledge distillation; uncertainty estimation
AB Class-Incremental Learning (CIL) aims at incrementally learning novel classes without forgetting old ones. This capability becomes more challenging when novel tasks contain one or a few labeled training samples, which leads to a more practical learning scenario, i.e., Few-Shot Class-Incremental Learning (FSCIL). The dilemma on FSCIL lies in serious overfitting and exacerbated catastrophic forgetting caused by the limited training data from novel classes. In this paper, excited by the easy accessibility of unlabeled data, we conduct a pioneering work and focus on a Semi-Supervised Few-Shot Class-Incremental Learning (Semi-FSCIL) problem, which requires the model incrementally to learn new classes from extremely limited labeled samples and a large number of unlabeled samples. To address this problem, a simple but efficient framework is first constructed based on the knowledge distillation technique to alleviate catastrophic forgetting. To efficiently mitigate the overfitting problem on novel categories with unlabeled data, uncertainty-guided semi-supervised learning is incorporated into this framework to select unlabeled samples into incremental learning sessions considering the model uncertainty. This process provides extra reliable supervision for the distillation process and contributes to better formulating the class means. Our extensive experiments on CIFAR100, miniImageNet and CUB200 datasets demonstrate the promising performance of our proposed method, and define baselines in this new research direction.
C1 [Cui, Yawen; Pietikainen, Matti] Univ Oulu, CMVS, Oulu 90570, Finland.
   [Deng, Wanxia] NUDT, Sch Meteorol & Oceanog, Changsha 410073, Peoples R China.
   [Xu, Xin; Liu, Zhen] NUDT, Coll Intelligent Sci, Changsha 410073, Peoples R China.
   [Liu, Zhong] Natl Univ Def Technol NUDT, Coll Syst Engn, Lab Big Data & decis, Changsha 410073, Peoples R China.
   [Liu, Li] Natl Univ Def Technol NUDT, Coll Syst Engn, Lab Big Data & Decis, Changsha 410073, Peoples R China.
   [Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal CMVS, Oulu 90570, Finland.
C3 University of Oulu; National University of Defense Technology - China;
   National University of Defense Technology - China; National University
   of Defense Technology - China; National University of Defense Technology
   - China; University of Oulu
RP Liu, L (corresponding author), Natl Univ Def Technol NUDT, Coll Syst Engn, Lab Big Data & Decis, Changsha 410073, Peoples R China.
EM yawen.cui@oulu.fi; wanxiadeng@163.com; xinxu@nudt.edu.cn;
   zhen_liu@nudt.edu.cn; liuzhong@nudt.edu.cn; matti.pietikainen@oulu.fi;
   dreamliu2010@gmail.com
RI Liu, Li/JQW-6992-2023
OI Liu, Zhen/0000-0002-1233-1494; Liu, li/0000-0002-2011-2873; Pietikainen,
   Matti/0000-0003-2263-6731
FU National Key Research and Development Program of China [2021YFB3100800];
   Academy of Finland [331883]; National Natural Science Foundation of
   China [61872379, 62022091]; China Scholarship Council (CSC)
   [201903170129]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2021YFB3100800, in part by the
   the Academy of Finland under Grant 331883, in part by the National
   Natural Science Foundation of China under Grants 61872379, 62022091 and
   62022091, and in part by the China Scholarship Council (CSC) under Grant
   201903170129. The Associate Editor coordinating the review of
   thismanuscript and approving it for publication was Prof. Guo-Jun Qi.
CR Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753
   Berthelot D, 2019, ADV NEUR IN, V32
   Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15
   Chapelle O., 2005, Proceedings of the tenth international workshop on artificial intelligence and statistics, P57
   Chaudhry A., 2018, P ICLR INT C LEARN R, P1
   Chen K, 2021, P INT C LEARN REPR, P1
   Chen TY, 2022, IEEE T MULTIMEDIA, V24, P2975, DOI 10.1109/TMM.2021.3091859
   Chen ZT, 2019, IEEE T IMAGE PROCESS, V28, P4594, DOI 10.1109/TIP.2019.2910052
   Cheraghian A, 2021, PROC CVPR IEEE, P2534, DOI 10.1109/CVPR46437.2021.00256
   Cui YW, 2021, IEEE IMAGE PROC, P1239, DOI 10.1109/ICIP42928.2021.9506346
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4
   Finn C, 2017, PR MACH LEARN RES, V70
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Hsu K., 2018, INT C LEARN REPR, P10132
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Ji ZL, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00083
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Joachims T., 2003, P 20 INT C MACH LEAR, V20, P290, DOI DOI 10.1145/2612669.2612699
   Jung H, 2018, AAAI CONF ARTIF INTE, P3358
   Khodadadeh S, 2019, ADV NEUR IN, V32
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Lake B., 2011, P ANN M COGNITIVE SC, P1
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Liu B, 2019, IEEE INT CONF COMP V, P1317, DOI 10.1109/ICCVW.2019.00167
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu Y., 2018, P INT C LEARN REPR, P1
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Neyshabur B, 2017, ADV NEUR IN, V30
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Pu YC, 2016, ADV NEUR IN, V29
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Reed S, 2016, PR MACH LEARN RES, V48
   Ren M., 2018, INT C LEARNING REPRE, DOI DOI 10.1109/IPFA.2018.8452547
   Rizve M. N., 2021, P INT C LEARN REPR
   Santoro A, 2016, PR MACH LEARN RES, V48
   Shi WW, 2018, LECT NOTES COMPUT SC, V11209, P311, DOI 10.1007/978-3-030-01228-1_19
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Shyam P, 2017, PR MACH LEARN RES, V70
   Snell J, 2017, ADV NEUR IN, V30
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang X, 2023, IEEE T PATTERN ANAL, V45, P5549, DOI 10.1109/TPAMI.2022.3203630
   Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37
   Xiaoyu Tao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12180, DOI 10.1109/CVPR42600.2020.01220
   Yikai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12833, DOI 10.1109/CVPR42600.2020.01285
   Yu XD, 2010, LECT NOTES COMPUT SC, V6315, P127
   Yu Z., 2020, CVPR, P12856
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhang C, 2021, PROC CVPR IEEE, P12450, DOI 10.1109/CVPR46437.2021.01227
   Zhou T., 2020, INT C MACH LEARN, P11523
   Zhu K, 2021, PROC CVPR IEEE, P6797, DOI 10.1109/CVPR46437.2021.00673
NR 67
TC 5
Z9 5
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6422
EP 6435
DI 10.1109/TMM.2022.3208743
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500055
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Fu, ZR
   Mao, ZD
   Hu, B
   Liu, AA
   Zhang, YD
AF Fu, Zheren
   Mao, Zhendong
   Hu, Bo
   Liu, An-An
   Zhang, Yongdong
TI Intra-Class Adaptive Augmentation With Neighbor Correction for Deep
   Metric Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep metric learning; distribution estimation; sample generation;
   semantic augmentation
AB Deep metric learning aims to learn an embedding space, where semantically similar samples are close together and dissimilar ones are repelled against. To explore more hard and informative training signals for augmentation and generalization, recent methods focus on generating synthetic samples to boost metric learning losses. However, these methods just use the deterministic and class-independent generations (e.g., simple linear interpolation), which only can cover the limited part of distribution spaces around original samples. They have overlooked the wide characteristic changes of different classes and can not model abundant intra-class variations for generations. Therefore, generated samples not only lack rich semantics within the certain class, but also might be noisy signals to disturb training. In this paper, we propose a novel intra-class adaptive augmentation (IAA) framework for deep metric learning. We reasonably estimate intra-class variations for every class and generate adaptive synthetic samples to support hard samples mining and boostmetric learning losses. Further, for most datasets that have a few samples within the class, we propose the neighbor correction to revise the inaccurate estimations, according to our correlation discovery where similar classes generally have similar variation distributions. Extensive experiments on five benchmarks show our method significantly improves and outperforms the state-of-the-art methods on retrieval performances by 3%-6%.
C1 [Fu, Zheren] Univ Sci & Technol China, Sch Cyberspace Sci & Technol, Hefei 230027, Peoples R China.
   [Mao, Zhendong; Hu, Bo; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Peoples R China.
   [Mao, Zhendong; Zhang, Yongdong] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230022, Peoples R China.
   [Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Tianjin University
RP Mao, ZD (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Peoples R China.; Mao, ZD (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230022, Peoples R China.
EM fzr@mail.ustc.edu.cn; zdmao@ustc.edu.cn; hubo@ustc.edu.cn;
   anan0422@gmail.com; zhyd73@ustc.edu.cn
RI Fu, Zheren/HSF-3449-2023
OI Fu, Zheren/0000-0001-8389-8642
FU National Natural Science Foundation of China [62222212, U19A2057,
   62121002]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62222212, U19A2057, and 62121002.
CR Akaike H, 1973, 2 INT S INF THEOR, P267, DOI [DOI 10.1007/978-1-4612-1694-0_15, 10.1007/978-1-4612-0919-5_38, DOI 10.1007/978-1-4612-0919-5_38]
   Bai Y, 2017, IEEE INT CON MULTI, P1452, DOI 10.1109/ICME.2017.8019371
   Bengio Y., 2013, P 30 INT C MACHINE L, P552
   Brock A., 2019, PROC INT C LEARN RE
   Cai Qi, 2020, P NEURIPS, V33, P12638
   Cakir F, 2019, PROC CVPR IEEE, P1861, DOI 10.1109/CVPR.2019.00196
   Chen BH, 2021, AAAI CONF ARTIF INTE, V35, P982
   Chen FY, 2021, IEEE T MULTIMEDIA, V23, P3073, DOI 10.1109/TMM.2020.3019710
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294
   Fu ZR, 2021, AAAI CONF ARTIF INTE, V35, P1370
   Gu G, 2021, AAAI CONF ARTIF INTE, V35, P1460
   Gu G, 2020, AAAI CONF ARTIF INTE, V34, P10853
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YQ, 2021, LECT NOTES COMPUT SC, V12892, P487, DOI 10.1007/978-3-030-86340-1_39
   Huang W, 2021, IEEE T MULTIMEDIA, V24, P3327, DOI 10.1109/TMM.2021.3096068
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kim DH, 2021, IEEE T MULTIMEDIA, V24, P3533, DOI 10.1109/TMM.2021.3101944
   Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330
   Kingma D. P., 2015, P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR, P1
   Ko B, 2020, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR42600.2020.00728
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lee K., 2022, PROC BRIT MACH VIS
   Lehmann EL., 1998, THEORY POINT ESTIMAT, DOI 10.1007/b98854
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li S, 2021, PROC CVPR IEEE, P5208, DOI 10.1109/CVPR46437.2021.00517
   Lin XD, 2018, LECT NOTES COMPUT SC, V11219, P714, DOI 10.1007/978-3-030-01267-0_42
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu JL, 2020, PROC CVPR IEEE, P2967, DOI 10.1109/CVPR42600.2020.00304
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Milbich T., 2020, PROC EUR C COMPUT V, P590
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Musgrave Kevin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P681, DOI 10.1007/978-3-030-58595-2_41
   Myers J. L., 2013, Japanese J. Nat. Med. Serv., V55
   Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925
   Opitz M, 2017, IEEE I CONF COMP VIS, P5199, DOI 10.1109/ICCV.2017.555
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Rolinek Michal, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7617, DOI 10.1109/CVPR42600.2020.00764
   Roth K, 2019, IEEE I CONF COMP VIS, P7999, DOI 10.1109/ICCV.2019.00809
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sanakoyeu A, 2019, PROC CVPR IEEE, P471, DOI 10.1109/CVPR.2019.00056
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Suh Y, 2019, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2019.00742
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Szegedy C., 2015, PROC IEEE C COMPU V, P1
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wang XS, 2019, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR.2019.00535
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wang YL, 2019, ADV NEUR IN, V32
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753
   Xu M., 2021, PROC INT C LEAR REP
   Yang JC, 2012, IEEE T MULTIMEDIA, V14, P1642, DOI 10.1109/TMM.2012.2198458
   Yang S, 2022, IEEE T PATTERN ANAL, V44, P9830, DOI 10.1109/TPAMI.2021.3132021
   Yang YH, 2023, IEEE T MULTIMEDIA, V25, P280, DOI 10.1109/TMM.2021.3125134
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1640, DOI 10.1109/TMM.2020.3001527
   Yuan TT, 2019, PROC CVPR IEEE, P4810, DOI 10.1109/CVPR.2019.00495
   Zhai A., 2019, PROC 30 BRIT MACH V
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao WL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9867, DOI 10.1109/ICCV48922.2021.00974
   Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31
   Zheng AH, 2022, IEEE T MULTIMEDIA, V24, P338, DOI 10.1109/TMM.2021.3050089
   Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
NR 75
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7758
EP 7771
DI 10.1109/TMM.2022.3227414
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Guo, XB
   Kong, AWK
   Kot, A
AF Guo, Xiaobao
   Kong, Adams Wai-Kin
   Kot, Alex
TI Deep Multimodal Sequence Fusion by Regularized Expressive Representation
   Distillation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal sequence fusion; multimodal sentiment analysis;
   regularization
ID RECOGNITION; EMOTION
AB Multimodal sequence learning aims to utilize information from different modalities to enhance overall performance. Mainstream works often follow an intermediate-fusion pipeline, which explores both modality-specific and modality-supplementary information for fusion. However, the unaligned and heterogeneously distributed multimodal sequences pose significant challenges to the fusion task: 1) to extract both effective unimodal and crossmodal representations and 2) to overcome the overfitting issue in joint multimodal sequence optimization. In this work, we propose regularized expressive representation distillation (RERD) that aims to seek effective multimodal representations and to enhance the generalization of fusion. First, to improve unimodal representation learning, unimodal representations are assigned to multi-head distillation encoders, where the unimodal representations are iteratively updated through distillation attention layers. Second, to alleviate the overfitting issue in joint crossmodal optimization, a multimodal sinkhorn distance regularizer is proposed to reinforce the expressive representation extraction and to reduce the modality gap before fusion adaptively. These representations produce a comprehensive view of the multimodal sequences, which are utilized for downstream fusion tasks. Experimental results on several popular benchmarks demonstrate that the proposed method achieves state-of-the-art performance, compared with widely used baselines for deep multimodal sequence fusion, as shown in https://github.com/Redaimao/RERD.
C1 [Guo, Xiaobao] Nanyang Technol Univ Singapore, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Guo, Xiaobao; Kong, Adams Wai-Kin; Kot, Alex] Nanyang Technol Univ, Interdisciplinary Grad Programme, Rapid Rich Object Search ROSE Lab, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Kong, AWK (corresponding author), Nanyang Technol Univ, Interdisciplinary Grad Programme, Rapid Rich Object Search ROSE Lab, Singapore 639798, Singapore.
EM xiaobao001@e.ntu.edu.sg; AdamsKong@ntu.edu.sg; eackot@ntu.edu.sg
OI Kot, Alex/0000-0001-6262-8125; Guo, Xiaobao/0000-0002-3427-8540
FU Nanyang Technological University
FX No Statement Available
CR Baltrusaitis Tadas, 2018, The Handbook of Multimodal-Multisensor Interfaces, P17, DOI 10.1145/
   Brown T., 2020, Advances in Neural Information Processing Systems, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, DOI 10.5555/3495724.3495883]
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chauhan DS, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5647
   Chen MY, 2018, IEEE SIGNAL PROC LET, V25, P1440, DOI 10.1109/LSP.2018.2860246
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Cuturi M., 2013, ADV NEURAL INFORM PR, P2292, DOI DOI 10.48550/ARXIV.1306.0895
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Glodek M, 2011, LECT NOTES COMPUT SC, V6975, P359, DOI 10.1007/978-3-642-24571-8_47
   Gunes H, 2005, IEEE SYS MAN CYBERN, P3437
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Hamelin N, 2017, J RETAIL CONSUM SERV, V36, P103, DOI 10.1016/j.jretconser.2017.01.001
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, ADV NEUR IN, V9, P473
   Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045
   Li RN, 2019, INT CONF ACOUST SPEE, P6675, DOI 10.1109/icassp.2019.8682154
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Liu PF, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1, DOI 10.18653/v1/P17-1001
   Mai SJ, 2020, AAAI CONF ARTIF INTE, V34, P164
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Mai SJ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P481
   McFee B., 2015, P PYTH SCI C AUST TX, P18, DOI 10.25080/Majora-7b98e3ed-003
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI DOI 10.1145/2070481.2070509
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21
   Rubner L. J., 1997, P ARPA IM UND WORKSH, P661
   Shenoy A, 2020, PROCEEDINGS OF THE SECOND GRAND CHALLENGE AND WORKSHOP ON MULTIMODAL LANGUAGE (CHALLENGE-HML), VOL 1, P19
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tian HM, 2019, WORLD WIDE WEB, V22, P1325, DOI 10.1007/s11280-018-0548-3
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Vielzeuf V., 2017, P 19 ACM INT C MULT, P569
   Villani C., 2021, American Mathematical Soc., V58
   Wang D., 2020, P IEEECVF C COMPUTER, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wang HH, 2017, IEEE INT CON MULTI, P949, DOI 10.1109/ICME.2017.8019301
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Yu WM, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3718
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhou HY, 2021, AAAI CONF ARTIF INTE, V35, P11106
NR 53
TC 6
Z9 6
U1 25
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2085
EP 2096
DI 10.1109/TMM.2022.3142448
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD7R0
UT WOS:001116593700007
DA 2024-07-18
ER

PT J
AU Ji, YL
   Ma, S
   Xu, X
   Li, XL
   Shen, HT
AF Ji, Yanli
   Ma, Shuo
   Xu, Xing
   Li, Xuelong
   Shen, Heng Tao
TI Self-Supervised Fine-Grained Cycle-Separation Network (FSCN) for
   Visual-Audio Separation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio source separation; Fine-grained Cycle-Separation (FCSN) Network;
   Self-supervised learning; Visual-guided separation
AB Audio mixture separation is still challenging due to heavy overlaps and interactions. To correctly separate audio mixtures, we propose a novel self-supervised Fine-grained Cycle-Separation Network (FCSN) for vision-guided audio mixture separation. In the proposed approach, we design a two-stage procedure to perform self-supervised separation on audio mixtures. Using visual information as guidance, a primary-stage separation is realized via a U-net network, then the residual spectrogram is calculated by removing separated spectrograms from the original audio mixture. At the second-stage separation, a cycle-separation module is proposed to refine separation using separated results and the residual spectrogram. Self-supervision learning between vision and audio modalities is presented to push the cycle separation until the residual spectrogram becomes empty. Extensive experiments are evaluated on three large-scale datasets, MUSIC (MUSIC-21), AudioSet, and VGGSound. Experiment results certify that our approach outperforms the state-of-the-art approaches, and demonstrate the effectiveness for separating audio mixtures with overlap and interaction.
C1 [Ji, Yanli; Ma, Shuo; Xu, Xing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Xian 710072, Peoples R China.
   [Shen, Heng Tao] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 University of Electronic Science & Technology of China; Northwestern
   Polytechnical University; Peng Cheng Laboratory
RP Shen, HT (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
EM yanliji@uestc.edu.cn; mashuo@std.uestc.edu.cn; xing.xu@uestc.edu.cn;
   xuelong_li@ieee.org; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021; Li, Xue-long/AFU-6301-2022
OI Li, Xue-long/0000-0003-2037-2525
FU Science and Technology Innovation Committee of Shenzhen Municipality
   Foundation [JCYJ20210324132203007]; National Key Research and
   Development Program of China [2018AAA0102200]
FX This work was supported by the Science and Technology Innovation
   Committee of Shenzhen Municipality Foundation under Grant
   JCYJ20210324132203007, and in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102200.
CR Afouras T, 2018, INTERSPEECH, P3244
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Chandna P, 2017, LECT NOTES COMPUT SC, V10169, P258, DOI 10.1007/978-3-319-53547-0_25
   Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI [10.1109/ICASSP40776.2020.9053174, 10.1109/icassp40776.2020.9053174]
   Chuang Gan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10475, DOI 10.1109/CVPR42600.2020.01049
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Gabbay A, 2018, INTERSPEECH, P1170
   Gao RH, 2019, IEEE I CONF COMP VIS, P3878, DOI 10.1109/ICCV.2019.00398
   Gao RH, 2018, LECT NOTES COMPUT SC, V11207, P36, DOI 10.1007/978-3-030-01219-9_3
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Grais EM, 2017, IEEE-ACM T AUDIO SPE, V25, P1469, DOI 10.1109/TASLP.2017.2716443
   GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317
   Haykin S, 2005, NEURAL COMPUT, V17, P1875, DOI 10.1162/0899766054322964
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hegde SB, 2021, IEEE WINT CONF APPL, P1925, DOI 10.1109/WACV48630.2021.00197
   Hershey JR, 2016, INT CONF ACOUST SPEE, P31, DOI 10.1109/ICASSP.2016.7471631
   Hu D., 2020, Advances in Neural Information Processing Systems, P10077
   Hu D, 2019, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR.2019.00947
   Hu D, 2019, PROC CVPR IEEE, P7964, DOI 10.1109/CVPR.2019.00816
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Korbar B, 2018, ADV NEUR IN, V31
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Li XF, 2013, IEEE T CYBERNETICS, V43, P1199, DOI 10.1109/TSMCB.2012.2226443
   Liu, 2020, ECCV, P52, DOI DOI 10.1007/978-3-030-58610-24
   Liu Y, 2020, IEEE T MULTIMEDIA, V22, P934, DOI 10.1109/TMM.2019.2937185
   McDermott JH, 2009, CURR BIOL, V19, pR1024, DOI 10.1016/j.cub.2009.09.005
   Mittal A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.75
   Morrone G, 2019, INT CONF ACOUST SPEE, P6900, DOI 10.1109/ICASSP.2019.8682061
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nie FP, 2020, IEEE T KNOWL DATA EN, V32, P1167, DOI 10.1109/TKDE.2019.2901853
   Nugraha AA, 2016, IEEE-ACM T AUDIO SPE, V24, P1652, DOI 10.1109/TASLP.2016.2580946
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Pu J, 2020, IEEE T CYBERNETICS, V50, P2288, DOI 10.1109/TCYB.2018.2883607
   Qian XY, 2022, IEEE T MULTIMEDIA, V24, P942, DOI 10.1109/TMM.2021.3061800
   Raffel C., 2014, P ISMIR, P367
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rui Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P292, DOI 10.1007/978-3-030-58565-5_18
   Simpson AJR, 2015, LECT NOTES COMPUT SC, V9237, P429, DOI 10.1007/978-3-319-22482-4_50
   Sriskandaraja K, 2018, INTERSPEECH, P671, DOI 10.21437/Interspeech.2018-1819
   Takahashi N, 2019, INTERSPEECH, P1348, DOI 10.21437/Interspeech.2019-1550
   Tian YP, 2021, PROC CVPR IEEE, P2744, DOI 10.1109/CVPR46437.2021.00277
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005
   Virtanen T, 2007, IEEE T AUDIO SPEECH, V15, P1066, DOI 10.1109/TASL.2006.885253
   Wang JR, 2020, IEEE WINT CONF APPL, P3298, DOI 10.1109/WACV45572.2020.9093345
   Wen Y., 2018, P 6 INT C LEARN REPR
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Xu XD, 2019, IEEE I CONF COMP VIS, P882, DOI 10.1109/ICCV.2019.00097
   Yang L, 2018, INT C PATT RECOG, P2050, DOI 10.1109/ICPR.2018.8545493
   Yapeng Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P436, DOI 10.1007/978-3-030-58580-8_26
   Yook D, 2016, IEEE T CYBERNETICS, V46, P20, DOI 10.1109/TCYB.2015.2391252
   Yul D, 2017, INT CONF ACOUST SPEE, P241, DOI 10.1109/ICASSP.2017.7952154
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zheng AH, 2022, IEEE T MULTIMEDIA, V24, P338, DOI 10.1109/TMM.2021.3050089
NR 56
TC 0
Z9 0
U1 3
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5864
EP 5876
DI 10.1109/TMM.2022.3200282
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500016
DA 2024-07-18
ER

PT J
AU Jiang, N
   Sheng, B
   Li, P
   Lee, TY
AF Jiang, Nan
   Sheng, Bin
   Li, Ping
   Lee, Tong-Yee
TI PhotoHelper: Portrait Photographing Guidance Via Deep Feature Retrieval
   and Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic assessment; deep feature fusion; image retrieval;
   photographing guidance; spatial composition rule
ID PHOTO COMPOSITION; AESTHETICS
AB We introduce a new photographing guidance (PhotoHelper) for amateur photographers to enhance their portrait photo quality using deep feature retrieval and fusion. In our model, we comprehensively integrate empirical aesthetic rules, traditional machine learning algorithms and deep neural networks to extract different kinds of features in both color and space aspects. With these features, we build a modified random forest with a structured photograph collection to identify types of photos. We also define the composition matching score to measure the similarity between the given photo and the reference photo. By combining all of the above processes, a one-stop deep portrait photographing guidance is constructed to provide users with professional reference photographs that are similar to the current scene and automatically generate spatial composition guidance according to the user-selected reference photo. Experiments and evaluations show that the aesthetic quality of portrait photos can be significantly improved via the composition guidance of our photographing guidance approach.
C1 [Jiang, Nan; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 Shanghai Jiao Tong University; Hong Kong Polytechnic University;
   National Cheng Kung University
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
EM njiang2021@sjtu.edu.cn; shengbin@cs.sjtu.edu.cn; p.li@polyu.edu.hk;
   tonylee@mail.ncku.edu.tw
RI Li, Ping/AAO-2019-2020
OI Li, Ping/0000-0002-1503-0240; Sheng, Bin/0000-0001-8678-2784
FU National Natural Science Foundation of China [61872241, 61572316]; Hong
   Kong Polytechnic University [P0030419, P0030929, P0035358]; Ministry of
   Science and Technology, Taiwan [110-2221-E-006-135-MY3]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872241 and 61572316, in part by The
   Hong Kong Polytechnic University under Grants P0030419, P0030929, and
   P0035358, and in part by the Ministry of Science and Technology under
   Grant 110-2221-E-006-135-MY3, Taiwan. The Associate Editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Liangliang Cao.
CR Almazàn EJ, 2017, PROC CVPR IEEE, P5854, DOI 10.1109/CVPR.2017.620
   [Anonymous], 2011, Wireless Communications and Signal Processing (WCSP), 2011 International Conference on
   [Anonymous], 2010, ACM MULTIMEDIA
   Balasubramanian G. P., 2008, P INT SOC OPT ENG, V6806, P536
   Campbell A, 2015, LECT NOTES COMPUT SC, V9027, P27, DOI 10.1007/978-3-319-16498-4_3
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen YX, 2018, IEEE T CYBERNETICS, V48, P3092, DOI 10.1109/TCYB.2017.2758350
   Collier John., 1986, VISUAL ANTHR PHOTOGR
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu X, 2019, 2019 IEEE VISUALIZATION CONFERENCE (VIS), P126, DOI [10.1109/VISUAL.2019.8933570, 10.1109/visual.2019.8933570]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu SM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508381
   Jahanian A., 2015, P INT SOC OPT ENG, V9394
   Jin Y, 2012, COMPUT GRAPH-UK, V36, P955, DOI 10.1016/j.cag.2012.07.007
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Lei T, 2019, IEEE T FUZZY SYST, V27, P1753, DOI 10.1109/TFUZZ.2018.2889018
   Liang Y, 2018, IEEE T VIS COMPUT GR, V24, P2728, DOI 10.1109/TVCG.2017.2764895
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Lu XH, 2015, IEEE IMAGE PROC, P507, DOI 10.1109/ICIP.2015.7350850
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Obdrzálek S, 2012, IEEE ENG MED BIO, P1188, DOI 10.1109/EMBC.2012.6346149
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Park J, 2012, IEEE IMAGE PROC, P2741, DOI 10.1109/ICIP.2012.6467466
   Qiu Q., 2014, P INT C LEARN REPR, P1
   Setlur Vidya., 2005, MUM, V154, P59, DOI DOI 10.1145/1149488.1149499
   Simonyan Karen, 2014, WORKSH P INT C LEARN
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Sun Y, 2014, ADV NEUR IN, V27
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   von Gioi RG, 2012, IMAGE PROCESS ON LIN, V2, P35, DOI 10.5201/ipol.2012.gjmr-lsd
   Wang J., 2007, P IEEECVF C COMPUTER, P1
   XU L, 1990, PATTERN RECOGN LETT, V11, P331, DOI 10.1016/0167-8655(90)90042-Z
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhang SH, 2019, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2019.00098
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu Z, 2016, IEEE T VIS COMPUT GR, V22, P1945, DOI 10.1109/TVCG.2015.2480081
NR 43
TC 37
Z9 37
U1 6
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2226
EP 2238
DI 10.1109/TMM.2022.3144890
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100049
HC N
HP Y
DA 2024-07-18
ER

PT J
AU Lei, XZ
   Fei, ZX
   Zhou, WJ
   Zhou, HY
   Fei, MR
AF Lei, Xiaozhou
   Fei, Zixiang
   Zhou, Wenju
   Zhou, Huiyu
   Fei, Minrui
TI Low-Light Image Enhancement Using the Cell Vibration Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low light; image enhancement; cell vibration model; guided filtering;
   image fusion
ID QUALITY ASSESSMENT; ALGORITHM; RETINEX; REMOVAL; FUSION
AB Low light very likely leads to the degradation of an image's quality and even causes visual task failures. Existing image enhancement technologies are prone to overenhancement, color distortion or time consumption, and their adaptability is fairly limited. Therefore, we propose a new single low-light image lightness enhancement method. First, an energy model is presented based on the analysis of membrane vibrations induced by photon stimulations. Then, based on the unique mathematical properties of the energy model and combined with the gamma correction model, a new global lightness enhancement model is proposed. Furthermore, a special relationship between image lightness and gamma intensity is found. Finally, a local fusion strategy, including segmentation, filtering and fusion, is proposed to optimize the local details of the global lightness enhancement images. Experimental results show that the proposed algorithm is superior to nine state-of-the-art methods in avoiding color distortion, restoring the textures of dark areas, reproducing natural colors and reducing time cost.
C1 [Lei, Xiaozhou; Zhou, Wenju; Fei, Minrui] Shanghai Univ, Sch Mechatron Engn & Automation, Shanghai Key Lab Power Stn Automation Technol, Shanghai 200444, Peoples R China.
   [Fei, Zixiang] Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
   [Zhou, Huiyu] Univ Leicester, Schoolo Comp & Math Sci f, Leicester LE1 7RH, England.
C3 Shanghai University; Shanghai University; University of Leicester
RP Fei, ZX (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
EM sigmoid@qq.com; zxfei@shu.edu.cn; zhouwenju@shu.edu.cn;
   hz143@leicester.ac.uk; mrfei@staff.shu.edu.cn
RI Zhou, Huiyu/O-2692-2014; guo, yi/KHC-4669-2024; su, lin/KHC-5034-2024
OI Zhou, Huiyu/0000-0003-1634-9840; Lei, Xiaozhou/0000-0003-3458-5919;
   Zhou, wenju/0000-0002-4800-5981; Fei, Zixiang/0000-0003-3692-3467
FU Natural Science Foundation of China [61877065]; Key Project of Science
   and Technology Commission of Shanghai Municipality [19510750300]; 111
   Project [D18003]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61877065, in part by the Key Project of Science and
   Technology Commission of Shanghai Municipality under Grant 19510750300,
   and in part by the 111 Project under Grant D18003.
CR Aamir M, 2019, I COMP CONF WAVELET, P317, DOI [10.1109/ICCWAMTIP47768.2019.9067714, 10.1109/iccwamtip47768.2019.9067714]
   Abdelhamed A., 2018, Smartphone image denoising Dataset(SIDD)
   Abdelhamed A., 2019, P IEEE COMP VIS PATT, P2211
   Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Ahn H, 2013, IEEE ICCE, P153, DOI 10.1109/ICCE.2013.6486837
   Bertalmio Marcelo., 2014, Image Processing for Cinema
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Caselles V, 1999, IEEE T IMAGE PROCESS, V8, P220, DOI 10.1109/83.743856
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Cepeda-Negrete J, 2018, IEEE ACCESS, V6, P14935, DOI 10.1109/ACCESS.2017.2763898
   Chandrasekharan R, 2018, IEEE SIGNAL PROC LET, V25, P813, DOI 10.1109/LSP.2018.2812861
   Chang M, 2022, IEEE T MULTIMEDIA, V24, P702, DOI 10.1109/TMM.2021.3058586
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P418, DOI 10.1137/0806023
   Einstein A., 1905, ANN PHYS-NEW YORK, V17, P1, DOI DOI 10.1002/ANDP.19053220607
   Frankle J. A., 1983, US Patent, Patent No. [4 384 336, 4384336, US, 4384336]
   Fu HY, 2012, INT C PATT RECOG, P3656
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Funt B, 2000, EIGHTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P112
   Gonzalez RC., 2011, DIGITAL IMAGE PROCES
   Gu ZH, 2020, IEEE T IMAGE PROCESS, V29, P3239, DOI 10.1109/TIP.2019.2958144
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   He KM, 2015, Arxiv, DOI [arXiv:1505.00996, DOI 10.48550/ARXIV.1505.00996]
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu YM, 2015, 2015 8TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P119, DOI 10.1109/CISP.2015.7407861
   HUMMEL R, 1977, COMPUT VISION GRAPH, V6, P184, DOI 10.1016/S0146-664X(77)80011-7
   Jiang XS, 2013, IEEE IMAGE PROC, P553, DOI 10.1109/ICIP.2013.6738114
   Kasas S, 2015, P NATL ACAD SCI USA, V112, P378, DOI 10.1073/pnas.1415348112
   Kawasaki K, 2014, 2014 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P33, DOI 10.1109/APCCAS.2014.7032712
   Kim W, 2019, IEEE ACCESS, V7, P129150, DOI 10.1109/ACCESS.2019.2940452
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Kundu D., 2016, ESPL LIVE HDR IMAGE
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P4725, DOI 10.1109/TIP.2017.2713945
   LAND EH, 1964, AM SCI, V52, P247
   LAND EH, 1986, P NATL ACAD SCI USA, V83, P3078, DOI 10.1073/pnas.83.10.3078
   Li CL, 2019, IEEE ACCESS, V7, P163395, DOI 10.1109/ACCESS.2019.2952545
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Lisani JL, 2018, IMAGE PROCESS ON LIN, V8, P408, DOI 10.5201/ipol.2018.236
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Meylan L, 2006, IEEE T IMAGE PROCESS, V15, P2820, DOI 10.1109/TIP.2006.877312
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moroney N, 2000, EIGHTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P108
   Pelling AE, 2004, SCIENCE, V305, P1147, DOI 10.1126/science.1097640
   Reinhard E., 2006, HIGH DYNAMIC RANGE I, DOI 10.1016/B978-012585263-0/50005-1
   Shao Zhenfeng, 2015, Geomatics and Information Science of Wuhan University, V40, P32, DOI 10.13203/j.whugis20130142
   Shapley R, 2011, VISION RES, V51, P701, DOI 10.1016/j.visres.2011.02.012
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Singh H, 2022, IEEE T SYST MAN CY-S, V52, P2275, DOI 10.1109/TSMC.2021.3049402
   Singh H, 2018, COMPUT ELECTR ENG, V70, P462, DOI 10.1016/j.compeleceng.2017.06.029
   Srinivas K, 2020, IET IMAGE PROCESS, V14, P668, DOI 10.1049/iet-ipr.2019.0781
   Tan JL, 2018, 2018 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), P168, DOI 10.1109/ICALIP.2018.8455806
   THOMAS FC, 1994, MATH PROGRAM, V67, P189
   Tianyou Pei, 2019, 2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC), P218, DOI 10.1109/ICIVC47709.2019.8980953
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WC, 2019, INFORM SCIENCES, V496, P25, DOI 10.1016/j.ins.2019.05.015
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Yu SY, 2019, IEEE T CIRC SYST VID, V29, P28, DOI 10.1109/TCSVT.2017.2763180
   Zhang Ya-fei, 2013, Computer Engineering and Design, V34, P1752
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zhu ZQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024335
NR 68
TC 5
Z9 5
U1 3
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4439
EP 4454
DI 10.1109/TMM.2022.3175634
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200026
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Li, L
   Li, Z
   Liu, S
   Li, HQ
AF Li, Li
   Li, Zhu
   Liu, Shan
   Li, Houqiang
TI Frame-Level Rate Control for Geometry-Based LiDAR Point Cloud
   Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit rate; Point cloud compression; Geometry; Laser radar; Parameter
   estimation; Standards; Software algorithms; Bit allocation;
   geometry-based point cloud compression; LiDAR point cloud; rate control;
   rate control model parameter estimation
ID OPTIMAL BIT ALLOCATION; RATE CONTROL ALGORITHM; DELAY RATE CONTROL;
   VIDEO; MODEL
AB The state-of-the-art compression method for Light Detection And Ranging (LiDAR) point clouds is the geometry-based point cloud compression (G-PCC) standard developed by Moving Pictures Experts Group immersive media working group (MPEG-I). However, there are currently no rate control algorithms designed specifically for Geometry-based LiDAR point cloud compression (G-LPCC). In this paper, we propose the first frame-level rate control algorithm for G-LPCC. We mainly have the following contributions in our proposed rate control algorithm. First, we model the rate-distortion (R-D) relationship for both the geometry and attribute. As the geometry bitrate is mainly determined by the frame-level geometry quantizer QG, we propose a relationship between the geometry bitrate and QG. In addition, as the attribute bitrate can be influenced by both the attribute quantizer QA and QG, we build a relationship among the attribute bitrate, QG, and QA. Second, we propose a bit allocation algorithm between the geometry and attribute based on the R-D modeling. The QG and QA are modeled into a proper relationship to obtain geometry and attribute bits to achieve good R-D performance. Third, we propose using the point density of LiDAR point clouds to estimate the geometry model parameters. The point density is calculated using the average distance between each point and its nearest neighbor after excluding some noisy points. The proposed rate control algorithm is implemented in the G-PCC reference software. The experimental results show that the proposed rate control algorithm can control the bitrate accurately with satisfactory R-D performance.
C1 [Li, Li; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geo Spatial Informat Proc & A, Hefei 230027, Peoples R China.
   [Li, Zhu] Univ Missouri, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
   [Liu, Shan] Tencent Amer, 661 Bryant St, Palo Alto, CA 94301 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Missouri System; University of Missouri Kansas
   City
RP Li, L (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geo Spatial Informat Proc & A, Hefei 230027, Peoples R China.
EM lil1@ustc.edu.cn; zhu.li@ieee.org; shanl@tencent.com; lihq@ustc.edu.cn
OI , Shan/0000-0002-1442-1207; Li, Zhu/0000-0002-8246-177X
FU Natural Science Foundation of China [62171429, 1747751, 62021001]; USTC
   Research Funds of the Double First-Class Initiative [YD3490002001];
   Fundamental Research Funds for the Central Universities [WK3490000006]
FX This work was supported in part by the Natural Science Foundation of
   China underGrants 62171429, 1747751, and 62021001, in part by the USTC
   Research Funds of the Double First-Class Initiative under Grant
   YD3490002001, and in part by the Fundamental Research Funds for the
   Central Universities under Grant WK3490000006.
CR Bjontegaard G, 2001, VCEGM33
   BROSS B, 2018, WORKING DRAFT 3 OF V
   Cai Q, 2020, IEEE T CIRC SYST VID, V30, P2215, DOI 10.1109/TCSVT.2019.2914100
   Chen ZZ, 2019, IEEE T IMAGE PROCESS, V28, P4541, DOI 10.1109/TIP.2019.2911180
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   Ferguson KL, 2009, IEEE T CIRC SYST VID, V19, P1057, DOI 10.1109/TCSVT.2009.2020339
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Gao YB, 2019, IEEE T CIRC SYST VID, V29, P546, DOI 10.1109/TCSVT.2017.2787190
   Guo HW, 2020, IEEE T BROADCAST, V66, P113, DOI 10.1109/TBC.2019.2917402
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P840, DOI 10.1109/TCSVT.2002.804883
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P928, DOI 10.1109/76.937431
   Hou JH, 2015, IEEE T CIRC SYST VID, V25, P51, DOI 10.1109/TCSVT.2014.2329376
   Hu SD, 2011, IEEE T CIRC SYST VID, V21, P1152, DOI 10.1109/TCSVT.2011.2138810
   Jiang MQ, 2006, IEEE T MULTIMEDIA, V8, P467, DOI 10.1109/TMM.2006.870713
   Jiang MQ, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1055, DOI 10.1109/ICME.2004.1394389
   KARCZEWICZ M, 2013, INTRA FRAME RATE CON
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LI B, 2012, RATE CONTROL BY RLAM
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P6237, DOI 10.1109/TIP.2020.2989576
   Li L, 2018, IEEE T CIRC SYST VID, V28, P130, DOI 10.1109/TCSVT.2016.2598672
   Li L, 2016, IEEE T MULTIMEDIA, V18, P2023, DOI 10.1109/TMM.2016.2595264
   Li SX, 2017, IEEE T CIRC SYST VID, V27, P2409, DOI 10.1109/TCSVT.2016.2589878
   LI Y, 2022, V2X SIM A VIRTUAL CO
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Liu Q, 2018, ASIAPAC SIGN INFO PR, P1981, DOI 10.23919/APSIPA.2018.8659653
   Liu QL, 2020, IEEE INT CONF COMMUN, P1, DOI [10.1109/iccc49849.2020.9238836, 10.1109/ICCC49849.2020.9238836, 10.1109/IPDPSW50202.2020.00177]
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   MAMMOU K, 2018, JTC1SC29WG11M42640 I
   Qi Liu, 2021, IEEE Transactions on Multimedia, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Schnabel R., 2006, P S POINT BAS GRAPH, V6, P111, DOI DOI 10.2312/SPBG/SPBG06/111-120
   Schwarz H, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1929, DOI 10.1109/ICME.2006.262934
   Schwarz S., 2018, ISO/IEC JTC1/SC29/WG11
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Seo CW, 2010, IEEE T CIRC SYST VID, V20, P1210, DOI 10.1109/TCSVT.2010.2057011
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tianwu Yang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P85, DOI 10.1109/ICME.2012.171
   TULVAN C, 2016, JTC1SC29WG11 MPEG201
   Wang SS, 2013, IEEE J-STSP, V7, P1101, DOI 10.1109/JSTSP.2013.2272240
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei J, 2004, IEEE SIGNAL PROC LET, V11, P694, DOI 10.1109/LSP.2004.831671
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang Qi, 2022, IEEE Trans Pattern Anal Mach Intell, V44, P3015, DOI 10.1109/TPAMI.2020.3047083
   Yongming Li, 2017, 2017 IEEE 19th International Conference on e-Health Networking, Applications and Services (Healthcom), P1, DOI 10.1109/HealthCom.2017.8210820
   Zhou SM, 2007, IEEE T CIRC SYST VID, V17, P996, DOI 10.1109/TCSVT.2007.903123
   2022, LIGHT DETECTION AND
   2022, GEOMETRY BASED POINT
NR 48
TC 5
Z9 5
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3855
EP 3867
DI 10.1109/TMM.2022.3167810
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500024
DA 2024-07-18
ER

PT J
AU Lin, CY
   Rong, XW
   Yu, XY
AF Lin, Cunyi
   Rong, Xianwei
   Yu, Xiaoyan
TI MSAFF-Net: Multiscale Attention Feature Fusion Networks for Single Image
   Dehazing and Beyond
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; deep neural network; feature fusion; multiscale
   convolutions; single image dehazing
ID VISION; FRAMEWORK
AB Single image dehazing is a critical problem in computer vision. However, most recently proposed learning-based dehazing methods achieve unsatisfactory quality with dehazed images due to inaccurate parametric estimation. The size of these models is also large to be applied with mobile devices' limited resources. Last, most models are tailored to image dehazing, achieving poor migration. Thus, we propose a compact multiscale attention feature fusion network with a model size of 2 MB called MSAFF-Net to perform end-to-end single image dehazing. In the proposed model, we design a simple and powerful feature extraction module to extract complex features from hazy images. We use a channel attention module and a multiscale spatial attention module to consider the regions with haze-relevant features. To our knowledge, this study is the first to directly apply the attention mechanism rather than to embed it into certain modules for single image dehazing. We compare MSAFF-Net with other approaches on the NTIRE18, RESIDE, and Middlebury Stereo datasets. We show that MSAFF-Net achieves comparable or better performance than other models. We also extend MSAFF-Net to single image deraining, and various experiments demonstrate its effectiveness. Results suggest that MSAFF-Net can directly restore clear images using channels with the most useful haze- or rain-relevant features and spatial locations.
C1 [Lin, Cunyi; Rong, Xianwei; Yu, Xiaoyan] Harbin Normal Univ, Sch Phys & Elect Engn, Harbin 150025, Peoples R China.
C3 Harbin Normal University
RP Yu, XY (corresponding author), Harbin Normal Univ, Sch Phys & Elect Engn, Harbin 150025, Peoples R China.
EM 2323591525@qq.com; rongxianwei@hrbnu.edu.cn; yuxiaoyan@hrbnu.edu.cn
FU National Natural Science Foundation of China [61401127]; Natural Science
   Foundation of Heilongjiang Province [F2018022]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61401127 and in part by the Natural
   Science Foundation of Heilongjiang Province under Grant F2018022.& nbsp;
CR Ancuti C, 2018, P IEEE C COMP VIS PA, P891
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cartney E.J., 1969, OPTICS ATMOSPHERE SC
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Du YX, 2019, IEEE COMPUT SOC CONF, P1824, DOI 10.1109/CVPRW.2019.00233
   Dudhane A, 2020, IEEE T IMAGE PROCESS, V29, P628, DOI 10.1109/TIP.2019.2934360
   Fazlali H, 2020, MULTIMED TOOLS APPL, V79, P29493, DOI 10.1007/s11042-020-09383-7
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Gao YY, 2019, IEEE T MULTIMEDIA, V21, P351, DOI 10.1109/TMM.2018.2856095
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gu ZQ, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11243008
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He L., 2020, IEEE ACCESS, V7
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jaderberg M, 2015, ADV NEUR IN, V28
   Lei Ba J., 2016, arXiv
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li C., 2017, arXiv
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Schechner YY, 2003, APPL OPTICS, V42, P511, DOI 10.1364/AO.42.000511
   Sharma T, 2020, MULTIMED TOOLS APPL, V79, P30769, DOI 10.1007/s11042-020-09496-z
   Shen LH, 2019, IEEE T MULTIMEDIA, V21, P1093, DOI 10.1109/TMM.2018.2871955
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sim H, 2018, IEEE COMPUT SOC CONF, P1025, DOI 10.1109/CVPRW.2018.00136
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Wang AN, 2019, IEEE T IMAGE PROCESS, V28, P381, DOI 10.1109/TIP.2018.2868567
   Wang CS, 2020, IEEE ACCESS, V8, P9488, DOI 10.1109/ACCESS.2020.2964271
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang YJ, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS & SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P994
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu L, 2019, IEEE ACCESS, V7, P125000, DOI 10.1109/ACCESS.2019.2925900
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yi QS, 2022, IEEE T MULTIMEDIA, V24, P3114, DOI 10.1109/TMM.2021.3093724
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P1975, DOI 10.1109/TCSVT.2019.2912145
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhu HY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1234
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 54
TC 23
Z9 23
U1 34
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3089
EP 3100
DI 10.1109/TMM.2022.3155937
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200011
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Sheng, XH
   Li, JH
   Li, B
   Li, L
   Liu, D
   Lu, Y
AF Sheng, Xihua
   Li, Jiahao
   Li, Bin
   Li, Li
   Liu, Dong
   Lu, Yan
TI Temporal Context Mining for Learned Video Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video compression; Encoding; Video codecs; Entropy; Decoding; Image
   coding; Software; Deep neural network; end-to-end compression; learned
   video compression; temporal context mining; temporal context re-filling
AB Applying deep learning to video compression has attracted increasing attention in recent few years. In this work, we address end-to-end learned video compression with a special focus on better learning and utilizing temporal contexts. We propose to propagate not only the last reconstructed frame but also the feature before obtaining the reconstructed frame for temporal context mining. From the propagated feature, we learn multi-scale temporal contexts and re-fill the learned temporal contexts into the modules of our compression scheme, including the contextual encoder-decoder, the frame generator, and the temporal context encoder. We discard the parallelization-unfriendly auto-regressive entropy model to pursue a more practical encoding and decoding time. Experimental results show that our proposed scheme achieves a higher compression ratio than the existing learned video codecs. Our scheme also outperforms x264 and x265 (representing industrial software for H.264 and H.265, respectively) as well as the official reference software for H.264, H.265, and H.266 (JM, HM, and VTM, respectively). Specifically, when intra period is 32 and oriented to PSNR, our scheme outperforms H.265-HM by 14.4% bit rate saving; when oriented to MS-SSIM, our scheme outperforms H.266-VTM by 21.1% bit rate saving.
C1 [Sheng, Xihua; Li, Jiahao; Li, Bin; Lu, Yan] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Sheng, Xihua; Li, Li; Liu, Dong] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Anhui, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Li, B (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM xhsheng@mail.ustc.edu.cn; li.jiahao@microsoft.com; libin@microsoft.com;
   lil1@ustc.edu.cn; dongeliu@ustc.edu.cn; yanlu@microsoft.com
RI Liu, Dong/K-7488-2012
OI Liu, Dong/0000-0001-9100-2906; Li, Jiahao/0000-0003-4559-0086
CR Agustsson E., 2020, P IEEE CVF C COMP VI, P8503
   [Anonymous], 2015, JM-19.0
   [Anonymous], 2003, x264
   [Anonymous], 2018, HM-16.20
   [Anonymous], 2013, x265
   [Anonymous], 2021, VTM-13.2
   Balle J, 2018, ICLR
   Begaint J., 2020, ARXIV
   Bjontegaard G, 2001, VCEGM33
   Chen ZB, 2020, IEEE T CIRC SYST VID, V30, P566, DOI 10.1109/TCSVT.2019.2892608
   Cheng ZX, 2019, PROC CVPR IEEE, P10063, DOI 10.1109/CVPR.2019.01031
   Ding WP, 2007, IEEE T IMAGE PROCESS, V16, P416, DOI 10.1109/TIP.2006.888341
   Djelouah A, 2019, IEEE I CONF COMP VIS, P6430, DOI 10.1109/ICCV.2019.00652
   Feng RS, 2020, IEEE COMPUT SOC CONF, P529, DOI 10.1109/CVPRW50498.2020.00068
   Flynn D., 2013, JCT-VC Document, JCTVC-N1006
   Guo Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P456, DOI 10.1007/978-3-030-58536-5_27
   Guo ZY, 2021, Arxiv, DOI arXiv:2112.13309
   Guo ZY, 2022, IEEE T CIRC SYST VID, V32, P2329, DOI 10.1109/TCSVT.2021.3089491
   Habibian A, 2019, IEEE I CONF COMP VIS, P7032, DOI 10.1109/ICCV.2019.00713
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu ZH, 2021, PROC CVPR IEEE, P1502, DOI 10.1109/CVPR46437.2021.00155
   Katsenou AV, 2021, IEEE T MULTIMEDIA, V23, P26, DOI 10.1109/TMM.2020.2976591
   Li J., 2021, Advances in Neural Information Processing Systems, V34, P18114
   Lin JP, 2020, PROC CVPR IEEE, P3543, DOI 10.1109/CVPR42600.2020.00360
   Liu BW, 2021, PROC CVPR IEEE, P701, DOI 10.1109/CVPR46437.2021.00076
   Liu HJ, 2021, IEEE T CIRC SYST VID, V31, P3182, DOI 10.1109/TCSVT.2020.3035680
   Liu HJ, 2020, AAAI CONF ARTIF INTE, V34, P11580
   Liu Jerry, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P453, DOI 10.1007/978-3-030-58520-4_27
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Lu G, 2021, IEEE T PATTERN ANAL, V43, P3292, DOI 10.1109/TPAMI.2020.2988453
   Ma D, 2022, IEEE T MULTIMEDIA, V24, P3847, DOI 10.1109/TMM.2021.3108943
   Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Mentzer F, 2022, LECT NOTES COMPUT SC, V13686, P562, DOI 10.1007/978-3-031-19809-0_32
   Mercat A, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P297, DOI 10.1145/3339825.3394937
   Minnen D, 2018, ADV NEUR IN, V31
   Pessoa J, 2020, IEEE WRK SIG PRO SYS, P276, DOI 10.1109/sips50750.2020.9195249
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Rippel O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14459, DOI 10.1109/ICCV48922.2021.01421
   Rippel O, 2019, IEEE I CONF COMP VIS, P3453, DOI 10.1109/ICCV.2019.00355
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang J., 2020, P EUR C COMP VIS ECC, P405
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wenyu Sun, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P239, DOI 10.1007/978-3-030-58577-8_15
   Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang R., 2022, P 31 INT JOINT C ART, P1537
   Yang R., 2021, P 9 INT C LEARN REPR
   Yang R, 2021, IEEE J-STSP, V15, P388, DOI 10.1109/JSTSP.2020.3043590
   Yang R, 2020, PROC CVPR IEEE, P6627, DOI 10.1109/CVPR42600.2020.00666
   Yilmaz MA, 2022, IEEE T IMAGE PROCESS, V31, P974, DOI 10.1109/TIP.2021.3138300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhihao Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P193, DOI 10.1007/978-3-030-58536-5_12
NR 56
TC 15
Z9 16
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7311
EP 7322
DI 10.1109/TMM.2022.3220421
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000044
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Verde, S
   Pasquini, C
   Lago, F
   Goller, A
   De Natale, F
   Piva, A
   Boato, G
AF Verde, Sebastiano
   Pasquini, Cecilia
   Lago, Federica
   Goller, Alessandro
   De Natale, Francesco
   Piva, Alessandro
   Boato, Giulia
TI Multi-Clue Reconstruction of Sharing Chains for Social Media Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Provenance analysis; social media forensics
ID CLASSIFICATION; RECOGNITION
AB The amount of multimedia content shared everyday, combined with the level of realism reached by recent fake-generating technologies, threatens to impair the trustworthiness of online information sources. The process of uploading and sharing data tends to hinder standard media forensic analyses, since multiple re-sharing steps progressively hide the traces of past manipulations. At the same time though, new traces are introduced by the platforms themselves, enabling the reconstruction of the sharing history of digital objects, with possible applications in information flow monitoring and source identification. In this work, we propose a supervised framework for the reconstruction of image sharing chains on social media platforms. The system is structured as a cascade of backtracking blocks, each of them tracing back one step of the sharing chain at a time. Blocks are designed as ensembles of classifiers trained to analyse the input image independently from one another by leveraging different feature representations that describe both content and container of the media object. Individual decisions are then properly combined by a late fusion strategy. Results highlight the advantages of employing multiple clues, which allow accurately tracing back up to three steps along the sharing chain.
C1 [Verde, Sebastiano; Pasquini, Cecilia; Lago, Federica; Goller, Alessandro; De Natale, Francesco; Boato, Giulia] Univ Trento, Dipartimento Ingn & Sci Informaz, I-38123 Trento, Trentino Alto A, Italy.
   [De Natale, Francesco; Boato, Giulia] Consorzio Nazl Interuniv Telecomunicazioni, I-38123 Trento, Italy.
   [Piva, Alessandro] Univ Florence, Dipartimento Ingn Informaz, I-50139 Florence, Tuscany, Italy.
   [Piva, Alessandro] Consorzio Nazl Interuniv Telecomunicazioni, I-50139 Trento, Italy.
C3 University of Trento; University of Florence
RP Verde, S (corresponding author), Univ Trento, Dipartimento Ingn & Sci Informaz, I-38123 Trento, Trentino Alto A, Italy.
EM sebastiano.verde@unitn.it; cecilia.pasquini@unitn.it;
   federica.lago-1@unitn.it; alessandro.goller@studenti.unitn.it;
   francesco.denatale@unitn.it; alessandro.piva@unifi.it;
   giulia.boato@unitn.it
RI ; Piva, Alessandro/B-8948-2008
OI Pasquini, Cecilia/0000-0002-2125-6983; Piva,
   Alessandro/0000-0002-3047-0519; De Natale, Francesco/0000-0003-2566-6995
FU Defense Advanced Research Projects Agency
FX No Statement Available
CR Allen G., 2017, ARTIF INTELL, P47
   Amerini I., 2017, 2017 IEEE WORKSH INF, P1
   Amerini I, 2019, IEEE ACCESS, V7, P35264, DOI 10.1109/ACCESS.2019.2903876
   Caldelli R, 2018, EUR SIGNAL PR CONF, P1357, DOI 10.23919/EUSIPCO.2018.8553160
   Caldelli R, 2017, IEEE T INF FOREN SEC, V12, P1299, DOI 10.1109/TIFS.2017.2656842
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Dang-Nguyen D.T., 2015, P ACM MULT SYST C, P219
   Giudice O, 2017, LECT NOTES COMPUT SC, V10485, P625, DOI 10.1007/978-3-319-68548-9_57
   Harvey P., 2021, Exiftool
   Ho TK, 2002, PATTERN ANAL APPL, V5, P102, DOI 10.1007/s100440200009
   Huang Y. S., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P347, DOI 10.1109/CVPR.1993.1626170
   HUANG YS, 1995, IEEE T PATTERN ANAL, V17, P90, DOI 10.1109/34.368145
   Kee E, 2011, IEEE T INF FOREN SEC, V6, P1066, DOI 10.1109/TIFS.2011.2128309
   Mada BE, 2019, IEEE T MULTIMEDIA, V21, P603, DOI 10.1109/TMM.2019.2893548
   Mazumdar A, 2019, LECT NOTES COMPUT SC, V11941, P226, DOI 10.1007/978-3-030-34869-4_25
   Moltisanti M, 2015, LECT NOTES COMPUT SC, V9280, P506, DOI 10.1007/978-3-319-23234-8_47
   Moreno-Seco F, 2006, LECT NOTES COMPUT SC, V4109, P705
   Mullan P, 2019, DIGIT INVEST, V28, pS68, DOI 10.1016/j.diin.2019.01.016
   Ngo M., 2021, IEEE Trans. Multimedia, V24, P377
   Pasquini C., 2016, P 4 ACM WORKSH INF H, P11, DOI DOI 10.1145/2909827.2930787
   Pasquini C, 2021, EURASIP J INF SECUR, V2021, DOI 10.1186/s13635-021-00117-2
   Pevny T, 2008, IEEE T INF FOREN SEC, V3, P247, DOI 10.1109/TIFS.2008.922456
   Phan Q., 2018, PROC IEEE INT WORKSH, P1
   Ponti M. P.  Jr., 2011, 2011 24th SIBGRAPI Conference on Graphics, Patterns and Images: Tutorials, P1, DOI 10.1109/SIBGRAPI-T.2011.9
   Phan QT, 2019, INT CONF ACOUST SPEE, P8266, DOI 10.1109/ICASSP.2019.8683144
   Raudys A, 2003, LECT NOTES COMPUT SC, V2709, P55
   Ruta D., 2000, Computing and Information Systems, V7, P1
   Shullani D, 2017, EURASIP J INF SECUR, DOI 10.1186/s13635-017-0067-2
   Siddiqui N, 2019, 2019 FIFTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP 2019), P97, DOI 10.1109/ICIIP47207.2019.8985933
   Smith, 2019, 126 AMAZING SOCIAL M
   Statista Research Department, 2021, Hours of video uploaded to youtube every minute as of may 2019
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Verdoliva L, 2020, IEEE J-STSP, V14, P910, DOI 10.1109/JSTSP.2020.3002101
   Yang PP, 2020, IEEE J-STSP, V14, P947, DOI 10.1109/JSTSP.2020.3008088
NR 34
TC 4
Z9 4
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9491
EP 9505
DI 10.1109/TMM.2023.3253389
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, WB
   Pagnucco, M
   Xu, CP
   Song, Y
AF Wang, Wenbin
   Pagnucco, Maurice
   Xu, Chengpei
   Song, Yang
TI InterREC: An Interpretable Method for Referring Expression Comprehension
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognition; Visualization; Task analysis; Transformers; Feature
   extraction; Linguistics; Representation learning; Referring expression
   comprehension; transfer learning; Bayesian network; reasoning
AB Referring Expression Comprehension (REC) aims to locate the target object in the image according to a referring expression. This is a challenging task owing to the need for understanding both natural language and visual information and interpretable reasoning between them. Most existing implicit reasoning-based REC methods lack interpretability, while explicit reasoning-based REC methods have lower accuracy. To achieve competitive accuracy while providing adequate interpretability, in this work, we propose a novel explicit reasoning-based method named InterREC. First, in order to address the challenge of multi-modal understanding, we design two neural network modules based on text-image representation learning: a Text-Region Matching Module to align objects in the image and noun phrases in the expression, and a Text-Relation Matching Module to align relations between objects in the image and relational phrases in the expression. Additionally, we design a Reasoning Order Tree for handling complex expressions, which can reduce complex expressions to multiple object-relation-object triplets and therefore identify the inference order and reduce the difficulty of reasoning. At the same time, to achieve an interpretable reasoning step, we design a Bayesian Network-based explicit reasoning method. Based on the comparative evaluation on various datasets, our method achieves higher accuracy than existing explicit reasoning-based REC methods, and the visualization results demonstrate the method's high interpretability.
C1 [Wang, Wenbin; Pagnucco, Maurice; Song, Yang] Univ New South Wales, Sch Comp Sci & Engn, Kensington, NSW 2033, Australia.
   [Xu, Chengpei] Univ New South Wales, Sch Minerals & Energy Resources Engn, Kensington, NSW 2033, Australia.
C3 University of New South Wales Sydney; University of New South Wales
   Sydney
RP Song, Y (corresponding author), Univ New South Wales, Sch Comp Sci & Engn, Kensington, NSW 2033, Australia.
EM wenbin.wang@unsw.edu.au; morri@cse.unsw.edu.au; Chengpei.Xu@unsw.edu.au;
   yang.song1@unsw.edu.au
RI Pagnucco, Maurice/AAY-1262-2020; Song, Yangyi/JBJ-7119-2023
OI Pagnucco, Maurice/0000-0001-7712-6646; Song, Yangyi/0000-0002-3649-014X;
   WANG, WENBIN/0000-0001-9710-0136; Song, Yang/0000-0003-1283-1672
CR Chen J, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3143
   Cho J., 2021, PMLR, P1931
   Cho J, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P8785
   Cirik Volkan, 2018, P 2018 C N AM CHAPT, V2, P781, DOI DOI 10.18653/V1/N18-2123
   Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Fan FL, 2021, IEEE T RADIAT PLASMA, V5, P741, DOI 10.1109/TRPMS.2021.3066428
   Gan Zhe, 2020, NEURIPS
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong RC, 2022, IEEE T PATTERN ANAL, V44, P684, DOI 10.1109/TPAMI.2019.2911066
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Huang BB, 2021, PROC CVPR IEEE, P16883, DOI 10.1109/CVPR46437.2021.01661
   Huang ZC, 2020, Arxiv, DOI arXiv:2004.00849
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Jiang H., 2020, IEEE CVF C COMP VIS, DOI 10.1109/CVPR42600.2020.01028
   Jing C., 2022, P C AAAI ART INT, V2
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li JH, 2021, ADV NEUR IN, V34
   Li ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4395, DOI 10.1145/3503161.3548341
   Liao Y., 2020, P IEEE CVF C COMP VI, P10877, DOI 10.24963/ijcai.2018/155
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Liu GY, 2021, IEEE T NEUR NET LEAR, V32, P3786, DOI 10.1109/TNNLS.2021.3099165
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Liu XJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P539, DOI 10.1145/3343031.3351074
   Liu YF, 2020, AAAI CONF ARTIF INTE, V34, P11645
   Lu JS, 2019, ADV NEUR IN, V32
   Luo G, 2022, IEEE T IMAGE PROCESS, V31, P3386, DOI 10.1109/TIP.2021.3139234
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Mihaljevic B, 2021, NEUROCOMPUTING, V456, P648, DOI 10.1016/j.neucom.2021.01.138
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Pearl J., 1988, PROBABILISTIC REASON
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Reimers N, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P567
   Schuster S., 2015, P 4 WORKSHOP VISION, P70
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Sibei Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9949, DOI 10.1109/CVPR42600.2020.00997
   Su Weijie, 2020, INT C LEARN REPR
   Subramanian S, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P5198
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang P., 2022, P INT C MACH LEARN, p23 318
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Xie YW, 2021, MMPT '21: PROCEEDINGS OF THE 2021 WORKSHOP ON MULTI-MODAL PRE-TRAINING FOR MULTIMEDIA UNDERSTANDING, P14, DOI 10.1145/3463945.3469055
   Xiong DH, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12020216
   Yang L, 2022, PROC CVPR IEEE, P9489, DOI 10.1109/CVPR52688.2022.00928
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P3208
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhu CY, 2022, LECT NOTES COMPUT SC, V13695, P598, DOI 10.1007/978-3-031-19833-5_35
   Zhu HD, 2021, IEEE WINT CONF APPL, P2209, DOI 10.1109/WACV48630.2021.00226
   Zhu JL, 2021, IEEE T IND INFORM, V17, P596, DOI 10.1109/TII.2020.2964154
   Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447
NR 65
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9330
EP 9342
DI 10.1109/TMM.2023.3251111
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200024
DA 2024-07-18
ER

PT J
AU Wang, X
   Shu, XJ
   Zhang, SL
   Jiang, B
   Wang, YW
   Tian, YH
   Wu, F
AF Wang, Xiao
   Shu, Xiujun
   Zhang, Shiliang
   Jiang, Bo
   Wang, Yaowei
   Tian, Yonghong
   Wu, Feng
TI MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Tracking; Heuristic algorithms; Visualization; Task
   analysis; Kernel; Vehicle dynamics; RGB-T tracking; dynamic filter
   generation; multi-modal fusion; local and global search; deep learning
ID VIDEO SALIENCY DETECTION; FUSION TRACKING; INFRARED IMAGES; OBJECT
   TRACKING; NETWORK
AB Many RGB-T trackers attempt to attain robust feature representation by utilizing an adaptive weighting scheme (or attention mechanism). Different from these works, we propose a new dynamic modality-aware filter generation module (named MFGNet) to boost the message communication between visible and thermal data by adaptively adjusting the convolutional kernels for various input images in practical tracking. Given the image pairs as input, we first encode their features with the backbone network. Then, we concatenate these feature maps and generate dynamic modality-aware filters with two independent networks. The visible and thermal filters will be used to conduct a dynamic convolutional operation on their corresponding input feature maps respectively. Inspired by residual connection, both the generated visible and thermal feature maps will be summarized with input feature maps. The augmented feature maps will be fed into the RoI align module to generate instance-level features for subsequent classification. To address issues caused by heavy occlusion, fast motion and out-of-view, we propose to conduct a joint local and global search by exploiting a new direction-aware target driven attention mechanism. The spatial and temporal recurrent neural network is used to capture the direction-aware context for accurate global attention prediction. Extensive experiments on three large-scale RGB-T tracking benchmark datasets validated the effectiveness of our proposed algorithm.
C1 [Wang, Xiao; Shu, Xiujun; Zhang, Shiliang; Wang, Yaowei] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Zhang, Shiliang] Peking Univ, Peng Cheng Lab, Beijing 100871, Peoples R China.
   [Jiang, Bo] Anhui Univ, Sch Comp Sci & Technol, Hefei 230093, Peoples R China.
   [Tian, Yonghong] Peking Univ, Beijing 100871, Peoples R China.
   [Wu, Feng] Univ Sci & Technol China, Sch Informat Sci, Hefei 230026, Peoples R China.
C3 Peng Cheng Laboratory; Peking University; Anhui University; Peking
   University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Wang, YW (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM wangxiaocvpr@foxmail.com; shuxj@pcl.ac.cn; slzhang.jdl@pku.edu.cn;
   jiangbo@ahu.edu.cn; yaoweiwang@pku.edu.cn; yhtian@pku.edu.cn;
   fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024
FU Peng Cheng Laboratory Research Project [PCL2021A07]; National Natural
   Science Foundation of China [61825101, 62027804, 62076004, 62102205,
   U20B2052]; Natural Science Foundation of Anhui Province [2108085Y23];
   Postdoctoral Innovative Talent Support Program [BX20200174]; China
   Postdoctoral Science Foundation [2020M682828]; Peng Cheng Laboratory
   Research Project [PCL2021A07]; National Natural Science Foundation of
   China [61825101, 62027804, 62076004, 62102205, U20B2052]; Natural
   Science Foundation of Anhui Province [2108085Y23]; Postdoctoral
   Innovative Talent Support Program [BX20200174]; China Postdoctoral
   Science Foundation [2020M682828]
FX This work was supported in part by the Peng Cheng Laboratory Research
   Project under Grant PCL2021A07, in part by the National Natural Science
   Foundation of China under Grants 61825101, 62027804, 62076004, 62102205,
   and U20B2052, in part by the Natural Science Foundation of Anhui
   Province under Grant 2108085Y23, in part by the Postdoctoral Innovative
   Talent Support Program under Grant BX20200174, and in part by the China
   Postdoctoral Science Foundation Funded Project under Grant 2020M682828.
CR [Anonymous], 2017, IEEE INT S CIRC SYST
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2016, PATTERN RECOGN, V52, P410, DOI 10.1016/j.patcog.2015.09.033
   Chen CLZ, 2015, PATTERN RECOGN, V48, P2885, DOI 10.1016/j.patcog.2015.01.025
   Chen G., 2011, P 14 INT C INF FUS, P1
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513
   Cui Z, 2016, PROC CVPR IEEE, P1449, DOI 10.1109/CVPR.2016.161
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   De Brabandere B, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diba A, 2019, IEEE I CONF COMP VIS, P6191, DOI 10.1109/ICCV.2019.00629
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gan Q, 2015, Arxiv, DOI arXiv:1511.06425
   Gao Y, 2019, IEEE INT CONF COMP V, P91, DOI 10.1109/ICCVW.2019.00017
   Gong JJ, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1576
   Han YZ, 2022, IEEE T PATTERN ANAL, V44, P7436, DOI 10.1109/TPAMI.2021.3117837
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jianbo Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P769, DOI 10.1007/978-3-030-58595-2_46
   Jung I., 2018, P ECCV, P83
   Kang D., 2017, Advances in Neural Information Processing Systems, P3867
   Kim HU, 2015, IEEE I CONF COMP VIS, P3011, DOI 10.1109/ICCV.2015.345
   Li CL, 2019, IEEE INT CONF COMP V, P2262, DOI 10.1109/ICCVW.2019.00279
   Li CL, 2019, IEEE T CIRC SYST VID, V29, P2913, DOI 10.1109/TCSVT.2018.2874312
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49
   Li CL, 2018, SIGNAL PROCESS-IMAGE, V68, P207, DOI 10.1016/j.image.2018.08.004
   Li CL, 2018, NEUROCOMPUTING, V281, P78, DOI 10.1016/j.neucom.2017.11.068
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Li YX, 2021, IEEE T CIRC SYST VID, V31, P2315, DOI 10.1109/TCSVT.2020.3023080
   Liu C, 2018, IEEE T MULTIMEDIA, V20, P889, DOI 10.1109/TMM.2017.2760633
   Liu FH, 2017, IEEE T MULTIMEDIA, V19, P2680, DOI 10.1109/TMM.2017.2708424
   Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Lukezic A, 2019, IEEE I CONF COMP VIS, P10012, DOI 10.1109/ICCV.2019.01011
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Marvasti-Zadeh SM, 2022, IEEE T INTELL TRANSP, V23, P3943, DOI 10.1109/TITS.2020.3046478
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ondruska P, 2016, Arxiv, DOI arXiv:1604.05091
   Ondrúska P, 2016, AAAI CONF ARTIF INTE, P3361
   Park E, 2018, LECT NOTES COMPUT SC, V11207, P587, DOI 10.1007/978-3-030-01219-9_35
   Paszke A, 2019, ADV NEUR IN, V32
   Pu S, 2018, Arxiv, DOI arXiv:1810.03851
   Rahman MM, 2020, IEEE ACCESS, V8, P100857, DOI 10.1109/ACCESS.2020.2997917
   Shen DH, 2018, Arxiv, DOI arXiv:1709.08294
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shen W., 2018, arXiv
   Shi XJ, 2015, ADV NEUR IN, V28
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang CQ, 2020, PROC CVPR IEEE, P7062, DOI 10.1109/CVPR42600.2020.00709
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang X., 2019, P 30 BRIT MACH VIS C, P131
   Wang X, 2023, IEEE Transactions on Cybernetics
   Wang X, 2018, Arxiv, DOI arXiv:1811.10014
   Wang X, 2021, IEEE T CIRC SYST VID, V31, P4895, DOI 10.1109/TCSVT.2021.3056684
   Wang X, 2021, PROC CVPR IEEE, P13758, DOI 10.1109/CVPR46437.2021.01355
   Wang X, 2022, IEEE T NEUR NET LEAR, V33, P6931, DOI 10.1109/TNNLS.2021.3083933
   Wang X, 2019, SIGNAL PROCESS-IMAGE, V75, P158, DOI 10.1016/j.image.2019.03.012
   Wang X, 2018, PROC CVPR IEEE, P4864, DOI 10.1109/CVPR.2018.00511
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JL, 2018, LECT NOTES COMPUT SC, V11214, P188, DOI 10.1007/978-3-030-01249-6_12
   Yang K, 2020, KNOWL-BASED SYST, V203, DOI 10.1016/j.knosys.2020.106079
   Yang R., 2019, 2019 IEEE INT C IMAG, P3975, DOI DOI 10.1109/ICIP.2019.8803528
   Yang R, 2021, NEUROCOMPUTING, V462, P365, DOI 10.1016/j.neucom.2021.08.012
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Yang TY, 2017, IEEE INT CONF COMP V, P2010, DOI 10.1109/ICCVW.2017.235
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Ye LW, 2020, IEEE T MULTIMEDIA, V22, P3224, DOI 10.1109/TMM.2020.2971171
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Yu-Syuan Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12493, DOI 10.1109/CVPR42600.2020.01251
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zhang LC, 2019, IEEE INT CONF COMP V, P2252, DOI 10.1109/ICCVW.2019.00278
   Zhang PY, 2020, Arxiv, DOI arXiv:2012.04176
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang XC, 2020, INFORM FUSION, V63, P166, DOI 10.1016/j.inffus.2020.05.002
   Zhang ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13319, DOI 10.1109/ICCV48922.2021.01309
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhong BN, 2019, IEEE T IMAGE PROCESS, V28, P2331, DOI 10.1109/TIP.2018.2885238
   Zhou QQ, 2019, IEEE T MULTIMEDIA, V21, P1183, DOI 10.1109/TMM.2018.2875360
   Zhu YB, 2022, Arxiv, DOI arXiv:2202.05659
   Zhu YB, 2021, IEEE T INTELL VEHICL, V6, P121, DOI 10.1109/TIV.2020.2980735
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
NR 99
TC 16
Z9 17
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4335
EP 4348
DI 10.1109/TMM.2022.3174341
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, KJ
   Yang, Y
   Liu, Q
   Zhang, XP
AF Wu, Kejun
   Yang, You
   Liu, Qiong
   Zhang, Xiao-Ping
TI Focal Stack Image Compression Based on Basis-Quadtree Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Encoding; Redundancy; Predictive models; Optimization;
   Visualization; Three-dimensional displays; Focal stack images; image
   representation; image compression; video coding
ID MULTI-FOCUS IMAGE
AB In this paper, we propose an efficient compression scheme for focal stack images (FoSIs) based on a new basis-quadtree representation. In the new basis-quadtree representation, FoSIs are initially reorganized as co-located block groups in the depth dimension. In each group, selective basis blocks and adaptive quadtree partition are optimized to predict the focused or defocused co-located blocks by intra-group approximation. By solving a joint optimization problem, FoSIs can be efficiently represented by the optimal basis blocks, corresponding quadtree partition and approximation parameters, which will be compressed separately. Then, these basis blocks are stitched into several new frames (basis frames) according to their original locations and partition modes. Basis frames are compressed by our designed encoder, where the intra-group approximation is embedded into the high efficiency video coding (HEVC) encoder. Thus, the redundancies of basis blocks can be further eliminated. Finally, the approximation parameters are refined to suppress the amplified errors caused by introduced compression blur after basis frame coding. The refined parameters are compressed losslessly and multiplexed with the bitstream of the basis frames to ensure the reconstruction quality of FoSIs. Experiments on 12 test sequences demonstrate that the proposed scheme can obtain higher coding performance than the state-of-the-art comparison schemes. Specifically, the proposed scheme achieves up to 5.23 dB PSNR gains and 71.59% bitrate savings over the HEVC baseline scheme on sequences I03 and I05, respectively.
C1 [Wu, Kejun; Yang, You; Liu, Qiong] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
   [Wu, Kejun; Yang, You; Liu, Qiong] Wuhan Natl Lab Optoelect, Wuhan 430074, Hubei, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Toronto Metropolitan University
RP Liu, Q (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
EM wukejun@hust.edu.cn; yangyou@hust.edu.cn; q.liu@hust.edu.cn;
   xzhang@ee.ryerson.ca
RI Zhang, Xiao-Ping (Steven)/B-1436-2016
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069
FU National Key Research and Development Program of China [2020YFB2103501];
   National Natural Science Foundation of China [61971203]; Wuhan Science
   and Technology Bureau [2020020601012222]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB2103501, in part by the
   National Natural Science Foundation of China under Grant 61971203, and
   in part by the Wuhan Science and Technology Bureau under Grant
   2020020601012222.
CR Ali U, 2021, IEEE T IMAGE PROCESS, V30, P7215, DOI 10.1109/TIP.2021.3100268
   Bjontegaard G, 2001, VCEGM33
   Chandramouli P, 2018, IEEE T IMAGE PROCESS, V27, P1723, DOI 10.1109/TIP.2017.2775062
   Chang JHR, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275015
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P3722, DOI 10.1109/TMM.2021.3106775
   Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137
   DUONG VV, 2019, P IEEE INT S BROADB, P1
   Ge XH, 2017, IEEE T MULTIMEDIA, V19, P2345, DOI 10.1109/TMM.2017.2733461
   Gu DH, 2020, IEEE T MULTIMEDIA, V22, P1720, DOI 10.1109/TMM.2020.2971170
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Harnik D, 2014, IEEE DATA COMPR CONF, P223, DOI 10.1109/DCC.2014.66
   Hosseini MS, 2019, IEEE T IMAGE PROCESS, V28, P4510, DOI 10.1109/TIP.2019.2906582
   Khire S, 2012, IEEE ENG MED BIO, P5424, DOI 10.1109/EMBC.2012.6347221
   Kim W, 2021, IEEE T IMAGE PROCESS, V30, P559, DOI 10.1109/TIP.2020.3036782
   Li JC, 2021, IEEE T MULTIMEDIA, V23, P2986, DOI 10.1109/TMM.2021.3068561
   Li Q, 2021, IEEE T COMPUT IMAG, V7, P124, DOI 10.1109/TCI.2020.3046189
   MCCANN K, 2014, HIGH EFFICIENCY VIDE, P1
   Meng CL, 2021, IEEE T MULTIMEDIA, V24, P3193, DOI 10.1109/TMM.2021.3096071
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Mosquera-Zamudio A, 2019, DIAGN CYTOPATHOL, V47, P35, DOI 10.1002/dc.23992
   RERBEK M, 2016, P 8TH INT C QUAL MUL, P1
   Rizkallah M, 2016, EUR SIGNAL PR CONF, P898, DOI 10.1109/EUSIPCO.2016.7760378
   Sakamoto T, 2012, IEEE IMAGE PROC, P2901, DOI 10.1109/ICIP.2012.6467506
   Takahashi K, 2018, IEEE T IMAGE PROCESS, V27, P4571, DOI 10.1109/TIP.2018.2839263
   Torres C, 2018, IEEE T MULTIMEDIA, V20, P3057, DOI 10.1109/TMM.2018.2829162
   Wu KJ, 2022, IEEE T CIRC SYST VID, V32, P523, DOI 10.1109/TCSVT.2021.3066523
   Wu KJ, 2020, OPT EXPRESS, V28, P40024, DOI 10.1364/OE.413523
   Wu KJ, 2020, IEEE DATA COMPR CONF, P63, DOI 10.1109/DCC47342.2020.00014
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Yoo S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1453, DOI 10.1109/ICASSP.2018.8462234
   Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734
   Zarella MD, 2019, J MED IMAGING, V6, DOI 10.1117/1.JMI.6.4.047502
   Zhang XX, 2021, IEEE T MULTIMEDIA, V23, P3215, DOI 10.1109/TMM.2020.3021989
   Zhao X, 2019, RESULTS PHYS, V12, P1520, DOI 10.1016/j.rinp.2019.01.045
   Zheng S, 2021, IEEE T MULTIMEDIA, V23, P3577, DOI 10.1109/TMM.2020.3028479
   Zheng S, 2020, INT CONF ACOUST SPEE, P4337, DOI [10.1109/ICASSP40776.2020.9053742, 10.1109/icassp40776.2020.9053742]
   Zheng S, 2019, IEEE T MULTIMEDIA, V21, P1905, DOI 10.1109/TMM.2019.2891415
   Zhou X, 2020, IEEE T IMAGE PROCESS, V29, P3227, DOI 10.1109/TIP.2019.2957941
NR 39
TC 12
Z9 12
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3975
EP 3988
DI 10.1109/TMM.2022.3169055
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500032
DA 2024-07-18
ER

PT J
AU Wu, SX
   Dong, C
   Qiao, Y
AF Wu, Shixiang
   Dong, Chao
   Qiao, Yu
TI Blind Image Restoration Based on Cycle-Consistent Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image restoration; Training; Degradation; Unsupervised learning;
   Training data; Task analysis; Image reconstruction; Blind image
   restoration; generative adversarial network; super-resolution;
   unsupervised
ID SUPERRESOLUTION
AB This paper studies the blind image restoration where the ground truth is unavailable and the downsampling process is unknown. This complicated setting makes supervised learning and accurate kernel estimation impossible. Inspired by the recent success of image-to-image translation, this paper resorts to the unsupervised Cycle-consistent based framework to tackle this challenging problem. Different from the image-to-image task, the fidelity of reconstructed image is important for image restoration. Therefore, to improve the reconstruction ability of the Cycle-consistent network, we make explorations from the following aspects. First, we constrain low-frequency content in data to preserve the content of output from LR input. Second, we impose constraint on the content of training data to provide better supervision for discriminator, helping to suppress high-frequency artifacts or fake textures. Third, we average model parameters to further improve the generated image quality and help with model selection for GAN-based methods. Since GAN-based methods tend to produce various artifacts with different models, model average could realize a smoother control of balancing artifacts and fidelity. We have conducted extensive experiments on real noise and super resolution datasets to validate the effectiveness of the above techniques. The proposed ECycleGAN also demonstrates superior performance to SOTA methods in two applications - blind SR and blind denoising.
C1 [Wu, Shixiang; Dong, Chao; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Hong Kong 518055, Guangdong, Peoples R China.
   [Wu, Shixiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Dong, Chao; Qiao, Yu] Shanghai AI Lab, Shanghai 200030, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Shanghai Artificial Intelligence Laboratory
RP Qiao, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Hong Kong 518055, Guangdong, Peoples R China.
EM sx.wu@siat.ac.cn; chao.dong@siat.ac.cn; yu.qiao@siat.ac.cn
FU National Natural Science Foundation of China [61906184]; Joint
   Laboratory of CAS-HK; Shenzhen Research Program [RCJC20200714114557087];
   Shanghai Committee of Science and Technology, China [21DZ1100100]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61906184, in part by the Joint
   Laboratory of CAS-HK, in part by the Shenzhen Research Program under
   Grant RCJC20200714114557087, and in part by the Shanghai Committee of
   Science and Technology, China under Grant 21DZ1100100.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2018, P EUROPEAN C COMPUTE
   Bégin I, 2004, INT C PATT RECOG, P85, DOI 10.1109/ICPR.2004.1334046
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12
   Dabov K., 2009, Signal Processing with Adaptive Sparse Structured Representations (SPARS'09)
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239546, 10.1145/1276377.1276496]
   Franzen R, 1999, Kodak lossless true color image suite
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gui J., 2020, PROC IEEECVF C COMPU, P14471
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He Y, 2009, IMAGE VISION COMPUT, V27, P364, DOI 10.1016/j.imavis.2008.05.010
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Ignatov A, 2018, IEEE COMPUT SOC CONF, P804, DOI 10.1109/CVPRW.2018.00112
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Ke RH, 2022, IEEE T PATTERN ANAL, V44, P5796, DOI 10.1109/TPAMI.2021.3070382
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu LL, 2020, IEEE T NEUR NET LEAR, V31, P3540, DOI 10.1109/TNNLS.2019.2944979
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu MY, 2017, ADV NEUR IN, V30
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lugmayr A, 2019, IEEE INT CONF COMP V, P3408, DOI 10.1109/ICCVW.2019.00423
   Lugmayr Andreas, 2020, CVPR WORKSH
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Mao X., 2018, PROC IEEE C COMPUT V, P852
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Rosales R, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P472
   Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Timofte R., 2018, P IEEE C COMP VIS PA, P852
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wenchao Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14471, DOI 10.1109/CVPR42600.2020.01449
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang YB, 2020, IEEE T IMAGE PROCESS, V29, P1101, DOI 10.1109/TIP.2019.2938347
   Zhang YF, 2020, IEEE T MULTIMEDIA, V22, P1407, DOI 10.1109/TMM.2019.2943750
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 53
TC 9
Z9 9
U1 6
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1111
EP 1124
DI 10.1109/TMM.2021.3139209
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100007
DA 2024-07-18
ER

PT J
AU Yu, L
   Wang, BS
   He, JW
   Xia, GS
   Yang, W
AF Yu, Lei
   Wang, Bishan
   He, Jingwei
   Xia, Gui-Song
   Yang, Wen
TI Single Image Deraining With Continuous Rain Density Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Rain; Convolutional codes; Estimation; Encoding; Convolutional neural
   networks; Minimization; Mathematical models; Convolutional sparse
   coding; image deraining; rain density estimation
ID REMOVAL; MODEL
AB Single image deraining (SIDR) often suffers from over/under deraining due to the nonuniformity of rain densities and the variety of raindrop scales. In this paper, we propose a continuous density-guided network (CODE-Net) for SIDR. Particularly, it is composed of a rain streak extractor and a denoiser, where the convolutional sparse coding (CSC) is exploited to filter out noises from the extracted rain streaks. Inspired by the reweighted iterative soft-threshold (ISTA) for CSC, we address the problem of continuous rain density estimation by learning the weights with channel attention blocks from sparse codes. We further develop a multiscale strategy to depict rain streaks appearing at different scales. Experiments on synthetic and real-world data demonstrate the superiority of our methods over recent state-of-the-arts, in terms of both quantitative and qualitative results. Additionally, instead of quantizing rain density with several levels, our CODE-Net can provide continuous-valued estimations of rain densities, which is more desirable in real applications.
C1 [Yu, Lei; Wang, Bishan; He, Jingwei; Yang, Wen] Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
   [Xia, Gui-Song] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 Wuhan University; Wuhan University
RP Yu, L (corresponding author), Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
EM yuleiwhu@gmail.com; wangbs@whu.edu.cn; jingwei_he@whu.edu.cn;
   guisong.xia@whu.edu.cn; yangwen@whu.edu.cn
RI Xia, Gui-Song/HII-9232-2022; Yang, Wen/AAM-8352-2020
OI Xia, Gui-Song/0000-0001-7660-6090; Yang, Wen/0000-0002-3263-8768; Yu,
   Lei/0000-0002-7329-4631
FU National Natural Science Foundation of China [61871297]; Natural Science
   Foundation of Hubei Province of China [2021CFB467]; Fundamental Research
   Funds for the Central University [2042020kf0019]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61871297, in part by the Natural Science
   Foundation of Hubei Province of China under Grant 2021CFB467, and in
   part by the Fundamental Research Funds for the Central University under
   Grant 2042020kf0019.
CR Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jin X, 2019, IEEE IMAGE PROC, P2761, DOI [10.1109/ICIP.2019.8803238, 10.1109/icip.2019.8803238]
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li MH, 2018, PROC CVPR IEEE, P6644, DOI 10.1109/CVPR.2018.00695
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li SY, 2019, PROC CVPR IEEE, P3833, DOI 10.1109/CVPR.2019.00396
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lin X, 2021, IEEE T MULTIMEDIA, V23, P664, DOI 10.1109/TMM.2020.2987703
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Monga V, 2021, IEEE SIGNAL PROC MAG, V38, P18, DOI 10.1109/MSP.2020.3016905
   Papyan V, 2017, J MACH LEARN RES, V18, P1
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian JD, 2018, IEEE T MULTIMEDIA, V20, P2659, DOI 10.1109/TMM.2018.2808763
   Wang C, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1643, DOI 10.1145/3394171.3413820
   Wang H, 2020, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR42600.2020.00317
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang YL, 2021, PROC CVPR IEEE, P13370, DOI 10.1109/CVPR46437.2021.01317
   Wang YL, 2021, IEEE T MULTIMEDIA, V23, P2481, DOI 10.1109/TMM.2020.3013383
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Yang W., 2017, PROC CVPR IEEE, P1685, DOI DOI 10.1109/CVPR.2017.183
   Yang WH, 2020, AAAI CONF ARTIF INTE, V34, P12629
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Yang WH, 2019, IEEE T IMAGE PROCESS, V28, P2948, DOI 10.1109/TIP.2019.2892685
   Yang YZ, 2019, IEEE INT CON MULTI, P1378, DOI 10.1109/ICME.2019.00239
   Yasarla R, 2020, PROC CVPR IEEE, P2723, DOI 10.1109/CVPR42600.2020.00280
   Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 49
TC 8
Z9 8
U1 4
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 443
EP 456
DI 10.1109/TMM.2021.3127360
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, HG
   Li, HD
   Koniusz, P
AF Zhang, Hongguang
   Li, Hongdong
   Koniusz, Piotr
TI Multi-Level Second-Order Few-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Pipelines; Image recognition; Visualization; Feature
   extraction; Training; Streaming media; Few-shot learning; second-order
   statistics; image classification; action recognition
ID FINE-GRAINED IMAGE; COVARIANCE; RETRIEVAL
AB We propose a Multi-level Second-order (MlSo) few-shot learning network for supervised or unsupervised few-shot image classification and few-shot action recognition. We leverage so-called power-normalized second-order base learner streams combined with features that express multiple levels of visual abstraction, and we use self-supervised discriminating mechanisms. As Second-order Pooling (SoP) is popular in image recognition, we employ its basic element-wise variant in our pipeline. The goal of multi-level feature design is to extract feature representations at different layer-wise levels of CNN, realizing several levels of visual abstraction to achieve robust few-shot learning. As SoP can handle convolutional feature maps of varying spatial sizes, we also introduce image inputs at multiple spatial scales into MlSo. To exploit the discriminative information from multi-level and multi-scale features, we develop a Feature Matching (FM) module that reweights their respective branches. We also introduce a self-supervised step, which is a discriminator of the spatial level and the scale of abstraction. Our pipeline is trained in an end-to-end manner. With a simple architecture, we demonstrate respectable results on standard datasets such as Omniglot, mini-ImageNet, tiered-ImageNet, Open MIC, fine-grained datasets such as CUB Birds, Stanford Dogs and Cars, and action recognition datasets such as HMDB51, UCF101, and mini-MIT.
C1 [Zhang, Hongguang] AMS, Syst Engn Inst, Shanghai 100141, Peoples R China.
   [Li, Hongdong; Koniusz, Piotr] Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT 0200, Australia.
   [Koniusz, Piotr] CSIRO, Data61, Acton, ACT 2601, Australia.
C3 Australian National University; Commonwealth Scientific & Industrial
   Research Organisation (CSIRO)
RP Zhang, HG (corresponding author), AMS, Syst Engn Inst, Shanghai 100141, Peoples R China.
EM u6027953@alumni.anu.edu.au; hongdong.li@anu.edu.au;
   piotr.koniusz@data61.csiro.au
OI li, hongdong/0000-0003-4125-1554
FU National Natural Science Foundation of China [62106282]; AMS Equipment
   Development Research Fund [ZXD2020C2316]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62106282 and in part by the AMS
   Equipment Development Research Fund under Grant ZXD2020C2316.Part of
   this work is done during Hongguang Zhang's stay at ANU. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Guo-Jun Qi.
CR [Anonymous], 2010, P INT C MACH LEARN
   [Anonymous], 2006, P IEEE C COMP VIS PA
   Antoniou A., 2019, PROC INT C LEARN REP
   Bart E, 2005, PROC CVPR IEEE, P672
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Chen Q, 2013, IEEE T MULTIMEDIA, V15, P521, DOI 10.1109/TMM.2012.2236306
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Donahue J., 2016, arXiv
   Fink M., 2005, 2005 Advances in Neural Information Processing Systems, V17, P449
   Finn C, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2019, IEEE I CONF COMP VIS, P8058, DOI 10.1109/ICCV.2019.00815
   Gidaris S, 2019, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2019.00011
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Guo K, 2013, IEEE T IMAGE PROCESS, V22, P2479, DOI 10.1109/TIP.2013.2252622
   Guo M, 2018, LECT NOTES COMPUT SC, V11205, P673, DOI 10.1007/978-3-030-01246-5_40
   Harandi M, 2017, PR MACH LEARN RES, V70
   Hsu K., 2018, PROC INT C LEARN REP
   Hu GS, 2017, IEEE I CONF COMP VIS, P3764, DOI 10.1109/ICCV.2017.404
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jong-Chyi Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P645, DOI 10.1007/978-3-030-58571-6_38
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khodadadeh S, 2019, ADV NEUR IN, V32
   Khosla A., 2011, C COMP VIS PATT REC
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Klicpera Johannes, 2019, P ICLR
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Koniusz P., 2013, HAL00922524
   Koniusz P., 2017, P IEEE C COMPUTER VI, P4478
   Koniusz P, 2022, IEEE T PATTERN ANAL, V44, P648, DOI 10.1109/TPAMI.2021.3107160
   Koniusz P, 2022, IEEE T PATTERN ANAL, V44, P591, DOI 10.1109/TPAMI.2021.3107164
   Koniusz P, 2018, LECT NOTES COMPUT SC, V11220, P815, DOI 10.1007/978-3-030-01270-0_48
   Koniusz P, 2018, PROC CVPR IEEE, P5774, DOI 10.1109/CVPR.2018.00605
   Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667
   Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3
   Koniusz P, 2013, COMPUT VIS IMAGE UND, V117, P479, DOI 10.1016/j.cviu.2012.10.010
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lake B., 2011, P ANN M COGNITIVE SC, P1
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li ZG, 2017, Arxiv, DOI arXiv:1707.09835
   Lin T.-Y., 2018, P EUR C COMP VIS
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu Y., 2018, PROC INT C LEARN REP
   Mishra A, 2018, IEEE WINT CONF APPL, P372, DOI 10.1109/WACV.2018.00047
   Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Phaphuangwittayakul A, 2022, IEEE T MULTIMEDIA, V24, P2205, DOI 10.1109/TMM.2021.3077729
   Rahman Saimunur, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P1, DOI 10.1007/978-3-030-58555-6_1
   Rajapakse J.C., 2004, Neural Information Processing: Research and Development
   Ren M., 2018, INT C LEARNING REPRE, DOI DOI 10.1109/IPFA.2018.8452547
   Ren MY, 2019, ADV NEUR IN, V32
   Romero Andres, 2013, PROC 6 INT C COMPUT, P1
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Rusu Andrei A., 2019, PROC INT C LEARN REP
   Santoro A, 2017, ADV NEUR IN, V30
   Satorras V.G., 2018, P INT C LEARN REPR V
   Shih Y.-F., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P4123
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Simon Christian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P556, DOI 10.1007/978-3-030-58598-3_33
   Snell J, 2017, ADV NEUR IN, V30
   Song G, 2021, IEEE T MULTIMEDIA, V23, P1708, DOI 10.1109/TMM.2020.3002177
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun K., 2019, C UNC ART INT, P465
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tang L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P817
   Thorndike EL, 1901, PSYCHOL REV, V8, P247, DOI 10.1037/h0071363
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang L., 2021, arXiv
   Wang Q, 2011, IEEE T SYST MAN CY B, V41, P385, DOI 10.1109/TSMCB.2010.2056366
   Wang XJ, 2020, LECT NOTES ARTIF INT, V12084, P53, DOI 10.1007/978-3-030-47426-3_5
   Wang Y, 2019, Arxiv, DOI arXiv:1911.04623
   Weinberger K.Q., 2006, Advances in neural information processing systems, P1473
   Wu F, 2019, PR MACH LEARN RES, V97
   Xu BH, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P379, DOI 10.1145/3206025.3206028
   Xu HH, 2019, Arxiv, DOI arXiv:1912.12674
   Zhang H., 2020, P EUR C COMP VIS, P525
   Zhang HG, 2021, PROC CVPR IEEE, P9427, DOI 10.1109/CVPR46437.2021.00931
   Zhang HG, 2019, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR.2019.00288
   Zhang HG, 2019, IEEE WINT CONF APPL, P1185, DOI 10.1109/WACV.2019.00131
   Zhang J, 2019, IEEE I CONF COMP VIS, P1685, DOI 10.1109/ICCV.2019.00177
   Zhang JJ, 2021, INT J COMPUT VISION, V129, P300, DOI 10.1007/s11263-020-01376-1
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu H., 2020, INT C LEARNING REPRE
   Zhu K, 2021, IEEE T MULTIMEDIA, V23, P3726, DOI 10.1109/TMM.2020.3031062
   Zhu LC, 2018, LECT NOTES COMPUT SC, V11211, P782, DOI 10.1007/978-3-030-01234-2_46
   Zhu YH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1090
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
   Zintgraf Luisa, 2019, PMLR, P7693
NR 108
TC 14
Z9 15
U1 2
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2111
EP 2126
DI 10.1109/TMM.2022.3142955
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100040
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, N
   Liu, N
   Han, JW
   Wan, KY
   Shao, L
AF Zhang, Ni
   Liu, Nian
   Han, Junwei
   Wan, Kaiyuan
   Shao, Ling
TI Face De-Occlusion With Deep Cascade Guidance Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face de-occlusion; face inpainting; face parsing; GAN
ID RECOGNITION; IMAGE
AB Occlusion is a challenging yet commonly seen problem for facial perception. Existing works resort to deep learning models and perform model training on synthesized data due to the lack of paired real-world data. As a result,they usually perform unsatisfactorily on real-world occluded faces because of domain gaps. In this paper, we decompose the face de-occlusion task into three stages, i.e., occlusion detection, face parsing, and face reconstruction, to alleviate this issue. We first perform occlusion detection and use its results as guidance for the second stage to conduct occlusion-free face parsing. As such, face de-occlusion is first performed on the face paring space with less difficulty. We can train these two stages on both synthesized and real-world images, hence can obtain accurate results for the latter. In the last stage, we use the domain-agnostic occlusion detection map and the face parsing map as the guidance to conduct face reconstruction, thus can reduce the impact of appearance information and improve the model performance on real-world data. Aiming at improving the model capacity of inferring occluded facial appearance, we also propose two types of reference modules to use relevant facial parts to enhance the reconstruction of occluded regions. Consequently, our proposed model achieves promising face de-occlusion results on real-world images.
C1 [Zhang, Ni; Han, Junwei; Wan, Kaiyuan] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Liu, Nian; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Northwestern Polytechnical University
RP Han, JW (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
EM nnizhang.1995@gmail.com; nian.liu@inceptioniai.org;
   junweihan2010@gmail.com; kaiyuan.wan0106@gmail.com;
   ling.shao@inceptioniai.org
RI Shao, Ling/D-3535-2011
OI Zhang, Ni/0000-0002-6645-3366; Liu, Nian/0000-0002-0825-6081
FU Key-Area Research and Development Program of Guangdong Province
   [2019B010110001]; National Natural Science Foundation of China
   [62136007, U1801265, U20B2065, 62036005]; Key Ramp;D Program of Shaanxi
   Province [2021ZDLGY01-08]
FX & nbsp;This work was supported in part by the Key-Area Research and
   Development Program of Guangdong Province under Grant 2019B010110001, in
   part by the National Natural Science Foundation of China under Grants
   62136007, U1801265, U20B2065, and 62036005, and in part by the Key R & D
   Program of Shaanxi Province under Grant 2021ZDLGY01-08.
CR Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cai JC, 2021, IEEE T INF FOREN SEC, V16, P1044, DOI 10.1109/TIFS.2020.3023793
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng LL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1099, DOI 10.1145/2733373.2806291
   Dahmane M, 2014, IEEE T MULTIMEDIA, V16, P1574, DOI 10.1109/TMM.2014.2321113
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dong J., 2020, PROC INT C MULTIMEDI, P1
   Garcia C, 1999, IEEE T MULTIMEDIA, V1, P264, DOI 10.1109/6046.784465
   Ge SM, 2020, IEEE T CIRC SYST VID, V30, P3387, DOI 10.1109/TCSVT.2020.2967754
   Ge SM, 2017, PROC CVPR IEEE, P426, DOI 10.1109/CVPR.2017.53
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   He LX, 2019, IEEE T IMAGE PROCESS, V28, P791, DOI 10.1109/TIP.2018.2870946
   Hu BW, 2021, IEEE T CYBERNETICS, V51, P4373, DOI 10.1109/TCYB.2020.2995496
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li JS, 2019, IEEE T CIRC SYST VID, V29, P104, DOI 10.1109/TCSVT.2017.2778227
   Li SZ, 2001, PROC CVPR IEEE, P207
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liao HF, 2019, LECT NOTES COMPUT SC, V11361, P382, DOI 10.1007/978-3-030-20887-5_24
   Liao SC, 2013, IEEE T PATTERN ANAL, V35, P1193, DOI 10.1109/TPAMI.2012.191
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Maharjan RS, 2020, PROC SPIE, V11519, DOI 10.1117/12.2573031
   Mao QR, 2017, IEEE T MULTIMEDIA, V19, P861, DOI 10.1109/TMM.2016.2629282
   Martínez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Miyato T, 2018, Arxiv, DOI arXiv:1802.05957
   Ni BB, 2011, IEEE T MULTIMEDIA, V13, P1217, DOI 10.1109/TMM.2011.2167317
   Oh HJ, 2008, IMAGE VISION COMPUT, V26, P1515, DOI 10.1016/j.imavis.2008.04.016
   Osherov E, 2017, IEEE I CONF COMP VIS, P550, DOI 10.1109/ICCV.2017.67
   Park JS, 2005, IEEE T PATTERN ANAL, V27, P805, DOI 10.1109/TPAMI.2005.103
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Paszke A, 2019, ADV NEUR IN, V32
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Salimans T, 2016, ADV NEUR IN, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song LX, 2019, IEEE I CONF COMP VIS, P773, DOI 10.1109/ICCV.2019.00086
   Song LS, 2019, AAAI CONF ARTIF INTE, P2506
   Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI [10.1007/978-3-030-01216-8_1, 10.1109/APCAP.2017.8420330]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang YC, 2012, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2012.6247936
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weng RL, 2013, IEEE I CONF COMP VIS, P601, DOI 10.1109/ICCV.2013.80
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yan XS, 2019, IEEE I CONF COMP VIS, P7617, DOI 10.1109/ICCV.2019.00771
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yang M, 2017, PATTERN RECOGN, V66, P117, DOI 10.1016/j.patcog.2016.12.028
   Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419
   Yang Yang, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12305), P14, DOI 10.1007/978-3-030-60633-6_2
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yuan XW, 2019, IEEE I CONF COMP VIS, P10061, DOI 10.1109/ICCV.2019.01016
   Zeng D, 2021, IET BIOMETRICS, V10, P581, DOI 10.1049/bme2.12029
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang X, 2020, Arxiv, DOI arXiv:2002.02909
   Zhao F, 2018, IEEE T IMAGE PROCESS, V27, P778, DOI 10.1109/TIP.2017.2771408
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 69
TC 4
Z9 4
U1 3
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3217
EP 3229
DI 10.1109/TMM.2022.3157036
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200021
DA 2024-07-18
ER

PT J
AU Zhang, RS
   Quan, WZ
   Zhang, Y
   Wang, J
   Yan, DM
AF Zhang, Ruisong
   Quan, Weize
   Zhang, Yong
   Wang, Jue
   Yan, Dong-Ming
TI W-Net: Structure and Texture Interaction for Image Inpainting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Index Terms-Image inpainting; structure and texture; convolutional
   neural network; attention
AB Recent literature has developed two advanced tools for image inpainting: appearance propagation and attention matching. However, given the ineffective feature reorganization and vulnerable attention maps, existing works yield suboptimal results with distorted structures and inconsistent contents. Furthermore, we observe that deep sampling layers (DSL) and shallow skip connections (SSC) in U-Net separately promote image structure inference and texture synthesis. To address the above two issues, we devise a W-shaped network (W-Net), which consists of two key components: a texture spatial attention (TSA) module in SSC and a structure channel excitation (SCE) module in DSL. W-Net is a two-stage network, with coarse and refined structures derived at each stage. Meanwhile, the TSA module fills incomplete textures with reliable attention scores under the guidance of coarse structures, which effectively diminishes inconsistency from appearance to semantics. The SCE module rectifies structures according to the difference between coarse structures and refined structures enhanced by texture features. Then the module motivates them to produce more reasonable shapes. Complete textures and refined structures constitute desired inpainted images, as the output of W-Net. Experiments on multiple datasets demonstrate the superior performance of W-Net.
C1 [Zhang, Ruisong; Quan, Weize; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zhang, Yong; Wang, Jue] Tencent, AI Lab, Shenzhen 518000, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Tencent
RP Yan, DM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM zhangruisong2019@ia.ac.cn; qweizework@gmail.com;
   zhangyong201303@gmail.com; arphid@gmail.com; yandongming@gmail.com
RI Wang, Jue/GVU-0480-2022
OI Yan, Dong-Ming/0000-0003-2209-2404; Wang, Jue/0000-0002-3641-3136
FU National Natural Science Foundation of China [62102418, 62172415];
   Tencent AI Laboratory Rhino-Bird Focused Research Program [JR202127];
   Open Project Program of National Key Laboratory of Fundamental Science
   on Synthetic Vision, Sichuan University [2021SCUVS002]; Open Research
   Fund Program of State key Laboratory of Hydroscience and Engineering,
   Tsinghua University [sklhse-2022-D-04]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102418 and 62172415, in part by the
   Tencent AI Laboratory Rhino-Bird Focused Research Program under Grant
   JR202127, in part by the Open Project Program of National Key Laboratory
   of Fundamental Science on Synthetic Vision, Sichuan University under
   Grant 2021SCUVS002, and in part by the Open Research Fund Program of
   State key Laboratory of Hydroscience and Engineering, Tsinghua
   University, under Grant sklhse-2022-D-04. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Mr. Jingkuan Song.
CR Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bian XW, 2022, COMPUT VIS MEDIA, V8, P273, DOI 10.1007/s41095-021-0242-8
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14114, DOI 10.1109/ICCV48922.2021.01387
   Guo ZY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2496, DOI 10.1145/3343031.3351022
   Hensel M, 2017, ADV NEUR IN, V30
   Hongyu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P725, DOI 10.1007/978-3-030-58536-5_43
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601205
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2018, P INT C LEARN REPR I
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Lahiri Avisek, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13693, DOI 10.1109/CVPR42600.2020.01371
   Li X., 2021, P INT C COMP VIS MON, P2011
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng JL, 2021, PROC CVPR IEEE, P10770, DOI 10.1109/CVPR46437.2021.01063
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun HY, 2023, IEEE T MULTIMEDIA, V25, P4240, DOI 10.1109/TMM.2022.3174413
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wan ZY, 2020, PROC CVPR IEEE, P2744, DOI 10.1109/CVPR42600.2020.00282
   Wang N, 2021, IEEE T IMAGE PROCESS, V30, P1784, DOI 10.1109/TIP.2020.3048629
   Wang TF, 2021, PROC CVPR IEEE, P5116, DOI 10.1109/CVPR46437.2021.00508
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HW, 2022, IEEE T MULTIMEDIA, V24, P4016, DOI 10.1109/TMM.2021.3111491
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yi Z., 2020, CVPR, P7508
   Yu F., 2015, ARXIV
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng XX, 2022, COMPUT VIS MEDIA, V8, P239, DOI 10.1007/s41095-021-0238-4
   Zeng Y., 2021, P IEEE CVF INT C COM, P14164
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhan XH, 2020, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR42600.2020.00384
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang HR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1939, DOI 10.1145/3240508.3240625
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang RS, 2020, COMPUT GRAPH FORUM, V39, P471, DOI 10.1111/cgf.14160
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou T., 2020, IEEECVF C COMPUTER V, P7680
   Zhu MY, 2021, IEEE T IMAGE PROCESS, V30, P4855, DOI 10.1109/TIP.2021.3076310
NR 63
TC 1
Z9 1
U1 8
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7299
EP 7310
DI 10.1109/TMM.2022.3219728
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000043
DA 2024-07-18
ER

PT J
AU Gao, LL
   Ji, YL
   Gedamu, KM
   Zhu, XF
   Xu, X
   Shen, HT
AF Gao, Lingling
   Ji, Yanli
   Gedamu, Kumie
   Zhu, Xiaofeng
   Xu, Xing
   Shen, Heng Tao
TI View-Invariant Human Action Recognition Via View Transformation Network
   (VTN)
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Arbitrary-view action recognition; attention learning; view rotation of
   skeletons; view transformation network (VTN)
AB Since the human body is non-rigid, actions captured in different views always involve action occlusion and information loss. Recently, view-variation-related human action recognition is still a challenging problem. To address the problem, we propose a View Transformation Network (VTN) that realizes the view normalization by transforming arbitrary-view action samples to a base view to seek for a view-invariant representation. an attention learning module is designed to learn a co-attention for action samples of different views, that contributes to output a similar feature representation to erase the view diversity in different views. Extensive and fair evaluations are performed on the UESTC varying-view RGB-D dataset, the NTU RGB-D 60 dataset, and the NTU RGB-D 120 dataset, where three evaluation types, i.e. X-subject, X-view, and A-view recognition, are performed. Experiments illustrate that our VTN model achieves outstanding performance.
C1 [Gao, Lingling; Ji, Yanli; Gedamu, Kumie; Zhu, Xiaofeng; Xu, Xing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Ji, YL (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM 202021080214@std.uestc.edu.cn; yanliji@uestc.edu.cn;
   alemugedamu@gmail.com; seanzhuxf@gmail.com; xing.xu@uestc.edu.cn;
   shenhengtao@hotmail.com
RI Zhu, Xiaofeng/HII-5291-2022; Shen, Heng Tao/ABD-5331-2021; Gao,
   Lingling/KBQ-7954-2024; Kumie, Gedamu Alemu/HSD-1627-2023
OI Zhu, Xiaofeng/0000-0001-6840-0578; Gao, Lingling/0000-0002-4521-8703
FU Sichuan Science and Technology Program, China [2019ZDZX0008,
   2020YFS0057]
FX This work was supported in part by the Sichuan Science and Technology
   Program, China under Grants 2019ZDZX0008 and 2020YFS0057.
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   Cai ZW, 2014, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2014.83
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cheng ZW, 2012, LECT NOTES COMPUT SC, V7584, P52, DOI 10.1007/978-3-642-33868-7_6
   Gupta A, 2014, PROC CVPR IEEE, P2601, DOI 10.1109/CVPR.2014.333
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Hu JF, 2019, IEEE T PATTERN ANAL, V41, P2568, DOI 10.1109/TPAMI.2018.2863279
   Hu JF, 2016, IEEE T CIRC SYST VID, V26, P647, DOI 10.1109/TCSVT.2015.2397200
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Iosifidis A, 2012, IEEE T NEUR NET LEAR, V23, P412, DOI 10.1109/TNNLS.2011.2181865
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Ji XF, 2010, IEEE T SYST MAN CY C, V40, P13, DOI 10.1109/TSMCC.2009.2027608
   Ji YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P574, DOI 10.1145/3343031.3350959
   Ji YL, 2020, IEEE T IMAGE PROCESS, V29, P2742, DOI 10.1109/TIP.2019.2952088
   Ji YL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1510, DOI 10.1145/3240508.3240675
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Liu AA, 2017, IEEE T CYBERNETICS, V47, P1781, DOI 10.1109/TCYB.2016.2582918
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P1453, DOI 10.1109/TPAMI.2019.2898954
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu L, 2016, AAAI CONF ARTIF INTE, P1266
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu Y, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1617
   Liu Y, 2016, NEUROCOMPUTING, V181, P108, DOI 10.1016/j.neucom.2015.08.096
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4
   Peng W, 2020, AAAI CONF ARTIF INTE, V34, P2669
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167
   Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389
   Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860
   Roy D, 2019, IEEE T MULTIMEDIA, V21, P1672, DOI 10.1109/TMM.2018.2887021
   Ryoo MS, 2015, ACMIEEE INT CONF HUM, P295, DOI 10.1145/2696454.2696462
   Salakhutdinov, 2015, ARXIV151104119
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Thakkar K. C., 2018, PRAC BRIZ MACH VIS C, P270
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wei P, 2019, IEEE T MULTIMEDIA, V21, P2195, DOI 10.1109/TMM.2019.2897902
   Wei P, 2013, IEEE I CONF COMP VIS, P3272, DOI 10.1109/ICCV.2013.406
   Yan P, 2008, PROC CVPR IEEE, P3080
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang YZ, 2015, AAAI CONF ARTIF INTE, P3686
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
NR 63
TC 11
Z9 11
U1 3
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4493
EP 4503
DI 10.1109/TMM.2021.3119177
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7B6NT
UT WOS:000899248400004
DA 2024-07-18
ER

PT J
AU Li, B
   Zhu, YL
   Wang, YT
   Lin, CW
   Ghanem, B
   Shen, LL
AF Li, Bing
   Zhu, Yuanlue
   Wang, Yitong
   Lin, Chia-Wen
   Ghanem, Bernard
   Shen, Linlin
TI AniGAN: Style-Guided Generative Adversarial Networks for Unsupervised
   Anime Face Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Generators; Faces; Transforms; Task analysis; Decoding;
   Visualization; GAN; Image translation; non-photorealistic rendering;
   style transfer
ID IMAGE SYNTHESIS
AB In this paper, we propose a novel framework to translate a portrait photo-face into an anime appearance. Different from existing translation methods which do not designate specific styles, we aim to synthesize anime-faces which are style-consistent with a given reference anime-face. However, unlike typical translation tasks, such anime-face translation is particularly challenging due to the large and complex variations of appearances among anime-faces. Existing methods often fail to transfer the styles of reference anime-faces to the generated anime-faces, or introduce noticeable artifacts/distortions in the local shapes of their generated anime-faces. We propose a novel GAN-based anime-face translator, called AniGAN, to synthesize high-quality anime-faces. Specifically, a new generator architecture is proposed to simultaneously transfer color/texture styles and transform local facial shapes into anime-like counterparts based on the style of a reference anime-face, while preserving the global structure of the source photo-face. New normalization functions are designed for the generator to further improve local shape transformation and color/texture style transfer. Besides, we propose a double-branch discriminator to learn domain-specific distributions through individual branches and learn cross-domain shared distributions via shared layers, helping generate visually pleasing anime-faces and effectively mitigate artifacts/distortions. Extensive experiments on benchmark datasets qualitatively and quantitatively demonstrate the superiority of our method over state-of-the-art methods.
C1 [Li, Bing; Ghanem, Bernard] King Abdullah Univ Sci & Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia.
   [Zhu, Yuanlue; Wang, Yitong] ByteDance, Shenzhen 518060, Peoples R China.
   [Zhu, Yuanlue; Shen, Linlin] Shenzhen Univ, Coll Comp Sci & Software Engn, Comp Vis Inst, Shenzhen 518060, Peoples R China.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Ind Technol Res Inst, Elect & Optoelect Syst Res Labs, Hsinchu, Taiwan.
   [Shen, Linlin] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518060, Peoples R China.
   [Shen, Linlin] Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
C3 King Abdullah University of Science & Technology; Shenzhen University;
   National Tsing Hua University; National Tsing Hua University; Industrial
   Technology Research Institute - Taiwan; Shenzhen Institute of Artificial
   Intelligence & Robotics for Society; Shenzhen University
RP Li, B (corresponding author), King Abdullah Univ Sci & Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia.; Shen, LL (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Comp Vis Inst, Shenzhen 518060, Peoples R China.; Shen, LL (corresponding author), Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518060, Peoples R China.; Shen, LL (corresponding author), Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM bing.li@kaust.edu.sa; zhuyuanlue2017@email.szu.edu.cn;
   wangyitong@pku.edu.cn; cwlin@ee.nthu.edu.tw;
   bernard.ghanem@kaust.edu.sa; llshen@szu.edu.cn
RI Lin, Chia-Wen/M-4571-2013; Wang, Yitong/KBA-1959-2024; Shen,
   Linlin/AEX-9392-2022; Wang, Junzhe/KCK-4991-2024; Li,
   Bing/ITV-4568-2023; li, chunlin/KFS-0761-2024; Ghanem,
   Bernard/J-7605-2017
OI Shen, Linlin/0000-0003-1420-0815; Li, Bing/0000-0002-1875-2919; Ghanem,
   Bernard/0000-0002-5534-587X
FU King Abdullah University of Science and Technology (KAUST) Office of
   Sponsored Research through the Visual Computing Center (VCC); Ministry
   of Science and Technology, Taiwan [MOST 110-2634-F-007-015]
FX This work was supported in part by the King Abdullah University of
   Science and Technology (KAUST) Office of Sponsored Research through the
   Visual Computing Center (VCC) funding, and in part by the Ministry of
   Science and Technology, Taiwan, under Grant MOST 110-2634-F-007-015. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publicationwas Dr. Palaiahnakote Shivakumara. (Bing Li
   and Yuanlue Zhu contributed equally to this work.)
CR [Anonymous], DANBOORU COMMUNITY G
   Brock A, 2019, PROC INT C LEARN REP
   Chen HJ, 2019, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR.2019.01028
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1172, DOI 10.1145/3240508.3240655
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaidi Cao, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275046
   Karras T., 2018, P INT C LEARN REP
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim J., 2020, P INT C LEARN REP
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lei Ba J., 2016, arXiv
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liang XD, 2018, LECT NOTES COMPUT SC, V11217, P574, DOI 10.1007/978-3-030-01261-8_34
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu MY, 2017, ADV NEUR IN, V30
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma Liqian, 2019, P ICLR
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   nagadomi, 2009, AN
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Paszke A, 2019, ADV NEUR IN, V32
   Runtao Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P36, DOI 10.1007/978-3-030-58580-8_3
   Shi YC, 2019, PROC CVPR IEEE, P10754, DOI 10.1109/CVPR.2019.01102
   Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI [10.1007/978-3-030-01216-8_1, 10.1109/APCAP.2017.8420330]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Yang C, 2020, PROC CVPR IEEE, P5920, DOI 10.1109/CVPR42600.2020.00596
   Yang C, 2019, IEEE T IMAGE PROCESS, V28, P4845, DOI 10.1109/TIP.2019.2914583
   Yaniv J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322984
   Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 47
TC 16
Z9 17
U1 9
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4077
EP 4091
DI 10.1109/TMM.2021.3113786
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400029
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Qiu, S
   Anwar, S
   Barnes, N
AF Qiu, Shi
   Anwar, Saeed
   Barnes, Nick
TI Geometric Back-Projection Network for Point Cloud Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Task analysis; Geometry;
   Visualization; Shape; Redundancy; Point Cloud Classification; 3D Deep
   Learning; Attention Mechanism; Geometric Features; Error-correcting
   Feedback
AB As the basic task of point cloud analysis, classification is fundamental but always challenging. To address some unsolved problems of existing methods, we propose a network that captures geometric features of point clouds for better representations. To achieve this, on the one hand, we enrich the geometric information of points in low-level 3D space explicitly. On the other hand, we apply CNN-based structures in high-level feature spaces to learn local geometric context implicitly. Specifically, we leverage an idea of error-correcting feedback structure to capture the local features of point clouds comprehensively. Furthermore, an attention module based on channel affinity assists the feature map to avoid possible redundancy by emphasizing its distinct channels. The performance on both synthetic and real-world point clouds datasets demonstrate the superiority and applicability of our network. Comparing with other state-of-the-art methods, our approach balances accuracy and efficiency.
C1 [Qiu, Shi; Anwar, Saeed] CSIRO, Data61, Canberra, ACT 2601, Australia.
   [Qiu, Shi; Anwar, Saeed] Australian Natl Univ, Res Sch Engn, Canberra, ACT 2601, Australia.
   [Barnes, Nick] Australian Natl Univ, Sch Comp, Canberra, ACT 2601, Australia.
C3 Commonwealth Scientific & Industrial Research Organisation (CSIRO);
   Australian National University; Australian National University
RP Qiu, S (corresponding author), CSIRO, Data61, Canberra, ACT 2601, Australia.; Qiu, S (corresponding author), Australian Natl Univ, Res Sch Engn, Canberra, ACT 2601, Australia.
EM shi.qiu@anu.edu.au; saeed.anwar@data61.csiro.au; nick.barnes@anu.edu.au
RI Anwar, Saeed/X-5224-2019; Barnes, Nick/Y-2744-2018
OI Qiu, Shi/0000-0001-9958-180X; Barnes, Nick/0000-0002-9343-9535
CR [Anonymous], 2003, 7 CENTR EUR SEM COMP
   [Anonymous], 2001, Int. Arch. Photogramm, Remote Sens., DOI DOI 10.5194/ISPRSARCHIVES-XL-5-W2-207-2013
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Ben-Shabat Y, 2018, IEEE ROBOT AUTOM LET, V3, P3145, DOI 10.1109/LRA.2018.2850061
   Blais F, 2004, J ELECTRON IMAGING, V13, P231, DOI 10.1117/1.1631921
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53
   Chen C, 2021, NEUROCOMPUTING, V438, P122, DOI 10.1016/j.neucom.2021.01.095
   Chen JD, 2018, IEEE INT CONF ROBOT, P2164, DOI 10.1109/ICRA.2018.8461095
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Engelmann F, 2020, IEEE INT CONF ROBOT, P9463, DOI [10.1109/icra40945.2020.9197503, 10.1109/ICRA40945.2020.9197503]
   Fang PF, 2019, IEEE I CONF COMP VIS, P8029, DOI 10.1109/ICCV.2019.00812
   Feng MT, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107446
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jaboyedoff M, 2012, NAT HAZARDS, V61, P5, DOI 10.1007/s11069-010-9634-2
   Kang Z., 2019, SPRING INT C NEUR IN, P35
   Kitano H, 2002, NATURE, V420, P206, DOI 10.1038/nature01254
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li RH, 2020, PROC CVPR IEEE, P6377, DOI 10.1109/CVPR42600.2020.00641
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YY, 2018, ADV NEUR IN, V31
   Liu X H, 2019, P 27 ACM INT C MULT, P989, DOI DOI 10.1145/3343031.3350960
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu YC, 2019, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2019.00534
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu ZS, 2019, IEEE COMPUT SOC CONF, P2041, DOI 10.1109/CVPRW.2019.00256
   Loshchilov I, 2016, ARXIV
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mérigot Q, 2011, IEEE T VIS COMPUT GR, V17, P743, DOI 10.1109/TVCG.2010.261
   Mitra N.J., 2004, P 2004 EUROGRAPHICSA, P22, DOI 10.1145/1057432.1057435
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Nezhadarya E., 2020, P IEEE CVF C COMP VI, p12 956
   Paigwar A, 2019, IEEE COMPUT SOC CONF, P1297, DOI 10.1109/CVPRW.2019.00169
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Rosa S., 2020, 2020 P IEEECVF C COM, P11108, DOI [DOI 10.1109/CVPR42600.2020.01112, 10.1109/CVPR42600.2020.01112]
   Roveri R, 2018, PROC CVPR IEEE, P4176, DOI 10.1109/CVPR.2018.00439
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun YB, 2020, IEEE WINT CONF APPL, P61, DOI [10.1109/wacv45572.2020.9093430, 10.1109/WACV45572.2020.9093430]
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2017, ARXIV171010903, DOI DOI 10.48550/ARXIV.1710.10903
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   WEISS LE, 1987, IEEE T ROBOTIC AUTOM, V3, P404, DOI 10.1109/JRA.1987.1087115
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu Y, 2018, ADV SOC SCI EDUC HUM, V284, P87
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhang WX, 2019, PROC CVPR IEEE, P12428, DOI 10.1109/CVPR.2019.01272
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
NR 78
TC 92
Z9 99
U1 16
U2 69
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1943
EP 1955
DI 10.1109/TMM.2021.3074240
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200013
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Song, XB
   Zhou, DF
   Li, W
   Dai, YC
   Liu, L
   Li, HD
   Yang, RG
   Zhang, LJ
AF Song, Xibin
   Zhou, Dingfu
   Li, Wei
   Dai, Yuchao
   Liu, Liu
   Li, Hongdong
   Yang, Ruigang
   Zhang, Liangjun
TI WAFP-Net: Weighted Attention Fusion Based Progressive Residual Learning
   for Depth Map Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Degradation; Superresolution; Color; Feature extraction; Laser radar;
   Image edge detection; Three-dimensional displays; Attention fusion;
   depth; super-resolution; residual learning
ID IMAGE SUPERRESOLUTION; ACCURATE; NETWORK
AB Despite the remarkable progresses achieved in depth map super-resolution (DSR), it remains a major challenge to tackle with real-world degradation of low-resolution (LR) depth maps. Synthetic datasets are mainly used in existing DSR approaches, which is quite different from what would get from a real depth sensor. Besides, the enhancements of features in existing DSR approaches are not sufficiently enough, which also limit the performance. To alleviate these problems, we first propose two types of degradation models to describe the generation of LR depth maps, including bi-cubic down-sampling with noise and interval down-sampling, and different DSR models are learned correspondingly. Then, we propose a weighted attention fusion strategy that is embedded into a progressive residual learning framework, which guarantees that the high-resolution (HR) depth maps can be well recovered in a coarse-to-fine manner. The weighted attention fusion strategy can enhance the features with abundant high-frequency components in both global and local manners, thus better HR depth maps can be expected. Besides, to re-use the effective information in the progressive process sufficiently, a multi-stage fusion module is combined into the proposed framework, and the Total Generalized Variation (TGV) regularization and input loss are exploited to further improve the performance of our method. Extensive experiments of different benchmarks demonstrate the superiority of our approach over the state-of-the-art (SOTA) approaches.
C1 [Song, Xibin; Zhou, Dingfu; Zhang, Liangjun] Baidu Res, Robot & Autonomous Driving Lab, Beijing 100000, Peoples R China.
   [Li, Wei] Shandong Univ, Sch Software, Jinan 250100, Shandong, Peoples R China.
   [Dai, Yuchao] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710060, Peoples R China.
   [Liu, Liu; Li, Hongdong] Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT 0200, Australia.
   [Liu, Liu; Li, Hongdong] Australian Ctr Robot Vis, Canberra, ACT 0200, Australia.
   [Yang, Ruigang] Univ Kentucky, Coll Engn, Lexington, KY 40506 USA.
C3 Baidu; Shandong University; Northwestern Polytechnical University;
   Australian National University; Australian Centre for Robotic Vision;
   University of Kentucky
RP Li, W (corresponding author), Shandong Univ, Sch Software, Jinan 250100, Shandong, Peoples R China.
EM song.sducg@gmail.com; dingfuzhou@gmail.com; wli@sdu.edu.cn;
   daiyuchao@gmail.com; Liu.Liu@anu.edu.au; hongdong.li@anu.edu.au;
   ryang@cs.uky.edu; liangjunzhang@baidu.com
RI Song, Xibin/AAF-8629-2019; Dai, Yuchao/F-7832-2015; Zhou,
   Dingfu/AAM-9192-2021
OI Dai, Yuchao/0000-0002-4432-7406; li, hongdong/0000-0003-4125-1554
FU Robotics and Autonomous Driving Laboratory of Baidu Research; National
   Key Research and Development Program of China [2018AAA0102803]; Natural
   Science Foundation of China [61871325, 61671387]
FX The work was supported in part by the Robotics and Autonomous Driving
   Laboratory of Baidu Research, in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102803, in part by the
   Natural Science Foundation of China under Grants 61871325 and 61671387.
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], 2007, P IEEE C COMP VIS PA
   Bichlmeier C., 2007, INT S MIX AUGM REAL, P129, DOI DOI 10.1109/ISMAR.2007.4538837
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Deng X, 2019, IEEE I CONF COMP VIS, P3076, DOI 10.1109/ICCV.2019.00317
   Dong WS, 2019, IEEE T PATTERN ANAL, V41, P2305, DOI 10.1109/TPAMI.2018.2873610
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Huang XY, 2020, IEEE T PATTERN ANAL, V42, P2702, DOI 10.1109/TPAMI.2019.2926463
   Huang XY, 2018, IEEE COMPUT SOC CONF, P1067, DOI 10.1109/CVPRW.2018.00141
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Jiang ZY, 2018, IEEE T IMAGE PROCESS, V27, P2587, DOI 10.1109/TIP.2018.2806089
   Keighrey C, 2021, IEEE T MULTIMEDIA, V23, P333, DOI 10.1109/TMM.2020.2982046
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lei JJ, 2017, IEEE T IMAGE PROCESS, V26, P1732, DOI 10.1109/TIP.2017.2656463
   Li YJ, 2019, IEEE T PATTERN ANAL, V41, P1909, DOI 10.1109/TPAMI.2018.2890623
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu LN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12717, DOI 10.1109/ICCV48922.2021.01250
   Liu LN, 2021, AAAI CONF ARTIF INTE, V35, P2136
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1636, DOI 10.1109/TIP.2018.2875506
   Liu ZS, 2019, IEEE INT CONF COMP V, P3517, DOI 10.1109/ICCVW.2019.00436
   Lo KH, 2018, IEEE T CYBERNETICS, V48, P371, DOI 10.1109/TCYB.2016.2637661
   Lu JJ, 2015, PROC CVPR IEEE, P2245, DOI 10.1109/CVPR.2015.7298837
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Mandal S, 2017, IEEE T IMAGE PROCESS, V26, P119, DOI 10.1109/TIP.2016.2621410
   MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482
   Martull S., 2012, P ICPR WORKSHOP TRAK, V111, P117
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Ning Q, 2021, IEEE J-STSP, V15, P240, DOI 10.1109/JSTSP.2020.3037516
   OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955
   Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428
   Riegler G, 2016, ARXIV160708569
   Riegler G, 2016, LECT NOTES COMPUT SC, V9907, P268, DOI 10.1007/978-3-319-46487-9_17
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song XB, 2021, IEEE T IMAGE PROCESS, V30, P4691, DOI 10.1109/TIP.2021.3074306
   Song XB, 2019, IEEE T CIRC SYST VID, V29, P2323, DOI 10.1109/TCSVT.2018.2866399
   Song XB, 2020, PROC CVPR IEEE, P5630, DOI 10.1109/CVPR42600.2020.00567
   Song XB, 2019, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2019.00560
   Song XB, 2017, LECT NOTES COMPUT SC, V10114, P360, DOI 10.1007/978-3-319-54190-7_22
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tosic I, 2014, IEEE T IMAGE PROCESS, V23, P2122, DOI 10.1109/TIP.2014.2312645
   Tsai TJ, 2015, IEEE T MULTIMEDIA, V17, P1550, DOI 10.1109/TMM.2015.2454332
   Voynov O, 2019, IEEE I CONF COMP VIS, P5652, DOI 10.1109/ICCV.2019.00575
   Wang J, 2020, IEEE T MULTIMEDIA, V22, P1470, DOI 10.1109/TMM.2019.2946075
   Wang ZH, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107274
   Xie J, 2016, IEEE T IMAGE PROCESS, V25, P428, DOI 10.1109/TIP.2015.2501749
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1426, DOI 10.1109/TMM.2020.2997126
   Ye XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1809, DOI 10.1145/3394171.3413874
   Yu S., 2019, IEEE ACCESS, V7, P140553
   Zhang DY, 2021, IEEE T MULTIMEDIA, V23, P2172, DOI 10.1109/TMM.2020.3008041
   Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328
   Zhang K, 2019, PROC CVPR IEEE, P1671, DOI 10.1109/CVPR.2019.00177
   Zhang YL, 2019, Arxiv, DOI arXiv:1903.10082
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou KL, 2019, INT CONF ACOUST SPEE, P1847, DOI 10.1109/ICASSP.2019.8683395
   Zhou RF, 2019, IEEE I CONF COMP VIS, P2433, DOI 10.1109/ICCV.2019.00252
NR 71
TC 5
Z9 5
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4113
EP 4127
DI 10.1109/TMM.2021.3118282
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400032
DA 2024-07-18
ER

PT J
AU Yu, LY
   Xie, HT
   Zhang, YD
AF Yu, Lingyun
   Xie, Hongtao
   Zhang, Yongdong
TI Multimodal Learning for Temporally Coherent Talking Face Generation With
   Articulator Synergy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mouth; Lips; Faces; Facial animation; Shape; Visualization; Task
   analysis; Articulator synergy; multimodal learning; talking face
   generation; adversarial training; video synthesis
ID FRAMEWORK
AB Talking face generation is a demanding task to synthesize a high quality video with accurate lip synchronization and rhythmic head motion. However, existing methods always suffer from unrealistic facial animations, because 1) they only take single-mode input, but ignore the complementarity of multimodal inputs for lip-sync improvement; 2) they only explore lip movements, but ignore the articulator synergy between lips and jaw; 3) they generate each video frame in a temporal-independent way, but ignore the temporal continuity among the entire video. To address these limitations, in this paper, we present a novel method to generate realistic and temporally coherent talking heads by considering multimodal inputs, articulator synergy, inter-frame consistency and intra-frame consistency. Firstly, for landmark prediction, a novel Multiple Synergy Network (MSN) is proposed to improve the accuracy of landmark prediction by incorporating multimodal inputs (i.e., audio and text inputs). Besides, instead of merely considering lip landmarks, we also explore the jaw movements to ensure articulator synergy among lips and jaw. Secondly, for realistic video generation, a Video Consistency Network (VCN) is proposed conditioned on the predicted landmarks. In VCN, the optical flow is adopted to model the temporal continuity between frames to ensure inter-frame consistency. Meanwhile, a mouth generation branch is proposed to enhance mouth texture and the corresponding mouth mask is employed to ensure intra-frame consistency between the mouth area and the others. Extensive experiments demonstrate that our approach exhibits excellent superiority on lip-sync and can generate photo-realistic facial animations. Project is available at http://imcc.ustc.edu.cn/project/tfgen/.
C1 [Yu, Lingyun; Xie, Hongtao; Zhang, Yongdong] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China.
   [Yu, Lingyun; Xie, Hongtao; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xie, HT; Zhang, YD (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China.; Xie, HT; Zhang, YD (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
EM yuly@mail.ustc.edu.cn; htxie@ustc.edu.cn; zhyd73@ustc.edu.cn
RI li, chunlin/KFS-0761-2024; Wang, Yitong/KBA-1959-2024; Wang,
   Junzhe/KCK-4991-2024
FU National Nature Science Foundation of China [62022076, U1936210,
   62032006, 62102127]; China Postdoctoral Science Foundation
   [2020M682035]; Anhui Postdoctoral Research Activities Foundation
   [2020B436]; Fundamental Research Funds for the Central Universities
   [WK3480000011]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grants 62022076, U1936210, 62032006, and
   62102127, in part by the China Postdoctoral Science Foundation under
   Grant 2020M682035, in part by Anhui Postdoctoral Research Activities
   Foundation under Grant 2020B436, and in part by the Fundamental Research
   Funds for the Central Universities under Grant WK3480000011.
CR Afouras Triantafyllos, 2020, COMPUTER VISION ECCV
   [Anonymous], 2017, arXiv preprint arXiv:1801.01442
   Bai S., 2018, CoRR
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chen PH, 2020, IEEE T IMAGE PROCESS, V29, P8292, DOI 10.1109/TIP.2020.3009820
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Chuang Gan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P758, DOI 10.1007/978-3-030-58621-8_44
   Chung J. S., 2017, BRIT MACHINE VIS C B
   Ebdelli M, 2015, IEEE T IMAGE PROCESS, V24, P3034, DOI 10.1109/TIP.2015.2437193
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Fried O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323028
   Fukada T., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P137, DOI 10.1109/ICASSP.1992.225953
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Gan Chuang, 2020, P IEEECVF C COMPUTER, P10478, DOI DOI 10.1109/CVPR42600.2020.01049
   Gao RH, 2019, PROC CVPR IEEE, P324, DOI 10.1109/CVPR.2019.00041
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Li JM, 2021, PROC CVPR IEEE, P6454, DOI 10.1109/CVPR46437.2021.00639
   Ling ZH, 2010, SPEECH COMMUN, V52, P834, DOI 10.1016/j.specom.2010.06.006
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Moghadam AA, 2014, IEEE T IMAGE PROCESS, V23, P4055, DOI 10.1109/TIP.2014.2321476
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Rasch J, 2020, IEEE T IMAGE PROCESS, V29, P9678, DOI 10.1109/TIP.2020.3030590
   Rossler A., 2018, ARXIV180309179E
   Shang ZH, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107950
   Siarohin A, 2019, ADV NEUR IN, V32
   Song Linsen, 2020, ARXIV PREPRINT ARXIV
   Song Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P919
   Sorensen T, 2019, J ACOUST SOC AM, V145, P1504, DOI 10.1121/1.5093538
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Turvey MT., 1977, PERCEIVING ACTING KN
   Vougioukas Konstantinos, 2018, PROC BRIT MACH VIS C
   Wang TC, 2021, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR46437.2021.00991
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang TZ, 2018, ADV NEUR IN, V31
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2021, IEEE T MULTIMEDIA, V23, P4027, DOI 10.1109/TMM.2020.3037461
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Wu Z., 2016, SSW, P202, DOI DOI 10.21437/SSW.2016-33
   Yi Ran, 2020, ARXIV200210137
   Young S., 2002, CAMBRIDGE UNIVERS EN, V3, P12
   Yu LY, 2021, IEEE T CIRC SYST VID, V31, P203, DOI 10.1109/TCSVT.2020.2973374
   Yu LY, 2019, IEEE T MULTIMEDIA, V21, P1621, DOI 10.1109/TMM.2018.2887027
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
NR 57
TC 6
Z9 7
U1 3
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2950
EP 2962
DI 10.1109/TMM.2021.3091863
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000021
DA 2024-07-18
ER

PT J
AU Zhong, YY
   Deng, WH
   Fang, H
   Hu, JI
   Zhao, DY
   Li, X
   Wen, DC
AF Zhong, Yaoyao
   Deng, Weihong
   Fang, Han
   Hu, Jiani
   Zhao, Dongyue
   Li, Xian
   Wen, Dongchao
TI Dynamic Training Data Dropout for Robust Deep Face Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Face recognition; Databases; Predictive models; Noise
   measurement; Training data; Data models; Face recognition; image
   recognition; noise measurement; supervised learning; training
ID VERIFICATION
AB Learning with noise is a practically challenging problem in deep face recognition. Despite the success of large margin softmax loss functions, these methods are designed for clean face databases. Considering the inevitable noise in the large scale databases, we first analyze the performance of noise in the training databases. For noise-robust deep face recognition, we propose a dynamic training data dropout (DTDD) method to dynamically filter the noise in the training database and gradually form a stable refined database for model learning. Specifically, we leverage the information provided by the model predictions of accumulated training epochs, which can distinguish regular samples and noise effectively and accurately. The proposed DTDD method is easy and stable for implementation, and can be combined with existing state-of-the-art loss functions and network architectures. Extensive experiments on CASIA-WebFace, VGGFace2, and MS-Celeb-1 M databases empirically demonstrate that our proposed method can robustly train deep face recognition models in the presence of label noise and low quality images.
C1 [Zhong, Yaoyao; Deng, Weihong; Fang, Han; Hu, Jiani] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Pattern Recognit & Intelligent Syst Lab, Beijing 100876, Peoples R China.
   [Zhao, Dongyue; Li, Xian; Wen, Dongchao] Canon Innovat Solut Beijing Co Ltd, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Deng, WH (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Pattern Recognit & Intelligent Syst Lab, Beijing 100876, Peoples R China.
EM zhongyaoyao@bupt.edu.cn; whdeng@bupt.edu.cn; fanghan@bupt.edu.cn;
   jnhu@bupt.edu.cn; zhaodongyue@canon-is.com.cn; lixian@canon-is.com.cn;
   wendongchao@canon-is.com.cn
RI Deng, Wei/GWC-9207-2022; fang, han/GSN-6404-2022
OI Deng, Weihong/0000-0001-5952-6996; Zhao, Dongyue/0000-0002-0135-0614;
   Wen, Dongchao/0000-0001-7311-1842; Zhong, Yaoyao/0000-0002-2671-9350
FU Canon Innovative Solution (Beijing) Company, Ltd. [OLA20011]; BUPT
   Excellent Ph.D.; Students Foundation [CX2020201]
FX This work was supported in part by Canon Innovative Solution (Beijing)
   Company, Ltd. under Grant OLA20011 and in part by BUPT Excellent Ph.D.
   Students Foundation under Grant CX2020201. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dan Zeng.
CR Algan G, 2021, KNOWL-BASED SYST, V215, DOI 10.1016/j.knosys.2021.106771
   [Anonymous], 2015, 3 INT C LEARN REPR I
   [Anonymous], 2020, ADV NEURAL INFORM PR
   [Anonymous], 2018, ABS180401159 CORR
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chen BH, 2017, PROC CVPR IEEE, P4021, DOI 10.1109/CVPR.2017.428
   Chen P., P AAAI C ART INT, V35, p11 442
   Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3
   Crosswhite N, 2018, IMAGE VISION COMPUT, V79, P35, DOI 10.1016/j.imavis.2018.09.002
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2017, IEEE COMPUT SOC CONF, P2006, DOI 10.1109/CVPRW.2017.251
   Deng WH, 2017, PATTERN RECOGN, V66, P63, DOI 10.1016/j.patcog.2016.11.023
   Du H., 2020, ECCV, P36
   Duan YQ, 2019, PROC CVPR IEEE, P3410, DOI 10.1109/CVPR.2019.00353
   Frénay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Ge SM, 2020, IEEE T CIRC SYST VID, V30, P3387, DOI 10.1109/TCSVT.2020.2967754
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Gold JR, 2017, PLAN HIST ENVIRON SE, P1
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2018, ADV NEUR IN, V31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu W, 2019, PROC CVPR IEEE, P11879, DOI 10.1109/CVPR.2019.01216
   Huang G. B., 2007, Technical Report, DOI 10.1.1. 122.8268
   Huang JC, 2019, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2019.00342
   Jiankang Deng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P741, DOI 10.1007/978-3-030-58621-8_43
   Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527
   Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803
   Koh PW, 2017, PR MACH LEARN RES, V70
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li SK, 2020, AAAI CONF ARTIF INTE, V34, P4667
   Liu H, 2019, PROC CVPR IEEE, P11939, DOI 10.1109/CVPR.2019.01222
   Liu J., 2015, ARXIV150607310
   Liu W, 2017, AIP CONF PROC, V1794, DOI 10.1063/1.4971938
   Liu WY, 2018, ADV NEUR IN, V31
   Liu Y., IEEE T CIRC SYST VID
   Lu C, 2015, AAAI CONF ARTIF INTE, P3811
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Nguyen D. T, 2020, P 8 INT C LEARN REPR, P741
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Pereyra G., 2017, ICLR WORKSH, P313
   Pleiss G., 2020, Advances in Neural Information Processing Systems, V33, P17044
   Ranjan R., 2017, ARXIV170309507
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sengupta S, 2016, IEEE WINT CONF APPL
   Sharma P, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901319
   Shi YC, 2019, IEEE I CONF COMP VIS, P6901, DOI 10.1109/ICCV.2019.00700
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sukhbaatar S., 2015, P 3 INT C LEARN REPR
   Sun YP, 2015, ADV DIFFER EQU-NY, P1, DOI 10.1186/s13662-015-0433-7
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Wang F, 2018, LECT NOTES COMPUT SC, V11213, P780, DOI 10.1007/978-3-030-01240-3_47
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081
   Wang XB, 2019, IEEE I CONF COMP VIS, P9357, DOI 10.1109/ICCV.2019.00945
   Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906
   Wang ZF, 2020, AAAI CONF ARTIF INTE, V34, P6340
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xie WD, 2018, LECT NOTES COMPUT SC, V11215, P811, DOI 10.1007/978-3-030-01252-6_48
   Xie Weidi, 2018, Proc. British Machine Vision Conference
   Yang JL, 2017, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2017.554
   Yi Dong, 2014, ARXIV14117923
   Yin X, 2019, PROC CVPR IEEE, P5697, DOI 10.1109/CVPR.2019.00585
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang X, 2019, PROC CVPR IEEE, P10815, DOI 10.1109/CVPR.2019.01108
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhang X, 2019, PROC CVPR IEEE, P9898, DOI 10.1109/CVPR.2019.01014
   Zhang Yang, 2020, IEEE C COMP VIS PATT
   Zheng T, 2018, BEIJING U POSTS TELE, V5, P7
   Zheng Tianyue, 2017, CROSS AGE LFW DATABA
   Zhong YY, 2019, PROC CVPR IEEE, P7804, DOI 10.1109/CVPR.2019.00800
NR 82
TC 8
Z9 9
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1186
EP 1197
DI 10.1109/TMM.2021.3123478
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800015
DA 2024-07-18
ER

PT J
AU Qian, SS
   Xue, DZ
   Fang, Q
   Xu, CS
AF Qian, Shengsheng
   Xue, Dizhan
   Fang, Quan
   Xu, Changsheng
TI Adaptive Label-Aware Graph Convolutional Networks for Cross-Modal
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Semantics; Task analysis; Adaptation models; Adaptive
   systems; Birds; Oceans; Cross-modal retrieval; Deep learning; Graph
   convolutional networks
AB The cross-modal retrieval task has raised continuous attention in recent years with the increasing scale of multi-modal data, which has broad application prospects including multimedia data management and intelligent search engine. Most existing methods mainly project data of different modalities into a common representation space where label information is often exploited to distinguish samples from different semantic categories. However, they typically treat each label as an independent individual and ignore the underlying semantic structure of labels. In this paper, we propose an end-to-end adaptive label-aware graph convolutional network (ALGCN) by designing both the instance representation learning branch and the label representation learning branch, which can obtain modality-invariant and discriminative representations for cross-modal retrieval. Firstly, we construct an instance representation learning branch to transform instances of different modalities into a common representation space. Secondly, we adopt Graph Convolutional Network (GCN) to learn inter-dependent classifiers in the label representation learning branch. In addition, a novel adaptive correlation matrix is proposed to efficiently explore and preserve the semantic structure of labels in a data-driven manner. Together with a robust self-supervision loss for GCN, the GCN model can be supervised to learn an effective and robust correlation matrix for feature propagation. Comprehensive experimental results on three benchmark datasets, NUS-WIDE, MIRFlickr and MS-COCO, demonstrate the superiority of ALGCN, compared with the state-of-the-art methods in cross-modal retrieval.
C1 [Qian, Shengsheng; Xue, Dizhan; Fang, Quan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Qian, Shengsheng; Xue, Dizhan; Fang, Quan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Guangdong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM shengsheng.qian@nlpr.ia.ac.cn; xuedizhan17@mails.ucas.ac.cn;
   qfang@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; Xue, Dizhan/KLJ-8222-2024; xu, cj/HJZ-3488-2023
OI Xue, Dizhan/0000-0002-0173-1556; xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2017YFB1002804];
   National Natural Science Foundation of China [62036012, 62072456,
   61720106006, 61572503, 61802405, 61872424, 61702509, 61832002, 61936005,
   U1705262]; Key Research Program of Frontier Sciences, CAS
   [QYZDJ-SSW-JSC039]; Open Research Projects of Zhejiang Laboratory
   [2021KE0AB05]; Tencent WeChat Rhino-Bird Focused Research Program
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB1002804, in part by the
   National Natural Science Foundation of China under Grants 62036012,
   62072456, 61720106006, 61572503, 61802405, 61872424, 61702509, 61832002,
   61936005, and U1705262, in part by the Key Research Program of Frontier
   Sciences, CAS, under Grant QYZDJ-SSW-JSC039, in part by the Open
   Research Projects of Zhejiang Laboratory umder Grant 2021KE0AB05, and in
   part by the Tencent WeChat Rhino-Bird Focused Research Program. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris.
CR Akaho S., 2001, INT M PSYCH SOC
   Andrew G., 2013, ICML, P1247
   Chen SC, 2010, INT J MULTIMED DATA, V1, P1, DOI 10.4018/jmdem.2010111201
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding BY, 2005, J COMPUT GRAPH STAT, V14, P280, DOI 10.1198/106186005X47697
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Hao YB, 2020, IEEE T MULTIMEDIA, V22, P188, DOI 10.1109/TMM.2019.2923121
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1157, DOI 10.1145/3343031.3350966
   Huet J., 2021, ARXIV210111552
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Joulin A., 2016, ARXIV
   Kingma D.P., 2014, ARXIV14126980
   Kipf TN, 2017, INT C LEARN REPR
   Kumar H, 2020, ACM INT CONF PR SER, P90, DOI 10.1145/3371158.3371169
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li D., 2003, P 11 ACM INT C MULTI, P604
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1379, DOI 10.1145/3397271.3401086
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Mikolov T., 2013, INT C LEARNING REPRE, P1
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Paszke A., 2017, NIPS W
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rumelhart David E., 1986, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, V1, P318
   Simonyan K., 2014, 14091556 ARXIV
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Srivastava Neelam., 2012, The Postcolonial Gramsci, P1
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tang J., 2018, ARXIV181112013
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang Y, 2020, AAAI CONF ARTIF INTE, V34, P12265
   Wang ZJ, 2021, IEEE T MULTIMEDIA, V23, P1274, DOI 10.1109/TMM.2020.2995267
   Wu DY, 2019, PROC CVPR IEEE, P9061, DOI 10.1109/CVPR.2019.00928
   Wu GS, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2854
   Wu M, 2020, IEEE DATA MINING, P681, DOI 10.1109/ICDM50108.2020.00077
   Wu M, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1457, DOI 10.1145/3366423.3380219
   Wu YL, 2020, IEEE T MULTIMEDIA, V22, P1310, DOI 10.1109/TMM.2019.2942494
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863
   Yang X, 2022, IEEE T PATTERN ANAL, V44, P1992, DOI 10.1109/TPAMI.2020.3026079
   Yang X, 2019, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2019.00419
   Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
NR 64
TC 16
Z9 17
U1 5
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 6
PY 2021
VL 24
BP 3520
EP 3532
DI 10.1109/TMM.2021.3101642
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NO
UT WOS:000824707300001
DA 2024-07-18
ER

PT J
AU Xu, K
   Jiang, XH
   Sun, TF
AF Xu, Ke
   Jiang, Xinghao
   Sun, Tanfeng
TI Gait Recognition Based on Local Graphical Skeleton Descriptor With
   Pairwise Similarity Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Feature extraction; Gait recognition; Dynamics; Legged
   locomotion; Deep learning; Three-dimensional displays; Gait recognition;
   skeleton; LGSD; pairwise similarity network
AB Gait recognition aims to identify a human through a walking sequence. It is a challenging task in computer vision since monocular camera loses most of the 3D information. Previous works described gait features with the contours of shape or the global geometrical characters of skeleton. So little work is researched on the local patterns of gait skeleton. In this paper, to resist the dress changes and speed changes, a Local Graphical Skeleton Descriptor (LGSD) is proposed to describe both the inner and intra local graphical patterns of a human gait skeleton. The gait features from the same or different identities are paired up and a Pairwise Similarity Network (PSN) is proposed to maximize the similarity of True matched pairs and minimize the similarity of False matched pairs. The contributions of our method are: 1) LGSD is proposed to describe human gait by computing four novel local geometrical patterns of skeleton sequences, which makes use of the intuitive cognition of gait based on the prior knowledge of mankind. 2) PSN is implemented by a two-stream CNN structure to build the gait model, which fused two popular gait recognition strategies. 3) The robustness of our method to dress changes and speed changes is proved on the public datasets. We have also achieved some state-of-the-art results on these datasets. The proposed method is examined on three public gait datasets which have RGB or infrared frames for evaluation: the CASIA-B dataset, the NLPR gait database, and the CASIA-C dataset. The performers in these datasets are walking under different views, speeds or dresses. The results are further compared with previous approaches to confirm the effectiveness and the advantages of our method.
C1 [Xu, Ke; Jiang, Xinghao] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
   [Sun, Tanfeng] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Natl Engn Lab Informat Content Anal Technol, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Jiang, XH (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM 113025816@sjtu.edu.cn; xhjiang@sjtu.edu.cn; tfsun@sjtu.edu.cn
FU Nature Natural Science Foundation of China [62002220]; China
   Postdoctoral Science Foundation [2020M671124, 2020T130413]
FX This work was supported in part by the Nature Natural Science Foundation
   of China under Grant 62002220 and in part by China Postdoctoral Science
   Foundation under Grants 2020M671124 and 2020T130413. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. Ferdous Sohel.
CR Ahmed M, 2014, PROC SPIE, V9139, DOI 10.1117/12.2052588
   [Anonymous], 2016, IEEE T PATTERN ANAL
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Castro FM, 2020, NEURAL COMPUT APPL, V32, P14173, DOI 10.1007/s00521-020-04811-z
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Fengjia Yang, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11858), P632, DOI 10.1007/978-3-030-31723-2_54
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Hu MD, 2013, IEEE T INF FOREN SEC, V8, P2034, DOI 10.1109/TIFS.2013.2287605
   Jia N, 2018, IET BIOMETRICS, V7, P287, DOI 10.1049/iet-bmt.2017.0151
   Kastaniotis D, 2015, PATTERN RECOGN LETT, V68, P327, DOI 10.1016/j.patrec.2015.06.020
   Kovac J, 2019, MULTIMED TOOLS APPL, V78, P5621, DOI 10.1007/s11042-017-5469-0
   Lam THW, 2006, LECT NOTES COMPUT SC, V3832, P612
   Liao RJ, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107069
   Makihara Yasushi, 2012, IPSJ Transactions on Computer Vision and Applications, V4, P42, DOI 10.2197/ipsjtcva.4.53
   Ming D, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P246, DOI 10.1109/VECIMS.2009.5068902
   Prathap C, 2015, 2015 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P2302, DOI 10.1109/ICACCI.2015.7275961
   Rijun Liao, 2017, Biometric Recognition. 12th Chinese Conference, CCBR 2017. Proceedings: LNCS 10568, P474, DOI 10.1007/978-3-319-69923-3_51
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Song CF, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106988
   Sun JD, 2018, MULTIMED TOOLS APPL, V77, P24909, DOI 10.1007/s11042-018-5722-1
   Tan DL, 2006, INT C PATT RECOG, P1000
   Teepe T., 2021, ARXIV210111228
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Wang W, 2016, IEEE IMAGE PROC, P3151, DOI 10.1109/ICIP.2016.7532940
   Xing XL, 2016, PATTERN RECOGN, V50, P107, DOI 10.1016/j.patcog.2015.08.011
   Yu SQ, 2017, IEEE COMPUT SOC CONF, P532, DOI 10.1109/CVPRW.2017.80
   Yu SQ, 2017, NEUROCOMPUTING, V239, P81, DOI 10.1016/j.neucom.2017.02.006
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zheng S, 2011, IEEE IMAGE PROC, DOI 10.1109/ICIP.2011.6115889
NR 31
TC 11
Z9 12
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 8
PY 2021
VL 24
BP 3265
EP 3275
DI 10.1109/TMM.2021.3095809
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NL
UT WOS:000824707000001
DA 2024-07-18
ER

PT J
AU Fan, B
   Liu, HM
   Zeng, H
   Zhang, JY
   Liu, X
   Han, JW
AF Fan, Bin
   Liu, Hongmin
   Zeng, Hui
   Zhang, Jiyong
   Liu, Xin
   Han, Junwei
TI Deep Unsupervised Binary Descriptor Learning Through Locality
   Consistency and Self Distinctiveness
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Machine learning; Robustness; Quantization (signal); Binary codes; Task
   analysis; Feature extraction; Training; Unsupervised learning; deep
   learning; pattern matching; feature extraction; compact descriptor;
   mobile applications; image retrieval
ID IMAGE; SELECTION; REPRESENTATIONS; DIFFERENCE; FEATURES; SCALE; CODES
AB Deep learning has been successfully applied to learn local feature descriptors in recent years. However, most of existing methods are supervised methods relying on a large number of labeled training patches, which are also proposed for learning real valued descriptors. In this paper, we propose a novel unsupervised deep learning method for binary descriptor learning. The binary descriptors are much more compact and efficient than the real valued descriptors and unsupervised leaning is highly required in many applications due to its label-free characteristic as the annotations are sometimes expensive to obtain. The core idea of our method is to explore the locality consistency in the descriptor space as well as to distinguish different patches while maintaining the ability to match a patch with its geometric transformed ones. We also give a theorical analysis about the role of batch normalization in learning effective binary descriptors. Benefited from this analysis, there is no need to append two additional losses on minimizing the quantization error and maximizing the entropy to the final learning objective like previous works did, thus simplifying our network training. Experiments on four benchmarks demonstrate that the proposed method is able to learn binary descriptors significantly outperforming previous unsupervised binary descriptors, even superior to most supervised ones. Especially, it obtains 21.2% of improvement on the UBC Phototour dataset, and 19.8%, 26.7%, 26.0% of improvements for patch verification, matching, retrieval tasks respectively on the HPatches dataset compared to the previous best unsupervised method.
C1 [Fan, Bin; Liu, Hongmin; Zeng, Hui] Univ Sci & Technol Beijing, Beijing Engn Res Ctr Ind Spectrum Imaging, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.
   [Liu, Hongmin] Henan Polytech Univ, Sch Comp Sci & Tech, Jiaozuo 454000, Henan, Peoples R China.
   [Zhang, Jiyong; Liu, Xin] Hangzhou Dianzi Univ, Hangzhou 310027, Peoples R China.
   [Han, Junwei] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
C3 University of Science & Technology Beijing; Henan Polytechnic
   University; Hangzhou Dianzi University; Northwestern Polytechnical
   University
RP Liu, HM (corresponding author), Univ Sci & Technol Beijing, Beijing Engn Res Ctr Ind Spectrum Imaging, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.
EM bin.fan@ieee.org; hmliu_82@163.com; hzeng@ustb.edu.cn;
   jzhang@hdu.edu.cn; xinliu@hdu.edu.cn; junweihan2010@gmail.com
RI Fan, Bin/HKN-3438-2023; ZHU, JIALI/JNE-3065-2023
OI Zhang, Jiyong/0000-0001-9600-8477; fan, bin/0000-0002-1155-467X; Liu,
   Hongmin/0000-0001-9834-4087
FU National Natural Science Foundation of China [61876180, 61973029,
   61573352]; Beijing Natural Science Foundation [4202073]; Henan
   University Scientific and Technological Innovation Team Support Program
   [19IRTSTHN012]; Young Elite Scientists Sponsorship Program by CAST
   [2018QNRC001]; Fundamental Research Funds for the Central Universities
   [FRF-BD-17-002A]
FX Manuscript received March 12, 2020; revised June 23, 2020 and August 2,
   2020; accepted August 7, 2020. Date of publication August 17, 2020; date
   of current versionAugust 24, 2021. Thisworkwas supported in part by
   theNational Natural Science Foundation of China under Grants 61876180,
   61973029, and 61573352, in part by the Beijing Natural Science
   Foundation under Grant 4202073, in part by the Henan University
   Scientific and Technological Innovation Team Support Program under Grant
   19IRTSTHN012, in part by the Young Elite Scientists Sponsorship Program
   by CAST under Grant 2018QNRC001, and in part by the Fundamental Research
   Funds for the Central Universities under Grant FRF-BD-17-002A. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr Mei-Ling Shyu. (Corresponding
   author: Hongmin Liu.)
CR Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   Bai JL, 2019, IEEE T MULTIMEDIA, V21, P3178, DOI 10.1109/TMM.2019.2920601
   Bai JL, 2020, IEEE T MULTIMEDIA, V22, P215, DOI 10.1109/TMM.2019.2922130
   Balntas V, 2018, IEEE T PATTERN ANAL, V40, P555, DOI 10.1109/TPAMI.2017.2679193
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Boix X, 2013, PROC CVPR IEEE, P2842, DOI 10.1109/CVPR.2013.366
   Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Duan YQ, 2018, PROC CVPR IEEE, P8270, DOI 10.1109/CVPR.2018.00863
   Duan YQ, 2017, PROC CVPR IEEE, P4857, DOI 10.1109/CVPR.2017.516
   Fan B, 2019, IEEE T IMAGE PROCESS, V28, P4774, DOI 10.1109/TIP.2019.2909640
   Fan B, 2014, IEEE T IMAGE PROCESS, V23, P2583, DOI 10.1109/TIP.2014.2317981
   Gao YQ, 2015, IEEE T IMAGE PROCESS, V24, P4820, DOI 10.1109/TIP.2015.2469093
   He K, 2018, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2018.00069
   He K, 2018, PROC CVPR IEEE, P4023, DOI 10.1109/CVPR.2018.00423
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Kang C, 2019, IEEE T MULTIMEDIA, V21, P1563, DOI 10.1109/TMM.2018.2883868
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, IEEE INT CONF MULTI
   Kuanar S, 2018, PICT COD SYMP, P164, DOI 10.1109/PCS.2018.8456278
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li WJ, 2016, IJCAI, P1711
   Lin K, 2019, IEEE T PATTERN ANAL, V41, P1501, DOI 10.1109/TPAMI.2018.2833865
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu SG, 2019, IEEE T MULTIMEDIA, V21, P2461, DOI 10.1109/TMM.2019.2903413
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma C, 2020, IEEE T MULTIMEDIA, V22, P760, DOI 10.1109/TMM.2019.2931808
   Ma L, 2017, IEEE T MULTIMEDIA, V19, P2545, DOI 10.1109/TMM.2017.2703089
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mishchuk A., 2017, P ADV NEURAL INFORM, P4826
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Paulin M, 2015, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2015.19
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Ren JF, 2017, IEEE T MULTIMEDIA, V19, P447, DOI 10.1109/TMM.2016.2618218
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Spinoulas L, 2015, IEEE COMPUT SOC CONF
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Trzcinski T, 2015, IEEE T PATTERN ANAL, V37, P597, DOI 10.1109/TPAMI.2014.2343961
   Trzcinski T, 2012, LECT NOTES COMPUT SC, V7572, P228, DOI 10.1007/978-3-642-33718-5_17
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang ZH, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.16
   Wang ZH, 2016, IEEE T PATTERN ANAL, V38, P2198, DOI 10.1109/TPAMI.2015.2513396
   Wang ZH, 2014, LECT NOTES COMPUT SC, V8695, P94, DOI 10.1007/978-3-319-10584-0_7
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang X, 2014, IEEE T PATTERN ANAL, V36, P188, DOI 10.1109/TPAMI.2013.150
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhang LG, 2019, IEEE I CONF COMP VIS, P2969, DOI 10.1109/ICCV.2019.00306
   Zhang W, 2018, IEEE T MULTIMEDIA, V20, P880, DOI 10.1109/TMM.2017.2760102
   Zheng J, 2019, IEEE T MULTIMEDIA, V21, P2561, DOI 10.1109/TMM.2019.2905692
   Zieba M., 2018, ADV NEURAL INFORM PR, P3608
NR 65
TC 28
Z9 30
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2770
EP 2781
DI 10.1109/TMM.2020.3016122
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600018
DA 2024-07-18
ER

PT J
AU Guarda, AFR
   Rodrigues, NMM
   Pereira, F
AF Guarda, Andre F. R.
   Rodrigues, Nuno M. M.
   Pereira, Fernando
TI Constant Size Point Cloud Clustering: A Compact, Non-Overlapping
   Solution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Clustering algorithms; Clustering methods;
   Transform coding; Encoding; Image segmentation; Complexity theory; Point
   cloud; point cloud clustering; constant size; compactness;
   non-overlapping
ID COMPRESSION; REPRESENTATION
AB Point clouds have recently become a popular 3D representation model for many application domains, notably virtual and augmented reality. Since point cloud data is often very large, processing a point cloud may require that it be segmented into smaller clusters. For example, the input to deep learning-based methods like auto-encoders should be constant size point cloud clusters, which are ideally compact and non-overlapping. However, given the unorganized nature of point clouds, defining the specific data segments to code is not always trivial. This paper proposes a point cloud clustering algorithm which targets five main goals: i) clusters with a constant number of points; ii) compact clusters, i.e., with low dispersion; iii) non-overlapping clusters, i.e., not intersecting each other; iv) ability to scale with the number of points; and v) low complexity. After appropriate initialization, the proposed algorithm transfers points between neighboring clusters as a propagation wave, filling or emptying clusters until they achieve the same size. The proposed algorithm is unique since there is no other point cloud clustering method available in the literature offering the same clustering features for large point clouds at such low complexity.
C1 [Guarda, Andre F. R.; Pereira, Fernando] Univ Lisbon, Inst Super Tecn, P-1649004 Lisbon, Portugal.
   [Guarda, Andre F. R.; Pereira, Fernando] Inst Telecomunicacoes, P-1649004 Lisbon, Portugal.
   [Rodrigues, Nuno M. M.] Inst Politecn Leiria, ESTG, P-1049001 Leiria, Portugal.
   [Rodrigues, Nuno M. M.] Inst Telecomunicacoes, P-1049001 Leiria, Portugal.
C3 Universidade de Lisboa; Instituto de Telecomunicacoes; Polytechnic
   Institute of Leiria
RP Guarda, AFR (corresponding author), Univ Lisbon, Inst Super Tecn, P-1649004 Lisbon, Portugal.; Guarda, AFR (corresponding author), Inst Telecomunicacoes, P-1649004 Lisbon, Portugal.
EM andre.guarda@lx.it.pt; nuno.rodrigues@co.it.pt; fp@lx.it.pt
RI Pereira, Fernando/K-4046-2012; Rodrigues, Nuno M. M./AFO-6998-2022;
   Pereira, Fernando/HNR-7786-2023
OI Rodrigues, Nuno M. M./0000-0001-9536-1017; Bernardo Pereira, Fernando
   Manuel/0000-0001-6100-947X; Guarda, Andre/0000-0001-5996-1074
FU Fundacao para a Ciencia e Tecnologia (FCT), Portugal
   [SFRH/BD/118218/2016]; FCT/MEC; FEDER - PT2020 partnership Agreement
   [UID/EEA/50008/2019]; Fundação para a Ciência e a Tecnologia
   [SFRH/BD/118218/2016] Funding Source: FCT
FX This work was supported in part by Fundacao para a Ciencia e Tecnologia
   (FCT), Portugal, under Grant SFRH/BD/118218/2016, and in part by FCT/MEC
   through national funds and when applicable co-funded by FEDER - PT2020
   partnership Agreement under Project UID/EEA/50008/2019.
CR Achlioptas P., 2018, LEARNING REPRESENTAT
   Althoff T., 2011, BALANCED CLUSTERING
   Nguyen A, 2013, PROCEEDINGS OF THE 2013 6TH IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P225, DOI 10.1109/RAM.2013.6758588
   [Anonymous], 2018, MATLAB COMP VIS SYST
   [Anonymous], 2016, GENERATIVE DISCRIMIN
   [Anonymous], 2018, DEEP K MEANS JOINTLY
   [Anonymous], 2009, THESIS
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880
   Banerjee A, 2006, DATA MIN KNOWL DISC, V13, P365, DOI 10.1007/s10618-006-0040-z
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Conti C, 2018, IEEE T MULTIMEDIA, V20, P2905, DOI 10.1109/TMM.2018.2825882
   Cupec R., 2016, ARP3DTR1 U OS FAC EL
   Ebrahimi T., 2018, P IEEE INT C MULT EX
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   Grilli E, 2017, INT ARCH PHOTOGRAMM, V42-2, P339, DOI 10.5194/isprs-archives-XLII-2-W3-339-2017
   Guarda A. F. R., C2NO
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Koberstein A, 2008, COMPUT OPTIM APPL, V41, P185, DOI 10.1007/s10589-008-9207-4
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Malinen MI, 2014, LECT NOTES COMPUT SC, V8621, P32, DOI 10.1007/978-3-662-44415-3_4
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Mekuria R, 2014, IEEE T MULTIMEDIA, V16, P1809, DOI 10.1109/TMM.2014.2331919
   MPEG, 2017, N16763 MPEG
   Multimedia Signal Processing Group Instituto de Telecomunicacoes Lx, CONST SIZ POINT CLOU
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Rokach L, 2005, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, P321, DOI 10.1007/0-387-25465-X_15
   Schubert E., 2015, Proceedings of the VLDB Endowment, V8, P1976, DOI DOI 10.14778/2824032.2824115
   Schwarz S., 2018, N17345 MPEG
   Sugimoto K, 2017, ASIAPAC SIGN INFO PR, P364, DOI 10.1109/APSIPA.2017.8282059
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tang W, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11030338
   Tian K, 2017, LECT NOTES ARTIF INT, V10535, P809, DOI 10.1007/978-3-319-71246-8_49
   Vlachos E, 2018, IEEE T MULTIMEDIA, V20, P3276, DOI 10.1109/TMM.2018.2839911
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yan X. C., SAME SIZE K MEANS CL
   Yang Y., 2018, FOLDINGNET POINT CLO
NR 43
TC 8
Z9 8
U1 8
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 77
EP 91
DI 10.1109/TMM.2020.2974325
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600007
DA 2024-07-18
ER

PT J
AU He, WG
   Cai, ZC
   Wang, YM
AF He, Wenguang
   Cai, Zhanchuan
   Wang, Yaomin
TI High-Fidelity Reversible Image Watermarking Based on Effective
   Prediction Error-Pairs Modification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Two dimensional displays; Histograms; Distortion; Watermarking;
   Biomedical imaging; Media; Correlation; Reversible image watermarking;
   pairwise prediction-error expansion; content-dependent; adaptive 2D
   mapping; flexible spatial location
ID DIFFERENCE EXPANSION
AB In reversible watermarking for image authentication, less degradation of the marked image is always desirable. For minimum distortion, the pairwise prediction-error expansion (PEE) technique was recently proposed to modify errors jointly. Although its superiority over conventional PEE has been verified, its potential has not been fully exploited yet. In this paper, we focus on optimal modification and propose an enhanced pairwise PEE. First, it is observed in PVO-based pairwise PEE that the histogram peak varies with relative location when predicting the largest/smallest two pixels. Then, a more effective 2D mapping is proposed by content-dependently selecting the expansion bin after introducing spatial location into prediction. Next, the 2D mapping is further extended considering prediction in non-smooth region tends to produce errors with large magnitude. Finally, we also propose to flexibly define the spatial location to achieve content-dependent prediction and further enhancement. Experimental results demonstrate that the proposed scheme achieves better capacity-distortion trade-off and outperforms several state-of-the-art schemes.
C1 [He, Wenguang; Cai, Zhanchuan; Wang, Yaomin] Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
   [He, Wenguang; Wang, Yaomin] Guangdong Med Univ, Sch Biomed Engn, Guangzhou 524023, Guangdong, Peoples R China.
C3 Macau University of Science & Technology; Guangdong Medical University
RP Cai, ZC (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
EM 56207403@qq.com; zccai@must.edu.mo; 249668530@qq.com
OI , Yaomin/0000-0003-4696-0907; he, wenguang/0000-0003-1051-389X
FU National Natural Science Foundation of China [61802074]; Guangdong Basic
   and Applied Basic Research Foundation [2020A1515010760]; Medical
   Scientific Research Fund of Guangdong Province, China [A2018186];
   National Basic Research Program of China (973 Program) [2011CB302400];
   Science and Technology Development Fund of Macau [0012/2018/A1,
   0069/2018/A2]; Open Project Program of State Key Laboratory of Virtual
   Reality Technology and Systems, Beihang University [VRLAB2019C02]; Open
   Fund of the State Key Laboratory of Remote Sensing Science
   [OFSLRSS201901]; Open Project Program of the State Key Lab of CAD & CG
   of Zhejiang University [A1910]; Guangdong Provincial College Innovative
   Projects [2018KQNCX346, 2018KTSCX300]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61802074, in part by Guangdong Basic and
   Applied Basic Research Foundation (2020A1515010760), in part by the
   Medical Scientific Research Fund of Guangdong Province, China (no.
   A2018186), in part by the National Basic Research Program of China (973
   Program) underGrant 2011CB302400, in part by the Science and Technology
   Development Fund of Macau under Grants 0012/2018/A1 and 0069/2018/A2, in
   part by the Open Project Program of State Key Laboratory of Virtual
   Reality Technology and Systems, Beihang University under Grant
   VRLAB2019C02, in part by the Open Fund of the State Key Laboratory of
   Remote Sensing Science under Grant OFSLRSS201901, in part by the Open
   Project Program of the State Key Lab of CAD & CG of Zhejiang University
   under Grant A1910, and in part by Guangdong Provincial College
   Innovative Projects no.2018KQNCX346 and 2018KTSCX300.
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   Awranjeb M, 2005, J ELECTRON IMAGING, V14, DOI 10.1117/1.1877523
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Coltuc D, 2007, IEEE SIGNAL PROC LET, V14, P255, DOI 10.1109/LSP.2006.884895
   Coltuc D, 2012, IEEE T IMAGE PROCESS, V21, P412, DOI 10.1109/TIP.2011.2162424
   Coltuc D, 2011, IEEE T INF FOREN SEC, V6, P873, DOI 10.1109/TIFS.2011.2145372
   Dragoi IC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549458
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   Fridrich J, 2002, EURASIP J APPL SIG P, V2002, P185, DOI 10.1155/S1110865702000537
   Gao XB, 2011, IEEE T CIRC SYST VID, V21, P1061, DOI 10.1109/TCSVT.2011.2130410
   He WG, 2018, INFORM SCIENCES, V467, P784, DOI 10.1016/j.ins.2018.04.088
   He WG, 2017, J VIS COMMUN IMAGE R, V46, P58, DOI 10.1016/j.jvcir.2017.03.010
   Hong W, 2012, OPT COMMUN, V285, P101, DOI 10.1016/j.optcom.2011.09.005
   Hu YJ, 2009, IEEE T CIRC SYST VID, V19, P250, DOI 10.1109/TCSVT.2008.2009252
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Lee SK, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1321, DOI 10.1109/icme.2006.262782
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Li XL, 2013, IEEE T INF FOREN SEC, V8, P1091, DOI 10.1109/TIFS.2013.2261062
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Li XL, 2013, SIGNAL PROCESS, V93, P198, DOI 10.1016/j.sigpro.2012.07.025
   Li XL, 2011, IEEE T IMAGE PROCESS, V20, P3524, DOI 10.1109/TIP.2011.2150233
   Luo LX, 2010, IEEE T INF FOREN SEC, V5, P187, DOI 10.1109/TIFS.2009.2035975
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2016, J VIS COMMUN IMAGE R, V39, P12, DOI 10.1016/j.jvcir.2016.05.005
   Ou B, 2016, J VIS COMMUN IMAGE R, V38, P328, DOI 10.1016/j.jvcir.2016.03.011
   Ou B, 2014, SIGNAL PROCESS-IMAGE, V29, P760, DOI 10.1016/j.image.2014.05.003
   Ou B, 2013, J SYST SOFTWARE, V86, P2700, DOI 10.1016/j.jss.2013.05.077
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Pei QQ, 2013, J SYST SOFTWARE, V86, P2841, DOI 10.1016/j.jss.2013.06.055
   Peng F, 2014, DIGIT SIGNAL PROCESS, V25, P255, DOI 10.1016/j.dsp.2013.11.002
   Qu X, 2015, SIGNAL PROCESS, V111, P249, DOI 10.1016/j.sigpro.2015.01.002
   Rad Reza Moradi, 2014, IEEE Trans Image Process, V23, P1463, DOI 10.1109/TIP.2014.2302681
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Tsai P, 2009, SIGNAL PROCESS, V89, P1129, DOI 10.1016/j.sigpro.2008.12.017
   Wang X, 2015, INFORM SCIENCES, V310, P16, DOI 10.1016/j.ins.2015.03.022
   Wang X, 2010, IEEE SIGNAL PROC LET, V17, P567, DOI 10.1109/LSP.2010.2046930
   Weng SW, 2019, INFORM SCIENCES, V489, P136, DOI 10.1016/j.ins.2019.03.032
   Weng SW, 2008, IEEE SIGNAL PROC LET, V15, P721, DOI 10.1109/LSP.2008.2001984
   Wu HT, 2012, SIGNAL PROCESS, V92, P3000, DOI 10.1016/j.sigpro.2012.05.034
NR 41
TC 24
Z9 25
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 52
EP 63
DI 10.1109/TMM.2020.2982042
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600005
DA 2024-07-18
ER

PT J
AU Hidayati, SC
   Goh, TW
   Chan, JSG
   Hsu, CC
   See, J
   Wong, LK
   Hua, KL
   Tsao, Y
   Cheng, WH
AF Hidayati, Shintami Chusnul
   Goh, Ting Wei
   Chan, Ji-Sheng Gary
   Hsu, Cheng-Chun
   See, John
   Wong, Lai-Kuan
   Hua, Kai-Lung
   Tsao, Yu
   Cheng, Wen-Huang
TI Dress With Style: Learning Style From Joint Deep Embedding of Clothing
   Styles and Body Shapes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Shape; Correlation; Shape measurement; Task analysis; Big
   Data; Semantics; Fashion analysis; recommender system; human body shape;
   clothing style; correlation
ID RETRIEVAL; COLOR
AB Body shape is about proportion, and fashion style is all about dressing those proportions to look their very best. Figuring out the styles to suit a body shape can be a daunting task for many people. It is, therefore, essential to develop a framework for learning the compatibility of body shapes and clothing styles. Though fashion designers and fashion stylists have analyzed the correlation between human body shapes and fashion styles for a long time, this issue did not receive much attention in multimedia science. In this paper, we present a novel style recommender, on the basis of the user's body attributes. The rich amount of fashion styling knowledge from social big data is exploited for this purpose. We first construct a joint embedding of clothing styles and human body measurements with deep multimodal representation learning on a reference dataset that has been sorted to meet the fashion rules. We then discover the relevant semantic features by propagation and selection in clothing style and body shape graphs. Experiments demonstrate the effectiveness of the proposed framework when compared with several baseline methods.
C1 [Hidayati, Shintami Chusnul] Inst Teknol Sepuluh Nopember, Dept Informat, Surabaya 60111, Indonesia.
   [Goh, Ting Wei; Chan, Ji-Sheng Gary; See, John; Wong, Lai-Kuan] Multimedia Univ, Fac Comp & Informat, Cyberjaya 63100, Malaysia.
   [Hsu, Cheng-Chun; Hua, Kai-Lung] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Cheng, Wen-Huang] Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
   [Cheng, Wen-Huang] Natl Chiao Tung Univ, Inst Elect, Hsinchu 300, Taiwan.
C3 Institut Teknologi Sepuluh Nopember; Multimedia University; National
   Taiwan University of Science & Technology; Academia Sinica - Taiwan;
   National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University
RP Cheng, WH (corresponding author), Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu 300, Taiwan.; Cheng, WH (corresponding author), Natl Chiao Tung Univ, Inst Elect, Hsinchu 300, Taiwan.
EM shintami@its.ac.id; tingwei.goh@gmail.com; garychan6015@gmail.com;
   b10415009@mail.ntust.edu.tw; johnsee@mmu.edu.my; lkwong@mmu.edu.my;
   hua@mail.ntust.edu.tw; yu.tsao@citi.sinica.edu.tw; whcheng@nctu.edu.tw
RI Wong, Lai Kuan/AAO-7014-2021; Tsao, Yu/AAP-4779-2020; See,
   John/C-8633-2013; Hidayati, Shintami Chusnul/AAK-7047-2020
OI Wong, Lai Kuan/0000-0002-4517-0391; Tsao, Yu/0000-0001-6956-0418; See,
   John/0000-0003-3005-4109; Hidayati, Shintami
   Chusnul/0000-0001-5045-4842; Chan, Ji Sheng Gary/0000-0003-1525-9018;
   Hua, Kai-Lung/0000-0002-7735-243X
FU Ministry of Science and Technology of Taiwan [MOST-108-2218-E-009-056,
   MOST-108-2745-8-009-002, MOST-108-2218-E-002-055,
   MOST-109-2634-F-007-013]
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under Grants MOST-108-2218-E-009-056,
   MOST-108-2745-8-009-002, MOST-108-2218-E-002-055, and
   MOST-109-2634-F-007-013.
CR [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2012, P 20 ACM INT C MULT
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bayou B., 2006, SCI SEXY DRESS FIT Y
   Collings Kat, 2018, FOOLPROOF WAY FIND O
   Francis C., 2017, BODY SHAPE CALCULATO
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486
   Hidayati SC, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P471, DOI 10.1109/MIPR.2019.00095
   Hidayati SC, 2018, IEEE T CYBERNETICS, V48, P1647, DOI 10.1109/TCYB.2017.2712634
   Hidayati SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P438, DOI 10.1145/3240508.3240546
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Kelly, 2011, PETITE BODY TYPES
   Kelly Clinton., 2005, Dress Your Best: The Complete Guide To Finding The Style That's Right For Your Body
   Kindes M., 2006, Body image: new research
   Krakauer NY, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0039504
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   Larkey J., 1992, FLATTER YOUR FIGURE
   LeCun Y., 1995, The handbook of brain theory and neural networks, V3361, DOI [10.5555/303568.303704, DOI 10.5555/303568.303704]
   Lee JY, 2007, INT J CLOTH SCI TECH, V19, P374, DOI 10.1108/09556220710819555
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   London S., 2013, TRUTH STYLE
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   McCormack H., 2005, The shape of things to wear: scientists identifyhowwomen'sfigureshavechangedin50years
   Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21
   Sanchez-Riera J, 2017, LECT NOTES COMPUT SC, V10132, P662, DOI 10.1007/978-3-319-51811-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun GL, 2018, MULTIMED TOOLS APPL, V77, P17731, DOI 10.1007/s11042-017-5245-1
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Vijayanarasimhan S, 2011, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2011.5995545
   Vukotic V, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P343, DOI 10.1145/2911996.2912064
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Xie HT, 2019, IEEE T MULTIMEDIA, V21, P1248, DOI 10.1109/TMM.2018.2872898
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
NR 44
TC 29
Z9 30
U1 3
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 365
EP 377
DI 10.1109/TMM.2020.2980195
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600029
DA 2024-07-18
ER

PT J
AU Jiang, XH
   Zhang, L
   Zhang, TZ
   Lv, P
   Zhou, B
   Pang, YW
   Xu, ML
   Xu, CS
AF Jiang, Xiaoheng
   Zhang, Li
   Zhang, Tianzhu
   Lv, Pei
   Zhou, Bing
   Pang, Yanwei
   Xu, Mingliang
   Xu, Changsheng
TI Density-Aware Multi-Task Learning for Crowd Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Semantics; Estimation; Feature extraction; Convolutional
   neural networks; Cameras; Head; Convolutional neural network; crowd
   counting; density-level classification; density map estimation;
   multi-task learning
ID DEEP
AB In this paper, we present a method called density-aware convolutional neural network (DensityCNN) to perform the crowd counting task in various crowded scenes. The key idea of the DensityCNN is to utilize high-level semantic information to provide guidance and constraint when generating density maps. To this end, we implement the DensityCNN by adopting a multi-task CNN structure to jointly learn density-level classification and density map estimation. The density-level classification task learns multi-channel semantic features that are aware of the density distributions of the input image. This task is accomplished via our specially designed group-based convolutional structure in a supervised learning manner. In the density map estimation task, these semantic features are deployed together with high-dimension convolutional features to generate density maps with lower count errors. Extensive experiments on four challenging crowd datasets (ShanghaiTech, UCF_CC_50, UCF-QNCF, and WorldExpo'10) and one vehicle dataset TRANCOS demonstrate the effectiveness of the proposed method.
C1 [Jiang, Xiaoheng; Zhang, Li; Lv, Pei; Zhou, Bing; Xu, Mingliang] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Peoples R China.
   [Zhang, Tianzhu] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Pang, Yanwei] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Zhengzhou University; Chinese Academy of Sciences; University of Science
   & Technology of China, CAS; Tianjin University; Chinese Academy of
   Sciences; Institute of Automation, CAS
RP Xu, ML (corresponding author), Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Peoples R China.
EM jiangxiaoheng@zzu.edu.cn; laridzhang@gmail.com; tzzhang@ustc.edu.cn;
   ielvpei@zzu.edu.cn; iebzhou@zzu.edu.cn; pyw@tju.edu.cn;
   iexumingliang@zzu.edu.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022; Zhang,
   Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106; , Pei/0000-0002-2654-0561; Zhang,
   Li/0000-0003-1535-3960; zhang, tian zhu/0000-0003-1856-9564; xu, chang
   sheng/0000-0001-8343-9665; ZHOU, BING/0000-0003-3446-3903
FU National Natural Science Foundation of China [61802351, 61822701,
   61872324, 61772474]; China Postdoctoral Science Foundation
   [2018M632802]; Key R&D and Promotion Projects in Henan Province
   [192102310258]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61802351, 61822701, 61872324, and
   61772474, in part by China Postdoctoral Science Foundation under Grant
   2018M632802, and in part by Key R&D and Promotion Projects in Henan
   Province under Grant 192102310258.
CR Arteta C, 2016, LECT NOTES COMPUT SC, V9911, P483, DOI 10.1007/978-3-319-46478-7_30
   Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300
   Cao JL, 2017, IEEE T IMAGE PROCESS, V26, P3210, DOI 10.1109/TIP.2017.2694224
   Cao JL, 2016, IEEE T IMAGE PROCESS, V25, P5538, DOI 10.1109/TIP.2016.2609807
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dong L, 2018, IEEE T MULTIMEDIA, V20, P2012, DOI 10.1109/TMM.2017.2788205
   Fiaschi L, 2012, INT C PATT RECOG, P2685
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Guerrero-Gómez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jiang X., 2019, IEEE T NEUR NET LEAR, DOI [10.1109/TNNLS.2019.2933, DOI 10.1109/TNNLS.2019.2933]
   Jiang XT, 2018, IEEE T HUM-MACH SYST, V48, P219, DOI [10.1109/THMS.2017.2693245, 10.1109/TNNLS.2017.2689098]
   Kang D, 2019, IEEE T CIRC SYST VID, V29, P1408, DOI 10.1109/TCSVT.2018.2837153
   Kingma D. P., 2014, arXiv
   Kumagai S, 2018, MACH VISION APPL, V29, P1119, DOI 10.1007/s00138-018-0955-6
   Leibe B, 2005, PROC CVPR IEEE, P878
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li YK, 2018, IEEE T MULTIMEDIA, V20, P3289, DOI 10.1109/TMM.2018.2834873
   Liu C, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON SMART CITY AND SYSTEMS ENGINEERING (ICSCSE), P849, DOI 10.1109/ICSCSE.2018.00183
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XL, 2019, IEEE T PATTERN ANAL, V41, P1862, DOI 10.1109/TPAMI.2019.2899857
   Marsden M, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P27, DOI 10.5220/0006097300270033
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Pang YW, 2018, IEEE T NEUR NET LEAR, V29, P1587, DOI 10.1109/TNNLS.2017.2676130
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shang C, 2016, IEEE IMAGE PROC, P1215, DOI 10.1109/ICIP.2016.7532551
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P2303, DOI 10.1109/TNNLS.2014.2308519
   Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550
   Shi ZL, 2018, PROC CVPR IEEE, P5382, DOI 10.1109/CVPR.2018.00564
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindagi V. A., 2017, 2017 14 IEEE INT C A, P1
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Xu ML, 2019, PATTERN RECOGN LETT, V125, P563, DOI 10.1016/j.patrec.2019.02.026
   Xu ML, 2018, IEEE T CIRC SYST VID, V28, P2814, DOI 10.1109/TCSVT.2017.2731866
   Xu ML, 2017, IEEE T IMAGE PROCESS, V26, P5811, DOI 10.1109/TIP.2017.2737321
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang G, 2017, IEEE IMAGE PROC, P3675, DOI 10.1109/ICIP.2017.8296968
   Zhang L, 2018, IEEE WINT CONF APPL, P1113, DOI 10.1109/WACV.2018.00127
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhu Y, 2019, PROC CVPR IEEE, P8848, DOI 10.1109/CVPR.2019.00906
NR 48
TC 45
Z9 49
U1 1
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 443
EP 453
DI 10.1109/TMM.2020.2980945
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600035
DA 2024-07-18
ER

PT J
AU Lim, S
   Kim, W
AF Lim, Seokjae
   Kim, Wonjun
TI DSLR: Deep Stacked Laplacian Restorer for Low-Light Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Laplace equations; Image restoration; Lighting; Image enhancement;
   Visualization; Image color analysis; Histograms; Low-light image
   enhancement; Laplacian pyramid; deep-stacked laplacian restorer (DSLR);
   decomposition-based scheme
ID HISTOGRAM; RETINEX
AB Various images captured in complicated lighting conditions often suffer from deterioration of the image quality. Such poor quality not only dissatisfies the user expectation but also may lead to a significant performance drop in many applications. In this paper, anovel method for low-light image enhancement is proposed by leveraging useful propertiesof the Laplacian pyramid both in image and feature spaces. Specifically, the proposed method, so-called a deep stacked Laplacian restorer (DSLR), is capable of separately recovering the global illumination and local details from the original input, and progressively combining them in the image space. Moreover, the Laplacian pyramid defined in the feature space makes such recovering processes more efficient based on abundant connectionsof higher-order residuals in a multiscale structure. This decomposition-based scheme is fairly desirable for learning the highly nonlinear relation between degraded images and their enhanced results. Experimental results on various datasets demonstrate that the proposed DSLR outperforms state-of-the-art methods. The code and model are publicly available at: https://github.com/SeokjaeLIM/DSLR-release.
C1 [Lim, Seokjae; Kim, Wonjun] Konkuk Univ, Dept Elect & Elect Engn, Seoul 05029, South Korea.
C3 Konkuk University
RP Kim, W (corresponding author), Konkuk Univ, Dept Elect & Elect Engn, Seoul 05029, South Korea.
EM hgg08@konkuk.ac.kr; wonjkim@konkuk.ac.kr
RI Kim, Wonjun/JXN-3386-2024
OI Lim, Seokjae/0000-0001-5633-645X
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [2020R1F1A1068080]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MSIT) (No.
   2020R1F1A1068080).
CR [Anonymous], 2016, ARXIV E PRINTS
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Han JH, 2011, IEEE T IMAGE PROCESS, V20, P506, DOI 10.1109/TIP.2010.2068555
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jang JH, 2012, IEEE T IMAGE PROCESS, V21, P3479, DOI 10.1109/TIP.2012.2197014
   Jiang Y, 2019, ARXIV PREPRINT ARXIV
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kim W, 2019, IEEE ACCESS, V7, P129150, DOI 10.1109/ACCESS.2019.2940452
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Kingma D. P., 2014, arXiv
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Paszke A., 2017, NIPS W
   Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Singhal A., 2001, IEEE DATA ENG B, V24, P35
   Sun CC, 2005, IEEE T CONSUM ELECTR, V51, P1300
   Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Yuanming Hu, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3181974
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
NR 50
TC 116
Z9 129
U1 10
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4272
EP 4284
DI 10.1109/TMM.2020.3039361
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900028
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Pan, LM
   Chen, JJ
   Liu, ST
   Ngo, CW
   Kan, MY
   Chua, TS
AF Pan, Liangming
   Chen, Jingjing
   Liu, Shaoteng
   Ngo, Chong-Wah
   Kan, Min-Yen
   Chua, Tat-Seng
TI A Hybrid Approach for Detecting Prerequisite Relations in Multi-Modal
   Food Recipes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Training; Task analysis; Semantics; Pipelines; Deep
   learning; Predictive models; Food recipes; cooking workflow;
   prerequisite trees; multi-modal fusion; cause-and-effect reasoning; deep
   learning
AB Modeling the structure of culinary recipes is the core of recipe representation learning. Current approaches mostly focus on extracting the workflow graph from recipes based on text descriptions. Process images, which constitute an important part of cooking recipes, has rarely been investigated in recipe structure modeling. We study this recipe structure problem from a multi-modal learning perspective, by proposing a prerequisite tree to represent recipes with cooking images at a step-level granularity. We propose a simple-yet-effective two-stage framework to automatically construct the prerequisite tree for a recipe by (1) utilizing a trained classifier to detect pairwise prerequisite relations that fuses multi-modal features as input; then (2) applying different strategies (greedy method, maximum weight, and beam search) to build the tree structure. Experiments on the MM-ReS dataset demonstrates the advantages of introducing process images for recipe structure modeling. Also, compared with neural methods which require large numbers of training data, we show that our two-stage pipeline can achieve promising results using only 400 labeled prerequisite trees as training data.
C1 [Pan, Liangming; Kan, Min-Yen; Chua, Tat-Seng] Natl Univ Singapore, Singapore 117417, Singapore.
   [Chen, Jingjing] Fudan Univ, Shanghai 200438, Peoples R China.
   [Ngo, Chong-Wah] City Univ Hong Kong, Hong Kong 518057, Peoples R China.
   [Liu, Shaoteng] Xi An Jiao Tong Univ, Xian 215123, Shanxi, Peoples R China.
C3 National University of Singapore; Fudan University; City University of
   Hong Kong; Xi'an Jiaotong University
RP Chen, JJ (corresponding author), Fudan Univ, Shanghai 200438, Peoples R China.
EM e0272310@u.nus.edu; chenjingjing@fudan.edu.cn; ls2662@stu.xjtu.edu.cn;
   cscwngo@cityu.edu.hk; kanmy@comp.nus.edu.sg; dcscts@nus.edu.sg
RI chen, JJ/HGB-6029-2022
FU NSF China [62072116]; National Research Foundation, Singapore, under its
   International Research Centres in Singapore Funding Initiative
FX This work was supported in part by the NSF China under Grant 62072116
   and in part by the National Research Foundation, Singapore, under its
   International Research Centres in Singapore Funding Initiative. The
   associate editor coordinating the reviewof this manuscript and approving
   it for publication was M. Daoudi.
CR [Anonymous], 2008, P 17 INT C WORLD WID, DOI DOI 10.1145/1367497.1367629
   [Anonymous], 2012, Proceedings of the 2nd ACM international workshop on Interactive multimedia on mobile and portable devices
   [Anonymous], 2015, P 2015 C EMP METH NA
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2020, AAAI CONF ARTIF INTE, V34, P10542
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chung J., 2017, P ICLR
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng LX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P112, DOI 10.1145/3343031.3351147
   Devlin J., 2018, BERT PRE TRAINING DE
   Dong L., 2019, CoRR
   Gordon J, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P866
   Hamada R., 2005, 13th Annual ACM International Conference on Multimedia, P371, DOI 10.1145/1101149.1101228
   Harashima J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1229, DOI 10.1145/3077136.3080686
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328
   Huang XP, 2015, LECT NOTES ARTIF INT, V9165, P247, DOI 10.1007/978-3-319-20910-4_18
   Jermsurawong J., 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, P781
   Karikome S., 2012, IEICE TECHNICAL REPO, V112, P43
   Kiddon C., 2015, P 2015 C EMP METH NA
   Kiela Douwe, 2019, ARXIV190902950
   Koch G., 2015, P DEEP LEARN WORKSH
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liang C, 2017, AAAI CONF ARTIF INTE, P4786
   Liu J, 2011, VLDB J, V20, P335, DOI 10.1007/s00778-010-0198-2
   Marin J, IEEE T PATTERN ANAL
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Ming ZY, 2018, LECT NOTES COMPUT SC, V10705, P129, DOI 10.1007/978-3-319-73600-6_12
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Pan L, 2020, P ACMMM, P1132
   Pan L., 2017, P 8 INT JOINT C NAT, V1, P875
   Pan LM, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1447, DOI 10.18653/v1/P17-1133
   Pan LL, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON COLLABORATION AND INTERNET COMPUTING (CIC), P181, DOI 10.1109/CIC.2017.00033
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Salvador A, 2019, PROC CVPR IEEE, P10445, DOI 10.1109/CVPR.2019.01070
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Scheines R., 2014, Proceedings of Educational Data Mining, P355
   Talukdar P.P., 2012, P 7 WORKSH BUILD ED, P307
   Vaswani A, 2017, ADV NEUR IN, V30
   Vuong A., 2011, EDM 2011 - Proceedings Of The 4Th International Conference On Educational Data Mining, (EDM 2011 - Proceedings of the 4th International Conference on Educational Data Mining), P211
   Walter K., 2011, P INT C CAS BAS REAS, P207
   Wang H, 2019, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR.2019.00364
   Wang ST, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P317, DOI 10.1145/2983323.2983725
   Wang X, 2015, IEEE INT CONF MULTI
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yamakata Yoko, 2016, 2016 IEEE International Conference on Multimedia & Expo: Workshops (ICMEW), DOI 10.1109/ICMEW.2016.7574705
   Yamakata Yoko, 2013, Social Informatics. 5th International Conference, SocInfo 2013. Proceedings: LNCS 8238, P241, DOI 10.1007/978-3-319-03260-3_21
   Yamakata Y, 2017, T JAPANESE SOC ARTIF, V32
   Yang YM, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P159, DOI 10.1145/2684822.2685292
   Yang ZL, 2019, ADV NEUR IN, V32
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 56
TC 0
Z9 0
U1 2
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4491
EP 4501
DI 10.1109/TMM.2020.3042706
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Shi, YX
   Wei, Z
   Ling, HF
   Wang, ZY
   Shen, JL
   Li, P
AF Shi, Yuxuan
   Wei, Zhen
   Ling, Hefei
   Wang, Ziyang
   Shen, Jialie
   Li, Ping
TI Person Retrieval in Surveillance Videos Via Deep Attribute Mining and
   Reasoning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognition; Feature extraction; Hair; Semantics; Training; Robustness;
   Convolution; Person retrieval; person re-identification; human
   attribute; graph convolutional network
ID NEURAL-NETWORK; REIDENTIFICATION; IDENTIFICATION
AB Person retrieval largely relies on the appearance features of pedestrians. This task is rather more difficult in surveillance videos due to the limitations of extracting robust appearance features brought by the cross-view and cross-camera data with lower image resolution, motion blur, occlusion and other kinds of image degradation. To build up a more reliable person retrieval system, recent works introduced appearance attribute models to describe and distinguish different persons with high-level semantic concepts. Despite the progress of previous works, the value of utilizing appearance attributes is still under-explored. On one hand, existing methods lack for concise and precise attribute representations that are specific for each attribute category and, in the meantime, are able to filter noisy information in irrelevant spatial locations and useless patterns. On the other hand, correlation and reasoning between different attributes are neglected, which could generate more useful information and add more robustness to the retrieval system. In this paper, we propose an Attribute Mining and Reasoning (AMR) framework which is capable to handle the issues in question. The AMR makes better use of appearance attributes with two main components. First, the AMR disentangles the representations of different attributes by localizing their spatial positions and identifying their effective patterns in a weakly supervised manner. To achieve more reliable localization, we propose the Attribute Localization Ensemble (ALE) module that is consisted of multiple localization heads and a voting mechanism. Second, we introduce the Attribute Reasoning (AR) module to correlate different attributes together with the global appearance features and discover their latent relations to generate more comprehensive descriptions of pedestrians. Extensive experiments on DukeMTMC-ReID and Market-1501 datasets demonstrate the effectiveness of the proposed AMR framework as well as its superiority over the existing state-of-the-art methods. The AMR model also shows great generalization ability on the unseen CUHK03 dataset when it is only trained on Market-1501 dataset.
C1 [Shi, Yuxuan; Ling, Hefei; Wang, Ziyang; Li, Ping] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
   [Wei, Zhen] Ecole Polytech Fed Lausanne, Sch Comp & Commun Sci, CH-1015 Lausanne, Switzerland.
   [Shen, Jialie] Queens Univ Belfast, Belfast BT7 1NN, Antrim, North Ireland.
C3 Huazhong University of Science & Technology; Swiss Federal Institutes of
   Technology Domain; Ecole Polytechnique Federale de Lausanne; Queens
   University Belfast
RP Ling, HF (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
EM shiyx@hust.edu.cn; zhen.wei@hotmail.com; lhefei@hust.edu.cn;
   wangzyll@hust.edu.cn; jialie@gmail.com; lpshome@hust.edu.cn
OI Ling, Hefei/0000-0001-6797-7412; Shi, Yuxuan/0000-0001-7858-5369
FU Natural Science Foundation of China [61972169, U153203]; National Key
   Research and Development Program of China [2016QY01W0200]; Major
   Scientific and Technological Project of Hubei Province [2018AAA068,
   2019AAA051]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61972169 and U153203, in part by the National Key
   Research and Development Program of China (2016QY01W0200), and in part
   by the Major Scientific and Technological Project of Hubei Province
   (2018AAA068 and 2019AAA051).
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   [Anonymous], 2017, EUROPEAN C COMPUTER
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Bak S, 2017, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR.2017.171
   Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chen HR, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   CHEN W, 2017, PROC C COMPUT VIS PA, V26, P3492
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Fox NA, 2007, IEEE T MULTIMEDIA, V9, P701, DOI 10.1109/TMM.2007.893339
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Huang K, 2016, ARXIV160307054
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jose C, 2016, LECT NOTES COMPUT SC, V9909, P875, DOI 10.1007/978-3-319-46454-1_53
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Karaman S, 2014, PATTERN RECOGN, V47, P3767, DOI 10.1016/j.patcog.2014.06.003
   Karanam S., 2016, ARXIV PREPRINT ARXIV
   Khamis S, 2015, LECT NOTES COMPUT SC, V8927, P134, DOI 10.1007/978-3-319-16199-0_10
   Kipf TN, 2017, INT C LEARN REPR
   Lan X., 2018, P NEUR INF PROC SYST, P7528, DOI 10.48550/arXiv.1806.04606
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Layne R, 2014, ADV COMPUT VIS PATT, P93, DOI 10.1007/978-1-4471-6296-4_5
   Leibe B., 2017, ARXIV170307737CS
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin J, 2017, PROC CVPR IEEE, P3396, DOI 10.1109/CVPR.2017.362
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Ling HF, 2019, NEUROCOMPUTING, V347, P109, DOI 10.1016/j.neucom.2019.01.027
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Matsukawa T, 2016, INT C PATT RECOG, P2428, DOI 10.1109/ICPR.2016.7900000
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186
   Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Shi Y., 2020, IEEE T MULTIMEDIA
   Shi YX, 2020, NEUROCOMPUTING, V402, P124, DOI 10.1016/j.neucom.2020.03.057
   Shi ZY, 2015, PROC CVPR IEEE, P4184, DOI 10.1109/CVPR.2015.7299046
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2018, IEEE T PATTERN ANAL, V40, P1167, DOI 10.1109/TPAMI.2017.2679002
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Su C, 2015, IEEE I CONF COMP VIS, P3739, DOI 10.1109/ICCV.2015.426
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Sunderrajan S, 2016, IEEE T MULTIMEDIA, V18, P51, DOI 10.1109/TMM.2015.2496139
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Ustinova E., 2017, AVSS, P1
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang H., 2018, IEEE CVPR
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   WE Z, 2019, P IEEE C COMP VIS PA, P7108
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu SX, 2016, IEEE WINT CONF APPL
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Yao HT, 2017, IEEE INT CON MULTI, P1440, DOI 10.1109/ICME.2017.8019485
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zhang, 2019, PHYS REV, V94, P53
   Zhang HW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P187, DOI 10.1145/2647868.2654915
   Zhang JY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P845, DOI 10.1145/3343031.3350897
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou, 2018, ARXIV180506323
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhou Y, 2018, IEEE T INTELL TRANSP, V19, P1973, DOI 10.1109/TITS.2017.2740303
NR 96
TC 26
Z9 26
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4376
EP 4387
DI 10.1109/TMM.2020.3042068
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900036
DA 2024-07-18
ER

PT J
AU Virtusio, JJ
   Tan, DS
   Cheng, WH
   Tanveer, M
   Hua, KL
AF Virtusio, John Jethro
   Tan, Daniel Stanley
   Cheng, Wen-Huang
   Tanveer, M.
   Hua, Kai-Lung
TI Enabling Artistic Control Over Pattern Density and Stroke Strength
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Feature extraction; Image resolution; Adaptation models;
   Network architecture; Integrated circuits; Image color analysis; Style
   transfer control; style representation
AB Despite the remarkable results and numerous advancements in neural style transfer, achieving artistic control is still a challenging feat, primarily since existing methodologies treat the style representation as a black-box model. This oversight significantly limits the range of possible artistic manipulations. In this paper, we propose a method to enable artistic control on any correlation-based style transfer models along with guiding intuitions. Our focus is on controlling two perceptual factors: Pattern Density and Stroke Strength. To achieve this, we introduce the centered Gram style representation and manipulate it with our variance-aware adaptive weighting and correlation-based selective masking. Through several experiments and comparisons with the state-of-the-art, we show that we can achieve artistic control with competitive stylization quality. Additionally, since our method involves manipulating style representation, it can easily be adapted to popular style transfer models. We analyze different style representation properties to propose rules that govern the style transfer process, which is critical towards achieving artistic control over pattern density and stroke strength.
C1 [Virtusio, John Jethro; Tan, Daniel Stanley; Hua, Kai-Lung] Natl Taiwan Univ Sci & Technol, CSIE, Taipei 106, Taiwan.
   [Cheng, Wen-Huang] Natl Chiao Tung Univ, Inst Elect, Hsinchu 30010, Taiwan.
   [Tanveer, M.] Indian Inst Technol Indore, Discipline Math, Indore 453552, India.
C3 National Taiwan University of Science & Technology; National Yang Ming
   Chiao Tung University; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Indore
RP Hua, KL (corresponding author), Natl Taiwan Univ Sci & Technol, CSIE, Taipei 106, Taiwan.
EM D10715811@mail.ntust.edu.tw; D10515805@mail.ntust.edu.tw;
   whcheng@nctu.edu.tw; mtanveer@iiti.ac.in; hua@mail.ntust.edu.tw
RI Tanveer, Mohammad/I-4585-2013
OI Tanveer, Mohammad/0000-0002-5727-3697; Hua,
   Kai-Lung/0000-0002-7735-243X; Tan, Daniel Stanley/0000-0002-8071-9060
FU Center for Cyber-physical System Innovation; Center of Intelligent
   Robots from The Featured Areas Research Center Program; Ministry of
   Science and Technology of Taiwan [MOST109-2218-E-011-010,
   MOST108-2221-E-011-116, MOST108-2622-E-011-016-CC3]
FX This work was supported in part by the Center for Cyber-physical System
   Innovation and in part by Center of Intelligent Robots from The Featured
   Areas Research Center Program within the framework of the Higher
   Education Sprout Project by the Ministry of Education (MOE) in Taiwan
   and Ministry of Science and Technology of Taiwan under Grants
   MOST109-2218-E-011-010, MOST108-2221-E-011-116,
   andMOST108-2622-E-011-016-CC3.
CR Barnes C., 2017, ARXIVABS170108893
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gatys L., 2015, NIPS
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   JING Y, IN PRESS, DOI DOI 10.1109/TVCG.2019.2921336
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Kingma D. P., 2014, arXiv
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Li YJ, 2017, ADV NEUR IN, V30
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Olszewska JI, 2019, KEOD: PROCEEDINGS OF THE 11TH INTERNATIONAL JOINT CONFERENCE ON KNOWLEDGE DISCOVERY, KNOWLEDGE ENGINEERING AND KNOWLEDGE MANAGEMENT - VOL 2: KEOD, P435, DOI 10.5220/0008354804350441
   Olszewska JI, 2019, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE (ICAART), VOL 2, P850, DOI 10.5220/0007585208500856
   Ople JJM, 2020, IEEE ACCESS, V8, P53942, DOI 10.1109/ACCESS.2020.2980996
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan DS, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071587
   Tan DS, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071708
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Ulyanov Dmitry, 2016, arXiv
   YAN C, IN PRESS, DOI DOI 10.1109/TPAMI.2020.2975798
   YAN C, IN PRESS, DOI DOI 10.1109/TMM.2020.2967645
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 31
TC 5
Z9 5
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2273
EP 2285
DI 10.1109/TMM.2020.3009484
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800009
DA 2024-07-18
ER

PT J
AU Xu, K
   Wen, LY
   Li, GR
   Huang, QM
AF Xu, Kai
   Wen, Longyin
   Li, Guorong
   Huang, Qingming
TI Self-Supervised Deep TripleNet for Video Object Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Object segmentation; Motion segmentation; Image matching;
   Video sequences; Task analysis; Annotations; Video object segmentation;
   self-supervised learning
AB Most of previous video object segmentation methods require a large amount of pixel-level annotated video data to construct a robust model. It is quite expensive to label segmentation mask in video. In this article, we propose a self-supervised triplenet for video object segmentation, which only leverages nearly unlimited unlabeled video data in training phase. Our method consists of two modules, i.e., the temporal motion module and the appearance matching module. The temporal motion module is trained based on the pixel correspondence between two video frames in a self-supervised manner, which models the motion patterns between two video frames and propagates the labels from one frame to another. Meanwhile, the appearance matching module encodes the reference frame and its corresponding mask to generate the segmentation mask of the same object in target frame. The appearance matching module can adjust and refine the output results of temporal motion module, and avoid error accumulation by matching the reference appearance. In order to train the appearance matching module in self-supervised manner, we propose two mask generation strategies: foreground region mask generation and random color region mask generation. Extensive experiments conducted on four challenging video object segmentation datasets, i.e., DAVIS-2017, Youtube-VOS, DAVIS-2016 and SegTrack v2, demonstrate that the proposed method performs favorable against the state-of-the-art self-supervised methods, and performs even competitively with fully-supervised methods. We also show our self-supervised approach has actually superior generalizability to the majority of supervised methods.
C1 [Xu, Kai; Li, Guorong; Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100190, Peoples R China.
   [Wen, Longyin] ByteDance AI Lab, Mountain View, CA 94043 USA.
   [Wen, Longyin] JD Finance Amer Corp, Mountain View, CA USA.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Li, GR (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100190, Peoples R China.
EM xukai16@mails.ucas.ac.cn; longyin.wen@jd.com; liguorong@ucas.ac.cn;
   qmhuang@ucas.ac.cn
RI yang, zhuo/JPK-3133-2023; Li, Guo/JNR-1700-2023
FU Italy-China Collaboration Project TALENT [2018YFE0118400]; National
   Natural Science Foundation of China [61620106009, 61772494, 61931008,
   U1636214, 61836002, 61976069]; Key Research Program of Frontier
   Sciences, CAS [QYZDJ-SSW-SYS013]; Youth Innovation Promotion Association
   CAS
FX This work was supported in part by the Italy-China Collaboration Project
   TALENT under Grant 2018YFE0118400, in part by the National Natural
   Science Foundation of China under Grants 61620106009, 61772494,
   61931008, U1636214, 61836002, and 61976069, in part by the Key Research
   Program of Frontier Sciences, CAS: QYZDJ-SSW-SYS013, and in part by
   Youth Innovation Promotion Association CAS. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xin Geng.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Croitoru I, 2017, IEEE I CONF COMP VIS, P4345, DOI 10.1109/ICCV.2017.465
   Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Goroshin R, 2015, IEEE I CONF COMP VIS, P4086, DOI 10.1109/ICCV.2015.465
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jin J., 2013, Information Sciences and Systems (CISS), 2013 47th Annual Conference on, P1
   Lai Z., 2019, ARXIV190500875
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li SY, 2018, LECT NOTES COMPUT SC, V11207, P215, DOI 10.1007/978-3-030-01219-9_13
   Li XT, 2019, ADV NEUR IN, V32
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu SF, 2018, LECT NOTES COMPUT SC, V11211, P89, DOI 10.1007/978-3-030-01234-2_6
   Nagaraja NS, 2015, IEEE I CONF COMP VIS, P3235, DOI 10.1109/ICCV.2015.370
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Paszke A., 2017, ADV NEURAL INF PROCE, V9, P1, DOI DOI 10.1017/CB09781107707221.009
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J., 2017, ARXIV170400675
   Siam M, 2019, IEEE INT CONF ROBOT, P50, DOI [10.1109/ICRA.2019.8794254, 10.1109/icra.2019.8794254]
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Stretcu O., 2015, BMVC, P3
   Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480
   Tokmakov P, 2017, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2017.64
   Tung HYF, 2017, ADV NEUR IN, V30
   Voigtlaender P., 2017, BMVC, P1000
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Vondrick C, 2018, LECT NOTES COMPUT SC, V11217, P402, DOI 10.1007/978-3-030-01261-8_24
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang XL, 2019, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2019.00267
   Wang XL, 2017, IEEE I CONF COMP VIS, P1338, DOI 10.1109/ICCV.2017.149
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xu N., 2018, ARXIV180903327
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Zhang Y, 2018, IEEE T PATTERN ANAL, V40, P1741, DOI 10.1109/TPAMI.2017.2727049
   Zhang Y, 2018, IEEE T IMAGE PROCESS, V27, P4245, DOI 10.1109/TIP.2018.2806995
NR 47
TC 10
Z9 10
U1 3
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3530
EP 3539
DI 10.1109/TMM.2020.3026913
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100008
DA 2024-07-18
ER

PT J
AU Yang, SJ
   Li, L
   Wang, SH
   Zhang, WG
   Huang, QM
   Tian, Q
AF Yang, Shijie
   Li, Liang
   Wang, Shuhui
   Zhang, Weigang
   Huang, Qingming
   Tian, Qi
TI Graph Regularized Encoder-Decoder Networks for Image Representation
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Laplace equations; Visualization; Manifolds; Image reconstruction; Task
   analysis; Decoding; Semantics; Auto-encoder; encoder-decoder; graph
   regularizer; image representation learning
ID DIMENSIONALITY REDUCTION; MATRIX
AB Image representation learning with encoder-decoder networks plays a fundamental role in multimedia processing. Recent findings show that traditional encoder-decoders can be negatively affected by small visual perturbations. The learned non-smooth feature embedding cannot guarantee to capture semantic-meaningful geometric distance between visually-similar image samples. Inspired by manifold learning, we propose a graph regularized encoder-decoder network, which can preserve local geometric information of the code embedding space. More discriminative feature embedding is learnt to attain both high-level image semantic and neighbor relationship of image clusters. The proposed graph regularizer is formulated upon multi-layer perceptions. It uses the local invariance principle to explicitly reconstruct the geometric similarity graph. Theoretical analysis is provided to show the connection between our deep regularizer and traditional graph Laplacian regularizer. Practically, the network complexity is alleviated by anchor based bipartite graph, and this leverages our method into large scale scenario. Experimental evaluations show the comparable results of the proposed method with state-of-the-art models on different tasks.
C1 [Yang, Shijie] Univ Chinese Acad Sci UCAS, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Yang, Shijie] UCAS, Key Lab Big Data Min & Knowledge Management, Beijing 101408, Peoples R China.
   [Li, Liang; Wang, Shuhui; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zhang, Weigang] Harbin Inst Technol, Sch Comp Sci & Technol, Weihai 264209, Peoples R China.
   [Zhang, Weigang] Chinese Acad Sci, Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Tian, Qi] Huawei Noahs Ark Lab, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Computing Technology,
   CAS; Harbin Institute of Technology; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Huawei Technologies
RP Li, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM shijie.yang@vipl.ict.ac.cn; liang.li@ict.ac.cn; wangshuhui@ict.ac.cn;
   wgzhang@hit.edu.cn; qmhuang@ucas.ac.cn; tian.qi1@huawei.com
RI Zhang, Weigang/GZA-9095-2022
OI Li, Liang/0000-0002-1943-8219; Zhang, Weigang/0000-0003-0042-7074
FU National Key R&D Program of China [2018YFE0303104]; National Natural
   Science Foundation of China [61771457, 61732007, 61672497, U1636214,
   61931008, 61772494, 61836002]; Key Research Program of Frontier
   Sciences, CAS [QYZDJ-SSW-SYS013]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFE0303104, in part by National Natural Science
   Foundation of China: 61771457, 61732007, 61672497, U1636214, 61931008,
   61772494 and 61836002, in part by Key Research Program of Frontier
   Sciences, CAS: QYZDJ-SSW-SYS013.
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   [Anonymous], 2011, P ICML
   [Anonymous], 1998, THEMNIST DATABASE HA
   [Anonymous], 2015, INT CONF MACH LEARN
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Beichen Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8753, DOI 10.1109/CVPR42600.2020.00878
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Cai D, 2005, IEEE T KNOWL DATA EN, V17, P1624, DOI 10.1109/TKDE.2005.198
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Cho K., 2014, ARXIV14061078
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Eckart C, 1936, PSYCHOMETRIKA, V1, P211, DOI 10.1007/BF02288367
   Getoor, 2011, P 28 INT C MACH LEAR, P833
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Guan NY, 2011, IEEE T IMAGE PROCESS, V20, P2030, DOI 10.1109/TIP.2011.2105496
   He XF, 2004, ADV NEUR IN, V16, P153
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hoffman M., 2010, ADV NEURAL INFORM PR, V23, P856
   Jia K, 2015, NEUROCOMPUTING, V160, P250, DOI 10.1016/j.neucom.2015.02.023
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Lan Z., 2015, COMPUT SCI, V9241
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Li L, 2012, IEEE T MULTIMEDIA, V14, P1401, DOI 10.1109/TMM.2012.2194993
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Li Z., 2012, P AAAI C ART INT, P1026
   Li Z., 2018, IEEE T NEUR NET LEAR
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Li Zhizhong, 2018, IEEE transactions on pattern analysis and machine intelligence
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu LX, 2019, IEEE I CONF COMP VIS, P4239, DOI 10.1109/ICCV.2019.00434
   Liu X., 2020, IEEE COMPUT VIS PATT
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Mansimov E., 2016, INT C LEARN REPR ICL
   Nene S. A., 1996, Tech. Rep. CUCS-005-96
   Ng A., 2011, CS294A LECT NOTES
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Reed S, 2016, PR MACH LEARN RES, V48
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Silberer C, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P721
   Tian F, 2014, AAAI CONF ARTIF INTE, P1293
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wu JL, 2019, IEEE I CONF COMP VIS, P8149, DOI 10.1109/ICCV.2019.00824
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Wu YL, 2019, IEEE T IMAGE PROCESS, V28, P4299, DOI 10.1109/TIP.2019.2908774
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yang SJ, 2017, PROC CVPR IEEE, P7053, DOI 10.1109/CVPR.2017.746
NR 55
TC 4
Z9 5
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3124
EP 3136
DI 10.1109/TMM.2020.3020697
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000014
DA 2024-07-18
ER

PT J
AU Zhan, HJ
   Yi, CY
   Shi, BX
   Lin, J
   Duan, LY
   Kot, AC
AF Zhan, Huijing
   Yi, Chenyu
   Shi, Boxin
   Lin, Jie
   Duan, Ling-Yu
   Kot, Alex C.
TI Pose-Normalized and Appearance-Preserved Street-to-Shop Clothing Image
   Generation and Feature Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Image generation; Shape; Task analysis; Semantics; Gallium
   nitride; Visualization; Pose-normalization; image generation; feature
   learning; street-to-shop
AB We tackle the task of street-to-shop clothing image synthesis. Given a daily person image with a particular clothing item captured in the street scenario, we aim to synthesize the frontal facing view of that item in the shop scenario. This problem has the following challenges: 1) the distinct visual discrepancy between the street and shop scenario; 2) the severe shape deformation of clothing in the presence of an arbitrary human pose; 3) the preservation of fine-grained details during the process of clothing image generation. In this paper, we jointly solve these difficulties by proposing a Pose-Normalized and Appearance-Preserved Generative Adversarial Network (PNAP-GAN). More specifically, conditioned on the clothing-agnostic representation (i.e., clothing landmarks and semantic parsing map), we disentangle the shape and appearance synthesis in a coarse-to-fine framework. Moreover, a semantic embedding loss is introduced to guide the domain transfer in the semantic level (i.e., keeping the clothing attributes). With the synthesized frontal shop image, a pose-normalized representation in complementary to the domain-invariant feature learnt from the original street image are integrated to facilitate the problem of street-to-shop clothing retrieval. Extensive experiments conducted demonstrate the effectiveness of the proposed PNAP-GAN on generating high quality frontal-view images and the excellence of the learnt pose-normalized features on the retrieval task than existing methods. In addition, we demonstrate that the pose-normalized retrieval feature benefits the cross-scenario (i.e., street-to-shop) clothing image generation in a semantic-preserved manner.
C1 [Zhan, Huijing; Yi, Chenyu] Nanyang Technol Univ, Rapid Rich Object Search ROSE Lab, Sch Elect & Elect Engn, Singapore 637553, Singapore.
   [Zhan, Huijing; Lin, Jie] Inst Infocomm Res I2R, Singapore 138634, Singapore.
   [Shi, Boxin; Duan, Ling-Yu] Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R);
   Peking University; Nanyang Technological University
RP Shi, BX (corresponding author), Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
EM zhan-huijing@i2r.a-star.edu.sg; yich0003@e.ntu.edu.sg;
   shiboxin@pku.edu.cn; lin-j@i2r.a-star.edu.sg; lingyu@pku.edu.cn;
   eackot@ntu.edu.sg
OI Kot, Alex/0000-0001-6262-8125; Yi, Chenyu/0000-0001-5002-6549
FU Agency for Science, Technology and Research (A*STAR) under its AME
   Programmatic Funds Project [A1892b0026]; NTU-PKU Joint Research
   Institute; National Natural Science Foundation of China [U1611461,
   61872012]; National Key RAMP;D Program of China [2019YFF0302902];
   Shenzhen Municipal Science and Technology Program
   [JCYJ20170818141146428]; Beijing Academy of Artificial Intelligence
   (BAAI)
FX This work was supported in part by the Agency for Science, Technology
   and Research (A*STAR) under its AME Programmatic Funds Project
   A1892b0026, in part by NTU-PKU Joint Research Institute with donation
   from Ng Teng Fong Charitable Foundation, in part by the National Natural
   Science Foundation of China under Grants U1611461 and 61872012, in part
   by National Key R&D Program of China under Grant 2019YFF0302902, in part
   by Shenzhen Municipal Science and Technology Program under Grant
   JCYJ20170818141146428, and in part by Beijing Academy of Artificial
   Intelligence (BAAI).
CR [Anonymous], 2015, 3 INT C LEARN REPR
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Charlton, 2019, BWORLD ROBOT CONTROL
   Chen Q, 2015, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR.2015.7299169
   Cheng Y, 2018, INT CONF INFO SCI, P472, DOI 10.1109/ICIST.2018.8426080
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Cole F, 2017, PROC CVPR IEEE, P3386, DOI 10.1109/CVPR.2017.361
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han XT, 2019, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2019.00458
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang YG, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3184745
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kim TH, 2017, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2017.435
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Raj A, 2018, LECT NOTES COMPUT SC, V11216, P679, DOI 10.1007/978-3-030-01258-8_41
   Salimans T, 2016, ADV NEUR IN, V29
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Zagoruyko S., 2015, PROC CVPR IEEE, P4353, DOI DOI 10.1109/CVPR.2015.7299064
   Zamyatin E, 2018, PROCEDIA COMPUT SCI, V136, P200, DOI 10.1016/j.procs.2018.08.254
   Zhan HJ, 2019, COMPUT VIS IMAGE UND, V180, P23, DOI 10.1016/j.cviu.2019.01.001
   Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26
   Zhang MJ, 2019, IEEE T IMAGE PROCESS, V28, P642, DOI 10.1109/TIP.2018.2869688
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhao B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P383, DOI 10.1145/3240508.3240536
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 56
TC 11
Z9 11
U1 3
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 133
EP 144
DI 10.1109/TMM.2020.2978669
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600011
DA 2024-07-18
ER

PT J
AU Zhang, ZJ
   Zhao, Z
   Zhang, Z
   Lin, ZJ
   Wang, Q
   Hong, RC
AF Zhang, Zijian
   Zhao, Zhou
   Zhang, Zhu
   Lin, Zhijie
   Wang, Qi
   Hong, Richang
TI Temporal Textual Localization in Video via Adversarial Bi-Directional
   Interaction Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bidirectional control; Semantics; Task analysis; Correlation; Natural
   languages; Visualization; Microsoft Windows; Adversarial learning;
   bi-directional self-attention; multi-task learning; texual video
   localization
AB Given a natural language description, temporal textual localization aims to localize the most relevant segment in an untrimmed video, which is a natural and imperative extension of temporal action localization. Most existing temporal textual localization works neglect the long-range semantic modeling in video contents and lack accurate textual understanding. Moreover, they remain in single-task learning and fail to exploit multi-view supervised information. Based on these observations, we introduce a novel adversarial bi-directional interaction network, which is a global framework to retrieve the target segment directly. Specifically, we propose a bi-directional attention mechanism to build bi-directional information interaction, which captures long-range semantic dependencies from video context and enhances textual representation learning. After localization, we further advise an auxiliary discriminator network to verify the localization result and boost the performance by adversarial training process. We adopt multi-task learning approach to train our model, including: (1) predicting coordinate probability distribution task, which selects start and end frame to localize target segment; (2) predicting frame-level correlation distribution task, which calculates the correlation between frame and description; (3) auxiliary adversarial learning task, which calculates matched score between localization and description to boost the performance. The extensive experiments on ActivityNet Captions and TACoS show the significant effectiveness and efficiency of our method.
C1 [Zhang, Zijian; Zhao, Zhou; Zhang, Zhu; Lin, Zhijie] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Zhang, Zijian; Zhao, Zhou] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Wang, Qi] Alibaba Inc, Hangzhou 310027, Peoples R China.
   [Hong, Richang] Hefei Univ Technol, Dept Comp Sci & Technol, Hefei 230009, Peoples R China.
C3 Zhejiang University; Xidian University; Alibaba Group; Hefei University
   of Technology
RP Zhao, Z (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
EM ckczzj@zju.edu.cn; zhaozhou@zju.edu.cn; zhangzhu@zju.edu.cn;
   linzhijie@zju.edu.cn; wq140362@alibaba-inc.com; hongrc@hfut.edu.cn
RI Zhao, zhuo/JYO-7894-2024; zhao, zhao/JAC-1686-2023
OI Zhang, Zijian/0000-0001-8308-768X; Zhang, Zhu/0000-0003-4195-5770
FU National Key R&D Program of China [2018AAA0100603]; Zhejiang Natural
   Science Foundation [LR19F020006]; National Natural Science Foundation of
   China [61836002, U1611461, 61751209, 61932009, 61722204]; Fundamental
   Research Funds for the Central Universities [2020QNA5024]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0100603, in part by Zhejiang Natural Science
   Foundation under Grant LR19F020006, in part by the National Natural
   Science Foundation of China under Grants 61836002, U1611461, 61751209,
   61932009, and 61722204, and in part by the Fundamental Research Funds
   for the Central Universities under Grant 2020QNA5024. The associate
   editor coordinating the reviewof this manuscript and approving it for
   publication was Dr. Federica Battisti. (Corresponding author: Zhou
   Zhao.)
CR Alayrac JB, 2016, PROC CVPR IEEE, P4575, DOI 10.1109/CVPR.2016.495
   Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen DQ, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1870, DOI 10.18653/v1/P17-1171
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gao J., 2017, ARXIV170704818
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hendricks Lisa Anne, 2018, EMNLP
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2015, 29 ANN C NEURAL INFO, V28
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lin DH, 2014, PROC CVPR IEEE, P2657, DOI 10.1109/CVPR.2014.340
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Lin ZJ, 2020, IEEE T IMAGE PROCESS, V29, P3750, DOI 10.1109/TIP.2020.2965987
   Lin Zhijie, 2019, ARXIV191108199
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Luo JY, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2309, DOI 10.1145/3357384.3358104
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Mikolov Tomas, 2013, Preprints
   Mou Lili., 2015, Natural language inference by tree-based convolution and heuristic matching
   Naim I, 2014, AAAI CONF ARTIF INTE, P1558
   Otani Mayu, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P651, DOI 10.1007/978-3-319-46604-0_46
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Sener O, 2015, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2015.509
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Song Y C, 2016, IJCAI, P2025
   Sun C, 2015, IEEE I CONF COMP VIS, P2596, DOI 10.1109/ICCV.2015.298
   Tapaswi M, 2015, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2015.7298792
   Tellex Stefanie., 2009, P ACM INT C IM VID R, P38
   Vaswani A, 2017, ADV NEUR IN, V30
   Xiao SW, 2020, AAAI CONF ARTIF INTE, V34, P12426
   Xiao SW, 2020, IEEE T IMAGE PROCESS, V29, P5889, DOI 10.1109/TIP.2020.2985868
   XU H, 2018, ARXIV180405113
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Yang HT, 2018, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2018.00157
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yao Ting, 2019, IEEE T MM, V22, P1577
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2019, AAAI CONF ARTIF INTE, P9127
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Zhang DS, 2004, IEEE T MULTIMEDIA, V6, P450, DOI 10.1109/TMM.2004.827505
   Zhang Z., 2020, P 28 ACM INT C MULT, P4098
   Zhang Z., 2020, ARXIV200806941
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang Zhu, 2020, P IEEECVF C COMPUTER, P10668
   Zhu XQ, 2005, IEEE T MULTIMEDIA, V7, P648, DOI 10.1109/TMM.2005.850977
NR 61
TC 15
Z9 16
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3306
EP 3317
DI 10.1109/TMM.2020.3023339
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000028
DA 2024-07-18
ER

PT J
AU Zhao, F
   Zhao, WD
AF Zhao, Fan
   Zhao, Wenda
TI Learning Specific and General Realm Feature Representations for Image
   Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Feature extraction; Biomedical imaging; Image edge
   detection; Visualization; Remote sensing; Transforms; Universal image
   fusion framework; adaptive realm feature extraction strategy; realm
   activation mechanism; no-reference perceptual metric loss
ID MULTI-FOCUS; SPARSE REPRESENTATION; ENHANCEMENT; TRANSFORM; FRAMEWORK;
   FILTER
AB A universal fusion framework for handling multi-realm image fusion reduces the cost of manual selection in varied applications. Addressing the generality of multiple realms and the sensitivity of specific realm, we propose a novel universal framework for multi-realm image fusion through learning realm-specific and realm-general feature representations. Shared principle network, adaptive realm feature extraction strategy and realm activation mechanism are designed for facilitating high generalization of across-realm and sensitivity of specific-realm simultaneously. In addition, we present realm-specific no-reference perceptual metric losses based on the edge details and contrast for optimizing the learning process, making the fused image exhibit more specific appearance. Moreover, we collect a new multi-realm image fusion dataset (MRIF), consisting of infrared and visual images, medical images and multispectral images, to facilitate our training and testing. Experimental results show that the fused image obtained by the proposed method achieves superior performance compared with the state-of-the-art methods on MRIF and the other three datasets including infrared and visual images, medical images and remote sensing images, respectively.
C1 [Zhao, Fan] Liaoning Normal Univ, Sch Phys & Elect Technol, Dalian 116029, Peoples R China.
   [Zhao, Wenda] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
C3 Liaoning Normal University; Dalian University of Technology
RP Zhao, WD (corresponding author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
EM Fan_Zhao20@163.com; zhaowenda@dlut.edu.cn
FU National Natural Science Foundation of China [61801077]; China
   Postdoctoral Science Foundation [2019T120206, 2017M611221]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61801077, in part by China Postdoctoral Science
   Foundation under Grants 2019T120206 and 2017M611221.
CR Chen C, 2019, LECT NOTES COMPUT SC, V11766, P447, DOI 10.1007/978-3-030-32248-9_50
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Du J, 2017, IEEE T IMAGE PROCESS, V26, P5855, DOI 10.1109/TIP.2017.2745202
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hill P, 2017, IEEE T IMAGE PROCESS, V26, P1076, DOI 10.1109/TIP.2016.2633863
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Joo D, 2018, PROC CVPR IEEE, P1635, DOI 10.1109/CVPR.2018.00176
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579
   Kong WW, 2019, IEEE T INSTRUM MEAS, V68, P938, DOI 10.1109/TIM.2018.2865046
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2013, IEEE T GEOSCI REMOTE, V51, P4779, DOI 10.1109/TGRS.2012.2230332
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li ST, 2012, IEEE T BIO-MED ENG, V59, P3450, DOI 10.1109/TBME.2012.2217493
   Li ST, 2012, IEEE T CONSUM ELECTR, V58, P626, DOI 10.1109/TCE.2012.6227469
   Li XH, 2019, ISPRS J PHOTOGRAMM, V148, P103, DOI 10.1016/j.isprsjprs.2018.12.013
   Li XH, 2016, IEEE J-STARS, V9, P3629, DOI 10.1109/JSTARS.2016.2533547
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Liang XC, 2019, IEEE SENS J, V19, P7107, DOI 10.1109/JSEN.2019.2913281
   Liu XY, 2020, INFORM FUSION, V55, P1, DOI 10.1016/j.inffus.2019.07.010
   Liu YJ, 2019, PROC CVPR IEEE, P7186, DOI 10.1109/CVPR.2019.00736
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Mao X., 2018, INT JOINT C ART INT, P2553
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Nava R., 2007, Proc. SPIE, V34, P94
   Nejati M, 2015, INFORM FUSION, V25, P72, DOI 10.1016/j.inffus.2014.10.004
   Pohl C, 1998, INT J REMOTE SENS, V19, P823, DOI 10.1080/014311698215748
   Pu Y., 2018, P INT C MACH LEARN, P4148
   Rebuffi SA, 2017, ADV NEUR IN, V30
   Shibata T, 2019, IEEE T COMPUT IMAG, V5, P82, DOI 10.1109/TCI.2018.2879021
   Simone G., 2002, Information Fusion, V3, P3, DOI 10.1016/S1566-2535(01)00056-2
   Tamaazousti Y, 2020, IEEE T PATTERN ANAL, V42, P2212, DOI 10.1109/TPAMI.2019.2913857
   Tang H., 2018, P AS C COMP VIS ACCV, P3
   Toet A, 2014, FIGSHARE DATA, DOI 10.6084/m9.figshare.1008029.v1
   Upla KP, 2015, IEEE T GEOSCI REMOTE, V53, P3210, DOI 10.1109/TGRS.2014.2371812
   Vishwakarma A, 2019, IEEE T INSTRUM MEAS, V68, P3367, DOI 10.1109/TIM.2018.2877285
   Wang JD, 2018, PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON CROWD SCIENCE AND ENGINEERING (ICCSE 2018), DOI 10.1145/3265689.3265705
   Wang XD, 2019, PROC CVPR IEEE, P7281, DOI 10.1109/CVPR.2019.00746
   Wei HJ, 2015, IEEE T MED IMAGING, V34, P306, DOI 10.1109/TMI.2014.2356792
   Wei Q, 2015, IEEE T GEOSCI REMOTE, V53, P3658, DOI 10.1109/TGRS.2014.2381272
   Xia Z., 2019, ARXIV190802895
   Xu H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3954
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yang JF, 2017, IEEE I CONF COMP VIS, P1753, DOI 10.1109/ICCV.2017.193
   Yin HT, 2018, IEEE T BIO-MED ENG, V65, P2622, DOI 10.1109/TBME.2018.2811243
   Yin M, 2019, IEEE T INSTRUM MEAS, V68, P49, DOI 10.1109/TIM.2018.2838778
   Zhang JY, 2004, IEEE IMAGE PROC, P973
   Zhang K, 2019, IEEE T GEOSCI REMOTE, V57, P1117, DOI 10.1109/TGRS.2018.2864750
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang Q, 2016, IEEE T IMAGE PROCESS, V25, P2045, DOI 10.1109/TIP.2016.2524212
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao WD, 2019, IEEE T CIRC SYST VID, V29, P1102, DOI 10.1109/TCSVT.2018.2821177
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhao WD, 2017, IEEE T INSTRUM MEAS, V66, P2283, DOI 10.1109/TIM.2017.2700198
   Zhao WD, 2016, INFORM FUSION, V27, P138, DOI 10.1016/j.inffus.2015.06.003
   Zhu XX, 2013, IEEE T GEOSCI REMOTE, V51, P2827, DOI 10.1109/TGRS.2012.2213604
NR 60
TC 25
Z9 25
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2745
EP 2756
DI 10.1109/TMM.2020.3016123
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600016
DA 2024-07-18
ER

PT J
AU Zhou, ZG
   Zhou, WG
   Lv, XT
   Huang, X
   Wang, XY
   Li, HQ
AF Zhou, Zhengguang
   Zhou, Wengang
   Lv, Xutao
   Huang, Xuan
   Wang, Xiaoyu
   Li, Houqiang
TI Progressive Learning of Low-Precision Networks for Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quantization (signal); Training; Neural networks; Convolution;
   Acceleration; Task analysis; Complexity theory; Low-precision networks;
   quantization; expanding; weakening; image classification
ID NEURAL-NETWORK
AB Recent years have witnessed a great advance of deep learning in a variety of vision tasks. Many state-of-the-art deep neural networks suffer from large size and high complexity, which makes them difficult to deploy in resource-limited platforms such as mobile devices. To this end, low-precision neural networks are widely studied that quantize weights or activations into the low-bit format. Although efficient, low-precision networks are usually difficult to train and encounter severe accuracy degradation. In this paper, we propose a new training strategy based on progressive learning for image classification. First, we equip each low-precision convolutional layer with an ancillary full-precision convolutional layer based on a low-precision network structure. Second, a decay method is introduced to reduce the output of the added full-precision convolution gradually, which keeps the resulting topology structure the same as the original low-precision convolution. Extensive experiments on SVHN, CIFAR and ILSVRC-2012 datasets reveal that the proposed method can bring faster convergence and higher accuracy for low-precision neural networks.
C1 [Zhou, Zhengguang; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Lv, Xutao; Huang, Xuan; Wang, Xiaoyu] Intellifusion Inc, Shenzhen 518000, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM zhgzh164@mail.ustc.edu.cn; zhwg@ustc.edu.cn; lvxutao@gmail.com;
   huang.xuan.intellif@gmail.com; fanghuaxue@gmail.com; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013; wang, xiaoyu/HJP-6901-2023
OI Wang, Xiaoyu/0000-0002-6431-8822
FU NSFC [61836011, 61822208]; Youth Innovation Promotion Association CAS
   [2018497]
FX The work of W. Zhou was supported in part by NSFC under Contract
   61822208 and in part by Youth Innovation Promotion Association CAS
   2018497. The work of H. Li was supported by NSFC under Contract
   61836011.
CR Abadi Martin, 2016, arXiv
   [Anonymous], 2016, ARXIV160202830
   [Anonymous], 2018, ARXIV PREPRINT ARXIV
   Bengio Yoshua, 2013, CoRR abs/1308.3432
   Cai ZW, 2017, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR.2017.574
   Courbariaux Y., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.5555/2969442.2969588
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Dean J., 2015, NIPS DEEP LEARNING R
   Denil M., 2013, P 26 INT C NEUR INF, P2148
   Faraone J, 2018, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR.2018.00452
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737
   Han S, 2015, P ADV NEUR INF PROC, V2015, P1135
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Howard A. G., 2017, ARXIV170404861
   Hu Hengyuan, 2016, Network trimming: A data-driven neuron pruning approach towards efficient deep architectures
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hubara I, 2018, J MACH LEARN RES, V18
   Iandola F. N., 2016, ARXIV160207360, DOI 10.1007/978-3-319-24553-9
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jaderberg M., 2014, CORR
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kingma D. P., 2014, arXiv
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Lebedev V., 2015, ICLR, P1
   [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1
   Li H., 2017, PRUNING FILTERS EFFI, DOI DOI 10.48550/ARXIV.1608.08710
   Lin XF, 2017, ADV NEUR IN, V30
   Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810
   Mishra A., 2018, INT C LEARN REPR ICL, P1
   Netzer Y., 2011, NIPS WORKSHOP DEEP L, V2011
   Park E, 2017, PROC CVPR IEEE, P7197, DOI 10.1109/CVPR.2017.761
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero A., 2014, ARXIV14126550
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Tang W, 2017, AAAI CONF ARTIF INTE, P2625
   Tung F, 2018, PROC CVPR IEEE, P7873, DOI 10.1109/CVPR.2018.00821
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang PS, 2018, PROC CVPR IEEE, P4376, DOI 10.1109/CVPR.2018.00460
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhou A., 2017, ARXIV170203044
   Zhou GR, 2018, AAAI CONF ARTIF INTE, P4580
   Zhou Shuchang, 2016, ARXIV160606160
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826
NR 57
TC 4
Z9 4
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 871
EP 882
DI 10.1109/TMM.2020.2990087
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300004
DA 2024-07-18
ER

PT J
AU Fang, FM
   Wang, TT
   Wang, Y
   Zeng, TY
   Zhang, GX
AF Fang, Faming
   Wang, Tingting
   Wang, Yang
   Zeng, Tieyong
   Zhang, Guixu
TI Variational Single Image Dehazing for Enhanced Visualization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Single image dehazing; color space; variational model; image
   enhancement; ADMM
ID WEATHER; MODEL
AB In this paper, we investigate the challenging task of removing haze from a single natural image. The analysis on the haze formation model shows that the atmospheric veil has much less relevance to chrominance than luminance, which motivates us to neglect the haze in the chrominance channel and concentrate on the luminance channel in the dehazing process. Besides, the experimental study illustrates that the YUV color space is most suitable for image dehazing. Accordingly, a variational model is proposed in the Y channel of the YUV color space by combining the reformulation of the haze model and the two effective priors. As we mainly focus on the Y channel, most of the chrominance information of the image is preserved after dehazing. The numerical procedure based on the alternating direction method of multipliers (ADMM) scheme is presented to obtain the optimal solution. Extensive experimental results on real-world hazy images and synthetic dataset demonstrate clearly that our method can unveil the details and recover vivid color information, which is competitive among many existing dehazing algorithms. Further experiments show that our model also can be applied for image enhancement.
C1 [Fang, Faming; Wang, Tingting; Zhang, Guixu] East China Normal Univ, Sch Comp Sci & Technol, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200062, Peoples R China.
   [Wang, Yang] Hong Kong Univ Sci & Technol, Dept Math, Hong Kong, Peoples R China.
   [Zeng, Tieyong] Chinese Univ Hong Kong, Dept Math, Hong Kong, Peoples R China.
C3 East China Normal University; Hong Kong University of Science &
   Technology; Chinese University of Hong Kong
RP Zhang, GX (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200062, Peoples R China.; Zeng, TY (corresponding author), Chinese Univ Hong Kong, Dept Math, Hong Kong, Peoples R China.
EM fmfang@cs.ecnu.edu.cn; ttwang@stu.ecnu.edu.cn; yangwang@ust.hk;
   zeng@math.cuhk.edu.hk; gxzhang@cs.ecnu.edu.cn
RI Zeng, Tieyong/B-7147-2009
OI ZENG, Tieyong/0000-0002-0688-202X
FU Key Project of the National Natural Science Foundation of China
   [61731009]; National Science Foundation of China [61871185, 11671002];
   NSFC/RGC Joint Research Scheme - Research Grants Council of the Hong
   Kong Special Administrative Region, China; National Natural Science
   Foundation of China [N_CUHK415/19]; Chenguang Program - Shanghai
   Education Development Foundation; Shanghai Municipal Education
   Commission [17CG25, RGC 14300219]; CUHK start-up [4053296]; CUHK [DAG
   4053342]
FX This work was supported in part by the Key Project of the National
   Natural Science Foundation of China under Grant 61731009, in part by the
   National Science Foundation of China under Grants 61871185 and 11671002,
   in part by the NSFC/RGC Joint Research Scheme sponsored by the Research
   Grants Council of the Hong Kong Special Administrative Region, China,
   and the National Natural Science Foundation of China (Project No.
   N_CUHK415/19), and in part by the "Chenguang Program" supported by the
   Shanghai Education Development Foundation and Shanghai Municipal
   Education Commission (17CG25), RGC 14300219, CUHK start-up 4053296, and
   CUHK DAG 4053342.
CR Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Ancuti C, 2016, IEEE IMAGE PROC, P2256, DOI 10.1109/ICIP.2016.7532760
   [Anonymous], 2010, THESIS
   [Anonymous], 2010, ACM T GRAPHIC
   [Anonymous], 2002, PROC IEEE C COMPUT V
   [Anonymous], 1954, PHYS TODAY
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bin Xie, 2010, Proceedings 2010 International Conference on Intelligent System Design and Engineering Application (ISDEA 2010), P848, DOI 10.1109/ISDEA.2010.141
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Caraffa L, 2013, IEEE INT VEH SYM, P994, DOI 10.1109/IVS.2013.6629596
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Galdran A, 2018, PROC CVPR IEEE, P8212, DOI 10.1109/CVPR.2018.00857
   Galdran A, 2017, IEEE SIGNAL PROC LET, V24, P151, DOI 10.1109/LSP.2016.2643168
   Galdran A, 2015, SIAM J IMAGING SCI, V8, P1519, DOI 10.1137/15M1008889
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He LY, 2017, IEEE T IMAGE PROCESS, V26, P1063, DOI 10.1109/TIP.2016.2644267
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li Y, 2015, IEEE I CONF COMP VIS, P226, DOI 10.1109/ICCV.2015.34
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Liu Q, 2018, IEEE T IMAGE PROCESS, V27, P5178, DOI 10.1109/TIP.2018.2849928
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Ng MK, 2011, SIAM J IMAGING SCI, V4, P345, DOI 10.1137/100806588
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   Omer I, 2004, PROC CVPR IEEE, P946
   Podpora M., 2014, FEDCSIS POSITION PAP, P29, DOI DOI 10.15439/2014F206
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Sulami M, 2014, IEEE INT CONF COMPUT
   Tai XC, 2009, LECT NOTES COMPUT SC, V5567, P502
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang JB, 2015, NEUROCOMPUTING, V149, P718, DOI 10.1016/j.neucom.2014.08.005
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Xu K, 2012, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION APPLICATIONS (ICCIA 2012), P663
   Yan Wang, 2010, 2010 IEEE International Conference on Intelligent Computing and Intelligent Systems (ICIS 2010), P789, DOI 10.1109/ICICISYS.2010.5658614
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhu MZ, 2018, IEEE SIGNAL PROC LET, V25, P174, DOI 10.1109/LSP.2017.2780886
   Zhu SY, 2018, IEEE T MULTIMEDIA, V20, P525, DOI 10.1109/TMM.2017.2749162
   Zomet A, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P27, DOI 10.1109/ACV.2002.1182150
NR 57
TC 26
Z9 26
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2537
EP 2550
DI 10.1109/TMM.2019.2958755
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000005
DA 2024-07-18
ER

PT J
AU Do, TT
   Hoang, T
   Le Tan, DK
   Doan, AD
   Cheung, NM
AF Thanh-Toan Do
   Tuan Hoang
   Le Tan, Dang-Khoa
   Anh-Dzung Doan
   Cheung, Ngai-Man
TI Compact Hash Code Learning With Binary Deep Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary constraint optimization; image search; learning to hash
ID QUANTIZATION
AB Learning compact binary codes for image retrieval problem using deep neural networks has recently attracted increasing attention. However, training deep hashing networks is challenging due to the binary constraints on the hash codes. In this paper, we propose deep network models and learning algorithms for learning binary hash codes given image representations under both unsupervised and supervised manners. The novelty of our network design is that we constrain one hidden layer to directly output the binary codes. This design has overcome a challenging problem in some previous works: optimizing non-smooth objective functions because of binarization. In addition, we propose to incorporate independence and balance properties in the direct and strict forms into the learning schemes. We also include a similarity preserving property in our objective functions. The resulting optimizations involving these binary, independence, and balance constraints are difficult to solve. To tackle this difficulty, we propose to learn the networks with alternating optimization and careful relaxation. Furthermore, by leveraging the powerful capacity of convolutional neural networks, we propose an end-to-end architecture that jointly learns to extract visual features and produce binary hash codes. Experimental results for the benchmark datasets show that the proposed methods compare favorably or outperform the state of the art.
C1 [Thanh-Toan Do] Univ Liverpool, Liverpool L69 3BX, Merseyside, England.
   [Tuan Hoang; Le Tan, Dang-Khoa; Cheung, Ngai-Man] Singapore Univ Technol & Design, Singapore 487372, Singapore.
   [Anh-Dzung Doan] Univ Adelaide, Adelaide, SA 5005, Australia.
C3 University of Liverpool; Singapore University of Technology & Design;
   University of Adelaide
RP Do, TT (corresponding author), Univ Liverpool, Liverpool L69 3BX, Merseyside, England.
EM thanh-toan.do@liverpool.ac.uk; nguyenanhtuan_hoang@mymail.sutd.edu.sg;
   letandang_khoa@sutd.edu.sg; dung.doan@adelaide.edu.au;
   ngaiman_cheung@sutd.edu.sg
RI Doan, Anh-Dzung/HNP-8107-2023
OI Doan, Dung/0000-0001-5517-070X; NGUYEN ANH TUAN,
   HOANG/0000-0002-1076-8043; Cheung, Ngai-Man (Man)/0000-0003-0135-3791
FU National Research Foundation Singapore under its AI Singapore Programme
   Award [AISG-100E-2018-005]; ST Electronics; National Research Foundation
   (NRF), Prime Minister's Office, Singapore under Corporate Laboratory at
   University Scheme (Programme Title: STEE Infosec - SUTD Corporate
   Laboratory)
FX This research was supported in part by the National Research Foundation
   Singapore under its AI Singapore Programme Award AISG-100E-2018-005, and
   in part by both ST Electronics and the National Research Foundation
   (NRF), Prime Minister's Office, Singapore under Corporate Laboratory at
   University Scheme (Programme Title: STEE Infosec - SUTD Corporate
   Laboratory).
CR [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], MNIST DATABASE HANDW
   [Anonymous], 2009, NIPS
   [Anonymous], 2009, NEURIPS
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Do TT, 2018, IEEE T PATTERN ANAL, V40, P626, DOI 10.1109/TPAMI.2017.2686861
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Grauman K., 2013, Machine Learning for Computer Vision, V411, P49, DOI 10.1007/978-3-642-28661-2_3
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Irschara A, 2009, PROC CVPR IEEE, P2591, DOI 10.1109/CVPRW.2009.5206587
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Krizhevsky A., 2012, ADV NEURAL INF PROCE, V25, P1097
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li WJ, 2016, IJCAI, P1711
   Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P2352, DOI 10.1109/TIP.2017.2678163
   Mao XJ, 2017, IEEE T MULTIMEDIA, V19, P382, DOI 10.1109/TMM.2016.2614858
   Tran NT, 2019, IEEE T IMAGE PROCESS, V28, P1675, DOI 10.1109/TIP.2018.2881829
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Sattler T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.76
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Do TT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314051
   Do TT, 2019, IEEE T IMAGE PROCESS, V28, P4954, DOI 10.1109/TIP.2019.2913509
   Do TT, 2016, LECT NOTES COMPUT SC, V9906, P802, DOI 10.1007/978-3-319-46475-6_49
   Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14
   Do TT, 2015, PROC CVPR IEEE, P3556, DOI 10.1109/CVPR.2015.7298978
   Torii A, 2015, IEEE T PATTERN ANAL, V37, P2346, DOI 10.1109/TPAMI.2015.2409868
   Hoang T, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1600, DOI 10.1145/3123266.3123417
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang J., 2014, CoRR
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Zhang HW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P781, DOI 10.1145/2964284.2964308
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 67
TC 15
Z9 15
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 992
EP 1004
DI 10.1109/TMM.2019.2935680
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Fan, CL
   Yen, SC
   Huang, CY
   Hsu, CH
AF Fan, Ching-Ling
   Yen, Shou-Cheng
   Huang, Chun-Ying
   Hsu, Cheng-Hsin
TI Optimizing Fixation Prediction Using Recurrent Neural Networks for 360°
   Video Streaming in Head-Mounted Virtual Reality
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360 degrees video; Virtual Reality; HMD; prediction; machine learning;
   RNN; tiled streaming
ID SALIENCY
AB We study the problem of predicting the viewing probability of different parts of 360. videos when streaming them to head-mounted displays. We propose a fixation prediction network based on recurrent neural network, which leverages sensor and content features. The content features are derived by computer vision (CV) algorithms, which may suffer from inferior performance due to various types of distortion caused by diverse 360. video projection models. We propose a unified approach with overlapping virtual viewports to eliminate such negative effects, andwe evaluate our proposed solution using severalCValgorithms, such as saliency detection, face detection, and object detection. We find that overlapping virtual viewports increase the performance of these existing CV algorithms that were not trained for 360. videos. We next fine-tune our fixation prediction network with diverse design options, including: 1) with or without overlapping virtual viewports, 2) with or without future content features, and 3) different feature sampling rates. We empirically choose the best fixation prediction network and use it in a 360. video streaming system. We conduct extensive trace-driven simulations with a large-scale dataset to quantify the performance of the 360. video streaming system with different fixation prediction algorithms. The results show that our proposed fixation prediction network outperforms other algorithms in several aspects, such as: 1) achieving comparable video quality (average gaps between -0.05 and 0.92 dB), 2) consuming much less bandwidth (average bandwidth reduction by up to 8Mb/s), 3) reducing the rebuffering time (on average 40 s in bandwidth-limited 4G cellular networks), and 4) running in real-time (at most 124 ms).
C1 [Fan, Ching-Ling; Yen, Shou-Cheng; Hsu, Cheng-Hsin] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
   [Huang, Chun-Ying] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
C3 National Tsing Hua University; National Yang Ming Chiao Tung University
RP Hsu, CH (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
EM ch.ling.fan@gmail.com; shoucheng.yen@gmail.com; chuang@cs.nctu.edu.tw;
   cha16@alumni.sfu.ca
FU Ministry of Science and Technology of Taiwan [107-2221-E-007-091-MY3,
   107-2221-E-009-028-MY3]; NOVATEK Fellowship
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under Grants #107-2221-E-007-091-MY3 and
   #107-2221-E-009-028-MY3 and in part by a NOVATEK Fellowship.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Alface PR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1105, DOI 10.1145/3123266.3123307
   Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], MPEG DASH CLIENT SER
   [Anonymous], FAC OC RIFT
   [Anonymous], NS 3 NETW SIM
   [Anonymous], JVET ITU T JOINT VID
   [Anonymous], 2016, HTC VIVE
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2017, GLOB STAT MOB NETW
   [Anonymous], SURVEY 360 VIDEO STR
   [Anonymous], 360 DEGREE VIDEO CAS
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], SAMS GEAR VR
   [Anonymous], 2014, COMPUT VISUAL MEDIA
   [Anonymous], 2016, 2016 IEEE INT C MULT
   [Anonymous], ZETTE ER TRENDS AN
   Aparicio-Pardo Ramon., 2015, Proceedings of the 6th ACM Multimedia Systems Conference, MMSys'15, P49
   Assens M, 2017, IEEE INT CONF COMP V, P2331, DOI 10.1109/ICCVW.2017.275
   Ayrey B., 2017, INTRO FACEBOOK 360 G
   Ban Y., 2018, PROC IEEE INT C MULT, P1
   Bao Y., 2017, PROC IEEE INT C SENS, P1
   Ben Yahia M, 2018, IEEE INT SYM MULTIM, P89, DOI 10.1109/ISM.2018.00023
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Bradski G, 2000, DR DOBBS J, V25, P120
   Chaabouni S, 2016, IEEE IMAGE PROC, P1604, DOI 10.1109/ICIP.2016.7532629
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   D'Acunto L, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P302, DOI 10.1145/2910017.2910634
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duanmu F, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P13, DOI 10.1145/3097895.3097898
   Fan C, 2017, SIXTEENTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, P67
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu CW, 2009, IEEE T MULTIMEDIA, V11, P634, DOI 10.1109/TMM.2009.2017626
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Galleguillos C, 2010, COMPUT VIS IMAGE UND, V114, P712, DOI 10.1016/j.cviu.2010.02.004
   Graf M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P261, DOI 10.1145/3083187.3084016
   Hjelmås E, 2001, COMPUT VIS IMAGE UND, V83, P236, DOI 10.1006/cviu.2001.0921
   Huang CY, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2537855
   IDC, 2017, WORLDW AUGM VIRT REA
   Kavak Y, 2017, SIGNAL PROCESS-IMAGE, V51, P13, DOI 10.1016/j.image.2016.11.003
   Khiem N. Quang Minh, 2011, P 2 ANN ACM C MULT S, P211, DOI DOI 10.1145/1943552.1943581
   Kopf J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982405
   Kuzyakov E., 2016, Next-generation video encoding techniques for 360 video and VR-Engineering at Meta
   Langley A, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P183, DOI 10.1145/3098822.3098842
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Lin KM, 2016, COMPUT GRAPH FORUM, V35, P479, DOI 10.1111/cgf.12848
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1477, DOI 10.1109/ICME.2006.262821
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Lo WC, 2018, IEEE INT SYM MULTIM, P44, DOI 10.1109/ISM.2018.00016
   Lo WC, 2017, ASIA-PAC NETW OPER M, P205, DOI 10.1109/APNOMS.2017.8094203
   Lucas B., 1981, Proc. DARPA Image Understanding Workshop, P121
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Mavlankar A, 2010, SIGNALS COMMUN TECHN, P431, DOI 10.1007/978-3-642-12802-8_19
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Misra K, 2013, IEEE J-STSP, V7, P969, DOI 10.1109/JSTSP.2013.2271451
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Nguyen T.V., 2013, ACM MM, P987
   Niamut OA, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P46, DOI 10.1145/2910017.2910606
   Petrangeli S, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P225, DOI 10.1145/3083187.3083224
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   Redmon A., 2017, P IEEE C COMP VIS PA, P7263, DOI 10.1109/cvpr.2017.690
   Skupin R, 2017, CONSUM COMM NETWORK, P613, DOI 10.1109/CCNC.2017.7983191
   Viitanen M, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1179, DOI 10.1145/2964284.2973796
   Wagner D, 2010, P IEEE VIRT REAL ANN, P211, DOI 10.1109/VR.2010.5444786
   Wang H, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2801123
   Wang KZ, 2015, IEEE T IMAGE PROCESS, V24, P3019, DOI 10.1109/TIP.2015.2432712
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Yao H, 2019, WIRELESS NETW-GER, P1, DOI 10.1007/978-3-030-15028-0
   Yen S.-C., 2019, P 24 ACM WORKSH PACK, P7
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhou C, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P27, DOI 10.1145/3083187.3083190
NR 79
TC 26
Z9 31
U1 4
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 744
EP 759
DI 10.1109/TMM.2019.2931807
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700014
DA 2024-07-18
ER

PT J
AU Li, JN
   Wong, YK
   Zhao, Q
   Kankanhalli, MS
AF Li, Junnan
   Wong, Yongkang
   Zhao, Qi
   Kankanhalli, Mohan S.
TI Video Storytelling: Textual Summaries for Events
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Semantics; Streaming media; Recurrent
   neural networks; Measurement; Natural languages; Video storytelling;
   video captioning; sentence retrieval; multimodal embedding learning
ID VISION; IMAGE
AB Bridging vision and natural language is a longstanding goal in computer vision and multimedia research. While earlier works focus on generating a single-sentence description for visual content, recent works have studied paragraph generation. In this paper, we introduce the problem of video storytelling, which aims at generating coherent and succinct stories for long videos. Video storytelling introduces new challenges, mainly due to the diversity of the story and the length and complexity of the video. We propose novel methods to address the challenges. First, we propose a context-aware framework for multimodal embedding learning, where we design a residual bidirectional recurrent neural network to leverage contextual information from past and future. The multimodal embedding is then used to retrieve sentences for video clips. Second, we propose a Narrator model to select clips that are representative of the underlying storyline. The Narrator is formulated as a reinforcement learning agent, which is trained by directly optimizing the textual metric of the generated story. We evaluate our method on the video story dataset, a new dataset that we have collected to enable the study. We compare our method with multiple state-of-the-art baselines and show that our method achieves better performance, in terms of quantitative measures and user study.
C1 [Li, Junnan] Natl Univ Singapore, Sch Integrat Sci & Engn, Singapore 119077, Singapore.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Zhao, Qi] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
C3 National University of Singapore; National University of Singapore;
   University of Minnesota System; University of Minnesota Twin Cities
RP Li, JN (corresponding author), Natl Univ Singapore, Sch Integrat Sci & Engn, Singapore 119077, Singapore.
EM lijunnan@u.nus.edu; yongkang.wong@nus.edu.sg; qzhao@cs.umn.edu;
   mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019; zhao, qi/KGK-3760-2024
OI Kankanhalli, Mohan/0000-0002-4846-2015; Wong,
   Yongkang/0000-0002-1239-4428
FU National Research Foundation, Prime Minister's Office, Singapore under
   its Strategic Capability Research Centres Funding Initiative
FX This work was supported by the National Research Foundation, Prime
   Minister's Office, Singapore under its Strategic Capability Research
   Centres Funding Initiative. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Marco
   Bertini.
CR [Anonymous], N AM CHAPTER ASS COM
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], IEEE T CIRCUITS SYST
   Ba J., 2015, P ICLR, P1, DOI DOI 10.1016/J.JCYT.2014.02.008
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Bingjie Xu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P2019, DOI 10.1109/CVPR.2019.00212
   Chen B, 2017, CLUSTER COMPUT, V20, P413, DOI 10.1007/s10586-017-0731-9
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Devlin J., 2018, BERT PRE TRAINING DE
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Dufaux F, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P275, DOI 10.1109/ICIP.2000.899354
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Goldman DB, 2006, ACM T GRAPHIC, V25, P862, DOI 10.1145/1141911.1141967
   Gong BQ, 2014, ADV NEUR IN, V27
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Hall DT, 2015, RES CAREERS, P1
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Huang T.-H., 2016, NAACL HLT, P1233
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kilickaya M, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P199
   Kingma D. P., 2014, arXiv
   Kiros Ryan., 2014, ABS14112539 CORR
   Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lan SY, 2018, PROC CVPR IEEE, P6771, DOI 10.1109/CVPR.2018.00708
   Li Jiwei, 2016, NAACL, P110
   Li J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1, DOI 10.1145/3123266.3123432
   Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289
   Liang XD, 2017, IEEE I CONF COMP VIS, P3382, DOI 10.1109/ICCV.2017.364
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu AA, 2018, IEEE ACCESS, V6, P68463, DOI 10.1109/ACCESS.2018.2879642
   Liu AA, 2017, COMPUT VIS IMAGE UND, V163, P113, DOI 10.1016/j.cviu.2017.04.013
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Liu Y, 2017, AAAI CONF ARTIF INTE, P1445
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park C. C., 2015, Advances in neural information processing systems, P73
   Plummer BA, 2017, PROC CVPR IEEE, P1052, DOI 10.1109/CVPR.2017.118
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Ranzato M, 2016, 4 INT C LEARN REPR I
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rohrbach A, 2014, LECT NOTES COMPUT SC, V8753, P184, DOI 10.1007/978-3-319-11752-2_15
   Sah S, 2017, IEEE WINT CONF APPL, P989, DOI 10.1109/WACV.2017.115
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Tejero-de-Pablos A, 2018, IEEE T MULTIMEDIA, V20, P2000, DOI 10.1109/TMM.2018.2794265
   Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386
   Varini P, 2017, IEEE T MULTIMEDIA, V19, P2832, DOI 10.1109/TMM.2017.2705915
   Vasudevan AB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P582, DOI 10.1145/3123266.3123297
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yeung SH, 2014, 2014 IEEE INTERNATIONAL WIRELESS SYMPOSIUM (IWS)
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhang K, 2016, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2016.120
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 73
TC 17
Z9 17
U1 4
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 554
EP 565
DI 10.1109/TMM.2019.2930041
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tu, ZZ
   Xia, T
   Li, CL
   Wang, XX
   Ma, Y
   Tang, J
AF Tu, Zhengzheng
   Xia, Tian
   Li, Chenglong
   Wang, Xiaoxiao
   Ma, Yan
   Tang, Jin
TI RGB-T Image Saliency Detection via Collaborative Graph Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image saliency detection; RGB-thermal fusion; Collaborative graph; Joint
   optimization; Benchmark dataset
ID OBJECT DETECTION
AB Image saliency detection is an active research topic in the community of computer vision and multimedia. Fusing complementary RGB and thermal infrared data has been proven to be effective for image saliency detection. In this paper, we propose an effective approach for RGB-T image saliency detection. Our approach relies on a novel collaborative graph learning algorithm. In particular, we take superpixels as graph nodes, and collaboratively use hierarchical deep features to jointly learn graph affinity and node saliency in a unified optimization framework. Moreover, we contribute a more challenging dataset for the purpose of RGB-T image saliency detection, which contains 1000 spatially aligned RGB-T image pairs and their ground truth annotations. Extensive experiments on the public dataset and the newly created dataset suggest that the proposed approach performs favorably against the state-of-the-art RGB-T saliency detection methods.
C1 [Tu, Zhengzheng; Xia, Tian; Li, Chenglong; Wang, Xiaoxiao; Ma, Yan; Tang, Jin] Anhui Univ, Sch Comp Sci & Technol, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
   [Li, Chenglong] Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Li, CL (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
EM zhengzhengahu@163.com; tianxia.ahu@foxmail.com; lcl1314@foxmail.com;
   xiaoxiao9212@foxmail.com; m17856174397@163.com; tangjin@ahu.edu.cn
RI Li, Chenglong/AAH-4234-2019
FU NSFC [61860206004]; NationalNatural Science Foundation of China
   [61602006, 61702002, 61872005]; Natural Science Foundation of Anhui
   Province [1808085QF187]; Natural Science Foundation of Anhui Higher
   Education Institution of China [KJ2017A017]; Institute of Physical
   Science and Information Technology, Anhui University
FX This work was supported in part by the NSFC Key Projects in
   International (Regional) Cooperation and Exchanges underGrant
   61860206004, in part by theNationalNatural Science Foundation of China
   under Grants 61602006, 61702002, and 61872005, in part by the Natural
   Science Foundation of Anhui Province under Grant 1808085QF187, in part
   by the Natural Science Foundation of Anhui Higher Education Institution
   of China under Grant KJ2017A017, and in part by the Open Fund for
   Discipline Construction, Institute of Physical Science and Information
   Technology, Anhui University. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Xiaochun Cao.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2018, ARXIV180508982
   Bai S, 2016, LECT NOTES COMPUT SC, V9906, P592, DOI 10.1007/978-3-319-46475-6_37
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Dou H, 2017, IEEE T MULTIMEDIA, V19, P1718, DOI 10.1109/TMM.2017.2689327
   Gao F, 2018, IEEE ACCESS, V6, P1000, DOI 10.1109/ACCESS.2017.2777444
   Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940
   Guo XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3547
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang F, 2017, IEEE T IMAGE PROCESS, V26, P1911, DOI 10.1109/TIP.2017.2669878
   Jiang B, 2018, NEUROCOMPUTING, V314, P215, DOI 10.1016/j.neucom.2018.06.064
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li C, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P359
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49
   Li CL, 2017, AAAI CONF ARTIF INTE, P4126
   Li CL, 2019, IEEE T PATTERN ANAL, V41, P2770, DOI 10.1109/TPAMI.2018.2864965
   Li CL, 2017, IEEE T SYST MAN CY-S, V47, P673, DOI 10.1109/TSMC.2016.2627052
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin ZC, 2011, PROG MOL BIOL TRANSL, V98, P1, DOI 10.1016/B978-0-12-385506-0.00001-6
   Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/TPAMI.2018.2865311, 10.1109/INTMAG.2018.8508195]
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Wang QS, 2016, PROC CVPR IEEE, P535, DOI 10.1109/CVPR.2016.64
   Wang TT, 2016, LECT NOTES COMPUT SC, V9912, P450, DOI 10.1007/978-3-319-46484-8_27
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang S, 2018, IEEE T CIRC SYST VID, V28, P2574, DOI 10.1109/TCSVT.2017.2721460
   Zhang LH, 2018, IEEE T IMAGE PROCESS, V27, P987, DOI 10.1109/TIP.2017.2766787
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zhu XZ, 2018, NEUROCOMPUTING, V312, P239, DOI 10.1016/j.neucom.2018.05.106
NR 44
TC 110
Z9 114
U1 5
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 160
EP 173
DI 10.1109/TMM.2019.2924578
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, CX
   Yan, TK
   Luo, X
   Nie, LQ
   Xu, XS
AF Li, Chuan-Xiang
   Yan, Ting-Kun
   Luo, Xin
   Nie, Liqiang
   Xu, Xin-Shun
TI Supervised Robust Discrete Multimodal Hashing for Cross-Media Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximate nearest neighbor search; learning to hash; multimodal
   hashing; cross-media retrieval; discrete hashing
ID IMAGE SEARCH; SCALE; ALGORITHMS; CODES
AB The hashing-based approximate nearest neighbors search is able to reduce the storage cost and improve query speed. Therefore, they have attracted much attention in these years. Moreover, some hashing methods have been proposed for cross-modal retrieval tasks. However, there are still some issues that need to be further addressed. For example, some of them only construct a simple similarity matrix when learning hash functions or binary codes, which may lose some useful information. Some of them solve the hard discrete optimization problem by relaxing the binary constraints and quantizing the solution to obtain the final results, which may generate large quantization errors. To address these challenges, we present a new supervised cross-modal hashing method, named supervised robust discrete multimodal hashing (SRDMH). Specifically, it incorporates full label information into the hash functions learning to preserve the similarity in the original space. In addition, instead of relaxing the binary constraints, it is able to learn the binary codes and hash functions simultaneously. Moreover, it adopts a flexible $\ell _{2,p}$ loss with nonlinear kernel embedding and introduces an intermediate presentation of the binary codes. In light of this, it becomes more robust and easier to solve by an iterative algorithm presented in this paper. To evaluate its performance, we conduct extensive experiments on three benchmark datasets. The results verify that SRDMH outperforms seven state-of-the-art cross-modal hashing methods. In addition, we also extend it to the classification task. Compared with other hashing methods, SRDMH also obtains better results when its binary codes are used for classification.
C1 [Li, Chuan-Xiang; Luo, Xin; Xu, Xin-Shun] Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
   [Yan, Ting-Kun; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Shandong, Peoples R China.
C3 Shandong University; Shandong University
RP Xu, XS (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
EM chuanxiang.lee@gmail.com; yantk.sdu@gmail.com; luoxin.lxin@gmail.com;
   nieliqiang@gmail.com; xuxinshun@sdu.edu.cn
RI ARSLAN, Okan/AAA-3232-2020; Luo, Xin/HNR-3191-2023
OI Luo, Xin/0000-0002-6901-5476
FU National Natural Science Foundation of China [61872428, 61573212,
   61772310]; Key Research and Development Program of Shandong Province
   [2018CXGC0708]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61872428, Grant 61573212, and Grant
   61772310, and in part by the Key Research and Development Program of
   Shandong Province under Grant 2018CXGC0708. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Zhu Li.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2012, NEURIPS
   [Anonymous], 2012, Book A probabilistic model for multimodal hash function learning, DOI DOI 10.1145/2339530.2339678
   [Anonymous], 2009, NIPS
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao Y, 2017, AAAI CONF ARTIF INTE, P3974
   Chen ZD, 2018, AAAI CONF ARTIF INTE, P274
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Hao YB, 2017, IEEE T IMAGE PROCESS, V26, P5531, DOI 10.1109/TIP.2017.2737329
   Hu ZH, 2017, PROC CVPR IEEE, P3288, DOI 10.1109/CVPR.2017.350
   Huang HJ, 2017, IEEE INT CON MULTI, P1159, DOI 10.1109/ICME.2017.8019499
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu H, 2017, P INT COMP SOFTW APP, P738, DOI 10.1109/COMPSAC.2017.131
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180
   Liu XL, 2014, PATTERN RECOGN, V47, P748, DOI 10.1016/j.patcog.2013.08.022
   Long MS, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P579, DOI 10.1145/2911451.2911493
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2518
   Luo X, 2019, IEEE T IMAGE PROCESS, V28, P2962, DOI 10.1109/TIP.2019.2892703
   Luo X, 2018, ACM/SIGIR PROCEEDINGS 2018, P735, DOI 10.1145/3209978.3210035
   Peng Y., 2018, ARXIV180202904
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Sheng Q, 2013, 2013 FIFTH INTERNATIONAL CONFERENCE ON GEO-INFORMATION TECHNOLOGIES FOR NATURAL DISASTER MANAGEMENT (GIT4NDM), P143, DOI 10.1109/GIT4NDM.2013.27
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang M, 2015, IEEE T CYBERNETICS, V45, P1561, DOI 10.1109/TCYB.2014.2356136
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xu XS, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P17, DOI 10.1109/CISP-BMEI.2016.7852675
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yan TK, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1271, DOI 10.1145/2983323.2983743
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yang Y, 2015, IEEE T CYBERNETICS, V45, P1069, DOI 10.1109/TCYB.2014.2344015
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P212, DOI 10.1109/TCSVT.2017.2771332
   Zhang J, 2020, IEEE T CYBERNETICS, V50, P489, DOI 10.1109/TCYB.2018.2868826
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang PF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1762, DOI 10.1145/3123266.3123320
   Zhang W, 2018, IEEE T CIRC SYST VID, V28, P2768, DOI 10.1109/TCSVT.2017.2718188
   Zhang W, 2018, IEEE T MULTIMEDIA, V20, P880, DOI 10.1109/TMM.2017.2760102
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P2042, DOI 10.1109/TIP.2017.2672440
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 67
TC 29
Z9 32
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2863
EP 2877
DI 10.1109/TMM.2019.2912714
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000014
DA 2024-07-18
ER

PT J
AU Li, SM
   Han, X
   Chang, YL
AF Li, Sumei
   Han, Xu
   Chang, Yongli
TI Adaptive Cyclopean Image-Based Stereoscopic Image-Quality Assessment
   Using Ensemble Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive cyclopean image; salient map; ensemble learning; stereoscopic
   image quality assessment
AB In this paper, we proposed an effective 3-D image-quality assessment method based on an adaptive cyclopean image by using ensemble learning. Our cyclopean image is not only suitable for a symmetrical distortion image, but also especially suitable for an asymmetrical distortion image. This adaptivity of our cyclopean image can be attributed to the consideration of gain control and gain enhancement in a binocular rivalry visual mechanism. In addition, we use a salient map to modify our cyclopean image to let the salient area of our cyclopean become more attractive. As a result, we can get better results. To remove redundant information out from our cyclopean, the sparse representation is applied to extract essential features. Finally, to get better regression accuracy on extracted feature, we use ensemble learning to get the final quality score of a stereoscopic image. The ensemble learner can improve the regression accuracy by 2% than a single learner. Experimental results show that the proposed algorithm outperforms the state-of-the-art methods on two publicly available stereoscopic image-quality assessment databases LIVE I and LIVE II.
C1 [Li, Sumei; Han, Xu; Chang, Yongli] Tianjin Univ, Dept Informat & Commun Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Han, X (corresponding author), Tianjin Univ, Dept Informat & Commun Engn, Tianjin 300072, Peoples R China.
EM tjnklsm@163.com; 2628426498@qq.com; cyl920611@163.com
OI Li, Sumei/0000-0002-4793-3161; han, xu/0000-0002-1066-5474
FU National Natural Science Foundation of China [61520106002, 61002028]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61520106002 and 61002028.
CR [Anonymous], P IEEE 4 INT C CONS
   [Anonymous], 2017, AM J ROENTGENOL
   [Anonymous], 2017, 2017 INT SMART CIT C
   [Anonymous], IEEE J SEL TOPICS SI
   [Anonymous], P INF TECHN
   [Anonymous], IEEE T SIGN PROCESS
   [Anonymous], COMFORT PERFORMANCE
   [Anonymous], J VIS
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Jia SY, 2014, J SYST ENG ELECTRON, V25, P183, DOI 10.1109/JSEE.2014.00022
   Li KM, 2015, ELECTRON LETT, V51, P1994, DOI 10.1049/el.2015.2049
   Lin YC, 2017, IEEE J-STSP, V11, P89, DOI 10.1109/JSTSP.2016.2632422
   Lin YH, 2014, IEEE T IMAGE PROCESS, V23, P1527, DOI 10.1109/TIP.2014.2302686
   Lu KX, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON INTERNET OF THINGS (ITHINGS) AND IEEE GREEN COMPUTING AND COMMUNICATIONS (GREENCOM) AND IEEE CYBER, PHYSICAL AND SOCIAL COMPUTING (CPSCOM) AND IEEE SMART DATA (SMARTDATA), P750, DOI 10.1109/iThings-GreenCom-CPSCom-SmartData.2016.158
   Lv YQ, 2016, SIGNAL PROCESS-IMAGE, V47, P346, DOI 10.1016/j.image.2016.07.003
   Ma J, 2018, IEEE ACCESS, V6, P2768, DOI 10.1109/ACCESS.2017.2785282
   Maalouf A, 2011, INT CONF ACOUST SPEE, P1161
   Mitchell T.M., 1997, Machine learning, V45, P81
   Paisitkriangkrai S, 2016, IEEE T PATTERN ANAL, V38, P1243, DOI 10.1109/TPAMI.2015.2474388
   Pan D, 2017, INT CONF SYST INFORM, P1297, DOI 10.1109/ICSAI.2017.8248486
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Shao F, 2018, IEEE T MULTIMEDIA, V20, P2605, DOI 10.1109/TMM.2018.2817072
   Shao F, 2017, IEEE T MULTIMEDIA, V19, P1821, DOI 10.1109/TMM.2017.2685240
   Shao F, 2016, IEEE T MULTIMEDIA, V18, P2104, DOI 10.1109/TMM.2016.2594142
   Shao F, 2015, IEEE SIGNAL PROC LET, V22, P1548, DOI 10.1109/LSP.2015.2413946
   Tekin C, 2017, IEEE T SIGNAL PROCES, V65, P888, DOI 10.1109/TSP.2016.2626250
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Xing LY, 2012, IEEE T MULTIMEDIA, V14, P326, DOI 10.1109/TMM.2011.2172402
   Xu XG, 2017, ELECTRON LETT, V53, P1468, DOI [10.1049/el.2017.2625, 10.1049/el.2017.2501]
   Yang YY, 2018, PUBLIC HEALTH NUTR, V21, P2915, DOI 10.1017/S1368980018001982
   You J, 2010, IEEE CONF WIREL MOB, P1, DOI 10.1109/WIMOB.2010.5644989
NR 32
TC 11
Z9 14
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2616
EP 2624
DI 10.1109/TMM.2019.2907470
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400015
DA 2024-07-18
ER

PT J
AU Aldahdooh, A
   Masala, E
   Janssens, O
   Van Wallendael, G
   Barkowsky, M
   Le Callet, P
AF Aldahdooh, Ahmed
   Masala, Enrico
   Janssens, Olivier
   Van Wallendael, Glenn
   Barkowsky, Marcus
   Le Callet, Patrick
TI Improved Performance Measures for Video Quality Assessment Algorithms
   Using Training and Validation Sets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video quality; no-reference VQA; HRC selection; datasets evaluation;
   content features
ID INDEX
AB The training and performance analysis of objective video quality assessment algorithms is complex due to the huge variety of possible content classes and transmission distortions. Several secondary issues such as free parameters in machine learning algorithms and alignment of subjective datasets put an additional burden on the developer. In this paper, three subsequent steps are presented to address such issues. First, the content and coding parameter space of a large-scale database is used to select dedicated subsets for training objective algorithms. This aims at providing a method for selecting the most significant contents and coding parameters from all imaginable combinations. In the practical case where only a limited set is available, it also helps us to avoid redundancy in the training subset selection. The second step is a discussion on performance measures for algorithms that employ machine-learning methods. The particularity of the performance measures is that the quality of the training and verification datasets is taken into consideration. Common issues that often use existing measures are presented, and improved or complementary methods are proposed. The measures are applied to two examples of no-reference objective assessment algorithms using the aforementioned subsets of the large-scale database. While limited in terms of practical applications, this sandbox approach of objectively predicting an objectively evaluated video sequences allows for eliminating additional influence factors from subjective studies. In the third step, the proposed performance measures are applied to the practical case of training and analyzing assessment algorithms on readily available subjectively annotated image datasets. The presentation method in this part of the paper can also be used as an exemplified recommendation for reporting in-depth information on the performance. Using this presentation method, future publications presenting newly developed quality assessment algorithms may be significantly improved.
C1 [Aldahdooh, Ahmed] Univ Coll Appl Sci, Gaza 890, Palestine.
   [Masala, Enrico] Politecn Torino, Control & Comp Engn Dept, I-10129 Turin, Italy.
   [Janssens, Olivier; Van Wallendael, Glenn] Univ Ghent, IDLab, IMEC, B-9052 Ghent, Belgium.
   [Barkowsky, Marcus] Univ Appl Sci, Deggendorf Inst Technol, D-94469 Deggendorf, Germany.
   [Le Callet, Patrick] Univ Nantes, Univ Bretagne Loire, F-44035 Nantes, France.
C3 Polytechnic University of Turin; IMEC; Ghent University; Nantes
   Universite
RP Aldahdooh, A (corresponding author), Univ Coll Appl Sci, Gaza 890, Palestine.
EM adahdooh@ucas.edu.ps; enrico.masala@polito.it;
   odjansse.janssens@ugent.be; glenn.vanwallendael@ugent.be;
   Marcus.Barkowsky@th-deg.de; patrick.lecallet@univ-nantes.fr
RI Van Wallendael, Glenn/H-8315-2015; Le Callet, Patrick/F-5772-2010;
   Masala, Enrico/B-6973-2008
OI Van Wallendael, Glenn/0000-0001-9530-3466; Masala,
   Enrico/0000-0001-8906-354X; Janssens, Olivier/0000-0001-7367-0628
FU Marie Sklodowska-Curie under the PROVISION (PeRceptually Optimised VIdeo
   CompresSION) [608231]
FX This work was supported by the Marie Sklodowska-Curie under the
   PROVISION (PeRceptually Optimised VIdeo CompresSION) project bearing
   Grant 608231 and Call Identifier: FP7-PEOPLE-2013-ITN. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Xiaokang Yang. (Corresponding author: Ahmed
   Aldahdooh.)
CR Aabed Mohammed A., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2015, DOI 10.1109/ICASSP.2014.6853952
   Aldahdooh A., 2016, PROC 8 INT C QUAL MU, P1
   Aldahdooh A, 2018, MULTIMED TOOLS APPL, V77, P16113, DOI 10.1007/s11042-017-5180-1
   Aldahdooh A, 2018, DIGIT SIGNAL PROCESS, V77, P195, DOI 10.1016/j.dsp.2017.09.013
   Anegekuh L, 2015, IEEE T MULTIMEDIA, V17, P1323, DOI 10.1109/TMM.2015.2444098
   [Anonymous], 2015, INT WORKSHOP QUALITY
   [Anonymous], 2016, P PICT COD S PCS
   [Anonymous], IEEE T MULTIMED
   [Anonymous], 2016, MULTIMEDIA SIGNAL PR
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], 2003, FINAL REPORT VIDEO Q
   Barkowsky M, 2015, IEICE T COMMUN, VE98B, P2, DOI 10.1587/transcom.E98.B.2
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Engelke U, 2009, SIGNAL PROCESS-IMAGE, V24, P525, DOI 10.1016/j.image.2009.06.005
   Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001
   Gu K, 2015, IEEE T BROADCAST, V61, P520, DOI 10.1109/TBC.2015.2459851
   Gu K, 2015, IEEE T CIRC SYST VID, V25, P1480, DOI 10.1109/TCSVT.2014.2372392
   Guo JF, 2016, INT CONF COMP SCI ED, P712, DOI 10.1109/ICCSE.2016.7581667
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   He X, 2014, P 8 INT WORKSHOP DAT, P1, DOI [DOI 10.1145/2648584.2648589, 10.1145/2648584.2648589]
   Izumi Kosuke, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P49, DOI 10.1109/QoMEX.2014.6982287
   Janssens O, 2016, ENG APPL ARTIF INTEL, V55, P331, DOI 10.1016/j.engappai.2016.08.003
   Le Callet P., 2005, Subjective quality assessment irccyn/ivc database
   Li Y, 2008, IEEE T MULTIMEDIA, V10, P885, DOI 10.1109/TMM.2008.922860
   Min XK, 2018, SIGNAL PROCESS, V145, P127, DOI 10.1016/j.sigpro.2017.10.025
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Naccari M, 2009, IEEE T MULTIMEDIA, V11, P932, DOI 10.1109/TMM.2009.2021785
   Narwaria M, 2012, IEEE T MULTIMEDIA, V14, P525, DOI 10.1109/TMM.2012.2190589
   Narwaria M, 2012, IEEE T SYST MAN CY B, V42, P347, DOI 10.1109/TSMCB.2011.2163391
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Ponomarenko N, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P407
   Shahid M, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-40
   Sung YH, 2012, IEEE T MULTIMEDIA, V14, P693, DOI 10.1109/TMM.2012.2186793
   Vandewiele G, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1469, DOI 10.1145/3041021.3051699
   Yi HR, 2005, PATTERN RECOGN LETT, V26, P1221, DOI 10.1016/j.patrec.2004.11.011
   Yuen M, 1998, SIGNAL PROCESS, V70, P247, DOI 10.1016/S0165-1684(98)00128-5
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zhang CS, 2017, EXPERT SYST APPL, V82, P128, DOI 10.1016/j.eswa.2017.04.003
NR 42
TC 9
Z9 9
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2026
EP 2041
DI 10.1109/TMM.2018.2882091
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700011
DA 2024-07-18
ER

PT J
AU Cui, RP
   Liu, H
   Zhang, CS
AF Cui, Runpeng
   Liu, Hu
   Zhang, Changshui
TI A Deep Neural Framework for Continuous Sign Language Recognition by
   Iterative Training
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Continuous sign language recognition; sequence learning; iterative
   training; multimodal fusion
ID HAND GESTURE RECOGNITION
AB This work develops a continuous sign language (SL) recognition framework with deep neural networks, which directly transcribes videos of SL sentences to sequences of ordered gloss labels. Previous methods dealing with continuous SL recognition usually employ hidden Markov models with limited capacity to capture the temporal information. In contrast, our proposed architecture adopts deep convolutional neural networks with stacked temporal fusion layers as the feature extraction module, and bidirectional recurrent neural networks as the sequence learning module. We propose an iterative optimization process for our architecture to fully exploit the representation capability of deep neural networks with limited data. We first train the end-to-end recognition model for alignment proposal, and then use the alignment proposal as strong supervisory information to directly tune the feature extraction module. This training process can run iteratively to achieve improvements on the recognition performance. We further contribute by exploring the multimodal fusion of RGB images and optical flow in sign language. Our method is evaluated on two challenging SL recognition benchmarks, and outperforms the state of the art by a relative improvement of more than 15% on both databases.
C1 [Cui, Runpeng; Liu, Hu; Zhang, Changshui] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst,Dept Aut, Inst Artificial Intelligence,Tsinghua Univ THUAI, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Cui, RP (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst,Dept Aut, Inst Artificial Intelligence,Tsinghua Univ THUAI, Beijing 100084, Peoples R China.
EM crp16@mails.tsinghua.edu.cn; liuhu15@mails.tsinghua.edu.cn;
   zcs@mail.tsinghua.edu.cn
RI Wang, Ling/AGR-4917-2022; Zhang, Chang/HTO-2939-2023; ARSLAN,
   Okan/AAA-3232-2020; CUI, Runpeng/AAK-3464-2020
OI Wang, Ling/0000-0003-0272-2974; Cui, Runpeng/0000-0002-4737-788X
FU National Natural Science Foundation of China (NSFC) [61876095,
   61751308]; Beijing Natural Science Foundation [L172037]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61876095 and Grant 61751308, and
   in part by the Beijing Natural Science Foundation under Grant L172037.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Guillaume Gravier.
CR [Anonymous], WORKSH EUR C COMP VI
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2012, IEEE COMP SOC C COMP, DOI DOI 10.1109/CVPRW.2012.6239187
   [Anonymous], ARXIV14126980 THEAN
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2008, IEEE INT C AUTOMATIC, DOI [DOI 10.1109/AFGR.2008.4813472, 10.1109/AFGR.2008.4813472]
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, ARXIV150702159
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], ARXIV160802059
   [Anonymous], 2002, P 15 INT C NEUR INF
   [Anonymous], 2018, INT J COMPUT VISION, DOI DOI 10.1007/s11263-016-0957-7
   Bengio Y., 2014, TECHNICAL REPORT
   Bowden R, 2016, P 7 WORKSH REPR PROC, P121
   Buehler P, 2009, PROC CVPR IEEE, P2953, DOI 10.1109/CVPRW.2009.5206523
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cooper H, 2012, J MACH LEARN RES, V13, P2205
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Evangelidis GD, 2015, LECT NOTES COMPUT SC, V8925, P595, DOI 10.1007/978-3-319-16178-5_42
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Forster J, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1911
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2016, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR.2016.412
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Molchanov Pavlo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301342
   Molchanov P., 2016, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, P4207, DOI DOI 10.1109/CVPR.2016.456
   Monnier C, 2015, LECT NOTES COMPUT SC, V8925, P491, DOI 10.1007/978-3-319-16178-5_34
   Ong SCW, 2005, IEEE T PATTERN ANAL, V27, P873, DOI 10.1109/TPAMI.2005.112
   Pfister T, 2014, LECT NOTES COMPUT SC, V8694, P814, DOI 10.1007/978-3-319-10599-4_52
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang LC, 2014, IEEE T MULTIMEDIA, V16, P751, DOI 10.1109/TMM.2014.2298382
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
NR 48
TC 164
Z9 174
U1 1
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1880
EP 1891
DI 10.1109/TMM.2018.2889563
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700021
DA 2024-07-18
ER

PT J
AU Zhang, ZJ
   Wu, Q
   Wang, Y
   Chen, F
AF Zhang, Zongjian
   Wu, Qiang
   Wang, Yang
   Chen, Fang
TI High-Quality Image Captioning With Fine-Grained and Semantic-Guided
   Visual Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image captioning; attention mechanism; fine-grained resolution; semantic
   guidance; fully convolutional network-long short term memory framework
AB The soft-attention mechanism is regarded as one of the representative methods for image captioning. Based on the end-to-end convolutional neural network (CNN)-long short term memory (LSTM) framework, the soft-attention mechanism attempts to link the semantic representation in text (i.e., captioning) with relevant visual information in the image for the first time. Motivated by this approach, several state-of-the-art attention methods are proposed. However, due to the constraints of CNN architecture, the given image is only segmented to the fixed-resolution grid at a coarse level. The visual feature extracted from each grid indiscriminately fuses all inside objects and/or their portions. There is no semantic link between grid cells. In addition, the large area "stuff" (e.g., the sky or a beach) cannot be represented using the current methods. To address these problems, this paper proposes a new model based on the fully convolutional network (FCN)-LSTM framework, which can generate an attention map at a fine-grained grid-wise resolution. Moreover, the visual feature of each grid cell is contributed only by the principal object. By adopting the grid-wise labels (i.e., semantic segmentation), the visual representations of different grid cells are correlated to each other. With the ability to attend to large area "stuff," our method can further summarize an additional semantic context from semantic labels. This method can provide comprehensive context information to the language LSTM decoder. In this way, a mechanism of fine-grained and semantic-guided visual attention is created, which can accurately link the relevant visual information with each semantic meaning inside the text. Demonstrated by three experiments including both qualitative and quantitative analyses, our model can generate captions of high quality, specifically high levels of accuracy, completeness, and diversity. Moreover, our model significantly outperforms all other methods that use VGG-based CNN encoders without fine-tuning.
C1 [Zhang, Zongjian; Wu, Qiang; Chen, Fang] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Wang, Yang] CSIRO, Data61, Eveleigh, NSW 2015, Australia.
C3 University of Technology Sydney; Commonwealth Scientific & Industrial
   Research Organisation (CSIRO)
RP Zhang, ZJ (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
EM Zongjian.Zhang@student.uts.edu.au; Qiang.Wu@uts.edu.au;
   Yang.Wang@data61.csiro.au; Fang.Chen@uts.edu.au
RI Chen, Fang/HKF-8052-2023
OI Chen, Fang/0000-0002-5660-5143; Chen, Fang/0000-0003-4971-8729; Wu,
   Qiang/0000-0001-5641-2483; Wang, Yang/0000-0002-6815-0879
CR [Anonymous], 1997, NEURAL COMPUT
   Bahdanau D., 2015, P INT C LEARN REPR, P940
   Baraldi L, 2017, IEEE T MULTIMEDIA, V19, P955, DOI 10.1109/TMM.2016.2644872
   Biswas P, 2005, I CONF VLSI DESIGN, P651
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Chen L., 2017, IEEE T PATTERN ANAL, V40, P824
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Mao J., 2015, P INT C LEARN REPR, P1000
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park CC, 2017, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2017.681
   Ramanishka V, 2017, PROC CVPR IEEE, P3135, DOI 10.1109/CVPR.2017.334
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tavakoli HR, 2017, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2017.272
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang LJ, 2017, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR.2017.214
   Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yao T, 2017, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR.2017.559
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang TZ, 2012, IEEE T MULTIMEDIA, V14, P1206, DOI 10.1109/TMM.2012.2191944
   Zhang ZJ, 2018, IEEE WINT CONF APPL, P1709, DOI 10.1109/WACV.2018.00190
   Zhou LW, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P305, DOI 10.1145/3126686.3126717
NR 48
TC 51
Z9 53
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1681
EP 1693
DI 10.1109/TMM.2018.2888822
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700006
OA Green Published
DA 2024-07-18
ER

PT J
AU Wang, QR
   Yuan, C
   Wang, JD
   Zeng, WJ
AF Wang, Qiurui
   Yuan, Chun
   Wang, Jingdong
   Zeng, Wenjun
TI Learning Attentional Recurrent Neural Network for Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tracking; recurrent neural networks; attention model
ID OBJECT TRACKING
AB Existing visual tracking methods face many challenges: 1) the changed size and number of targets over time, occlusion in discrete frames, and mis-identification for crossing targets. Long short-term memory (LSTM) has the advantage of modeling long-term tasks and is suitable for tracking. We propose a novel online attentional recurrent neural network (ARNN) model for visual tracking, whose core component is a two-layer bidirectional LSTM along the x-and y-axes. Several bidirectional LSTMs can be cascaded or parallelly connected together to exploit multiscale target features and can give more precise tracked object locations. Each bidirectional LSTM utilizes the convolutional features of a convolutional neural network inside two bounding boxes from two frames to check whether the target in the current frame is the one in previous frames. An attention mechanism is also adopted to enhance the proposed model to better express the patch-level features of the tracking targets. Interattention and intra-attention models are proposed to imitate the temporal and spatial tracking mechanism of primate visual cortex. Interattention learns to overcome the occlusion problem, and intra-attention is able to mark important regions to better trace the target. The bidirectional LSTM and the attention mechanism are jointly trained. The combination of them further improves the accuracy of target tracking in videos. The outstanding performances in the experiments demonstrate the effectiveness of our proposed online method ARNN and yield competitive results compared with the state-of-the-art tracking methods.
C1 [Wang, Qiurui; Yuan, Chun] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
   [Wang, Jingdong; Zeng, Wenjun] Microsoft Res Asia, Internet Media Grp, Beijing 100080, Peoples R China.
C3 Tsinghua University; Microsoft Research Asia; Microsoft
RP Yuan, C (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
EM wangqr12@mails.tsinghua.edu.cn; yuanc@sz.tsinghua.edu.cn;
   jingdw@microsoft.com; wezeng@microsoft.com
RI ; Wang, Jingdong/E-9920-2017
OI Wang, Qiurui/0000-0003-0508-3418; Wang, Jingdong/0000-0002-4888-4445
FU National High Technology Research and Development Plan (863 Plan)
   [2015AA015803]; National Natural Science Foundation of China [U1433112];
   Joint Research Center of Tencent; Tsinghua University
FX This work was supported in part by the National High Technology Research
   and Development Plan (863 Plan) under Grant 2015AA015803, in part by the
   National Natural Science Foundation of China Project under Grant
   U1433112, and in part by the Joint Research Center of Tencent and
   Tsinghua University. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Xavier
   Giro-i-Nieto. (Corresponding author: Chun Yuan.)
CR Altwaijry H, 2016, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2016.385
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, ARXIV150401942
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 2016, CVPR
   [Anonymous], 2015, Deep learning
   [Anonymous], P INT C IM PROC
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2016, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2015.2509974
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468
   Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347
   Cui Z, 2016, PROC CVPR IEEE, P1449, DOI 10.1109/CVPR.2016.161
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Geiger A, 2014, IEEE T PATTERN ANAL, V36, P1012, DOI 10.1109/TPAMI.2013.185
   Haque A, 2016, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2016.138
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kieritz H, 2016, 2016 13TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P122, DOI 10.1109/AVSS.2016.7738059
   Kim C, 2015, IEEE I CONF COMP VIS, P4696, DOI 10.1109/ICCV.2015.533
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Liang M., 2015, Advances in Neural Information Processing Systems, P937
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Meng JJ, 2016, IEEE T MULTIMEDIA, V18, P116, DOI 10.1109/TMM.2015.2500734
   Milan A, 2017, AAAI CONF ARTIF INTE, P4225
   Milan A, 2015, PROC CVPR IEEE, P5397, DOI 10.1109/CVPR.2015.7299178
   Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103
   Ondrúska P, 2016, AAAI CONF ARTIF INTE, P3361
   Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang Siyu., 2015, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P5033
   Tieleman T., 2012, COURSERA NEURAL NETW
   Wang, 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1601.06823
   Wang B, 2017, IEEE T PATTERN ANAL, V39, P589, DOI 10.1109/TPAMI.2016.2551245
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiang Y, 2015, IEEE I CONF COMP VIS, P4705, DOI 10.1109/ICCV.2015.534
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155
   Yuan Li, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2953, DOI 10.1109/CVPRW.2009.5206735
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 67
TC 37
Z9 39
U1 3
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 930
EP 942
DI 10.1109/TMM.2018.2869277
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700010
DA 2024-07-18
ER

PT J
AU Chen, YT
   Wang, J
   Bai, YN
   Castañón, G
   Saligrama, V
AF Chen, Yuting
   Wang, Joseph
   Bai, Yannan
   Castanon, Gregory
   Saligrama, Venkatesh
TI Probabilistic Semantic Retrieval for Surveillance Videos With Activity
   Graphs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Activity retrieval; grounding; probabilistic model; surveillance video;
   subgraph matching
ID SHOT EVENT DETECTION; ACTION RECOGNITION; FUSION
AB We present a novel framework for finding complex activities matching user-described queries in cluttered surveillance videos. The wide diversity of queries coupled with the unavailability of annotated activity data limits our ability to train activity models. To bridge the semantic gap, we propose letting users describe an activity as a semantic graph with object attributes and inter-object relationships associated with nodes and edges, respectively. We learn node/edge-level visual predictors during training and, at test-time, propose retrieving activity by identifying likely locations that match the semantic graph. We formulate a novel conditional random field-based probabilistic activity localization objective that accounts for misdetections, misclassifications and track losses, and outputs a likelihood score for a candidate grounded location of the query in the video. We seek groundings that maximize overall precision and recall. To handle the combinatorial search over all high-probability groundings, we propose a highest precision subgraph matching algorithm. Our method outperforms existing retrieval methods on benchmarked datasets.
C1 [Chen, Yuting; Wang, Joseph; Bai, Yannan; Castanon, Gregory; Saligrama, Venkatesh] Boston Univ, Boston, MA 02215 USA.
   [Wang, Joseph] Amazon, Cambridge, MA 02142 USA.
   [Castanon, Gregory] Syst & Technol Res, Woburn, MA 01801 USA.
C3 Boston University; Amazon.com
RP Saligrama, V (corresponding author), Boston Univ, Boston, MA 02215 USA.
EM yutingch@bu.edu; wangjose@amazon.com; ynbai@bu.edu;
   gregory.castanon@stresearch.com; srv@bu.edu
OI Chen, Yuting/0000-0003-2698-408X; Saligrama,
   Venkatesh/0000-0002-0675-2268
FU ONR [N00014-13-C-0288]; U.S. Department of Homeland Security, Science
   and Technology Directorate, Office of University Programs
   [2013-ST-061-ED00]; NGA Grant [HM1582-09-1-0037]
FX This work was supported in part by ONR contract N00014-13-C-0288, the
   U.S. Department of Homeland Security, Science and Technology
   Directorate, Office of University Programs, under Grant Award
   2013-ST-061-ED00, and NGA Grant HM1582-09-1-0037. The views and
   conclusions contained in this document are those of the authors and
   should not be interpreted as necessarily representing the social
   policies, either expressed or implied, of the ONR, NGA or DHS. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yap-Peng Tan. (Corresponding
   author: Venkatesh Saligrama.)
CR Andriyenko A, 2012, PROC CVPR IEEE, P1926, DOI 10.1109/CVPR.2012.6247893
   [Anonymous], 2017, ARXIV170508421
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2016, PROCEEDINGS OF THE I, DOI DOI 10.1109/CVPR.2016.649
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], COMP VIS ECCV WORKSH
   [Anonymous], 2016, PIOTRS COMPUTER VISI
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Castanon G.D., 2012, Proceedings of the 20th ACM international conference on Multimedia, P309
   Castañón G, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P391, DOI 10.1145/2733373.2806229
   Chang XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2234
   Chen X, 2012, IEEE T MULTIMEDIA, V14, P3, DOI 10.1109/TMM.2011.2167223
   Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Choe TE, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P787, DOI 10.1109/ICCVW.2013.108
   Choi W, 2014, IEEE T PATTERN ANAL, V36, P1242, DOI 10.1109/TPAMI.2013.220
   Das A, 2014, LECT NOTES COMPUT SC, V8690, P330, DOI 10.1007/978-3-319-10605-2_22
   Divakaran A., 2014, U.S. Patent App, Patent No. [14/286,305, 14286305]
   Elhoseiny M, 2016, AAAI CONF ARTIF INTE, P3478
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gan C, 2015, AAAI CONF ARTIF INTE, P3769
   Han XT, 2017, IEEE T MULTIMEDIA, V19, P1583, DOI 10.1109/TMM.2017.2671414
   Hao YB, 2017, IEEE T IMAGE PROCESS, V26, P5531, DOI 10.1109/TIP.2017.2737329
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   Jain M, 2015, IEEE I CONF COMP VIS, P4588, DOI 10.1109/ICCV.2015.521
   Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Lafferty John, 2001, INT C MACH LEARN ICM
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lin DH, 2014, PROC CVPR IEEE, P2657, DOI 10.1109/CVPR.2014.340
   Lin J, 2017, IEEE T MULTIMEDIA, V19, P1968, DOI 10.1109/TMM.2017.2713410
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Ma ZG, 2014, IEEE T PATTERN ANAL, V36, P1789, DOI 10.1109/TPAMI.2014.2306419
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Mazloom M, 2016, IEEE T MULTIMEDIA, V18, P1378, DOI 10.1109/TMM.2016.2559947
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Oh SM, 2011, PROC CVPR IEEE
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Platt JC, 2000, ADV NEUR IN, P61
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shu TM, 2015, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR.2015.7299088
   Shunguang Wu, 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P1254
   Song H, 2018, IEEE T MULTIMEDIA, V20, P1088, DOI 10.1109/TMM.2017.2763322
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   Wang F, 2014, IEEE T MULTIMEDIA, V16, P1303, DOI 10.1109/TMM.2014.2315780
   Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu S, 2014, PROC CVPR IEEE, P2665, DOI 10.1109/CVPR.2014.341
   Xiao JJ, 2010, PROC CVPR IEEE, P679, DOI 10.1109/CVPR.2010.5540151
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P1342, DOI 10.1109/TMM.2008.2004912
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang Y, 2013, IEEE I CONF COMP VIS, P2104, DOI 10.1109/ICCV.2013.456
   Ye H, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P435, DOI 10.1145/2671188.2749406
   Yu LT, 2016, IEEE T MULTIMEDIA, V18, P1590, DOI 10.1109/TMM.2016.2557059
   Zhang SW, 2018, IEEE T MULTIMEDIA, V20, P769, DOI 10.1109/TMM.2017.2758524
   Zhang Y, 2016, PROC CVPR IEEE, P5985, DOI 10.1109/CVPR.2016.644
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
   Zheng WS, 2016, IEEE T PATTERN ANAL, V38, P591, DOI 10.1109/TPAMI.2015.2453984
NR 65
TC 9
Z9 11
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 704
EP 716
DI 10.1109/TMM.2018.2865860
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Qian, YF
   Wu, D
   Hossain, MS
   Ghoneim, A
   Chen, M
AF Zhang, Yin
   Qian, Yongfeng
   Wu, Di
   Hossain, M. Shamim
   Ghoneim, Ahmed
   Chen, Min
TI Emotion-Aware Multimedia Systems Security
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Security analysis; Access control; emotion interaction; identity
   authentication; social robot
ID ACCESS-CONTROL; CLOUD; AUTHENTICATION; INTERNET; THINGS
AB The interactive robot is expected to support emotion analysis and utilize the deep learning and machine learning to provide users with continuous emotional care. However, it is a great challenge to securely acquire sufficient data for emotion analysis such that the privacy of emotional data is adequately protected. To address the security issue, this paper proposes a security policy based on identity authentication and access control to ensure the security certificate through an interactive robot or edge devices while the access control of private data stored in the edge cloud is adequately protected. Specifically, this paper adopts a polynomial-based access control policy and designs a secure and effective access control scheme. At the same time, this paper puts forward an identity authentication mechanism in view of edge cloud systems, which can reduce the computational overhead and authentication delay in a collaborative authentication of multiple edge clouds. The effectiveness of the proposed access control policy and identity authentication mechanism is verified by an actual testbed platform.
C1 [Zhang, Yin] Zhongnan Univ Econ & Law, Sch Informat & Safety Engn, Wuhan 430073, Hubei, Peoples R China.
   [Qian, Yongfeng] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Hubei, Peoples R China.
   [Wu, Di] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Wu, Di] Guangdong Key Lab Big Data Anal & Proc, Guangzhou 510006, Guangdong, Peoples R China.
   [Hossain, M. Shamim; Ghoneim, Ahmed] King Saud Univ, Dept Software Engn, Coll Comp & Informat Sci, Riyadh 11543, Saudi Arabia.
   [Ghoneim, Ahmed] Menoufia Univ, Dept Math & Comp Sci, Fac Sci, Shibin Al Kawm 32511, Egypt.
   [Chen, Min] Wuhan Natl Lab Optoelect, Wuhan 430074, Hubei, Peoples R China.
   [Chen, Min] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Hubei, Peoples R China.
C3 Zhongnan University of Economics & Law; China University of Geosciences;
   Sun Yat Sen University; King Saud University; Egyptian Knowledge Bank
   (EKB); Menofia University; Huazhong University of Science & Technology;
   Huazhong University of Science & Technology
RP Qian, YF (corresponding author), China Univ Geosci, Sch Comp Sci, Wuhan 430074, Hubei, Peoples R China.
EM yin.zhang.cn@ieee.org; yongfengqian@ieee.org; wudi27@mail.sysu.edu.cn;
   mshossain@ksu.edu.sa; ghoneim@ksu.edu.sa; minchen2012@hust.edu.cn
RI Chen, Min/N-9350-2015; Zhang, Yin/O-2149-2015; Wu, Di/HNP-3772-2023;
   Zhang, Yin/K-2414-2019; wu, di/IYS-9217-2023; Qian,
   Yongfeng/ABB-3098-2021; Guizani, Mohsen/AAX-4534-2021; ghoneim,
   ahmed/L-3019-2013; Hossain, M. Shamim/K-1362-2014
OI Chen, Min/0000-0002-0960-4447; Zhang, Yin/0000-0002-1772-0763; Zhang,
   Yin/0000-0002-8103-8937; Qian, Yongfeng/0000-0001-7204-8829; Guizani,
   Mohsen/0000-0002-8972-8094; Hossain, M. Shamim/0000-0001-5906-9422;
   Ghoneim, Ahmed/0000-0003-2076-8925
FU Deanship of Scientific Research at King Saud University, Riyadh, Saudi
   Arabia [RGP-229]
FX This work was supported by the Deanship of Scientific Research at King
   Saud University, Riyadh, Saudi Arabia through the research group Project
   RGP-229. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Honggang Wang. (Corresponding
   author: Yongfeng Qiang.)
CR Ali M, 2015, INFORM SCIENCES, V305, P357, DOI 10.1016/j.ins.2015.01.025
   [Anonymous], 2014, P 3 WORKSH HOT TOP S
   Bacis E., 2016, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, P217
   Butun I, 2016, IEEE COMMUN MAG, V54, P47, DOI 10.1109/MCOM.2016.7452265
   Chen M, 2018, IEEE ACCESS, V6, P64766, DOI 10.1109/ACCESS.2018.2877919
   Chen M, 2017, IEEE ACCESS, V5, P326, DOI 10.1109/ACCESS.2016.2641480
   Chen X, 2016, IEEE ACM T NETWORK, V24, P2827, DOI 10.1109/TNET.2015.2487344
   Chen Y, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1375, DOI 10.1145/3133956.3134102
   Cheng L, 2017, IEEE T INF FOREN SEC, V12, P604, DOI 10.1109/TIFS.2016.2624741
   Derbez P, 2016, LECT NOTES COMPUT SC, V9815, P157, DOI 10.1007/978-3-662-53008-5_6
   Ferrag MA, 2017, SECUR COMMUN NETW, DOI 10.1155/2017/6562953
   Lopez PG, 2015, ACM SIGCOMM COMP COM, V45, P37, DOI 10.1145/2831347.2831354
   Hao YX, 2019, IEEE MULTIMEDIA, V26, P31, DOI 10.1109/MMUL.2018.2879590
   He DB, 2015, IEEE T INF FOREN SEC, V10, P2681, DOI 10.1109/TIFS.2015.2473820
   He DB, 2015, IEEE INTERNET THINGS, V2, P72, DOI 10.1109/JIOT.2014.2360121
   He WJ, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P255
   Jung TH, 2015, IEEE T INF FOREN SEC, V10, P190, DOI 10.1109/TIFS.2014.2368352
   Kim KS, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1449, DOI 10.1145/3133956.3133970
   Liu H, 2015, IEEE T PARALL DISTR, V26, P241, DOI 10.1109/TPDS.2014.2308218
   Liu JK, 2016, IEEE T INF FOREN SEC, V11, P484, DOI 10.1109/TIFS.2015.2493983
   Ni JB, 2018, IEEE COMMUN SURV TUT, V20, P601, DOI 10.1109/COMST.2017.2762345
   Qian YF, 2018, COMPUT ELECTR ENG, V72, P266, DOI 10.1016/j.compeleceng.2018.08.021
   Qian YF, 2019, INFORM FUSION, V46, P141, DOI 10.1016/j.inffus.2018.06.004
   Tong L., 2016, P 35 ANN IEEE INT C, P1
   Yang K, 2014, IEEE T PARALL DISTR, V25, P1735, DOI 10.1109/TPDS.2013.253
   Yang Y, 2019, INFORM SCIENCES, V479, P567, DOI 10.1016/j.ins.2018.02.005
   Younis YA, 2014, J INF SECUR APPL, V19, P45, DOI 10.1016/j.jisa.2014.04.003
   Zhang LH, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P57, DOI 10.1145/3133956.3133962
   Zhou J, 2015, IEEE T PARALL DISTR, V26, P1693, DOI 10.1109/TPDS.2014.2314119
NR 29
TC 57
Z9 59
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 617
EP 624
DI 10.1109/TMM.2018.2882744
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800008
DA 2024-07-18
ER

PT J
AU Li, BC
   Liu, XZ
   Dinesh, K
   Duan, ZY
   Sharma, G
AF Li, Bochen
   Liu, Xinzhao
   Dinesh, Karthik
   Duan, Zhiyao
   Sharma, Gaurav
TI Creating a Multitrack Classical Music Performance Dataset for Multimodal
   Music Analysis: Challenges, Insights, and Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal music dataset; audio-visual analysis; music performance;
   synchronization
ID TRANSCRIPTION
AB We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches to address the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely used music audio datasets. We anticipate that the dataset will be useful for the development and evaluation of existing music information retrieval (MIR) tasks, as well as for novel multimodal tasks. We benchmark two existing MIR tasks (multipitch analysis and score-informed source separation) on the dataset and compare them with other existing music audio datasets. In addition, we consider two novel multimodal MIR tasks (visually informed multipitch analysis and polyphonic vibrato analysis) enabled by the dataset and provide evaluation measurements and baseline systems for future comparisons (from our recent work). Finally, we propose several emerging research directions that the dataset enables.
C1 [Li, Bochen; Liu, Xinzhao; Dinesh, Karthik; Duan, Zhiyao; Sharma, Gaurav] Univ Rochester, Dept Elect & Comp Engn, Rochester, NY 14627 USA.
   [Liu, Xinzhao] Listent Amer Corp, Bothell, WA 98021 USA.
C3 University of Rochester
RP Li, BC (corresponding author), Univ Rochester, Dept Elect & Comp Engn, Rochester, NY 14627 USA.
EM bochen.li@rochester.edu; xinzhao.liu@rochester.edu;
   kdinesh@rochester.edu; zhiyao.duan@rochester.edu;
   gaurav.sharma@rochester.edu
RI Sharma, Gaurav/A-1154-2007; Sharma, Gaurav/HTO-3197-2023
OI Sharma, Gaurav/0000-0001-9735-9519; Duan, Zhiyao/0000-0002-8334-9974
FU National Science Foundation [1741472]; Direct For Computer & Info Scie &
   Enginr; Div Of Information & Intelligent Systems [1741472] Funding
   Source: National Science Foundation
FX This work was supported in part by the National Science Foundation under
   Grant 1741472. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Martha Larson.
   (Bochen Li and Xinzhao Liu contributed equally to this work.)
CR Abesser J, 2011, INT CONF ACOUST SPEE, P189
   Akbari M, 2015, IEEE T MULTIMEDIA, V17, P2113, DOI 10.1109/TMM.2015.2473702
   [Anonymous], 2007, EURASIP J ADV SIG PR
   [Anonymous], 2016, arXiv
   [Anonymous], 2018, P EUROPEAN C COMPUTE
   Bay M., 2009, ISMIR, P315, DOI [DOI 10.5281/ZENODO.1418241, 10.5281/zenodo.1418241]
   Bazzica A., 2017, arXiv preprint arXiv:1706.09556
   Bazzica A, 2016, COMPUT VIS IMAGE UND, V144, P188, DOI 10.1016/j.cviu.2015.09.009
   Bazzica Alessio., 2014, Proceedings of the International Society for Music Information Retrieval (ISMIR), P201
   Benetos E, 2012, EUR SIGNAL PR CONF, P2153
   Bittner R. M., 2014, P 15 INT SOC MUS INF, V14, P155
   Burns AM., 2006, Proceedings of the International Conference on New Interfaces for Musical Expression, P196
   Caramiaux B, 2012, J NEW MUSIC RES, V41, P13, DOI 10.1080/09298215.2011.643314
   Cartwright M., 2014, Proceedings of the 19th international conference on Intelligent User Interfaces, P365, DOI [DOI 10.1145/2557500.2557530, 10.1145/2557500.2557530]
   Chan TS, 2015, INT CONF ACOUST SPEE, P718, DOI 10.1109/ICASSP.2015.7178063
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Dinesh K, 2017, INT CONF ACOUST SPEE, P3021, DOI 10.1109/ICASSP.2017.7952711
   Duan ZY, 2014, IEEE-ACM T AUDIO SPE, V22, P138, DOI 10.1109/TASLP.2013.2285484
   Duan ZY, 2011, IEEE J-STSP, V5, P1205, DOI 10.1109/JSTSP.2011.2159701
   Duan ZY, 2010, IEEE T AUDIO SPEECH, V18, P2121, DOI 10.1109/TASL.2010.2042119
   Emiya V, 2010, IEEE T AUDIO SPEECH, V18, P1643, DOI 10.1109/TASL.2009.2038819
   Fritsch J, 2013, INT CONF ACOUST SPEE, P888, DOI 10.1109/ICASSP.2013.6637776
   Fujishima T., 1999, P INT COMP MUS C, P464
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gillet O, 2005, INT CONF ACOUST SPEE, P205
   Gillet O., 2006, P 7 INT SOC MUS INF, P156
   Gorodnichy D.O., 2006, The 3rd Canadian Conference on Computer and Robot Vision, P63
   Hargreaves S, 2012, IEEE T AUDIO SPEECH, V20, P2637, DOI 10.1109/TASL.2012.2209419
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Kerdvibulvech C, 2007, LECT NOTES COMPUT SC, V4872, P625
   Li B., 2017, P SOUND MUS COMP, P159
   Li B., 2017, P INT SOC MUS INF RE, P123
   Li BC, 2017, INT CONF ACOUST SPEE, P2906, DOI 10.1109/ICASSP.2017.7952688
   Marchini M, 2014, J NEW MUSIC RES, V43, P303, DOI 10.1080/09298215.2014.922999
   Mauch Matthias, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P659, DOI 10.1109/ICASSP.2014.6853678
   Mauch Matthias, 2015, P INT C TECHN MUS NO, P23
   Miron M, 2016, J ELECTR COMPUT ENG, V2016, DOI 10.1155/2016/8363507
   Murphy D., 2003, P DAN C PATT REC IM, V2003, P59
   Nishimura T., 2002, ISMIR, P287
   Oka A, 2013, KOR-JPN JT WORKS FR, P1, DOI 10.1109/FCV.2013.6485449
   Paleari M, 2008, IEEE IMAGE PROC, P93, DOI 10.1109/ICIP.2008.4711699
   Parekh S, 2017, INT CONF ACOUST SPEE, P6, DOI 10.1109/ICASSP.2017.7951787
   Pätynen J, 2008, ACTA ACUST UNITED AC, V94, P856, DOI 10.3813/AAA.918104
   Perez-Carrillo Alfonso., 2015, Proceedings of the International Symposium on Computer Music Multidisciplinary Research (CMMR), P71
   Platz F, 2012, MUSIC PERCEPT, V30, P71, DOI 10.1525/MP.2012.30.1.71
   Radicioni Daniele., 2004, Proceedings of the Conference on Interdisciplinary Musicology (CIM), V17, P97
   Scarr J., 2010, 25th International Conference of Image and Vision Computing New Zealand (IVCNZ), P1
   SHIMODA S, 1989, IEEE T BROADCAST, V35, P357, DOI 10.1109/11.40835
   Shlizerman E., 2017, ARXIV171209382
   Su L., 2015, PROC CMMR S, P309
   Su L, 2014, IEEE-ACM T AUDIO SPE, V22, P2122, DOI 10.1109/TASLP.2014.2362006
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Tomasi C, 1991, DETECTION TRACKING P
   Tsay CJ, 2013, P NATL ACAD SCI USA, V110, P14580, DOI 10.1073/pnas.1221454110
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005
   Vinyes M., 2008, MTG MASS database
   Yao BP, 2013, IEEE I CONF COMP VIS, P2512, DOI 10.1109/ICCV.2013.312
   Zhang Bingjun., 2007, P ACM INT C MULT, P521
NR 58
TC 60
Z9 66
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 522
EP 535
DI 10.1109/TMM.2018.2856090
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400020
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Qiao, YG
   Jiao, LC
   Yang, SY
   Hou, BA
AF Qiao, Yiguo
   Jiao, Licheng
   Yang, Shuyuan
   Hou, Biao
TI A Novel Segmentation Based Depth Map Up-Sampling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; SLIC; region-growing; region merging; joint
   trilateral filtering
ID IMAGE SEGMENTATION; ADAPTIVE SUPPORT; TEXTURE; VIDEO; SUPERRESOLUTION
AB A novel color image segmentation-based depth map upsampling method is proposed in this paper. In this method, the color image is segmented into a certain number of connected regions first. Based on the segmentation result, the target pixels will be interpolated by the seed pixels(1) regionally. In the segmentation part, simple linear iterative clustering is introduced to generate superpixels in the first place. Then, the obtained superpixels will be judged whether they are correct-clustered or not, and the incorrect-clustered ones will be subdivided with an adaptive region-growing strategy. Third, the regions that have no seed will be constantly merged into their nearest neighbors, until seed pixel can be found in each independent region. Finally, adjacent regions that have quite small depth gaps will be united as one. The proposed color image segmentation strictly follows the guidance of the depth; therefore, the segmented regions adhere to the depth boundary well. In the interpolation part, the targets will be interpolated with their surrounding seeds weighted by a joint trilateral filter (JTF). The JTF is constructed by three terms: the color term, the distance term, and the region term, which are driven by the previous segmentation result. Experimental results indicate that our method greatly reduces depth bleeding and depth confusion artifacts, and leads to clear depth boundary in the up-sampled image. Comparisons with the state of art verify the advantages of the proposed method in both visual experience and quantitative evaluations.
C1 [Qiao, Yiguo; Jiao, Licheng; Yang, Shuyuan; Hou, Biao] Xidian Univ, Joint Int Res Lab Intelligent Percept & Computat, Sch Artificial Intelligence,Key Lab Intelligent P, Int Res Ctr Intelligent Percept & Computat,Minist, Xian 710071, Shaanxi, Peoples R China.
C3 Xidian University
RP Qiao, YG (corresponding author), Xidian Univ, Joint Int Res Lab Intelligent Percept & Computat, Sch Artificial Intelligence,Key Lab Intelligent P, Int Res Ctr Intelligent Percept & Computat,Minist, Xian 710071, Shaanxi, Peoples R China.
EM qiaoyiguo123@126.com; lchjiao@mail.xidian.edu.cn; syyang@xidian.edu.cn;
   avcodec@163.com
RI Jiao, Licheng/JOZ-0842-2023
OI Jiao, Licheng/0000-0003-3354-9617; Qiao, Yiguo/0000-0003-4586-3152
FU Major Research Plan of the National Natural Science Foundation of China
   [91438201, 91438103]; China Postdoctoral Science Foundation
   [2017M620441]; Xidian University New Teacher Innovation Fund Project
   [XJS18032]
FX This work was supported in part by the Major Research Plan of the
   National Natural Science Foundation of China under Grant 91438201 and
   Grant 91438103, in part by the China Postdoctoral Science Foundation
   under Grant 2017M620441, and in part by the Xidian University New
   Teacher Innovation Fund Project under Grant XJS18032. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Balakrishnan Prabhakaran.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913
   [Anonymous], 2008, WORKSH MULT MULT SEN
   [Anonymous], 2002, IEEE T PATTERN ANAL
   Choi O, 2014, IEEE T IMAGE PROCESS, V23, P3321, DOI 10.1109/TIP.2014.2329766
   Dehmeshi J, 2008, IEEE T MED IMAGING, V27, P467, DOI 10.1109/TMI.2007.907555
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Foix S, 2011, IEEE SENS J, V11, P1917, DOI 10.1109/JSEN.2010.2101060
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Haris K, 1998, IEEE T IMAGE PROCESS, V7, P1684, DOI 10.1109/83.730380
   Hill B, 1997, ACM T GRAPHIC, V16, P109, DOI 10.1145/248210.248212
   Hong SM, 2016, ASIAPAC SIGN INFO PR
   Hu K., 2013, P IEEE VLSI TEST S, P1
   Jung SW, 2013, IEEE T CIRC SYST VID, V23, P269, DOI 10.1109/TCSVT.2012.2203734
   Kang MK, 2014, IEEE SIGNAL PROC LET, V21, P150, DOI 10.1109/LSP.2013.2295252
   Kang YS, 2014, ELECTRON LETT, V50, P170, DOI 10.1049/el.2013.3956
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kim J, 2014, SIGNAL PROCESS-IMAGE, V29, P506, DOI 10.1016/j.image.2014.01.011
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239453
   Lee SB, 2013, ELECTRON LETT, V49, DOI 10.1049/el.2013.1015
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Liu YJ, 2016, PROC CVPR IEEE, P651, DOI 10.1109/CVPR.2016.77
   Lo KH, 2018, IEEE T CYBERNETICS, V48, P371, DOI 10.1109/TCYB.2016.2637661
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Maugey T, 2016, IEEE T IMAGE PROCESS, V25, P1808, DOI 10.1109/TIP.2016.2530303
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Palomera-Pérez MA, 2010, IEEE T INF TECHNOL B, V14, P500, DOI 10.1109/TITB.2009.2036604
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   PAVLIDIS T, 1990, IEEE T PATTERN ANAL, V12, P225, DOI 10.1109/34.49050
   Petrazzuoli G, 2014, IEEE T MULTIMEDIA, V16, P1834, DOI 10.1109/TMM.2014.2342201
   Ren DN, 2015, IEEE T MULTIMEDIA, V17, P307, DOI 10.1109/TMM.2015.2389714
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tilton JC, 2012, IEEE T GEOSCI REMOTE, V50, P4454, DOI 10.1109/TGRS.2012.2190079
   Ugarriza LG, 2009, IEEE T IMAGE PROCESS, V18, P2275, DOI 10.1109/TIP.2009.2025555
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Yang QX, 2013, IEEE T IMAGE PROCESS, V22, P4841, DOI 10.1109/TIP.2013.2278917
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Yu P, 2012, IEEE T GEOSCI REMOTE, V50, P1302, DOI 10.1109/TGRS.2011.2164085
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 44
TC 14
Z9 14
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 1
EP 14
DI 10.1109/TMM.2018.2845699
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, J
   Du, J
   Dai, LR
AF Zhang, Jianshu
   Du, Jun
   Dai, Lirong
TI Track, Attend, and Parse (TAP): An End-to-End Framework for Online
   Handwritten Mathematical Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Online handwritten mathematical expression recognition (OHMER);
   end-to-end framework; gated recurrent unit (GRU); guided hybrid
   attention (GHA); ensemble
ID STRUCTURAL-ANALYSIS
AB In this paper, we introduce Track, Attend, and Parse (TAP), an end-to-end approach based on neural networks for online handwritten mathematical expression recognition (OHMER). The architecture of TAP consists of a tracker and a parser. The tracker employs a stack of bidirectional recurrent neural networks with gated recurrent units (GRU) to model the input handwritten traces, which can fully utilize the dynamic trajectory information in OHMER. Followed by the tracker, the parser adopts a GRU equipped with guided hybrid attention (GHA) to generate LATEX notations. The proposed GHA is composed of a coverage-based spatial attention, a temporal attention, and an attention guider. Moreover, we demonstrate the strong complementarity between offline information with static-image input and online information with ink-trajectory input by blending a fully convolutional networks-based watcher into TAP. Inherently, unlike traditional methods, this end-to-end framework does not require the explicit symbol segmentation and a predefined expression grammar for parsing. Validated on a benchmark published by the CROHME competition, the proposed approach outperforms the state-of-the-art methods and achieves the best reported results with an expression recognition accuracy of 61.16% on CROHME2014 and 57.02% on CROHME 2016, using only official training dataset.
C1 [Zhang, Jianshu; Du, Jun; Dai, Lirong] Univ Sci & Technol China, Natl Engn Lab Speech & Language Informat Proc, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Du, J (corresponding author), Univ Sci & Technol China, Natl Engn Lab Speech & Language Informat Proc, Hefei 230026, Anhui, Peoples R China.
EM xysszjs@mail.ustc.edu.cn; jundu@ustc.edu.cn; lrdai@ustc.edu.cn
RI DAI, Jinjia/KCL-5110-2024
FU National Key R&D Program of China [2017YFB1002202]; National Natural
   Science Foundation of China [61671422, U1613211]; Key Science and
   Technology Project of Anhui Province [17030901005]; MOE-Microsoft Key
   Laboratory of USTC
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1002202, in part by the National Natural Science
   Foundation of China under Grant 61671422 and Grant U1613211, in part by
   the Key Science and Technology Project of Anhui Province under Grant
   17030901005, and in part by the MOE-Microsoft Key Laboratory of USTC.
CR Alvaro F, 2016, PATTERN RECOGN, V51, P135, DOI 10.1016/j.patcog.2015.09.013
   Alvaro F, 2014, PATTERN RECOGN LETT, V35, P58, DOI 10.1016/j.patrec.2012.09.023
   Anderson R. H., 1967, S INT SYST EXP APPL, P436, DOI DOI 10.1145/2402536.2402585
   [Anonymous], ARXIV180110109
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.345
   [Anonymous], ARXIV14108206
   [Anonymous], ARXIV14061078
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], ARXIV171101889
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], ARXIV151107916
   [Anonymous], 2012, GOOGL MOUNT VIEW 2 A
   [Anonymous], 1994, LaTeX: A Document Preparation System: User's Guide and Reference Manual
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Awal AM, 2014, PATTERN RECOGN LETT, V35, P68, DOI 10.1016/j.patrec.2012.10.024
   Bahdanau D., ICLR
   Bai ZL, 2005, PROC INT CONF DOC, P262
   Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618
   BELAID A, 1984, IEEE T PATTERN ANAL, V6, P105, DOI 10.1109/TPAMI.1984.4767483
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Berger J., 2010, Proceedings of the Python for Scientific Computing Conference (SciPy), number Scipy, P1
   Chan KF, 2001, PATTERN RECOGN, V34, P1671, DOI 10.1016/S0031-3203(00)00102-3
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Chou P. A., 1989, Proceedings of the SPIE - The International Society for Optical Engineering, V1199, P852, DOI 10.1117/12.970095
   Chung J., ARXIV14123555
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   DAMASHEK M, 1995, SCIENCE, V267, P843, DOI 10.1126/science.267.5199.843
   Deng YT, 2017, 34 INT C MACHINE LEA, V70
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1
   Eto Y, 2001, PROC INT CONF DOC, P762, DOI 10.1109/ICDAR.2001.953891
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves Alex, 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721
   Hirata NST, 2015, PATTERN RECOGN, V48, P837, DOI 10.1016/j.patcog.2014.09.015
   Hu L, 2016, INT CONF FRONT HAND, P337, DOI [10.1109/ICFHR.2016.0070, 10.1109/ICFHR.2016.65]
   Jaekyu Ha, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P956, DOI 10.1109/ICDAR.1995.602060
   Kam-Fai Chan, 2000, International Journal on Document Analysis and Recognition, V3, P3, DOI 10.1007/PL00013549
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Klakow D, 2002, SPEECH COMMUN, V38, P19, DOI 10.1016/S0167-6393(01)00041-3
   Kosmala A, 1998, INT C PATT RECOG, P1306, DOI 10.1109/ICPR.1998.711941
   Lavirotte S, 1998, P SOC PHOTO-OPT INS, V3305, P44, DOI 10.1117/12.304644
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Liu CX, 2017, AAAI CONF ARTIF INTE, P4176
   MacLean S, 2013, INT J DOC ANAL RECOG, V16, P139, DOI 10.1007/s10032-012-0184-x
   Mouchère H, 2016, INT CONF FRONT HAND, P607, DOI [10.1109/ICFHR.2016.0116, 10.1109/ICFHR.2016.108]
   Mouchère H, 2014, INT CONF FRONT HAND, P791, DOI 10.1109/ICFHR.2014.138
   Mouchère H, 2016, INT J DOC ANAL RECOG, V19, P173, DOI 10.1007/s10032-016-0263-5
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Rhee TH, 2009, PATTERN RECOGN, V42, P3192, DOI 10.1016/j.patcog.2008.10.036
   Sutskever I, 2014, ADV NEUR IN, V27
   Tu ZP, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P76
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   WINKLER HJ, 1995, INT CONF ACOUST SPEE, P2459, DOI 10.1109/ICASSP.1995.480046
   Xu K., 2015, COMPUTER SCI, P2048
   Yamamoto, 2006, P INT WORKSH FRONT H, P249
   Zanibbi R, 2002, IEEE T PATTERN ANAL, V24, P1455, DOI 10.1109/TPAMI.2002.1046157
   Zeiler M. D., ARXIV12125701
   Zhang JS, 2017, PROC INT CONF DOC, P902, DOI 10.1109/ICDAR.2017.152
   Zhang JS, 2016, INTERSPEECH, P1785, DOI 10.21437/Interspeech.2016-117
   Zhang JS, 2017, PATTERN RECOGN, V71, P196, DOI 10.1016/j.patcog.2017.06.017
   Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
NR 64
TC 68
Z9 78
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 221
EP 233
DI 10.1109/TMM.2018.2844689
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700019
DA 2024-07-18
ER

PT J
AU Chen, YD
   Lai, ZH
   Wong, WK
   Shen, LL
   Hu, QH
AF Chen, Yudong
   Lai, Zhihui
   Wong, Wai Keung
   Shen, Linlin
   Hu, Qinghua
TI Low-Rank Linear Embedding for Image Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Manifold learning; robust regression model; feature selection
ID DISCRIMINANT-ANALYSIS; FACE-RECOGNITION; COMPONENT; REPRESENTATION;
   REGRESSION; FRAMEWORK; 2D-LDA
AB Locality preserving projections (LPP) has been widely studied and extended in recent years, because of its promising performance in feature extraction. In this paper, we propose a modified version of the LPP by constructing a novel regression model. To improve the performance of the model, we impose a low-rank constraint on the regression matrix to discover the latent relations between different neighbors. By using the L-2,L-1-norm as a metric for the loss function, we can further minimize the reconstruction error and derive a robust model. Furthermore, the L-2,L-1-norm regularization term is added to obtain a jointly sparse regression matrix for feature selection. An iterative algorithm with guaranteed convergence is designed to solve the optimization problem. To validate the recognition efficiency, we apply the algorithm to a series of benchmark datasets containing face and character images for feature extraction. The experimental results show that the proposed method is better than some existing methods. The code of this paper can be downloaded from http://www.scholat.com/laizhihui.
C1 [Chen, Yudong; Lai, Zhihui; Shen, Linlin] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Lai, Zhihui; Wong, Wai Keung] Hong Kong Polytech Univ, Hong Kong, Hong Kong, Peoples R China.
   [Hu, Qinghua] Tianjin Univ, Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300072, Peoples R China.
C3 Shenzhen University; Hong Kong Polytechnic University; Tianjin
   University
RP Lai, ZH (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM andrewlin7@qq.com; lai_zhi_hui@163.com; calvin.wong@polyu.edu.hk;
   llshen@szu.edu.cn; huqinghua@tju.edu.cn
RI Lai, Zhihui/R-1000-2019; Shen, Linlin/AEX-9392-2022; Hu,
   Qinghua/B-8857-2008
OI Lai, Zhihui/0000-0002-4388-3080; Shen, Linlin/0000-0003-1420-0815; 
FU Natural Science Foundation of China [61573248, 61773328, 61703283,
   61672357, 61732011]; Natural Science Foundation of Guangdong Province
   [2017A030313367]; Shenzhen Municipal Science and Technology Innovation
   Council [JCYJ20160429182058044, JCYJ20170302153434048]; Hong Kong
   Polytechnic University [G-UA2B]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61573248, 61773328, 61703283, 61672357, and 61732011;
   in part by the Natural Science Foundation of Guangdong Province under
   Grant 2017A030313367; in part by the Shenzhen Municipal Science and
   Technology Innovation Council under Grants JCYJ20160429182058044 and
   JCYJ20170302153434048; and in part by the Hong Kong Polytechnic
   University under Project G-UA2B.
CR [Anonymous], P 2 ACM INT C MULT R
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P 19 ACM SIGKDD C KN
   Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Cai D, 2006, IEEE T IMAGE PROCESS, V15, P3608, DOI 10.1109/TIP.2006.881945
   Chen RJ, 2018, IEEE ACCESS, V6, P15087, DOI 10.1109/ACCESS.2018.2815606
   Chen SB, 2007, NEUROCOMPUTING, V70, P912, DOI 10.1016/j.neucom.2006.10.032
   Chen XJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1525
   De la Torre F, 2012, IEEE T PATTERN ANAL, V34, P1041, DOI 10.1109/TPAMI.2011.184
   Ding C., 2006, P 23 INT C MACH LEAR, P281, DOI DOI 10.1145/1143844.1143880
   Gui J, 2012, PATTERN RECOGN, V45, P2884, DOI 10.1016/j.patcog.2012.02.005
   He XF, 2004, ADV NEUR IN, V16, P153
   Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642
   Huang G.B., 2008, PROC WORKSHOP FACES
   Lai ZH, 2017, IEEE T CYBERNETICS, V47, P3733, DOI 10.1109/TCYB.2016.2578642
   Lai ZH, 2016, IEEE T NEUR NET LEAR, V27, P723, DOI 10.1109/TNNLS.2015.2422994
   Lai ZH, 2014, IEEE T NEUR NET LEAR, V25, P1942, DOI 10.1109/TNNLS.2013.2297381
   Li M, 2005, PATTERN RECOGN LETT, V26, P527, DOI 10.1016/j.patrec.2004.09.007
   Liu S, 2010, INT CONF SIGN PROCES, P1410, DOI 10.1109/ICOSP.2010.5656924
   Liu X, 2013, NEUROIMAGE, V83, P148, DOI 10.1016/j.neuroimage.2013.06.033
   Lu YJ, 2009, IEEE T MULTIMEDIA, V11, P1289, DOI 10.1109/TMM.2009.2030632
   Lu YW, 2017, IEEE T MULTIMEDIA, V19, P2391, DOI 10.1109/TMM.2017.2703130
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Phillips PJ, 1997, PROC CVPR IEEE, P137, DOI 10.1109/CVPR.1997.609311
   Qian JJ, 2015, PATTERN RECOGN, V48, P3145, DOI 10.1016/j.patcog.2015.04.017
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sun BY, 2010, IEEE T NEURAL NETWOR, V21, P163, DOI 10.1109/TNN.2009.2036363
   Wang R, 2017, IEEE T IMAGE PROCESS, V26, P5019, DOI 10.1109/TIP.2017.2726188
   Wang SF, 2011, IEEE T PATTERN ANAL, V33, P2115, DOI 10.1109/TPAMI.2011.88
   Wen JJ, 2016, PATTERN RECOGN, V60, P515, DOI 10.1016/j.patcog.2016.06.006
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Wong WK, 2012, PATTERN RECOGN, V45, P186, DOI 10.1016/j.patcog.2011.05.014
   Xiang Shuo, 2012, KDD, V2012, P480
   Xu Y, 2010, PATTERN RECOGN, V43, P4165, DOI 10.1016/j.patcog.2010.06.016
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P1023, DOI 10.1109/TNNLS.2013.2249088
   Yang Y., 2011, P 22 INT JOINT C ART, P1589
   Yi SY, 2017, PATTERN RECOGN, V61, P524, DOI 10.1016/j.patcog.2016.08.025
   Zhang FL, 2015, IEEE T NEUR NET LEAR, V26, P2247, DOI 10.1109/TNNLS.2014.2376530
   Zhang Z, 2017, IEEE T IMAGE PROCESS, V26, P1607, DOI 10.1109/TIP.2017.2654163
   Zhang Z, 2016, IEEE T IMAGE PROCESS, V25, P2429, DOI 10.1109/TIP.2016.2547180
   Zhang Z, 2013, IEEE T IMAGE PROCESS, V22, P4640, DOI 10.1109/TIP.2013.2277780
   Zhang Z, 2017, IEEE T IMAGE PROCESS, V26, P1466, DOI 10.1109/TIP.2017.2651396
   Zheng WS, 2008, PATTERN RECOGN, V41, P2156, DOI 10.1016/j.patcog.2007.11.025
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 46
TC 35
Z9 36
U1 0
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3212
EP 3222
DI 10.1109/TMM.2018.2834867
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600003
DA 2024-07-18
ER

PT J
AU Bagci, KT
   Tekalp, AM
AF Bagci, K. Tolga
   Tekalp, A. Murat
TI Dynamic Resource Allocation by Batch Optimization for Value-Added Video
   Services Over SDN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple service-levels; value-added services; path computation; batch
   optimization; software defined networking (SDN); quality of service
   (QoS)
ID NETWORKS; ALGORITHMS
AB We propose a video service architecture and a novel resource allocation optimization framework to enable network service providers (NSP) to offer value-added video services (VAVS) over software-defined networking including different service levels, service-level awareness of users, and associated business models. To this effect, we introduce a new batch-optimization framework, where resource (path, bitrate, and admission control) allocations for a small group of flows (consisting of new service requests and some existing ones) are performed simultaneously as the number of new service requests and network conditions vary. The optimization problem becomes NP-complete when path computations are jointly (re-)optimized as a group in order to accommodate all service requests to the extent possible, to best utilize entire network resources in a fair manner, and maximize network service provider's revenue. In order to compute dynamic resource allocations online, we propose a heuristic group-constrained-shortest path procedure that aims for a fair allocation of resources among a group of requests with the same service level, while maximizing the total NSP revenue. Experimental results demonstrate the feasibility of the proposed method for possible deployment by NSP to offer future VAVS, and that the proposed solution is close to the optimal solution, which is approximately computed using a divide-and conquer strategy, for varying network size and traffic load conditions. In particular, we show that processing service requests in batches significantly improves total revenue and fairness in congested mode of operation.
C1 [Bagci, K. Tolga; Tekalp, A. Murat] Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
C3 Koc University
RP Bagci, KT (corresponding author), Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
EM kbagci@ku.edu.tr; mtekalp@ku.edu.tr
RI Tekalp, Murat/AAW-1060-2020
OI Tekalp, Ahmet Murat/0000-0003-1465-8121; Bagci,
   Tolga/0000-0001-8965-0015
FU TUBITAK Project [115E299]; Turkish Academy of Sciences
FX This work was supported by TUBITAK Project 115E299. The work of A. Murat
   Tekalp was supported by the Turkish Academy of Sciences.
CR Akyildiz IF, 2014, COMPUT NETW, V71, P1, DOI 10.1016/j.comnet.2014.06.002
   Alimi R., 2014, 7285 RFC
   [Anonymous], GEN ALG MOD SYST GAM
   [Anonymous], 2009, 5440 RFC
   [Anonymous], 2012, CISC VIS NETW IND GL
   [Anonymous], 3813 RFC
   [Anonymous], 2016, CISC VIS NETW IND GL
   Bentaleb A, 2017, IEEE T MULTIMEDIA, V19, P2136, DOI 10.1109/TMM.2017.2733344
   Calvert KL, 1997, IEEE COMMUN MAG, V35, P160, DOI 10.1109/35.587723
   Chen SG, 2008, IEEE ACM T NETWORK, V16, P105, DOI 10.1109/TNET.2007.897965
   Cheng B, 2015, IEEE T AUTOM SCI ENG, V12, P1104, DOI 10.1109/TASE.2014.2387212
   Cisco, 2017, Cisco7 Feb.
   Egilmez HE, 2014, IEEE T MULTIMEDIA, V16, P1597, DOI 10.1109/TMM.2014.2325791
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   Flach T, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P468, DOI 10.1145/2934872.2934873
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Huang HW, 2014, 2014 IEEE 22ND INTERNATIONAL SYMPOSIUM OF QUALITY OF SERVICE (IWQOS), P141, DOI 10.1109/IWQoS.2014.6914313
   Karakus M., 2016, J NETWORK COMPUT APP
   Kuipers F, 2002, IEEE COMMUN MAG, V40, P50, DOI 10.1109/MCOM.2002.1106159
   Mori T, 2010, LECT NOTES COMPUT SC, V6003, P17, DOI 10.1007/978-3-642-12365-8_2
   Palma David, 2014, 2014 Third European Workshop on Software Defined Networks (EWSDN), P125, DOI 10.1109/EWSDN.2014.34
   Sideris A, 2014, INT CONF TELECOMM, P29, DOI 10.1109/TEMU.2014.6917731
   Veloso E, 2006, IEEE ACM T NETWORK, V14, P133, DOI 10.1109/TNET.2005.863709
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu JY, 2017, IEEE J SEL AREA COMM, V35, P30, DOI 10.1109/JSAC.2016.2632599
   Xue GL, 2008, IEEE ACM T NETWORK, V16, P656, DOI 10.1109/TNET.2007.900712
   Yang J, 2017, IEEE T MULTIMEDIA, V19, P619, DOI 10.1109/TMM.2016.2629280
   Zegura EW, 1996, IEEE INFOCOM SER, P594, DOI 10.1109/INFCOM.1996.493353
   Zhang SY, 2010, IEEE T WIREL COMMUN, V9, P1370, DOI 10.1109/TWC.2010.04.081437
   Zhu ZQ, 2013, IEEE T MULTIMEDIA, V15, P758, DOI 10.1109/TMM.2013.2238908
NR 30
TC 12
Z9 12
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3084
EP 3096
DI 10.1109/TMM.2018.2823907
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800017
DA 2024-07-18
ER

PT J
AU Jin, X
   Liu, ZQ
   Li, Q
   Dai, QH
AF Jin, Xin
   Liu, Zhanqi
   Li, Qian
   Dai, Qionghai
TI Depth Assisted Adaptive Workload Balancing for Parallel View Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Free viewpoint television; parallel view synthesis; workload prediction;
   workload balancing; depth image-based rendering
AB Depth image-based rendering has been adopted by MPEG as the recommended view synthesis technique for free viewpoint TV applications. In this paper, a workload balancing algorithm is proposed for parallel view synthesis on multicore platforms. First, view synthesis workload is defined as the function of the number of hole-pixels in the warped images. Then, a novel depth assisted prediction method is proposed to predict the number of hole-pixels in the current frame by exploiting the depth differences between the neighboring frames, which reflects the movement of objects in video content. Feeding the predicted workload to the proposed cost function, each input frame is partitioned adaptively to balance the synthesis workload among the cores. The proposed workload prediction method outperforms the existing approaches both in terms of frame average prediction error and standard deviation in prediction error. Applying the proposed workload balancing method, the parallel view synthesis system provides higher acceleration ratio and better synchronization performance among the cores compared with other parallel processing systems without sacrificing the subjective and objective quality. It is also robust to different platforms, which shows high potential in being applied to mobile oriented applications.
C1 [Jin, Xin; Liu, Zhanqi; Li, Qian; Dai, Qionghai] Tsinghua Univ, Shenzhen Key Lab Broadband Network & Multimedia, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University
RP Jin, X (corresponding author), Tsinghua Univ, Shenzhen Key Lab Broadband Network & Multimedia, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
EM jin.xin@sz.tsinghua.edu.cn; liu-zq13@mails.tsinghua.edu.cn;
   liq15@mails.tsinghua.edu.cn; ghdai@tsinghua.edu.cn
RI Li, Zexi/KFA-6939-2024; Dai, Qionghai/ABD-5298-2021; jin,
   xin/GQZ-5811-2022
OI Dai, Qionghai/0000-0001-7043-3061; 
FU National Natural Science Foundation of China (NSFC) [61771275]; Youth
   Top-notch Talent of Guangdong Special Support Program China
   [2016TQ03X998]
FX This work was supported in part by the project of National Natural
   Science Foundation of China (NSFC) under Grant 61771275, and in part by
   the Youth Top-notch Talent of Guangdong Special Support Program China
   under Grant 2016TQ03X998.
CR [Anonymous], 2014, P 2014 3 INT C DEV R
   [Anonymous], 2012, METH SUBJ ASS QUAL T
   [Anonymous], MPEG2010N11631 ISOIE
   [Anonymous], JCT3VG1100 JCT3V
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Borg M., 2016, P 18 MED EL C LEM CY, P1
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Flautner Krisztian., 2001, MOBILE COMPUTING NET, P260
   Govil Kinshuk., 1995, Proceedings of the 1st annual international conference on Mobile computing and networking, P13, DOI DOI 10.1145/215530.215546
   Grunwald Dirk., 2000, Proceedings of the 4th conference on Symposium on Operating System Design Implementation - Volume 4, OSDI'00, V4, P6
   Idehara A., ESS2008
   Ito Masayuki, 2008, 2008 IEEE International Solid-State Circuits Conference - Digest of Technical Papers, P90, DOI 10.1109/ISSCC.2008.4523071
   Jin SS, 2014, P ANN HICSS, P2536, DOI 10.1109/HICSS.2014.318
   Lee CK, 2009, ASIAS TRANSFORM, P1, DOI 10.1145/1666778.1666796
   Lin D, 2009, IEEE SIGNAL PROC MAG, V26, P103, DOI 10.1109/MSP.2009.934116
   Liu ZQ, 2015, INT CONF ACOUST SPEE, P1558, DOI 10.1109/ICASSP.2015.7178232
   Liu ZQ, 2015, LECT NOTES COMPUT SC, V9315, P3, DOI 10.1007/978-3-319-24078-7_1
   McMillan Leonard, 1997, THESIS
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   NYA PN, 2011, IEEE T MULTIMED, V13, P453
   Pering T, 1998, 1998 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN - PROCEEDINGS, P76, DOI 10.1109/LPE.1998.708159
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Sinha A, 2001, VLSI DESIGN 2001: FOURTEENTH INTERNATIONAL CONFERENCE ON VLSI DESIGN, P221, DOI 10.1109/ICVD.2001.902664
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Tian D, 2009, PROC SPIE, V7443, DOI 10.1117/12.829372
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Zhanqi Liu, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457884
NR 28
TC 2
Z9 2
U1 7
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2891
EP 2904
DI 10.1109/TMM.2018.2827781
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800003
DA 2024-07-18
ER

PT J
AU Lee, J
   Lee, JS
AF Lee, Junghyuk
   Lee, Jong-Seok
TI Music Popularity: Metrics, Characteristics, and Audio-Based Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Music popularity prediction; musical complexity; hit song; music
   popularity metrics
ID BOX-OFFICE PERFORMANCE; PREFERENCES
AB Understanding music popularity is important not only for the artists who create and perform music but also for the music-related industry. It has not been studied well how music popularity can be defined, what its characteristics are, and whether it can be predicted, which are addressed in this paper. We first define eight popularity metrics to cover multiple aspects of popularity. Then, the analysis of each popularity metric is conducted with long-term real-world chart data to deeply understand the characteristics of music popularity in the real world. We also build classification models for predicting popularity metrics using acoustic data. In particular, we focus on evaluating features describing music complexity together with other conventional acoustic features including MPEG-7 and Mel-frequency cepstral coefficient (MFCC) features. The results show that, although room still exists for improvement, it is feasible to predict the popularity metrics of a song significantly better than random chance based on its audio signal, particularly using both the complexity and MFCC features.
C1 [Lee, Junghyuk; Lee, Jong-Seok] Yonsei Univ, Sch Integrated Technol, Seoul 03722, South Korea.
C3 Yonsei University
RP Lee, JS (corresponding author), Yonsei Univ, Sch Integrated Technol, Seoul 03722, South Korea.
EM junghyuklee@yonsei.ac.kr; jong-seok.lee@yonsei.ac.kr
RI Lee, Jong-Seok/AAF-5197-2020
OI Lee, Jong-Seok/0000-0001-5255-4425; Lee, Junghyuk/0000-0002-6164-0728
FU Ministry of Science, ICT and Future Planning, South Korea, under the IT
   Consilience Creative Program [IITP-2017-2017-0-01015]
FX This research was supported by the Ministry of Science, ICT and Future
   Planning, South Korea, under the IT Consilience Creative Program
   (IITP-2017-2017-0-01015), supervised by the Institute for Information
   and Communications Technology Promotion.
CR Aucouturier JJ, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P105, DOI 10.1109/ICME.2002.1035729
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   Chintagunta PK, 2010, MARKET SCI, V29, P944, DOI 10.1287/mksc.1100.0572
   Dhanaraj R., 2005, Proceedings of the International Conference on Music Information Retrieval, P488
   Eck D., 2008, Advances in neural information processing systems, P385
   Fang-Fei Kuo, 2005, 13th Annual ACM International Conference on Multimedia, P507
   Friedlander J. P., 2014, TECH REP
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Hung-Chen Chen, 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P231, DOI 10.1145/502585.502625
   Lee J, 2015, PROC CIRP, V38, P3, DOI 10.1016/j.procir.2015.08.026
   Lee K, 2014, IEEE T MULTIMEDIA, V16, P1201, DOI 10.1109/TMM.2014.2311012
   Logan Beth, 2004, ISMIR, P425
   Ma ZY, 2013, J AM SOC INF SCI TEC, V64, P1399, DOI 10.1002/asi.22844
   MacCallum RM, 2012, P NATL ACAD SCI USA, V109, P12081, DOI 10.1073/pnas.1203182109
   Mauch M., 2010, P 11 INT SOC MUS INF, P135
   Mauch M, 2015, ROY SOC OPEN SCI, V2, DOI 10.1098/rsos.150081
   Mauch Matthias., 2011, P 12 INT C MUSIC INF, P489
   Mermelstein P., 1976, 1976 Joint Workshop on Pattern Recognition and Artificial Intelligence
   Pachet F, 2012, CH CRC DATA MIN KNOW, P305
   Pampalk E, 2003, DAFX-03: 6TH INTERNATIONAL CONFERENCE ON DIGITAL AUDIO EFFECTS, PROCEEDINGS, P7
   Parry R. M., 2004, TECH REP
   Pettijohn TF, 2010, CURR PSYCHOL, V29, P328, DOI 10.1007/s12144-010-9092-8
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P1231, DOI 10.1109/TMM.2013.2261481
   Salganik MJ, 2006, SCIENCE, V311, P854, DOI 10.1126/science.1121066
   Schäfer T, 2010, PSYCHOL AESTHET CREA, V4, P223, DOI 10.1037/a0018374
   Schubert E, 2004, MUSIC PERCEPT, V21, P561, DOI 10.1525/mp.2004.21.4.561
   Serrà J, 2012, SCI REP-UK, V2, DOI 10.1038/srep00521
   STECK L, 1975, J EXP PSYCHOL HUMAN, V104, P170
   Traverso S, 2015, IEEE T MULTIMEDIA, V17, P1839, DOI 10.1109/TMM.2015.2458043
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Wu JQ, 2016, IEEE T MULTIMEDIA, V18, P1882, DOI 10.1109/TMM.2016.2579600
   Yang LC, 2017, INT CONF ACOUST SPEE, P621, DOI 10.1109/ICASSP.2017.7952230
   Yu L.-C., 2017, ARXIV171010814
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
   Zufryden FS, 1996, J ADVERTISING RES, V36, P29
NR 37
TC 23
Z9 25
U1 7
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3173
EP 3182
DI 10.1109/TMM.2018.2820903
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800024
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ma, JQ
   Shao, WY
   Ye, H
   Wang, L
   Wang, H
   Zheng, YB
   Xue, XY
AF Ma, Jianqi
   Shao, Weiyuan
   Ye, Hao
   Wang, Li
   Wang, Hong
   Zheng, Yingbin
   Xue, Xiangyang
TI Arbitrary-Oriented Scene Text Detection via Rotation Proposals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene text detection; arbitrary oriented; rotation proposals
ID READING TEXT; LINE DETECTION; ROBUST; COMPETITION; TRACKING; IMAGES;
   VIDEO
AB This paper introduces a novel rotation-based framework for arbitrary-oriented text detection in natural scene images. We present the Rotation Region Proposal Networks, which are designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit into the text region in terms of the orientation. The Rotation Region-of-Interest pooling layer is proposed to project arbitrary-oriented proposals to a feature map for a text region classifier. The whole framework is built upon a region-proposal-based architecture, which ensures the computational efficiency of the arbitrary-oriented text detection compared with previous text detection systems. We conduct experiments using the rotation-based framework on three real-world scene text detection datasets and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches.
C1 [Ma, Jianqi; Wang, Li; Xue, Xiangyang] Fudan Univ, Sch Comp Sci, Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
   [Shao, Weiyuan; Ye, Hao; Wang, Hong; Zheng, Yingbin] Chinese Acad Sci, Shanghai Adv Res Inst, Shanghai 201210, Peoples R China.
C3 Fudan University; Chinese Academy of Sciences; Shanghai Advanced
   Research Institute, CAS
RP Zheng, YB (corresponding author), Chinese Acad Sci, Shanghai Adv Res Inst, Shanghai 201210, Peoples R China.
EM majq16@fudan.edu.cn; shaowy@sari.ac.cn; yeh@sari.ac.cn;
   wangli16@fudan.edu.cn; wang_hong@sari.ac.cn; zhengyb@sari.ac.cn;
   xyxue@fudan.edu.cn
RI Ma, Jianqi/JOZ-6280-2023; Zheng, Yingbin/AAG-9018-2020; Shi,
   Yaolin/JXN-8322-2024
OI Zheng, Yingbin/0000-0002-5590-9292; Doloriel, Chandler
   Timm/0000-0003-3820-7317; Ma, Jianqi/0000-0002-1628-1156
FU National Key R&D Program of China [2017YFC0803700]; National Natural
   Science Foundation of China [61602459, 61572138, U1611461]; Science and
   Technology Commission of Shanghai Municipality [17511101902,
   16JC1420400]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFC0803700, in part by the National Natural Science
   Foundation of China under Grants 61602459, 61572138, and U1611461, and
   in part by the Science and Technology Commission of Shanghai
   Municipality under Grants 17511101902 and 16JC1420400.
CR [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], 2014, Handbook of Document Image Processing and Recognition, DOI [10.1007/978-0-85729-859-1_28, DOI 10.1007/978-0-85729-859-1_28]
   [Anonymous], 2016, ABS160609002 CORR
   [Anonymous], 2017, IEEE ACCESS
   [Anonymous], ARXIV160603473
   [Anonymous], 2014, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2014.2353813
   Bazazian D., 2017, ARKIV170205089
   Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102
   Bouman KL, 2011, IEEE T MULTIMEDIA, V13, P922, DOI 10.1109/TMM.2011.2154317
   Chen D, 2000, SURVEY TEXT DETECTIO, P00
   Chen XR, 2004, PROC CVPR IEEE, P366
   Cho H, 2016, PROC CVPR IEEE, P3566, DOI 10.1109/CVPR.2016.388
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Jaderberg M., 2015, ICLR, DOI DOI 10.48550/ARXIV.1506.02025
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jung K, 2004, PATTERN RECOGN, V37, P977, DOI 10.1016/j.patcog.2003.10.012
   Kang L, 2014, PROC CVPR IEEE, P4034, DOI 10.1109/CVPR.2014.514
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Liu XQ, 2012, IEEE T MULTIMEDIA, V14, P482, DOI 10.1109/TMM.2011.2177646
   Liu YL, 2017, PROC CVPR IEEE, P3454, DOI 10.1109/CVPR.2017.368
   Lucas SM, 2003, PROC INT CONF DOC, P682
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Neumann L, 2013, IEEE I CONF COMP VIS, P97, DOI 10.1109/ICCV.2013.19
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   PLAISTED DA, 1987, J ALGORITHM, V8, P405, DOI 10.1016/0196-6774(87)90020-4
   Qin SY, 2017, PROC INT CONF DOC, P1275, DOI 10.1109/ICDAR.2017.210
   Redmon J., 2016, 2016 IEEE Conf. Comp. Vis. Patt. Recog. (CVPR), P779
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Risnumawan A, 2014, EXPERT SYST APPL, V41, P8027, DOI 10.1016/j.eswa.2014.07.008
   Shahab A, 2011, PROC INT CONF DOC, P1491, DOI 10.1109/ICDAR.2011.296
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tian SX, 2015, IEEE I CONF COMP VIS, P4651, DOI 10.1109/ICCV.2015.528
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43
   Wang L, 2017, IEEE INT CON MULTI, P1135, DOI 10.1109/ICME.2017.8019461
   Wang T, 2012, INT C PATT RECOG, P3304
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yin XC, 2016, IEEE T IMAGE PROCESS, V25, P2752, DOI 10.1109/TIP.2016.2554321
   Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zhang SY, 2016, INT CONF ACOUST SPEE, P2633, DOI 10.1109/ICASSP.2016.7472154
   Zhang Z, 2016, PROC CVPR IEEE, P4159, DOI 10.1109/CVPR.2016.451
   Zhong Z., 2016, IEEE INT C AC
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 58
TC 803
Z9 922
U1 24
U2 168
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3111
EP 3122
DI 10.1109/TMM.2018.2818020
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800019
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Bai, Y
   Lou, YH
   Gao, F
   Wang, SQ
   Wu, YW
   Duan, LY
AF Bai, Yan
   Lou, Yihang
   Gao, Feng
   Wang, Shiqi
   Wu, Yuwei
   Duan, Ling-Yu
TI Group-Sensitive Triplet Embedding for Vehicle Reidentification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Vehicle re-identification; metric learning; intra-class variance;
   embedding; retrieval; surveillance
AB The widespread use of surveillance cameras toward smart and safe cities poses the critical but challenging problem of vehicle reidentification (Re-ID). The state-of-the-art research work performed vehicle Re-ID relying on deep metric learning with a triplet network. However, most existing methods basically ignore the impact of intraclass variance-incorporated embedding on the performance of vehicle reidentification, in which robust fine-grained features for large-scale vehicle Re-ID have not been fully studied. In this paper, we propose a deep metric learning method, group-sensitive-triplet embedding (GS-TRE), to recognize and retrieve vehicles, in which intraclass variance is elegantly modeled by incorporating an intermediate representation "group" between samples and each individual vehicle in the triplet network learning. To capture the intraclass variance attributes of each individual vehicle, we utilize an online grouping method to partition samples within each vehicle ID into a few groups, and build up the triplet samples at multiple granularities across different vehicle IDs as well as different groups within the same vehicle ID to learn fine-grained features. In particular, we construct a large-scale vehicle database "PKU-Vehicle," consisting of 10 million vehicle images captured by different surveillance cameras in several cities, to evaluate the vehicle Re-ID performance in real-world video surveillance applications. Extensive experiments over benchmark datasets VehicleID, VeRI, and CompCar have shown that the proposed GS-TRE significantly outperforms the state-of-the-art approaches for vehicle Re-ID.
C1 [Bai, Yan; Lou, Yihang; Gao, Feng; Duan, Ling-Yu] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
   [Bai, Yan; Lou, Yihang] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Wu, Yuwei] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
C3 Peking University; Peking University; City University of Hong Kong;
   Beijing Institute of Technology
RP Duan, LY (corresponding author), Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
EM yanbai@pku.edu.cn; yihang@pku.edu.cn; gaof@pku.edu.cn;
   shiqwang@cityu.edu.hk; wuyuwei@bit.edu.cn; lingyu@pku.edu.cn
FU National Natural Science Foundation of China [U1611461, 61661146005,
   61390515]; National Key Research and Development Program of China
   [2016YFB1001501]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1611461, 61661146005, and 61390515,
   and in part by the National Key Research and Development Program of
   China under Grant 2016YFB1001501.
CR [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2016, P IEEE C COMP VIS PA
   [Anonymous], PROC CVPR IEEE
   [Anonymous], INT J INNOVAT TECHNO
   [Anonymous], P BRIT MACH VIS C 20
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2017, ARXIV170307737
   [Anonymous], 2016, Applications of Computer Vision Workshops (WACVW), 2016 IEEE Winter, DOI [10.1109/WACVW.2016.7470114, DOI 10.1109/WACVW.2016.7470114]
   [Anonymous], 2016, P 2016 IEEE INT C MU, DOI DOI 10.1155/2016/8191254
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2017, P IEEE C COMPUTER VI
   Bai Y, 2017, IEEE INT CON MULTI, P1452, DOI 10.1109/ICME.2017.8019371
   Bhattarai B, 2016, PROC CVPR IEEE, P4226, DOI 10.1109/CVPR.2016.458
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Du S, 2013, IEEE T CIRC SYST VID, V23, P322, DOI 10.1109/TCSVT.2012.2203741
   Ernst JM, 2014, IEEE INTEL TRANSP SY, V6, P50, DOI 10.1109/MITS.2013.2288648
   Feris RS, 2012, IEEE T MULTIMEDIA, V14, P28, DOI 10.1109/TMM.2011.2170666
   Frías-Velázquez A, 2015, ENG APPL ARTIF INTEL, V45, P220, DOI 10.1016/j.engappai.2015.06.024
   Gou C, 2016, IEEE T INTELL TRANSP, V17, P1096, DOI 10.1109/TITS.2015.2496545
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Khosla A., 2011, P IEEE C COMP VIS PA, V2
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu X, 2017, AAAI CONF ARTIF INTE, P4190
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995
   Ramnath K, 2014, IEEE WINT CONF APPL, P285, DOI 10.1109/WACV.2014.6836087
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Simo-Serra E, 2016, PROC CVPR IEEE, P298, DOI 10.1109/CVPR.2016.39
   Simonyan K., 2014, 14091556 ARXIV
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tadmor O., 2016, P ADV NEUR INF PROC, P1396
   Tang Y, 2017, MULTIMED TOOLS APPL, V76, P5817, DOI 10.1007/s11042-015-2520-x
   Tian Y, 2014, J ZHEJIANG U-SCI C, V15, P372, DOI 10.1631/jzus.C1300291
   Ustinova E, 2016, ADV NEUR IN, V29
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xiang Y, 2015, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2015.7298800
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XF, 2016, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2016.126
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127
NR 62
TC 186
Z9 203
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2385
EP 2399
DI 10.1109/TMM.2018.2796240
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200012
DA 2024-07-18
ER

PT J
AU Tian, LC
   Li, MC
   Hao, Y
   Liu, J
   Zhang, GY
   Chen, YQ
AF Tian, Luchao
   Li, Mingchen
   Hao, Yu
   Liu, Jun
   Zhang, Guyue
   Chen, Yan Qiu
TI Robust 3-D Human Detection in Complex Environments With a Depth Camera
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D human detection; depth data; real-time; deep learning
ID REAL-TIME
AB Human detection has received great attention during the past few decades, which is yet still a challenging problem. In this paper, we focus on the problem of 3-D human detection, i.e., finding the human bodies and determining their 3-D coordinates in complex 3-D space using depth data only. Since the traditional sliding-window-based approaches for target localization are time-consuming and the recent deep-learning-based object detectors generate too many region proposals, we propose to utilize the candidate head-top locating stage to efficiently and quickly find the plausible head-top locations. In the second stage, we propose a Depth map, Multiorder depth template, and Height difference map representation encoding three channels of information for each candidate region to utilize the neural network pretrained on large-scale well-annotated datasets to classify the candidate regions. We evaluate our method on four publicly available challenging datasets. Extensive experimental results demonstrate that the proposed method is superior to the state-of-the-art methods while achieving real-time performance.
C1 [Tian, Luchao; Li, Mingchen; Hao, Yu; Zhang, Guyue; Chen, Yan Qiu] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
   [Liu, Jun] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Fudan University; Nanyang Technological University
RP Chen, YQ (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
EM lctian14@fudan.edu.cn; mcli13@fudan.edu.cn; haoy16@fudan.edu.cn;
   jliu029@ntu.edu.sg; guyuezhang13@fudan.edu.cn; chenyq@fudan.edu.cn
RI ; Zhang, Guyue/D-7621-2019
OI Liu, Jun/0000-0002-4365-4165; Zhang, Guyue/0000-0001-9061-1750; Li,
   Mingchen/0000-0001-5181-8193
FU Science and Technology Commission of Shanghai Municipality [17ZR1402300]
FX This work was supported by the Science and Technology Commission of
   Shanghai Municipality under Grant 17ZR1402300.
CR [Anonymous], CVPR 2011 WORKSHOPS, DOI DOI 10.1109/CVPRW.2011.5981811
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.248
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2016, 2016 IEEE 17th International Symposium on A World of Wireless, Mobile and Multimedia Networks (WoWMoM)
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, ARXIV151202325
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Enzweiler M, 2010, PROC CVPR IEEE, P990, DOI 10.1109/CVPR.2010.5540111
   Ess A, 2009, IEEE T PATTERN ANAL, V31, P1831, DOI 10.1109/TPAMI.2009.109
   Felzenszwalb P., 2008, PROC IEEE C COMPUT V, P1
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Herrera CD, 2012, IEEE T PATTERN ANAL, V34, P2058, DOI 10.1109/TPAMI.2012.125
   Hosang J, 2015, PROC CVPR IEEE, P4073, DOI 10.1109/CVPR.2015.7299034
   Ioffe S, 2001, PROC CVPR IEEE, P180
   Jafari OH, 2014, IEEE INT CONF ROBOT, P5636, DOI 10.1109/ICRA.2014.6907688
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YS, 2015, PROC CVPR IEEE, P240, DOI 10.1109/CVPR.2015.7298620
   Levi K, 2004, PROC CVPR IEEE, P53
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu J, 2015, J VIS COMMUN IMAGE R, V31, P177, DOI 10.1016/j.jvcir.2015.06.014
   Liu J, 2015, PATTERN RECOGN LETT, V53, P16, DOI 10.1016/j.patrec.2014.09.013
   Lu T, 2017, IEEE INT CON MULTI, P1362, DOI 10.1109/ICME.2017.8019298
   Mu Y., 2008, PROC IEEE C COMPUT V, P1
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Simonyan K., 2014, 14091556 ARXIV
   Spinello L, 2011, IEEE INT C INT ROBOT, P3838, DOI 10.1109/IROS.2011.6048835
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang SP, 2010, IEICE T INF SYST, VE93D, P1737, DOI 10.1587/transinf.E93.D.1737
   Tian LC, 2017, IEEE INT CON MULTI, P1542, DOI 10.1109/ICME.2017.8019303
   Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wu B, 2005, IEEE I CONF COMP VIS, P90
   Wu B., 2008, PROC IEEE C COMPUT V, P1
   Wu JX, 2011, IEEE INT CONF ROBOT, P860
   Zeng XY, 2013, IEEE I CONF COMP VIS, P121, DOI 10.1109/ICCV.2013.22
   Zhang GY, 2016, FRONT ARTIF INTEL AP, V285, P304, DOI 10.3233/978-1-61499-672-9-304
   Zhang G, 2016, INT CONF ACOUST SPEE, P2004, DOI 10.1109/ICASSP.2016.7472028
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhou BL, 2014, ADV NEUR IN, V27
NR 52
TC 18
Z9 19
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2249
EP 2261
DI 10.1109/TMM.2018.2803526
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200002
DA 2024-07-18
ER

PT J
AU Li, T
   He, XH
   Qing, LB
   Teng, QZ
   Chen, HG
AF Li, Tao
   He, Xiaohai
   Qing, Linbo
   Teng, Qizhi
   Chen, Honggang
TI An Iterative Framework of Cascaded Deblocking and Superresolution for
   Compressed Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image super-resolution; image deblocking; low-rank approximation; sparse
   representation; group sparsity constraint
ID ARTIFACT REDUCTION; INTERPOLATION; ALGORITHM; DCT
AB Superresolution (SR) of compressed images is challenging due to the combination of resolution loss and compression artifacts. To solve these intertwined problems, the conventional cascading framework splits the solution into independent deblocking and SR subprocesses, where some existing high-frequency (HF) components are often oversmoothed during deblocking and information exchange between cascaded deblocking and SR remains untouched. In this paper, we propose an iterative cascading framework after analyzing the correlation between the two subprocesses. Deblocking is provided with a shape-adaptive low-rank prior to well preserve edges and an extra prior to restore the lost HF components. The latter prior represents an important feedback link from SR to deblocking, which is a novel design in this framework. To provide an accurate and noise-robust feedback of the extra prior, an SR method via singular value decomposition projection is also developed. The extensive experimental results demonstrate the superior performance of the proposed method.
C1 [Li, Tao; He, Xiaohai; Qing, Linbo; Teng, Qizhi; Chen, Honggang] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Sichuan, Peoples R China.
   [Li, Tao] Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu 610039, Sichuan, Peoples R China.
C3 Sichuan University; Xihua University
RP He, XH (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Sichuan, Peoples R China.
EM lucia634@163.com; hxh@scu.edu.cn; qing_lb@scu.edu.cn; qzteng@scu.edu.cn;
   honggang.chen@stu.scu.edu.cn
FU National Natural Science Foundation of China [61471248]; Chengdu Science
   and Technology Project for Benefiting People [2015-HM01-00293-SF]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61471248, and in part by the Chengdu
   Science and Technology Project for Benefiting People under Grant
   2015-HM01-00293-SF.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chang HB, 2014, IEEE T SIGNAL PROCES, V62, P718, DOI 10.1109/TSP.2013.2290508
   Chen XX, 2014, IEEE SIGNAL PROC LET, V21, P79, DOI 10.1109/LSP.2013.2286417
   Dabov K., 2009, Signal Processing with Adaptive Sparse Structured Representations (SPARS'09)
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai SY, 2009, IEEE T IMAGE PROCESS, V18, P969, DOI 10.1109/TIP.2009.2012908
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P700, DOI 10.1109/TIP.2012.2221729
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jancsary J, 2012, LECT NOTES COMPUT SC, V7578, P112, DOI 10.1007/978-3-642-33786-4_9
   Jung C, 2012, SIGNAL PROCESS-IMAGE, V27, P663, DOI 10.1016/j.image.2012.03.002
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Katkovnik V, 2002, J MATH IMAGING VIS, V16, P223, DOI 10.1023/A:1020329726980
   Li MD, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P101, DOI 10.1109/VCIP.2014.7051514
   Li T, 2016, NEUROCOMPUTING, V216, P1, DOI 10.1016/j.neucom.2016.06.066
   Liu XM, 2016, IEEE T IMAGE PROCESS, V25, P1649, DOI 10.1109/TIP.2016.2526910
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Ren C, 2016, IEEE T IMAGE PROCESS, V25, P2168, DOI 10.1109/TIP.2016.2542442
   Ren J, 2013, IEEE DATA COMPR CONF, P516, DOI 10.1109/DCC.2013.95
   Sun DQ, 2007, IEEE T IMAGE PROCESS, V16, P2743, DOI 10.1109/TIP.2007.904969
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang Q, 2007, IEEE T IMAGE PROCESS, V16, P889, DOI 10.1109/TIP.2007.891794
   Wu JQ, 2015, IEEE IMAGE PROC, P3495, DOI 10.1109/ICIP.2015.7351454
   Xiao J, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P607
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yang YY, 1997, IEEE T IMAGE PROCESS, V6, P1345, DOI 10.1109/83.624945
   Yeh CH, 2014, J VIS COMMUN IMAGE R, V25, P891, DOI 10.1016/j.jvcir.2014.02.012
   Yoo SB, 2014, IEEE T MULTIMEDIA, V16, P1536, DOI 10.1109/TMM.2014.2327563
   Yu K., 2016, arXiv
   Yu T, 2012, 6 INTLERNATIONAL C F, P1
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhai GT, 2008, IEEE T CIRC SYST VID, V18, P122, DOI 10.1109/TCSVT.2007.906942
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
   Zhang YB, 2016, IEEE T MULTIMEDIA, V18, P405, DOI 10.1109/TMM.2015.2512046
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P2057, DOI 10.1109/TCSVT.2016.2580399
   Zhu Y, 2014, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR.2014.373
NR 48
TC 30
Z9 31
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1305
EP 1320
DI 10.1109/TMM.2017.2766889
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400003
DA 2024-07-18
ER

PT J
AU Liu, Z
   Lin, ZC
   Wei, XG
   Chan, SC
AF Liu, Zhong
   Lin, Zhouchi
   Wei, Xiguang
   Chan, Shing-Chow
TI A New Model-Based Method for Multi-View Human Body Tracking and Its
   Application to View Transfer in Image-Based Rendering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human body tracking; textured deformable model; model-based and
   image-based rendering; fly-over view transfer; free-form deformation
   artifacts suppression
ID ARTICULATED HUMAN MOTION; HUMAN POSE; ALGORITHM; CAPTURE; REGISTRATION;
   SILHOUETTE; ANIMATION; PEOPLE
AB This paper proposes a new multi-view human body tracking and model-based rendering system with a textured deformable human body model and explores its practical application to view transfer in image-based rendering (IBR). The proposed approach first reconstructs an initial 3-D model of the human subject offline using Kinect depth cameras or from a general 3-D model if such information is unavailable in the original data. The human pose of the subject is then tracked with multi-view videos using an annealed particle filter (APF)-based tracker with a new color-based likelihood function and a Bayesian-Kalman filter smoother. The previous captured model can then be deformed to the new position for model-based rendering. An immediate application of the proposed approach is to support fly-over effects for view transfer in IBR systems with limited cameras. It avoids the reconstruction of the complete dynamic 3-D model where a large number of cameras may be required. Moreover, for static background, the background can be rendered using IBR with precaptured depth maps to further enhance the user's experience. To reduce the artifacts during fly-over caused by tracking errors and model deformation, a novel morphing technique utilizing a new free-form deformation-based artifacts suppression (FFD-AS) method and other user interface design techniques are also proposed. It allows smooth transition between the original view and the model-rendered views. The performance of the proposed algorithm is evaluated using the publicly available HumanEva dataset and our captured RGB-D multi-view dataset. Experimental results show that the proposed APF-based tracker offered improved tracking performance compared with the conventional bidirectional silhouette likelihood criterion. The proposed morphing approach is also shown to be effective in mitigating the rendering artifacts during view transfer.
C1 [Liu, Zhong; Lin, Zhouchi; Wei, Xiguang; Chan, Shing-Chow] Univ Hong Kong, TheDepartment Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 University of Hong Kong
RP Chan, SC (corresponding author), Univ Hong Kong, TheDepartment Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM liuzhong@eee.hku.hk; zclin@eee.hku.hk; xgwei@eee.hku.hk;
   scchan@eee.hku.hk
RI wei, xiguang/AAL-5410-2021
OI Liu, Zhong/0000-0001-9650-6097
FU Hong Kong research Grant council under GRF Project [710512]
FX This work was supported in part by the Hong Kong research Grant council
   under GRF Project 710512. This paper was presented in part at the IEEE
   International Symposium on Circuits and Systems, Lisbon, Portugal, May
   2015. (Zhong Liu, Zhouchi Lin, Xiguang Wei, and Shing-Chow Chan
   contributed equally to this work.)
CR Alexiadis D.S., 2011, ACM international conference on Multimedia, P659
   Alexiadis DS, 2014, IEEE T MULTIMEDIA, V16, P1391, DOI 10.1109/TMM.2014.2317311
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2005, P 2005 S INTERACTIVE, DOI [DOI 10.1145/1053427.10534294, DOI 10.1145/1053427.1053429]
   [Anonymous], P IEEE INT C COMP VI, DOI DOI 10.1109/CVPR.2000.854758
   [Anonymous], 2008, Image-Based Rendering
   Asteriadis S., 2013, Proceedings of the 6th International Conference on Computer Vision / Computer Graphics Collaboration Techniques and Applications. MIRAGE '13, P3, DOI DOI 10.1145/2466715.2466727
   Brox T, 2010, IEEE T PATTERN ANAL, V32, P402, DOI 10.1109/TPAMI.2009.32
   Chan SC, 2007, IEEE SIGNAL PROC MAG, V24, P22, DOI 10.1109/MSP.2007.905702
   Chen X, 2012, EUR J APPL PHYSIOL, V112, P2603, DOI 10.1007/s00421-011-2227-2
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733
   Crum W. R., 2014, BRIT J RADIOL, V77, pS140
   Cui JS, 2008, IMAGE VISION COMPUT, V26, P240, DOI 10.1016/j.imavis.2007.05.005
   de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697
   De Craene M, 2012, MED IMAGE ANAL, V16, P427, DOI 10.1016/j.media.2011.10.006
   Dockstader SL, 2001, P IEEE, V89, P1441, DOI 10.1109/5.959340
   Fornefett M., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P402, DOI 10.1109/CVPR.1999.786970
   Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755
   Guo F, 2009, INT CONF ACOUST SPEE, P1781, DOI 10.1109/ICASSP.2009.4959950
   Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x
   Hernández PC, 2007, IEEE T MULTIMEDIA, V9, P754, DOI 10.1109/TMM.2007.893342
   Huang XL, 2006, IEEE T PATTERN ANAL, V28, P1303, DOI 10.1109/TPAMI.2006.171
   Ilya B., 2007, ACM T GRAPHIC, V26
   Isard M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P343, DOI 10.1007/BFb0015549
   Jia W., 2012, ELECTROINFORMATION T, P1
   Jin J, 2016, IEEE T MULTIMEDIA, V18, P953, DOI 10.1109/TMM.2016.2539825
   Julier SJ, 1997, P SOC PHOTO-OPT INS, V3068, P182, DOI 10.1117/12.280797
   Kadu H, 2014, IEEE T MULTIMEDIA, V16, P2191, DOI 10.1109/TMM.2014.2360793
   Kam Lai, 2012, Proceedings of the 2012 IEEE Southwest Symposium on Image Analysis & Interpretation (SSIAI 2012), P185, DOI 10.1109/SSIAI.2012.6202484
   Lankton S, 2008, IEEE T IMAGE PROCESS, V17, P2029, DOI 10.1109/TIP.2008.2004611
   Lee A., BJOG INT J OBSTET GY, V102, P302
   Lee SY, 1996, J VISUAL COMP ANIMAT, V7, P3, DOI 10.1002/(SICI)1099-1778(199601)7:1<3::AID-VIS131>3.0.CO;2-U
   LEE SY, 1996, P 1996 INT S BACT PO, P127
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Liu YB, 2013, IEEE T PATTERN ANAL, V35, P2720, DOI 10.1109/TPAMI.2013.47
   Liu Z, 2015, IEEE INT SYMP CIRC S, P1038, DOI 10.1109/ISCAS.2015.7168814
   Liu Z, 2013, IEEE INT SYMP CIRC S, P713, DOI 10.1109/ISCAS.2013.6571946
   Mittal A, 2003, INT J COMPUT VISION, V51, P189, DOI 10.1023/A:1021849801764
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Ng KT, 2012, IEEE T MULTIMEDIA, V14, P1631, DOI 10.1109/TMM.2012.2199291
   Nocedal J., 1999, NUMERICAL OPTIMIZATI
   Pock T, 2007, LECT NOTES COMPUT SC, V4792, P511
   Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Seung-Yong Lee, 1995, Computer Graphics Proceedings. SIGGRAPH 95, P439
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Solh M, 2012, IEEE J-STSP, V6, P495, DOI 10.1109/JSTSP.2012.2204723
   Takahashi S, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P70, DOI 10.1109/PCCGA.2001.962859
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Wolberg G, 1998, VISUAL COMPUT, V14, P360, DOI 10.1007/s003710050148
   Zhang S., 2014, SENSORS J, V15, P2679
   Zhang Yi., 2005, 2005 IEEE INT C INFO, P11, DOI [10.1109/ICIA.2005, DOI 10.1109/ICIA.2005]
   Zhang Z, 2013, IEEE T MULTIMEDIA, V15, P106, DOI 10.1109/TMM.2012.2225040
   Zhu ZY, 2012, IEEE T CIRC SYST VID, V22, P1405, DOI 10.1109/TCSVT.2012.2198133
NR 57
TC 13
Z9 14
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1321
EP 1334
DI 10.1109/TMM.2017.2767781
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400004
DA 2024-07-18
ER

PT J
AU Deng, YH
   Li, YS
   Seet, R
   Tang, XY
   Cai, WT
AF Deng, Yunhua
   Li, Yusen
   Seet, Ronald
   Tang, Xueyan
   Cai, Wentong
TI The Server Allocation Problem for Session-Based Multiplayer Cloud Gaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud gaming; multiplayer online games; real-time multimedia; resource
   allocation; cloud computing
ID DISTRIBUTED INTERACTIVE APPLICATIONS; CLIENT ASSIGNMENT; ONLINE GAMES;
   LATENCY; QUALITY; SERVICE
AB Advances in cloud computing and GPU virtualization are allowing the game industry to move into a cloud gaming era. In this paper, we consider multiplayer cloud gaming (MCG), which is the natural integration of multiplayer online gaming and cloud gaming paradigms. With MCG, a game server and a set of rendering servers for the players need to be located and launched in the clouds for each game session. We formulate an MCG server allocation problem with the objective of minimizing the total server rental and bandwidth cost charged by the cloud to support an MCG session. The MCG server allocation problem is hard to solve optimally. We propose several efficient heuristics to address the problem and carry out theoretical analysis for the proposed hill-climbing algorithm. We conduct extensive experiments using real Internet latency and cloud pricing datasets to evaluate the effectiveness of our proposed algorithms as well as several alternatives. Experimental results show that our best algorithm can achieve near-optimal cost under real-time latency constraints.
C1 [Deng, Yunhua] Huawei Technol, Shenzhen 518129, Peoples R China.
   [Li, Yusen] Nankai Univ, Dept Comp Sci & Informat Secur, Tianjin 300350, Peoples R China.
   [Seet, Ronald; Tang, Xueyan; Cai, Wentong] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Huawei Technologies; Nankai University; Nanyang Technological University
RP Li, YS (corresponding author), Nankai Univ, Dept Comp Sci & Informat Secur, Tianjin 300350, Peoples R China.
EM dengyunhua1@huawei.com; liyusen@nbjl.nankai.edu.cn;
   ronaldseet@ntu.edu.sg; asxytang@ntu.edu.sg; aswtcai@ntu.edu.sg
RI Tang, Xueyan/A-3703-2011; Cai, Wentong/A-3720-2011
OI Cai, Wentong/0000-0002-0183-3835; Li, Yusen/0000-0001-6623-350X
FU National Research Foundation, Prime Minister's Office, Singapore, under
   IDM Futures Funding Initiative; Singapore Ministry of Education Academic
   Research Fund [MOE2013-T2-2-067]; NSF of China [61602266, 61373018]; NSF
   of Tianjin [16JCYBJC41900]
FX This work was supported in part by the National Research Foundation,
   Prime Minister's Office, Singapore, under its IDM Futures Funding
   Initiative, in part by Singapore Ministry of Education Academic Research
   Fund Tier 2 under Grant MOE2013-T2-2-067, in part by the NSF of China
   under Grants 61602266 and 61373018, and in part by the NSF of Tianjin
   under Grant 16JCYBJC41900.
CR [Anonymous], 2011, P ACM SIGMETRICS JOI
   [Anonymous], 2017, COUNTER STRIKE SERVE
   [Anonymous], P 11 ANN WORKSH NETW
   [Anonymous], 2017, STAR WARS BATTLEFRON
   [Anonymous], 2010, 2010 P IEEE INFOCOM, DOI DOI 10.1109/INFCOM.2010.5461933
   [Anonymous], 2015, P IEEE 81 VEH TECHN
   [Anonymous], 2003, PROC IMC, DOI DOI 10.1145/948205.948211
   Barroso LA, 2007, COMPUTER, V40, P33, DOI 10.1109/MC.2007.443
   Brun J, 2006, COMMUN ACM, V49, P46, DOI 10.1145/1167838.1167861
   Cai W, 2016, IEEE ACCESS, V4, P7605, DOI 10.1109/ACCESS.2016.2590500
   Cai W, 2016, P IEEE, V104, P687, DOI 10.1109/JPROC.2016.2539418
   Chen K.-T., 2011, P 19 ACM INT C MULT, P1269
   Chen KT, 2006, COMMUN ACM, V49, P34, DOI 10.1145/1167838.1167859
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Chen Yan Chen Yan, 2011, Guizhou Agricultural Sciences, P1
   Chen YR, 2010, IEEE GLOBE WORK, P882, DOI 10.1109/GLOCOMW.2010.5700451
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Deng Yunhua, 2016, P 24 ACM INT C MULT, P918
   Duong NBT, 2012, IEEE T PARALL DISTR, V23, P304, DOI 10.1109/TPDS.2011.107
   Garcia-Dorado J., IEEE T CLOU IN PRESS
   Gargolinski S., 2005, Proceedings of 4th ACM SIGCOMM Workshop on Network and System Support for Games, P1
   Hong HJ, 2015, IEEE T CLOUD COMPUT, V3, P42, DOI 10.1109/TCC.2014.2338295
   Huang CY, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2537855
   HUANG CX, 2015, PROC 2 WORKSH MOB, V20, P19
   Jarschel M, 2013, MATH COMPUT MODEL, V57, P2883, DOI 10.1016/j.mcm.2011.12.014
   Jia AL, 2015, ACM T KNOWL DISCOV D, V10, DOI 10.1145/2736698
   Lee K., 2015, ANAL CLOUD GAMING PL, P151
   Lee KW, 2005, COMPUT NETW, V49, P84, DOI 10.1016/j.comnet.2005.04.006
   Li YS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P805, DOI 10.1145/2733373.2807978
   Li YS, 2014, PROCEEDINGS OF THE 26TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES (SPAA'14), P2, DOI 10.1145/2612669.2612675
   Lin L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P347, DOI 10.1145/2647868.2654943
   Manweiler J., 2011, Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services, P71, DOI DOI 10.1145/1999995.2000003
   Mohamed Asan Basiri M, 2016, IEEE INT S VLSI DES, P1
   Pantel Lothar., 2002, NETGAMES 02 P 1 WORK, P79
   Rochman Yuval, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P49
   Shea R., 2015, P 6 ACM MULT SYST C, P97
   Tian H, 2015, IEEE T CIRC SYST VID, V25, P2064, DOI 10.1109/TCSVT.2015.2416563
   Wang HY, 2014, I C NETWORK PROTOCOL, P37, DOI 10.1109/ICNP.2014.25
   Wang SX, 2013, IEEE T MULTIMEDIA, V15, P870, DOI 10.1109/TMM.2013.2240674
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu D, 2014, IEEE T CIRC SYST VID, V24, P1405, DOI 10.1109/TCSVT.2014.2302543
   Wu Z, 2013, ACM SIGCOMM COMP COM, V43, P13, DOI 10.1145/2479957.2479960
   Yaw S, 2015, ACM SIGCOMM COMP COM, V45, P5, DOI 10.1145/2805789.2805791
   Zhang L, 2014, IEEE T PARALL DISTR, V25, P785, DOI 10.1109/TPDS.2013.47
   Zhang L, 2012, IEEE ACM T NETWORK, V20, P1707, DOI 10.1109/TNET.2012.2187674
   Zhang Q, 2013, IEEE J SEL AREA COMM, V31, P762, DOI 10.1109/JSAC.2013.SUP2.1213008
   Zhang YH, 2016, IEEE T PARALL DISTR, V27, P1239, DOI 10.1109/TPDS.2015.2433916
   Zheng HY, 2016, IEEE T PARALL DISTR, V27, P271, DOI 10.1109/TPDS.2015.2388473
   Zheng HY, 2015, IEEE T COMPUT, V64, P2752, DOI 10.1109/TC.2014.2378252
NR 49
TC 34
Z9 38
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1233
EP 1245
DI 10.1109/TMM.2017.2760621
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400017
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Li, F
   Shuang, F
   Liu, ZY
   Qian, XM
AF Li, Fan
   Shuang, Fu
   Liu, Ziyi
   Qian, Xueming
TI A Cost-Constrained Video Quality Satisfaction Study on Mobile Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile video; quality satisfaction modeling; cost-constrained QoE
   framework; mobile traffic; user profile
ID MODEL; NETWORKS
AB Mobile videos on smartphones have been widely used and enjoyed by increasing numbers of people in recent years; however, due to the high bit rates of video streams, viewers must pay high data communication costs when watching videos over cellular networks. Thus, the viewers' quality of experience (QoE) for streaming videos is influenced by their psychological states under this cost pressure. This paper evaluates the cost-constrained video quality. First, a cost-constrained QoE framework that considers technical factors, content types and cost pressure is developed. Then, a cost-constrained video quality satisfaction (CVQS) model is proposed to assess users' video quality satisfaction (QS) considering the three aspects. Finally, the proposed model is verified by empirical analysis, mathematical fitting and experimental simulations. The proposed CVQS model maximizes the QS of videos by considering the cost to users, the users' personality profiles and objective video quality. An example application that employs the CVQS model is provided to demonstrate the applicability and effectiveness of the model.
C1 [Li, Fan; Shuang, Fu; Liu, Ziyi; Qian, Xueming] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University
RP Li, F (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Shaanxi, Peoples R China.
EM lifan@mail.xjtu.edu.cn; yoyo088@stu.xjtu.edu.cn;
   lzy1016@stu.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn
RI Liu, ziyi/HHM-8313-2022
FU National Science Foundation of China [61671365, 61372091]; Natural
   Science Basic Research Plan in Shaanxi Province of China [2017JM6018];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Science Foundation of
   China (61671365, 61372091), in part by the Natural Science Basic
   Research Plan in Shaanxi Province of China (2017JM6018), and in part by
   the Fundamental Research Funds for the Central Universities.
CR Alberti C, 2013, INT WORK QUAL MULTIM, P58, DOI 10.1109/QoMEX.2013.6603211
   Anegekuh L, 2015, IEEE T MULTIMEDIA, V17, P1323, DOI 10.1109/TMM.2015.2444098
   [Anonymous], 2015, OP MOD NETW PLANN VI
   [Anonymous], 2007, OP MOD VID TEL APPL
   [Anonymous], 2014, CHIN MAJ TEL OP OV S
   [Anonymous], 2016, PAR BITSTR BAS QUAL
   [Anonymous], 1997, 138187 ISOIEC
   [Anonymous], 2014, ISOIECDIS230091
   [Anonymous], 2012, Encyclopedia of Mathematics
   [Anonymous], 2012, TELECOMMUNICATIONS E
   [Anonymous], 2002, METH SUBJ ASS QUAL T
   [Anonymous], 2013, AMD 2 APP 3 US ITU T
   [Anonymous], P 40 HICSS IEEE COMP
   [Anonymous], 2015, FOG COMPUTING INTERN
   [Anonymous], 2007, SUBJ VID QUAL ASS ME
   Chen ZB, 2016, IEEE T CIRC SYST VID, V26, P1029, DOI 10.1109/TCSVT.2015.2441432
   Dinse GE, 2011, J AGR BIOL ENVIR ST, V16, P221, DOI 10.1007/s13253-010-0045-3
   Hamblen M., 2011, COMPUT WORLD
   Hosek J, 2015, 2015 38TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P607, DOI 10.1109/TSP.2015.7296335
   Joskowicz J., 2010, IEEE INT WORKSH TECH, P1, DOI DOI 10.1109/CQR.2010.5619912
   Joskowicz J, 2013, IEEE T BROADCAST, V59, P569, DOI 10.1109/TBC.2013.2277951
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Le Callet P., 2012, QUALINET WHITE PAPER
   LIN Q, 1992, IEEE T GEOSCI REMOTE, V30, P560, DOI 10.1109/36.142934
   Moldovan C., 2017, P IEEE TEL C JAN, P7
   Molnar A., 2010, Proceedings 10th International Conference on Intelligent Systems Design and Applications (ISDA 2010), P871, DOI 10.1109/ISDA.2010.5687153
   Molnar A, 2013, IEEE T BROADCAST, V59, P484, DOI 10.1109/TBC.2013.2244786
   Molnar A, 2012, CONSUM COMM NETWORK, P265, DOI 10.1109/CCNC.2012.6181099
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Nightingale J, 2012, IEEE T CONSUM ELECTR, V58, P404, DOI 10.1109/TCE.2012.6227440
   Nur G, 2012, IEEE T CIRC SYST VID, V22, P225, DOI 10.1109/TCSVT.2011.2160600
   O'Hara K, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P857
   Pedersen HA, 2016, IEEE ACM T NETWORK, V24, P996, DOI 10.1109/TNET.2015.2410298
   Ries M., 2008, P INT C CONS EL ICCE, P1
   Sackl Andreas, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P57, DOI 10.1109/QoMEX.2014.6982291
   Sackl A., 2012, P IEEE QUAL MULT EXP, P1
   Sackl A, 2013, INT WORK QUAL MULTIM, P40, DOI 10.1109/QoMEX.2013.6603204
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shen Y, 2012, IEEE GLOBE WORK, P1314, DOI 10.1109/GLOCOMW.2012.6477772
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Song Wei., 2011, Proceedings_of_the_19th_ACM international_conference_on_Multimedia, P403
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Weinberger D., 2013, SUPPORTED MEDIA FORM
   Wuensch K. L., 2009, What is a Likert Scale? and How Do You Pronounce 'Likert?'
   Yan Z., 2016, IEEE T CIRCUITS SYST, V27, P207
   Zhang F, 2013, IEEE T IMAGE PROCESS, V22, P1534, DOI 10.1109/TIP.2012.2233486
   Zwickl P, 2013, IEEE GLOB COMM CONF, P1310, DOI 10.1109/GLOCOM.2013.6831255
NR 48
TC 33
Z9 33
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1154
EP 1168
DI 10.1109/TMM.2017.2764329
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400011
DA 2024-07-18
ER

PT J
AU Yang, YM
   Pouyanfar, S
   Tian, HM
   Chen, M
   Chen, SC
   Shyu, ML
AF Yang, Yimin
   Pouyanfar, Samira
   Tian, Haiman
   Chen, Min
   Chen, Shu-Ching
   Shyu, Mei-Ling
TI IF-MCA: Importance Factor-Based Multiple Correspondence Analysis for
   Multimedia Data Analytics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Importance factor; information gain; feature selection; multiple
   correspondence analysis (MCA)
ID ATTRIBUTES; SELECTION
AB Multimedia concept detection is a challenging topic due to the well-known class imbalance issue, where the data instances are distributed unevenly across different classes. This problem becomes even more prominent when the minority class that contains an extremely small proportion of the data represents the concept of interest as has occurred in many real-world applications such as frauds in banking transactions and goal events in soccer videos. Traditional data mining approaches often have difficulty handling largely skewed data distributions. To address this issue, in this paper, an importance-factor (IF)-based multiple correspondence analysis (MCA) framework is proposed to deal with the imbalanced datasets. Specifically, a hierarchical information gain analysis method, which is inspired by the decision tree algorithm, is presented for critical feature selection and IF assignment. Then, the derived IF is incorporated with the MCA algorithm for effective concept detection and retrieval. The comparison results in video concept detection using the disaster dataset and the soccer dataset demonstrate the effectiveness of the proposed framework.
C1 [Yang, Yimin; Pouyanfar, Samira; Tian, Haiman; Chen, Shu-Ching] Florida Int Univ, Sch Comp & Informat Sci, Miami, FL 33199 USA.
   [Chen, Min] Univ Washington, Sch STEM, Comp & Software Syst Div, Bothell, WA 98011 USA.
   [Shyu, Mei-Ling] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA.
C3 State University System of Florida; Florida International University;
   University of Washington; University of Washington Bothell; University
   of Miami
RP Shyu, ML (corresponding author), Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA.
EM yyang010@cs.fiu.edu; spouy001@cs.fiu.odu; htian005@cs.fiu.edu;
   minchen2@u.washington.edu; chens@cs.fiu.edu; shyu@miami.edu
RI Pouyanfar, Samira/V-2135-2019
OI Chen, Shu-Ching/0000-0001-9209-390X; Tian, Haiman/0000-0002-8363-8514
FU NSF [HRD-1547798, CNS-1461926]
FX The work of S.-C. Chen was supported in part by NSF HRD-1547798 and
   CNS-1461926. The associate editor coordinating the review of this
   manuscript and approvins it for publication was Dr. Sen-Ching Samson
   Cheung.
CR Abdi H., 2007, Encyclopedia of measurement and statistics, DOI DOI 10.4135/9781412952644.N299
   Akbani R, 2004, LECT NOTES COMPUT SC, V3201, P39, DOI 10.1007/978-3-540-30115-8_7
   Chen M, 2014, MOBILE NETW APPL, V19, P171, DOI 10.1007/s11036-013-0489-0
   Chen S.-C., 2005, VIDEO DATA MANAGEMEN, P217
   Chen S.-C., 2000, International Journal of Networking and Information Systems, Special Issue on Video Data, V3, P9
   Chen SC, 2006, INT J COMPUT APPL T, V27, P312, DOI 10.1504/IJCAT.2006.012001
   Chen Shu-Ching., 1997, International Symposium on Multimedia Information Processing, P441
   Chen X. wen, 2008, P 14 ACM SIGKDD INT, P124, DOI [DOI 10.1145/1401890.1401910, 10.1145/1401890.1401910]
   Chen X, 2009, IEEE T SYST MAN CY C, V39, P228, DOI 10.1109/TSMCC.2008.2007257
   Djuric Nemanja, 2013, 2013 IEEE International Conference on Big Data, P458, DOI 10.1109/BigData.2013.6691607
   FAYYAD UM, 1992, MACH LEARN, V8, P87, DOI 10.1007/BF00994007
   FAYYAD UM, 1993, IJCAI-93, VOLS 1 AND 2, P1022
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Fu ZW, 2001, IEEE C EVOL COMPUTAT, P1382, DOI 10.1109/CEC.2001.934352
   Hall M., 2009, ACM SIGKDD Explor. Newsl, V11, P18, DOI DOI 10.1145/1656274.1656278
   Hall M. A., 2000, P 17 INT C MACH LEAR, P359, DOI DOI 10.5555/645529.657793
   Kham N. S. M., 2011, INT J INF ED TECHNOL, V1, P392
   Koyutürk M, 2005, IEEE T KNOWL DATA EN, V17, P447, DOI 10.1109/TKDE.2005.55
   Lai C, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-235
   Lin L, 2010, INT J MULTIMED DATA, V1, P37, DOI 10.4018/jmdem.2010111203
   Lin L, 2008, IEEE INT SYM MULTIM, P316, DOI 10.1109/ISM.2008.111
   Lu YL, 2007, PROCEEDINGS OF 2007 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P1930
   Peres-Neto PR, 2005, COMPUT STAT DATA AN, V49, P974, DOI 10.1016/j.csda.2004.06.015
   Plasberg JH, 2009, IEEE T MULTIMEDIA, V11, P565, DOI 10.1109/TMM.2009.2012944
   Pouyanfar S, 2017, INT J SEMANT COMPUT, V11, P85, DOI 10.1142/S1793351X17400050
   Quinlan J. R., 1993, PROGRAMS MACHINE LEA
   Quinlan JR, 1996, J ARTIF INTELL RES, V4, P77, DOI 10.1613/jair.279
   Shu-Ching Chen, 1999, Proceedings 11th International Conference on Tools with Artificial Intelligence, P175, DOI 10.1109/TAI.1999.809783
   Shyu M.-L., 2001, Knowledge and Information Systems, V3, P319, DOI 10.1007/PL00011671
   Shyu ML, 2008, IEEE T MULTIMEDIA, V10, P252, DOI 10.1109/TMM.2007.911830
   Smith J., 2013, Revitalising Grasslands to Sustain our Communities: Proceedings, 22nd International Grassland Congress, 15-19 September, 2013, Sydney, Australia, P1
   Tang J., 2008, EURASIP Journal on Advanced Signal Processing, V2008, P1
   Tao Meng, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P860, DOI 10.1109/ICME.2012.134
   Wang LJ, 2013, COMPUT MATH METHOD M, V2013, DOI 10.1155/2013/645921
   WHITE AP, 1994, MACH LEARN, V15, P321, DOI 10.1007/BF00993349
   Yang YM, 2014, IEEE MULTIMEDIA, V21, P36, DOI 10.1109/MMUL.2013.33
   Yang YM, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P100, DOI 10.1109/ISM.2012.28
   Zhao L, 2015, IEEE T MULTIMEDIA, V17, P1936, DOI 10.1109/TMM.2015.2477058
   Zhu QS, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION (IRI), P390, DOI 10.1109/IRI.2011.6009579
   Zhu QS, 2010, IEEE INT C SEMANT CO, P462, DOI 10.1109/ICSC.2010.65
NR 40
TC 5
Z9 5
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 1024
EP 1032
DI 10.1109/TMM.2017.2760623
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000020
DA 2024-07-18
ER

PT J
AU Zahálka, J
   Rudinac, S
   Jonsson, BD
   Koelma, DC
   Worring, M
AF Zahalka, Jan
   Rudinac, Stevan
   Jonsson, Bjorn Dor
   Koelma, Dennis C.
   Worring, Marcel
TI Blackthorn: Large-Scale Interactive Multimodal Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data compression; feature selection; interactive multimodal learning;
   multimedia analysis; YFCC100M
ID VIDEO
AB This paper presents Blackthorn, an efficient interactive multimodal learning approach facilitating analysis of multimedia collections of up to 100 million items on a single high-end workstation. Blackthorn features efficient data compression, feature selection, and optimizations to the interactive learning process. The Ratio-64 data representation introduced in this paper only costs tens of bytes per item yet preserves most of the visual and textual semantic information with good accuracy. The optimized interactive learning model scores the Ratio-64-compressed data directly, greatly reducing the computational requirements. The experiments compare Blackthorn with two baselines: Conventional relevance feedback, and relevance feedback using product quantization to compress the features. The results show that Blackthorn is up to 77.5 x faster than the conventional relevance feedback alternative, while outperforming the baseline with respect to the relevance of results: It vastly outperforms the baseline on recall over time and reaches up to 108% of its precision. Compared to the product quantization variant, Blackthorn is just as fast, while producing more relevant results. On the full YFCC100M dataset, Blackthorn performs one complete interaction round in roughly 1 s while maintaining adequate relevance of results, thus opening multimedia collections comprising up to 100 million items to fully interactive learning-based analysis.
C1 [Zahalka, Jan; Rudinac, Stevan; Koelma, Dennis C.; Worring, Marcel] Univ Amsterdam, Informat Inst, NL-1012 WX Amsterdam, Netherlands.
   [Jonsson, Bjorn Dor] IT Univ Copenhagen, DK-2300 Copenhagen, Denmark.
   [Jonsson, Bjorn Dor] Reykjavik Univ, IS-101 Reykjavik, Iceland.
C3 University of Amsterdam; IT University Copenhagen; Reykjavik University
RP Zahálka, J (corresponding author), Univ Amsterdam, Informat Inst, NL-1012 WX Amsterdam, Netherlands.
EM j.zahalka@uva.nl; s.rudinac@uva.nl; bjorn@ru.is; d.c.koelma@uva.nl;
   m.worring@uva.nl
RI Zahálka, Jan/AAR-5242-2020; Worring, Marcel/JRW-7059-2023
OI Koelma, Dennis/0000-0002-2207-5682; Zahalka, Jan/0000-0002-6743-3607;
   Jonsson, Bjorn THor/0000-0003-0889-3491; Worring,
   Marcel/0000-0003-4097-4136
FU project Sort-It-Out: Visual Analytics for Multimedia collections
   [12540]; project Database Support for Multimedia Analytics [040.11.525]
FX This work is part of research projects Sort-It-Out: Visual Analytics for
   Multimedia collections, project number 12540, and Database Support for
   Multimedia Analytics, project number 040.11.525.
CR Aggarwal CC, 2001, LECT NOTES COMPUT SC, V1973, P420
   Aly R., 2010, SHOT RETRIEVAL SEARC, P241
   Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Bernd J, 2015, MMCOMMONS'15: PROCEEDINGS OF THE 2015 WORKSHOP ON COMMUNITY-ORGANIZED MULTIMODAL MINING: OPPORTUNITIES FOR NOVEL SOLUTIONS, P1, DOI 10.1145/2814815.2816986
   Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bondugula S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P823, DOI 10.1145/2733373.2806340
   Choi J., 2015, P MEDIAEVAL
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   de Rooij O, 2013, IEEE T MULTIMEDIA, V15, P898, DOI 10.1109/TMM.2013.2237894
   Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]
   Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101
   Gudmundsson GP, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P1, DOI 10.1145/3083187.3083200
   Hinneburg A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P506
   Huang TS, 2008, P IEEE, V96, P648, DOI 10.1109/JPROC.2008.916364
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P49, DOI 10.1145/2733373.2806237
   Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298
   Kalkowski S, 2015, MMCOMMONS'15: PROCEEDINGS OF THE 2015 WORKSHOP ON COMMUNITY-ORGANIZED MULTIMODAL MINING: OPPORTUNITIES FOR NOVEL SOLUTIONS, P25, DOI 10.1145/2814815.2814820
   Kovashka A, 2015, INT J COMPUT VISION, V115, P185, DOI 10.1007/s11263-015-0814-0
   Larson M., 2011, Proceedings of the 1st ACM International Conference on Multimedia Retrieval, P1
   Lejsek H., 2011, P 1 ACM INT C MULT R, P54
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Mironica I, 2016, COMPUT VIS IMAGE UND, V143, P38, DOI 10.1016/j.cviu.2015.10.005
   Moise D., 2013, Proceedings of the 3rd ACM International conference on multimedia retrieval - ICMR'13, P17, DOI DOI 10.1145/2461466.2461470
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   North C, 2006, IEEE COMPUT GRAPH, V26, P6, DOI 10.1109/MCG.2006.70
   Rao V, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P189, DOI 10.1145/2911996.2911998
   Rehurek R., 2010, LREC, DOI DOI 10.13140/2.1.2393.1847
   Richard Bellman., 1961, Adaptive control processes: A guided tour
   Rudinac S, 2012, INT J MULTIMED INF R, V1, P263, DOI 10.1007/s13735-012-0018-0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schoeffmann K, 2014, IEEE MULTIMEDIA, V21, P8, DOI 10.1109/MMUL.2014.56
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Snoek CGM, 2008, IEEE MULTIMEDIA, V15, P86, DOI 10.1109/MMUL.2008.21
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Sundaram N, 2013, PROC VLDB ENDOW, V6, P1930, DOI 10.14778/2556549.2556574
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Wactlar HD, 1996, COMPUTER, V29, P46, DOI 10.1109/2.493456
   Wang JC, 2014, KEY ENG MATER, V579-580, P517, DOI 10.4028/www.scientific.net/KEM.579-580.517
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Zahálka J, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P231, DOI 10.1145/2733373.2806279
   Zahalka J, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P333, DOI 10.1145/2911996.2912062
   Zahálka J, 2014, IEEE CONF VIS ANAL, P3, DOI 10.1109/VAST.2014.7042476
   Zhang L, 2013, PROCEEDINGS OF THE EIGHTH INTERNATIONAL SYMPOSIUM ON VITICULTURE AND ENOLOGY (2013), P123
   Zhou B., 2016, ARXIV161002055V1
NR 49
TC 13
Z9 13
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 687
EP 698
DI 10.1109/TMM.2017.2755986
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500014
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Liu, YT
   Zhai, GT
   Gu, K
   Liu, XM
   Zhao, DB
   Gao, W
AF Liu, Yutao
   Zhai, Guangtao
   Gu, Ke
   Liu, Xianming
   Zhao, Debin
   Gao, Wen
TI Reduced-Reference Image Quality Assessment in Free-Energy Principle and
   Sparse Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Free-energy principle; image quality assessment (IQA); reduced-reference
   (RR); sparse representation; visual saliency
ID INFORMATION
AB The free-energy principle in recent studies of brain theory and neuroscience models the perception and understanding of the outside scene as an active inference process, in which the brain tries to account for the visual scene with an internal generative model. Specifically, with the internal generative model, the brain yields corresponding predictions for its encountered visual scenes. Then, the discrepancy between the visual input and its brain prediction should be closely related to the quality of perceptions. On the other hand, sparse representation has been evidenced to resemble the strategy of the primary visual cortex in the brain for representing natural images. With the strong neurobiological support for sparse representation, in this paper, we approximate the internal generative model with sparse representation and propose an image quality metric accordingly, which is named FSI (free-energy principle and sparse representation-based index for image quality assessment). In FSI, the reference and distorted images are, respectively, predicted by the sparse representation at first. Then, the difference between the entropies of the prediction discrepancies is defined to measure the image quality. Experimental results on four large-scale image databases confirm the effectiveness of the FSI and its superiority over representative image quality assessment methods. The FSI belongs to reduced-reference methods, and it only needs a single number from the reference image for quality estimation.
C1 [Liu, Yutao; Liu, Xianming; Zhao, Debin; Gao, Wen] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Gao, Wen] Peking Univ, Natl Engn Lab Video Technol, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Gao, Wen] Peking Univ, Key Lab Machine Percept, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Harbin Institute of Technology; Shanghai Jiao Tong University; Beijing
   University of Technology; Peking University; Peking University
RP Liu, XM (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM yt.liu@hit.edu.cn; zhaiguangtao@gmail.com; guke@bjut.edu.cn;
   xmliu.hit@gmail.com; dbzhao@hit.edu.cn; wgao@pku.edu.cn
RI Gu, Ke/AAJ-9684-2021; Zhao, Debin/JEP-0204-2023; Zhai,
   Guangtao/X-5949-2019
OI Zhai, Guangtao/0000-0001-8165-9322
FU Major State Basic Research Development Program of China (973 Program)
   [2015CB351804]; National Science Foundation of China [61672193]
FX This work was supported in part by the Major State Basic Research
   Development Program of China (973 Program) under Grant 2015CB351804 and
   in part by the National Science Foundation of China under Grant
   61672193. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Ivan Bajic.
   (Corresponding author: Xianming Liu.)
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2003, The visual neurosciences
   [Anonymous], MICT IMAGE QUALITY E
   [Anonymous], IEEE T NEUR IN PRESS
   [Anonymous], PROC SCAND CONF IM
   [Anonymous], 2016, LIVE IMAGE QUALITY A
   [Anonymous], 2010, Categorical image quality (CSIQ) database
   [Anonymous], 2016, ACM T MULTIM COMPUT
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], FRONT PHYS
   Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Duan LJ, 2011, PROC CVPR IEEE, P473, DOI 10.1109/CVPR.2011.5995676
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Gu K, 2015, IEEE T CIRC SYST VID, V25, P1480, DOI 10.1109/TCSVT.2014.2372392
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Liu XM, 2017, IEEE T IMAGE PROCESS, V26, P782, DOI 10.1109/TIP.2016.2623481
   Liu XM, 2017, IEEE T IMAGE PROCESS, V26, P509, DOI 10.1109/TIP.2016.2627807
   Liu YT, 2017, J VIS COMMUN IMAGE R, V46, P70, DOI 10.1016/j.jvcir.2017.03.007
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Min XK, 2016, IEEE INT CON MULTI
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Rohaly A.M., 2000, ITU T STANDARDS CONT, P9
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Vikram TN, 2012, PATTERN RECOGN, V45, P3114, DOI 10.1016/j.patcog.2012.02.009
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Wang Z, 2006, IEEE T IMAGE PROCESS, V15, P1680, DOI 10.1109/TIP.2005.864165
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Xu Y, 2015, IEEE T IMAGE PROCESS, V24, P2098, DOI 10.1109/TIP.2015.2413298
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang Y, 2014, SIGNAL PROCESS-IMAGE, V29, P725, DOI 10.1016/j.image.2014.05.004
NR 64
TC 98
Z9 102
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 379
EP 391
DI 10.1109/TMM.2017.2729020
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200010
DA 2024-07-18
ER

PT J
AU Jiang, RQ
   Zhou, H
   Zhang, WM
   Yu, NH
AF Jiang, Ruiqi
   Zhou, Hang
   Zhang, Weiming
   Yu, Nenghai
TI Reversible Data Hiding in Encrypted Three-Dimensional Mesh Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Curvature; reversible data hiding; three-dimensional (3D) mesh models;
   vertex partition
ID WATERMARKING; PREDICTION; EXPANSION; ALGORITHM
AB Reversible data hiding in encrypted domain (RDH-ED) has greatly attracted researchers as the original content can be losslessly reconstructed after the embedded data are extracted, while the content owner's privacy remains protected. Most of the existing RDH-ED algorithms are designed for grayscale/color images, which cannot be directly applied to other carriers, such as three-dimensional (3D) meshes. With the rapid development of 3D related applications, 3D models have been widely used on the Internet, which motivated us to design a reliable RDH-ED scheme for 3D meshes. The proposed method maps decimals of the vertex coordinates into integers first, so that a bit-stream encryption technique can be executed. With a data-hiding key, several least-significant bits are operated to embed data. By using the encryption key, a receiver can roughly reconstruct the content of the mesh. According to the data-hiding key, with the aid of spatial correlation in natural mesh models, the embedded data can be successfully extracted and the original mesh can be perfectly recovered. Experiments show that the proposed method has a high data-embedding payload, maintains high values of the decrypted meshes, and has low computational complexity.
C1 [Jiang, Ruiqi; Zhou, Hang; Zhang, Weiming; Yu, Nenghai] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhang, WM (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Anhui, Peoples R China.
EM jrq123@mail.ustc.edu.cn; zh2991@mail.ustc.edu.cn; zhangwm@ustc.edu.cn;
   ynh@ustc.edu.cn
RI Zhou, Hang/AAI-5565-2021
OI Zhang, Weiming/0000-0001-5576-6108
FU Natural Science Foundation of China [61572452, U1636201]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61572452 and Grant U1636201. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Balakrishnan Prabhakaran.
CR [Anonymous], P 42 ANN ALL C COMM
   BESL PJ, 1986, COMPUT VISION GRAPH, V33, P33, DOI 10.1016/0734-189X(86)90220-3
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Chandramouli Ramaswamy., 2014, SECURE CLOUD COMPUTI, P1
   Chao MW, 2009, IEEE T VIS COMPUT GR, V15, P274, DOI 10.1109/TVCG.2008.94
   Cheng YM, 2006, VISUAL COMPUT, V22, P845, DOI 10.1007/s00371-006-0069-4
   Chou D, 2009, INT J INNOV COMPUT I, V5, P1893
   De Vleeschouwer C, 2003, IEEE T MULTIMEDIA, V5, P97, DOI 10.1109/TMM.2003.809729
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hu XC, 2015, IEEE T INF FOREN SEC, V10, P653, DOI 10.1109/TIFS.2015.2392556
   Hu XC, 2013, IEEE T INF FOREN SEC, V8, P779, DOI 10.1109/TIFS.2013.2256131
   Hu YJ, 2008, IEEE T MULTIMEDIA, V10, P1500, DOI 10.1109/TMM.2008.2007341
   Hwang K, 2010, IEEE INTERNET COMPUT, V14, P14, DOI 10.1109/MIC.2010.86
   Johnson M, 2004, IEEE T SIGNAL PROCES, V52, P2992, DOI 10.1109/TSP.2004.833860
   Lin C., 1982, Proceedings of the Workshop on Computer Vision: Representation and Control, P38
   Liu W, 2010, IEEE T IMAGE PROCESS, V19, P1097, DOI 10.1109/TIP.2009.2038773
   Luo H, 2006, 2006 IEEE International Symposium on Signal Processing and Information Technology, Vols 1 and 2, P863, DOI 10.1109/ISSPIT.2006.270919
   Luo H, 2006, IIH-MSP: 2006 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P487
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Malvar HS, 2003, IEEE T SIGNAL PROCES, V51, P898, DOI 10.1109/TSP.2003.809385
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   RYAN W., 2004, CRC HDB CODING SIGNA
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   STOKELY EM, 1992, IEEE T PATTERN ANAL, V14, P833, DOI 10.1109/34.149594
   Sun Z, 2006, IIH-MSP: 2006 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P593
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang CM, 2005, COMPUT GRAPH FORUM, V24, P591, DOI 10.1111/j.1467-8659.2005.00884.x
   Wang K, 2007, LECT NOTES COMPUT SC, V4567, P50
   Wu HT, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P801
   Wu HT, 2005, 2005 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P774
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang WM, 2015, IEEE T IMAGE PROCESS, V24, P294, DOI 10.1109/TIP.2014.2358881
   Zhang WM, 2014, SIGNAL PROCESS, V94, P118, DOI 10.1016/j.sigpro.2013.06.023
   Zhang WM, 2013, IEEE T IMAGE PROCESS, V22, P2775, DOI 10.1109/TIP.2013.2257814
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zhang XP, 2008, IEEE T MULTIMEDIA, V10, P1490, DOI 10.1109/TMM.2008.2007334
   Zhou JT, 2016, IEEE T CIRC SYST VID, V26, P441, DOI 10.1109/TCSVT.2015.2416591
   Ziguang L, 2007, PROCEEDINGS OF THE 26TH CHINESE CONTROL CONFERENCE, VOL 6, P233
   ZUCKER SW, 1981, IEEE T PATTERN ANAL, V3, P324, DOI 10.1109/TPAMI.1981.4767105
NR 47
TC 69
Z9 71
U1 4
U2 60
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 55
EP 67
DI 10.1109/TMM.2017.2723244
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700005
DA 2024-07-18
ER

PT J
AU Toni, L
   Frossard, P
AF Toni, Laura
   Frossard, Pascal
TI Optimal Representations for Adaptive Streaming in Interactive Multiview
   Video Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Immersive communication; multiview video; dynamic adaptive streaming
   over HTTP; interger linear programming
AB Interactive multiview video streaming (IMVS) services permit to remotely navigate within a 3D scene with an immersive experience. This is possible by transmitting a set of reference camera views (anchor views), which are used by the clients to freely navigate in the scene and possibly synthesize additional viewpoints of interest. From a networking perspective, the big challenge in IMVS systems is to deliver to each client the best set of anchor views that maximizes the navigation quality, minimizes the view-switching delay and yet satisfies the network constraints. Integrating adaptive streaming solutions in free-viewpoint systems offers a promising solution to deploy IMVS in large and heterogeneous scenarios, as long as the multiview video representations on the server are properly selected. Therefore, we propose to optimize the multiview data at the server by minimizing the overall resource requirements while offering a good navigation quality to the different users. We propose a representation set optimization problem for multiview adaptive streaming systems, and we show that it is NP-hard. Therefore, we introduce the concept of multiview navigation segment that permits to cast the video representation set selection as an integer linear programming problem with a bounded computational complexity. We then show that the proposed solution reduces the computational complexity, while preserving optimality in most of the 3D scenes. We finally provide simulation results for different classes of users and show the gain offered by an optimal multiview video representation selection compared to recommended representation sets (e.g., Netflix and Apple ones) or to a baseline representation selection algorithm, where the encoding parameters are decided a priori for all the camera views.
C1 [Toni, Laura; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
   [Toni, Laura] UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; University of London; University College London
RP Toni, L (corresponding author), UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
EM l.toni@ucl.ac.uk; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
FU Swiss National Science Foundation under CHIST-ERA project CONCERT [FNS
   20CH21 151569]
FX This work was supported in part by the Swiss National Science Foundation
   under the CHIST-ERA project CONCERT (A Context-Adaptive Content
   Ecosystem Under Uncertainty) FNS 20CH21 151569. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shiwen Mao. (Corresponding author: Laura Toni.)
CR Abreu A. D., 2016, ABS160306123 ARXIV
   [Anonymous], P 7 INT C MULT SYST
   [Anonymous], 2014, PROC NETW OPERATING
   [Anonymous], 2016, P 26 INT WORKSH NETW
   [Anonymous], 2013, IBM ILOG CPLEX OPT S
   [Anonymous], 2014, P INT C SIGN PROC CO
   Aparicio-Pardo Ramon., 2015, Proceedings of the 6th ACM Multimedia Systems Conference, MMSys'15, P49
   Apple, 2016, US HTTP LIV STREAM
   Besson A, 2013, IEEE IMAGE PROC, P59, DOI 10.1109/ICIP.2013.6738013
   Bosc E, 2013, ANN TELECOMMUN, V68, P615, DOI 10.1007/s12243-013-0363-x
   Calagari K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P337, DOI 10.1145/2647868.2654899
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Chiariotti F, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P77, DOI 10.1145/2910017.2910603
   De Abreu A, 2015, J VIS COMMUN IMAGE R, V33, P255, DOI 10.1016/j.jvcir.2015.09.010
   El Essaili A, 2015, IEEE T CIRC SYST VID, V25, P988, DOI 10.1109/TCSVT.2014.2367355
   Gao Z, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P801, DOI 10.1145/2733373.2807971
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Netflix, 2015, PerTitle Encode Optimization
   Su TY, 2016, J REAL-TIME IMAGE PR, V12, P329, DOI 10.1007/s11554-015-0504-8
   Tian D, 2009, PROC SPIE, V7443, DOI 10.1117/12.829372
   Toni L., 2016, 160904196V1 ARXIV
   Toni L, 2016, IEEE T MULTIMEDIA, V18, P852, DOI 10.1109/TMM.2016.2537207
   Toni L, 2015, IEEE T MULTIMEDIA, V17, P1604, DOI 10.1109/TMM.2015.2450020
   Toni L, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700294
   Toni L, 2013, IEEE INT WORKSH MULT, P446, DOI 10.1109/MMSP.2013.6659330
   Thang TC, 2014, IEEE J SEL AREA COMM, V32, P693, DOI 10.1109/JSAC.2014.140403
   VQM, 2007, VID QUAL RES VQM SOF
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhao MC, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P221, DOI 10.1109/PCS.2015.7170079
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 30
TC 17
Z9 18
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2775
EP 2787
DI 10.1109/TMM.2017.2713644
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Al-Zubaidy, H
   Fodor, V
   Dan, G
   Flierl, M
AF Al-Zubaidy, Hussein
   Fodor, Viktoria
   Dan, Gyorgy
   Flierl, Markus
TI Reliable Video Streaming With Strict Playout Deadline in Multihop
   Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multihop fading channels; network calculus; performance analysis;
   scalable video coding; wireless multimedia
ID ADAPTATION; QUALITY; MOBILE
AB Motivated by emerging vision-based intelligent services, we consider the problem of rate adaptation for high-quality and low-delay visual information delivery over wireless networks using scalable video coding. Rate adaptation in this setting is inherently challenging due to the interplay between the variability of the wireless channels, the queuing at the network nodes, and the frame-based decoding and playback of the video content at the receiver at very short time scales. To address the problem, we propose a low-complexity model-based rate adaptation algorithm for scalable video streaming systems, building on a novel performance model based on stochastic network calculus. We validate the analytic model using extensive simulations. We show that it allows fast near-optimal rate adaptation for fixed transmission paths, as well as cross-layer optimized routing and video rate adaptation in mesh networks, with less than 10% quality degradation compared to the best achievable performance.
C1 [Al-Zubaidy, Hussein; Fodor, Viktoria; Dan, Gyorgy; Flierl, Markus] KTH Royal Inst Technol, Sch Elect Engn, Access Linnaeus Ctr, S-11428 Stockholm, Sweden.
C3 Royal Institute of Technology
RP Al-Zubaidy, H (corresponding author), KTH Royal Inst Technol, Sch Elect Engn, Access Linnaeus Ctr, S-11428 Stockholm, Sweden.
EM hzubaidy@kth.se; vjfodor@kth.se; gyuri@kth.se; mflierl@kth.se
RI Dan, György/H-8604-2012
OI Dan, György/0000-0002-4876-0223; Flierl, Markus
   Helmut/0000-0002-7807-5681; /0000-0002-0150-2489
CR Accettura N., 2011, Proceedings of the 2011 IEEE International Conference on Mechatronics (ICM), P767, DOI 10.1109/ICMECH.2011.5971218
   Al-Zubaidy H, 2016, IEEE ACM T NETWORK, V24, P204, DOI 10.1109/TNET.2014.2360675
   Al-Zubaidy H, 2013, IEEE INFOCOM SER, P1833
   [Anonymous], P 2 INT C COMM NETW
   [Anonymous], 2011, P 2011 3 INT ASIA PA
   [Anonymous], 2015, CISC VIS NETW IND GL
   [Anonymous], 2006, P IEEE GLOBECOM
   [Anonymous], CORR
   [Anonymous], P IEEE INT C COMM SY
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], 4G LTE LTE ADVANED M
   Baroffio L, 2014, IEEE IMAGE PROC, P3408, DOI 10.1109/ICIP.2014.7025690
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Chang C.-S., 2000, TELEC NETW COMP SYST
   Chen SW, 2016, IEEE T CIRC SYST VID, V26, P1146, DOI 10.1109/TCSVT.2015.2437071
   Chen SW, 2014, IEEE INT C NETW SENS, P221, DOI 10.1109/ICNSC.2014.6819629
   Ciucu Florin, 2011, Performance Evaluation Review, V39, P359, DOI 10.1145/2007116.2007177
   Davies B., 1978, Integral Transforms and their Applications
   Fidler M, 2006, INT WORKSH QUAL SERV, P261, DOI 10.1109/IWQOS.2006.250477
   Gerla M, 2014, 2014 IEEE WORLD FORUM ON INTERNET OF THINGS (WF-IOT), P241, DOI 10.1109/WF-IoT.2014.6803166
   Ghodoussi M, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P1882, DOI 10.1109/ROBOT.2002.1014815
   Halperin D, 2011, ACM SIGCOMM COMP COM, V41, P53, DOI 10.1145/1925861.1925870
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Jiang Y., 2008, Stochastic Network Calculus
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liebeherr J, 2011, IEEE J SEL AREA COMM, V29, P1009, DOI 10.1109/JSAC.2011.110511
   Lin HL, 2012, IEEE COMMUN LETT, V16, P1349, DOI 10.1109/LCOMM.2012.070512.120760
   Matsubara Daisuke, 2013, The Future Internet, P27, DOI 10.1007/978-3-642-38082-2_3
   MCELIECE RJ, 1984, IEEE T INFORM THEORY, V30, P44, DOI 10.1109/TIT.1984.1056848
   Meng SB, 2016, IEEE T MULTIMEDIA, V18, P1124, DOI 10.1109/TMM.2016.2535270
   Movassaghi S, 2014, IEEE COMMUN SURV TUT, V16, P1658, DOI 10.1109/SURV.2013.121313.00064
   Nightingale J, 2013, 2013 IEEE 24TH INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), P3573, DOI 10.1109/PIMRC.2013.6666769
   Nishiyama H, 2014, IEEE COMMUN MAG, V52, P56, DOI 10.1109/MCOM.2014.6807947
   Petreska N, 2015, IEEE ICC, P5998, DOI 10.1109/ICC.2015.7249278
   Rizk Amr., 2015, 2015 IFIP Networking Conference (IFIP Networking), P1
   Rufenacht D, 2016, IEEE T IMAGE PROCESS, V25, P39, DOI 10.1109/TIP.2015.2496332
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Verticale G, 2009, GLOB TELECOMM CONF, P635
   Wu DP, 2006, MOBILE NETW APPL, V11, P91, DOI 10.1007/s11036-005-4463-3
   Yang J, 2011, IEEE T MULTIMEDIA, V13, P1141, DOI 10.1109/TMM.2011.2160158
   Yang M, 2015, MOBILE NETW APPL, V20, P4, DOI 10.1007/s11036-014-0533-8
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 46
TC 7
Z9 8
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2238
EP 2251
DI 10.1109/TMM.2017.2742399
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600009
DA 2024-07-18
ER

PT J
AU Saltarin, J
   Bourtsoulatze, E
   Thomos, N
   Braun, T
AF Saltarin, Jonnahtan
   Bourtsoulatze, Eirina
   Thomos, Nikolaos
   Braun, Torsten
TI Adaptive Video Streaming With Network Coding Enabled Named Data
   Networking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Network coding; multimedia communication; streaming media; content
   distribution networks; next generation networking
ID RESILIENT; DELAY
AB The fast and huge increase of Internet traffic motivates the development of new communication methods that can deal with the growing volume of data traffic. To this aim, named data networking (NDN) has been proposed as a future Internet architecture that enables ubiquitous in-network caching and naturally supports multipath data delivery. Particular attention has been given to using dynamic adaptive streaming over HTTP to enable video streaming in NDN as in both schemes data transmission is triggered and controlled by the clients. However, state-of-the-art works do not consider the multipath capabilities of NDN and the potential improvements that multipath communication brings, such as increased throughput and reliability, which are fundamental for video streaming systems. In this paper, we present a novel architecture for dynamic adaptive streaming over network coding enabled NDN. In comparison to previous works proposing dynamic adaptive streaming over NDN, our architecture exploits network coding to efficiently use the multiple paths connecting the clients to the sources. Moreover, our architecture enables efficient multisource video streaming and improves resiliency to Data packet losses. The experimental evaluation shows that our architecture leads to reduced data traffic load on the sources, increased cache-hit rate at the in-network caches and faster adaptation of the requested video quality by the clients. The performance gains are verified through simulations in a Netflix-like scenario.
C1 [Saltarin, Jonnahtan; Bourtsoulatze, Eirina; Braun, Torsten] Univ Bern, CH-3012 Bern, Switzerland.
   [Bourtsoulatze, Eirina] Imperial Coll London, London SW7 2AZ, England.
   [Thomos, Nikolaos] Univ Essex, Colchester CO4 3SQ, Essex, England.
C3 University of Bern; Imperial College London; University of Essex
RP Saltarin, J (corresponding author), Univ Bern, CH-3012 Bern, Switzerland.
EM saltarin@inf.unibe.ch; e.bourtsoulatze@imperial.ac.uk;
   nthomos@essex.ac.uk; braun@inf.unibe.ch
RI Bourtsoulatze, Eirina/ABG-5003-2021; Braun, Torsten/AAA-2592-2019;
   Thomos, Nikolaos/AAU-2328-2020
OI Braun, Torsten/0000-0001-5968-7108; Thomos, Nikolaos/0000-0001-7266-2642
FU Swiss National Science Foundation [149225]
FX This work was supported by the Swiss National Science Foundation under
   Grant 149225. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shiwen Mao.
   (Corresponding author: Jonnahtan Saltarin.)
CR Aaron A., 2015, NETFLIX TECH BLOG PE
   Afanasyev A., 2016, NDN0021
   Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], 2017, NAM DAT NETW FORW DA
   [Anonymous], 2016, VISUAL NETWORKING IN
   [Anonymous], CORR
   [Anonymous], 2016, CORR
   [Anonymous], 2016, NETFL ISP SPEED IND
   [Anonymous], 2017, NETWORK SIMULATOR NS
   [Anonymous], 2013, 6824 RFC
   Bourtsoulatze E, 2014, IEEE T MULTIMEDIA, V16, P1752, DOI 10.1109/TMM.2014.2328320
   Chou PA, 2007, IEEE SIGNAL PROC MAG, V24, P77, DOI 10.1109/MSP.2007.904818
   Cleju N, 2011, IEEE T MULTIMEDIA, V13, P1103, DOI 10.1109/TMM.2011.2161448
   Detti A., 2012, Proc. WOWMOM, P1
   Fayazbakhsh SK, 2013, ACM SIGCOMM COMP COM, V43, P147, DOI 10.1145/2534169.2486023
   Ho T., 2003, P ANN ALLERTON C COM, V41, P11
   ISO/IEC, 2014, Rep. 23009-1. ISO/IEC JTCI/SC29/WG11
   Kreuzberger C, 2016, Amust framework-adaptive multimedia streaming simulation framework for ns-3 and ndnsim
   Lederer Stefan., 2013, MULTIMEDIA EXPO ICME, P1
   Llorca J, 2013, IEEE ICC, P3557, DOI 10.1109/ICC.2013.6655103
   Mastorakis S., 2017, COMPUT COMMUN REV, V47, P1
   Matsuzono K., 2017, P IEEE INFOCOM 17 AT
   Montpetit M.-J., 2012, Proceedings of the 1st ACM workshop on Emerging Name-Oriented Mobile Networking Design - Architecture, Algorithms, and Applications, NoM '12, P31
   Mueller C., 2013, P 2013 IEEE INT C MU
   Pedersen MV, 2011, LECT NOTES COMPUT SC, V6827, P145, DOI 10.1007/978-3-642-23041-7_15
   Plass M.F., 2009, P 5 INT C EMERGING N, P1, DOI [10.1145/1658939.1658941, DOI 10.1145/1658939.1658941]
   Rossini G, 2013, COMPUT COMMUN, V36, P771, DOI 10.1016/j.comcom.2013.01.008
   Saltarin J., 2016, P IEEE INFOCOM 16 SA
   Schneider K.M., 2015, Proceedings of the 2nd International Conference on Information-Centric Networking, P137
   Sheikh AM, 2014, IEEE T MULTIMEDIA, V16, P2294, DOI 10.1109/TMM.2014.2357716
   Sipos M., IEEE T CLOU IN PRESS
   Sun Y, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P363, DOI 10.1145/2674005.2675003
   Sundararajan JK, 2011, P IEEE, V99, P490, DOI 10.1109/JPROC.2010.2093850
   Thomos N, 2015, IEEE T MULTIMEDIA, V17, P893, DOI 10.1109/TMM.2015.2425228
   Wang LJ, 2016, PROCEEDINGS OF 2016 IEEE 18TH INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS; IEEE 14TH INTERNATIONAL CONFERENCE ON SMART CITY; IEEE 2ND INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (HPCC/SMARTCITY/DSS), P166, DOI [10.1109/HPCC-SmartCity-DSS.2016.0034, 10.1109/HPCC-SmartCity-DSS.2016.221]
   Wu C, 2008, IEEE T PARALL DISTR, V19, P77, DOI 10.1109/TPDS.2007.1119
   Wu Q., 2013, Proceedings of the 3rd ACM SIGCOMM workshop on Information-centric networking, ICN '13, P41
   Wu YN, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P143
   Zhang LX, 2014, ACM SIGCOMM COMP COM, V44, P66, DOI 10.1145/2656877.2656887
NR 39
TC 24
Z9 26
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2182
EP 2196
DI 10.1109/TMM.2017.2737950
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600005
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Maiero, J
   Kruijff, E
   Hinkenjann, A
   Ghinea, G
AF Maiero, Jens
   Kruijff, Ernst
   Hinkenjann, Andre
   Ghinea, Gheorghita
TI Focus-Plus-Context Techniques for Picoprojection-Based Interaction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Focus plus context; projection based systems; spatial augmented reality;
   zooming interfaces
ID VISUALIZATION
AB In this paper, we report on novel zooming interface methods that deploy a small handheld projector. Using mobile projections to visualize object/environment-related information on real objects introduces new aspects for zooming interfaces. Different approaches are investigated that focus on maintaining a level of context while exploring detailed information. Doing so, we propose methods that provide alternative contextual cues within a single projector and deploy the potential of zoom lenses to support a multilevel zooming approach. Furthermore, we look into the correlation between pixel density, distance to the target, and projection size. Alongside these techniques, we report on multiple user studies, in which we quantified the projection limitations and validated various interactive visualization approaches. Thereby, we focused on solving issues related to pixel density, brightness, and contrast that affect the design of more effective legible zooming interfaces for handheld projectors.
C1 [Maiero, Jens] Bonn Rhein Sieg Univ Appl Sci, Inst Visual Comp, D-53757 St Augustin, Germany.
   [Maiero, Jens; Kruijff, Ernst; Hinkenjann, Andre] Bonn Rhein Sieg Univ Appl Sci, Dept Comp Sci, D-53757 St Augustin, Germany.
   [Maiero, Jens; Ghinea, Gheorghita] Brunel Univ, Dept Comp Sci, London UB8 3PH, England.
C3 Hochschule Bonn Rhein Sieg; Hochschule Bonn Rhein Sieg; Brunel
   University
RP Maiero, J (corresponding author), Bonn Rhein Sieg Univ Appl Sci, Inst Visual Comp, D-53757 St Augustin, Germany.; Maiero, J (corresponding author), Bonn Rhein Sieg Univ Appl Sci, Dept Comp Sci, D-53757 St Augustin, Germany.
EM jens.maiero@h-brs.de; ernst.kruijff@h-brs.de; andre.hinkenjann@h-brs.de;
   george.ghinea@brunel.ac.uk
RI Ghinea, Gheorghita/AAG-6770-2020
OI Ghinea, Gheorghita/0000-0003-2578-5580; Maiero,
   Jens/0000-0001-6719-475X; Hinkenjann, Andre/0000-0002-8391-7652
CR Ahmaniemi TeemuTuomas., 2009, Proceedings of the 2009 international conference on Multimodal interfaces, ICMI-MLMI '09, P335, DOI [10.1145/1647314.1647383, DOI 10.1145/1647314.1647383]
   [Anonymous], 2007, ISMAR 07
   Baudisch Patrick., 2001, 14 ANN ACM S USER IN, P31, DOI DOI 10.1145/502348.502354
   Bier EA., 1993, Proceedings of the 20th annual conference on Computer graphics and interactive techniques, P73
   Bimber O, 2005, COMPUTER, V38, P48, DOI 10.1109/MC.2005.17
   Blattner M. M., 1989, Human-Computer Interaction, V4, P11, DOI 10.1207/s15327051hci0401_1
   Cao X., 2006, Proc. UIST' 06, P225, DOI DOI 10.1145/1166253.1166289
   Cockburn A, 2008, ACM COMPUT SURV, V41, DOI 10.1145/1456650.1456652
   Eckel G, 2001, FIFTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P571, DOI 10.1109/IV.2001.942112
   Goldstein E.B., 2002, Sensation and perception, V6ieme
   Jerome CJ., 2006, P HUM FACT ERG SOC A, V50, P2114, DOI 10.1177/154193120605001785
   Kim S., 2010, P 28 INT C HUM FACT, P3631
   Kincaid R, 2010, IEEE T VIS COMPUT GR, V16, P900, DOI 10.1109/TVCG.2010.193
   Langlotz T., 2013, P 25 AUSTR COMP HUM, P545, DOI [DOI 10.1145/2541016.2541022, 10.1145/2541016.2541022]
   Li ZR, 2011, IEEE T MULTIMEDIA, V13, P155, DOI 10.1109/TMM.2010.2092421
   Marquardt Nicolai, 2011, Proceedings of the 24th annual ACM symposium on User interface software and technology-UIST'11, P315, DOI 10.1145/2047196.2047238
   NI T, 2011, P SIGCHI C HUM FACT, P3333
   Ridel B, 2014, ACM J COMPUT CULT HE, V7, DOI 10.1145/2611376
   SCHILIT B. N., 1998, P 27 INT C HUM FACT, P249
   Spindler M, 2014, PERS UBIQUIT COMPUT, V18, P1213, DOI 10.1007/s00779-013-0730-7
   Spindler Martin., 2009, Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces-ITS '09, P69, DOI [DOI 10.1145/1731903.1731920, 10.1145/1731903, DOI 10.1145/1731903]
   Steimle J., 2013, P SIGCHI C HUM FACT, P2873
   Tominski C., 2014, P EUROVIS STAT OF TH, P43
   Tominski C, 2012, IEEE T VIS COMPUT GR, V18, P2565, DOI 10.1109/TVCG.2012.265
   Weigel M., 2012, 2012103114 U CALG DE
   Willis K.D. D., 2013, Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction, P331, DOI DOI 10.1145/2460625.2460682
   Winkler C, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P4117, DOI 10.1145/2556288.2557365
   Yi JS, 2007, IEEE T VIS COMPUT GR, V13, P1224, DOI 10.1109/TVCG.2007.70515
   Zhao J, 2011, IEEE T VIS COMPUT GR, V17, P2422, DOI 10.1109/TVCG.2011.195
   Zhao X, 2012, IEEE T VIS COMPUT GR, V18, P1928, DOI 10.1109/TVCG.2012.70
   Zotkin DN, 2004, IEEE T MULTIMEDIA, V6, P553, DOI [10.1109/TMM.2004.827516, 10.1109/tmm.2004.827516]
   [No title captured]
NR 32
TC 1
Z9 1
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1521
EP 1530
DI 10.1109/TMM.2017.2673410
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cen, N
   Guan, ZY
   Melodia, T
AF Cen, Nan
   Guan, Zhangyu
   Melodia, Tommaso
TI Interview Motion Compensated Joint Decoding for Compressively Sampled
   Multiview Video Streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiview video streaming; compressed sensing (CS); Internet of Things
   (IoT); 360 degrees video
ID MULTIMEDIA SENSOR NETWORKS; IMAGE QUALITY ASSESSMENT; MANY-CORE
   PROCESSORS; PARALLEL FRAMEWORK; HEVC; RECONSTRUCTION
AB In this paper, we design a novel multiview video encoding/decoding architecture for wirelessly multiview video streaming applications, e.g., 360 degrees video, Internet of Things (IoT) multimedia sensing, among others, based on distributed video coding and compressed sensing principles. Specifically, we focus on joint decoding of independently encoded compressively sampled multiview video streams. We first propose a novel side-information (SI) generation method based on a new interview motion compensation algorithm for multiview video joint reconstruction at the decoder end. Then, we propose a technique to fuse the received measurements with resampled measurements from the generated SI to perform the final recovery. Based on the proposed joint reconstruction method, we also derive a blind video quality estimation technique that can be used to adapt online the video encoding rate at the sensors to guarantee desired quality levels in multiview video streaming. Extensive simulation results of real multiview video traces show the effectiveness of the proposed fusion reconstruction method with the assistance of SI generated by an interview motion compensation method. Moreover, they also illustrate that the blind quality estimation algorithm can accurately estimate the reconstruction quality.
C1 [Cen, Nan; Guan, Zhangyu; Melodia, Tommaso] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
C3 Northeastern University
RP Cen, N (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
EM ncen@ece.neu.edu; zguan@ece.neu.edu; melodia@ece.neu.edu
RI Cen, Nan/W-6916-2019
OI Melodia, Tommaso/0000-0002-2719-1789; , Nan/0000-0002-0480-5303
FU U.S. National Science Foundation [CNS1422874]; U.S. Office of Naval
   Research [N00014-16-1-2213, ARMY W911NF-17-1-0034]
FX This work is based upon material supported in part by the U.S. National
   Science Foundation under Grant CNS1422874, and in part by the U.S.
   Office of Naval Research under Grant N00014-16-1-2213 and Grant ARMY
   W911NF-17-1-0034. This paper was presented in part at the Picture Coding
   Symposium, San Jose, CA, December 2013. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xiaoqing Zhu.
CR Akyildiz IF, 2007, COMPUT NETW, V51, P921, DOI 10.1016/j.comnet.2006.10.002
   Al-Fuqaha A, 2015, IEEE COMMUN SURV TUT, V17, P2347, DOI 10.1109/COMST.2015.2444095
   [Anonymous], 2015, P ACM INT S MOB AD H
   Boyd S., 2004, CONVEX OPTIMIZATION
   Budagavi M, 2015, IEEE IMAGE PROC, P750, DOI 10.1109/ICIP.2015.7350899
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Cen N, 2013, PICT COD SYMP, P341, DOI 10.1109/PCS.2013.6737753
   Chen H. W., 2010, VIS COMMUN IMAGE PRO, V7744, P1
   Chen X, 2009, INT CONF ACOUST SPEE, P1005, DOI 10.1109/ICASSP.2009.4959756
   Cossalter M, 2010, IEEE T MULTIMEDIA, V12, P168, DOI 10.1109/TMM.2010.2041105
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P278, DOI 10.1109/TMM.2011.2181491
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430
   Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281
   Gao K, 2011, IEEE T SIGNAL PROCES, V59, P4759, DOI 10.1109/TSP.2011.2160860
   Guan ZY, 2014, COMPUTER, V47, P60, DOI 10.1109/MC.2014.114
   HUANG AM, 2008, P WOWMOM JUN, P1
   Jamil F. H., 2011, P INT C MECH MAY, P1
   Kang LW, 2009, INT CONF ACOUST SPEE, P1169, DOI 10.1109/ICASSP.2009.4959797
   Koya T., 1981, P NTC81 NEW ORL LA N
   Liu HX, 2014, IEEE T MULTIMEDIA, V16, P1549, DOI 10.1109/TMM.2014.2328324
   Liu Y, 2016, IEEE T MULTIMEDIA, V18, P351, DOI 10.1109/TMM.2016.2514848
   Liu Y, 2013, IEEE T CIRC SYST VID, V23, P438, DOI 10.1109/TCSVT.2012.2207269
   Nesterov I. E., 1994, SIAM STUDIES APPL MA
   Pudlewski S., 2014, IEEE J-STSP, V9, P6
   Pudlewski S, 2013, IEEE T MULTIMEDIA, V15, P2072, DOI 10.1109/TMM.2013.2280245
   Pudlewski S, 2013, IEEE COMMUN SURV TUT, V15, P754, DOI 10.1109/SURV.2012.121912.00154
   Pudlewski S, 2012, IEEE T MOBILE COMPUT, V11, P1060, DOI 10.1109/TMC.2011.175
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Thirumalai V, 2013, J VIS COMMUN IMAGE R, V24, P649, DOI 10.1016/j.jvcir.2011.12.004
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Trocan M., 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P111, DOI 10.1109/MMSP.2010.5662003
   Trocan M, 2010, IEEE INT CON MULTI, P1225, DOI 10.1109/ICME.2010.5583411
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan C, 2014, ELECTRON LETT, V50, P805, DOI 10.1049/el.2014.0611
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yan CG, 2014, ELECTRON LETT, V50, P367, DOI 10.1049/el.2013.3235
   Yan CG, 2014, IEEE SIGNAL PROC LET, V21, P573, DOI 10.1109/LSP.2014.2310494
NR 39
TC 10
Z9 10
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1117
EP 1126
DI 10.1109/TMM.2017.2653770
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400001
OA Bronze
DA 2024-07-18
ER

PT J
AU Sansone, E
   Apostolidis, K
   Conci, N
   Boato, G
   Mezaris, V
   De Natale, FGB
AF Sansone, Emanuele
   Apostolidis, Konstantinos
   Conci, Nicola
   Boato, Giulia
   Mezaris, Vasileios
   De Natale, Francesco G. B.
TI Automatic Synchronization of Multi-user Photo Galleries
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Events; Markov networks; multimedia synchronization; multimodal;
   weighted graph
ID COLLECTIONS; IDENTIFICATION; SCENE
AB In this paper we address the issue of photo galleries synchronization, where pictures related to the same event are collected by different users. Existing solutions to address the problem are usually based on unrealistic assumptions, like time consistency across photo galleries, and often heavily rely on heuristics, therefore limiting the applicability to real-world scenarios. We propose a solution that achieves better generalization performance for the synchronization task compared to the available literature. The method is characterized by three stages: at first, deep convolutional neural network features are used to assess the visual similarity among the photos; then, pairs of similar photos are detected across different galleries and used to construct a graph; eventually, a probabilistic graphical model is used to estimate the temporal offset of each pair of galleries, by traversing the minimum spanning tree extracted from this graph. The experimental evaluation is conducted on four publicly available datasets covering different types of events, demonstrating the strength of our proposed method. A thorough discussion of the obtained results is provided for a critical assessment of the quality in synchronization.
C1 [Sansone, Emanuele; Conci, Nicola; Boato, Giulia; De Natale, Francesco G. B.] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
   [Apostolidis, Konstantinos; Mezaris, Vasileios] Ctr Res & Technol Hellas, Informat Technol Inst, Thermi 57001, Greece.
C3 University of Trento; Centre for Research & Technology Hellas
RP Sansone, E (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
EM e.sansone@unitn.it; kapost@iti.gr; nicola.conci@unitn.it;
   giulia.boato@unitn.it; bmezaris@iti.gr; francesco.denatale@unitn.it
RI Conci, Nicola/AAH-4671-2020
OI Conci, Nicola/0000-0002-7858-0928
FU European Union's Horizon 2020 Research and Innovation Programme
   [H2020-687786 InVID]
FX This work was supported by the European Union's Horizon 2020 Research
   and Innovation Programme under Grant H2020-687786 InVID. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Tao Mei.
CR [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   Apostolidis K., 2014, P MEDIAEVAL BARC SPA, V1263
   Apostolidis K., 2015, P MEDIAEVAL WURZ GER
   Apostolidis K., 2015, P 2 ACM INT WORKSH H, P1
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Broilo M, 2012, IEEE IMAGE PROC, P1945, DOI 10.1109/ICIP.2012.6467267
   Choi JY, 2011, IEEE T MULTIMEDIA, V13, P14, DOI 10.1109/TMM.2010.2087320
   Conci N., 2015, P MEDIAEVAL WURZ GER
   Conci N., 2014, P MEDIAEVAL BARC SPA
   Dekel T, 2013, IEEE I CONF COMP VIS, P977, DOI 10.1109/ICCV.2013.125
   Duan XH, 2013, IEEE T MULTIMEDIA, V15, P167, DOI 10.1109/TMM.2012.2225029
   Fassold H., 2015, P MEDIAEVAL WURZ GER
   Graham A., 2002, JCDL 2002. Proceedings of the Second ACM/IEEE-CS Joint Conference on Digital Libraries, P326, DOI 10.1145/544220.544301
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272
   Kherfi ML, 2007, IEEE T MULTIMEDIA, V9, P893, DOI 10.1109/TMM.2007.893349
   Kim G, 2015, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2015.7298927
   Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538
   Kim G, 2014, PROC CVPR IEEE, P3882, DOI 10.1109/CVPR.2014.496
   Kim G, 2013, PROC CVPR IEEE, P620, DOI 10.1109/CVPR.2013.86
   Kim G, 2012, PROC CVPR IEEE, P837, DOI 10.1109/CVPR.2012.6247756
   Lin L, 2015, IEEE T CIRC SYST VID, V25, P251, DOI 10.1109/TCSVT.2014.2313897
   Liu S, 2012, IEEE T MULTIMEDIA, V14, P361, DOI 10.1109/TMM.2011.2174780
   Messing DS, 2001, IEEE IMAGE PROC, P670, DOI 10.1109/ICIP.2001.959134
   Mulhem P, 2003, LECT NOTES COMPUT SC, V2728, P321
   Nowak P., 2014, P MEDIAEVAL BARC SPA, V1263
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pigeau A., 2003, P 3 INT WORKSH CONT, P111
   Rabbath M, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037684
   Rosani A, 2015, IEEE T MULTIMEDIA, V17, P1359, DOI 10.1109/TMM.2015.2441003
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P1231, DOI 10.1109/TMM.2013.2261481
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Sansone E., 2014, P MEDIAEVAL BARC SPA, V1263
   Sinha Pinaki., 2009, P 17 ACM INT C MULTI, P1131
   Song DJ, 2015, IEEE DATA COMPR CONF, P353, DOI 10.1109/DCC.2015.85
   Song DJ, 2015, IEEE I CONF COMP VIS, P1922, DOI 10.1109/ICCV.2015.223
   Suematsu N, 2012, INT C PATT RECOG, P2355
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tsai JT, 2014, IEEE T MULTIMEDIA, V16, P2229, DOI 10.1109/TMM.2014.2359769
   Tzelepis C, 2016, IMAGE VISION COMPUT, V53, P3, DOI 10.1016/j.imavis.2016.05.005
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Wang RX, 2010, IEEE T MULTIMEDIA, V12, P803, DOI 10.1109/TMM.2010.2057411
   Xie LX, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P3, DOI 10.1145/2671188.2749289
   Yang JC, 2012, IEEE T MULTIMEDIA, V14, P1642, DOI 10.1109/TMM.2012.2198458
   Yuan JS, 2010, IEEE T MULTIMEDIA, V12, P705, DOI 10.1109/TMM.2010.2051868
   Zagoruyko S., 2015, PROC CVPR IEEE, P4353, DOI DOI 10.1109/CVPR.2015.7299064
   Zaharieva M., 2014, P MEDIAEVAL OCT, V1263
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
NR 51
TC 3
Z9 3
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1285
EP 1298
DI 10.1109/TMM.2017.2655446
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400014
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, J
   Zhao, SQ
   Sheng, VS
   Zhang, J
   Ye, C
   Zhao, PP
   Cui, ZM
AF Wu, Jian
   Zhao, Shiquan
   Sheng, Victor S.
   Zhang, Jing
   Ye, Chen
   Zhao, Pengpeng
   Cui, Zhiming
TI Weak-Labeled Active Learning With Conditional Label Dependence for
   Multilabel Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multilabel active learning; weak label; image classification; label
   correlation; conditional label dependence
ID ANNOTATION; RETRIEVAL; ALGORITHM; SEGMENTATION
AB Multilabel image classification has been a hot topic in the field of computer vision and image understanding in recent years. To achieve better classification performance with fewer labeled images, multilabel active learning is used for this scenario. Several active learning methods have been proposed for multilabel image classification. However, all of them assume that either all training images have complete labels or label correlations are given at the beginning. These two assumptions are unrealistic. In fact, it is very difficult to obtain complete labels for each example, in particular when the size of labels in a multilabel dataset is very large. Typically, only partial labels are available. This is one type of "weak label" problem. To solve this weak label problem inside multilabel active learning, this paper proposes a novel solution called AE-WLMAL. AE-WLMAL explores conditional label correlations on the weak label problem with the help of input features and then utilizes label correlations to construct a unified sampling strategy and evaluate the informativeness of each example-label pair in a multilabel dataset for active sampling. In addition, a pruning strategy is adopted to further improve its computation efficiency. Moreover, AE-WLAML exploits label correlations to infer labels for unlabeled images, which further reduces human labeling cost. Our experimental results on seven real-world datasets show that AE-WLMAL consistently outperforms existing approaches.
C1 [Wu, Jian; Zhao, Shiquan; Ye, Chen; Zhao, Pengpeng; Cui, Zhiming] Soochow Univ, Inst Intelligent Informat Proc & Applicat, Suzhou 215006, Peoples R China.
   [Sheng, Victor S.] Univ Cent Arkansas, Dept Comp Sci, Conway, AR 72035 USA.
   [Zhang, Jing] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
C3 Soochow University - China; University of Central Arkansas; Nanjing
   University of Science & Technology
RP Wu, J (corresponding author), Soochow Univ, Inst Intelligent Informat Proc & Applicat, Suzhou 215006, Peoples R China.
EM jianwu@suda.edu.cn; 20134227054@stu.suda.edu.cn; ssheng@uca.edu;
   jingzhang.cs@gmail.com; 20134227041@stu.suda.edu.cn; ppzhao@suda.edu.cn;
   szzmcui@suda.edu.cn
RI Zhao, Peng/HDO-7507-2022
FU Natural Science Foundation of China [61402311, 61603186]; Jiangsu
   Province Colleges and Universities Natural Science Research Project
   [13KJB520021]; U.S. National Science Foundation [IIS-1115417]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61402311 and Grant 61603186, in part by the Jiangsu
   Province Colleges and Universities Natural Science Research Project
   under Grant 13KJB520021, and in part by the U.S. National Science
   Foundation under Grant IIS-1115417. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Winston Hsu.
CR [Anonymous], 2008, IEEE C COMP VIS PATT
   [Anonymous], 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587383
   [Anonymous], 2010, ACTIVE LEARNING LIT
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   Bang Zhang, 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P401, DOI 10.1109/WACV.2012.6163043
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chen BJ, 2015, J MATH IMAGING VIS, V51, P124, DOI 10.1007/s10851-014-0511-6
   Dembczynski K, 2012, MACH LEARN, V88, P5, DOI 10.1007/s10994-012-5285-8
   Gu B., 2016, IEEE Transactions on Neural Networks and Learning Systems, DOI DOI 10.1109/TNNLS.2016.2527796
   Gu B, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3532
   Howe J, 2006, WIRED, V14, P1, DOI DOI 10.1086/599595
   Huang S.J., 2012, 26 AAAI C ART INT, P949
   Joshi Ajay J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2372, DOI 10.1109/CVPRW.2009.5206627
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Li X., 2013, IJCAI, P1479
   Li XC, 2004, IEEE IMAGE PROC, P2207
   Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477
   Nasierding G, 2009, IEEE SYS MAN CYBERN, P4514, DOI 10.1109/ICSMC.2009.5346902
   Pang YW, 2011, IEEE IMAGE PROC, P1797, DOI 10.1109/ICIP.2011.6115811
   Picard D, 2008, IEEE T MULTIMEDIA, V10, P1356, DOI 10.1109/TMM.2008.2004913
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Settles B., 2008, P NIPS WORKSHOP COST, VVolume 1
   Sheng Victor S., 2012, 2012 45th Hawaii International Conference on System Sciences (HICSS), P1097, DOI 10.1109/HICSS.2012.552
   Spyromitros E, 2008, LECT NOTES ARTIF INT, V5138, P401, DOI 10.1007/978-3-540-87881-0_40
   Sun YY, 2010, AAAI CONF ARTIF INTE, P593
   Tang JH, 2012, IEEE T IMAGE PROCESS, V21, P2354, DOI 10.1109/TIP.2011.2180916
   Tsoumakas G., 2009, P INT WORKSH LEARN M, P101, DOI DOI 10.1109/ICCI-CC.2015.7259416
   Tsoumakas G, 2011, J MACH LEARN RES, V12, P2411
   Tuia D, 2011, IEEE J-STSP, V5, P606, DOI 10.1109/JSTSP.2011.2139193
   Vijayanarasimhan S, 2009, PROC CVPR IEEE, P2262, DOI 10.1109/CVPRW.2009.5206705
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wen XZ, 2015, INFORM SCIENCES, V295, P395, DOI 10.1016/j.ins.2014.10.040
   Wu J, 2014, IEEE IMAGE PROC, P5227, DOI 10.1109/ICIP.2014.7026058
   Xia ZH, 2016, MULTIMED TOOLS APPL, V75, P1947, DOI 10.1007/s11042-014-2381-8
   Xu X. S., 2011, P 19 INT C MULT, P1377
   Yang S.-J., 2013, Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, P1862
   Yang Y, 2012, IEEE T IMAGE PROCESS, V21, P1339, DOI 10.1109/TIP.2011.2169269
   Ye C, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P583, DOI 10.1145/2671188.2749365
   Yu G, 2012, P ACM C BIOINF COMP, P202, DOI DOI 10.1145/2382936.2382962
   Yu Y, 2014, EXPERT SYST APPL, V41, P2989, DOI 10.1016/j.eswa.2013.10.030
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhang B, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2302675
   Zhang C, 2002, IEEE T MULTIMEDIA, V4, P260, DOI 10.1109/TMM.2002.1017738
   Zhang Y, 2010, AAAI CONF ARTIF INTE, P667
   Zhao SQ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1127, DOI 10.1145/2733373.2806298
   Zheng YH, 2015, J INTELL FUZZY SYST, V28, P961, DOI 10.3233/IFS-141378
   Zhu X, 2009, Synthesis Lectures on Artificial Intelligence and Machine Learning, V3, P1, DOI 10.1007/978-3-031-01548-9
NR 47
TC 28
Z9 28
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1156
EP 1169
DI 10.1109/TMM.2017.2652065
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400004
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhang, L
   Ma, BP
   Li, GR
   Huang, QM
   Tian, Q
AF Zhang, Liang
   Ma, Bingpeng
   Li, Guorong
   Huang, Qingming
   Tian, Qi
TI Cross-Modal Retrieval Using Multiordered Discriminative Structured
   Subspace Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; documents and images; multimedia
ID FEATURES; IMAGES; SPACE; RANK
AB This paper proposes a novel method for cross-modal retrieval. In addition to the traditional vector (text)-to-vector (image) framework, we adopt a matrix (text)-to-matrix (image) framework to faithfully characterize the structures of different feature spaces. Moreover, we propose a novel metric learning framework to learn a discriminative structured subspace, in which the underlying data distribution is preserved for ensuring a desirablemetric. Concretely, there are three steps for the proposed method. First, the multiorder statistics are used to represent images and texts for enriching the feature information. We jointly use the covariance (second-order), mean (first-order), and bags of visual (textual) features (zeroth-order) to characterize each image and text. Second, considering that the heterogeneous covariance matrices lie on the different Riemannian manifolds and the other features on the different Euclidean spaces, respectively, we propose a unified metric learning framework integrating multiple distance metrics, one for each order statistical feature. This framework preserves the underlying data distribution and exploits complementary information for better matching heterogeneous data. Finally, the similarity between the different modalities can be measured by transforming the multiorder statistical features to the common subspace. The performance of the proposed method over the previous methods has been demonstrated through the experiments on two public datasets.
C1 [Zhang, Liang; Ma, Bingpeng; Li, Guorong; Huang, Qingming] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 100190, Peoples R China.
   [Zhang, Liang; Ma, Bingpeng; Li, Guorong; Huang, Qingming] Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing 100190, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; University of Texas System; University
   of Texas at San Antonio (UTSA)
RP Ma, BP (corresponding author), Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 100190, Peoples R China.; Ma, BP (corresponding author), Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing 100190, Peoples R China.
EM zhangliang14@mails.ucas.ac.cn; bpma@ucas.ac.cn; liguorong@ucas.ac.cn;
   qmhuang@ucas.ac.cn; qitian@cs.utsa.edu
RI Li, Guorong/C-3806-2015
FU National Basic Research Program of China (973 Program [2015CB351800,
   2012CB316400]; National Natural Science Foundation of China [61572465,
   61332016, 61429201, 61620106009, U1636214, 61303153]; Key Research
   Program of Frontier Sciences, Chinese Academy of Sciences
   [QYZDJ-SSW-SYS013]; ARO [W911NF-15-1-0290]; Faculty Research Gift Awards
   by the NEC Laboratories of America and Blippar
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2015CB351800 and Grant 2012CB316400,
   in part by the National Natural Science Foundation of China under Grant
   61572465, Grant 61332016, Grant 61429201, Grant 61620106009, Grant
   U1636214, and Grant 61303153, and in part by the Key Research Program of
   Frontier Sciences, Chinese Academy of Sciences under Grant
   QYZDJ-SSW-SYS013. The work of Q. Tian was supported in part by the ARO
   Grant W911NF-15-1-0290 and the Faculty Research Gift Awards by the NEC
   Laboratories of America and Blippar. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Enrico Magli. (Corresponding author: Bingpeng Ma.)
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], 2013, P 21 ACM INT C MULT
   Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996
   Bai B, 2010, INFORM RETRIEVAL, V13, P291, DOI 10.1007/s10791-009-9117-9
   Chen X, 2012, IEEE T MULTIMEDIA, V14, P3, DOI 10.1109/TMM.2011.2167223
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Herbrich R, 2000, ADV NEUR IN, P115
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huang ZW, 2014, PROC CVPR IEEE, P1677, DOI 10.1109/CVPR.2014.217
   Irie G, 2015, IEEE I CONF COMP VIS, P1886, DOI 10.1109/ICCV.2015.219
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Katsurai M, 2014, IEEE T MULTIMEDIA, V16, P1059, DOI 10.1109/TMM.2014.2306655
   Li X., 2008, PROC IEEE C COMPUT V, P1
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JW, 2013, IEEE I CONF COMP VIS, P329, DOI 10.1109/ICCV.2013.48
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Verma Y., 2014, BMVC, V1, P2
   Wang C., 2011, P 22 INT JOINT C ART, P1541
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965
   Wu F., 2013, P ACM INT C MULT, P877
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhang L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1355, DOI 10.1145/2964284.2964336
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 42
TC 52
Z9 52
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1220
EP 1233
DI 10.1109/TMM.2016.2646219
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400009
DA 2024-07-18
ER

PT J
AU Zhao, B
   Wu, X
   Feng, JS
   Peng, Q
   Yan, SC
AF Zhao, Bo
   Wu, Xiao
   Feng, Jiashi
   Peng, Qiang
   Yan, Shuicheng
TI Diversified Visual Attention Networks for Fine-Grained Object
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; fine-grained object classification;
   long-short-term-memory (LSTM); visual attention
ID SEGMENTATION
AB Fine-grained object classification attracts increasing attention in multimedia applications. However, it is a quite challenging problem due to the subtle interclass difference and large intraclass variation. Recently, visual attention models have been applied to automatically localize the discriminative regions of an image for better capturing critical difference, which have demonstrated promising performance. Unfortunately, without consideration of the diversity in attention process, most of existing attentionmodels perform poorly in classifying fine-grained objects. In this paper, we propose a diversified visual attention network (DVAN) to address the problem of fine-grained object classification, which substantially relieves the dependency on strongly supervised information for learning to localize discriminative regions compared with attention-less models. More importantly, DVAN explicitly pursues the diversity of attention and is able to gather discriminative information to the maximal extent. Multiple attention canvases are generated to extract convolutional features for attention. An LSTM recurrent unit is employed to learn the attentiveness and discrimination of attention canvases. The proposed DVAN has the ability to attend the object from coarse to fine granularity, and a dynamic internal representation for classification is built up by incrementally combining the information from different locations and scales of the image. Extensive experiments conducted on CUB-2011, Stanford Dogs, and Stanford Cars datasets have demonstrated that the proposed DVAN achieves competitive performance compared to the state-of-the-art approaches, without using any prior knowledge, user interaction, or external resource in training and testing.
C1 [Zhao, Bo; Wu, Xiao; Peng, Qiang] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Peoples R China.
   [Feng, Jiashi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
C3 Southwest Jiaotong University; National University of Singapore
RP Wu, X (corresponding author), Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Peoples R China.
EM zhaobo.cs@gmail.com; wuxiaohk@gmail.com; elefjia@nus.edu.sg;
   qpeng@home.swjtu.edu.cn; eleyans@nus.edu.sg
RI Feng, Jiashi/AGX-6209-2022; Yan, Shuicheng/HCI-1431-2022
OI Wu, Xiao/0000-0002-8322-8558
FU National Natural Science Foundation of China [61373121, 61328205];
   Program for Sichuan Provincial Science Fund for Distinguished Young
   Scholars [13QNJJ0149]; Fundamental Research Funds for the Central
   Universities; China Scholarship Council [201507000032]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61373121 and Grant 61328205, in part by
   the Program for Sichuan Provincial Science Fund for Distinguished Young
   Scholars under Grant 13QNJJ0149, in part by the Fundamental Research
   Funds for the Central Universities, and in part by the China Scholarship
   Council under Grant 201507000032. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Marco Bertini. (Corresponding author: Xiao Wu.)
CR [Anonymous], P NIPS TIM SER WORKS
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2010, ADV NEUR INF PROC SY
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2015, P INT C LEARNING REP
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, ARXIV150907481
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], 2014, CORR
   [Anonymous], P INT C LEARN REPR M
   [Anonymous], P INT C PATT REC
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Ba J., 2015, P ICLR, P1, DOI DOI 10.1016/J.JCYT.2014.02.008
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chai YN, 2011, IEEE I CONF COMP VIS, P2579, DOI 10.1109/ICCV.2011.6126546
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Gavves E, 2015, INT J COMPUT VISION, V111, P191, DOI 10.1007/s11263-014-0741-5
   Ge ZY, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301271
   Gosselin PH, 2014, PATTERN RECOGN LETT, V49, P92, DOI 10.1016/j.patrec.2014.06.011
   Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krause J, 2014, INT C PATT RECOG, P26, DOI 10.1109/ICPR.2014.15
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13
   Liu Xiao., 2016, CoRR abs/1603.06765
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Nguyen TV, 2013, IEEE T MULTIMEDIA, V15, P1910, DOI 10.1109/TMM.2013.2272919
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wah C, 2014, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2014.115
   Wah Catherine, 2011, Technical report
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Xu K., 2015, COMPUTER SCI, P2048
   Yamaguchi K, 2015, IEEE T PATTERN ANAL, V37, P1028, DOI 10.1109/TPAMI.2014.2353624
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Yang S., 2012, Advances in Neural Information Processing Systems, P3122
   Yao BP, 2012, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2012.6248088
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhu L, 2015, IEEE T MULTIMEDIA, V17, P981, DOI 10.1109/TMM.2015.2431496
NR 54
TC 298
Z9 311
U1 5
U2 87
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1245
EP 1256
DI 10.1109/TMM.2017.2648498
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400011
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Madeo, S
   Bober, M
AF Madeo, Simone
   Bober, Miroslaw
TI Fast, Compact, and Discriminative: Evaluation of Binary Descriptors for
   Mobile Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary descriptor; descriptor benchmarking; image retrieval; local
   feature descriptor
ID IMAGE RETRIEVAL; SCALE
AB Local feature descriptors underpin many diverse applications, supporting object recognition, image registration, database search, 3D reconstruction, and more. The recent phenomenal growth in mobile devices and mobile computing in general has created demand for descriptors that are not only discriminative, but also compact in size and fast to extract and match. In response, a large number of binary descriptors have been proposed, each claiming to overcome some limitations of the predecessors. This paper provides a comprehensive evaluation of several promising binary designs. We show that existing evaluation methodologies are not sufficient to fully characterize descriptors' performance and propose a new evaluation protocol and a challenging dataset. In contrast to the previous reviews, we investigate the effects of the matching criteria, operating points, and compaction methods, showing that they all have a major impact on the systems' design and performance. Finally, we provide descriptor extraction times for both general-purpose systems and mobile devices, in order to better understand the real complexity of the extraction task. The objective is to provide a comprehensive reference and a guide that will help in selection and design of the future descriptors.
C1 [Madeo, Simone] Scuola Super Sant Anna, Real Time Syst Lab, I-56127 Pisa, Italy.
   [Madeo, Simone; Bober, Miroslaw] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 Scuola Superiore Sant'Anna; University of Surrey
RP Madeo, S (corresponding author), Scuola Super Sant Anna, Real Time Syst Lab, I-56127 Pisa, Italy.
EM s.madeo@sssup.it; m.bober@surrey.ac.uk
RI Bober, Miroslaw Z/C-6205-2009
OI Madeo, Simone/0000-0002-7186-8240; Bober, Miroslaw/0000-0001-9484-9125
FU European Commission [610691]
FX This work was supported in part by the European Commission 7th Framework
   Programme under Grant Nr. 610691.
CR Agrawal M, 2008, LECT NOTES COMPUT SC, V5305, P102, DOI 10.1007/978-3-540-88693-8_8
   Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   [Anonymous], 2011, RR7656 INRIA
   [Anonymous], 2013, 18 INT C DIG SIGN PR
   [Anonymous], 2011, P 2 ANN ACM C MULTIM
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Baroffio L, 2014, IEEE IMAGE PROC, P5686, DOI 10.1109/ICIP.2014.7026150
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bekele D, 2013, IEEE IMAGE PROC, P3652, DOI 10.1109/ICIP.2013.6738753
   Bellarbi A, 2014, INT SYM MIX AUGMENT, P251, DOI 10.1109/ISMAR.2014.6948435
   Bradski G., 2008, LEARNING OPENCV
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chandrasekhar V, 2014, IEEE DATA COMPR CONF, P3, DOI 10.1109/DCC.2014.50
   Chandrasekhar V, 2012, INT J COMPUT VISION, V96, P384, DOI 10.1007/s11263-011-0453-z
   Chen L, 2012, IEEE T MULTIMEDIA, V14, P1057, DOI 10.1109/TMM.2012.2187435
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Duan LY, 2014, IEEE T MULTIMEDIA, V16, P346, DOI 10.1109/TMM.2013.2293063
   Eakins J., 1999, LIBRARY AND INFORMATION BRIEFINGS, V85, P1
   Eikvil L, 2014, INT C PATT RECOG, P154, DOI 10.1109/ICPR.2014.36
   Fan B, 2012, IEEE T PATTERN ANAL, V34, P2031, DOI 10.1109/TPAMI.2011.277
   Figat J, 2014, LECT NOTES COMPUT SC, V8671, P187, DOI 10.1007/978-3-319-11331-9_23
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   Heinly J, 2012, LECT NOTES COMPUT SC, V7573, P759, DOI 10.1007/978-3-642-33709-3_54
   Iwamoto K, 2013, IEEE IMAGE PROC, P2915, DOI 10.1109/ICIP.2013.6738600
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Ke Y, 2005, PROC CVPR IEEE, P597
   Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Levi G., 2015, ARXIV PREPRINT ARXIV
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Liu Zishun., 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P72
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Madeo S., 2016, SURREY MOBILE DATASE
   Mair E, 2010, LECT NOTES COMPUT SC, V6312, P183, DOI 10.1007/978-3-642-15552-9_14
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Miksik O, 2012, INT C PATT RECOG, P2681
   Paschalakis S., 2012, M25929 ISOIEC JTCI S
   Paschalakis S., 2013, 10 MPEG M GEN NOV
   Qi H, 2014, IEEE T MULTIMEDIA, V16, P1963, DOI 10.1109/TMM.2014.2345026
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Saha S, 2012, IEEE IMAGE PROC, P2345, DOI 10.1109/ICIP.2012.6467367
   Shao T.S. H., 2004, Zubud-zurich buildings database for image based recognition," ed
   Simonyan K, 2014, IEEE T PATTERN ANAL, V36, P1573, DOI 10.1109/TPAMI.2014.2301163
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Trzcinski T, 2013, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2013.370
   Trzcinski T, 2012, LECT NOTES COMPUT SC, V7572, P228, DOI 10.1007/978-3-642-33718-5_17
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang ZH, 2011, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2011.6126294
   Winder S, 2009, PROC CVPR IEEE, P178, DOI 10.1109/CVPRW.2009.5206839
   Wu F., 2013, P BRIT MACH VIS C, P161
   Xu XW, 2014, IEEE T IMAGE PROCESS, V23, P2983, DOI 10.1109/TIP.2014.2324824
   Yang XH, 2016, ADV SOC SCI EDUC HUM, V70, P478
   Yang X, 2012, INT SYM MIX AUGMENT, P49, DOI 10.1109/ISMAR.2012.6402537
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P3671, DOI 10.1109/TIP.2014.2330794
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
NR 64
TC 14
Z9 14
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 221
EP 235
DI 10.1109/TMM.2016.2615521
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tang, MF
   Wu, X
   Agrawal, P
   Pongpaichet, S
   Jain, R
AF Tang, Mengfan
   Wu, Xiao
   Agrawal, Pranav
   Pongpaichet, Siripen
   Jain, Ramesh
TI Integration of Diverse Data Sources for Spatial PM2.5 Data Interpolation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Diverse data; data fusion and integration; eventShop operators;
   geospaital interpolation; PM2.5
ID DATA FUSION; FRAMEWORK
AB Heterogeneous data fusion from disparate geospatial sensors has drawn increasing attention in multimedia. Unfortunately, environmental sensors are usually sparsely and preferentially located, which restricts situation recognition of geographical regions and results in uncertainty in derived inferences. Spatial interpolation is an effective way to solve the problem of data sparsity, which demands the availability of related data sources. However, these data sources are usually in different resolutions, distributions, scales, and densities, which poses a major challenge in data integration. To address this problem, we present a novel spatial interpolation framework to incorporate diverse data sources and model the spatial processes explicitly at multiple resolutions. Spectral analysis is deployed to generate features at multiple spatial resolutions and to improve the interpolation accuracy at unobserved locations. A statistical operator based on the spatial Gaussian process is implemented and integrated into a geospatial situation recognition system, which can analyze heterogeneous spatio- temporal data streams derived from sensors. To verify the effectiveness and efficiency of the proposed framework, this framework is applied to the PM2.5 air pollution application. Experiments conducted in California, USA, demonstrate that the proposed method outperforms state-of-theart approaches.
C1 [Tang, Mengfan; Agrawal, Pranav; Pongpaichet, Siripen; Jain, Ramesh] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
   [Wu, Xiao] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Peoples R China.
C3 University of California System; University of California Irvine;
   Southwest Jiaotong University
RP Tang, MF (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
EM mengfant@ics.uci.edu; wuxiaohk@home.swjtu.edu.cn; pagrawal@ics.uci.edu;
   spongpaichet@ics.uci.edu; jain@ics.uci.edu
OI Wu, Xiao/0000-0002-8322-8558
FU National Natural Science Foundation of China [61373121]; Program for
   Sichuan Provincial Science Fund for Distinguished Young Scholars
   [13QNJJ0149]; Donald Bren Foundation
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61373121, and in part by the Program for
   Sichuan Provincial Science Fund for Distinguished Young Scholars under
   Grant 13QNJJ0149. The work of R. Jain was supported by the Donald Bren
   Foundation.
CR [Anonymous], 2012, INT C DATA MINING SO
   Aune E, 2014, STAT COMPUT, V24, P247, DOI 10.1007/s11222-012-9368-y
   Banerjee S, 2008, J R STAT SOC B, V70, P825, DOI 10.1111/j.1467-9868.2008.00663.x
   Berrocal VJ, 2010, J AGR BIOL ENVIR ST, V15, P176, DOI 10.1007/s13253-009-0004-z
   Bonilla E., 2007, Proceedings of the Advances in Neural Information Processing Systems, P153
   Byun D.W., SCI ALGORITHMS EPA M
   Choo J., 2012, SDM, P177, DOI DOI 10.1137/1.9781611972825.16
   Crumeyrolle S, 2014, ATMOS CHEM PHYS, V14, P2139, DOI 10.5194/acp-14-2139-2014
   Fuentes M, 2005, BIOMETRICS, V61, P36, DOI 10.1111/j.0006-341X.2005.030821.x
   Gao MY, 2012, PROCEEDINGS OF THE 3RD ANNUAL ACM WEB SCIENCE CONFERENCE, 2012, P105
   Goovaerts P, 1999, GEODERMA, V89, P1, DOI 10.1016/S0016-7061(98)00078-0
   Gupta A., 2013, P 21 ACM INT C MULTI, P203
   Nguyen H, 2012, J AM STAT ASSOC, V107, P1004, DOI 10.1080/01621459.2012.694717
   Hall DL, 1997, P IEEE, V85, P6, DOI [10.1109/5.554205, 10.1109/ISCAS.1998.705329]
   Hengl T, 2004, GEODERMA, V120, P75, DOI 10.1016/j.geoderma.2003.08.018
   Isaaks E.H., 1989, Applied Geostatistics, 551.72 ISA
   Jain R, 2011, IT PROF, V13, P8, DOI 10.1109/MITP.2011.86
   Jiang AH, 2015, I C INTELL COMPUT TE, P722, DOI 10.1109/ICICTA.2015.183
   Kang J, 2015, IEEE GEOSCI REMOTE S, V12, P92, DOI 10.1109/LGRS.2014.2326775
   Kaufman CG, 2008, J AM STAT ASSOC, V103, P1545, DOI 10.1198/016214508000000959
   Li J, 2014, ENVIRON MODELL SOFTW, V53, P173, DOI 10.1016/j.envsoft.2013.12.008
   Mengyun Tang, 2015, 2015 IEEE 34th International Performance Computing and Communications Conference (IPCCC), P1, DOI 10.1109/PCCC.2015.7410268
   Mitas L., 1999, SPATIAL INTERPOLATIO
   Neteler M., 2007, OPEN SOURCE GIS GRAS
   Oliver MA, 2010, GEOSTATISTICAL APPLICATIONS FOR PRECISION AGRICULTURE, P1, DOI 10.1007/978-90-481-9133-8_1
   Paciorek CJ, 2012, J ROY STAT SOC C, V61, P429, DOI 10.1111/j.1467-9876.2011.01035.x
   Paladugu A, 2013, IEEE INT CON MULTI
   Perry M., 2008, THESIS
   Pongpaichet S, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P1359
   Quiñonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Razeghi G, 2016, ATMOS ENVIRON, V137, P90, DOI 10.1016/j.atmosenv.2016.04.031
   Reich BJ, 2014, BIOMETRICS, V70, P932, DOI 10.1111/biom.12196
   Rohde RA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0135749
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sheth A, 2008, IEEE INTERNET COMPUT, V12, P81, DOI 10.1109/MIC.2008.46
   Singh V.K., 2010, P INT C MULTIMEDIA, P481
   Tang HY, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY AND SECURITY (QRS 2016), P1, DOI 10.1109/QRS.2016.11
   Tang M., 2015, P ACM WEB SCI C, P30
   van Donkelaar A, 2006, J GEOPHYS RES-ATMOS, V111, DOI 10.1029/2005JD006996
   Wikle CK, 2010, CH CRC HANDB MOD STA, P107
   Xu Y, 2010, ANN APPL STAT, V4, P589, DOI 10.1214/09-AOAS293
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zhang JX, 2010, INT J IMAGE DATA FUS, V1, P5, DOI 10.1080/19479830903561035
   Zhang Y, 2013, J MATER CHEM B, V1, P132, DOI 10.1039/c2tb00071g
   Zhang ZY, 2013, IEEE T PATTERN ANAL, V35, P1717, DOI 10.1109/TPAMI.2012.274
   Zheng Y, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1436, DOI 10.1145/2487575.2488188
NR 46
TC 29
Z9 30
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 408
EP 417
DI 10.1109/TMM.2016.2613639
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800015
DA 2024-07-18
ER

PT J
AU Wu, PF
   Liu, YG
   Ye, M
   Li, J
   Du, SL
AF Wu, Pengfei
   Liu, Yiguang
   Ye, Mao
   Li, Jie
   Du, Shuangli
TI Fast and Adaptive 3D Reconstruction With Extensively High Completeness
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D reconstruction; general purpose graphics processing units (GPGPU);
   hierarchical parallel computing; multiple view stereo
ID MULTIVIEW STEREO
AB The seed-and-expand scheme is appropriate for multiple view stereo, since it can build dense point clouds adaptively by avoiding unnecessary computation. However, due to the irregularity of the algorithm, it is not suitable for parallel computing on general public utilities (GPU). This paper is the first attempt to implement the irregular seed-and-expand method on GPU for multiple view stereo problems. Meanwhile, a hierarchical parallel computing architecture is also proposed to maximize the usage of both CPU and GPU. The adaptivity of the seed-and-expand scheme is pushed further by processing a pixel several rounds while, in order to maintain regularity for GPU implementation, every seed has exactly the same behavior in a single round of optimization. The high adaptivity also improves the robustness of the proposed method, thus aggressive matching score and a view selection method can be used to improve the reconstruction completeness extensively, without smearing out local details and lowering the accuracy. Compared with the state of the art, the proposed method achieves higher accuracy and completeness on standard datasets. The proposed method is also very fast. It is maximally five times faster than other methods running on a CPU and is on par with the regular depth map-based methods on GPU, which are naturally suitable for GPU acceleration.
C1 [Wu, Pengfei; Liu, Yiguang; Li, Jie; Du, Shuangli] Sichuan Univ, Sch Comp Sci, Vis & Image Proc Lab, Chengdu 610065, Peoples R China.
   [Ye, Mao] Univ Elect Sci & Technol China, Minist Educ, Key Lab NeuroInformat, Sch Comp Sci & Engn,Ctr Robot, Chengdu 610051, Peoples R China.
C3 Sichuan University; University of Electronic Science & Technology of
   China
RP Wu, PF (corresponding author), Sichuan Univ, Sch Comp Sci, Vis & Image Proc Lab, Chengdu 610065, Peoples R China.
EM wpfnihao@gmail.com; liuyg@scu.edu.cn; yem_mei29@hotmail.com;
   ljcd86@gmail.com; duliconcolor@gmail.com
RI Ye, Mao/K-3012-2019; Liu, Yiguang/C-6404-2011
OI Ye, Mao/0000-0001-9253-1332; Ye, Mao/0000-0003-4760-8702
FU NSFC [61571313]; National Key Research and Development Program
   [2016YFB0800600]; Sichuan Province [2014HH0048];  [2015-GH02-00001-HZ]; 
   [2015-GH02-00008-HZ]
FX This work was supported by NSFC under Grant 61571313, by the National
   Key Research and Development Program under Grant 2016YFB0800600, by
   Sichuan Province under Grant 2014HH0048, and by Chengdu City under Grant
   2015-GH02-00001-HZ and Grant 2015-GH02-00008-HZ.
CR Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   [Anonymous], 2011, CASIA DATASET
   Bailer C, 2012, LECT NOTES COMPUT SC, V7574, P398, DOI 10.1007/978-3-642-33712-3_29
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   Fuhrmann S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601163
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Y, 2010, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2010.5539802
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Vu HH, 2012, IEEE T PATTERN ANAL, V34, P889, DOI 10.1109/TPAMI.2011.172
   Jancosek M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3121, DOI 10.1109/CVPR.2011.5995693
   Jung IL, 2013, IEEE T MULTIMEDIA, V15, P56, DOI 10.1109/TMM.2012.2225041
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kordelas GA, 2016, IEEE T MULTIMEDIA, V18, P155, DOI 10.1109/TMM.2015.2505905
   Maier-Hein L, 2013, MED IMAGE ANAL, V17, P974, DOI 10.1016/j.media.2013.04.003
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Park M, 2013, IEEE T MULTIMEDIA, V15, P1569, DOI 10.1109/TMM.2013.2264926
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Shen SH, 2013, IEEE T IMAGE PROCESS, V22, P1901, DOI 10.1109/TIP.2013.2237921
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Strecha C, 2008, PROC CVPR IEEE, P2838
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Wei J., 2014, Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific, P1
   Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25
   Xiao X, 2012, IEEE T MULTIMEDIA, V14, P1246, DOI 10.1109/TMM.2012.2190384
   Xue BD, 2016, J VIS COMMUN IMAGE R, V35, P15, DOI 10.1016/j.jvcir.2015.11.007
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
   Youngjung Uh, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P393, DOI 10.1109/3DV.2014.35
   Zheng EL, 2014, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2014.196
NR 35
TC 20
Z9 20
U1 2
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 266
EP 278
DI 10.1109/TMM.2016.2612761
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800004
DA 2024-07-18
ER

PT J
AU Jiang, JJ
   Chen, C
   Ma, JY
   Wang, Z
   Wang, ZY
   Hu, RM
AF Jiang, Junjun
   Chen, Chen
   Ma, Jiayi
   Wang, Zheng
   Wang, Zhongyuan
   Hu, Ruimin
TI SRLSP: A Face Image Super-Resolution Algorithm Using Smooth Regression
   With Local Structure Prior
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face image super-resolution (SR); face recognition; local structure
   prior (LSP); low-resolution (LR); smooth regression
ID SPARSE REPRESENTATION; HALLUCINATING FACES; RECOGNITION; INTERPOLATION;
   RECONSTRUCTION; REGISTRATION; DATABASE; FRAME
AB The performance of traditional face recognition systems is sharply reduced when encountered with a low-resolution (LR) probe face image. To obtain much more detailed facial features, some face super-resolution (SR) methods have been proposed in the past decade. The basic idea of a face image SR is to generate a high-resolution (HR) face image from an LR one with the help of a set of training examples. It aims at transcending the limitations of optical imaging systems. In this paper, we regard face image SR as an image interpolation problem for domain-specific images. A missing intensity interpolation method based on smooth regression with a local structure prior (LSP), named SRLSP for short, is presented. In order to interpolate the missing intensities in a target HR image, we assume that face image patches at the same position share similar local structures, and use smooth regression to learn the relationship between LR pixels and missing HR pixels of one position patch. Performance comparison with the state-of-the-art SR algorithms on two public face databases and some real-world images shows the effectiveness of the proposed method for a face image SR in general. In addition, we conduct a face recognition experiment on the extended Yale-B face database based on the super-resolved HR faces. Experimental results clearly validate the advantages of our proposed SR method over the state-of-the-art SR methods in face recognition application.
C1 [Jiang, Junjun] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Peoples R China.
   [Jiang, Junjun] China Univ Geosci, Hubei Key Lab Intelligent Geoinformat Proc, Wuhan 430074, Peoples R China.
   [Chen, Chen] Univ Cent Florida, Ctr Comp Vis Res, Orlando, FL 32816 USA.
   [Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
   [Wang, Zheng; Wang, Zhongyuan; Hu, Ruimin] Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
C3 China University of Geosciences; China University of Geosciences; State
   University System of Florida; University of Central Florida; Wuhan
   University; Wuhan University
RP Ma, JY (corresponding author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
EM junjun0595@163.com; chenchen870713@gmail.com; jyma2010@gmail.com;
   wangzwhu@whu.edu.cn; wzy_hope@163.com; hrm1964@163.com
RI , Chen_Chen/A-8825-2015; Jiang, Junjun/L-7087-2019; Wang,
   Zheng/AAQ-8628-2020; Wang, Zheng/ABC-6029-2020; Ma, Jiayi/Y-2470-2019;
   Wang, Zhongyuan/ABD-2189-2020
OI , Chen_Chen/0000-0003-3957-7061; Jiang, Junjun/0000-0002-5694-505X;
   Wang, Zheng/0000-0003-3846-9157; Wang, Zheng/0000-0003-3846-9157; Ma,
   Jiayi/0000-0003-3264-3265; 
FU National Natural Science Foundation of China [61501413, 61503288,
   61671332]; Fundamental Research Funds for the Central Universities at
   China University of Geosciences (Wuhan) [CUGL160412]; China Postdoctoral
   Science Foundation [2016T90725]; Natural Science Fund of Hubei Province
   [2015CFB406]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61501413, Grant 61503288, and Grant 61671332, by the
   Fundamental Research Funds for the Central Universities at China
   University of Geosciences (Wuhan) under Grant CUGL160412, by the China
   Postdoctoral Science Foundation under Grant 2016T90725, and by the
   Natural Science Fund of Hubei Province under Grant 2015CFB406. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Martha Larson. (Corresponding
   author: Jiayi Ma.)
CR An L, 2014, SIGNAL PROCESS, V103, P184, DOI 10.1016/j.sigpro.2013.10.004
   An L, 2012, IEEE IMAGE PROC, P2209, DOI 10.1109/ICIP.2012.6467333
   [Anonymous], 2008, 2008 IEEE C COMP VIS
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen CLP, 2015, IEEE T IMAGE PROCESS, V24, P4014, DOI 10.1109/TIP.2015.2456432
   Chen D, 2017, SOFTWARE PRACT EXPER, V47, P405, DOI 10.1002/spe.2418
   Chen D, 2015, IEEE T PARALL DISTR, V26, P847, DOI 10.1109/TPDS.2014.2311805
   Chen D, 2015, IEEE T COMPUT, V64, P707, DOI 10.1109/TC.2013.2295806
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Gao GW, 2014, NEUROCOMPUTING, V134, P92, DOI 10.1016/j.neucom.2012.12.059
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Greenspan H, 2009, COMPUT J, V52, P43, DOI 10.1093/comjnl/bxm075
   He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713
   Hu Y, 2011, IEEE T IMAGE PROCESS, V20, P433, DOI 10.1109/TIP.2010.2063437
   Huang H, 2011, IEEE T CIRC SYST VID, V21, P1363, DOI 10.1109/TCSVT.2011.2163461
   Huang H, 2010, PATTERN RECOGN, V43, P2532, DOI 10.1016/j.patcog.2010.02.007
   Jian M, 2015, IEEE T CIRC SYST VID, V25, P1761, DOI 10.1109/TCSVT.2015.2400772
   Jiang JJ, 2017, IEEE T CYBERNETICS, V47, P3991, DOI 10.1109/TCYB.2016.2594184
   Jiang JJ, 2016, IEEE T CIRC SYST VID, V26, P1674, DOI 10.1109/TCSVT.2015.2433538
   Jiang JJ, 2016, INFORM SCIENCES, V367, P354, DOI 10.1016/j.ins.2016.05.032
   Jiang JJ, 2016, SIGNAL PROCESS, V124, P162, DOI 10.1016/j.sigpro.2015.09.026
   Jiang JJ, 2014, IEEE T IMAGE PROCESS, V23, P4220, DOI 10.1109/TIP.2014.2347201
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Jung CK, 2011, IEEE SIGNAL PROC LET, V18, P367, DOI 10.1109/LSP.2011.2140370
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Lan C, 2010, P INT C MULT, P883
   Li B, 2010, IEEE SIGNAL PROC LET, V17, P20, DOI 10.1109/LSP.2009.2031705
   Li K, 2016, PATTERN RECOGN, V51, P59, DOI 10.1016/j.patcog.2015.08.008
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Liu C, 2001, PROC CVPR IEEE, P192
   Liu LC, 2017, IEEE T CYBERNETICS, V47, P600, DOI 10.1109/TCYB.2016.2521428
   Lu H., 2016, REMOTE SENS, V8
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma JY, 2016, IEEE T IMAGE PROCESS, V25, P53, DOI 10.1109/TIP.2015.2467217
   Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954
   Ma JY, 2013, PATTERN RECOGN, V46, P3519, DOI 10.1016/j.patcog.2013.05.017
   Ma X, 2015, IEEE T HUM-MACH SYST, V45, P238, DOI 10.1109/THMS.2014.2375329
   Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019
   Park JS, 2008, IEEE T IMAGE PROCESS, V17, P1806, DOI 10.1109/TIP.2008.2001394
   Park SW, 2007, INT CONF ACOUST SPEE, P573
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Su K., 2005, 2005 IEEE International Conference on Multimedia and Expo
   Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tu CT, 2015, IMAGE VISION COMPUT, V44, P59, DOI 10.1016/j.imavis.2015.10.001
   Wang LZ, 2015, IEEE GEOSCI REMOTE S, V12, P736, DOI 10.1109/LGRS.2014.2360457
   Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Wang ZY, 2014, IEEE T CIRC SYST VID, V24, P802, DOI 10.1109/TCSVT.2013.2290574
   Wei CP, 2015, IEEE T IMAGE PROCESS, V24, P1722, DOI 10.1109/TIP.2015.2409738
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xing R, 2014, INT C INTEL HUM MACH, P67, DOI 10.1109/IHMSC.2014.119
   Xu X, 2013, 2013 3RD AUSTRALIAN CONTROL CONFERENCE (AUCC), P93, DOI 10.1109/AUCC.2013.6697254
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2015, INT CONF BIOMETR, P237, DOI 10.1109/ICB.2015.7139090
   Yao Y, 2008, COMPUT VIS IMAGE UND, V111, P111, DOI 10.1016/j.cviu.2007.09.004
   Ye SH, 2015, INT CONF ACOUST SPEE, P1196, DOI 10.1109/ICASSP.2015.7178159
   Zhang J, 2008, IEEE INT CON AUTO SC, P1, DOI 10.1109/COASE.2008.4626431
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang W, 2011, IEEE T IMAGE PROCESS, V20, P2769, DOI 10.1109/TIP.2011.2142001
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang YQ, 2015, IEEE T IMAGE PROCESS, V24, P2797, DOI 10.1109/TIP.2015.2431435
   Zhu YM, 2014, SIGNAL PROCESS-IMAGE, V29, P875, DOI 10.1016/j.image.2014.06.005
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
   Zhuang YT, 2007, PATTERN RECOGN, V40, P3178, DOI 10.1016/j.patcog.2007.03.011
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 73
TC 133
Z9 138
U1 1
U2 103
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 27
EP 40
DI 10.1109/TMM.2016.2601020
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200003
DA 2024-07-18
ER

PT J
AU Lentisco, CM
   Bellido, L
   Pastor, E
AF Lentisco, Carlos M.
   Bellido, Luis
   Pastor, Encarna
TI Reducing Latency for Multimedia Broadcast Services Over Mobile Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Digital multimedia broadcasting; mobile communication; multicast
   communication; buffer storage; adaptive streaming; multimedia content
   delivery
ID VIDEO; DELAY
AB Multimedia services over mobile networks pose several challenges, such as the efficient management of radio resources or the latency induced by network delays and buffering requirements on the multimedia players. In Long Term Evolution (LTE) networks, the definition of multimedia broadcast services over a common radio channel addresses the shortage of radio resources but introduces the problem of network error recovery. In order to address network errors on LTE multimedia broadcast services, the current standards propose the combined use of forward error correction and unicast recovery techniques at the application level. This paper shows how to efficiently synchronize the broadcasting server and the multimedia players and how to reduce service latency by limiting the multimedia player buffer length. This is accomplished by analyzing the relation between the different parameters of the LTE multimedia broadcast service, the multimedia player buffer length, and service interruptions. A case study is simulated to confirm how the quality of the multimedia service is improved by applying our proposals.
C1 [Lentisco, Carlos M.; Bellido, Luis; Pastor, Encarna] Univ Politecn Madrid, Dept Telemat Engn, E-28040 Madrid, Spain.
C3 Universidad Politecnica de Madrid
RP Lentisco, CM (corresponding author), Univ Politecn Madrid, Dept Telemat Engn, E-28040 Madrid, Spain.
EM clentisco@dit.upm.es; lbellido@dit.upm.es; epastor@dit.upm.es
RI Pastor, Encarna/A-6449-2014; Bellido Triana, Luis/GZH-2187-2022
OI Pastor, Encarna/0000-0003-1840-5471; BELLIDO TRIANA,
   LUIS/0000-0001-9591-0928
FU Spanish Ministry of Economy and Competitiveness [TEC2015-67834-R]
FX This work was supported in part by the Spanish Ministry of Economy and
   Competitiveness in the context of the Project GREDOS Reference
   TEC2015-67834-R (MINECO/FEDER, UE). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Shiwen Mao.
CR [Anonymous], 2010, J340 ITUT
   [Anonymous], 2008, HDB MOBILE BROADCAST, P239
   [Anonymous], P 14 INT S WIR PERS, DOI 10.1371/journal.pone.0016254.PMID
   [Anonymous], 5053 IETF RFC
   [Anonymous], 2014, 230091 ISOIEC
   Anttonen A, 2014, IEEE T MULTIMEDIA, V16, P1176, DOI 10.1109/TMM.2014.2306656
   Bing B., 2015, NEXT GENERATION VIDE, P245
   Bobarshad H, 2012, IEEE T MULTIMEDIA, V14, P401, DOI 10.1109/TMM.2011.2173477
   de Fez I, 2014, IEEE T MULTIMEDIA, V16, P1140, DOI 10.1109/TMM.2014.2307155
   de Fez I, 2012, IEEE T MULTIMEDIA, V14, P641, DOI 10.1109/TMM.2012.2190392
   Garcia MN, 2014, T-LAB SER TELECOMMUN, P129, DOI 10.1007/978-3-319-04855-0_5
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Kumar U., 2013, P ICNC, P555
   Lecompte D, 2012, IEEE COMMUN MAG, V50, P68, DOI 10.1109/MCOM.2012.6353684
   Lentisco C. M., 2014, P IEEE INT C WIR MOB, P691
   Lohmar T., 2011, P IEEE INT S WORLD W, P1
   Luby M., 2007, 5052 IETF RFC
   Monserrat JF, 2012, IEEE T BROADCAST, V58, P157, DOI 10.1109/TBC.2012.2191030
   Multimedia Broadcast/ Multicast Service (MBMS), 2016, 26346 3GPP TS
   Ramos FMV, 2013, IEEE COMMUN MAG, V51, P128, DOI 10.1109/MCOM.2013.6576350
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Tan E, 2012, IEEE T MULTIMEDIA, V14, P910, DOI 10.1109/TMM.2011.2180706
   Truong T., 2012, IEEE T CONSUM ELECTR, V58, P78
   Walsh R., 2012, 6726 IETF RFC
   Wei Sheng., 2014, NETWORK OPERATING SY, P37
   Yang C, 2015, IEEE T MULTIMEDIA, V17, P1096, DOI 10.1109/TMM.2015.2429552
NR 26
TC 14
Z9 15
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 173
EP 182
DI 10.1109/TMM.2016.2620605
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, Y
   Liu, PY
   Wu, YY
   Jia, KB
AF Gao, Yuan
   Liu, Pengyu
   Wu, Yueying
   Jia, Kebin
TI Quadtree Degeneration for HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Group of pictures (GOP); high efficiency video coding (HEVC); quadtree;
   quantization parameter (QP); scene content
ID VIDEO CODING HEVC; CU SIZE DECISION; MODE DECISION; EFFICIENCY;
   ALGORITHM
AB The quadtree is one of the most advanced techniques contributing to the excellent compression performance of high efficiency video coding (HEVC). However, the computational complexity increases because the quadtree examines all coding unit (CU) sizes to obtain the optimal CU partitioning. This paper focuses on quadtree degeneration based on a proposed quadtree probability mechanism. Two techniques, a quadtree probability model (QPM) procedure and a quadtree probability update (QPU) procedure, are proposed. The QPM process estimates a CU distribution model based on a quantization parameter (QP) and a group of pictures (GOP). Based on the model, a new quadtree is constructed by skipping low probability tree nodes. The QPU process is performed to update the new quadtree based on scene content change. Update addresses model distortion and ensures the accuracy of the new quadtree. Experimental results demonstrate that the proposed quadtree probability mechanism for quadtree degeneration considerably reduces average encoding time (27.55%) for the low delay condition. Applied to lossless coding, the proposed mechanism achieves a significant 43.10% encoding time reduction. The experiments also show that the proposed quadtree probability mechanism improves HEVC coding efficiency for a variety of applications and sequence characteristics.
C1 [Gao, Yuan; Liu, Pengyu; Wu, Yueying; Jia, Kebin] Beijing Univ Technol, Beijing Adv Innovat Ctr Future Internet Technol, Beijing 100124, Peoples R China.
   [Gao, Yuan; Liu, Pengyu; Wu, Yueying; Jia, Kebin] Beijing Lab Adv Informat Networks, Beijing 100124, Peoples R China.
   [Gao, Yuan; Liu, Pengyu; Wu, Yueying; Jia, Kebin] Beijing Univ Technol, Coll Elect Informat & Control Engn, Beijing 100124, Peoples R China.
C3 Beijing University of Technology; Beijing University of Technology
RP Gao, Y (corresponding author), Beijing Univ Technol, Beijing Adv Innovat Ctr Future Internet Technol, Beijing 100124, Peoples R China.; Gao, Y (corresponding author), Beijing Lab Adv Informat Networks, Beijing 100124, Peoples R China.
EM yuangaoyg001@emails.bjut.edu.cn; liupengyu@bjut.edu.cn;
   wuyueying@emails.bjut.edu.cn; kebinj@bjut.edu.cn
FU National Natural Science Foundation of China [61672064]; Project for the
   Key Project of Beijing Municipal Education Commission [KZ201610005007];
   Beijing Postdoctoral Research Foundation [2015ZZ-23]; China Postdoctoral
   Research Foundation [2015M580029, 2016T90022]; Computational
   Intelligence and Intelligent System of Beijing Key Laboratory Research
   Foundation [002000546615004]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672064, in part by the Project for the
   Key Project of Beijing Municipal Education Commission under Grant
   KZ201610005007, in part by the Beijing Postdoctoral Research Foundation
   under Grant 2015ZZ-23, in part by the China Postdoctoral Research
   Foundation under Grant 2015M580029 and Grant 2016T90022, and in part by
   the Computational Intelligence and Intelligent System of Beijing Key
   Laboratory Research Foundation under Grant 002000546615004. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Zhu Li.
CR Ahn S, 2015, IEEE T CIRC SYST VID, V25, P422, DOI 10.1109/TCSVT.2014.2360031
   Ahn YJ, 2016, J REAL-TIME IMAGE PR, V12, P419, DOI 10.1007/s11554-015-0487-5
   [Anonymous], 2001, VID COD EXP GROUP M
   [Anonymous], LECT NOTES ELECT ENG
   [Anonymous], 2014, 2014 IEEE INT C MULT, DOI DOI 10.1109/ICMEW.2014.6890543
   [Anonymous], 2013, JCTVCO0269 ISOIEC IT
   Cen YF, 2015, INFORM PROCESS LETT, V115, P719, DOI 10.1016/j.ipl.2015.04.001
   Cho S, 2013, IEEE T CIRC SYST VID, V23, P1555, DOI 10.1109/TCSVT.2013.2249017
   Guo LL, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.2.023036
   Huade S., 2014, P INT C AUD LANG IM, P143
   Jiménez-Moreno A, 2013, IEEE T MULTIMEDIA, V15, P1094, DOI 10.1109/TMM.2013.2241414
   Kim HS, 2016, IEEE T CIRC SYST VID, V26, P130, DOI 10.1109/TCSVT.2015.2444672
   Lee J, 2015, IEEE T CIRC SYST VID, V25, P411, DOI 10.1109/TCSVT.2014.2339612
   Li HL, 2009, IEEE T MULTIMEDIA, V11, P77, DOI 10.1109/TMM.2008.2008922
   Ma SW, 2014, IEEE T MULTIMEDIA, V16, P266, DOI 10.1109/TMM.2013.2284751
   Ma Z, 2011, IEEE T MULTIMEDIA, V13, P1240, DOI 10.1109/TMM.2011.2165056
   Marquardt D., 2013, P PICT COD S, P394
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Shen LQ, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700298
   Shen LQ, 2013, IEEE T CONSUM ELECTR, V59, P207, DOI 10.1109/TCE.2013.6490261
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Song YX, 2015, J VIS COMMUN IMAGE R, V33, P60, DOI 10.1016/j.jvcir.2015.07.001
   Sullivan G. J., 2010, SPIE APPL DIG IM PRO
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Won K, 2015, IEEE T BROADCAST, V61, P425, DOI 10.1109/TBC.2015.2432451
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Ye F., 2014, Proc. IEEE International Test Conference (ITC), P1
   Yu H., 2014, 18 JOINT COLL TEAM V
   Zhang MM, 2015, IEICE T INF SYST, VE98D, P467, DOI 10.1587/transinf.2014EDL8081
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
NR 31
TC 17
Z9 18
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2321
EP 2330
DI 10.1109/TMM.2016.2598481
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200001
DA 2024-07-18
ER

PT J
AU Renna, F
   Doyle, J
   Giotsas, V
   Andreopoulos, Y
AF Renna, Francesco
   Doyle, Joseph
   Giotsas, Vasileios
   Andreopoulos, Yiannis
TI Media Query Processing for the Internet-of-Things: Coupling of Device
   Energy Consumption and Cloud Infrastructure Billing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Analytic modeling; cloud computing; Internet-of-Things; visual search
ID WIRELESS SENSOR NETWORKS; INCREMENTAL REFINEMENT; VIDEO; COMPUTATION;
   DESIGN
AB Audio/visual recognition and retrieval applications have recently garnered significant attention within Internet-of-Things-oriented services, given that video cameras and audio processing chipsets are now ubiquitous even in low-end embedded systems. In the most typical scenario for such services, each device extracts audio/visual features and compacts them into feature descriptors, which comprise media queries. These queries are uploaded to a remote cloud computing service that performs content matching for classification or retrieval applications. Two of the most crucial aspects for such services are: 1) controlling the device energy consumption when using the service, and 2) reducing the billing cost incurred from the cloud infrastructure provider. In this paper, we derive analytic conditions for the optimal coupling between the device energy consumption and the incurred cloud infrastructure billing. Our framework encapsulates: the energy consumption to produce and transmit audio/visual queries, the billing rates of the cloud infrastructure, the number of devices concurrently connected to the same cloud server, the query volume constraint of each cluster of devices, and the statistics of the query data production volume per device. Our analytic results are validated via a deployment with: 1) the device side comprising compact image descriptors (queries) computed on Beaglebone Linux embedded platforms and transmitted to Amazon Web Services (AWS) Simple Storage Service, and 2) the cloud side carrying out image similarity detection via AWS Elastic Compute Cloud (EC2) instances, with the AWS Auto Scaling being used to control the number of instances according to the demand.
C1 [Renna, Francesco] Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge CB3 0WA, England.
   [Doyle, Joseph; Andreopoulos, Yiannis] UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
   [Giotsas, Vasileios] Dithen Ltd, London NW11 8NA, England.
C3 University of Cambridge; University of London; University College London
RP Renna, F (corresponding author), Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge CB3 0WA, England.
EM fr330@cam.ac.uk; j.doyle@ucl.ac.uk; v.giotsas@dithen.co.uk;
   i.andreopoulos@ucl.ac.uk
RI Renna, Francesco/AAE-4116-2019; Vergados, Dimitrios D./K-9402-2019
OI Renna, Francesco/0000-0002-8243-8350; Vergados, Dimitrios
   D./0000-0002-9436-0108; Andreopoulos, Ioannis/0000-0002-2714-4800;
   Doyle, Joseph/0000-0003-1840-9616; Giotsas,
   Vasileios/0000-0002-5277-6498
FU European Union [655282]; EPSRC [EP/M00113X/1, EP/K033166/1]; Innovate
   U.K. (project ACAME) [131983]; Marie Curie Actions (MSCA) [655282]
   Funding Source: Marie Curie Actions (MSCA); EPSRC [EP/K033166/1,
   EP/M00113X/1] Funding Source: UKRI
FX This work was supported in part by the European Union (Horizon 2020
   research and innovation programme under the Marie Sklodowska-Curie Grant
   655282 -F. Renna), in part by EPSRC under Grant EP/M00113X/1 and Grant
   EP/K033166/1, and in part by Innovate U.K. (project ACAME under Grant
   131983). The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Liang Zhou.
CR Alippi C, 2009, IEEE INSTRU MEAS MAG, V12, P16, DOI 10.1109/MIM.2009.4811133
   Anastasia D, 2012, IEEE T SIGNAL PROCES, V60, P2024, DOI 10.1109/TSP.2011.2176337
   Andreopoulos Y, 2008, IEEE T IMAGE PROCESS, V17, P1685, DOI 10.1109/TIP.2008.2001051
   Andreopoulos Y, 2008, IEEE T SIGNAL PROCES, V56, P140, DOI 10.1109/TSP.2007.906727
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2010, INTERNET MEASUREMENT, DOI DOI 10.1145/1879141.1879143
   [Anonymous], AWS SYSTEM ADM BEST
   [Anonymous], P 11 ACM C EMB NETW
   [Anonymous], ACOUST SPEECH SIG PR
   [Anonymous], 2010, INTERNAT J ADV NETWO
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Becker B. C., 2008, PROC 8 IEEE INT C AU, P1
   Benini L, 2000, IEEE T VLSI SYST, V8, P299, DOI 10.1109/92.845896
   Besbes H, 2013, IEEE T WIREL COMMUN, V12, P4916, DOI 10.1109/TWC.2013.092013.121649
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bredin H, 2006, INT CONF ACOUST SPEE, P621
   Buranapanichkit D, 2012, IEEE WIREL COMMUN LE, V1, P440, DOI 10.1109/WCL.2012.062512.120245
   Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750
   Dai M, 2009, IEEE T MULTIMEDIA, V11, P1010, DOI 10.1109/TMM.2009.2021802
   Foo B, 2008, IEEE T SIGNAL PROCES, V56, P797, DOI 10.1109/TSP.2007.906685
   GILKS WR, 1992, J R STAT SOC C-APPL, V41, P337
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Gubbi J, 2013, FUTURE GENER COMP SY, V29, P1645, DOI 10.1016/j.future.2013.01.010
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Kansal A, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1274858.1274870
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   Kulkarni P., 2005, 13th Annual ACM International Conference on Multimedia, P229, DOI 10.1145/1101149.1101191
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   Lan TH, 2003, IEEE T MULTIMEDIA, V5, P267, DOI 10.1109/TMM.2003.812714
   Leung VCM, 2013, IEEE NETWORK, V27, P4
   Li Q, 2007, IEEE T VEH TECHNOL, V56, P3533, DOI 10.1109/TVT.2007.901927
   Ma XQ, 2013, IEEE NETWORK, V27, P28, DOI 10.1109/MNET.2013.6616112
   Marcel S., 2010, IEEE C COMP VIS PATT
   Park K, 1996, 1996 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P171, DOI 10.1109/ICNP.1996.564935
   PAXSON V, 1995, IEEE ACM T NETWORK, V3, P226, DOI 10.1109/90.392383
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Redondi A, 2012, IEEE INT WORKSH MULT, P124, DOI 10.1109/MMSP.2012.6343427
   Redondi A, 2014, IEEE T CIRC SYST VID, V24, P2117, DOI 10.1109/TCSVT.2014.2329378
   Redondi A, 2013, AD HOC NETW, V11, P39, DOI 10.1016/j.adhoc.2012.04.006
   Ren SL, 2014, IEEE T SIGNAL PROCES, V62, P5439, DOI 10.1109/TSP.2014.2347260
   Rowe A, 2007, RTSS 2007: 28TH IEEE INTERNATIONAL REAL-TIME SYSTEMS SYMPOSIUM, PROCEEDINGS, P459, DOI 10.1109/RTSS.2007.50
   Sellahewa H, 2005, PROC SPIE, V5779, P173, DOI 10.1117/12.603483
   Serrà J, 2010, STUD COMPUT INTELL, V274, P307
   Shih CF, 2015, IEEE ICC, P6494, DOI 10.1109/ICC.2015.7249359
   Siewiorek D, 2012, IEEE SPECTRUM, V49, P54, DOI 10.1109/MSPEC.2012.6281134
   Soyata T, 2012, 2012 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS (ISCC), P59, DOI 10.1109/ISCC.2012.6249269
   Tagliasacchi M, 2006, IEEE IMAGE PROC, P593, DOI 10.1109/ICIP.2006.312405
   Tutuncuoglu K, 2012, IEEE T WIREL COMMUN, V11, P1180, DOI 10.1109/TWC.2012.012412.110805
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Werner-Allen G, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P381
   Winitzki S, 2003, LECT NOTES COMPUT SC, V2667, P780
   Yang J, 2012, IEEE T COMMUN, V60, P220, DOI 10.1109/TCOMM.2011.112811.100349
   Zhang WW, 2013, IEEE NETWORK, V27, P34, DOI 10.1109/MNET.2013.6616113
NR 53
TC 9
Z9 9
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2537
EP 2552
DI 10.1109/TMM.2016.2600438
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200019
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Silva, AF
   Farias, MCQ
   Redi, JA
AF Silva, Alexandre F.
   Farias, Mylene C. Q.
   Redi, Judith A.
TI Perceptual Annoyance Models for Videos With Combinations of Spatial and
   Temporal Artifacts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compression artifacts; human visual system modeling; subjective quality;
   video quality assessment
ID QUALITY ASSESSMENT; VISIBILITY
AB Understanding the perceptual impact of compression artifacts in video is one of the keys for designing better coding schemes and appropriate visual quality control chains. Although compression and transmission artifacts, such as blockiness, blurriness, and packet-loss, appear simultaneously in digital videos, traditionally they have been studied in isolation. In this paper, we report the results of three subjective quality assessment experiments aimed at studying perceptual characteristics of a set of artifacts common in digital videos. With this goal, first, we study the annoyance of each of three artifacts (blockiness, blurriness, and packet-loss) in isolation and then in combination. Based on the subjective evaluations, we design several models of the annoyance caused by the joint presence of these three artifacts on digital video.
C1 [Silva, Alexandre F.; Farias, Mylene C. Q.] Univ Brasilia, BR-70910900 Brasilia, DF, Brazil.
   [Redi, Judith A.] Delft Univ Technol, NL-2628 CD Delft, Netherlands.
C3 Universidade de Brasilia; Delft University of Technology
RP Silva, AF (corresponding author), Univ Brasilia, BR-70910900 Brasilia, DF, Brazil.
EM alexandrefieno@gmail.com; mylene@ieee.org; J.A.Redi@tudelft.nl
RI Silva, Alexandre Fieno/M-2364-2019; Farias, Mylene/C-4900-2015
OI Silva, Alexandre Fieno/0000-0003-1431-9525; Farias,
   Mylene/0000-0002-1957-9943
FU CAPES-Brazil and by Nuffic-Netherlands; NWO Veni [639.021.230]
FX This work was supported by CAPES-Brazil and by Nuffic-Netherlands. This
   work was supported in part by the NWO Veni under Grant 639.021.230. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xiaokang Yang.
CR Akaike H, 1973, 2 INT S INF THEOR, P267, DOI [DOI 10.1007/978-1-4612-1694-0_15, 10.1007/978-1-4612-0919-5_38, DOI 10.1007/978-1-4612-0919-5_38]
   [Anonymous], 2004, THESIS
   [Anonymous], FIN REP VID QUAL EXP
   [Anonymous], 2009, ENCY DATABASE SYSTEM, DOI DOI 10.1007/978-0-387-39940-9_565
   Babu R. V., 2004, P ENC ROTT NETH, P1
   Boulos F., 2009, P 4 INT WORKSH VID P, P1
   Brunnstrom K., 2012, Tech. Rep. Version 1.2, V3, P1
   Cermak GW, 1998, SMPTE J, V107, P226, DOI 10.5594/J06401
   Chandler DM, 2006, PROC SPIE, V6057, DOI 10.1117/12.655442
   de Ridder H, 2001, J ELECTRON IMAGING, V10, P47, DOI 10.1117/1.1335529
   DERIDDER H, 1992, P SOC PHOTO-OPT INS, V1666, P16, DOI 10.1117/12.135953
   Farias MCQ, 2005, IEEE IMAGE PROC, P3593
   Farias MCQ, 2007, IEEE T SIGNAL PROCES, V55, P2954, DOI 10.1109/TSP.2007.893963
   Farias MCQ, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.4.043013
   Gastaldo P, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-54
   Huynh-Thu Q, 2010, SIGNAL PROCESS-IMAGE, V25, P535, DOI 10.1016/j.image.2010.03.006
   Kayargadde V, 1996, J OPT SOC AM A, V13, P1178, DOI 10.1364/JOSAA.13.001178
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Libert JM, 1999, PROC SPIE, V3845, P254, DOI 10.1117/12.371209
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Moore MS, 2004, SIGNAL PROCESS-IMAGE, V19, P185, DOI 10.1016/j.image.2003.10.004
   Moorthy AK, 2011, MULTIMED TOOLS APPL, V51, P675, DOI 10.1007/s11042-010-0640-x
   Naccari M, 2009, IEEE T MULTIMEDIA, V11, P932, DOI 10.1109/TMM.2009.2021785
   Papadogiannakis A., 2006, P COREGRID INT WORKS
   Pinson M, 2003, P SOC PHOTO-OPT INS, V5150, P583, DOI 10.1117/12.509909
   Redi J. A., 2013, P SOC PHOTO-OPT INS, V8651
   Redi J, 2013, IEEE INT SYMP CIRC S, P1107, DOI 10.1109/ISCAS.2013.6572044
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   Vakili A, 2011, PROCEEDINGS OF ICNS 2011: THE SEVENTH INTERNATIONAL CONFERENCE ON NETWORKING AND SERVICES, P44
   Voran S. D., 2002, TECH REP
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Winkler S, 2006, IEEE T MULTIMEDIA, V8, P973, DOI 10.1109/TMM.2006.879871
   You JY, 2011, IEEE T MULTIMEDIA, V13, P1269, DOI 10.1109/TMM.2011.2172591
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
NR 34
TC 9
Z9 9
U1 2
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2446
EP 2456
DI 10.1109/TMM.2016.2601027
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200011
DA 2024-07-18
ER

PT J
AU Baytas, IM
   Lin, KX
   Wang, F
   Jain, AK
   Zhou, JY
AF Baytas, Inci M.
   Lin, Kaixiang
   Wang, Fei
   Jain, Anil K.
   Zhou, Jiayu
TI PHENOTREE: Interactive Visual Analytics for Hierarchical Phenotyping
   From Large-Scale Electronic Health Records
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE driven phenotyping; electronic health records (EHRs); hierarchical
   phenotyping; interactive visual analytics; sparse principal component
   analysis (SPCA)
AB Electronic health records (EHRs) capture comprehensive patient information in digital form from a variety of sources. Increasing availability of EHRs has facilitated development of data and visual analytic tools for healthcare analytics, such as clinical decision support and patient care management systems. Many healthcare analytic tools are used to investigate fundamental problems, such as study of patient population, exploring complicated interactions among patients and their medical histories, and extracting structured phenotypes characterizing the patient population. In this paper, we propose PHENOTREE, a novel data-driven, hierarchical, and interactive phenotyping tool, that enables physicians and medical researchers to participate in the phenotyping process of large-scale EHR cohorts. The proposed visual analytic tool allows users to interactively explore EHR cohorts, and generate, interpret, evaluate, and refine phenotypes by building and navigating a phenotype hierarchy. Specifically, given a cohort or subcohort, PHENOTREE employs sparse principal component analysis (SPCA) to identify key clinical features that characterize the population. The clinical features provide a natural way to generate deeper phenotypes at finer granularities by expanding the phenotype hierarchy. To facilitate the intensive computation required for interactive analytics, we design an efficient SPCA solver based on a variance reduced stochastic gradient technique. The benefits of our method are demonstrated by analyzing two different EHR patient cohorts, a public and a private dataset containing EHRs of 101 767 and 223 076 patients, respectively. Our evaluations show that PHENOTREE can detect clinically meaningful hierarchical phenotypes.
C1 [Baytas, Inci M.; Lin, Kaixiang; Jain, Anil K.; Zhou, Jiayu] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
   [Wang, Fei] Cornell Univ, Dept Healthcare Policy & Res, Div Hlth Informat, Ithaca, NY 14850 USA.
C3 Michigan State University; Cornell University
RP Baytas, IM (corresponding author), Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
EM baytasin@msu.edu; linkaixi@msu.edu; fei_wang@engr.uconn.edu;
   jain@cse.msu.edu; jiayuz@msu.edu
RI Baytas, Inci M./AAW-7592-2021; Zhou, Jiayu/M-1559-2019; Baytas, Inci
   M./AAU-3840-2020; zhou, jiayu/JCO-9937-2023
OI Baytas, Inci M./0000-0003-4765-2615; Baytas, Inci
   M./0000-0003-4765-2615; 
FU National Science Foundation [IIS-1565596, III-1615597, IIS-1650723];
   Office of Naval Research [N00014-14-1-0631]; Direct For Computer & Info
   Scie & Enginr; Div Of Information & Intelligent Systems [1565596]
   Funding Source: National Science Foundation
FX This work was supported in part by the National Science Foundation under
   Grant IIS-1565596 and Grant III-1615597, and in part by the Office of
   Naval Research under Grant N00014-14-1-0631. The work of F. Wang was
   supported in part by the National Science Foundation under Grant
   IIS-1650723. The guest editor coordinating the review of this manuscript
   and approving it for publication was Prof. Yingcai Wu.
CR [Anonymous], P ADV NEUR INF PROC
   [Anonymous], RADIAL REINGOLDTILFO
   [Anonymous], CORR
   Baytas IM, 2016, EURASIP J BIOINFORM, DOI 10.1186/s13637-016-0045-x
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boyd Stephen, 2003, lecture notes of EE392o, V2004, P2004
   Che ZP, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P507, DOI 10.1145/2783258.2783365
   Chun-Fu Wang, 2014, 2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P521, DOI 10.1109/BIBM.2014.6999214
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Gotz D, 2014, J BIOMED INFORM, V48, P148, DOI 10.1016/j.jbi.2014.01.007
   Gu Quanquan, 2014, Adv Neural Inf Process Syst, V2014, P1529
   Häyrinen K, 2008, INT J MED INFORM, V77, P291, DOI 10.1016/j.ijmedinf.2007.09.001
   Hein M., 2010, Adv. Neural Inf. Process., P847
   Ho JC, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P115, DOI 10.1145/2623330.2623658
   Huang CW, 2015, BMC MED INFORM DECIS, V15, DOI 10.1186/s12911-015-0218-7
   Johnson R., 2013, Adv Neural Inf Process Syst, V26, P315
   Journée M, 2010, J MACH LEARN RES, V11, P517
   Lussier Yves A, 2007, Proc Am Thorac Soc, V4, P18, DOI 10.1513/pats.200607-142JG
   Marlin BM, 2012, P 2 ACM SIGHIT INT H, P389
   Naikal N, 2011, IEEE I CONF COMP VIS, P818, DOI 10.1109/ICCV.2011.6126321
   Nitanda A, 2014, ADV NEUR IN, V27
   Perer A, 2015, J BIOMED INFORM, V56, P369, DOI 10.1016/j.jbi.2015.06.020
   Shamir O, 2015, PR MACH LEARN RES, V37, P144
   Strack B, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/781670
   Tracy RP, 2008, CURR OPIN LIPIDOL, V19, P151, DOI 10.1097/MOL.0b013e3282f73893
   Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892
   Xiang T, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002841
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhou JY, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P135, DOI 10.1145/2623330.2623711
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 30
TC 11
Z9 18
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2257
EP 2270
DI 10.1109/TMM.2016.2614225
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900013
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, M
   Leung, H
AF Li, Meng
   Leung, Howard
TI Multiview Skeletal Interaction Recognition Using Active Joint
   Interaction Graph
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Activity analysis; depth sensor; graph-based modeling; graph kernel;
   human interaction recognition; multiview; multiple kernel learning
ID REPRESENTATION
AB This paper addresses the problem of recognizing human skeletal interactions using multiview data captured from depth sensors. The interactions among people are important cues for group and crowd human behavior analysis. In this paper, we focus on modeling the person-person skeletal interactions for human activity recognition. First, we propose a novel graph model in each single-view case to encode class-specific person-person interaction patterns. Particularly, we model each person-person interaction by an attributed graph, which is designed to preserve the complex spatial structure among skeletal joints according to their activity levels as well as the spatio-temporal joint features. Then, combining the graph models for each single-view case, we propose the multigraph model to characterize each multiview interaction. Finally, we apply a general multiple kernel learning method to determine the optimal kernel weights for the proposed multigraph model while the optimal classifier is jointly learned. We evaluate the proposed approach on the M-2 I dataset, the SBU Kinect interaction dataset, and our interaction dataset. The experimental results show that our proposed approach outperforms several existing interaction recognition methods.
C1 [Li, Meng; Leung, Howard] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Li, M (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM mli269-c@my.cityu.edu.hk; howard@cityu.edu.hk
FU City University of Hong Kong [7004548]
FX This work was supported by the City University of Hong Kong under
   Project 7004548. The guest editor coordinating the review of this
   manuscript and approving it for publication was Prof. Daniel Keim.
CR Aggarwal JK, 2014, PATTERN RECOGN LETT, V48, P70, DOI 10.1016/j.patrec.2014.04.011
   Anh-Phuong Ta, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P196, DOI 10.1109/AVSS.2010.81
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Aydin R, 2014, IN C IND ENG ENG MAN, P1, DOI 10.1109/IEEM.2014.7058588
   Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007
   Borzeshi E. Z., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1295, DOI 10.1109/ICCVW.2011.6130401
   Chang XB, 2015, IEEE T IMAGE PROCESS, V24, P1905, DOI 10.1109/TIP.2015.2409564
   Chaudhry R, 2013, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW.2013.153
   Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929
   Gärtner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11
   Gaüzère B, 2012, INT C PATT RECOG, P1775
   Gaur U, 2011, IEEE I CONF COMP VIS, P2595, DOI 10.1109/ICCV.2011.6126548
   HAN F, 2016, CORR
   Hu T, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/795360
   Hussein, 2013, INT JOINT C ART INT
   Jia XF, 2012, INT C PATT RECOG, P3001
   Kadu H, 2014, IEEE T MULTIMEDIA, V16, P2191, DOI 10.1109/TMM.2014.2360793
   Kong Y, 2014, IEEE T PATTERN ANAL, V36, P1775, DOI 10.1109/TPAMI.2014.2303090
   Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Patron-Perez A, 2012, IEEE T PATTERN ANAL, V34, P2441, DOI 10.1109/TPAMI.2012.24
   Ryoo MS, 2011, INT J COMPUT VISION, V93, P183, DOI 10.1007/s11263-010-0355-5
   Sefidgar YS, 2015, COMPUT VIS IMAGE UND, V135, P16, DOI 10.1016/j.cviu.2015.02.012
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Tang JKT, 2008, COMPUT ANIMAT VIRT W, V19, P211, DOI 10.1002/cav.260
   Tang JKT, 2012, PATTERN RECOGN LETT, V33, P420, DOI 10.1016/j.patrec.2011.06.005
   Varma M., 2009, P 26 ANN INT C MACHI, P1065
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wu BX, 2014, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2014.334
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Xu N., 2015, Proceedings of the 23rd acm international conference on multimedia, P1195, DOI DOI 10.1145/2733373.2806315
   Yao BP, 2012, LECT NOTES COMPUT SC, V7575, P173, DOI 10.1007/978-3-642-33765-9_13
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhu W., 2015, P 30 AAAI C ART INT, P3697
NR 40
TC 30
Z9 33
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2293
EP 2302
DI 10.1109/TMM.2016.2614228
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900016
DA 2024-07-18
ER

PT J
AU Worring, M
   Koelma, D
   Zahálka, J
AF Worring, Marcel
   Koelma, Dennis
   Zahalka, Jan
TI Multimedia Pivot Tables for Multimedia Analytics on Image Collections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Exploration; information visualization; insight; visual analytics
ID INFORMATION VISUALIZATION; INTELLIGENT; RETRIEVAL
AB We propose a multimedia analytics solution for getting insight into image collections by extending the powerful analytic capabilities of pivot tables, found in the ubiquitous spreadsheets, to multimedia. We formalize the concept of multimedia pivot tables and give design rules and methods for the multimodal summarization, structuring, and browsing of the collection based on these tables, all optimized to support an analyst in getting structural and conclusive insights. Our proposed solution provides truly interactive analytics on the visual content of image collections through concept detection results, as well as tags, geolocation, time, and other metadata. We have performed user experiments with novice users on a dataset from Flickr to improve the initial design and with expert users in marketing and multimedia analysis on two domain-specific datasets collected from Instagram. The results show that analysts are indeed capable of deriving structural and conclusive insights using the proposed multimedia analytics solution. On our website, videos of the system in action are available.(1)
C1 [Worring, Marcel; Koelma, Dennis; Zahalka, Jan] Univ Amsterdam, Inst Informat, NL-1012 WX Amsterdam, Netherlands.
C3 University of Amsterdam
RP Worring, M (corresponding author), Univ Amsterdam, Inst Informat, NL-1012 WX Amsterdam, Netherlands.
EM m.worring@uva.nl; d.c.koelma@uva.nl; j.zahalka@uva.nl
RI Worring, Marcel/JRW-7059-2023; Zahálka, Jan/AAR-5242-2020
OI Worring, Marcel/0000-0003-4097-4136; Zahalka, Jan/0000-0002-6743-3607
CR Aigner W, 2011, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-0-85729-079-3
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Burtner R., 2013, P SOC PHOTO-OPT INS, V8654
   Chen Q, 2016, IEEE T VIS COMPUT GR, V22, P2315, DOI 10.1109/TVCG.2015.2505305
   Chi EHH, 1998, IEEE COMPUT GRAPH, V18, P30, DOI 10.1109/38.689659
   Chinchor NA, 2010, IEEE COMPUT GRAPH, V30, P52, DOI 10.1109/MCG.2010.92
   Christel M.G., 2009, SYNTHESIS LECT INFOR, V1, P1
   de Rooij O, 2013, IEEE T MULTIMEDIA, V15, P898, DOI 10.1109/TMM.2013.2237894
   de Rooij O, 2010, IEEE COMPUT GRAPH, V30, P42, DOI 10.1109/MCG.2010.66
   Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6
   Elmqvist N, 2010, IEEE T VIS COMPUT GR, V16, P439, DOI 10.1109/TVCG.2009.84
   Fisher D, 2010, IEEE T VIS COMPUT GR, V16, P1157, DOI 10.1109/TVCG.2010.222
   Girgensohn A., 2010, Proceedings of the 10th Annual Joint Conference on Digital Libraries, P187, DOI DOI 10.1145/1816123.1816151
   Gratzl S, 2013, IEEE T VIS COMPUT GR, V19, P2277, DOI 10.1109/TVCG.2013.173
   Gray J, 1997, DATA MIN KNOWL DISC, V1, P29, DOI 10.1023/A:1009726021843
   Höferlin B, 2012, IEEE CONF VIS ANAL, P23, DOI 10.1109/VAST.2012.6400492
   Jelen B., 2005, PIVOT TABLE DATA CHR
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   Jonsson B., 2016, MULTIMEDIA MODELING
   Kandel S, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1749
   Keim D.A., 2008, Lecture Notes In Computer Science
   Keim E.D., 2010, Mastering the information age: Solving problems with visual analytics, eurographics association
   Khosla A, 2013, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR.2013.348
   Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538
   Kucher K., 2014, P IEEE PAC VIS S APR, P117
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Luo H, 2008, SIGNAL PROCESS-IMAGE, V23, P538, DOI 10.1016/j.image.2008.04.014
   MacNeil S, 2013, COMPUT GRAPH FORUM, V32, P38, DOI 10.1111/cgf.12013
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Nguyen GP, 2008, J VISUAL LANG COMPUT, V19, P203, DOI 10.1016/j.jvlc.2006.09.002
   North C, 2006, IEEE COMPUT GRAPH, V26, P6, DOI 10.1109/MCG.2006.70
   North C, 2011, INFORM VISUAL, V10, P162, DOI 10.1177/1473871611415989
   Ryu D. S., 2010, ACM SAC, P1884
   Schoeffmann K., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P848, DOI 10.1109/ICME.2012.62
   Schoeffmann K, 2015, ACM COMPUT SURV, V48, DOI 10.1145/2808796
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek C. G. M., 2013, P TRECVID WORKSH GAI
   Stolte C, 2002, IEEE T VIS COMPUT GR, V8, P52, DOI 10.1109/2945.981851
   Thomas J. J., 2005, Illuminating the Path: The Research and Development Agenda for Visual Analytics
   Tomasson G., 2011, P INT C MULT RETR, P999
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   van der Corput P, 2016, IEEE PAC VIS SYMP, P152, DOI 10.1109/PACIFICVIS.2016.7465263
   Wang CL, 2015, INFORM VISUAL, V14, P183, DOI 10.1177/1473871613498519
   Ware C, 2008, MORG KAUF SER INTER, P1
   Worring M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P291, DOI 10.1145/2671188.2749312
   Yang J, 2006, IEEE CONF VIS ANAL, P191
   Zahálka J, 2014, IEEE CONF VIS ANAL, P3, DOI 10.1109/VAST.2014.7042476
   Zavesky E., Proceedings of the 2008 International Conference on Content-based Image and Video Retrieval, ser. CIVR '08. New York, NY, USA: ACM, P617, DOI [10.1145/1386352.1386442, DOI 10.1145/1386352.1386442]
NR 48
TC 10
Z9 11
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2217
EP 2227
DI 10.1109/TMM.2016.2614380
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900009
OA Green Published
DA 2024-07-18
ER

PT J
AU Karpushin, M
   Valenzise, G
   Dufaux, F
AF Karpushin, Maxim
   Valenzise, Giuseppe
   Dufaux, Frederic
TI Keypoint Detection in RGBD Images Based on an Anisotropic Scale Space
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anisotropic diffusion; keypoints; local features; RGBD; SIFT; texture
   plus depth
ID FRAMEWORK; FEATURES; ROBUST; COLOR
AB The increasing availability of texture+depth (RGBD) content has recently motivated research toward the design of image features able to employ the additional geometrical information provided by depth. Indeed, such features are supposed to provide higher robustness than conventional 2D features in the presence of large changes of camera viewpoint. In this paper, we consider the first stage of RGBD image matching, i.e., keypoint detection. In order to obtain viewpoint-covariant keypoints, we design a filtering process, which approximates a diffusion process along the surfaces of the scene, by means of the information provided by depth. Next, we employ this multiscale representation to find keypoints through a multiscale keypoint detector. The keypoints obtained by the proposed detector provide substantially higher stability to viewpoint changes than alternative 2D and RGBD feature extraction approaches, both in terms of repeatability and image classification accuracy. Furthermore, the proposed detector can be efficiently implemented on a GPU.
C1 [Karpushin, Maxim; Valenzise, Giuseppe; Dufaux, Frederic] Univ Paris Saclay, Lab Traitement & Commun Informat, Telecom ParisTech, F-75634 Paris, France.
C3 Universite Paris Saclay; IMT - Institut Mines-Telecom; Institut
   Polytechnique de Paris; Telecom Paris; Universite Paris Cite
RP Karpushin, M (corresponding author), Univ Paris Saclay, Lab Traitement & Commun Informat, Telecom ParisTech, F-75634 Paris, France.
EM maxim.karpushin@telecom-paristech.fr;
   giuseppe.valenzise@telecom-paristech.fr;
   frederic.dufaux@telecom-paristech.fr
RI Dufaux, Frederic/HJJ-1496-2023
OI Dufaux, Frederic/0000-0001-6388-4112
CR Agrawal M, 2008, LECT NOTES COMPUT SC, V5305, P102, DOI 10.1007/978-3-540-88693-8_8
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Altintakan UL, 2015, IEEE T MULTIMEDIA, V17, P323, DOI 10.1109/TMM.2014.2388312
   [Anonymous], 1SC29WG11 ISOIEC JTC
   [Anonymous], 2006, Geometric partial differential equations and image analysis
   [Anonymous], 1SC29WG11 ISOIEC JTC
   [Anonymous], 1998, ANISOTROPIC DIFFUSIO
   [Anonymous], 2015, P IEEE INT C MULT EX
   [Anonymous], 2013, 18 INT C DIG SIGN PR
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   BROWN M., 2002, BRIT MACHINE VISION, P656, DOI [10.5244/C.16.23, DOI 10.5244/C.16.23]
   Calderero F, 2014, SIAM J IMAGING SCI, V7, P1108, DOI 10.1137/130923142
   do Nascimento ER, 2013, NEUROCOMPUTING, V120, P141, DOI 10.1016/j.neucom.2012.08.064
   Fraundorfer F., 2005, Proceedings of computer vision and pattern recognition-CVPR workshops, P33
   Gobara M, 2006, LECT NOTES COMPUT SC, V3852, P643
   Gonzalez-Jorge H, 2013, MEASUREMENT, V46, P1800, DOI 10.1016/j.measurement.2013.01.011
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Harris C., 1988, ALVEY VISION C, P147151
   Heinly J, 2012, LECT NOTES COMPUT SC, V7573, P759, DOI 10.1007/978-3-642-33709-3_54
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Karpushin M, 2015, IEEE IMAGE PROC, P2399, DOI 10.1109/ICIP.2015.7351232
   Karpushin M, 2014, IEEE IMAGE PROC, P2809, DOI 10.1109/ICIP.2014.7025568
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   Koser K., 2007, PROC IEEE INT C COMP, P1
   Kovnatsky Artiom, 2011, INT C SCAL SPAC VAR, P616
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Lin BW, 2015, IEEE IMAGE PROC, P2611, DOI 10.1109/ICIP.2015.7351275
   Lindeberg T, 2003, LECT NOTES COMPUT SC, V2695, P148
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Lo TWR, 2009, COMPUT VIS IMAGE UND, V113, P1235, DOI 10.1016/j.cviu.2009.06.005
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Moreels P, 2007, INT J COMPUT VISION, V73, P263, DOI 10.1007/s11263-006-9967-1
   Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730
   Mukherjee D, 2015, MACH VISION APPL, V26, P443, DOI 10.1007/s00138-015-0679-9
   Pang YW, 2012, NEUROCOMPUTING, V85, P6, DOI 10.1016/j.neucom.2011.12.006
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Redondi A, 2015, AD HOC NETW, V28, P38, DOI 10.1016/j.adhoc.2015.01.008
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Su CC, 2013, IEEE T IMAGE PROCESS, V22, P2259, DOI 10.1109/TIP.2013.2249075
   Su CC, 2011, IEEE IMAGE PROC, P257, DOI 10.1109/ICIP.2011.6116191
   Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679
   Tosic I, 2014, IEEE IMAGE PROC, P1927, DOI 10.1109/ICIP.2014.7025386
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang SH, 2012, IEEE GEOSCI REMOTE S, V9, P649, DOI 10.1109/LGRS.2011.2177437
   Werghi N, 2015, IEEE T IMAGE PROCESS, V24, P220, DOI 10.1109/TIP.2014.2370253
   WU C., 2008, P CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587501
   Yaoyao Guo, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457809
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
NR 53
TC 13
Z9 14
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1762
EP 1771
DI 10.1109/TMM.2016.2590305
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Song, YF
   Chen, XW
   Wang, XG
   Zhang, Y
   Li, J
AF Song, Yafei
   Chen, Xiaowu
   Wang, Xiaogang
   Zhang, Yu
   Li, Jia
TI 6-DOF Image Localization From Massive Geo-Tagged Reference Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image localization; one-sided radial fundamental matrix estimation;
   relative pose estimation
ID RELATIVE POSE
AB The 6-degrees of freedom (DOF) image localization, which aims to calculate the spatial position and rotation of a camera, is a challenging problem for most location-based services. In existing approaches, this problem is often tackled by finding the matches between 2D image points and 3D structure points so as to derive the location information via direct linear transformation algorithm. However, as these 2D-to-3D-based approaches need to reconstruct the 3D structure points of the scene, they may not be flexible enough to employ massive and increasing geo-tagged data. To this end, this paper presents a novel approach for 6-DOF image localization by fusing candidate poses relative to reference images. In this approach, we propose to localize an input image according to the position and rotation information of multiple geo-tagged images retrieved from a reference dataset. From the reference images, an efficient relative pose estimation algorithm is proposed to derive a set of candidate poses for the input image. Each candidate pose encodes the relative rotation and direction of the input image with respect to a specific reference image. Finally, these candidate poses can be fused together by minimizing a well-defined geometry error so that the 6-DOF location of the input image is effectively derived. Experimental results show that our method can obtain satisfactory localization accuracy. In addition, the proposed relative pose estimation algorithm is much faster than existing work.
C1 [Song, Yafei; Chen, Xiaowu; Wang, Xiaogang; Zhang, Yu; Li, Jia] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Jia] Beihang Univ, Int Res Inst Multidisciplinary Sci, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Chen, XW (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM songyf@buaa.edu.cn; chen@buaa.edu.cn; wangxiaogang@buaa.edu.cn;
   octopus@buaa.edu.cn; jiali@buaa.edu.cn
RI wang, Xiaogang/J-5003-2017; lu, kai/KBB-4008-2024; Li, Jia/AAB-6431-2019
OI Li, Jia/0000-0002-4346-8696; Song, Yafei/0000-0003-3537-1015
FU National Natural Science Foundation of China [61325011, 61532003,
   61421003]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61325011, Grant 61532003, and Grant
   61421003. This paper was presented in part at the IEEE International
   Conference on Multimedia Big Data, Beijing, China, April 2015. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Shu-Ching Chen. (Corresponding
   author: Xiaowu Chen.)
CR [Anonymous], 2008, P 16 INT C MULTIMEDI, DOI [DOI 10.1145/1459359.1459577, 10.1145/1459359.1459577]
   Baatz G, 2012, INT J COMPUT VISION, V96, P315, DOI 10.1007/s11263-011-0458-7
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bergamo A, 2013, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2013.104
   Brito JH, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.96
   Bujnak M, 2009, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2009.5459402
   Cao S, 2013, PROC CVPR IEEE, P700, DOI 10.1109/CVPR.2013.96
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Crandall DJ, 2013, IEEE T PATTERN ANAL, V35, P2841, DOI 10.1109/TPAMI.2012.218
   Donoser M, 2014, PROC CVPR IEEE, P516, DOI 10.1109/CVPR.2014.73
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   Hao Q, 2012, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2012.6248104
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Irschara A, 2009, PROC CVPR IEEE, P2591, DOI 10.1109/CVPRW.2009.5206587
   Jhuo IH, 2010, LECT NOTES COMPUT SC, V5916, P196, DOI 10.1007/978-3-642-11301-7_22
   Jing YS, 2013, IEEE T MULTIMEDIA, V15, P2022, DOI 10.1109/TMM.2013.2279663
   Li Q, 2014, IEEE T IMAGE PROCESS, V23, P4812, DOI 10.1109/TIP.2014.2358193
   Li YP, 2012, LECT NOTES COMPUT SC, V7572, P15, DOI 10.1007/978-3-642-33718-5_2
   Li YP, 2010, LECT NOTES COMPUT SC, V6312, P791
   Lim H, 2012, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2012.6247782
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meier L., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P2992, DOI 10.1109/ICRA.2011.5980229
   Middelberg S, 2014, LECT NOTES COMPUT SC, V8690, P268, DOI 10.1007/978-3-319-10605-2_18
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Sattler T, 2011, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2011.6126302
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Song YF, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P156, DOI 10.1109/BigMM.2015.10
   Stewénius H, 2008, IMAGE VISION COMPUT, V26, P871, DOI 10.1016/j.imavis.2007.10.003
   Takacs G., 2008, MIR 08, P427, DOI DOI 10.1145/1460096.1460165
   Wagner D, 2010, IEEE T VIS COMPUT GR, V16, P355, DOI 10.1109/TVCG.2009.99
   Yibin Li, 2009, 2009 IEEE International Conference on Automation and Logistics (ICAL), P1957, DOI 10.1109/ICAL.2009.5262626
   Zamir AR, 2014, IEEE T PATTERN ANAL, V36, P1546, DOI 10.1109/TPAMI.2014.2299799
   Zhang Jerry, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3677, DOI 10.1109/ICIP.2011.6116517
   Zhang W, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P33, DOI 10.1109/3dpvt.2006.80
   Zhu L, 2015, IEEE T MULTIMEDIA, V17, P981, DOI 10.1109/TMM.2015.2431496
NR 38
TC 32
Z9 33
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1542
EP 1554
DI 10.1109/TMM.2016.2568743
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000008
DA 2024-07-18
ER

PT J
AU Zhang, YT
   Qian, XM
   Tan, XL
   Han, JW
   Tang, YY
AF Zhang, Yuting
   Qian, Xueming
   Tan, Xianglong
   Han, Junwei
   Tang, Yuanyan
TI Sketch-Based Image Retrieval by Salient Contour Reinforcement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contour matching; image retrieval; salient contour; sketch based image
   retrieval (SBIR)
ID SYSTEM; SCALE; COLOR
AB The paper presents a sketch-based image retrieval algorithm. One of the main challenges in sketch-based image retrieval (SBIR) is to measure the similarity between a sketch and an image. To tackle this problem, we propose an SBIR-based approach by salient contour reinforcement. In our approach, we divide the image contour into two types. The first is the global contour map. The second, called the salient contour map, is helpful to find out the object in images similar to the query. In addition, based on the two contour maps, we propose a new descriptor, namely an angular radial orientation partitioning (AROP) feature. It fully utilizes the edge pixels' orientation information in contour maps to identify the spatial relationships. Our AROP feature based on the two candidate contour maps is both efficient and effective to discover false matches of local features between sketches and images, and can greatly improve the retrieval performance. The application of the retrieval system based on this algorithm is established. The experiments on the image dataset with 0.3 million images show the effectiveness of the proposed method and comparisons with other algorithms are also given. Compared to baseline performance, the proposed method achieves 10% higher precision in top 5.
C1 [Zhang, Yuting; Qian, Xueming; Tan, Xianglong] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, SMILES Lab, Xian 710049, Peoples R China.
   [Qian, Xueming] Minist Educ, Key Lab Intelligent Networks & Network Security, Xian, Peoples R China.
   [Han, Junwei] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Tang, Yuanyan] Univ Macau, Taipa, Peoples R China.
C3 Xi'an Jiaotong University; Northwestern Polytechnical University;
   University of Macau
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, SMILES Lab, Xian 710049, Peoples R China.
EM zhangyuting@stu.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn;
   xjtuicemaple@sohu.com; jhan@nwpu.edu.cn; yytang@umac.edu.cn
RI ZHANG, YUTING/GXV-2460-2022
FU Program 973 [2012CB316400]; National Natural Science Foundation of China
   [61373121, 61173109, 61332018]; Microsoft Research Asia
FX This work was supported in part by the Program 973 under Grant
   2012CB316400, in part by the National Natural Science Foundation of
   China under Grant 61373121, Grant 61173109, and Grant 61332018, and in
   part by Microsoft Research Asia. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Balakrishnan Prabhakaran. (Corresponding author: Xueming Qian.)
CR [Anonymous], 2011, P 16 INT C INT US IN
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], P INT C MULT RETR
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao XC, 2013, IEEE I CONF COMP VIS, P313, DOI 10.1109/ICCV.2013.46
   Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460
   Cao Yang, 2010, P 18 ACM INT C MULT, P1605
   Chalechale A, 2005, IEEE T SYST MAN CY A, V35, P28, DOI 10.1109/TSMCA.2004.838464
   Chalechale A, 2004, IEE P-VIS IMAGE SIGN, V151, P93, DOI 10.1049/ip-vis:20040332
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dixon D, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P897
   Donmez N., 2012, INT S SKETCH BAS INT, P29
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185527
   Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266
   Eitz M, 2010, COMPUT GRAPH-UK, V34, P482, DOI 10.1016/j.cag.2010.07.002
   Eitz Mathias., 2009, P 6 EUR S SKETCH BAS, P29
   Hammond T., 2007, P ACM SIGGRAPH, P518
   Han JW, 2016, IEEE T CYBERNETICS, V46, P487, DOI 10.1109/TCYB.2015.2404432
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Han JW, 2015, IEEE T CYBERNETICS, V45, P1692, DOI 10.1109/TCYB.2014.2358647
   Han JW, 2014, ISPRS J PHOTOGRAMM, V89, P37, DOI 10.1016/j.isprsjprs.2013.12.011
   Han JW, 2013, IEEE T CIRC SYST VID, V23, P2009, DOI 10.1109/TCSVT.2013.2242594
   HIRATA K, 1992, LECT NOTES COMPUT SC, V580, P56, DOI 10.1007/BFb0032423
   Hong BW, 2015, IEEE T PATTERN ANAL, V37, P151, DOI 10.1109/TPAMI.2014.2342215
   Hu R, 2010, IEEE IMAGE PROC, P1025, DOI 10.1109/ICIP.2010.5649331
   Jacobs C. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P277, DOI 10.1145/218380.218454
   Jain AK, 1996, PATTERN RECOGN, V29, P1233, DOI 10.1016/0031-3203(95)00160-3
   Laviola JJ, 2004, ACM T GRAPHIC, V23, P432, DOI 10.1145/1015706.1015741
   Lee YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964922
   Li J, 2013, IEEE T MULTIMEDIA, V15, P2058, DOI 10.1109/TMM.2013.2280127
   Liang S, 2015, IEEE T PATTERN ANAL, V37, P1723, DOI 10.1109/TPAMI.2014.2369031
   Liu XM, 2012, PROC CVPR IEEE, P462, DOI 10.1109/CVPR.2012.6247709
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo L, 2013, IEEE T MULTIMEDIA, V15, P1174, DOI 10.1109/TMM.2013.2242450
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mathias E., 2012, ACM T GRAPH, V31
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Bada AMM, 2014, INT CONF ELECTR COMM, P183, DOI 10.1109/CONIELECOMP.2014.6808588
   Pan D, 2014, 2014 11TH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (FSKD), P825, DOI 10.1109/FSKD.2014.6980944
   Qian XM, 2016, IEEE T IMAGE PROCESS, V25, P195, DOI 10.1109/TIP.2015.2497145
   Qian XM, 2015, IEEE T CIRC SYST VID, V25, P1857, DOI 10.1109/TCSVT.2014.2369731
   Qian XM, 2015, IEEE T IMAGE PROCESS, V24, P4348, DOI 10.1109/TIP.2015.2462131
   Schneider RG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661231
   Sezgin T.M., 2001, PROC WORKSHOP PERCEP, P1
   Shih JL, 2001, IMAGE VISION COMPUT, V19, P1011, DOI 10.1016/S0262-8856(01)00063-4
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189
   Wang S, 2015, IEEE T MULTIMEDIA, V17, P1045, DOI 10.1109/TMM.2015.2431492
   Xue XH, 2005, IEEE T MULTIMEDIA, V7, P805, DOI 10.1109/TMM.2005.854471
   Yang XY, 2015, IEEE T IMAGE PROCESS, V24, P1709, DOI 10.1109/TIP.2015.2411433
   Zhang YP, 2015, J NUCL ENG RADIAT SC, V1, DOI 10.1115/1.4030364
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
NR 58
TC 49
Z9 51
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1604
EP 1615
DI 10.1109/TMM.2016.2568138
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000013
DA 2024-07-18
ER

PT J
AU Yuan, L
   Li, HA
   Wan, Y
AF Yuan, Lei
   Li, Huaan
   Wan, Yi
TI A Novel UEP Fountain Coding Scheme for Scalable Multimedia Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fountain codes; intermediate symbol recovery rates; scalable video
   coding; unequal error protection (UEP)
ID UNEQUAL ERROR PROTECTION; RAPTOR CODES; RATELESS CODES; LDPC CODES;
   DESIGN; NETWORKS
AB In this paper, we propose an efficient scheme to construct fountain codes that can provide unequal error protection (UEP) property. To reduce the coding complexity and improve the performance of previous duplicate UEP schemes in the literature, we implement different rate low-density parity-check codes instead of the duplication process and replace the high-complexity classical robust soliton distribution by low constant average degree distributions with high intermediate symbol recovery rates. Asymptotic analysis of both the duplicate scheme and our scheme is obtained by deriving unequal density evolution formulas over the binary erasure channel. Compared to previous UEP schemes, our scheme has lower complexity. Experimental results show that our scheme can obtain better decoding performance, especially for smaller input information length. Comparison of peak signal-noise ratio performance for the scalable video coding shows that, for moderate input information length, our scheme can provide a better basic video quality at lower overhead, but needs larger overhead to achieve a high video quality.
C1 [Yuan, Lei; Li, Huaan; Wan, Yi] Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou 730000, Peoples R China.
C3 Lanzhou University
RP Yuan, L (corresponding author), Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou 730000, Peoples R China.
EM yuanl@lzu.edu.cn; lihuaansmc@163.com; wanyi@lzu.edu.cn
RI Yuan, Lei/AAL-6265-2020
OI Yuan, Lei/0000-0002-2293-0626
FU Fundamental Research Funds for the Central Universities
   [lzujbky-2015-103]
FX This work was supported by the Fundamental Research Funds for the
   Central Universities lzujbky-2015-103. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Honggang Wang.
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   Arslan SS, 2012, IEEE T IMAGE PROCESS, V21, P3586, DOI 10.1109/TIP.2012.2195668
   Aydinlik M, 2009, IEEE T COMMUN, V57, P1215, DOI 10.1109/TCOMM.2009.05.060202
   Cataldi P, 2010, IEEE T IMAGE PROCESS, V19, P1491, DOI 10.1109/TIP.2010.2042985
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Gómez-Barquero D, 2009, IEEE T BROADCAST, V55, P396, DOI 10.1109/TBC.2008.2012024
   Hsiao HF, 2014, IEEE T CIRC SYST VID, V24, P1395, DOI 10.1109/TCSVT.2014.2302533
   Hu XY, 2005, IEEE T INFORM THEORY, V51, P386, DOI 10.1109/TIT.2004.839541
   Ji RR, 2012, PROC CVPR IEEE, P2925, DOI 10.1109/CVPR.2012.6248020
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Luby M., 1998, P 9 ANN ACM SIAM S D, P364
   Luby M, 2007, IEEE T BROADCAST, V53, P235, DOI 10.1109/TBC.2007.891703
   MacKay DJC, 2005, IEE P-COMMUN, V152, P1062, DOI 10.1049/ip-com:20050237
   MASNICK B, 1967, IEEE T INFORM THEORY, V13, P600, DOI 10.1109/TIT.1967.1054054
   Rahnavard N, 2006, IEEE COMMUN LETT, V10, P43, DOI 10.1109/LCOMM.2006.1576564
   Rahnavard N, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P449
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   Richardson TJ, 2001, IEEE T INFORM THEORY, V47, P619, DOI 10.1109/18.910578
   Richardson Tom, 2008, Modern Coding Theory
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Sandberg S, 2010, IEEE T COMMUN, V58, P802, DOI 10.1109/TCOMM.2010.03.0800352
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sejdinovic D, 2009, IEEE T COMMUN, V57, P2510, DOI 10.1109/TCOMM.2009.09.070616
   Sgardoni V, 2015, IEEE T MOBILE COMPUT, V14, P401, DOI 10.1109/TMC.2014.2331967
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Sivasubramanian B, 2008, IEEE T VEH TECHNOL, V57, P3905, DOI 10.1109/TVT.2008.923664
   Stefanovic C, 2011, IEEE J SEL AREA COMM, V29, P94, DOI 10.1109/JSAC.2011.110110
   Talari A, 2012, IEEE T COMMUN, V60, P1237, DOI 10.1109/TCOMM.2012.030712.110032
   Venkiah A, 2009, EURASIP J WIREL COMM, DOI 10.1155/2009/657970
   Vukobratovic D., 2009, P 5 INT ICST MOB MUL, P13
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   Yang XY, 2015, PATTERN RECOGN, V48, P3093, DOI 10.1016/j.patcog.2014.12.017
   Yang XY, 2015, IEEE T IMAGE PROCESS, V24, P1709, DOI 10.1109/TIP.2015.2411433
   Yen KK, 2013, IEEE T MULTIMEDIA, V15, P2162, DOI 10.1109/TMM.2013.2269898
   Yuan L, 2010, EUR T TELECOMMUN, V21, P30, DOI 10.1002/ett.1390
NR 35
TC 11
Z9 13
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1389
EP 1400
DI 10.1109/TMM.2016.2557079
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600014
DA 2024-07-18
ER

PT J
AU Huang, SL
   Izquierdo, E
   Hao, PW
AF Huang, Shenglan
   Izquierdo, Ebroul
   Hao, Pengwei
TI Bandwidth-Efficient Packet Scheduling for Live Streaming With Network
   Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed scheduling; live broadcasting; network coding; peer-to-peer
   TV systems
ID DELAY; PUSH
AB Network coding (NC) brings substantial improvements in terms of throughput and delay in collaborative media streaming applications. A key aspect of NC-driven live peer-to-peer streaming is the packet scheduling policy. Indeed, lack of synchronization among peers usually results in significantly redundant packet transmission, which in turn leads to severe bandwidth inefficiencies. In this paper, we address the problem of finding a suitable asynchronous packet scheduling policy that greatly helps to overcome this critical redundant transmission problem. We propose a bandwidth cost minimization technique under a full video packet recovery constraint. In order to add scalability and improved performance, we also further derive a distributed packet scheduling algorithm. Both implementation and analytical considerations of the proposed approaches are described in this paper. Experimental results confirm that the proposed algorithms deliver higher bandwidth efficiency with reduced redundancy and communication overhead rate and, consequently, better quality-of-service in terms of improved video quality and delivery ratio.
C1 [Huang, Shenglan; Izquierdo, Ebroul; Hao, Pengwei] Univ London, Elect Engn & Comp Sci, London E1 4NS, England.
C3 University of London
RP Huang, SL; Izquierdo, E; Hao, PW (corresponding author), Univ London, Elect Engn & Comp Sci, London E1 4NS, England.
EM s.huang@qmul.ac.uk; ebroul.izquierdo@qmul.ac.uk; p.hao@qmul.ac.uk
FU EU Grant [687605]; H2020 - Industrial Leadership [687605] Funding
   Source: H2020 - Industrial Leadership
FX The research reported in this paper was partially supported by the EU
   Grant 687605 under the H2020 framework program project "Converging
   broadcast and user generated content for interactive ultra-high
   definition services (Cognitus)."
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], 2003, 51 ALL C COMM CONTR
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   Chan KHK, 2010, IEEE T MULTIMEDIA, V12, P743, DOI 10.1109/TMM.2010.2053524
   Cui LZ, 2012, IEEE ICC, P2075, DOI 10.1109/ICC.2012.6364294
   Cuiping Jing, 2011, Proceedings of the 2011 14th International Conference on Network-Based Information Systems (NBiS 2011), P634, DOI 10.1109/NBiS.2011.119
   Feng C., 2008, Proceedings of the 16th ACM international conference on Multimedia, P269
   Guang X, 2013, IEEE T INFORM THEORY, V59, P1030, DOI 10.1109/TIT.2012.2222344
   Huang SL, 2014, IEEE IMAGE PROC, P3993, DOI 10.1109/ICIP.2014.7025811
   Jannotti J., 2000, OSDI 00 P 4 C S OPER, P14
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   Liu Zhong-Hua, 2011, International Journal of Neuropsychopharmacology, V14, P618, DOI 10.1017/S1461145710000520
   Magharei N, 2009, IEEE ACM T NETWORK, V17, P1052, DOI 10.1109/TNET.2008.2007434
   Magli E, 2013, IEEE T MULTIMEDIA, V15, P1195, DOI 10.1109/TMM.2013.2241415
   Nguyen AT., 2010, IEEE INFOCOM'10, San Diego, CA, USA, P1
   Pai V, 2005, LECT NOTES COMPUT SC, V3640, P127
   Pendarakis D., 2001, Proceedings of the 3rd Conference on USENIX Symposium on Internet Technologies and Systems - Volume 3, USITS'01, (Berkeley, CA, USA), P5
   Sanna M., 2013, 20 INT PACK VID WORK, P1
   Shao MK, 2011, IEEE T MULTIMEDIA, V13, P353, DOI 10.1109/TMM.2010.2095833
   Sheikh AM, 2014, IEEE T MULTIMEDIA, V16, P2294, DOI 10.1109/TMM.2014.2357716
   Sheikh AM, 2013, IEEE INFOCOM SER, P11
   Thomos N, 2011, IEEE T MULTIMEDIA, V13, P776, DOI 10.1109/TMM.2011.2111364
   Thomos N, 2009, IEEE INT CON MULTI, P730, DOI 10.1109/ICME.2009.5202598
   Tian RX, 2005, LECT NOTES COMPUT SC, V3640, P152, DOI 10.1007/11558989_14
   Toldo M., 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P400, DOI 10.1109/MMSP.2010.5662054
   Tran DA, 2003, IEEE INFOCOM SER, P1283
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Wang M, 2007, IEEE INFOCOM SER, P1082, DOI 10.1109/INFCOM.2007.130
   Xia RL, 2010, IEEE COMMUN SURV TUT, V12, P140, DOI 10.1109/SURV.2010.021110.00036
   Yu LJ, 2009, IEEE T CONSUM ELECTR, V55, P576, DOI 10.1109/TCE.2009.5174425
   Zhang XY, 2012, COMPUT NETW, V56, P3548, DOI 10.1016/j.comnet.2012.06.013
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 32
TC 13
Z9 14
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 752
EP 763
DI 10.1109/TMM.2016.2530411
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zen, G
   Porzi, L
   Sangineto, E
   Ricci, E
   Sebe, N
AF Zen, Gloria
   Porzi, Lorenzo
   Sangineto, Enver
   Ricci, Elisa
   Sebe, Nicu
TI Learning Personalized Models for Facial Expression Analysis and Gesture
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial expression analysis; gesture recognition; personalization;
   transductive parameter transfer; transfer learning
ID EARTH MOVERS DISTANCE
AB Facial expression and gesture recognition algorithms are key enabling technologies for human-computer interaction (HCI) systems. State of the art approaches for automatic detection of body movements and analyzing emotions from facial features heavily rely on advanced machine learning algorithms. Most of these methods are designed for the average user, but the assumption "one-size-fits-all" ignores diversity in cultural background, gender, ethnicity, and personal behavior, and limits their applicability in real-world scenarios. A possible solution is to build personalized interfaces, which practically implies learning person-specific classifiers and usually collecting a significant amount of labeled samples for each novel user. As data annotation is a tedious and time-consuming process, in this paper we present a framework for personalizing classification models which does not require labeled target data. Personalization is achieved by devising a novel transfer learning approach. Specifically, we propose a regression framework which exploits auxiliary (source) annotated data to learn the relation between person-specific sample distributions and parameters of the corresponding classifiers. Then, when considering a new target user, the classification model is computed by simply feeding the associated (unlabeled) sample distribution into the learned regression function. We evaluate the proposed approach in different applications: pain recognition and action unit detection using visual data and gestures classification using inertial measurements, demonstrating the generality of our method with respect to different input data types and basic classifiers. We also show the advantages of our approach in terms of accuracy and computational time both with respect to user-independent approaches and to previous personalization techniques.
C1 [Zen, Gloria; Sangineto, Enver; Sebe, Nicu] Univ Trent, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
   [Porzi, Lorenzo; Ricci, Elisa] TeV Fdn Bruno Kessler, I-38050 Trento, Italy.
   [Porzi, Lorenzo; Ricci, Elisa] Univ Perugia, Dept Engn, I-06125 Perugia, Italy.
C3 University of Trento; Fondazione Bruno Kessler; University of Perugia
RP Zen, G; Sangineto, E; Sebe, N (corresponding author), Univ Trent, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.; Porzi, L; Ricci, E (corresponding author), TeV Fdn Bruno Kessler, I-38050 Trento, Italy.
EM zen@disi.unitn.it; porzi@fbk.eu; sangineto@unitn.it; eliricci@fbk.eu;
   sebe@disi.unitn.it
RI Sebe, Niculae/KEC-2000-2024; Sangineto, Enver/AAS-9542-2020; Ricci,
   Elisa/IYS-6532-2023
OI Sebe, Niculae/0000-0002-6597-7248; Sangineto, Enver/0000-0002-5187-4133;
   Ricci, Elisa/0000-0002-0228-1147
FU European Union's Horizon Research and Innovation Programme [687757];
   H2020 - Industrial Leadership [687757] Funding Source: H2020 -
   Industrial Leadership
FX This work was supported by the European Union's Horizon 2020 Research
   and Innovation Programme under Grant 687757. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jiebo Luo.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2009, DATASET SHIFT MACHIN
   [Anonymous], P ICML
   [Anonymous], P ICML
   [Anonymous], 2013, 2013 10 IEEE INT C W
   [Anonymous], IMAGE VIS COMPUT
   [Anonymous], 2007, Proceedings of the 9th international conference on Multimodal interfaces
   [Anonymous], INT WORKSH FRONT HAN
   Bellet A., 2013, CoRR
   Bieber G., 2012, Proceedings of the 5th International Conference on PErvasive Technologies Related to Assistive Environments - PETRA'12 p, P1, DOI [DOI 10.1145/2413097.2413147, 10.1145/2413097.2413147]
   Blanchard G., 2011, ADV NEURAL INFORM PR, P2178
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Chen JX, 2013, PATTERN RECOGN LETT, V34, P1964, DOI 10.1016/j.patrec.2013.02.002
   Chu WS, 2013, PROC CVPR IEEE, P3515, DOI 10.1109/CVPR.2013.451
   Costante G, 2014, EUR SIGNAL PR CONF, P2530
   Daliri MR, 2013, CLIN EEG NEUROSCI, V44, P182, DOI 10.1177/1550059412471521
   Daume III Hal, 2007, ACL 2007, P256
   Dibekliolu H., 2012, MM'2012: Proceedings of the 20th ACM International Conference on Multimedia, P209
   Ding ZM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3453
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Guo ZY, 2013, IEEE T MULTIMEDIA, V15, P621, DOI 10.1109/TMM.2012.2234729
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Junker H, 2008, PATTERN RECOGN, V41, P2010, DOI 10.1016/j.patcog.2007.11.016
   Khan M., 2012, 2012 IEEE International Conference on Emerging Signal Processing Applications, P163, DOI 10.1109/ESPA.2012.6152471
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167
   Liu JY, 2009, PERVASIVE MOB COMPUT, V5, P657, DOI 10.1016/j.pmcj.2009.07.007
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mantyjarvi J., 2004, Proceedings of the 3rd international conference on Mobile and ubiquitous multimedia - MUM '04, P25
   Martinez A, 2012, J MACH LEARN RES, V13, P1589
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Perronnin F., 2007, P IEEE CVPR, P1
   Porzi Lorenzo., 2013, Proceedings of the 3rd ACM international workshop on Interactive multimedia on mobile portable devices - IMMPD '13, P19
   Prkachin KM, 2008, PAIN, V139, P267, DOI 10.1016/j.pain.2008.04.010
   Ricci E, 2013, IEEE T PATTERN ANAL, V35, P513, DOI 10.1109/TPAMI.2012.131
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sangineto E, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P357, DOI 10.1145/2647868.2654916
   Shao M, 2014, INT J COMPUT VISION, V109, P74, DOI 10.1007/s11263-014-0696-6
   Tuia D, 2011, IEEE GEOSCI REMOTE S, V8, P804, DOI 10.1109/LGRS.2011.2109934
   Valstar Michel F., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P921, DOI 10.1109/FG.2011.5771374
   Valstar M.F., 2006, International Conference on Multimodal Interfaces, P162, DOI DOI 10.1145/1180995.1181031
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Xing E., 2002, ADV NEURAL INFORM PR
   Yang J., 2007, DATA MINING WORKSHOP, P69
   Zen Gloria., 2014, P OF THE 16 INT C ON, P128, DOI DOI 10.1145/2663204.2663247
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
NR 47
TC 67
Z9 72
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 775
EP 788
DI 10.1109/TMM.2016.2523421
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300018
OA Green Published
DA 2024-07-18
ER

PT J
AU Kurdoglu, E
   Liu, Y
   Wang, Y
AF Kurdoglu, Eymen
   Liu, Yong
   Wang, Yao
TI Dealing With User Heterogeneity in P2P Multi-Party Video Conferencing:
   Layered Distribution Versus Partitioned Simulcast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Overlay networks; video codecs; video conferences
ID STANDARD; NETWORKS
AB We consider peer-to-peer multi-party video conferencing (P2P-MPVC), where users with different uplink-downlink capacities send their videos using multicast trees. One way to deal with user bandwidth heterogeneity is employing layered video coding, generating multiple layers with different rates, whereas an alternative is partitioning the receivers of each source and disseminating a different non-layered video version within each group. In this paper, we aim to maximize the received video quality for both systems under uplink-downlink capacity constraints, while constraining the number of hops the packets traverse to two. We first show any multicast tree is equivalent to a collection of 1-hop and 2-hop trees, under user uplink-downlink capacity constraints. This reveals that the packet overlay hop count can be limited to two without sacrificing the achievable rate performance. Assuming a fine granularity scalable stream that can be truncated at any rate, we propose an algorithm that solves for the number of video layers, layer rates, and distribution trees for the layered system. For the partitioned simulcast system, we develop an algorithm to determine the receiver partitions along with the video rate and the distribution trees for each group. Through numerical comparison, we show that the partitioned simulcast system achieves the same average receiving quality as the ideal layered system without any coding overhead for the four-user systems simulated, and better quality than the layered system when the layered coding overhead is only 20%. The two systems perform similarly for the six-user case if the layered coding overhead is 10%.
C1 [Kurdoglu, Eymen; Liu, Yong; Wang, Yao] NYU, Polytech Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 New York University; New York University Tandon School of Engineering
RP Kurdoglu, E (corresponding author), NYU, Polytech Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM eymen.kurdoglu@nyu.edu; yongliu@nyu.edu; yw523@nyu.edu
OI Kurdoglu, Eymen/0000-0002-7770-3148; Wang, Yao/0000-0003-3199-3802
CR [Anonymous], 2014, SVC BAS C SOL DEPL G
   [Anonymous], P ACM SIGCOMM AS WOR
   [Anonymous], VID FEAT
   [Anonymous], JM SOFTW VER 18 6
   [Anonymous], 2001, ITU T VCEG M AUST TE
   [Anonymous], 2015, TOLL TEST REP AVAYAL
   Boyd S., 2004, CONVEX OPTIMIZATION
   Canadi Igor., 2012, Proceedings of the 2012 ACM Conference on Internet Measurement Conference, P273
   Chen MH, 2008, PERF E R SI, V36, P169, DOI 10.1145/1384529.1375477
   Chen X., 2011, Proc. 19th ACM International Conference on Multimedia (MM '11), P493
   Eleftheriadis A., 2011, SVC VIDEO COMMUNICAT
   Google. Mountain View CA USA, HANG
   Hu H, 2012, IEEE IMAGE PROC, P717, DOI 10.1109/ICIP.2012.6466960
   Hua KL, 2013, COMPUT NETW, V57, P2856, DOI 10.1016/j.comnet.2013.06.014
   Kurdoglu E, 2014, IEEE CONF COMPUT, P239, DOI 10.1109/INFCOMW.2014.6849238
   LAN X, 2007, P ACM MULT C AUGSB G, P783
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Liang C, 2011, IEEE ACM T NETWORK, V19, P1704, DOI 10.1109/TNET.2011.2141680
   Ma Z., 2013, Power Electronics and Applications (EPE), 2013 15th European Conference On, P1
   Ou YF, 2014, IEEE T IMAGE PROCESS, V23, P2473, DOI 10.1109/TIP.2014.2303636
   Ponec M, 2011, IEEE T MULTIMEDIA, V13, P856, DOI 10.1109/TMM.2011.2161759
   SCHWARZ H, 2007, P IEEE INT C IM PROC, V2, P281
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Westerlund M., 2015, RTP TOPOLOGIES
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiao X, 2009, IEEE INFOCOM SER, P603, DOI 10.1109/INFCOM.2009.5061967
   Xu Y., 2012, P 2012 INTERNET MEAS, P371, DOI DOI 10.1145/2398776.2398816
NR 27
TC 20
Z9 20
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 90
EP 101
DI 10.1109/TMM.2015.2496872
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700009
DA 2024-07-18
ER

PT J
AU Shidanshidi, H
   Safaei, F
   Li, WQ
AF Shidanshidi, Hooman
   Safaei, Farzad
   Li, Wanqing
TI Estimation of Signal Distortion Using Effective Sampling Density for
   Light Field-Based Free Viewpoint Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Free viewpoint video; light field; rendering quality assessment
ID IMAGE; ACCURACY; METRICS
AB In a light field-based free viewpoint video (LF-based FVV) system, effective sampling density (ESD) is defined as the number of rays per unit area of the scene that has been acquired and is selected in the rendering process for reconstructing an unknown ray. This paper extends the concept of ESD and shows that ESD is a tractable metric that quantifies the joint impact of the imperfections of LF acquisition and rendering. By deriving and analyzing ESD for the commonly used LF acquisition and rendering methods, it is shown that ESD is an effective indicator determined by system parameters and can be used to directly estimate output video distortion without access to the ground truth. This claim is verified by extensive numerical simulations and comparison to PSNR. Furthermore, an empirical relationship between the output distortion (in PSNR) and the calculated ESD is established to allow direct assessment of the overall video distortion without an actual implementation of the system. A small scale subjective user study is also conducted which indicates a correlation of 0.91 between ESD and perceived quality.
C1 [Shidanshidi, Hooman; Safaei, Farzad; Li, Wanqing] Univ Wollongong, ICT Res Inst, Fac Engn & Informat Sci, Wollongong, NSW 2522, Australia.
C3 University of Wollongong
RP Shidanshidi, H (corresponding author), Univ Wollongong, ICT Res Inst, Fac Engn & Informat Sci, Wollongong, NSW 2522, Australia.
EM hooman@uow.edu.au; farzad@uow.edu.au; wanqing@uow.edu.au
RI Li, Wanqing/ABG-2620-2020
OI Safaei, Farzad/0000-0002-4322-4448; Li, Wanqing/0000-0002-4427-2687
CR Aaron I., 2000, 27 ANN C COMP GRAPH
   Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Agrawal A. K., 2010, U. S. Patent, Patent No. [US7965936 B2, 7965936B2]
   [Anonymous], 1999, document P.910, DOI 11.1002/1000/4751
   [Anonymous], ICME
   [Anonymous], P 5 AS C COMP VIS
   [Anonymous], 2001, T1TR722001
   Avcibas I, 2002, J ELECTRON IMAGING, V11, P206, DOI 10.1117/1.1455011
   Bosc E., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2597, DOI 10.1109/ICIP.2011.6116196
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Brandao T, 2006, IEEE IMAGE PROC, P2933, DOI 10.1109/ICIP.2006.313133
   Brill MH, 2004, SIGNAL PROCESS-IMAGE, V19, P101, DOI 10.1016/S0923-5965(03)00073-0
   Camahort E., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P117
   Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932
   Daniel N. W., 2000, 27 ANN C COMP GRAPH
   Do MN, 2012, IEEE T IMAGE PROCESS, V21, P708, DOI 10.1109/TIP.2011.2163895
   Ekmekcioglu E, 2011, IEEE J-STSP, V5, P352, DOI 10.1109/JSTSP.2010.2052783
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Han Z, 2007, INT CONF ACOUST SPEE, P773
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Kilner J, 2009, SIGNAL PROCESS-IMAGE, V24, P3, DOI 10.1016/j.image.2008.10.004
   Kim H, 2012, IEEE T CIRC SYST VID, V22, P1611, DOI 10.1109/TCSVT.2012.2202185
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li WF, 2009, IEEE T CIRC SYST VID, V19, P533, DOI 10.1109/TCSVT.2009.2014021
   Lin ZC, 2004, INT J COMPUT VISION, V58, P121, DOI 10.1023/B:VISI.0000015916.91741.27
   Liu SX, 2009, ELECTRON LETT, V45, P30, DOI 10.1049/el:20092399
   Lumsdaine A., 2008, Full resolution lightfield rendering
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Ng KT, 2012, IEEE T MULTIMEDIA, V14, P1631, DOI 10.1109/TMM.2012.2199291
   Pattinson T., 2010, DAGM YOUNG RES FOR, P1
   Pons AM, 1999, DISPLAYS, V20, P93, DOI 10.1016/S0141-9382(99)00009-8
   Scandarolli T, 2013, IEEE SIGNAL PROC LET, V20, P359, DOI 10.1109/LSP.2013.2246863
   Schwarz S, 2013, IEEE MULTIMEDIA, V20, P10, DOI 10.1109/MMUL.2013.53
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Seshadrinathan K, 2007, INT CONF ACOUST SPEE, P869
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shidanshidi H., 2011, ICME, P1
   Shidanshidi H., 2011, MMSP, P1
   Shum HY, 2004, ACM T GRAPHIC, V23, P143, DOI 10.1145/990002.990005
   Stewart J., 2003, EUR WORKSH REND LEUV
   Takahashi K, 2006, SIGNAL PROCESS-IMAGE, V21, P519, DOI 10.1016/j.image.2006.03.001
   Takahashi K, 2012, IEEE T IMAGE PROCESS, V21, P718, DOI 10.1109/TIP.2011.2172802
   Tanimoto M, 2012, SIGNAL PROCESS-IMAGE, V27, P555, DOI 10.1016/j.image.2012.02.016
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Wang QF, 2012, IEEE T CIRC SYST VID, V22, P875, DOI 10.1109/TCSVT.2011.2181229
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen W, 2010, INT CONF COMP SCI, P399, DOI 10.1109/ICCSIT.2010.5563964
   Winkler S, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P399, DOI 10.1109/ICIP.1998.999029
   Winkler S., 2007, P EUR SIGN PROC C, P3
   Yu JY, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P137, DOI 10.1109/PCCGA.2002.1167847
   Zhang C, 2003, IEEE T CIRC SYST VID, V13, P1038, DOI 10.1109/TCSVT.2003.817350
   Zhang C., 2006, SYNTHESIS LECT IMAGE, V2, P1
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 54
TC 25
Z9 25
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1677
EP 1693
DI 10.1109/TMM.2015.2447274
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400001
OA Green Published
DA 2024-07-18
ER

PT J
AU Kilanioti, I
AF Kilanioti, Irene
TI Improving Multimedia Content Delivery via Augmentation With Social
   Information: The Social Prefetcher Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content delivery networks (CDNs); internet measurements; social
   cascading; social video sharing; Twitter; YouTube
AB A significantly large proportion of HTTP traffic results from bandwidth-intensive multimedia content circulating through online social networks (OSNs). With multimedia content providers, such as YouTube, often relying on content delivery network (CDN) infrastructures, the pursuit lies in exploiting the user activity extracted from OSNs to improve the content prefetching mechanism. Aiming to reduce bandwidth usage, we incorporated a dynamic mechanism to CDNsim, a stand-alone CDN traffic simulator. This mechanism is based on a dynamic policy that takes patterns of information transmission over OSNs into account. Herein, we demonstrate that the performance of CDNs can be improved, and the cost of copying to surrogate servers is taken into consideration.
C1 Univ Cyprus, Dept Comp Sci, CY-2109 Nicosia, Cyprus.
C3 University of Cyprus
RP Kilanioti, I (corresponding author), Univ Cyprus, Dept Comp Sci, CY-2109 Nicosia, Cyprus.
EM irenekilanioti@gmail.com
RI Kilanioti, Irene/HHZ-0463-2022
OI Koilanioti, Eirini/0000-0002-4157-3900
FU Greek State Scholarships Foundation
FX The author is a Ph.D. candidate at the University of Cyprus supported by
   the Greek State Scholarships Foundation (Lifelong Learning Programme,
   Customized Evaluation Process). For the development of algorithms and to
   conduct the accompanying experiments, the Grid and Cloud infrastructure
   of the Department of Computer Science, University of Cyprus, as well as
   Amazon Web Services, were used. The author thanks K. Stamos for his
   discussions on CDNsim simulation tool logic.
CR Abhari A, 2010, MULTIMED TOOLS APPL, V46, P91, DOI 10.1007/s11042-009-0309-5
   [Anonymous], 2011, P 20 INT C WORLD WID
   [Anonymous], P 2 ACM EUROSYS WORK
   Bakshy E., 2012, P 21 INT C WORLD WID, P519
   Brodersen Anders, 2012, P 21 INT C WORLD WID, P241, DOI DOI 10.1145/2187836.2187870
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chard Kyle, 2010, 2010 IEEE 3rd International Conference on Cloud Computing (CLOUD 2010), P99, DOI 10.1109/CLOUD.2010.28
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Christodoulou G., 2012, P 13 INT C WEB INFOR, P426
   Dimou P., 2013, THESIS U CYPRUS NICO
   Easley D., 2010, Networks, Crowds, and Markets: Reasoning about a highly connected world, V8
   Figueiredo Flavio, 2011, P 4 ACM INT C WEB SE, P745, DOI DOI 10.1145/1935826.1935925
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Gill Z. L. P., 2008, P ACM SPIE MULT COMP
   Gupta Pankaj, 2013, P 22 INT C WORLD WID, P505, DOI DOI 10.1145/2488388.2488433
   Huang C, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Jacobson V, 2012, COMMUN ACM, V55, P117, DOI 10.1145/2063176.2063204
   Kilanioti I, 2014, WILEY SER PARA DIST, P449
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   Korn A, 2009, PHYSICA A, V388, P2221, DOI 10.1016/j.physa.2009.02.013
   Mitra S, 2011, ACM T WEB, V5, DOI 10.1145/1961659.1961662
   Myers S, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P913, DOI 10.1145/2566486.2568043
   Rodrigues Tiago., 2011, Proceedings of the 2011 ACM Special Interest Group on Data Communication Conference on Internet Measurement
   Scellato S., 2011, Proceedings of the 20th International Conference on World Wide Web, P457
   Stamos K, 2010, ACM T MODEL COMPUT S, V20, DOI 10.1145/1734222.1734226
   Torres R, 2011, INT CON DISTR COMP S, P248, DOI 10.1109/ICDCS.2011.43
   Traverso S., 2012, Proceedings of the 21st International Conference on World Wide Web, P151
NR 28
TC 12
Z9 12
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1460
EP 1470
DI 10.1109/TMM.2015.2459658
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000007
DA 2024-07-18
ER

PT J
AU Lu, SP
   Ceulemans, B
   Munteanu, A
   Schelkens, P
AF Lu, Shao-Ping
   Ceulemans, Beerend
   Munteanu, Adrian
   Schelkens, Peter
TI Spatio-Temporally Consistent Color and Structure Optimization for
   Multiview Video Color Correction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color correction; energy minimization; laplacian matrices; multiview
   video; spatio-temporal matching; structure preservation
ID IMAGE; SYSTEMS
AB When compared to conventional 2-D video, multiview video can significantly enhance the visual 3-D experience in 3-D applications by offering horizontal parallax. However, when processing images originating from different views, it is common that the colors between the different cameras are not well-calibrated. To solve this problem, a novel energy function-based color correction method for multiview camera setups is proposed to enforce that colors are as close as possible to those in the reference image but also that the overall structural information is well-preserved. The proposed system introduces a spatio-temporal correspondence matching method to ensure that each pixel in the input image gets bijectively mapped to a reference pixel. By combining this mapping with the original structural information, we construct a global optimization algorithm in a Laplacian matrix formulation and solve it using a sparse matrix solver. We further introduce a novel forward-reverse objective evaluation model to overcome the problem of lack of ground truth in this field. The visual comparisons are shown to outperform state-of-the-art multiview color correction methods, while the objective evaluation reports PSNR gains of up to 1.34 dB and SSIM gains of up to 3.2%, respectively.
C1 [Lu, Shao-Ping; Ceulemans, Beerend; Munteanu, Adrian; Schelkens, Peter] Vrije Univ Brussel, Dept Elect & Informat ETRO, B-1050 Brussels, Belgium.
C3 Vrije Universiteit Brussel
RP Lu, SP (corresponding author), Vrije Univ Brussel, Dept Elect & Informat ETRO, B-1050 Brussels, Belgium.
EM splu@etro.vub.ac.be; bceulema@etro.vub.ac.be; acmuntea@etro.vub.ac.be;
   pschelke@etro.vub.ac.be
RI Munteanu, Adrian/HKO-9955-2023; Schelkens, Peter/B-7831-2008
OI Munteanu, Adrian/0000-0001-7290-0428; Schelkens,
   Peter/0000-0003-0908-1655
FU iMinds visualization research program (HIVIZ), iMinds vzw; IWT under the
   ASPRO+ project
FX Manuscript received June 26, 2014; revised December 20, 2014 and
   February 19, 2015; accepted March 06, 2015. Date of publication March
   16, 2015; date of current version April 15, 2015. This work was
   supported by the iMinds visualization research program (HIVIZ), iMinds
   vzw, and by the IWT under the ASPRO+ project. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Chia-Wen Lin.
CR [Anonymous], 2006, JVTU052R2 ISOIEC MPE
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Boxin Shi, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P43
   Chen JY, 2014, IEEE INT CON MULTI
   Chen YS, 2006, INT C PATT RECOG, P734
   Doutre C, 2009, IEEE T CIRC SYST VID, V19, P1400, DOI 10.1109/TCSVT.2009.2022780
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fecker U., 2006, P PICT COD S BEIJ CH, P1
   Fecker U, 2008, IEEE T CIRC SYST VID, V18, P1258, DOI 10.1109/TCSVT.2008.926997
   Fezza SA, 2014, IEEE T CIRC SYST VID, V24, P1486, DOI 10.1109/TCSVT.2014.2309776
   Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224
   Hekstra AP, 2002, SIGNAL PROCESS-IMAGE, V17, P781, DOI 10.1016/S0923-5965(02)00056-5
   Hur JH, 2007, IEEE T CIRC SYST VID, V17, P1496, DOI 10.1109/TCSVT.2007.903774
   Ilie A, 2005, IEEE I CONF COMP VIS, P1268, DOI 10.1109/ICCV.2005.88
   Kim W, 2013, IEEE INT SYMP CIRC S, P2912, DOI 10.1109/ISCAS.2013.6572488
   Koenderink J, 2012, SIAM J IMAGING SCI, V5, P1213, DOI 10.1137/120861151
   Krishnan D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461992
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Lei YQ, 2012, IEEE T CIRC SYST VID, V22, P1332, DOI 10.1109/TCSVT.2012.2201670
   Li K, 2011, SIGNAL PROCESS-IMAGE, V26, P48, DOI 10.1016/j.image.2010.11.004
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Lu S., 2013, Proceedings of the International Conference on Image Processing, Computer Vision, and Pattern Recognition (IPCV). The Steering Committee of The World Congress in Computer Science, P1
   Mouffranc C., 2012, P AS C COMP VIS DAEJ, P206
   Polonen M., 2012, 3D Research, V3, P1
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Salmimaa M., 2014, P SID S DIG TECH PAP, V45, P801
   Shao F, 2007, INT CONF ACOUST SPEE, P969
   Shao F, 2010, J VIS COMMUN IMAGE R, V21, P392, DOI 10.1016/j.jvcir.2010.03.001
   Shi BX, 2009, IEEE INT CON MULTI, P65, DOI 10.1109/ICME.2009.5202437
   Shum H.-Y., 2006, IMAGE BASED RENDERIN
   Tehrani MP, 2010, J VIS COMMUN IMAGE R, V21, P377, DOI 10.1016/j.jvcir.2010.03.007
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang Q, 2011, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2011.6116722
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao XZ, 2009, COMPUT GRAPH FORUM, V28, P1879, DOI 10.1111/j.1467-8659.2009.01566.x
   Xu W, 2010, PROC CVPR IEEE, P263, DOI 10.1109/CVPR.2010.5540202
   Yamamoto K, 2007, IEEE T CIRC SYST VID, V17, P1436, DOI 10.1109/TCSVT.2007.903802
   Yamamoto K, 2008, INT J AUTOM COMPUT, V5, P234, DOI 10.1007/s11633-008-0234-5
   Zhong JH, 2014, IEEE SENS J, V14, P2955, DOI 10.1109/JSEN.2014.2319891
NR 40
TC 25
Z9 28
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 577
EP 590
DI 10.1109/TMM.2015.2412879
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Qadir, QM
   Kist, AA
   Zhang, ZW
AF Qadir, Qahhar Muhammad
   Kist, Alexander A.
   Zhang, Zhongwei
TI A Novel Traffic Rate Measurement Algorithm for Quality of
   Experience-Aware Video Admission Control
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement-based admission control (MBAC); optimization; QoE; video
ID BANDWIDTH ESTIMATION; NETWORKS; OPTIMIZATION; PERFORMANCE; SUPPORT;
   SCHEME; QOE
AB With the inevitable dominance of video traffic on the Internet, providing perceptually good video quality is becoming a challenging task. This is partly due to the bursty nature of video traffic, changing network conditions, and limitations of network transport protocols. This growth of video traffic has made quality of experience (QoE) of the end user the focus of the research community. In contrast, Internet service providers are concerned about maximizing revenue by accepting as many sessions as possible, as long as customers remain satisfied. However, there is still no entirely satisfactory admission algorithm for flows with variable rate. The trade-off between the number of sessions and perceived QoE can be optimized by exploiting the bursty nature of video traffic. This paper proposes a novel algorithm to determine the upper limit of the aggregate video rate that can exceed the available bandwidth without degrading the QoE of accepted video sessions. A parameter that defines the exceedable limit is defined. The proposed algorithm results in accepting more sessions without compromising the QoE of ongoing video sessions. Thus it contributes to the optimization of the QoE-Session trade-off in support of the expected growth of video traffic on the Internet.
C1 [Qadir, Qahhar Muhammad; Kist, Alexander A.] Univ So Queensland, Sch Mech & Elect Engn, Toowoomba, Qld 4350, Australia.
   [Qadir, Qahhar Muhammad] Salahaddin Univ, Dept Elect Engn, Hawler 44001, Iraq.
   [Zhang, Zhongwei] Univ So Queensland, Sch Agr Computat & Environm Sci, Toowoomba, Qld 4350, Australia.
C3 University of Southern Queensland; Salahaddin University; University of
   Southern Queensland
RP Qadir, QM (corresponding author), Univ So Queensland, Sch Mech & Elect Engn, Toowoomba, Qld 4350, Australia.
EM safeen.qadir@ieee.org; kist@ieee.org; zhongwei.zhang@usq.edu.au
RI Qadir, Qahhar Muhammad/A-7033-2016; Kist, Alexander/F-2798-2010
OI Qadir, Qahhar Muhammad/0000-0002-5702-8712; Kist,
   Alexander/0000-0001-9105-7050
CR Ammar D, 2012, C LOCAL COMPUT NETW, P537, DOI 10.1109/LCN.2012.6423672
   Ammar D, 2011, C LOCAL COMPUT NETW, P215, DOI 10.1109/LCN.2011.6115192
   [Anonymous], THESIS TU BERLIN BER
   [Anonymous], THESIS U CALIFORNIA
   [Anonymous], 2008, 2008 IEEE 68 VEH TEC
   [Anonymous], THESIS U GENT GENT
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], THESIS U POLITCNICA
   [Anonymous], VQEG MULT PROJ FIN R
   [Anonymous], THESIS U SCI TECHNOL
   [Anonymous], COMMENTS MEASUREMENT
   [Anonymous], THESIS NORWEGIAN U S
   [Anonymous], THESIS U CAMBRIDGE C
   [Anonymous], 5559 IETF TRUST RFC
   [Anonymous], 1997, ANOVA BASICS APPL ST, DOI DOI 10.1201/B15236
   [Anonymous], 2475 IETF RFC
   [Anonymous], MULTIMEDIA COMMUNICA
   [Anonymous], IPTVIL0050 ITUT FG
   Auge J., 2011, Proceedings of the 2011 23rd International Teletraffic Congress (ITC 2011), P206
   Breslau L., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1233, DOI 10.1109/INFCOM.2000.832506
   Brooks P, 2010, IEEE NETWORK, V24, P8, DOI 10.1109/MNET.2010.5430138
   Cavusoglu B, 2014, COMPUT COMMUN, V47, P34, DOI 10.1016/j.comcom.2014.04.008
   Cisco Systems Inc, 2014, CISC VIS NETW IND FO
   Gibbens RJ, 1997, TELETRAF SCI ENG, V2, P879
   Gross J, 2004, COMPUT COMMUN, V27, P1044, DOI 10.1016/j.comcom.2004.01.010
   Guerrero CD, 2010, COMPUT COMMUN, V33, P11, DOI 10.1016/j.comcom.2009.08.010
   Hamdaoui B, 2007, IEEE T WIREL COMMUN, V6, P4014, DOI 10.1109/TWC.2007.060155
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Jamin S, 1997, IEEE INFOCOM SER, P973, DOI 10.1109/INFCOM.1997.631035
   Jiang YM, 2005, 2005 NEXT GENERATION INTERNET NETWORKS, P318
   Khan A, 2010, IET COMMUN, V4, P1389, DOI 10.1049/iet-com.2009.0649
   Khan Asiya, 2009, Journal of Multimedia, V4, P228, DOI 10.4304/jmm.4.4.228-239
   Khan A., 2009, PROC IEEE INT C COMM, P1
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Kim D, 2012, IEEE T CONSUM ELECTR, V58, P374, DOI 10.1109/TCE.2012.6227436
   Latré S, 2011, IEEE COMMUN MAG, V49, P94, DOI 10.1109/MCOM.2011.6094011
   Latré S, 2011, J NETW SYST MANAG, V19, P32, DOI 10.1007/s10922-010-9183-8
   Latré S, 2009, COMPUT NETW, V53, P1587, DOI 10.1016/j.comnet.2008.11.004
   Lee G, 2014, IEEE COMMUN LETT, V18, P1995, DOI 10.1109/LCOMM.2014.2354409
   Li D, 2010, IEEE T WIREL COMMUN, V9, P338, DOI 10.1109/TWC.2010.01.090556
   Lima SR, 2007, IEEE COMMUN MAG, V45, P114, DOI 10.1109/MCOM.2007.343620
   Lübben R, 2014, IEEE ACM T NETWORK, V22, P484, DOI 10.1109/TNET.2013.2261914
   Ma XL, 2012, J SYST SOFTWARE, V85, P300, DOI 10.1016/j.jss.2011.08.016
   Menth M, 2012, IEEE ACM T NETWORK, V20, P422, DOI 10.1109/TNET.2012.2189415
   Nam SY, 2008, IEEE ACM T NETWORK, V16, P410, DOI 10.1109/TNET.2007.900403
   Nam SY, 2013, COMPUT NETW, V57, P61, DOI 10.1016/j.comnet.2012.08.015
   Nam SY, 2012, COMPUT COMMUN, V35, P431, DOI 10.1016/j.comcom.2011.11.011
   Nevin Anne, 2008, 2008 IEEE Symposium on Computers and Communications (ISCC), P1040, DOI 10.1109/ISCC.2008.4625655
   Papadimitriou P, 2007, COMPUT NETW, V51, P4377, DOI 10.1016/j.comnet.2007.06.018
   Piamrat K, 2009, PROCEEDINGS OF THE 2009 SIXTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: NEW GENERATIONS, VOLS 1-3, P1184, DOI 10.1109/ITNG.2009.121
   Qadir S, 2013, 2013 AUSTRALASIAN TELECOMMUNICATION NETWORKS AND APPLICATIONS CONFERENCE (ATNAC), P178, DOI 10.1109/ATNAC.2013.6705377
   Qiu JY, 2001, IEEE ACM T NETWORK, V9, P199, DOI 10.1109/90.917076
   Szigeti T., 2004, END TO END QOS NETWO
   Taher NC, 2014, COMPUT COMMUN, V39, P41, DOI 10.1016/j.comcom.2013.10.006
   Tommasi Franco, 2014, Journal of Communications, V9, P248
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wojcik R., 2013, 2013 International Conference on Computing, Networking and Communications (ICNC 2013), P922, DOI 10.1109/ICCNC.2013.6504213
   Xu Y, 2014, IEEE ACM T NETWORK, V22, P826, DOI 10.1109/TNET.2013.2260354
   Xu YW, 2013, 2013 IEEE EIGHTH INTERNATIONAL CONFERENCE ON INTELLIGENT SENSORS, SENSOR NETWORKS AND INFORMATION PROCESSING, P345, DOI 10.1109/ISSNIP.2013.6529814
   Zhang XG, 2013, IEEE T MULTIMEDIA, V15, P1446, DOI 10.1109/TMM.2013.2247988
NR 60
TC 15
Z9 15
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 711
EP 722
DI 10.1109/TMM.2015.2416637
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300012
OA Green Submitted, Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Ni, BB
   Xu, MD
   Nguyen, TV
   Wang, M
   Lang, CY
   Huang, ZY
   Yan, SC
AF Ni, Bingbing
   Xu, Mengdi
   Nguyen, Tam V.
   Wang, Meng
   Lang, Congyan
   Huang, Zhongyang
   Yan, Shuicheng
TI Touch Saliency: Characteristics and Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fixations; middle-level object category features; touch saliency; visual
   saliency
ID VISUAL-ATTENTION; SEARCH; MODEL; DATABASE; REGIONS
AB In this work, we propose an alternative ground truth to the eye fixation map in visual attention study, called touch saliency. As it can be directly collected from the recorded data of users' daily browsing behavior on widely used smart phone devices with touch screens, the touch saliency data is easy to obtain. Due to the limited screen size, smart phone users usually move and zoom in the images, and fix the region of interest on the screen when browsing images. Our studies are two-fold. First, we collect and study the characteristics of these touch screen fixation maps (named touch saliency) by comprehensive comparisons with their counterpart, the eye-fixation maps (namely, visual saliency). The comparisons show that the touch saliency is highly correlated with the eye fixations for the same stimuli, which indicates its utility in data collection for visual attention study. Based on the consistency between both touch saliency and visual saliency, our second task is to propose a unified saliency prediction model for both visual and touch saliency detection. This model utilizes middle-level object category features extracted from pre-segmented image superpixels as input to the recently proposed multitask sparsity pursuit (MTSP) framework for saliency prediction. Extensive evaluations show that the proposed middle-level category features can considerably improve the saliency prediction performance when taking both touch saliency and visual saliency as ground truth.
C1 [Ni, Bingbing] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Xu, Mengdi] Inst Infocomm Res I2R, Singapore 138632, Singapore.
   [Nguyen, Tam V.] Natl Univ Singapore, Singapore 119077, Singapore.
   [Wang, Meng] Hefei Univ Technol, Dept Elect & Comp Engn, Hefei 230009, Peoples R China.
   [Lang, Congyan] Beijing Jiaotong Univ, Dept Comp & Informat, Beijing 100044, Peoples R China.
   [Huang, Zhongyang] OmniVis Technol Singapore, Singapore 609935, Singapore.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); National University of Singapore; Hefei
   University of Technology; Beijing Jiaotong University; National
   University of Singapore
RP Ni, BB (corresponding author), Adv Digital Sci Ctr, Singapore 138632, Singapore.
EM bingbing.ni@adsc.com.sg; xumd@i2r.a-star.edu.sg; vantam@gmail.com;
   eric.mengwang@gmail.com; cylang@bjtu.edu.cn; zhongyanghuang@hotmail.com;
   eleyans@nus.edu.sg
RI Nguyen, Tam/HSG-3007-2023; Wang, Meng/ITR-8699-2023; Yan,
   Shuicheng/HCI-1431-2022; Xu, Meng/HJY-7139-2023; Nguyen,
   Tam/AAU-6504-2020
OI Nguyen, Tam/0000-0003-0236-7992
FU Singapore Ministry of Education [MOE2012-TIF-2-G-015]; National 973
   Program of China [2014CB347600]; NSFC [61272393, 61322201]; Program for
   New Century Excellent Talents in University [NCET-12-0836]; Open Project
   Program of the National Laboratory of Pattern Recognition (NLPR); Human
   Sixth Sense Programme at the Advanced Digital Sciences Center from
   Singapore's Agency for Science, Technology and Research (A*STAR)
FX This work was supported in part by the Singapore Ministry of Education
   under Grant MOE2012-TIF-2-G-015, by the National 973 Program of China
   under Grant 2014CB347600, by the NSFC under Grant 61272393 and Grant
   61322201, by the Program for New Century Excellent Talents in University
   under Grant NCET-12-0836, by the Open Project Program of the National
   Laboratory of Pattern Recognition (NLPR), and by a research grant for
   the Human Sixth Sense Programme at the Advanced Digital Sciences Center
   from Singapore's Agency for Science, Technology and Research (A*STAR).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shahram Shirani.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2009, IEEE I CONF COMP VIS, DOI 10.1109/ICCV.2009.5459175
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], ACM MULTIMEDIA
   Avraham T, 2010, IEEE T PATTERN ANAL, V32, P693, DOI 10.1109/TPAMI.2009.53
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Bragdon A., 2011, P INT C HUM FACT COM
   Bruce N., 2005, NIPS
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Carreira J., 2012, P EUR C COMP VIS
   Cerf M., 2008, ADV NEURAL INFORM PR, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ehinger KA, 2009, VIS COGN, V17, P945, DOI 10.1080/13506280902834720
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hong RC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037681
   Hu YQ, 2008, J VIS COMMUN IMAGE R, V19, P199, DOI 10.1016/j.jvcir.2007.11.001
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677
   Itti L., 1999, SPIE human vision and electronic imaging IV (HVEIaAZ99), V3644, P373
   Judd T., 2012, MIT CSAIL TR
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Lang CY, 2012, IEEE T IMAGE PROCESS, V21, P1327, DOI 10.1109/TIP.2011.2169274
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Navalpakkam V, 2007, NEURON, V53, P605, DOI 10.1016/j.neuron.2007.01.018
   Ouerhani N., 2004, ELECT LETT COMPUTER, V3, P13, DOI [10.5565/rev/elcvia.66, DOI 10.5565/REV/ELCVIA.66]
   Ramanathan S., 2010, P EUR C COMP VIS
   Setlur V., 2005, P INT C MOB UB MULT
   Shu G, 2013, PROC CVPR IEEE, P3721, DOI 10.1109/CVPR.2013.477
   Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   van der Linde I, 2009, SPATIAL VISION, V22, P161, DOI 10.1163/156856809787465636
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P330, DOI 10.1109/TMM.2010.2046364
   Xu M., 2012, ACM MULTIMEDIA
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
NR 45
TC 15
Z9 15
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1779
EP 1791
DI 10.1109/TMM.2014.2329275
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200024
DA 2024-07-18
ER

PT J
AU Schreiber, H
   Müller, M
AF Schreiber, Hendrik
   Mueller, Meinard
TI Accelerating Index-Based Audio Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio identification; content-based retrieval; fingerprint; indexing;
   music
AB In view of rapidly growing digital music collections and ubiquitous music consumption, the development of technologies for identifying, browsing, and managing audio content has become a major strand of research. In this context, audio identification (ID) systems for identifying audio recordings by means of short query audio clips have become of commercial relevance. In this paper, we take a closer look at a widely used audio ID system originally developed by Haitsma and Kalker and propose several modifications that yield significant improvements with regard to retrieval speed and storage requirements. As the main contribution, we introduce a measure that establishes a connection between the temporal correlation of hash values (used for indexing) and their ability to survive in the presence of noise and signal distortions. Based on this measure, we improve the overall performance of the audio ID system by means of four strategies. First, we change the way fingerprints (audio features) are generated to increase their reliability. Second, by prioritizing more reliable hash values when searching for reference entries, we achieve substantial gains in retrieval speed by a factor of almost seven. Third, by enlarging the query fingerprint, we increase our chances of identifying reliable hash values. Fourth, by indexing only the most reliable hashes, thus applying a sub-sampling strategy, we significantly lower the server side storage requirements by a factor of ten.
C1 [Schreiber, Hendrik] Tagtraum Ind Inc, Raleigh, NC 27608 USA.
   [Mueller, Meinard] Int Audio Labs Erlangen, D-91058 Erlangen, Germany.
C3 University of Erlangen Nuremberg
RP Schreiber, H (corresponding author), Tagtraum Ind Inc, Raleigh, NC 27608 USA.
EM hs@tagtraum.com; meinard.mueller@audiolabs-erlangen.de
RI Mueller, Meinard/U-2097-2019
OI Mueller, Meinard/0000-0001-6062-7524
CR [Anonymous], 2003, ISMIR
   [Anonymous], P 110 AES CONV AMST
   Apple Inc, 2012, APPL UNV NEW IT
   Apple Inc, IT MATCH
   Cano P, 2005, J VLSI SIG PROC SYST, V41, P271, DOI 10.1007/s11265-005-4151-3
   Cano P, 2005, STUD COMP INTELL, V2, P233
   Cano P., 2002, Audio Engineering Society Convention, V112, P1
   Casey M, 2007, INT CONF ACOUST SPEE, P1425
   Eerola T., 2009, P 10 INT SOC MUS INF, P621
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Gomez E., 2006, THESIS UPF BARCELONA
   Goto M., 2002, P ISMIR, V2
   Grosche P., 2012, Multimodal Music Processing, V3, P157
   Grosche P, 2012, INT CONF ACOUST SPEE, P473, DOI 10.1109/ICASSP.2012.6287919
   Haitsma J., 2002, P ISMIR 2002 3 INT C, P107
   Jang D., 2006, P 29 AUD ENG SOC C S, P38
   Kim Y.E., 2010, Proceedings of the 11th International Society for Music Information Retrieval Conference, P255
   Krumhansl C., 1990, OXFORD PSYCHOL SERIE
   Kurth F., 2002, P 112 AES CONV
   Kurth F, 2008, IEEE T AUDIO SPEECH, V16, P382, DOI 10.1109/TASL.2007.911552
   Marques G., 2011, P 12 INT SOC MUS INF, P795
   Mufin GmbH, AUD SERV
   Noland K., 2009, THESIS U LONDON LOND
   Rego D. P., 2012, SPOTIFY IS YOUR SAMS
   Scheirer ED, 1998, J ACOUST SOC AM, V103, P588, DOI 10.1121/1.421129
   Schreiber H., 2011, P 12 INT C MUS INF R, P127
   Serrà J, 2008, IEEE T AUDIO SPEECH, V16, P1138, DOI 10.1109/TASL.2008.924595
   SoundHound Inc, SOUNDH
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Wikipedia, LAST FM MUS CAT
   Wikipedia, CDDB
   Wikipedia, BITT
   YouTube LLC, CONT ID
NR 33
TC 4
Z9 4
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1654
EP 1664
DI 10.1109/TMM.2014.2318517
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200014
DA 2024-07-18
ER

PT J
AU Alexiadis, DS
   Daras, P
AF Alexiadis, Dimitrios S.
   Daras, Petros
TI Quaternionic Signal Processing Techniques for Automatic Evaluation of
   Dance Performances From MoCap Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dance analysis; motion capture data; quaternions; skeleton tracking;
   vector signal processing
ID HUMAN MOTION TRACKING; RECONSTRUCTION; POSE
AB In this paper, the problem of automatic dance performance evaluation from human Motion Capture (MoCap) data is addressed. A novel framework is presented, using data captured by Kinect-based human skeleton tracking, where the evaluation of user's performance is achieved against a gold-standard performance of a teacher. The framework addresses several technical challenges, including global and local temporal synchronization, spatial alignment and comparison of two "dance motion signals." Towards the solution of these technical challenges, a set of appropriate quaternionic vector-signal processing methodologies is proposed, where the 4D (spatiotemporal) human motion data are represented as sequences of pure quaternions. Such a quaternionic representation offers several advantages, including the facts that joint angles and rotations are inherently encoded in the phase of quaternions and the three coordinates variables (X, Y, Z) are treated jointly, with their intra-correlations being taken into account. Based on the theory of quaternions, a number of advantageous algorithms are formulated. Initially, global temporal synchronization of dance MoCap data is achieved by the use of quaternionic cross-correlations, which are invariant to rigid spatial transformations between the users. Secondly, a quaternions-based algorithm is proposed for the fast spatial alignment of dance MoCap data. Thirdly, the MoCap data can be temporally synchronized in a local fashion, using Dynamic Time Warping techniques adapted to the specific problem. Finally, a set of quaternionic correlation-based measures (scores) are proposed for evaluating and ranking the performance of a dancer. These quaternions-based scores are invariant to rigid transformations, as proved and demonstrated. A total score metric, through a weighted combination of three different metrics is proposed, where the weights are optimized using Particle Swarm Optimization (PSO). The presented experimental results using the Huawei/3DLife/EMC2 dataset are promising and verify the effectiveness of the proposed methods.
C1 [Alexiadis, Dimitrios S.; Daras, Petros] Ctr Res & Technol Hellas, Inst Informat Technol, GR-57001 Thessaloniki, Greece.
C3 Centre for Research & Technology Hellas
RP Alexiadis, DS (corresponding author), Ctr Res & Technol Hellas, Inst Informat Technol, GR-57001 Thessaloniki, Greece.
EM dalexiad@iti.gr; daras@iti.gr
RI Daras, Petros/F-5284-2012
OI Daras, Petros/0000-0003-3814-6710
FU European Commission [FP7-601170 RePlay]
FX This work was supported by the European Commission under contract
   FP7-601170 RePlay. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. K. Nahrstedt.
CR Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   Alexiadis DS, 2009, COMPUT VIS IMAGE UND, V113, P212, DOI 10.1016/j.cviu.2008.08.013
   Alexiadis DS, 2009, IEEE T IMAGE PROCESS, V18, P168, DOI 10.1109/TIP.2008.2007603
   Altun K, 2010, PATTERN RECOGN, V43, P3605, DOI 10.1016/j.patcog.2010.04.019
   [Anonymous], 2011, COMPUTER ANIMATION A
   Barbic J, 2004, PROC GRAPH INTERF, P185
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Camurri A., 1999, P INT C HUM ROB HURO, P9
   Chan JCP, 2011, IEEE T LEARN TECHNOL, V4, P187, DOI 10.1109/TLT.2010.27
   Chang IC, 2010, PATTERN RECOGN, V43, P3621, DOI 10.1016/j.patcog.2010.05.003
   Ell TA, 2007, IEEE T IMAGE PROCESS, V16, P22, DOI 10.1109/TIP.2006.884955
   Guo LQ, 2011, PATTERN RECOGN, V44, P187, DOI 10.1016/j.patcog.2010.08.017
   Hamilton W. R., 1901, ELEMENTS QUATERNIONS
   Horaud R, 2009, IEEE T PATTERN ANAL, V31, P158, DOI 10.1109/TPAMI.2008.108
   Izadi S., 2011, P ACM SIGGRAPH 2011
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Johnson M.P., 2003, Ph.D. Thesis
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Keogh E., 2002, Proceedings of the Twenty-eighth International Conference on Very Large Data Bases, P406
   Li C., 2007, ACM T MULTIM COMPUT, V3, P1
   Moxey CE, 2003, IEEE T SIGNAL PROCES, V51, P1941, DOI 10.1109/TSP.2003.812734
   Naveda L., 2008, ENACTIVE08 5 INT C E, P68
   Pennestrì E, 2010, ARCH MECH ENG, V57, P187, DOI 10.2478/v10180-010-0010-2
   Schwarz LA, 2012, PATTERN RECOGN, V45, P11, DOI 10.1016/j.patcog.2011.06.015
   Shiratori T, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P857, DOI 10.1109/AFGR.2004.1301641
   Tang J.K., 2011, P 5 INT C UB INF MAN, P1
   Vasudevan R, 2011, IEEE T MULTIMEDIA, V13, P573, DOI 10.1109/TMM.2011.2123871
   Wang LA, 2003, PATTERN RECOGN, V36, P585, DOI 10.1016/S0031-3203(02)00100-0
   Ward JA, 2006, IEEE T PATTERN ANAL, V28, P1553, DOI 10.1109/TPAMI.2006.197
   Zollhöfer M, 2011, COMPUT ANIMAT VIRT W, V22, P195, DOI 10.1002/cav.405
   Zou B, 2009, PATTERN RECOGN, V42, P1559, DOI 10.1016/j.patcog.2008.12.024
NR 31
TC 34
Z9 36
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1391
EP 1406
DI 10.1109/TMM.2014.2317311
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600020
DA 2024-07-18
ER

PT J
AU Han, YH
   Yang, Y
   Ma, ZG
   Shen, HQ
   Sebe, N
   Zhou, XF
AF Han, Yahong
   Yang, Yi
   Ma, Zhigang
   Shen, Haoquan
   Sebe, Nicu
   Zhou, Xiaofang
TI Image Attribute Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image attributes; domain adaptation; transfer learning; semi-supervised
   learning; multiple kernel learning; robust multiple kernel regression
ID OBJECTS
AB Visual attributes can be considered as a middle-level semantic cue that bridges the gap between low-level image features and high-level object classes. Thus, attributes have the advantage of transcending specific semantic categories or describing objects across categories. Since attributes are often human-nameable and domain specific, much work constructs attribute annotations ad hoc or take them from an application-dependent ontology. To facilitate other applications with attributes, it is necessary to develop methods which can adapt a well-defined set of attributes to novel images. In this paper, we propose a framework for image attribute adaptation. The goal is to automatically adapt the knowledge of attributes from a well-defined auxiliary image set to a target image set, thus assisting in predicting appropriate attributes for target images. In the proposed framework, we use a non-linear mapping function corresponding to multiple base kernels to map each training images of both the auxiliary and the target sets to a Reproducing Kernel Hilbert Space (RKHS), where we reduce the mismatch of data distributions between auxiliary and target images. In order to make use of un-labeled images, we incorporate a semi-supervised learning process. We also introduce a robust loss function into our framework to remove the shared irrelevance and noise of training images. Experiments on two couples of auxiliary-target image sets demonstrate that the proposed framework has better performance of predicting attributes for target testing images, compared to three baselines and two state-of-the-art domain adaptation methods.
C1 [Han, Yahong] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Han, Yahong] Tianjin Univ, Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300072, Peoples R China.
   [Yang, Yi; Zhou, Xiaofang] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
   [Ma, Zhigang; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, Trento, Italy.
   [Shen, Haoquan] Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Tianjin University; Tianjin University; University of Queensland;
   University of Trento; Zhejiang University
RP Han, YH (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM yahong@tju.edu.cn; yee.i.yang@gmail.com; ma@disi.unitn.it;
   shq422@zju.edu.cn; sebe@disi.unitn.it; zxf@itee.uq.edu.au
RI Ma, Zhigang/H-3543-2015; Sebe, Niculae/KEC-2000-2024; Yang,
   Yi/B-9273-2017; Lang, Ming/HIK-0758-2022; yang, yang/GWB-9426-2022;
   Zhou, Xiaofang/C-6169-2013; Zhou, Xiangfeng/KDO-8724-2024; yang,
   yang/GVT-5210-2022; yang, yang/HGT-7999-2022
OI Sebe, Niculae/0000-0002-6597-7248; Yang, Yi/0000-0002-0512-880X; Zhou,
   Xiaofang/0000-0001-6343-1455; 
FU NSFC [61202166]; National Program on the Key Basic Research Project
   [2013CB329301]; Doctoral Fund of Ministry of Education of China
   [20120032120042]; UQ Early Career Researcher (ECR) Grants Scheme
   [2013002401]
FX This work was supported in part by NSFC (under Grant 61202166), National
   Program on the Key Basic Research Project (under Grant 2013CB329301),
   and Doctoral Fund of Ministry of Education of China (under Grant
   20120032120042). The work of Y. Yang was supported in part by UQ Early
   Career Researcher (ECR) Grants Scheme (under Grant 2013002401). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Alan Hanjalic.
CR [Anonymous], P ACM MULT C MM 12 N
   [Anonymous], 2008, P ADV NEUR INF PROC
   [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], 2009, P CIKM
   Bach F., 2009, Proc. of Neural Information Processing Systems, P105
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Boyd S., 2004, CONVEX OPTIMIZATION
   Duan LX, 2010, PROC CVPR IEEE, P1959, DOI 10.1109/CVPR.2010.5539870
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Han YH, 2013, SIGNAL PROCESS, V93, P2169, DOI 10.1016/j.sigpro.2012.05.036
   Han YH, 2012, IEEE T IMAGE PROCESS, V21, P3066, DOI 10.1109/TIP.2012.2183880
   Hwang SJ, 2011, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2011.5995543
   Kulkarni G, 2011, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2011.5995466
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lewis D., 1991, HLT WORKSHOP SPEECH, P312, DOI [DOI 10.3115/112405.112471, 10.3115/112405.112471]
   Ma Z., 2012, MM 2012 - Proceedings of the 20th ACM International Conference on Multimedia, P469, DOI DOI 10.1145/2393347.2393414
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   OSHERSON DN, 1991, COGNITIVE SCI, V15, P251, DOI 10.1207/s15516709cog1502_3
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Saberian M. J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2929, DOI 10.1109/CVPR.2011.5995605
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yang JJ, 2012, IEEE T IMAGE PROCESS, V21, P2838, DOI 10.1109/TIP.2012.2183139
   Yang Y, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457456
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yu FX, 2012, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2012.6248023
   Yu XD, 2010, LECT NOTES COMPUT SC, V6315, P127
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
NR 34
TC 39
Z9 39
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1115
EP 1126
DI 10.1109/TMM.2014.2306092
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800018
DA 2024-07-18
ER

PT J
AU Huo, Y
   El-Hajjar, M
   Maunder, RG
   Hanzo, L
AF Huo, Yongkai
   El-Hajjar, Mohammed
   Maunder, Robert G.
   Hanzo, Lajos
TI Layered Wireless Video Relying on Minimum-Distortion Inter-Layer FEC
   Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layered video; inter-layer; unequal error protection; iterative
   detection; code rate optimization; video distortion
ID UNEQUAL ERROR PROTECTION; SCALABLE VIDEO; ALLOCATION ALGORITHM; CODED
   MODULATION; TRANSMISSION; CONVERGENCE; H.264/AVC; EXTENSION; DESIGN;
   BLOCK
AB Layered video coding is capable of progressively refining the reconstructed video quality with the aid of multiple layers of unequal importance. When the base layer (BL) is corrupted or lost due to channel impairments, the enhancement layers (ELs) must be discarded by the video decoder, regardless whether they are perfectly decoded or not, which implies that the transmission power assigned to the ELs is wasted. To circumvent this problem, we proposed a bit-level inter-layer forward error correction (IL-FEC) scheme for layered video transmission in our previous work, which implanted the systematic bits of the BL into the systematic bits of the ELs using exclusive-OR operations (XOR). This allowed the receiver to exploit the implanted bits of the ELs for assisting the BL's decoding and hence improved the overall system performance of our IL-FEC aided layered video scheme. In this treatise, we find the specific FEC coding rates in a real-time on-line fashion for the sake optimizing the overall system performance. The proposed procedure is widely applicable to diverse wireless transceivers and FEC codecs. Our simulation results show that the proposed optimized IL-FEC system outperforms the traditional optimal UEP by about 1.9 dB of E-b/N-0 at a peak signal-to-noise ratio (PSNR) of 38 dB. Viewing the improvements in terms of the video quality, 3.3 dB of PSNR improvement is attained at an of E-b/N-0 10 dB, when employing a recursive systematic convolutional (RSC) code.
C1 [Huo, Yongkai; El-Hajjar, Mohammed; Maunder, Robert G.; Hanzo, Lajos] Univ Southampton, Sch Elect & Comp Sci, Southampton SO9 5NH, Hants, England.
C3 University of Southampton
RP Huo, Y (corresponding author), Univ Southampton, Sch Elect & Comp Sci, Southampton SO9 5NH, Hants, England.
EM yh3g09@ecs.soton.ac.uk; meh@ecs.soton.ac.uk; rm@ecs.soton.ac.uk;
   lh@ecs.soton.ac.uk
RI Huo, Yongkai/F-6618-2015; Hanzo, Lajos/ITV-5242-2023; Hanzo,
   Lajos/S-4875-2016; El-Hajjar, Mohammed/S-5932-2016; Maunder, Robert
   G./S-4920-2016
OI Hanzo, Lajos/0000-0002-2636-5214; Hanzo, Lajos/0000-0002-2636-5214;
   El-Hajjar, Mohammed/0000-0002-7987-1401; Maunder, Robert
   G./0000-0002-7944-2615
FU EU's Concerto project, of the EPSRC under India-UK Advanced Technology
   Centre (IU-ATC); ERC; EPSRC [EP/L010550/1, EP/J015520/1, EP/J016640/1]
   Funding Source: UKRI
FX This work was supported by the EU's Concerto project, of the EPSRC under
   the auspices of the India-UK Advanced Technology Centre (IU-ATC) and
   that of the ERC's Advanced Fellow Grant. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Wenwu Zhu.
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   [Anonymous], 2010, JOINT VID TEAM JVT I
   [Anonymous], 2007, VIDEO COMPRESSION CO
   Aydinlik M, 2008, IEEE T COMMUN, V56, P555, DOI 10.1109/TCOMM.2008.050323
   BERROU C, 1993, IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS 93 : TECHNICAL PROGRAM, CONFERENCE RECORD, VOLS 1-3, P1064, DOI 10.1109/ICC.1993.397441
   Brüggen T, 2005, IEEE COMMUN LETT, V9, P484, DOI 10.1109/LCOMM.2005.1437345
   Chang YC, 2008, IEEE T CONSUM ELECTR, V54, P1066, DOI 10.1109/TCE.2008.4637589
   Chang YC, 2009, IEEE T CONSUM ELECTR, V55, P1089, DOI 10.1109/TCE.2009.5277961
   Chen JH, 2005, IEEE T COMMUN, V53, P1288, DOI 10.1109/TCOMM.2005.852852
   Clerc M, 2002, IEEE T EVOLUT COMPUT, V6, P58, DOI 10.1109/4235.985692
   Detti A, 2009, IEEE SYMP COMP COMMU, P1017
   Gong C, 2011, IEEE T COMMUN, V59, P1019, DOI 10.1109/TCOMM.2011.020411.090611
   Ha H, 2008, IEEE T CONSUM ELECTR, V54, P736, DOI 10.1109/TCE.2008.4560155
   HAGENAUER J, 1988, IEEE T COMMUN, V36, P389, DOI 10.1109/26.2763
   Hagenauer J, 1996, IEEE T INFORM THEORY, V42, P429, DOI 10.1109/18.485714
   Halloush M, 2011, IEEE T WIREL COMMUN, V10, P466, DOI 10.1109/TWC.2011.120810.090280
   Hanzo L., 2011, Turbo coding, turbo equalisation and space-time coding: EXIT-chart-aided near-capacity designs for wireless channels
   Hellge C, 2008, IEEE ICC, P480, DOI 10.1109/ICC.2008.95
   Hellge C, 2011, IEEE T MULTIMEDIA, V13, P551, DOI 10.1109/TMM.2011.2129499
   Huo YK, 2013, IEEE T CIRC SYST VID, V23, P1622, DOI 10.1109/TCSVT.2013.2254911
   Imaizumi H., 2002, MPEG 2 MULTIVIEW PRO, P169
   Khalili R, 2005, Proceedings of the 3rd Annual Communication Networks and Services Research Conference, P333
   Kumar V, 2006, IEEE T COMMUN, V54, P994, DOI 10.1109/TCOMM.2006.876842
   Maani E, 2010, IEEE T CIRC SYST VID, V20, P407, DOI 10.1109/TCSVT.2009.2035846
   Marx F, 2004, SIGNAL PROCESS-IMAGE, V19, P313, DOI 10.1016/j.image.2003.11.002
   MASNICK B, 1967, IEEE T INFORM THEORY, V13, P600, DOI 10.1109/TIT.1967.1054054
   Maunder RG, 2011, IEEE T VEH TECHNOL, V60, P762, DOI 10.1109/TVT.2011.2106809
   Nasruminallah, 2012, IEEE COMMUN SURV TUT, V14, P538, DOI 10.1109/SURV.2011.032211.00118
   Ng SX, 2005, IEE P-COMMUN, V152, P1116, DOI 10.1049/ip-com:20050236
   Nguyen K, 2010, J SIGNAL PROCESS SYS, V59, P319, DOI 10.1007/s11265-009-0342-7
   Pavlushkov V, 2006, IEEE T INFORM THEORY, V52, P700, DOI 10.1109/TIT.2005.862122
   Rahnavard N, 2006, IEEE COMMUN LETT, V10, P43, DOI 10.1109/LCOMM.2006.1576564
   Rahnavard N, 2007, IEEE T COMMUN, V55, P387, DOI 10.1109/TCOMM.2007.892436
   Schonfeld D, 2006, IEEE SIGNAL PROC LET, V13, P485, DOI 10.1109/LSP.2006.873142
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sejdinovic D, 2009, IEEE T COMMUN, V57, P2510, DOI 10.1109/TCOMM.2009.09.070616
   Stockhammer T, 2003, IEEE T CIRC SYST VID, V13, P657, DOI 10.1109/TCSVT.2003.815167
   Stockhammer T, 2008, INTERNET COMMUN, P239
   ten Brink S, 2001, IEEE T COMMUN, V49, P1727, DOI 10.1109/26.957394
   ten Brink S, 2004, IEEE T COMMUN, V52, P670, DOI 10.1109/TCOMM.2004.826370
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   Wang HH, 2005, IEEE T CIRC SYST VID, V15, P1505, DOI 10.1109/TCSVT.2005.857305
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Yang F, 2004, IEEE J SEL AREA COMM, V22, P777, DOI 10.1109/JSAC.2004.826008
   Yang XK, 2003, SIGNAL PROCESS-IMAGE, V18, P157, DOI 10.1016/S0923-5965(02)00128-5
   Zhan ZH, 2009, IEEE T SYST MAN CY B, V39, P1362, DOI 10.1109/TSMCB.2009.2015956
   Zhang TT, 1999, IEEE T BROADCAST, V45, P243, DOI 10.1109/11.796266
NR 48
TC 21
Z9 22
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 697
EP 710
DI 10.1109/TMM.2014.2300449
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500011
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Hsiao, CC
   Chu, SL
   Hsieh, CC
AF Hsiao, Chih-Chieh
   Chu, Slo-Li
   Hsieh, Chiu-Cheng
TI An Adaptive Thread Scheduling Mechanism With Low-Power Register File for
   Mobile GPUs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graphics hardware; mobile GPU; register file; thread scheduling
ID LATENCY; SHADER
AB In response to the remarkable increase in 3D applications in consumer electronics devices in recent years, graphics processing units (GPUs) have become widely available on mobile devices. These GPUs typically use hardware multithreaded shaders to improve their throughputs for real-time rendering, but they depend on duplicate register files to maintain the context of each hardware thread, increasing power consumption. However, the register usage of shading programs is often relatively low, which causes many registers to remain unused, thus wasting power. Long latency memory operations can also consume unnecessary power to activate registers. This study proposes a low-power register file with multiple power modes to reduce the power consumption of the register file. This study also presents an adaptive thread scheduling mechanism to achieve a tradeoff between the power consumption of the register file and frames per second (FPS). Results show that the average performance degradation from the proposed low-power register file is only 0.62%. The proposed adaptive thread scheduling has average under prediction ratio of 3.32%. The leakage reduction of the proposed low-power register file is 74.80%. This reduction can be improved to 81.49%, 82.22%, and 84.28% with adaptive thread scheduling at frame rates of 30, 25, and 20, respectively.
C1 [Hsiao, Chih-Chieh; Chu, Slo-Li; Hsieh, Chiu-Cheng] Chung Yuan Christian Univ, Dept Informat & Comp Engn, Chungli 32023, Taiwan.
C3 Chung Yuan Christian University
RP Hsiao, CC (corresponding author), Chung Yuan Christian Univ, Dept Informat & Comp Engn, Chungli 32023, Taiwan.
EM slchu@cycu.edu.tw
FU National Science Council of Republic of China, Taiwan [NSC
   101-2221-E-033-049]
FX This work was supported in part by the National Science Council of
   Republic of China, Taiwan under Grant NSC 101-2221-E-033-049. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yianis Andreopoulos.
CR Akenine-Möller T, 2008, P IEEE, V96, P779, DOI 10.1109/JPROC.2008.917719
   [Anonymous], SHAD MOD 3 SPEC
   [Anonymous], HPL200820
   Beigbeder Tom., 2004, NETGAMES 04, P144
   Borkar S, 1999, IEEE MICRO, V19, P23, DOI 10.1109/40.782564
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Flautner K, 2002, CONF PROC INT SYMP C, P148, DOI 10.1109/ISCA.2002.1003572
   Gebhart M, 2011, ISCA 2011: PROCEEDINGS OF THE 38TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P235, DOI 10.1145/2024723.2000093
   Ghinea G., 1998, Proceedings ACM Multimedia 98, P49, DOI 10.1145/290747.290754
   Gu Y, 2008, I CONF VLSI DESIGN, P679, DOI 10.1109/VLSI.2008.102
   Kaxiras S, 2005, ISLPED '05: PROCEEDINGS OF THE 2005 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P54, DOI 10.1109/LPE.2005.195485
   Mochocki B., 2006, PROC DESIGN AUTOMATI, P1
   Del Barrio VM, 2006, INT SYM PERFORM ANAL, P231
   Powell M, 2000, ISLPED '00: PROCEEDINGS OF THE 2000 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P90, DOI 10.1109/LPE.2000.876763
   Regan MJP, 1999, COMP GRAPH, P287, DOI 10.1145/311535.311569
   Slo-Li Chu, 2011, 2011 Proceedings of IEEE/IFIP 9th International Conference on Embedded and Ubiquitous Computing (EUC 2011), P166, DOI 10.1109/EUC.2011.15
   Sohn JH, 2006, IEEE J SOLID-ST CIRC, V41, P1081, DOI 10.1109/JSSC.2006.872869
   Wang PH, 2009, IEEE COMPUT ARCHIT L, V8, P9, DOI 10.1109/L-CA.2009.1
   Watson B, 2005, COMPUTER, V38, P54, DOI 10.1109/MC.2005.274
   Yu WKS, 2011, ISCA 2011: PROCEEDINGS OF THE 38TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P247, DOI 10.1145/2024723.2000094
NR 20
TC 7
Z9 8
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 60
EP 67
DI 10.1109/TMM.2013.2281584
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100006
DA 2024-07-18
ER

PT J
AU Xu, HT
   Zhai, GT
   Wu, XL
   Yang, XK
AF Xu, Hongteng
   Zhai, Guangtao
   Wu, Xiaolin
   Yang, Xiaokang
TI Generalized Equalization Model for Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrast enhancement; contrast gain; generalized equalization;
   nonlinearity of transform; tone mapping; white balancing
ID CONTRAST ENHANCEMENT; HISTOGRAM; GRAY
AB In this paper, we propose a generalized equalization model for image enhancement. Based on our analysis on the relationships between image histogram and contrast enhancement/white balancing, we first establish a generalized equalization model integrating contrast enhancement and white balancing into a unified framework of convex programming of image histogram. We show that many image enhancement tasks can be accomplished by the proposed model using different configurations of parameters. With two defining properties of histogram transform, namely contrast gain and nonlinearity, the model parameters for different enhancement applications can be optimized. We then derive an optimal image enhancement algorithm that theoretically achieves the best joint contrast enhancement and white balancing result with trading-off between contrast enhancement and tonal distortion. Subjective and objective experimental results show favorable performances of the proposed algorithm in applications of image enhancement, white balancing and tone correction. Computational complexity of the proposed method is also analyzed.
C1 [Xu, Hongteng; Zhai, Guangtao; Wu, Xiaolin; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
   [Wu, Xiaolin] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8G 4K1, Canada.
C3 Shanghai Jiao Tong University; McMaster University
RP Xu, HT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
EM hongtengxu@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn; xwu@ece.mcmaster.ca;
   xkyang@sjtu.edu.cn
RI Xu, Hongteng/AAB-1636-2021; Yang, Xiaokang/C-6137-2009; Zhai,
   Guangtao/X-5949-2019
OI Yang, Xiaokang/0000-0003-4029-3322; Zhai, Guangtao/0000-0001-8165-9322
FU NSFC [60932006, 61025005, 61001145, 61129001, 61221001, 61371146]; 111
   Project [B07022]
FX This work was supported in part by NSFC (60932006, 61025005, 61001145,
   61129001, 61221001, 61371146) and the 111 Project (B07022).. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xiao-Ping Zhang.
CR [Anonymous], ACM T GRAPH P ACM SI
   [Anonymous], P INT C STER ICS 07
   [Anonymous], P COMP GRAPH FOR
   [Anonymous], J FRANK I
   [Anonymous], P SPIE
   [Anonymous], P IS T 18 COL IM C
   [Anonymous], ACM T GRAPH P ACM SI
   [Anonymous], J DIGITAL IMAG
   [Anonymous], IEEE T IMAGE PROCESS
   [Anonymous], 2010, P ECCV
   [Anonymous], 2008, 2008 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2008.4587765
   [Anonymous], ACM T GRAPH P ACM SI
   [Anonymous], ACM T GRAPH P ACM SI
   [Anonymous], P 13 EUR WORKSH REND
   [Anonymous], 2009, TECHNICAL REPORT
   [Anonymous], 1998, P ICCV
   [Anonymous], P IS T SIDS COL IM C
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Barnard K, 2002, COLOR RES APPL, V27, P147, DOI 10.1002/col.10049
   Chen ZY, 2006, IEEE T IMAGE PROCESS, V15, P2303, DOI 10.1109/TIP.2006.875201
   Coltuc D, 2006, IEEE T IMAGE PROCESS, V15, P1143, DOI 10.1109/TIP.2005.864170
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Farid H, 2001, IEEE T IMAGE PROCESS, V10, P1428, DOI 10.1109/83.951529
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Finlayson G, 1998, PROC CVPR IEEE, P60, DOI 10.1109/CVPR.1998.698588
   Finlayson G. D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P835, DOI 10.1109/ICCV.1999.790308
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   FORSYTH DA, 1990, INT J COMPUT VISION, V5, P5, DOI 10.1007/BF00056770
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Han JH, 2011, IEEE T IMAGE PROCESS, V20, P506, DOI 10.1109/TIP.2010.2068555
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329
   Pu YF, 2010, IEEE T IMAGE PROCESS, V19, P491, DOI 10.1109/TIP.2009.2035980
   Shi YH, 2007, IEEE IMAGE PROC, P529
   Starck JL, 2003, IEEE T IMAGE PROCESS, V12, P706, DOI 10.1109/TIP.2003.813140
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Tan R., 2008, PROC IEEE INT C COMP, P1
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   van de Weijer J., 2007, PROC IEEE INT C COMP, P1
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wu XL, 2011, IEEE T IMAGE PROCESS, V20, P1262, DOI 10.1109/TIP.2010.2092438
   Zhu X., 2011, IEEE Workshop on Applications of Computer Vision WACV, P103
NR 44
TC 103
Z9 116
U1 2
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 68
EP 82
DI 10.1109/TMM.2013.2283453
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100007
DA 2024-07-18
ER

PT J
AU Luo, ZY
   Song, L
   Zheng, SB
   Ling, N
AF Luo, Zhengyi
   Song, Li
   Zheng, Shibao
   Ling, Nam
TI Raptor Codes Based Unequal Protection for Compressed Video According to
   Packet Priority
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Raptor codes; unequal error protection; visual communications
ID TRANSMISSION; BROADCAST
AB Raptor codes are state-of-the-art forward error correction (FEC) solutions for multimedia transmission, which have been applied to unequal error protection (UEP) of multi-layered media such as scalable video coding. In this paper, we address the problem of UEP for single-layered video over packet erasure channels. By exploiting the different priorities of video packets inside a group of pictures (GOP) and making full use of the good characteristics of standardized Raptor codes at large block length, we propose an optimized UEP framework for single-layered video and develop an efficient algorithm to solve it. Simulation results show that significant gains can be obtained by our method in case of packet losses.
C1 [Luo, Zhengyi; Song, Li; Zheng, Shibao] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200030, Peoples R China.
   [Ling, Nam] Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
C3 Shanghai Jiao Tong University; Santa Clara University
RP Luo, ZY (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200030, Peoples R China.
EM llzzyynjupt@sjtu.edu.cn; song_li@sjtu.edu.cn; sbzh@sjtu.edu.cn;
   nling@scu.edu
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   [Anonymous], 26346 3GPP TS
   Cataldi P, 2010, IEEE T IMAGE PROCESS, V19, P1491, DOI 10.1109/TIP.2010.2042985
   Eisenberg Y, 2002, IEEE T CIRC SYST VID, V12, P411, DOI 10.1109/TCSVT.2002.800309
   FANG T, 2005, P IEEE INT S CIRC SY
   Gallager R. G., 1968, INFORM THEORY RELIAB
   Goshi J, 2005, IEEE T CIRC SYST VID, V15, P412, DOI 10.1109/TCSVT.2004.842613
   Ha H, 2010, IEEE T CIRC SYST VID, V20, P1187, DOI 10.1109/TCSVT.2010.2051368
   Hellge C, 2011, IEEE T MULTIMEDIA, V13, P551, DOI 10.1109/TMM.2011.2129499
   LUBY M, 2006, P IEEE CONS COMM NET
   Luby M, 2007, IEEE T BROADCAST, V53, P235, DOI 10.1109/TBC.2007.891703
   MacKay DJC, 2005, IEE P-COMMUN, V152, P1062, DOI 10.1049/ip-com:20050237
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tillo T, 2011, IEEE T IMAGE PROCESS, V20, P1572, DOI 10.1109/TIP.2010.2095865
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   Wu Weiling, 1998, Acta Electronica Sinica, V26, P35
   Yang XK, 2005, IEEE T MULTIMEDIA, V7, P753, DOI 10.1109/TMM.2005.846782
   Zhang F, 2006, IEEE T MULTIMEDIA, V8, P1005, DOI 10.1109/TMM.2006.879865
   Zhang YF, 2011, IEEE INT CON MULTI, DOI 10.1109/ICME.2011.6011931
   Zhang YF, 2010, IEEE T MULTIMEDIA, V12, P1, DOI 10.1109/TMM.2009.2036290
NR 21
TC 17
Z9 19
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2208
EP 2213
DI 10.1109/TMM.2013.2280561
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900039
DA 2024-07-18
ER

PT J
AU Zhou, YP
   Fu, TZJ
   Chiu, DM
   Huang, Y
AF Zhou, Yipeng
   Fu, Tom Z. J.
   Chiu, Dah Ming
   Huang, Yan
TI An Adaptive Cloud Downloading Service
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud server; file downloading; helper; peer-to-peer; video
AB Video content downloading using the P2P approach is scalable, but does not always give good performance. Recently, subscription-based premium services have emerged, referred to as cloud downloading. In this service, the cloud storage and server caches user-interested content and updates the cache based on user downloading requests. If a requested video is not in the cache, the request is held in awaiting state until the cache is updated. We call this design server mode. An alternative design is to let the cloud server serve all downloading requests as soon as they arrive, behaving as a helper peer. We call this design helper mode. Our model and analysis show that both these designs are useful for certain operating regimes. The helper mode is good at handling a high request rate, while the server mode is good at scaling with video population size. We design an adaptive algorithm (AMS) to select the service mode automatically. Intuitively, AMS switches service mode from server mode to helper mode when too many peers request blocked movies, and vice versa. The ability of AMS to achieve good performance in different operating regimes is validated by simulation.
C1 [Zhou, Yipeng; Fu, Tom Z. J.; Chiu, Dah Ming] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong 00852, Hong Kong, Peoples R China.
   [Huang, Yan] Tencent Res, Shanghai 518057, Peoples R China.
C3 Chinese University of Hong Kong; Tencent
RP Zhou, YP (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong 00852, Hong Kong, Peoples R China.
EM zyp009@ie.cuhk.edu.hk; zjfu6@ie.cuhk.edu.hk; dmchiu@ie.cuhk.edu.hk;
   gale-huang@qq.com
RI Chiu, Dah Ming/F-1885-2011
OI Zhou, Yipeng/0000-0003-1533-0865
FU HK RGC [CUHK411611]
FX This work was supported by HK RGC under Grant CUHK411611. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Chang Wen Chen.
CR [Anonymous], P ACM SIGC
   [Anonymous], P 5 WSEAS INT C APPL
   [Anonymous], P INT PAR DISTR PROC
   [Anonymous], P WORKSH INF ALG PRO
   [Anonymous], MSRTR2004
   Chiu D.M., 2006, PROC 2 WORKSHOP NETW
   Fan B, 2006, PROCEEDINGS OF THE 2006 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P233
   Huang Y, 2011, P 19 ACM INT C MULT, P213
   Lin MH, 2007, PERFORM EVALUATION, V64, P856, DOI 10.1016/j.peva.2007.06.006
   Massoulie L., 2005, Performance Evaluation Review, V33, P2, DOI 10.1145/1071690.1064215
   Massoulie L, 2007, IEEE INFOCOM SER, P1073, DOI 10.1109/INFCOM.2007.129
   Mundinger J, 2008, J SCHEDULING, V11, P105, DOI 10.1007/s10951-007-0017-9
   Venkatasubramanian N, 1997, INT CON DISTR COMP S, P528, DOI 10.1109/ICDCS.1997.603406
NR 13
TC 7
Z9 9
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 802
EP 810
DI 10.1109/TMM.2013.2239628
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500009
DA 2024-07-18
ER

PT J
AU Koo, HI
AF Koo, Hyung Il
TI Segmentation and Rectification of Pictures in the Camera-Captured Images
   of Printed Documents
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Boundary interpolation; document image processing; image segmentation;
   picture rectification
ID SHAPE; EXTRACTION; SURFACES
AB This paper presents an algorithm that segments and rectifies pictures in camera-captured document images. Most of the conventional methods for this purpose require the 3-D shape of document surface, which are usually measured or inferred by a depth-measuring device, structured light, or stereo system. Unlike these methods, our method requires only a single-view image and a user-provided rough bounding box on the picture. Hence, the main features of the proposed algorithm are simple user interaction and short processing time: a mega-pixel size image can be segmented and rectified within 1-2 s, on receiving the user's bounding box. To achieve this goal, we develop a novel boundary extraction algorithm that exploits the specific properties of printed material. In the method, a set of boundary candidates is generated, and the optimal boundary is found by using an alternating optimization scheme. In addition to the segmentation method, we also propose a new rectification method, which can largely remove perspective distortions. Experimental results on a variety of images show that our method is efficient, robust, and easy to use.
C1 Ajou Univ, Div Elect & Comp Engn, Suwon 443749, South Korea.
C3 Ajou University
RP Koo, HI (corresponding author), Ajou Univ, Div Elect & Comp Engn, Suwon 443749, South Korea.
EM hikoo@ajou.ac.kr
FU Ajou University
FX This work was supported in part by the new Faculty Research Fund of Ajou
   University. The associate editor coordinating the review of this
   manuscript and approving it for publication was Ton Kalker.
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P SIGGRAPH
   [Anonymous], P 2006 IEEE COMP SOC
   [Anonymous], 1967, J AIRCRAFT
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Bradski G., 2008, LEARNING OPENCV
   Brown MS, 2006, IEEE T IMAGE PROCESS, V15, P1544, DOI 10.1109/TIP.2006.871082
   Brown MS, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P367, DOI 10.1109/ICCV.2001.937649
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao HG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P228, DOI 10.1109/ICCV.2003.1238346
   Chan T, 2005, PROC CVPR IEEE, P1164
   Clark P., 2001, P 12 BRIT MACH VIS C, P421
   Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5
   Freedman D, 2005, PROC CVPR IEEE, P755
   Fu B., 2007, P INT WORKSH CAM BAS
   Fukui K, 1995, IEICE T INF SYST, VE78D, P1533
   Gumerov NA, 2006, INT J COMPUT VISION, V66, P261, DOI 10.1007/s11263-005-3678-x
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Iketani A, 2007, LECT NOTES COMPUT SC, V4844, P73
   Kolmogorov V, 2005, IEEE I CONF COMP VIS, P564
   Koo HI, 2009, IEEE I CONF COMP VIS, P514, DOI 10.1109/ICCV.2009.5459195
   Koo HI, 2010, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2010.5540071
   Koo HI, 2009, IEEE T IMAGE PROCESS, V18, P1551, DOI 10.1109/TIP.2009.2019301
   Kuan YH, 2008, IEEE T MULTIMEDIA, V10, P832, DOI 10.1109/TMM.2008.922853
   Kumar MP, 2005, PROC CVPR IEEE, P18
   Liang J., 2005, International Journal on Document Analysis and Recognition, V7, P84, DOI 10.1007/s10032-004-0138-z
   Liang J, 2005, PROC CVPR IEEE, P338
   Liu XQ, 2010, IEEE T PATTERN ANAL, V32, P1182, DOI 10.1109/TPAMI.2009.120
   Matas J, 2000, COMPUT VIS IMAGE UND, V78, P119, DOI 10.1006/cviu.1999.0831
   McCleary J., 1995, GEOMETRY DIFFERENTIA
   Pilu M, 2001, PROC CVPR IEEE, P67
   Pilu M, 2001, PROC CVPR IEEE, P363
   Schoenemann T, 2009, IEEE I CONF COMP VIS, P17, DOI 10.1109/ICCV.2009.5459209
   Shafait F., 2007, 2nd Int. Workshop on Camera-Based Document Analysis and Recognition, P181
   Singh C, 2008, PATTERN RECOGN, V41, P3528, DOI 10.1016/j.patcog.2008.06.002
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Tsoi Yau-Chat, 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383251
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Ulges A., 2004, P 2004 ACM S DOC ENG, P198, DOI 10.1145/1030397.1030434
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Vu N, 2008, P IEEE C COMP VIS PA
   Yamashita A, 2004, INT C PATT RECOG, P482, DOI 10.1109/ICPR.2004.1334171
   Zhang L, 2005, PROC INT CONF DOC, P192
   Zhang WF, 2007, PR IEEE COMP DESIGN, P10
NR 44
TC 10
Z9 10
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 647
EP 660
DI 10.1109/TMM.2012.2236305
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900015
DA 2024-07-18
ER

PT J
AU Imamoglu, N
   Lin, WS
   Fang, YM
AF Imamoglu, Nevrez
   Lin, Weisi
   Fang, Yuming
TI A Saliency Detection Model Using Low-Level Features Based on Wavelet
   Transform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature map; saliency detection; saliency map; visual attention; wavelet
   transform
ID VISUAL-ATTENTION; GUIDED SEARCH; TOP-DOWN; SCENE
AB Researchers have been taking advantage of visual attention in various image processing applications such as image retargeting, video coding, etc. Recently, many saliency detection algorithms have been proposed by extracting features in spatial or transform domains. In this paper, a novel saliency detection model is introduced by utilizing low-level features obtained from the wavelet transform domain. Firstly, wavelet transform is employed to create the multi-scale feature maps which can represent different features from edge to texture. Then, we propose a computational model for the saliency map from these features. The proposed model aims to modulate local contrast at a location with its global saliency computed based on the likelihood of the features, and the proposed model considers local center-surround differences and global contrast in the final saliency map. Experimental evaluation depicts the promising results from the proposed model by outperforming the relevant state of the art saliency detection models.
C1 [Imamoglu, Nevrez; Lin, Weisi; Fang, Yuming] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Imamoglu, N (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM nimamoglu@ntu.edu.sg; wslin@ntu.edu.sg; fa0001ng@ntu.edu.sg
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011; İmamoğlu,
   Nevrez/AAC-4082-2019
OI Lin, Weisi/0000-0001-9866-1947; İmamoğlu, Nevrez/0000-0002-2661-599X
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 1955, PRINCIPLES GESTALT P
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], DIGITAL SIGNAL PROCE
   Cheng G., 2008, TRANSPORTATION RES B, P1
   FRINTROP S, 2005, THESIS RHEINISCHE FR
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L., 2000, THESIS CALTECH PASAD
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kocyigit Y., 2005, ITU DERGISI MUHENDIS, V4
   Lee Fugal D., 2009, CONCEPTUAL WAVELETS, P1
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Merry R. J. E, 2005, 200553 DCT EINDH U T
   Meur O L, 2006, IEEE T PATTERN ANAL, V28, P802
   Murray N., 2011, P IEEE INT C COMP VI
   Niebur E, 1998, ATTENTIVE BRAIN, P163
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Oliva A, 2003, IEEE IMAGE PROC, P253, DOI 10.1109/icip.2003.1246946
   Semmlow L., 2004, BIOSIGNAL BIOMEDICAL
   Theodoridis S, 2009, PATTERN RECOGNITION, 4RTH EDITION, P1
   Tian Q, 2001, J ELECTRON IMAGING, V10, P835, DOI 10.1117/1.1406945
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   WOLFE JM, 1989, J EXP PSYCHOL HUMAN, V15, P419, DOI 10.1037/0096-1523.15.3.419
   Wolfe JM, 2003, J EXP PSYCHOL HUMAN, V29, P483, DOI 10.1037/0096-1523.29.2.483
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
NR 28
TC 192
Z9 213
U1 0
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 96
EP 105
DI 10.1109/TMM.2012.2225034
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600008
DA 2024-07-18
ER

PT J
AU Jiang, JWJ
   Chan, SHG
   Chiang, M
   Rexford, J
   Ren, DT
   Wei, B
AF Jiang, Joe Wenjie
   Chan, S. -H. Gary
   Chiang, Mung
   Rexford, Jennifer
   Ren, D. Tony
   Wei, Bin
TI Global 1-Mbps Peer-Assisted Streaming: Fine-Grain Measurement of a
   Configurable Platform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Communications technology; communication systems; computer networks;
   peer-to-peer computing
AB High-resolution video is defining a new age of peer-assisted video streaming over the public Internet. Streaming over 1-Mbps videos in a scalable and global manner presents a challenging milestone. In this work, we examine the feasibility of 1-Mbps streaming through a global measurement study. In contrast to previous measurement studies that crawl commercial applications, we conduct fine-grain, controlled experiments on a configurable platform. We developed and deployed FastMesh-SIM, a novel peer-assisted streaming system that leverages proxies, scalable streaming trees and IP multicast to achieve 1-Mbps streaming at a global scale.
   With the configurability-enabled design, we are allowed to conduct controlled experiments by varying design decisions under a wide range of operating conditions, and measuring in-depth, fine-grain metrics at a per-hop, per-segment level. We collected hundreds of hours of streaming traces that broadcast live TV channels to more than 120 peers and 30 proxies, with a global geographic footprint over 8 different countries. Data analysis demonstrates how a set of design decisions collectively overcome the 1-Mbps barrier. The various operational issues we uncovered provide insights to service providers that want to deploy a commercial system at a larger scale and a higher streaming rate. By comparing theory and practice, we also confirm theory-inspired architectural decisions, and show that our system indeed achieves throughputs close to theoretical upper-bound calculated under many ideal assumptions.
C1 [Jiang, Joe Wenjie; Rexford, Jennifer] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA.
   [Chan, S. -H. Gary; Ren, D. Tony] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Chiang, Mung] Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA.
   [Wei, Bin] AT&T Labs Res, Florham Pk, NJ 07932 USA.
   [Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Sino Software Res Inst, Hong Kong, Hong Kong, Peoples R China.
   [Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Risk Management & Business Intelligence Program, Hong Kong, Hong Kong, Peoples R China.
   [Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Comp Engn Program, Hong Kong, Hong Kong, Peoples R China.
C3 Princeton University; Hong Kong University of Science & Technology;
   Princeton University; AT&T; Hong Kong University of Science &
   Technology; Hong Kong University of Science & Technology; Hong Kong
   University of Science & Technology
RP Jiang, JWJ (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA.
EM wenjiej@princeton.edu; gchan@cse.ust.hk; chiangm@princeton.edu;
   jrex@princeton.edu; tonyren@cse.ust.hk; bw@research.att.com
OI Rexford, Jennifer/0000-0002-0231-8165; Chan, Gary Shueng
   Han/0000-0003-4207-764X
FU AFOSR MURI [FA9550-09-1-0643]; Research Grant Council of the Hong Kong
   Special Administrative Region, China [611209]
FX This work was supported in part by AFOSR MURI grant FA9550-09-1-0643 and
   in part by the General Research Fund from the Research Grant Council of
   the Hong Kong Special Administrative Region, China (611209).
CR Alessandria E., 2009, P IEEE INFOCOM
   Ali S., 2006, P WORKSH REC ADV PEE
   Annapureddy S., 2007, Proc. Int'l WWW Conference, P903
   Cha M., 2008, P INT MEAS C
   Hei XJ, 2007, IEEE J SEL AREA COMM, V25, P1640, DOI 10.1109/JSAC.2007.071204
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Huang C., 2008, P NOSSDAV
   Huang Y., 2008, P ACM SIGCOMM
   Jiang J. W., GLOBAL 1 MBPS PEER A
   Jiang J. W., 2010, P INT C COMP COMM NE
   Jin X., 2007, J ADV MULTIMEDIA SPE
   Jin X, 2009, IEEE MULTIMEDIA, V16, P72, DOI 10.1109/MMUL.2009.6
   KARAGIANNIS T, 2005, P INT MEAS C
   Li Jin., 2005, P ACM SIGCOMM ASIA W
   Liu S., 2008, P ACM SIGMETRICS
   Liu S., 2010, P INT C DISTR COMP S
   Liu Y., 2007, P ACM MULT
   Liu Y, 2010, IEEE ACM T NETWORK, V18, P1195, DOI 10.1109/TNET.2009.2038155
   Ren D., 2008, P IEEE INFOCOM
   Sengupta S, 2011, IEEE T INFORM THEORY, V57, P5072, DOI 10.1109/TIT.2011.2145630
   Silverston T., 2006, CORR
   Wang Feng., 2008, P IEEE INFOCOM
   Wu C, 2007, IEEE J SEL AREA COMM, V25, P1612, DOI 10.1109/JSAC.2007.071202
   Wu C, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1386109.1386112
   Xie S, 2007, IEEE T MULTIMEDIA, V9, P1661, DOI 10.1109/TMM.2007.907469
   Xu Dongyan., 2004, Computer Networks, V44, P353
   Yin H., 2009, P ACM MULT
   Yiu WPK, 2006, IEEE T MULTIMEDIA, V8, P219, DOI 10.1109/TMM.2005.864268
   Zhang S., 2010, P INT C NETW PROT
NR 29
TC 1
Z9 1
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1456
EP 1468
DI 10.1109/TMM.2012.2196509
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Greco, C
   Cagnazzo, M
   Pesquet-Popescu, B
AF Greco, Claudio
   Cagnazzo, Marco
   Pesquet-Popescu, Beatrice
TI Low-Latency Video Streaming With Congestion Control in Mobile Ad-Hoc
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Congestion-distortion model; cooperative systems; cross-layer design;
   mobile ad-hoc networks; multimedia communication; multiple description
   coding
ID CONTENT DELIVERY; TRANSMISSION; MULTICAST; PROTOCOL; SCHEMES; QUALITY;
   DESIGN
AB In this paper, we address the challenge of delivering a video stream, encoded with multiple descriptions, in a mobile ad-hoc environment with low-latency constraints. This kind of application is meant to provide an efficient and reliable video communication tool in scenarios where the deployment of an infrastructure is not feasible, such as military and disaster relief applications. First, we present a recently proposed protocol that employs a reliable form of one-hop broadcast to build an efficient overlay network according to a multi-objective function that minimizes the number of packets injected in the network and maximizes the path diversity among descriptions. Then, we introduce the main contribution of this paper: a cross-layer congestion control strategy where the MAC layer is video-coding aware and adjusts its transmission parameters (namely, the RTS retry limit) via congestion/distortion optimization. The main challenge in this approach is providing a reliable estimation of congestion and distortion, given the limited information available at each node. Our simulations show that, if a stringent constraint of low delay is imposed, our technique grants a consistent gain in terms of both PSNR and delay reduction, for bitrates up to a few megabits per second.
C1 [Greco, Claudio; Cagnazzo, Marco; Pesquet-Popescu, Beatrice] TELECOM ParisTech, CNRS LTCI, Inst Mines TELECOM, Dept TSI, F-75634 Paris 13, France.
C3 Centre National de la Recherche Scientifique (CNRS); IMT - Institut
   Mines-Telecom; Institut Polytechnique de Paris; Telecom Paris
RP Greco, C (corresponding author), TELECOM ParisTech, CNRS LTCI, Inst Mines TELECOM, Dept TSI, F-75634 Paris 13, France.
EM greco@telecom-paristech.fr; cagnazzo@telecom-paristech.fr;
   pesquet@telecom-paristech.fr
RI Cagnazzo, Marco/AAZ-3881-2020
OI Cagnazzo, Marco/0000-0001-6731-3755
CR Alasti M, 2001, IEEE T INFORM THEORY, V47, P891, DOI 10.1109/18.915641
   [Anonymous], 2003, 51 ALL C COMM CONTR
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   APOSTOLOPOULOS JG, 2001, P SPIE VIS COMM IM P
   Badarneh OS, 2010, IEEE IPCCC, P222, DOI 10.1109/PCCC.2010.5682307
   Camp T, 2002, WIREL COMMUN MOB COM, V2, P483, DOI 10.1002/wcm.72
   Cataldi P, 2010, IEEE T IMAGE PROCESS, V19, P1491, DOI 10.1109/TIP.2010.2042985
   Crow BP, 1997, IEEE COMMUN MAG, V35, P116, DOI 10.1109/35.620533
   da Hora D. N., 2009, ELSEVIER J COMPUT CO, V1, P1
   El Fawal A., 2007, P IEEE ACM INT C AUT
   Frodigh M, 2000, ERICSSON REV, V77, P248
   Gerla M, 2005, PERVASIVE MOB COMPUT, V1, P77, DOI 10.1016/j.pmcj.2005.01.002
   Goudarzi P, 2010, IEEE T CONSUM ELECTR, V56, P2217, DOI 10.1109/TCE.2010.5681093
   Goyal VK, 2001, IEEE T INFORM THEORY, V47, P2199, DOI 10.1109/18.945243
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Greco C., 2010, P IEEE MULT SIGN PRO
   Greco C., 2011, P IEEE MULT SIGN PRO
   Greco C., 2011, P GRETSI
   Greco C, 2011, INT J COMMUN NETW DI, V7, P49, DOI 10.1504/IJCNDS.2011.040977
   Gupta P, 2000, IEEE T INFORM THEORY, V46, P388, DOI 10.1109/18.825799
   Hanzo L, 2009, IEEE COMMUN SURV TUT, V11, P78, DOI 10.1109/SURV.2009.090406
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   Lochert C, 2007, WIREL COMMUN MOB COM, V7, P655, DOI 10.1002/wcm.524
   Magli E, 2009, IEEE INT CON MULTI, P1488, DOI 10.1109/ICME.2009.5202785
   Marina M. K., 2001, P IEEE INT C COMM
   Maugey T., 2008, P EUR SIGN PROC C
   Maugey T, 2008, J VIS COMMUN IMAGE R, V19, P589, DOI 10.1016/j.jvcir.2008.09.002
   Monteiro J., 2010, TELECOMMUN SYST, V49, P113
   Nemoianu I. D., 2012, P IEEE INT C AC SPEE
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Puri R., 1999, Proceedings of the 33rd Asilomar Confe. Signals, Systems, V1, P342
   Radulovic I, 2010, IEEE T CIRC SYST VID, V20, P144, DOI 10.1109/TCSVT.2009.2026815
   REUDINK DO, 1980, TM801341 BELL LAB
   ROYER E, 2001, P IEEE INT C COMM
   Setton E, 2005, IEEE WIREL COMMUN, V12, P59, DOI 10.1109/MWC.2005.1497859
   Setton E., 2004, P IEEE MULT SIGN PRO
   Stoica I, 2003, IEEE ACM T NETWORK, V11, P17, DOI 10.1109/TNET.2002.808407
   Tillier C, 2007, EURASIP J IMAGE VIDE, DOI 10.1155/2007/31319
   Tizon N., 2009, P GRETSI
   Tourrilhes J, 1998, P IEEE INT S PERS IN
   VAISHAMPAYAN VA, 1993, IEEE T INFORM THEORY, V39, P821, DOI 10.1109/18.256491
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Villalón J, 2007, IEEE J SEL AREA COMM, V25, P699, DOI 10.1109/JSAC.2007.070507
   Wang Y, 2005, P IEEE, V93, P57, DOI 10.1109/JPROC.2004.839618
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Ye F, 2011, IEEE J SEL AREA COMM, V29, P151, DOI 10.1109/JSAC.2011.110115
   Zezza S, 2009, IEEE INT CON MULTI, P1586, DOI 10.1109/ICME.2009.5202819
   Zhu X., 2005, SIGNAL PROCESS-IMAGE, V20, P1
NR 48
TC 19
Z9 21
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1337
EP 1350
DI 10.1109/TMM.2012.2195480
PN 2
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400018
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Tian, XM
   Lu, YJ
   Yang, LJ
AF Tian, Xinmei
   Lu, Yijuan
   Yang, Linjun
TI Query Difficulty Prediction for Web Image Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; image search quality; query difficulty prediction (QDP)
ID AGE ESTIMATION; RETRIEVAL; DIVERGENCE
AB Image search plays an important role in our daily life. Given a query, the image search engine is to retrieve images related to it. However, different queries have different search difficulty levels. For some queries, they are easy to be retrieved (the search engine can return very good search results). While for others, they are difficult (the search results are very unsatisfactory). Thus, it is desirable to identify those "difficult" queries in order to handle them properly. Query difficulty prediction (QDP) is an attempt to predict the quality of the search result for a query over a given collection. QDP problem has been investigated for many years in text document retrieval, and its importance has been recognized in the information retrieval (IR) community. However, little effort has been conducted on the image query difficulty prediction problem for image search. Compared with QDP in document retrieval, QDP in image search is more challenging due to the noise of textual features and the well-known semantic gap of visual features. This paper aims to investigate the QDP problem in Web image search. A novel method is proposed to automatically predict the quality of image search results for an arbitrary query. This model is built based on a set of valuable features that are designed by exploring the visual characteristic of images in the search results. The experiments on two real image search datasets demonstrate the effectiveness of the proposed query difficulty prediction method. Two applications, including optimal image search engine selection and search results merging, are presented to show the promising applicability of QDP.
C1 [Tian, Xinmei] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Lu, Yijuan] SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
   [Yang, Linjun] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Texas State University System; Texas State University San
   Marcos; Microsoft Research Asia; Microsoft
RP Tian, XM (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM xinmei@ustc.edu.cn; yl12@txstate.edu; linjuny@microsoft.com
RI LU, YIJUAN/GNM-8769-2022
OI LU, YIJUAN/0000-0002-9855-8365
FU University of Science and Technology of China; Research Enhancement
   Program (REP); Texas State University; NSF [CRI 1058724]
FX This work is supported in part by start-up funding from the University
   of Science and Technology of China to X. Tian, in part by the Research
   Enhancement Program (REP), and start-up funding from the Texas State
   University and NSF CRI 1058724 to Y. Lu. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jinhui Tang.
CR Amati G, 2004, LECT NOTES COMPUT SC, V2997, P127
   [Anonymous], P ACM MULT
   [Anonymous], RANK CORRELATION MET
   [Anonymous], 2010, P ACM INT C MULT
   Aslam JA, 2007, LECT NOTES COMPUT SC, V4425, P198
   Carbonell JG, 1997, INT JOINT CONF ARTIF, P708
   Carmel D., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P390, DOI 10.1145/1148170.1148238
   Chang C.-C., 2004, Libsvm: a library for support vector machines, software
   Cronen-Townsend S., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P299
   Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6
   Duda R. O., 2001, PATTERN CLASSIFICATI, P517
   Geng B, 2010, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2010.5540003
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Gibbons J., 1992, Nonparametric Statistical Inference
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Hauff C., 2008, P 17 ACM C INF KNOWL, P439
   He B, 2004, LECT NOTES COMPUT SC, V3246, P43
   He JY, 2008, LECT NOTES COMPUT SC, V4956, P689
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Hong R., 2010, Proceedings of the 18th ACM international conference on Multimedia, P421
   Hsu WinstonH., 2007, ACM MM
   Imran H., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining Workshops (ICDMW 2010), P867, DOI 10.1109/ICDMW.2010.81
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jensen E. C., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P615, DOI 10.1145/1076034.1076155
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Krapac J, 2010, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR.2010.5540092
   Kreyszig E., 1997, ADV ENG MATH
   Kwok K.-L., 2004, P TREC
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Li YX, 2011, LECT NOTES COMPUT SC, V6524, P479
   LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   McLaughlin M. R., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P329, DOI 10.1145/1008992.1009050
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Rudinac S, 2010, LECT NOTES COMPUT SC, V5993, P645, DOI 10.1007/978-3-642-12275-0_67
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Tang JH, 2010, IEEE T MULTIMEDIA, V12, P131, DOI 10.1109/TMM.2009.2037373
   Tang JH, 2009, IEEE T SYST MAN CY B, V39, P409, DOI 10.1109/TSMCB.2008.2006045
   Tang Jinhui., 2009, Proceedings of ACM international conference on Multimedia, P223, DOI DOI 10.1145/1631272.1631305
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Tian X., 2008, ACM INT C MULTIMEDIA, P131, DOI DOI 10.1145/1459359.1459378.ISBN
   Tian XM, 2010, IEEE T IMAGE PROCESS, V19, P805, DOI 10.1109/TIP.2009.2035866
   Tian XM, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1509
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Xing X, 2010, LECT NOTES COMPUT SC, V5993, P581, DOI 10.1007/978-3-642-12275-0_52
   Yan R, 2003, LECT NOTES COMPUT SC, V2728, P238
   Yom-Tov E., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P512, DOI 10.1145/1076034.1076121
   Yun Zhou, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P543, DOI 10.1145/1277741.1277835
   Zhou Y., 2006, CIKM, P567
NR 53
TC 13
Z9 14
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 951
EP 962
DI 10.1109/TMM.2011.2177647
PN 1
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300002
DA 2024-07-18
ER

PT J
AU Zhang, TZ
   Xu, CS
   Zhu, GY
   Liu, S
   Lu, HQ
AF Zhang, Tianzhu
   Xu, Changsheng
   Zhu, Guangyu
   Liu, Si
   Lu, Hanqing
TI A Generic Framework for Video Annotation via Semi-Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Broadcast video; concave-convex procedure (CCCP); event detection;
   graph; Internet; multiple instance learning; semi-supervised learning;
   web-casting text
ID LINEAR NEIGHBORHOOD PROPAGATION
AB Learning-based video annotation is essential for video analysis and understanding, and many various approaches have been proposed to avoid the intensive labor costs of purely manual annotation. However, there lacks a generic framework due to several difficulties, such as dependence of domain knowledge, insufficiency of training data, no precise localization and inefficacy for large-scale video dataset. In this paper, we propose a novel approach based on semi-supervised learning by means of information from the Internet for interesting event annotation in videos. Concretely, a Fast Graph-based Semi-Supervised Multiple Instance Learning (FGSSMIL) algorithm, which aims to simultaneously tackle these difficulties in a generic framework for various video domains (e. g., sports, news, and movies), is proposed to jointly explore small-scale expert labeled videos and large-scale unlabeled videos to train the models. The expert labeled videos are obtained from the analysis and alignment of well-structured video related text (e. g., movie scripts, web-casting text, close caption). The unlabeled data are obtained by querying related events from the video search engine (e. g., YouTube, Google) in order to give more distributive information for event modeling. Two critical issues of FGSSMIL are: 1) how to calculate the weight assignment for a graph construction, where the weight of an edge specifies the similarity between two data points. To tackle this problem, we propose a novel Multiple Instance Learning Induced Similarity (MILIS) measure by learning instance sensitive classifiers; 2) how to solve the algorithm efficiently for large-scale dataset through an optimization approach. To address this issue, Concave-Convex Procedure (CCCP) and nonnegative multiplicative updating rule are adopted. We perform the extensive experiments in three popular video domains: movies, sports, and news. The results compared with the state-of-the-arts are promising and demonstrate the effectiveness and efficiency of our proposed approach.
C1 [Zhang, Tianzhu] Adv Digital Sci Ctr ADSC, Singapore 138632, Singapore.
   [Zhang, Tianzhu; Xu, Changsheng; Zhu, Guangyu; Liu, Si; Lu, Hanqing] China Singapore Inst Digital Media, Singapore 119613, Singapore.
   [Xu, Changsheng; Liu, Si; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zhu, Guangyu] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; National
   University of Singapore
RP Zhang, TZ (corresponding author), Adv Digital Sci Ctr ADSC, Singapore 138632, Singapore.
RI xu, cj/HJZ-3488-2023; Zhang, Tianzhu/AGY-9389-2022; Zhu,
   Guangyu/H-3805-2013
OI Zhang, Tianzhu/0000-0003-0764-6106; 
FU 973 Program [2010CB327905, 2012CB316304]; National Natural Science
   Foundation of China [60833006, 61070104, 90920303]
FX This work was supported in part by 973 Program (Project No.
   2010CB327905, 2012CB316304) and in part by the National Natural Science
   Foundation of China (Grant No. 60833006, 61070104, 90920303). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Chia-Wen Lin.
CR Andrews S., 2002, P NIPS
   [Anonymous], ARTIFICIAL INTELLIGE
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2008, P CVPR
   [Anonymous], 2003, CONVEX OPTIMIZATION
   [Anonymous], 2008, P ECCV
   [Anonymous], 2006, P BMVC
   [Anonymous], 2008, SEMISUPERVISED LEARN
   [Anonymous], P CVPR
   [Anonymous], P ICCV
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   [Anonymous], 2008, P CVPR
   Babaguchi N, 2004, IEEE T MULTIMEDIA, V6, P575, DOI [10.1109/TMM.2004.830811, 10.1109/tmm.2004.830811]
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Belkin M., 2002, P NIPS
   Buxton H., 2003, P IM VIS COMP
   Dao M.-S., 2008, AREA 08
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Dollar P., 2005, P PETS
   DUCHENNE O, 2009, P ICCV
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Fleischman M., 2008, P ACL 08 HLT
   Hauptmann A. G., 1998, ADV DIGITAL LIB
   Hu W., 2004, IEEE T SYST MAN CYBE
   HU Y, 2009, P ICCV
   Huang C., 2003, 007 COL U
   Jebara T., 2009, P INT C MACH LEARN
   Jia Y., 2008, AAAI 08
   Kato T, 2009, IEEE T NEURAL NETWOR, V20, P35, DOI 10.1109/TNN.2008.2003354
   Kim JG, 2003, INT J IMAG SYST TECH, V13, P267, DOI 10.1002/ima.10067
   Muller Meinard., 2007, Information Retrieval for Music and Motion, P65
   RUI Y, 2000, P ACM MULT LOS ANG C
   Singh A., 2008, P NIPS
   SMOLA A, 2005, P INT WORKSH ART INT
   Tang JH, 2008, IEEE T MULTIMEDIA, V10, P620, DOI 10.1109/TMM.2008.921853
   Tang JH, 2009, IEEE T SYST MAN CY B, V39, P409, DOI 10.1109/TSMCB.2008.2006045
   Wang C., 2008, MIR 08
   Wang J., 2005, P ACM INT C MULT
   Xu C., 2006, PROC 14 ANN ACM INT, P221, DOI DOI 10.1145/1180639.1180699
   YAN R, 2005, P CVPR
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
   Zhang T., 2010, P ACM MULT
   Zhang T., 2009, P CVPR
NR 43
TC 46
Z9 49
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1206
EP 1219
DI 10.1109/TMM.2012.2191944
PN 2
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400008
DA 2024-07-18
ER

PT J
AU Correia, P
   Assuncao, PA
   Silva, V
AF Correia, Pedro
   Assuncao, Pedro A.
   Silva, Vitor
TI Multiple Description of Coded Video for Path Diversity Streaming
   Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video streaming; drift control; multiple description; networks
   with path diversity
ID ALGORITHMS
AB This paper extends the current concept of multiple description coding (MDC) to the compressed domain, by proposing efficient splitting of standard single description coded (SDC) video into a multi-stream representation. A novel multiple description video splitting (MDVS) scheme is proposed to operate at network edges, for increased robustness in path diversity video streaming across heterogeneous communications chains. It is shown that poor performance of existing methods is mainly due to distortion accumulation, i.e., drift, when decoding is carried out with missing descriptions. The proposed scheme is able to effectively control drift distortion in both intra and inter predictive coding, even when only one description reaches the decoder. This is achieved by generating a controlled amount of relevant side information to compensate for drift accumulation, whenever any description is lost in its path. The simulation results show that any individual description can be decoded on its own without producing drift, achieving significant quality improvement at reduced redundancy cost. The overall performance evaluation, carried out by simulating video streaming over lossy networks with path diversity, also demonstrates that MDVS enables higher quality video in such heterogeneous networking environments, for a wide range of packet loss rates.
C1 [Correia, Pedro; Silva, Vitor] Univ Coimbra, DEEC, Coimbra, Portugal.
   [Correia, Pedro] Polytech Inst Tomar, Tomar, Portugal.
   [Assuncao, Pedro A.] Polytech Inst Leiria, Leiria, Portugal.
   [Correia, Pedro] Inst Telecomunicacoes, Coimbra, Portugal.
C3 Universidade de Coimbra; Instituto Politecnico de Tomar; Institute of
   Telecommunications - Coimbra; Universidade de Coimbra
RP Correia, P (corresponding author), Univ Coimbra, DEEC, Coimbra, Portugal.
EM pcor-reia@co.it.pt; paas-sunc@ieee.org; vitor@co.it.pt
RI Silva, Vitor/JFL-1067-2023; Assuncao, Pedro A. Amado/A-4827-2017;
   Correia, Pedro Frazão/AAL-6218-2021
OI Assuncao, Pedro A. Amado/0000-0001-9539-8311; Correia, Pedro
   Frazão/0000-0001-9451-136X; Silva, Vitor/0000-0003-2439-1184
FU Fundacao para a Ciencia e Tecnologia (FCT), Portugal
   [SFRH/BD/30087/2006, SFRH/BD/50035/2009]; Fundação para a Ciência e a
   Tecnologia [SFRH/BD/30087/2006] Funding Source: FCT
FX This work was supported by Fundacao para a Ciencia e Tecnologia (FCT),
   Portugal, under grants SFRH/BD/30087/2006 and SFRH/BD/50035/2009. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Monica Aguilar.
CR Abanoz TB, 2009, SIGNAL PROCESS-IMAGE, V24, P691, DOI 10.1016/j.image.2009.07.003
   Ahuja S, 2008, IEEE T MULTIMEDIA, V10, P1382, DOI 10.1109/TMM.2008.2004930
   Akyol E, 2007, IEEE J-STSP, V1, P231, DOI 10.1109/JSTSP.2007.901527
   Apostolopoulos J.G., 2002, P IEEE INFOCOM
   Bajic IV, 2003, IEEE T IMAGE PROCESS, V12, P1211, DOI 10.1109/TIP.2003.817248
   Biswas M, 2009, IEEE T CIRC SYST VID, V19, P1556, DOI 10.1109/TCSVT.2009.2026929
   Carnpana O, 2008, IEEE T CIRC SYST VID, V18, P268, DOI 10.1109/TCSVT.2008.918113
   Chen CM, 2007, J VIS COMMUN IMAGE R, V18, P191, DOI 10.1016/j.jvcir.2007.02.001
   Crave O, 2010, IEEE T CIRC SYST VID, V20, P769, DOI 10.1109/TCSVT.2010.2045805
   Essaili A. E., 2007, P INT C IM PROC ICIP, V6, P77
   Fan YH, 2011, IEEE T IMAGE PROCESS, V20, P1768, DOI 10.1109/TIP.2010.2097271
   Franchi N, 2005, IEEE T CIRC SYST VID, V15, P321, DOI 10.1109/TCSVT.2004.842606
   Frossard P, 2008, P IEEE, V96, P39, DOI 10.1109/JPROC.2007.909876
   Gan T, 2006, IEEE T IMAGE PROCESS, V15, P819, DOI 10.1109/TIP.2005.863960
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Kamonoonwatana D. A. N., 2010, P IEEE 17 INT C IM P, P1269
   Kim CS, 2001, IEEE T CIRC SYST VID, V11, P999, DOI 10.1109/76.946517
   Kim I. K., 2003, P INT WORKSH ADV IM
   Kim IK, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P109, DOI 10.1109/ICME.2006.262581
   Lamy-Bergot C., 2010, EURASIP J WIRELESS C, V2010
   Lee YC, 2004, IEEE T CIRC SYST VID, V14, P122, DOI 10.1109/TCSVT.2003.819182
   Lefol D, 2006, IEEE T CONSUM ELECTR, V52, P215
   Li Z., 2009, P IEEE INT WORKSH MU, P1
   Liao YT, 2011, IEEE T MULTIMEDIA, V13, P132, DOI 10.1109/TMM.2010.2089504
   Lin S., 2001, Proceedings of IEEE International Conference on Multimedia and Expo, P96
   Ma R, 2008, IEEE T SIGNAL PROCES, V56, P3996, DOI 10.1109/TSP.2008.925900
   Mao SW, 2005, IEEE WIREL COMMUN, V12, P42, DOI 10.1109/MWC.2005.1497857
   Matty KR, 2005, IEEE T CIRC SYST VID, V15, P928, DOI 10.1109/TCSVT.2005.848343
   Milani S, 2010, IEEE SIGNAL PROC LET, V17, P51, DOI 10.1109/LSP.2010.2051619
   Peraldo L, 2010, INT CONF ACOUST SPEE, P2330, DOI 10.1109/ICASSP.2010.5496045
   Puri R, 2001, IEEE T MULTIMEDIA, V3, P18, DOI 10.1109/6046.909591
   Radulovic I, 2010, IEEE T CIRC SYST VID, V20, P144, DOI 10.1109/TCSVT.2009.2026815
   Reibman AR, 2002, IEEE T CIRC SYST VID, V12, P193, DOI 10.1109/76.993440
   Shanableh T, 2008, SIGNAL PROCESS-IMAGE, V23, P610, DOI 10.1016/j.image.2008.05.005
   Su CC, 2008, SIGNAL PROCESS-IMAGE, V23, P677, DOI 10.1016/j.image.2008.07.002
   Tang XY, 2002, IEEE T CIRC SYST VID, V12, P566, DOI 10.1109/TCSVT.2002.800327
   Tillier C, 2007, EURASIP J IMAGE VIDE, DOI 10.1155/2007/31319
   Tillo T, 2008, IEEE T CIRC SYST VID, V18, P59, DOI 10.1109/TCSVT.2007.913751
   VAISHAMPAYAN VA, 1993, IEEE T INFORM THEORY, V39, P821, DOI 10.1109/18.256491
   Wang D, 2005, IEEE INT SYMP CIRC S, P960, DOI 10.1109/ISCAS.2005.1464749
   Wang D, 2005, IEEE INT SYMP CIRC S, P2719, DOI 10.1109/ISCAS.2005.1465188
   Wang Y, 2005, P IEEE, V93, P57, DOI 10.1109/JPROC.2004.839618
   Wang Y, 2002, IEEE T CIRC SYST VID, V12, P438, DOI 10.1109/TCSVT.2002.800320
   Wang Y, 2001, IEEE T IMAGE PROCESS, V10, P351, DOI 10.1109/83.908500
   Wang YK, 2007, IEEE T CIRC SYST VID, V17, P1149, DOI 10.1109/TCSVT.2007.906827
   Zhu C, 2009, IEEE T CIRC SYST VID, V19, P511, DOI 10.1109/TCSVT.2009.2013521
NR 46
TC 22
Z9 24
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 923
EP 935
DI 10.1109/TMM.2011.2182184
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700022
DA 2024-07-18
ER

PT J
AU Hajimirza, SN
   Proulx, MJ
   Izquierdo, E
AF Hajimirza, S. Navid
   Proulx, Michael J.
   Izquierdo, Ebroul
TI Reading Users' Minds From Their Eyes: A Method for Implicit Image
   Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaze tracking; human computer interaction; image annotation; image
   databases; image retrieval; implicit annotation; visual attention
ID GUIDANCE
AB This paper explores the possible solutions for image annotation and retrieval by implicitly monitoring user attention via eye-tracking. Features are extracted from the gaze trajectory of users examining sets of images to provide implicit information on the target template that guides visual attention. Our Gaze Inference System (GIS) is a fuzzy logic based framework that analyzes the gaze-movement features to assign a user interest level (UIL) from 0 to 1 to every image that appeared on the screen. Because some properties of the gaze features are unique for every user, our user adaptive framework builds a new processing system for every new user to achieve higher accuracy. The generated UILs can be used for image annotation purposes; however, the output of our system is not limited as it can be used also for retrieval or other scenarios. The developed framework produces promising and reliable UILs where approximately 53% of target images in the users' minds can be identified by the machine with an error of less than 20% and the top 10% of them with no error. We show in this paper that the existing information in gaze patterns can be employed to improve the machine's judgement of image content by assessment of human interest and attention to the objects inside virtual environments.
C1 [Hajimirza, S. Navid] Queen Mary Univ London, Multimedia & Vis Grp, Sch Elect Engn & Comp Sci, London, England.
   [Proulx, Michael J.] Queen Mary Univ London, Biol & Expt Psychol Grp, Sch Biol & Chem Sci, Crossmodal Cognit Lab,Res Ctr Psychol, London, England.
   [Izquierdo, Ebroul] Univ London, Multimedia & Vis Grp, Sch Elect Engn & Comp Sci Queen Mary, London, England.
C3 University of London; Queen Mary University London; University of
   London; Queen Mary University London; University of London
RP Hajimirza, SN (corresponding author), Queen Mary Univ London, Multimedia & Vis Grp, Sch Elect Engn & Comp Sci, London, England.
EM navid.hajimirza@elec.qmul.ac.uk; m.proulx@qmul.ac.uk;
   ebroul.izquierdo@elec.qmul.ac.uk
RI Proulx, Michael J/A-1045-2008
OI Proulx, Michael/0000-0003-4066-3645
FU European Commission [FP7-216444 PetaMedia]
FX This work was supported in part by the European Commission under
   contract FP7-216444 PetaMedia. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Shrikanth Narayanan.
CR Ajanki A, 2009, USER MODEL USER-ADAP, V19, P307, DOI 10.1007/s11257-009-9066-4
   [Anonymous], 2009, ICMI MLMI
   [Anonymous], 2010, P 2010 S EYE TRACK R
   [Anonymous], 2010, MATLAB VERS 7 10 0 R
   [Anonymous], 2007, PRINCIPLES SOFT COMP, DOI DOI 10.1007/978-3-540-35781-0
   Auer P., 2010, P WORKSH APPL PATT A, P21
   Celikyilmaz A, 2008, IEEE T FUZZY SYST, V16, P779, DOI 10.1109/TFUZZ.2007.905919
   Chiu S. L., 1994, J INTELL FUZZY SYST, V2, P267, DOI [10.3233/IFS-1994-2306, DOI 10.3233/IFS-1994-2306]
   Chowdhury N, 2008, PROCEEDINGS OF ICECE 2008, VOLS 1 AND 2, P488, DOI 10.1109/ICECE.2008.4769258
   Dan Li, 2010, Proceedings of the 2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2010), P460, DOI 10.1109/FSKD.2010.5569767
   Djordjevic D., 2006, THESIS QUEEN MARY U
   Estévez PA, 2009, IEEE T NEURAL NETWOR, V20, P189, DOI 10.1109/TNN.2008.2005601
   Hajimirza S., 2010, P 2010 11 INT WORKSH, P1
   Jang J.S.R., 1992, Neuro-Fuzzy Modeling: Architectures, Analyses and Applications
   Klami A., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P134, DOI DOI 10.1145/1460096.1460120
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Land MF, 2009, VISUAL NEUROSCI, V26, P51, DOI 10.1017/S0952523808080899
   Ludwig CJH, 2003, PERCEPT PSYCHOPHYS, V65, P1243, DOI 10.3758/BF03194849
   Ma H, 2010, IEEE T MULTIMEDIA, V12, P462, DOI 10.1109/TMM.2010.2051360
   Marquez FA, 2007, IEEE T FUZZY SYST, V15, P1162, DOI 10.1109/TFUZZ.2007.904121
   Oyekoya O., 2005, 2nd European Workshop on the Integration of Knowledge, Semantics and Digital Media Technology (EWIMT 2005), P139, DOI 10.1049/ic.2005.0723
   Pasupa Kitsuchart, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2009, DOI 10.1109/ICCVW.2009.5457528
   Proulx MJ, 2007, J EXP PSYCHOL HUMAN, V33, P48, DOI 10.1037/0096-1523.33.1.48
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Russell B., Labelme: The open annotation tool
   Seneviratne L., 2009, P 2009 16 INT C DIG, P1
   Shi R., 2004, IMAGE VIDEO RETRIEVA, P1951
   Smith JohnD., 2005, Proc. UIST 2005, P53, DOI DOI 10.1145/1095034.1095043
   Tomanek K, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P1158
   Verma N., 2009, P 2009 IEEE APPL IM, P1
   Wagner C, 2010, IEEE T FUZZY SYST, V18, P637, DOI 10.1109/TFUZZ.2010.2045386
   Wolfe JM, 2004, VISION RES, V44, P1411, DOI 10.1016/j.visres.2003.11.024
   Yarbus A. L., 1967, Eye Movements and Vision
   Yazdani J.-S. Lee., 2009, Proc. SIGMM Workshop on Social media, P81
   Zhang Y, 2010, PROCEEDINGS OF CHINA-CANADA WORKSHOP ON FINANCIAL ENGINEERING AND ENTERPRISE RISK MANAGEMENT 2010, P37, DOI 10.1145/1743666.1743674
   Zhiwei Zhu, 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P139
   Zhou SM, 2009, IEEE T KNOWL DATA EN, V21, P1191, DOI 10.1109/TKDE.2008.208
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 38
TC 17
Z9 20
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 805
EP 815
DI 10.1109/TMM.2012.2186792
PN 2
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700013
DA 2024-07-18
ER

PT J
AU Shiang, HP
   van der Schaar, M
AF Shiang, Hsien-Po
   van der Schaar, Mihaela
TI A Quality-Centric TCP-Friendly Congestion Control for Multimedia
   Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Finite-horizon Markov decision process; quality-based fairness;
   TCP-friendly congestion control for multimedia
ID VIDEO; PERFORMANCE
AB In this paper, we propose a quality-centric congestion control for multimedia streaming over wired IP networks, which we refer to as media-TCP-friendly congestion control (MTCC). Our solution adapts the sending rate to both the network condition and the application characteristics by explicitly considering the distortion impacts, delay deadlines, and interdependencies of different video packet classes. The media-aware solution is able to provide differential services for transmitting various packet classes and thereby, further improves the multimedia streaming quality compared to the conventional network-aware congestion control. We use finite-horizon Markov decision process (FHMDP) to determine the optimal congestion control policy that maximizes the long-term multimedia quality, while adhering to the horizon-K TCP-friendliness constraint, which ensures long-term fairness with existing TCP applications. Moreover, the proposed MTCC is able to achieve quality-based fairness among multimedia users. We derive sufficient conditions for multiple multimedia users to achieve quality-based fairness using MTCC congestion control. Note that the proposed solution only modifies the adaptation mechanism of the TCP congestion window size at the sender, without changing the design at the receiver side (i.e., each current TCP receiver can correctly receive and process MTCC streams). Our simulation results show that MTCC achieves more than 3 dB improvement in terms of PSNR over the conventional TCP congestion control approaches, with the largest improvements observed for real-time streaming applications requiring stringent playback delays.
C1 [Shiang, Hsien-Po; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Shiang, HP (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM hpshiang@ee.ucla.edu; mihaela@ee.ucla.edu
FU Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [0831549] Funding Source: National Science Foundation
CR [Anonymous], 2006, 4340 RFC IETF
   Bajic IV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P257
   Balk A, 2004, COMPUT NETW, V44, P415, DOI 10.1016/j.comnet.2003.12.002
   Bertsekas DP, 2017, DYNAMIC PROGRAMMING
   BOHACEK S, 2003, P IEEE INFOCOM
   Cai L, 2005, IEEE T MULTIMEDIA, V7, P339, DOI 10.1109/TMM.2005.843360
   Chou P.A., 2007, Multimedia over IP and Wireless Networks: Compression, Networking, and Systems
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   DAI M, 2004, P ICIP
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   FU F, 2009, STRUCTURAL SOLUTIONS
   Jain R., 1984, QUANTITATIVE MEASURE
   Li Q, 2007, IEEE T VEH TECHNOL, V56, P3533, DOI 10.1109/TVT.2007.901927
   LI Y, IEEE T MULT IN PRESS
   Li Y., 2007, P IEEE GLOBECOM WASH
   Mo JH, 2000, IEEE ACM T NETWORK, V8, P556, DOI 10.1109/90.879343
   Nguyen T., 2002, P INT PACK VID WORKS
   Ohm JR, 2004, SIGNAL PROCESS-IMAGE, V19, P877, DOI 10.1016/j.image.2004.06.004
   Padhye J, 2000, IEEE ACM T NETWORK, V8, P133, DOI 10.1109/90.842137
   Padhye J., 1999, P NOSSDAV 99
   Rejaie R., 1999, P IEEE INFOCOM
   Sastry NR, 2005, IEEE ACM T NETWORK, V13, P330, DOI 10.1109/TNET.2005.845545
   Seferoglu H., 2009, P INT PACK VID WORKS
   Shiang HP, 2007, IEEE J SEL AREA COMM, V25, P770, DOI 10.1109/JSAC.2007.070513
   Tan WT, 1999, IEEE T MULTIMEDIA, V1, P172, DOI 10.1109/6046.766738
   Taubman D., 2004, SVC CORE EXPERIMENT
   Turaga D. S., 2001, P PACK VID
   Wang B, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1352012.1352020
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu G, 2002, IEEE T AUTOMAT CONTR, V47, P979, DOI 10.1109/TAC.2002.1008362
   Zhang Q, 2001, IEEE T MULTIMEDIA, V3, P339, DOI 10.1109/6046.944477
   Zhu P, 2007, IEEE T MULTIMEDIA, V9, P366, DOI 10.1109/TMM.2006.886284
NR 32
TC 25
Z9 27
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 896
EP 909
DI 10.1109/TMM.2012.2187178
PN 2
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700020
DA 2024-07-18
ER

PT J
AU Chen, JW
   Xu, F
   He, Y
   Villasenor, J
   Han, YX
   Xu, Y
   Rong, YC
   Reader, C
   Wen, JT
AF Chen, Jianwen
   Xu, Feng
   He, Yun
   Villasenor, John
   Han, Yuxing
   Xu, Yan
   Rong, Yaocheng
   Reader, Cliff
   Wen, Jiangtao
TI Efficient Video Coding Using Legacy Algorithmic Approaches
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codec design; royalty; video coding; video standard
ID DECODER COMPLEXITY ANALYSIS; TRANSFORM
AB We show that for high bit rates, a video coding algorithm using a suitable combination of the QM coder and on other methods first published over 20 years ago can deliver video quality rivaling that of H.264 at lower complexity. This has implications both technically, since encoders built using these methods can be more power efficient, and commercially, given the complex licensing and intellectual property issues that accompany newer coding methods such as H.264 and MPEG-4. The methods described in this paper are the basis for the recent decision of the MPEG standards group to begin work on what is referred to as the "Type-1 Video Coding" standard, which, in addition to aiming for high coding efficiency, is intended to minimize royalty issues.
C1 [Chen, Jianwen; Villasenor, John; Han, Yuxing] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
   [Xu, Feng; He, Yun; Xu, Yan; Rong, Yaocheng] Tsinghua Unviers, Natl Lab Informat Sci & Technol, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Reader, Cliff] Peking Univ, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Wen, Jiangtao] Tsinghua Univ, Beijing 100084, Peoples R China.
C3 University of California System; University of California Los Angeles;
   Peking University; Tsinghua University
RP Chen, JW (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
EM jwchen@ee.ucla.edu; xu-feng@live.com; hey@tsinghua.edu.cn;
   villa@ee.ucla.edu; ericahan@ee.ucla.edu; xuyan850@gmail.com;
   yaocheng.rong@gmail.com; cliff@reader.com; jtwen@tsinghua.edu.cn
RI chen, jw/IQW-1558-2023; he, yun/JMB-6362-2023
FU  [973-2009CB320903];  [2010ZX03004-003]
FX This work was supported in part by Chinese 973-2009CB320903 and
   2010ZX03004-003. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Francesco G. B. De
   Natale.
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   [Anonymous], 1990, H21 ITUT
   [Anonymous], 1981, P NAT TEL C NEW ORL
   [Anonymous], 1989, CONTR SPEC GROUP COD
   [Anonymous], 1988, CONTR SPEC GROUP COD
   [Anonymous], 2010, RES MARK US RES BROA
   [Anonymous], 1987, CONTR SPEC GROUP COD
   [Anonymous], 2010, 36211 3 GPP
   [Anonymous], 2010, The H.264 Advanced Video Compression Standard
   [Anonymous], 2001, 15 M AUST TEX
   [Anonymous], 1986, CONTR SPEC GROUP COD
   [Anonymous], 1986, CONTR SPEC GROUP COD
   [Anonymous], 2010, 25308 3 GPP
   [Anonymous], 1989, 89168 MPEG
   [Anonymous], 2010, WEBM PROJECT EXPLORE
   [Anonymous], 1991, 11172 ISOIEC ISO CD
   [Anonymous], 1993, T82ISOIEC11544 ITUT
   [Anonymous], GEN COD MOV PICT ASS
   [Anonymous], 2010, CALL EV OPT 1 VID CO
   [Anonymous], 2011, MPEGLA MPEG LA ANNOU
   [Anonymous], 2010, WEBM PROJECT WEBM PR
   [Anonymous], 2010, WEBM PROJECT INTRO W
   [Anonymous], 1996, Techniques and standards for image, video, and audio coding
   [Anonymous], 1995, H263 ITUT
   Brofferio S., 1974, ALTA FREQUENZA   OCT, P836
   CAFFORIO C, 1979, SIGNAL PROCESS, V1, P133, DOI 10.1016/0165-1684(79)90015-X
   Chen J., 2011, MPEG2011M19455 ISOIE
   Chen J., 2010, MPEG2010M18570 ISOIE
   Chiariglione L., 2011, MPEG2011N11699 ISOIE
   Cisco white paper, 2010, CISC VIS NETW IND FO
   ERICSSON S, 1985, IEEE T COMMUN, V33, P1291, DOI 10.1109/TCOM.1985.1096251
   Ericsson S., 1985, P ICASSP MAR, P1081
   [FRG France GEC UK SEPA], 1987, FRG CONTR
   Gersho A., 1977, IEEE COMMUN MAG  SEP, P6
   GIORDA F, 1975, IEEE T COMMUN, V23, P1002, DOI 10.1109/TCOM.1975.1092910
   Glidden R., 2011, MPEG2011 ISOIECJTC1S
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   ITU-T, 1992, T81 ITUT
   J Jaswant R., 1979, INTERFRAME ADAPTIVE
   [KDD NTT NEC Fujitsu], 1986, KDD CONTR
   Koga T., 1983, P ICC83, V2, P1161
   Langdon J., 1990, WEBM PROJECT FREQUEN, Patent No. 4905297
   Langdon Jr G. G., 1990, uS Patent, Patent No. [4,905,297, 4905297]
   Lee SW, 2011, J VIS COMMUN IMAGE R, V22, P61, DOI 10.1016/j.jvcir.2010.10.004
   Micke T., 1986, THESIS U HANOVER HAN
   MITCHELL JL, 1988, IBM J RES DEV, V32, P727, DOI 10.1147/rd.326.0727
   NETRAVALI AN, 1979, AT&T TECH J, V58, P631, DOI 10.1002/j.1538-7305.1979.tb02238.x
   NINOMIYA Y, 1982, IEEE T COMMUN, V30, P201, DOI 10.1109/TCOM.1982.1095381
   [NTT KDD NEC Fujitsu], 1986, NTT CONTR
   Paczkowski J., 2010, GOOGLES ROYALTY FREE
   Pennebaker W. B., 1993, JPEG: Still image data compression standard
   PRATT WK, 1969, P IEEE, V57, P58, DOI 10.1109/PROC.1969.6869
   Rao K.R, 2014, DISCRETE COSINE TRAN
   Taki Y., 1974, Proc. Institute of Electronics and Communication Engineers Jpn. Annu. Conv.(IECEJ), P1263
   TASTO M, 1971, IEEE T COMMUN TECHN, VCO19, P957, DOI 10.1109/TCOM.1971.1090788
   Telese O., 1981, P IEEE S INF THEOR F
   U.K, 1987, UK CONTR
   U.K, 1986, UK CONTR
   Urban S. J., 1992, Journal of Electronic Imaging, V1, P5, DOI 10.1117/12.55177
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen JT, 1999, IEEE T INFORM THEORY, V45, P1307, DOI 10.1109/18.761289
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WINTZ PA, 1972, PR INST ELECTR ELECT, V60, P809, DOI 10.1109/PROC.1972.8780
   Woods J. W., 1969, P S PICT BANDW COMPR
   Zhang L., 2011, MPEG2011M19454 ISOIE
NR 65
TC 6
Z9 6
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 111
EP 120
DI 10.1109/TMM.2011.2169046
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100011
DA 2024-07-18
ER

PT J
AU Kang, LW
   Hsu, CY
   Chen, HW
   Lu, CS
   Lin, CY
   Pei, SC
AF Kang, Li-Wei
   Hsu, Chao-Yung
   Chen, Hung-Wei
   Lu, Chun-Shien
   Lin, Chih-Yang
   Pei, Soo-Chang
TI Feature-Based Sparse Representation for Image Similarity Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature detection; image copy detection; image recognition; image
   retrieval; image similarity assessment; sparse representation
ID COMPUTER VISION; CLASSIFICATION; PERFORMANCE; ALGORITHMS
AB Assessment of image similarity is fundamentally important to numerous multimedia applications. The goal of similarity assessment is to automatically assess the similarities among images in a perceptually consistent manner. In this paper, we interpret the image similarity assessment problem as an information fidelity problem. More specifically, we propose a feature-based approach to quantify the information that is present in a reference image and how much of this information can be extracted from a test image to assess the similarity between the two images. Here, we extract the feature points and their descriptors from an image, followed by learning the dictionary/basis for the descriptors in order to interpret the information present in this image. Then, we formulate the problem of the image similarity assessment in terms of sparse representation. To evaluate the applicability of the proposed feature-based sparse representation for image similarity assessment (FSRISA) technique, we apply FSRISA to three popular applications, namely, image copy detection, retrieval, and recognition by properly formulating them to sparse representation problems. Promising results have been obtained through simulations conducted on several public datasets, including the Stirmark benchmark, Corel-1000, COIL-20, COIL-100, and Caltech-101 datasets.
C1 [Kang, Li-Wei; Hsu, Chao-Yung; Chen, Hung-Wei; Lu, Chun-Shien] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   [Hsu, Chao-Yung; Chen, Hung-Wei; Pei, Soo-Chang] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10764, Taiwan.
   [Lin, Chih-Yang] Asia Univ, Dept Comp Sci & Informat Engn, Taichung, Taiwan.
   [Pei, Soo-Chang] Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
   [Pei, Soo-Chang] Natl Taiwan Univ, Coll Elect Engn & Comp Sci, Taipei 10764, Taiwan.
C3 Academia Sinica - Taiwan; National Taiwan University; Asia University
   Taiwan; National Taiwan University; National Taiwan University
RP Kang, LW (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
EM lcs@iis.sinica.edu.tw
RI cai, bo/G-1491-2010; Lin, Chih-Yang/HOF-2583-2023
OI Lin, Chih-Yang/0000-0002-0401-8473
FU National Science Council, Taiwan [NSC97-2628-E-001-011-MY3,
   NSC98-2631-H-001-013, NSC98-2811-E-001-008, NSC99-2218-E-001-010,
   NSC99-2811-E-001-006, NSC 99-2221-E-468-023]
FX Manuscript received October 31, 2010; revised March 03, 2011 and May 14,
   2011; accepted May 31, 2011. Date of publication June 09, 2011; date of
   current version September 16, 2011. This work was supported in part by
   the National Science Council, Taiwan, under Grants
   NSC97-2628-E-001-011-MY3, NSC98-2631-H-001-013, NSC98-2811-E-001-008,
   NSC99-2218-E-001-010, NSC99-2811-E-001-006, and NSC 99-2221-E-468-023. A
   preliminary version of this manuscript was presented in the 2010 IEEE
   International Conference on Multimedia and Expo [8]. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Ming-Ting Sun.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2008, TECHNION
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2009, P IEEE C COMP VIS PA
   Banerjee M, 2009, FUZZY SET SYST, V160, P3323, DOI 10.1016/j.fss.2009.02.024
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Cevikalp H, 2010, PATTERN RECOGN LETT, V31, P1285, DOI 10.1016/j.patrec.2010.03.009
   CHANDRASEKHAR V, 2010, P INT WORKSH MOB MUL
   Fadili MJ, 2010, P IEEE, V98, P983, DOI 10.1109/JPROC.2009.2024776
   Hsu Chao-Yung., 2009, ACM International Conference on Multimedia, P637
   HSU CY, 2011, P IS T SPIE MED WAT
   Kang L. W., 2010, P IEEE INT C MULT EX
   KE Y, 2004, P ACM MULT
   Kim C, 2003, SIGNAL PROCESS-IMAGE, V18, P169, DOI 10.1016/S0923-5965(02)00130-3
   Liu YN, 2010, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2010.5539934
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Monga V, 2006, IEEE T IMAGE PROCESS, V15, P3452, DOI 10.1109/TIP.2006.881948
   Nene S., 1996, CUCS00596 COL OBJ IM
   Nene S. A., 1996, CUCS00696 COL OBJ IM
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Papakostas GA, 2010, PATTERN RECOGN, V43, P58, DOI 10.1016/j.patcog.2009.05.008
   Pele O, 2008, LECT NOTES COMPUT SC, V5304, P495, DOI 10.1007/978-3-540-88690-7_37
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Riesen K, 2009, IMAGE VISION COMPUT, V27, P950, DOI 10.1016/j.imavis.2008.04.004
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892
   XU Z, 2010, MULTIMEDIA TOOLS JAN
   YANG A, 2010, P IEEE INT C IM PROC
   Yang AY, 2010, P IEEE, V98, P1077, DOI 10.1109/JPROC.2010.2040797
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhang TH, 2010, IEEE T SYST MAN CY B, V40, P253, DOI 10.1109/TSMCB.2009.2027473
   Zhao WL, 2009, IEEE T IMAGE PROCESS, V18, P412, DOI 10.1109/TIP.2008.2008900
NR 42
TC 49
Z9 56
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1019
EP 1030
DI 10.1109/TMM.2011.2159197
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300015
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, T
   Wang, JD
   Sun, J
   Zheng, NN
   Tang, XO
   Shum, HY
AF Liu, Tie
   Wang, Jingdong
   Sun, Jian
   Zheng, Nanning
   Tang, Xiaoou
   Shum, Heung-Yeung
TI Picture Collage
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 1-D collage; interactive collage; Markov chain Monto Carlo (MCMC)
   optimization; picture collage
AB In this paper, we address a novel problem of automatically creating a picture collage from a group of images. Picture collage is a kind of visual image summary-to arrange all input images on a given canvas, allowing overlay, to maximize visible visual information. We formulate the picture collage creation problem in a conditional random field model, which integrates image salience, canvas constraint, natural preference, and user interaction. Each image is represented by a group of weighted rectangles, which indicate the salient regions. Then picture collage is resolved by minimizing the energy, guided by the constraints. A two-step optimization method is proposed. First, a quick initialization algorithm based on the proposed 1-D collage method is presented. Second, a very efficient Markov chain Monte Carlo method is designed for the refined optimization. We also integrate user interaction in the formulation and optimization to obtain an interactive collage reflecting personalized preference. Visual and quantitative experimental evaluations indicate the efficiency of the proposed collage creation technique.
C1 [Liu, Tie] IBM Corp, China Res Lab, Analyt & Optimizat Dept, Beijing 100193, Peoples R China.
   [Wang, Jingdong] Microsoft Res Asia, Media Comp Grp, Beijing 100190, Peoples R China.
   [Sun, Jian] Microsoft Res Asia, Visual Comp Grp, Beijing 100190, Peoples R China.
   [Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
   [Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
   [Shum, Heung-Yeung] Microsoft, On Line Serv Div, RD, Redmond, WA 98052 USA.
C3 International Business Machines (IBM); Microsoft; Microsoft Research
   Asia; Microsoft Research Asia; Microsoft; Xi'an Jiaotong University;
   Chinese University of Hong Kong; Microsoft
RP Liu, T (corresponding author), IBM Corp, China Res Lab, Analyt & Optimizat Dept, Beijing 100193, Peoples R China.
EM liutiel@gmail.com; jingdw@microsoft.com; jiansun@microsoft.com;
   nnzheng@mail.xjtu.edu.cn; xtang@ie.cuhk.edu.hk; hshum@microsoft.com
RI Wang, Jingdong/E-9920-2017; Tang, Xiaoou/G-6509-2012; Liu,
   Tie/JXX-0080-2024
OI Wang, Jingdong/0000-0002-4888-4445; Liu, Tie/0000-0002-3251-0158
FU National Science Foundation of China [60635050]
FX Manuscript received October 20, 2008; revised July 07, 2009. First
   published August 21, 2009; current version published October 16, 2009.
   The work of T. Liu and N. Zheng was supported by a grant from the
   National Science Foundation of China ( No. 60635050). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Nadia Magnenat-Thalmann.
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   [Anonymous], 2006, P SIGCHI C HUM FACT
   [Anonymous], 2003, ACMMM
   [Anonymous], 2002, Monte Carlo strategies in scientific computing
   [Anonymous], P CVPR
   ATKINS CB, 2004, P ICIP, P2897
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800
   Fei-Fei L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1134, DOI 10.1109/ICCV.2003.1238476
   Geigel J, 2003, IEEE MULTIMEDIA, V10, P16, DOI 10.1109/MMUL.2003.1237547
   GIRGENSOHN A, 2004, P 17 ANN ACM S US IN
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jojic N, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P34
   Lafferty John, 2001, INT C MACH LEARN ICM
   Milenkovic VJ, 1999, COMP GEOM-THEOR APPL, V13, P3, DOI 10.1016/S0925-7721(99)00006-1
   Murata H, 1995, 1995 IEEE/ACM INTERNATIONAL CONFERENCE ON COMPUTER-AIDED DESIGN, P472, DOI 10.1109/ICCAD.1995.480159
   QIN Z, 2002, J COMP PHYS, V172, P827
   Rother C, 2005, PROC CVPR IEEE, P589
   Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Xiao R, 2004, IEEE T CIRC SYST VID, V14, P31, DOI 10.1109/TCSVT.2003.818351
NR 20
TC 24
Z9 31
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1225
EP 1239
DI 10.1109/TMM.2009.2030741
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300002
DA 2024-07-18
ER

PT J
AU Vukobratovic, D
   Stankovic, V
   Sejdinovic, D
   Stankovic, L
   Xiong, ZX
AF Vukobratovic, Dejan
   Stankovic, Vladimir
   Sejdinovic, Dino
   Stankovic, Lina
   Xiong, Zixiang
TI Scalable Video Multicast Using Expanding Window Fountain Codes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME 2008)
CY JUN 23-26, 2008
CL Hannover, GERMANY
SP IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Commun Soc, IEEE Comp Soc
DE Fountain codes; H264 SVC; scalable video multicast; unequal error
   protection
ID ERROR PROTECTION
AB Fountain codes were introduced as an efficient and universal forward error correction (FEC) solution for data multicast over lossy packet networks. They have recently been proposed for large scale multimedia content delivery in practical multimedia distribution systems. However, standard fountain codes, such as LT or Raptor codes, are not designed to meet unequal error protection (UEP) requirements typical in real-time scalable video multicast applications. In this paper, we propose recently introduced UEP expanding window fountain (EWF) codes as a flexible and efficient solution for real-time scalable video multicast. We demonstrate that the design flexibility and UEP performance make EWF codes ideally suited for this scenario, i.e., EWF codes offer a number of design parameters to be "tuned" at the server side to meet the different reception criteria of heterogeneous receivers. The performance analysis using both analytical results and simulation experiments of H. 264 scalable video coding (SVC) multicast to heterogeneous receiver classes confirms the flexibility and efficiency of the proposed EWF-based FEC solution.
C1 [Vukobratovic, Dejan] Univ Novi Sad, Dept Power Elect & Commun Engn, Novi Sad 21000, Serbia.
   [Vukobratovic, Dejan; Stankovic, Vladimir; Stankovic, Lina] Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XW, Lanark, Scotland.
   [Sejdinovic, Dino] Univ Bristol, Ctr Commun Res, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
   [Xiong, Zixiang] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.
C3 University of Novi Sad; University of Strathclyde; University of
   Bristol; Texas A&M University System; Texas A&M University College
   Station
RP Vukobratovic, D (corresponding author), Univ Novi Sad, Dept Power Elect & Commun Engn, Novi Sad 21000, Serbia.
EM dejanv@uns.ac.rs; vladimir.stankovic@eee.strath.ac.uk;
   d.sejdinovic@bristol.ac.uk; lina.stankovic@eee.strath.ac.uk;
   zx@ece.tamu.edu
RI Stankovic, Ljubisa/J-8988-2013; Vukobratovic, Dejan/HHZ-7827-2022;
   Stankovic, Vladimir/L-6584-2016
OI Stankovic, Lina/0000-0002-8112-1976; Stankovic,
   Vladimir/0000-0002-1075-2420; Vukobratovic, Dejan/0000-0002-5305-8420;
   Sejdinovic, Dino/0000-0001-5547-9213
CR [Anonymous], 2004, DIGITAL VIDEO BROADC
   BARQUERO DG, 2009, WILEY J WIRELESS COM, V9, P733
   Byers JW, 2002, IEEE J SEL AREA COMM, V20, P1528, DOI 10.1109/JSAC.2002.803996
   Chou PA, 2001, IEEE T MULTIMEDIA, V3, P108, DOI 10.1109/6046.909598
   Dimitrios F, 2009, IEEE INT C EMERG
   GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683
   Gasiba T., 2006, P 4 INT S TURB COD R
   Hamzaoui R, 2005, IEEE SIGNAL PROC MAG, V22, P91, DOI 10.1109/MSP.2005.1550192
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Schierl T, 2008, J VIS COMMUN IMAGE R, V19, P500, DOI 10.1016/j.jvcir.2008.06.004
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SEJDINOVIC D, 2007, P 41 ANN AS MAR
   SEJDINOVIC D, 1917, IEEE T COMM IN PRESS
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Stockhammer T, 2007, WIREL COMMUN MOB COM, V7, P235, DOI 10.1002/wcm.476
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   *UMTS MBMS, 2005, PROT COD
   VUKOBRATOVIC D, 2007, P 45 ANN ALL C MONT
   VUKOBRATOVIC D, 2008, P IEEE ICME INT C MU
   Wagner JP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1501, DOI 10.1109/ICME.2006.262827
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu Q, 2007, IEEE T CIRC SYST VID, V17, P901, DOI 10.1109/TCSVT.2007.897464
NR 25
TC 105
Z9 117
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1094
EP 1104
DI 10.1109/TMM.2009.2026087
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700006
DA 2024-07-18
ER

PT J
AU Fu, CW
   Wan, L
   Wong, TT
   Leung, CS
AF Fu, Chi-Wing
   Wan, Liang
   Wong, Tien-Tsin
   Leung, Chi-Sing
TI The Rhombic Dodecahedron Map: An Efficient Scheme for Encoding Panoramic
   Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graphics processing unit (GPU); omnidirectional video; panoramic video;
   rhombic dodecahedron; spherical mapping; video encoding
ID IMAGE COMPRESSION
AB Onmidirectional videos are usually mapped to planar domain for encoding with off-the-shelf video compression standards. However, existing work typically neglects the effect of the sphere-to-plane mapping. In this paper, we show that by carefully designing the mapping, we can improve the visual quality, stability and compression efficiency of encoding onmidirectional videos. Here we propose a novel mapping scheme, known as the rhombic dodecahedron map (RD map) to represent data over the spherical domain. By using a family of skew great circles as the subdivision kernel, the RD map not only produces a sampling pattern with very low discrepancy, it can also support a highly efficient data indexing mechanism over the spherical domain. Since the proposed map is quad-based, geodesic-aligned, and of very low area and shape distortion, we can reliably apply 2-D wavelet-based and DCT-based encoding methods that are originally designated to planar perspective videos. At the end, we perform a series of analysis and experiments to investigate and verify the effectiveness of the proposed method; with its ultra-fast data indexing capability, we show that we can playback onmidirectional videos with very high frame rates on conventional PCs with GPU support.
C1 [Fu, Chi-Wing] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Wan, Liang; Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Leung, Chi-Sing] City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
C3 Nanyang Technological University; Chinese University of Hong Kong; City
   University of Hong Kong
RP Fu, CW (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM cwfu@ntu.edu.sg; lwan@cse.cuhk.edu.hk; ttwong@cse.cuhk.edu.hk;
   eeleungc@cityu.edu.hk
RI FU, Chi-Wing/A-3716-2011; Fu, Chi-Wing/X-4703-2019
OI LEUNG, Chi Sing Andrew/0000-0003-0962-6723; Fu, Chi
   Wing/0000-0002-5238-593X
FU Research Grants Council of the Hong Kong Special Administrative Region;
   General Research Funds from Hong Kong Government [CUHK417107,
   CityU116508]; MOE AcRF Tier I Grant [13/08]
FX Manuscript received June 06, 2007; revised January 30, 2009. First
   published April 28, 2009; current version published May 15, 2009. This
   work was supported in part by the Research Grants Council of the Hong
   Kong Special Administrative Region, under General Research Funds from
   Hong Kong Government (Project No. CUHK417107 and CityU116508) and in
   part by the MOE AcRF Tier I Grant (Reference no. RG 13/08). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Sheila S. Hemami.
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Agarwala A, 2005, ACM T GRAPHIC, V24, P821, DOI 10.1145/1073204.1073268
   BAUERMANN I, 2004, P INT C COMP VIS GRA, P22
   Buss SR, 2001, ACM T GRAPHIC, V20, P95, DOI 10.1145/502122.502124
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Cui JJ, 1997, SIAM J SCI COMPUT, V18, P595, DOI 10.1137/S1064827595281344
   Dobkin DP, 1996, ACM T GRAPHIC, V15, P354, DOI 10.1145/234535.234536
   Foote J., 2000, ACM MULTIMEDIA, P487
   Grünheit C, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P209, DOI 10.1109/ICIP.2002.1038942
   Kang SB, 2000, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2000.855820
   Kimber Don., 2001, Proceedings of the ninth ACM international conference on Multimedia-MULTIMEDIA '01, P339
   Lee Dar-Shyang., 2002, P 10 ACM INT C MULT, P493
   LIAO C, 2003, P ACM MULT 2003 NOV, P546
   Majumder A, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P169, DOI 10.1145/319463.319485
   Meehan J., 1990, PANORAMIC PHOTOGRAPH
   Nayar SK, 1997, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1997.609369
   NEUMANN U, 2000, P ACM MULT 2000 OCT, P493
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   Niederreiter H, 1992, RANDOM NUMBER GENERA, V63
   Onoe Y, 1998, INT C PATT RECOG, P588, DOI 10.1109/ICPR.1998.711211
   *POINT GREY RES, LADYBUG2
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   SHIRLEY P, 1991, P EUR 91, P183
   Shum HY, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P644, DOI 10.1109/ICIP.2000.899536
   Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Smolic A, 2004, IEEE T CIRC SYST VID, V14, P348, DOI 10.1109/TCSVT.2004.823395
   SNYDER J., 2001, SAMPLING EFFICIENT M
   SNYDER JP, 1993, FLATTENING EARTH 2 T
   Sun X., 2001, P ACM MULT, P329, DOI DOI 10.1145/500141.500191
   SVOBODA T, 1998, P 5 EUR C COMP VIS, P218
   Szeliski Richard., 1997, P SIGGRAPH 97 COMPUT, P251, DOI DOI 10.1145/258734.258861
   Tang WK, 2005, IEEE T MULTIMEDIA, V7, P280, DOI 10.1109/TMM.2005.843811
   Usevitch BE, 2001, IEEE SIGNAL PROC MAG, V18, P22, DOI 10.1109/79.952803
   Weisstein Eric W., RHOMBIC DODECAHEDRON
   Wenninger M., 1979, Spherical Models
   Wong TT, 2001, IEEE COMPUT GRAPH, V21, P32, DOI 10.1109/38.909013
   WONG TT, 1997, ACM J GRAPH TOOLS, V2, P9
NR 39
TC 64
Z9 70
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 634
EP 644
DI 10.1109/TMM.2009.2017626
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900006
DA 2024-07-18
ER

PT J
AU Li, T
   Ogihara, M
   Peng, W
   Shao, B
   Zhu, SH
AF Li, Tao
   Ogihara, Mitsunori
   Peng, Wei
   Shao, Bo
   Zhu, Shenghuo
TI Music Clustering With Features From Different Information Sources
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clustering; different information sources; machine learning; music
   information retrieval
AB Efficient and intelligent music information retrieval is a very important topic of the 21st century. With the ultimate goal of building personal music information retrieval systems, this paper studies the problem of identifying "similar" artists using features from diverse information sources. In this paper, we first present a clustering algorithm that integrates features from both sources to perform bimodal learning. We then present an approach based on the generalized constraint clustering algorithm by incorporating the instance-level constraints. The algorithms are tested on a data set consisting of 570 songs from 53 albums of 41 artists using artist similarity provided by All Music Guide. Experimental results show that the accuracy of artist similarity identification can be significantly improved.
C1 [Li, Tao; Peng, Wei; Shao, Bo] Florida Int Univ, Sch Comp Sci, Miami, FL 33199 USA.
   [Ogihara, Mitsunori] Univ Miami, Dept Comp Sci, Coral Gables, FL 33146 USA.
   [Zhu, Shenghuo] NEC Labs America, Cupertino, CA 95014 USA.
C3 State University System of Florida; Florida International University;
   University of Miami
RP Li, T (corresponding author), Florida Int Univ, Sch Comp Sci, Miami, FL 33199 USA.
RI Ogihara, Mitsunori/AAB-8275-2020
OI Ogihara, Mitsunori/0000-0002-5690-7854
FU Ministry of Education of China at Fuzhou University
FX Manuscript received November 29, 2007: revised September 30, 2008. First
   published March 04, 2009: current version published March 18, 2009. The
   work of T. Li was supported in part by NSF Career Award IIS-054680. This
   work was supported in part by the Open Research Fund of the Lab of
   Spatial Data Mining and Information Sharing of Ministry of Education of
   China at Fuzhou University. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Horace
   Ho-Shing Ip.
CR ABENY S, 2002, P 40 ANN M ASS COMP, P360
   [Anonymous], 2002, P 19 INT C MACHINE L
   [Anonymous], 2002, ADV NEURAL INF PROCE
   [Anonymous], P INT C MACH LEARN
   [Anonymous], P SIAM INT C DAT MIN
   Basu S., 2004, P 10 ACM SIGKDD INT, P59, DOI DOI 10.1145/1014052.1014062
   Beck R, 1996, AGRO FOOD IND HI TEC, V7, P3
   BENNETT KP, 1999, ARTIF NEURAL NETW EN, P809
   Bill E., 1994, P 12 NATL C ARTIFICI, V1, P722
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Cilibrasi R, 2004, COMPUT MUSIC J, V28, P49, DOI 10.1162/0148926042728449
   Collins M., 1999, P JOINT SIGDAT C EMP
   Dasgupta S, 2002, ADV NEUR IN, V14, P375
   DAVIS J.S., 2000, Global NEST Journal, V2, P217, DOI [10.30955/gnj.000175, DOI 10.30955/GNJ.000175]
   de Sa VR, 1998, NEURAL COMPUT, V10, P1097, DOI 10.1162/089976698300017368
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Ding C, 2006, P 12 ACM SIGKDD INT, P126, DOI 10.1145/1150402.1150420
   GIONIS A, 2005, CLUSTERING AGGREGATI, P341
   Goldman SallyA., 2000, Proceedings of the Seventeenth International Conference on Machine Learning. ICML'00, P327
   Li T, 2005, KNOWL INF SYST, V7, P289, DOI 10.1007/s10115-004-0155-8
   LI T, 2003, P 26 ANN INT ACM SIG, P282
   Li T, 2006, IEEE DATA MINING, P372
   MILLIGAN GW, 1986, MULTIVAR BEHAV RES, V21, P441, DOI 10.1207/s15327906mbr2104_5
   MITTON R, 1987, INFORM PROCESS MANAG, V23, P495, DOI 10.1016/0306-4573(87)90116-6
   Montenegro YV, 2003, PARASITOL RES, V91, P1, DOI 10.1007/s00436-003-0901-y
   Nigam K., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P86, DOI 10.1145/354756.354805
   Pampalk E., 2002, Proceedings of the tenth ACM international conference on Multimedia, P570, DOI DOI 10.1145/641007.641121
   Peng W., 2007, P 8 INT C MUSIC INFO, P27
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Roth D, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P639
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Tsai WH, 2004, COMPUT MUSIC J, V28, P68, DOI 10.1162/0148926041790630
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Wagstaff K., 2001, P 18 INT C MACH LEAR, P577, DOI DOI 10.1109/TPAMI.2002.1017616
   Wu LZ, 1999, IEEE T MULTIMEDIA, V1, P334, DOI 10.1109/6046.807953
   Zhao Y, 2004, MACH LEARN, V55, P311, DOI 10.1023/B:MACH.0000027785.44527.d6
NR 36
TC 7
Z9 8
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 477
EP 485
DI 10.1109/TMM.2009.2012942
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300013
DA 2024-07-18
ER

PT J
AU Zhang, C
   Yin, P
   Rui, Y
   Cutler, R
   Viola, P
   Sun, XD
   Pinto, N
   Zhang, ZY
AF Zhang, Cha
   Yin, Pei
   Rui, Yong
   Cutler, Ross
   Viola, Paul
   Sun, Xinding
   Pinto, Nelson
   Zhang, Zhengyou
TI Boosting-Based Multimodal Speaker Detection for Distributed Meeting
   Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audiovisual fusion; boosting; speaker detection
ID TRACKING
AB Identifying the active speaker in a video of a distributed meeting can be very helpful for remote participants to understand the dynamics of the meeting. A straightforward application of such analysis is to stream a high resolution video of the speaker to the remote participants. In this paper, we present the challenges we met while designing a speaker detector for the Microsoft RoundTable distributed meeting device, and propose a novel boosting-based multimodal speaker detection (BMSD) algorithm. Instead of separately performing sound source localization (SSL) and multiperson detection (MPD) and subsequently fusing their individual results, the proposed algorithm fuses audio and visual information at feature level by using boosting to select features from a combined pool of both audio and visual features simultaneously. The result is a very accurate speaker detector with extremely high efficiency. In experiments that includes hundreds of real-world meetings, the proposed BMSD algorithm reduces the error rate of SSL-only approach by 24.6%, and the SSL and MPD fusion approach by 20.9%. To the best of our knowledge, this is the first real-time multimodal speaker detection algorithm that is deployed in commercial products.
C1 [Zhang, Cha; Rui, Yong; Viola, Paul; Zhang, Zhengyou] Microsoft Res, Redmond, WA 98052 USA.
   [Yin, Pei] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
   [Cutler, Ross; Sun, Xinding; Pinto, Nelson] Microsoft Corp, Redmond, WA 98052 USA.
C3 Microsoft; University System of Georgia; Georgia Institute of
   Technology; Microsoft
RP Zhang, C (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM chazhang@microsoft.com; pyin@cc.gatech.edu; yongrui@microsoft.com;
   rcutler@microsoft.com; viola@microsoft.com; xindings@microsoft.com;
   npinto@microsoft.com; zhang@microsoft.com
RI Zhang, Zhang/JAX-2097-2023; zhang, ZY/HJH-6535-2023; Cutler,
   Ross/GWM-5862-2022; zhang, zheng/HCH-9684-2022
CR [Anonymous], 2000, SYSTEM VIDEO SURVEIL
   [Anonymous], 2005, P CVPR
   [Anonymous], SPEECHREADING HUMANS
   BEAL MJ, 2002, P ECCV
   BESSON P, 2005, INFORM THEORETIC OPT
   Brand Matthew., 1997, P IEEE CVPR
   BRANDSTEIN M, 1997, P ICASSP
   Busso C., 2005, P IEEE ICASSP
   Chen YQ, 2004, P IEEE, V92, P485, DOI 10.1109/JPROC.2003.823146
   Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537
   CUTLER R, 2000, P IEEE ICME
   CUTLER R, 2002, P ACM C MULT
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   FISHER J, 2000, NIPS, P772
   Friedman J., 1998, ADDITIVE LOGISTIC RE
   GOODRIDGE SG, 1997, THESIS N CAROLINA ST
   GUSTAFSSON T, 2001, P ICASSP
   HERSHEY J, 2000, ADV NEURAL INFORM PR
   HSU W, 2004, SPIE EL IM
   IYENGAR G, 2000, INT RIAO C
   KAPRALOS B, 2002, AUDIO VISUAL LOCALIZ
   Mason L., 2000, NIPS
   NAPHADE M, 2001, P IEEE ICME
   NICKEL K, 2005, ICMI
   Nock H., 2003, P CIVR
   PAVLOVIC V, 2001, P IEEE CVPR
   RUI Y, 2005, P IEEE ICASSP
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   VERMAAK J, 2001, P IEEE ICCV
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   VIOLA P, 2003, P IEEE ICCV
   WANG H, 1997, P IEEE ICASSP
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   YOSHIMI BH, 2002, P ACM C MULT
   ZHANG C, 2007, ICASSP
   Zhang C., 2007, NIPS
   Zotkin DN, 2002, EURASIP J APPL SIG P, V2002, P1154, DOI 10.1155/S1110865702206058
NR 37
TC 31
Z9 38
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1541
EP 1552
DI 10.1109/TMM.2008.2007344
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600010
DA 2024-07-18
ER

PT J
AU Huang, CH
   Chuang, SC
   Wu, JL
AF Huang, Chun-Hsian
   Chuang, Shang-Chih
   Wu, Ja-Ling
TI Digital-invisible-ink data hiding based on spread-spectrum and
   quantization techniques
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE digital invisible ink (DII); plausible deniability; steganalysis;
   steganography
ID WATERMARKING
AB A novel data-hiding methodology, denoted as digital invisible ink (DII), is proposed to implement secure steganography systems. Like the real-world invisible ink, secret messages will be correctly revealed only after the marked works undergo certain prenegotiated manipulations, such as lossy compression and processing. Different from conventional data-hiding schemes where content processing or compression operations are undesirable, distortions caused by prenegotiated manipulations in DII-based schemes are indispensable steps for revealing genuine secrets' The proposed scheme is carried out based on two important data-hiding schemes: spread-spectrum watermarking and frequency-domain quantization,watermarking. In some application scenarios, the DII-based steganography system can provide plausible deniability and enhance the secrecy by taking cover with other messages. We show that DII-based schemes are indeed superior to existing plausibly deniable steganography approaches in many aspects. Moreover, potential security holes caused by deniable steganography systems are discussed.
C1 [Huang, Chun-Hsian; Chuang, Shang-Chih; Wu, Ja-Ling] Natl Taiwan Univ, Taipei 10764, Taiwan.
C3 National Taiwan University
RP Huang, CH (corresponding author), Natl Taiwan Univ, Taipei 10764, Taiwan.
EM bh@cmlab.csie.ntu.edu.tw; peiz@cmlab.csie.ntu.edu.tw;
   wjl@cmlab.csie.ntu.edu.tw
OI WU, JA-LING/0000-0002-3631-1551; Chuang, Shang-Chih/0000-0002-5801-6967
FU Ministry of Education of Taiwan [95R0062-AE00-02]
FX This work was supported in part by the Ministry of Education of Taiwan,
   R.O.C., under the Grant of Excellent Research Projects of National
   Taiwan University, 95R0062-AE00-02. The associate editor coordinating
   the review of this paper and approving it for publication was Dr. Mohan
   Kankanhalli.
CR [Anonymous], 2000, Digital Watermarking
   BAUER FL, 2000, DECRYPTED SECRETS ME, pCH1
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Cox I. J., 2002, Digital Watermarking
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   CRAVER S, 1998, P IH 98 PORTL OR APR
   FRIDRICH J, 2002, SPIE, V4675
   FRIDRICH J, 2001, SPIE MMSA
   FURHT B, 2005, MULTIMEDIA SECURIT 4
   FURHT B, 2005, MULTIMEDIA SECURIT 3
   Hsu CT, 1999, IEEE T IMAGE PROCESS, V8, P58, DOI 10.1109/83.736686
   HUANG CH, 2006, ACM MULT SEC WORKSH
   HUANG CH, 2006, ACM S INF COMP COMM
   HUANG CH, 2000, ACSSC 2000
   Johnson N.F., 1998, IEEE INF TECHN C NEW IEEE INF TECHN C NEW
   Johnson N. F., 1998, LECT NOTES COMPUTER, V1525
   LIN ET, 1999, P IM PROC IM QUAL IM
   Lu C.-S., 2004, Multimedia Security: Steganography and Digital Watermarking Techniques for Protection of Intellectual Property: Steganography and Digital Watermarking Techniques for Protection of Intellectual Property
   Lu CS, 2000, IEEE T MULTIMEDIA, V2, P209, DOI 10.1109/6046.890056
   MARVAR HS, 2003, IEEE T SIGNAL PROCES, V51, P898
   PROVOS N, 2001, 10 USENIX SEC S WASH
   Rigden Denis., 2004, SOE Syllabus: Lessons in Ungentlemanly Warfare
   SIMMONS GJ, CRYPTOGRAPHY
   SWANSON M, 1997, P INT C IM PROC
   WU M, 2003, MULTIMEDIA DATA HIDI
   2006, CHALLENGE 1 2 WAVILA
NR 26
TC 10
Z9 12
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 557
EP 569
DI 10.1109/TMM.2008.921733
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200001
OA Green Published
DA 2024-07-18
ER

PT J
AU Yang, Y
   Zhuang, YT
   Wu, F
   Pan, YH
AF Yang, Yi
   Zhuang, Yue-Ting
   Wu, Fei
   Pan, Yun-He
TI Harmonizing hierarchical manifolds for multimedia document semantics
   understanding and cross-media retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cross-media retrieval; hierarchical manifold learning; multimedia
   document
ID DISCRIMINANT-ANALYSIS; RELEVANCE-FEEDBACK; CLASSIFICATION; SEARCH
AB In this paper, we consider the problem of multimedia document (MMD) semantics understanding and content-based cross-media retrieval. An MMD is a set of media objects of different modalities but carrying the same semantics and the content-based cross-media retrieval is a new kind of retrieval method by which the query examples and search results can be of different modalities. Two levels of manifolds are learned to explore the relationships among all the data in the level of MMD and in the level of media object respectively. We first construct a Laplacian media object space for media object representation of each modality and an MMD semantic graph to learn the MMD semantic correlations. The characteristics of media objects propagate along the MMD semantic graph and an MMD semantic space is constructed to perform cross-media retrieval. Different methods are proposed to utilize relevance feedback and experiment shows that the proposed approaches are effective.
C1 [Yang, Yi; Zhuang, Yue-Ting; Wu, Fei; Pan, Yun-He] Zhejiang Univ, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Yang, Y (corresponding author), Zhejiang Univ, Hangzhou 310003, Zhejiang, Peoples R China.
EM yangyi_zju@yahoo.com.cn; yzhuang@cs.zju.edu.cn; wufei@cs.zju.edu.cn
RI Lang, Ming/HIK-0758-2022; yang, yang/GWB-9426-2022; Yang,
   Yi/B-9273-2017; yang, yang/GVT-5210-2022; yang, yang/HGT-7999-2022
OI Yang, Yi/0000-0002-0512-880X; 
FU National Natural Science Foundation of China [60533090, 60525108,
   2002CB312101]; Science and Technology Project of Zhejiang Province
   [2006C 13097, 2005C13032]; Program for Changjiang Scholars and
   Innovative Re-search Team in University [IRT0652]; Key Technology RD
   Program [2006BAH 02A 13-4]; China-U.S. Million Book Digital Library
   Project
FX This work was supported by the National Natural Science Foundation of
   China (60533090 and 60525108), by the 973 Program (2002CB312101), by the
   Science and Technology Project of Zhejiang Province (2006C 13097;
   2005C13032), by the Program for Changjiang Scholars and Innovative
   Re-search Team in University (IRT0652,PCSIRT), by the Key Technology R&D
   Program (2006BAH 02A 13-4), and by the China-U.S. Million Book Digital
   Library Project. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jiebo Luo.
CR [Anonymous], 2003, P NEUR INF PROC SYST
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Fan JP, 2004, IEEE T MULTIMEDIA, V6, P70, DOI 10.1109/TMM.2003.819583
   Guo GD, 2003, IEEE T NEURAL NETWOR, V14, P209, DOI 10.1109/TNN.2002.806626
   He J., 2004, P 12 ANN ACM INT C M, P9, DOI DOI 10.1145/1027527.1027531
   HE X, 2004, ACM SIGIR C INF RETR
   HE X, 2004, P ACM MULT C NEW YOR
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   HUANG TS, 2002, P JCIS
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Li J, 2006, IEEE T IMAGE PROCESS, V15, P3597, DOI 10.1109/TIP.2006.881938
   Liu F, 2004, IEEE T KNOWL DATA EN, V16, P28, DOI 10.1109/TKDE.2004.1264820
   MADDAGE NC, 2004, P ACM MULT C NEW YOR
   Matusik Wojciech, 2003, P SIGGRAPH
   MULLER M, 2005, P ACM SIGGRAPH 2005
   RUI Y, 2000, P IEEE C COMP VIS PA
   Seung HS, 2000, SCIENCE, V290, P2268, DOI 10.1126/science.290.5500.2268
   SHRAGER J, 1987, SCIENCE, V236, P1092, DOI 10.1126/science.236.4805.1092
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   Tao DC, 2007, IEEE T KNOWL DATA EN, V19, P568, DOI 10.1109/TKDE.2007.1003
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Vogt C. C., 1999, Information Retrieval, V1, P151, DOI 10.1023/A:1009980820262
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   Wu F, 2005, LECT NOTES COMPUT SC, V3767, P993
   XU CS, 2005, P ACM SIGIR C RES DE
   Yamasaki T, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/59535
   ZHANG H, 2007, P ACM MULT
   ZHUANG Y, P 13 INT MULTIMEDIA
   Zhuang YT, 2007, J VLSI SIG PROC SYST, V46, P153, DOI 10.1007/s11265-006-0020-y
   Zhuang YT, 2007, PATTERN RECOGN, V40, P3178, DOI 10.1016/j.patcog.2007.03.011
   [No title captured]
NR 34
TC 170
Z9 193
U1 0
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 437
EP 446
DI 10.1109/TMM.2008.917359
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100012
DA 2024-07-18
ER

PT J
AU Tseng, VS
   Su, JH
   Huang, JH
   Chen, CJ
AF Tseng, Vincent S.
   Su, Ja-Hwung
   Huang, Jhih-Hong
   Chen, Chih-Jen
TI Integrated mining of visual features, speech features, and frequent
   patterns for semantic video annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE association rule; data mining; frequent semantic patterns; sequential
   patterns; video annotation
AB To support effective multimedia information retrieval, video annotation has become an important topic in video content analysis. Existing video annotation methods put the focus on either the analysis of low-level features or simple semantic concepts, and they cannot reduce the gap between low-level features and high-level concepts. In this paper, we propose an innovative method for semantic video annotation through integrated mining of visual features, speech features, and frequent semantic patterns existing in the video. The proposed method mainly consists of two main phases: 1) Construction of four kinds of predictive annotation models, namely speech-association, visual-association, visual-sequential, and statistical models from annotated videos. 2) Fusion of these models for annotating un-annotated videos automatically. The main advantage of the proposed method lies in that all visual features, speech features, and semantic patterns are considered simultaneously. Moreover, the utilization of high-level rules can effectively complement the insufficiency of statistics-based methods in dealing with complex and broad keyword identification in video annotation. Through empirical evaluation on NIST TRECVID video datasets, the proposed approach is shown to enhance the performance of annotation substantially in terms of precision, recall, and F-measure.
C1 [Tseng, Vincent S.; Su, Ja-Hwung; Huang, Jhih-Hong; Chen, Chih-Jen] Natl Cheng Kung Univ, Dept Comp Sci & Informat, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Tseng, VS (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat, Tainan 701, Taiwan.
EM tsengsm@mail.ncku.edu.tw
CR Adams WH, 2003, EURASIP J APPL SIG P, V2003, P170, DOI 10.1155/S1110865703211173
   Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072
   AMIR A, 2003, P TRECVID WORKSH
   BALKE WT, 2004, P 6 INT C EXT DAT TE
   BALKE WT, 2004, P 30 INT C VER LARG
   Bartolini I., 2007, Multimedia Tools and Applications, V33, P275, DOI 10.1007/s11042-007-0103-1
   CHANG SF, 1998, IEEE T CIRCUITS SYST, V7
   Dorado A, 2004, IEEE T CIRC SYST VID, V14, P622, DOI 10.1109/TCSVT.2004.826764
   Fagin R, 2003, J COMPUT SYST SCI, V66, P614, DOI 10.1016/S0022-0000(03)00026-6
   Fagin R., 2001, P 20 ACM SIGACT SIGM
   FAGIN R, 2003, P ACM SIGMOD INT C M
   FAGIN R, 1996, P 15 ACM SIGACT SIGM
   Feng SL, 2004, PROC CVPR IEEE, P1002
   FLICKNER M, 1995, IEEE COMPUT MAG  SEP
   LAVRENKO V, 2004, P INT C AC SPEECH SI
   Li ZN, 1999, J VIS COMMUN IMAGE R, V10, P219, DOI 10.1006/jvci.1998.0403
   LUO Y, 2003, P IEEE INT C IM PROC
   Smith J. R., 1996, P ACM MULT C NOV
   Srinivasan MV, 1997, PATTERN RECOGN, V30, P593, DOI 10.1016/S0031-3203(96)00106-9
   TSENG VS, 2005, P IEEE INT WORKSH MA
   TSENG VS, 2005, P 7 INT WORKSH MULT
   TSENG VS, 2005, P 6 IEEE INT WORKSH
   Zhang RF, 2006, MULTIMEDIA SYST, V12, P27, DOI 10.1007/s00530-006-0025-1
   ZHANG Y, 2003, P IEEE INT C MULT EX
   ZHU X, 2005, IEEE T KNOWL DATA EN, V17
   ZHU X, 2003, P IEEE INT C MULT EX, V3, P333
   ZHU X, 2003, P 18 INT JOINT C ART, P1422
NR 27
TC 33
Z9 34
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 260
EP 267
DI 10.1109/TMM.2007.911832
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700010
DA 2024-07-18
ER

PT J
AU Hsu, CH
   Hefeeda, M
AF Hsu, Cheng-Hsin
   Hefeeda, Mohamed
TI Optimal coding of multilayer and multiversion video streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia communication; scalable coding; video quality optimization;
   video streaming
ID MULTICAST
AB Traditional video servers partially cope with heterogeneous client populations by maintaining a few versions of the same stream with different bit rates. More recent video servers leverage multilayer scalable coding techniques to customize the quality for individual clients. In both cases, heuristic, error-prone, techniques are currently used by administrators to determine either the rate of each stream version, or the granularity and rate of each layer in a multilayer scalable stream. In this paper, we propose an algorithm to determine the optimal rate and encoding granularity of each layer in a scalable video stream that maximizes a system-defined utility function for a given client distribution. The proposed algorithm can be used to compute the optimal rates of multiversion streams as well. Our algorithm is general in the sense that it can employ arbitrary utility functions for clients. We implement our algorithm and verify its optimality, and we show how various structuring of scalable video streams affect the client utilities. To demonstrate the generality of our algorithm, we consider three utility functions in our experiments. These utility functions model various aspects of streaming systems, including the effective rate received by clients, the mismatch between client bandwidth and received stream rate, and the client-perceived quality in terms of PSNR. We compare our algorithm against a heuristic algorithm that has been used before in the literature, and we show that our algorithm outperforms it in all cases.
C1 [Hsu, Cheng-Hsin; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
C3 Simon Fraser University
RP Hsu, CH (corresponding author), Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
EM mhefeeda@cs.sfu.ca
CR Cheung SY, 1996, IEEE INFOCOM SER, P553, DOI 10.1109/INFCOM.1996.493348
   Dai M, 2006, IEEE T MULTIMEDIA, V8, P1135, DOI 10.1109/TMM.2006.884626
   DECUETOS P, 2001, P INT PACK VID WORKS
   HSU C, 2007, P ACM INT WORKSH NET, P63
   JIANG T, 2000, P IEEE INFOCOM TEL A, P42
   *JOINT VID TEAM, 2007, JOINT SCAL VID MOD R
   Kamaci N, 2005, IEEE T CIRC SYST VID, V15, P994, DOI 10.1109/TCSVT.2005.852400
   Kim T, 2005, IEEE T MULTIMEDIA, V7, P1123, DOI 10.1109/TMM.2005.858376
   Kim T., 2001, P 11 INT WORKSHOP NE, P63
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Li X, 1999, IEEE NETWORK, V13, P46, DOI 10.1109/65.768488
   Li X., 1999, P S MULT COMP NETW 1, V3654, P175
   Liu JC, 2006, IEEE T MULTIMEDIA, V8, P162, DOI 10.1109/TMM.2005.861279
   Liu JC, 2003, IEEE INFOCOM SER, P630
   Liu JC, 2003, IEEE MULTIMEDIA, V10, P22, DOI 10.1109/MMUL.2003.1167919
   MCCANNE S, 1996, P ACM SIGCOMM 96 STA, P117
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Radulovic I, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1915, DOI 10.1109/ICME.2004.1394634
   SCHWARZ H, 2006, P EUR S MOB MED DEL
   SHACHAM N, 1992, IEEE INFOCOM SER, P2107, DOI 10.1109/INFCOM.1992.263483
   VANDERSCHAAR M, 2002, IEEE T CIRCUITS SYST, V12, P32
   Vicisano L, 1998, IEEE INFOCOM SER, P996, DOI 10.1109/INFCOM.1998.662909
   Yang YR, 2000, 2000 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P129, DOI 10.1109/ICNP.2000.896298
NR 23
TC 9
Z9 10
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 121
EP 131
DI 10.1109/TMM.2007.911224
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Purandare, D
   Guha, R
AF Purandare, Darshan
   Guha, Ratan
TI An alliance based peering scheme for P2P live media streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE media streaming; peer-to-peer; quality of service; small world network;
   video on demand
AB While recent measurement studies ha, le shown the effectiveness of P2P network in media streaming, there have been questions raised about the Quality of Service (QoS), reliability of streaming services and sub optimal uplink utilization in particular. P2P streaming systems are inherently less reliable because of churn, internet dynamics, node heterogeneity and randomness in the swarm. We present a new model for P2P media streaming based on clustering of peers, called alliances. We show that alliance formation is a loosely coupled and an effective way to organize the peers. We show that our model maps to a "small-world" network, which form efficient overlay structures and are robust to network perturbations such as churn. We present a comparative simulation based study of our model with CoolStreaming/DONet and present a quantitative performance evaluation. Simulation results are promising and show that our model scales well under varying workloads and conditions, delivers near optimal levels of QoS, and for most cases, performs at par or even better than CoolStreaming/DONet.
C1 Univ Cent Florida, Sch Elect Engn & Comp Sci, Orlando, FL 32816 USA.
C3 State University System of Florida; University of Central Florida
RP Purandare, D (corresponding author), Univ Cent Florida, Sch Elect Engn & Comp Sci, Orlando, FL 32816 USA.
EM shan@cs.ucf.edu; guha@cs.ucf.edu
CR Ali S, 2006, P ICST WORKSH REC AD
   Annapureddy S., 2006, P INT PROT TEL IPTV
   [Anonymous], 2000, P 32 ANN ACM S THEOR, DOI DOI 10.1145/335305.335325
   [Anonymous], 2001, RANDOM GRAPHS
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   BHARAMBE AR, 2006, P IEEE INF 2006 BARC
   Choi S, 2001, ELECTRON LETT, V37, P1559, DOI 10.1049/el:20011028
   Chu YH, 2002, IEEE J SEL AREA COMM, V20, P1456, DOI 10.1109/JSAC.2002.803066
   COHEN B, 2003, P P2P EC WORKSH BERK
   DANA C, 2005, P IEEE INT WORKSH MU
   FORD B, 2005, P 2005 USENIX TECH C
   FORD B, ACM SIGCOMM 2007
   Gkantsidis C, 2005, IEEE INFOCOM SER, P2235
   Guha S., 2005, 5 ACM SIGCOMMCONF IN, P199
   Hei X, 2006, P IPTV WORKSH INT WO
   LI J, 2004, MSRTR2004101
   MAGHAREI N, 2005, P ACM SIGCOMM 2005
   Medina A, 2001, NINTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P346, DOI 10.1109/MASCOT.2001.948886
   PIATEK M, 2007, P NSDI 07 APR
   Small T., 2006, Proc. ACM Multimedia, Santa Barbara, P539
   TRAN D, 2003, P IEEE INFOCOM 2003
   VLAVIANOS IM, 2006, P IEEE INF 2006 GLOB
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 24
TC 12
Z9 13
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1633
EP 1644
DI 10.1109/TMM.2007.907453
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900009
DA 2024-07-18
ER

PT J
AU Xie, S
   Li, B
   Keting, GY
   Zhang, X
AF Xie, Susu
   Li, Bo
   Keting, Gabriel Y.
   Zhang, Xinyan
TI Coolstreaming: Design, theory, and practice
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IPTV; measurement; peer-to-pcer technology; video streaming
AB Peer-to-peer (P2P) technology has found much success in applications like file distributions and VoIP yet, its adoption in live video streaming remains as an elusive goal. Our recent success in Coolstreaming system brings promises in this direction; however, it also reveals that there exist many practical engineering problems in real live streaming systems over the Internet. Our focus in this paper is on a nonoptimal real working system, in which we illustrate a set of existing practical problems and how they could be handled. We believe this is essential in providing the basic understanding of P2P streaming systems.
   This paper uses a set of real traces and attempts to develop some theoretical basis to demonstrate that a random peer partnership selection with a hybrid pull-push scheme has the potentially to scale. Specifically, first, we describe the fundamental system design tradeoffs and key changes in the design of a Coolstreaming system including substreaming, buffer management, scheduling and the adopt of a hybrid pull-push mechanism over the original pull-based content delivery approach; second, we examine the overlay topology and its convergence; third, using a combination of real traces and analysis, we quantitatively provide the insights on how the buffering technique resolves the problems associated with dynamics and heterogeneity; fourth, we show how substream and path diversity can help to alleviate the impact from congestion and churns; fifth, we discuss the system scalability and limitations.
C1 Hong Kong Univ Sci & Technol, Dept Comp Sci, Hong Kong, Peoples R China.
   Roxbeam Corp, Beijing, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Xie, S (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Clearwater Bay, Hong Kong, Peoples R China.
EM xiesusu@cse.ust.hk; bli@cse.list.hk; phgab@cse.ust.hk; zhang@roxbeam.com
RI Li, Bo/AAA-8968-2020
OI Li, Bo/0000-0002-7294-6888
CR BANERJEE S, 2003, P ACM SIGMETRICS JUN
   BANERJEE S, 2002, P ACM SIGCOMM AUG
   BISHOP M, 2006, P IEEE INFOCOM SEP
   CASTRO M, 2003, P SOSP OCT
   Castro M., 2002, IEEE J SEL AREAS COM, V20
   CHU Y, 2000, P ACM SIGM JUN
   CUI Y, 2003, P IEEE ICNP NOV
   EUGSTER P, 2004, IEEE COMPUTER    MAY
   Francis P., 2000, Yoid: Extending the internet multicast architecture
   Ganesh AJ, 2003, IEEE T COMPUT, V52, P139, DOI 10.1109/TC.2003.1176982
   Haccou Patsy, 2005, Branching Processes: Variation, Growth, and Extinction of Populations
   JANNOTTI DK, 2000, P OSDI OCT
   KUMAR R, 2007, P IEEE INF MAY
   MAGHAREI N, 2007, P IEEE INF MAY
   PADMANABHAN V, 2002, P NOSSDAV MAY
   QIU D, 2004, P ACM SIGCOMM AUG 30, V3
   SMALL T, 2006, P ACM MULT 06 OCT
   SUN Y, 2006, P ACM SIGCOMM   SEP
   VENKARAMAN V, 2006, P IEEE ICNP NOV
   VISHNUMURTHY V, 2006, P IEEE INF APR
   WANG M, 2007, P IEEE INF MAY
   WANG W, 2005, IEEE J SEL AREAS COM, V23
   ZHANG X, 2005, P IEEE INF MAR
   ZHANG X, 2005, P IEEE MMSP 2005 OCT
NR 24
TC 127
Z9 150
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1661
EP 1671
DI 10.1109/TMM.2007.907469
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900011
DA 2024-07-18
ER

PT J
AU Chen, OTC
   Chen, CC
AF Chen, Oseal T. -C.
   Chen, Chih-Chang
TI Automatic ally-determined region of interest in JPEG 2000
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive-binary-arithmetic-code; coding-decoding;
   digital-image-processing; discrete-wavelet-transform; image-coding;
   optimization; region of interest
ID IMAGE COMPRESSION; JPEG-2000; CLASSIFICATION; QUANTIZATION; EXTRACTION;
   SHIFT
AB This work presents an automatically-determined region of interest (ROI) scheme embedded in JPEG 2000. The proposed scheme analyzes the image content and then determines the probable ROI masks by examining the significant states of high-frequency subbands generated from embedded block coding with optimized truncation (EBCOT). Additionally, probable ROI masks are constructed in all bit planes of subbands by categorizing subblocks as either interesting or uninteresting, smoothing subblocks of interest, and grouping these subblocks based on an or no initial point. The rate-distortion (RD) pairs corresponding to all probable ROI masks are then estimated from the RD distribution during the Tier-2 coding process of EBCOT. Based on these estimations, the Lagrangian multiplier method is employed in the RD function to obtain the optimized ROI mask from the probable masks by minimizing the distortion of the ROI-encoded image at a given bit-rate constraint. ROI-encoded images obtained using the proposed scheme outperform ROI-encoded images obtained via the conventional schemes using fixed-square and object-segmentation masks, as judged by subjective visual perception and objective measurement in terms of peak signal-to-noise ratio. Particularly, the proposed scheme can easily adapt the ROI region with varied sizes and shapes according to the bit-rate constraint whereas the conventional schemes only adopt the fixed-square region and fixed segmented objects. Furthermore, when the proposed scheme is applied to motion JPEG 2000 for video compression, the centroid of the ROI mask in the previous frame can be used as an initial point for merging the subblocks of interest in the current frame to track the ROI masks in a video sequence. Therefore, the proposed scheme can easily be employed to improve the perceptual and objective performance in the ROI coding associated with JPEG 2000 and motion JPEG 2000.
C1 Natl Chung Cheng Univ, Dept Elect Engn, Media Lab, Chiayi 621, Taiwan.
C3 National Chung Cheng University
RP Chen, OTC (corresponding author), Natl Chung Cheng Univ, Dept Elect Engn, Media Lab, Chiayi 621, Taiwan.
EM oscat@ee.ccu.edu.tw; ccchen@sainlab.ee.ccu.edu.tw
CR Askelöf J, 2002, SIGNAL PROCESS-IMAGE, V17, P105, DOI 10.1016/S0923-5965(01)00026-1
   CHEN CC, 2003, P IEEE ISCAS, V2, P868
   CHEN CC, 2004, P IEEE ISCAS, V3, P869
   CHEN YB, 2003, P IEEE ICASSP, V3
   Ebrahimi-Moghadam A, 2005, IEEE T MULTIMEDIA, V7, P680, DOI 10.1109/TMM.2005.850967
   Fang HC, 2006, IEEE T MULTIMEDIA, V8, P645, DOI 10.1109/TMM.2006.876305
   FUKUHARA T, 2000, P IEEE INT C IM PROC, V2, P57
   Gatica-Perez D, 2001, IEEE T CIRC SYST VID, V11, P603, DOI 10.1109/76.920190
   Hariharakrishnan K, 2005, IEEE T MULTIMEDIA, V7, P853, DOI 10.1109/TMM.2005.854437
   *ISO, 2002, 154443 ISO IEC
   *ISO, 2000, ISO IEC JTC1 SC29 WG
   Joshi RL, 1997, IEEE T IMAGE PROCESS, V6, P1473, DOI 10.1109/83.641409
   KONG HS, 2005, P IEEE ISCAS, V2, P953
   KUO CJ, 2001, P IEEE INT S CIRC SY, V2, P753
   Lin CW, 2003, IEEE T CIRC SYST VID, V13, P982, DOI 10.1109/TCSVT.2003.816505
   LIN H, 2002, P IEEE INT C AC SPEE, V4, P3628
   LIU D, 2006, P IEEE ICPR, V2, P468
   Liu LJ, 2003, IEEE SIGNAL PROC LET, V10, P35, DOI 10.1109/LSP.2002.807867
   PAPAMARKOS N, 1996, P INT C EL CIRC SYST, V2, P684
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   ROMOS MG, 1997, P IEEE DCC, P66
   Schuster G.M., 1996, RATE DISTORTION BASE
   SUBEDAR MM, 2004, P IEEE ICIP, V2, P1293
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Taubman D, 2002, SIGNAL PROCESS-IMAGE, V17, P49, DOI 10.1016/S0923-5965(01)00028-5
   TSAI CW, 2004, P IEEE ICME, V2, P1423
   WAHL FM, 1982, COMPUT VISION GRAPH, V20, P375, DOI 10.1016/0146-664X(82)90059-4
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P160, DOI 10.1109/LSP.2002.1009009
   Yoo Y, 1999, IEEE T IMAGE PROCESS, V8, P1702, DOI 10.1109/83.806617
NR 29
TC 12
Z9 13
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1333
EP 1345
DI 10.1109/TMM.2007.906572
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400001
DA 2024-07-18
ER

PT J
AU Kherfi, ML
   Ziou, D
AF Kherfi, Mohammed Lamine
   Ziou, Djemel
TI Image collection organization and its application to indexing, browsing,
   summarization, and semantic retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content understanding and knowledge molding; digital libraries;
   indexing; searching; retrieving; query; and archiving databases
ID RELEVANCE FEEDBACK
AB In this paper, we present a new framework for organizing image collections into structures that can be used for indexing, browsing, retrieval and summarization. Instead of using tree-based techniques which are not suitable for images, we develop a new solution that is specifically designed for image collections. We consider both low-level image content and high-level semantics in an attempt to alleviate the semantic gap encountered by many systems. The fact that our model is based on a probabilistic framework makes it possible to combine it in a natural way with probabilistic techniques developed recently or image retrieval. The structure our model generates is applied for four purposes. The first is to provide retrieval module with an index, which allows it to improve retrieval time and accuracy, while the second is to provide users with a hierarchical browsing catalog that allows them to navigate the image collection by subject. This represents an additional step towards facilitating human-computer interaction in the context of image retrieval and navigation. The third aim is to provide users with a summarization of the general content of each class in the collection, and the fourth is a retrieval mechanism. Related issues such as relevance feedback and feature selection are also addressed. The experiments at the end of the paper show that the proposed framework yields some significant improvements.
C1 Univ Quebec Trois Rivieres, Dept Math & Comp Sci, Trois Rivieres, PQ G9A 5H7, Canada.
   Univ Sherbrooke, Dept Comp Sci, Sherbrooke, PQ J1K 2R1, Canada.
C3 University of Quebec; University of Quebec Trois Rivieres; University of
   Sherbrooke
RP Kherfi, ML (corresponding author), Univ Quebec Trois Rivieres, Dept Math & Comp Sci, CP 500, Trois Rivieres, PQ G9A 5H7, Canada.
EM kherfi@uqtr.ca; djemel.ziou@usherbrooke.ca
RI Kherfi, Mohammed Lamine/AAF-2930-2020
OI Kherfi, Mohammed Lamine/0000-0003-1017-1113
CR ASHBY FG, 1988, PSYCHOL REV, V95, P124, DOI 10.1037/0033-295X.95.1.124
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   BARNARD K, 2001, P IEEE ICCV VANC BC
   BECKMANN N, 1990, P ACM SIGMOD ATL CIT
   BENITEZ AB, 2002, P IEEE ICME LAUS SWI
   CHAOUCH Z, 2004, THESIS U SHERBROOKE
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gevers T, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P264, DOI 10.1109/MMCS.1999.779212
   GROSKY WI, 2001, P INT C INT MULT CON
   GUTTMAN A, 1984, P SCM SIGMOD BOST MA
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   HU C, 2000, ACM MULTIMEDIA LOS A
   Ishikawa Y., 1998, P 24 INT C VER LARG, P433
   KATAMAYA N, 1997, P ACM SIGMOD TUSC AZ
   Kherfi ML, 2006, IEEE T IMAGE PROCESS, V15, P1017, DOI 10.1109/TIP.2005.863969
   Kherfi ML, 2004, ACM COMPUT SURV, V36, P35, DOI 10.1145/1013208.1013210
   Kherfi ML, 2003, J VIS COMMUN IMAGE R, V14, P428, DOI 10.1016/S1047-3203(03)00043-9
   KHERFI ML, 2004, P IEEE ICPR CAMBR UK
   Lew MS, 2000, COMPUTER, V33, P46, DOI 10.1109/2.881694
   ROBINSON JT, 1981, P ACM SIGMOD ANN ARB
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   RUI Y, 2000, P IEEE INT C COMP VI
   RUI Y, 1997, P IEEE ICIP SANT BAR
   SCLAROFF S, 1997, P IEEE WORKSH CONT B
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SU Z, 2001, P SPIE EL IM SAN JOS
   Tieu K, 2004, INT J COMPUT VISION, V56, P17, DOI 10.1023/B:VISI.0000004830.93820.78
   WHITE DA, 1996, P INT C DAT ENG NEW
   ZHANG HJ, 1995, P SPIE C STOR RETR I, P36
   ZHOU X, 2001, P IEEE CVPR
   Zhou XS, 2002, IEEE MULTIMEDIA, V9, P23, DOI 10.1109/93.998050
NR 32
TC 15
Z9 17
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 893
EP 900
DI 10.1109/TMM.2007.893349
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200019
DA 2024-07-18
ER

PT J
AU Wang, F
   Ngo, CW
   Pong, TC
AF Wang, Feng
   Ngo, Chong-Wah
   Pong, Ting-Chuen
TI Lecture video enhancement and editing by integrating posture, gesture,
   and text
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE gesture; lecture video editing; posture and video text recognition
AB This paper describes a novel framework for automatic lecture video editing by gesture, posture, and video text recognition. In content analysis, the trajectory of hand movement is tracked and the intentional gestures are automatically extracted for recognition. In addition, head pose is estimated through overcoming the difficulties due to the complex lighting conditions in classrooms. The aim of recognition is to characterize the flow of lecturing with a series of regional focuses depicted by human postures and gestures. The regions of interest (ROIs) in videos are semantically structured with text recognition and the aid of external documents. By tracing the flow of lecturing, a finite state machine (FSM) which incorporates the gestures, postures, ROIs, general editing rules and constraints, is proposed to edit videos with novel views. The FSM is designed to generate appropriate simulated camera motion and cutting effects that suit the pace of a presenter's gestures and postures. To remedy the undesirable visual effects due to poor lighting conditions, we also propose approaches to automatically enhance the visibility and readability of slides and whiteboard images in the edited videos.
C1 Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology; City University of Hong
   Kong
RP Wang, F (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM wfeng@ust.hk; cwngo@cs.cityu.edu.hk; tcpong@ust.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
CR ABOWD G, 2000, ACM MULTIMEDIA, P187
   Bordwell David., 1986, FILM ART INTRO, V2nd
   CHEN M, 2002, ACM MULT C
   CHEN M, 2003, ACM MULT C
   EROL B, 2003, ACM MULTIMEDIA
   GLEICHER M, 2002, INT S SMART GRAPH
   GLEICHER M, 2000, ACM MULT C
   HE L, 2004, P IEEE INT C AC SPEE
   HE L, 1999, ACM MULT C
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   JU SX, 1998, IEEE T CIRC SYSTEMS
   KOVAC J, 2003, P INT COMP TOOL
   LIU Q, 2001, INT C HUM FACT COMP
   LIU T, 2003, INT C IM VID RETR
   LIU T, 2002, INT C MULT EXP
   LUIS J, 1999, P INT C IM AN PROC
   MACHNICKI E, 2002, MULT COMP NETW
   MAHMOOD TFS, 2000, ACM MULT C
   MARTIN J, 2000, P INT C AUT FAC GEST
   MATSUO Y, 2002, P ACM MULT C
   MUKHOPADHYAY S, 1999, P ACM MULT C
   NGO CW, 2002, INT C MULT EXP
   ONISHI M, 2004, P INT C PATT REC
   PHUNG DQ, 2003, P ACM MULT C
   ROWE LA, BMRC LECT BROWSER
   RUI Y, 2003, P INT C HUM FACT COM
   WANG F, 2003, P ACM MULT C
   WANG F, 2004, P INT C PATT REC
   WANG F, 2005, P ACM MULT C
NR 29
TC 7
Z9 9
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 397
EP 409
DI 10.1109/TMM.2006.886292
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900017
OA Green Published
DA 2024-07-18
ER

PT J
AU Wu, CH
   Hsieh, CH
   Huang, CL
AF Wu, Chung-Hsien
   Hsieh, Chia-Hsin
   Huang, Chien-Lin
TI Speech sentence compression based on speech segment extraction and
   concatenation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE speech concatenation; speech sentence compression
AB This correspondence presents a speech sentence compression scheme. A compressed word sequence is first extracted. Speech segments, in the spoken document, corresponding to the extracted words are selected for concatenation. Evaluation of the proposed approach shows the compressed speech sentence retains important and meaningful information and naturalness.
C1 Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Wu, CH (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
EM chwu@csie.ncku.edu.tw; ngsnail@csie.ncku.edu.tw; chicco@csie.ncku.edu.tw
RI Wu, Chung-Hsien/E-7970-2013; Huang, Chien-Lin/J-2468-2015
OI Wu, Chung-Hsien/0000-0002-3947-2123; Huang,
   Chien-Lin/0000-0003-3157-4173
CR Bikel DM, 2000, PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA, P155
   Hori C, 2003, IEEE T MULTIMEDIA, V5, P368, DOI 10.1109/TMM.2003.813274
   Hsieh CH, 2004, 2004 INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, P333
   Knight K, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P703
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Manning C.D., 1999, FDN STAT NATURAL LAN
   MANU I, 1999, ADV AUTOMATIC SUMMAR
   Ohtake Kiyonori, 2003, P ISCA IEEE WORKSH S, P167
   WU CH, 2004, J VLSI SIGNAL PROC, V36, P87
NR 9
TC 6
Z9 6
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 434
EP 438
DI 10.1109/TMM.2006.887995
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900023
DA 2024-07-18
ER

PT J
AU Bashir, FI
   Khokhar, AA
   Schonfeld, D
AF Bashir, Faisal I.
   Khokhar, Ashfaq A.
   Schonfeld, Dan
TI Real-time motion trajectory-based indexing and retrieval of video
   sequences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE principal component analysis; spectral clustering; string Matching;
   trajectory retrieval
ID RECOGNITION; REPRESENTATION; PERCEPTION
AB This paper presents a novel motion trajectory-based compact indexing and efficient retrieval mechanism for video sequences. Assuming trajectory information is already available, we represent trajectories as temporal ordering of subtrajectories. This approach solves the problem of trajectory representation when only partial trajectory information is available due to occlusion. It is achieved by a hypothesis testing-based method applied to curvature data computed from trajectories. The subtrajectories are then represented by their principal component analysis (PCA) coefficients for optimally compact representation. Different techniques are integrated to index and retrieve subtrajectories, including PCA, spectral clustering, and string matching. We assume a query by example mechanism where an example trajectory is presented to the system and the search system returns a ranked list of most similar items in the dataset. Experiments based on datasets obtained from University of California at Irvine's KDD archives and Columbia University's DVMM group demonstrate the superiority of our proposed PCA-based approaches in terms of indexing and retrieval times and precision recall ratios, when compared to other techniques in the literature.
C1 Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
   Univ Illinois, Chicago, IL 60607 USA.
C3 University of Illinois System; University of Illinois Chicago;
   University of Illinois Chicago Hospital
RP Bashir, FI (corresponding author), Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
EM ashfaq@uic.edu; dans@uic.edu
CR ABOUGHAZALEH N, 2000, P INT C VIS COMM IM
   [Anonymous], 2000, P 26 INT C VER LARG
   [Anonymous], 2004, P 21 INT C MACHINE L
   BASHIR F, 2003, P INT C IM PROC ICIP
   BASHIR F, 2004, P 6 ACM SIGMM MULT I
   BUZAN D, 2004, P INT C PATT REC ICP
   CHEN L, 2004, P 6 ACM SIGMM MULT I
   CHEN W, 2000, P IS T SPIE SAN JOS
   Clote P., 2000, Computational molecular biology: an introduction
   DIVAKARAN A, VIDEO MINING
   Faloutsos C., 1994, SIGMOD Record, V23, P419, DOI 10.1145/191843.191925
   Fink E, 2003, IEEE SYS MAN CYBERN, P2332
   Hettich S. D., UCI KDD ARCH
   Hongeng S, 2004, COMPUT VIS IMAGE UND, V96, P129, DOI 10.1016/j.cviu.2004.02.005
   Intille SS, 2001, COMPUT VIS IMAGE UND, V81, P414, DOI 10.1006/cviu.2000.0896
   Jeannin S, 2001, IEEE T CIRC SYST VID, V11, P720, DOI 10.1109/76.927428
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   KAHVECI T, P 17 INT C DAT ENG 2, P273
   Keogh E, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P226, DOI 10.1109/ICDM.2005.79
   Lin J., 2003, 8THACM SIGMOD WORKSH, DOI [10.1145/882082. 882086, DOI 10.1145/882082.882086]
   Ng AY, 2002, ADV NEUR IN, V14, P849
   PORIKLI F, 2004, P INT C COMP VIS PAT
   Rao C, 2002, INT J COMPUT VISION, V50, P203, DOI 10.1023/A:1020350100748
   REA N, 2004, P C IM VID RETR CIVR
   SAHOURIA E, 1999, P IEEE INT C IM PROC
   Shim CB, 2003, LECT NOTES COMPUT SC, V2728, P163
   SUN X, LNCS, V2195, P450
   Yacoob Y, 1999, COMPUT VIS IMAGE UND, V73, P232, DOI 10.1006/cviu.1998.0726
   Zacks JM, 2001, PSYCHOL BULL, V127, P3, DOI 10.1037//0033-2909.127.1.3
NR 30
TC 77
Z9 91
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 58
EP 65
DI 10.1109/TMM.2006.886346
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500007
DA 2024-07-18
ER

PT J
AU Myung, J
   Lee, W
   Shih, TK
AF Myung, Jihoon
   Lee, Wonjun
   Shih, Timothy K.
TI An adaptive mentoryless protocol for RFID tag collision arbitration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE collision resolution; RFID; tag anticollision; tag identification
AB A radio frequency identification (RFID) reader recognizes objects through wireless communications with RFID tags. Tag collision arbitration for passive tags is a significant issue for fast tag identification due to communication over a shared wireless channel. This paper presents an adaptive memoryless protocol, which is an improvement on the query tree protocol. Memoryless means that tags need not have additional memory except ID for identification. To reduce collisions and identify tags promptly, we use information obtained from the last process of tag identification at a reader. Our performance evaluation shows that the adaptive memoryless protocol causes fewer collisions and takes shorter delay for recognizing all tags while preserving lower communication overhead than other tree based tag anticollision protocols.
C1 Korea Univ, Dept Comp Sci & Engn, Seoul 136701, South Korea.
   Tamkang Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
C3 Korea University; Tamkang University
RP Myung, J (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul 136701, South Korea.
EM jmyung@korea.ac.kr; wlee@korea.ac.kr; tshih@cs.tku.eku.tw
RI Lee, Wonjun/H-9786-2012; along, super/E-8764-2014
OI Lee, Wonjun/0000-0001-5286-6541; 
CR [Anonymous], 2003, DRAFT PROT SPEC 900
   *EPC RAD FREQ ID P, 2004, CLASS 1 GEN 2 UHF
   Finkenzeller K., 2003, RFID Handbook: Fundamentals and Applications in Contactless Smart Cards and Identification
   GUERIN RA, 1987, IEEE T VEH TECHNOL, V36, P89, DOI 10.1109/T-VT.1987.24106
   Hush DR, 1998, 1998 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY - PROCEEDINGS, P107, DOI 10.1109/ISIT.1998.708695
   Information technology automatic identification and data capture techniques-radio frequency identification for item management air interface, 2003, 180006 ISO
   Jacomet Marcel., 1999, P IEEE C CIRCUITS SY, P269
   Law C., 2000, Proceedings of the 4th International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications, P75, DOI DOI 10.1109/TMM.2006.879817
   Sarma S, 2001, IEEE MICRO, V21, P50, DOI 10.1109/40.977758
   SCHARFELD TA, 2001, THESIS MIT, P92
   SCHOUTE FC, 1983, IEEE T COMMUN, V31, P565, DOI 10.1109/TCOM.1983.1095854
   Vogt H., 2002, Pervasive Computing. First International Conference, Pervasive 2002. Proceedings (Lecture Notes in Computer Science Vol.2414), P98
   WIESELTHIER JE, 1989, IEEE T COMMUN, V37, P125, DOI 10.1109/26.20080
   Zhai J, 2005, LECT NOTES COMPUT SC, V3483, P702
   Zhou F, 2004, ISLPED '04: PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P357
NR 15
TC 68
Z9 90
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 1096
EP 1101
DI 10.1109/TMM.2006.879817
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400020
DA 2024-07-18
ER

PT J
AU Chen, CY
   Huang, YW
   Lee, CL
   Chen, LG
AF Chen, Ching-Yeh
   Huang, Yu-Wen
   Lee, Chia-Lin
   Chen, Liang-Gee
TI One-pass computation-aware motion estimation with adaptive search
   strategy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive search strategy; block matching; computation-aware; motion
   estimation; one-pass
ID ALGORITHM
AB A computation-aware motion estimation algorithm is proposed in this paper. Its goal is to find the best block-matching results in a computation-limited and computation-variant environment. Our algorithm is characterized by a one-pass flow with adaptive search strategy. In the prior scheme, Tsai et al. propose that all macroblocks are processed simultaneously, and more computation is allocated to the macroblock with the largest distortion among the entire frame in a step-by-step fashion. This implies that random access of macroblocks is required, and the related information of neighboring macroblocks cannot be used to be prediction. The random access flow requires a huge memory size for all macroblocks to store the up-to-date minimum distortions, best motion vectors, and searching steps. On the contrary, our one-pass flow processes the macroblocks one by one, which can not only significantly reduce the memory size but also effectively utilize the context information of neighboring macroblocks to achieve faster speed and better quality. Moreover, in order to improve the video quality when the computation resource is still sufficient, the search pattern is allowed to adaptively change from diamond search to three step search, and then to full search. Last but not least, traditional block matching speed-up methods are also combined to provide much better computation-distortion curves.
C1 Natl Taiwan Univ, Grad Inst Elect Engn, DSP IC Design Lab, Taipei 106, Taiwan.
   Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Chen, CY (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, DSP IC Design Lab, Taipei 106, Taiwan.
EM cychen@video.ee.ntu.edu.tw; yuwen@video.ee.ntu.edu.tw;
   poohli@video.ee.ntu.edu.tw; lgchen@video.ee.ntu.edu.tw
RI Lee, Chia-Lin/P-2038-2015
OI Lee, Chia-Lin/0000-0002-5949-1989; Huang, Yu-Wen/0000-0003-3045-1104;
   CHEN, LIANG-GEE/0000-0001-9746-9355
CR [Anonymous], 1449610 ISO IEC AVC
   CHEN MJ, 1994, IEEE T CIRC SYST VID, V4, P504, DOI 10.1109/76.322998
   Cheung CK, 2000, IEEE T CIRC SYST VID, V10, P417, DOI 10.1109/76.836286
   *ISO IEC, 1996, 138182 ISO IEC
   *ISO IEC, 1999, 144962 ISO IEC
   *ISO IEC, 1993, 111722 ISO IEC
   *ISO IEC, 2000, JTC1SC29WG1 ISO IEC
   Tai PL, 2003, IEEE T CIRC SYST VID, V13, P901, DOI 10.1109/TCSVT.2003.816510
   THAN JY, 1998, IEEE T CIRCUITS SYST, V8, P369
   Tourapis AM, 2002, IEEE T CIRC SYST VID, V12, P934, DOI 10.1109/TCSVT.2002.804894
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhu S, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P292, DOI 10.1109/ICICS.1997.647106
NR 12
TC 21
Z9 22
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 698
EP 706
DI 10.1109/TMM.2006.876296
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300005
OA Green Published
DA 2024-07-18
ER

PT J
AU Huang, CL
   Shih, HC
   Chao, CY
AF Huang, Chung-Lin
   Shih, Huang-Chia
   Chao, Chung-Yuan
TI Semantic analysis of soccer video using dynamic Bayesian network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dynamic Bayesian network (DBN); temporal intervening network (TIN);
   video semantic analysis
AB Video semantic analysis is formulated based on the low-level image features and the high-level knowledge which is encoded in abstract, nongeometric representations. This paper introduces a semantic analysis system based on Bayesian network (BN) and dynamic Bayesian network (DBN). It is validated in the particular domain of soccer game videos. Based on BN/DBN, it can identify the special events in soccer games such as goal event, corner kick event,penalty kick event, and card event. The video analyzer extracts the low-level evidences, whereas the semantic analyzer uses BN/DBN to interpret the high-level semantics. Different from previous shot-based semantic analysis approaches, the proposed semantic analysis is frame-based for each input frame, it provides the current semantics of the event nodes as well as the hidden nodes. Another contribution is that the BN and DBN are automatically generated by the training process instead of determined by ad hoc. The last contribution is that we introduce a so-called temporal intervening network to improve the accuracy of the semantics output.
C1 Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30043, Taiwan.
   Mag Pixel Inc, Hsinchu, Taiwan.
C3 National Tsing Hua University
RP Huang, CL (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30043, Taiwan.
EM clhuang@ee.nthu.edu.tw; hc.shih@ieee.org
RI Shih, Huang-Chia/AAH-4966-2021
CR [Anonymous], 2001, DYNAMIC BAYESIAN NET
   ASSFALG J, 2003, COMPUT VIS IMAGE UND, V91
   CHANG P, 2002, P IEEE ICIP
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   FORBES J, 1995, P IJCAI
   Garg A, 2003, P IEEE, V91, P1355, DOI 10.1109/JPROC.2003.817119
   Gong YH, 2004, COMPUT VIS IMAGE UND, V96, P181, DOI 10.1016/j.cviu.2004.02.002
   HAN M, 2003, PCM 2003 C SING DEC
   Huang CL, 2001, IEEE T CIRC SYST VID, V11, P1281, DOI 10.1109/76.974682
   HWANG JN, 2002, IEEE ICASSP ORL FL M, V4, P4084
   Jensen FV, 1996, INTRO BAYESIAN NETWO
   Pan H, 2002, INT CONF ACOUST SPEE, P3385
   PETKOVIC M, 2002, P IEEE ICME LAUS SWI
   SADLIER DA, 2003, IEEE INT S SIGN PROC
   Shih HC, 2005, IEEE T BROADCAST, V51, P449, DOI 10.1109/TBC.2005.854169
   SHIH HC, 2004, P IEEE ICME TAIP TAI, V1, P595
   SNOEK CGM, 1908, P IEEE INT C MULT EX
   SUN X, 2003, MULTISPECTRAL IMAGE
   Takagi S, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P461
   WAN K, 2003, P ICASSP 2003, V3, P185
   Wu C, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P805, DOI 10.1109/ICME.2002.1035904
   XIE L, 2002, P INT C AC SPEECH SI, V4, P4096
   XU G, 2003, P ICIP, V1, P25
   Xu Peng., 2001, Proceedings of the 2001 IEEE International Conference on Multimedia and Expo (ICME), P721, DOI [10.1109/ICME.2001.1237822, DOI 10.1109/ICME.2001.1237822]
   ZHONG D, 2001, P IEEE INT C MULT EX, P713
   ZHOU W, 2000, ACM MULT C LOS ANG C
NR 26
TC 107
Z9 124
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 749
EP 760
DI 10.1109/TMM.2006.876289
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300010
OA Green Published
DA 2024-07-18
ER

PT J
AU Yeasin, M
   Bullot, B
   Sharma, R
AF Yeasin, Mohammed
   Bullot, Baptiste
   Sharma, Rajeev
TI Recognition of facial expressions and measurement of levels of interest
   from video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE emotions; face detection; hidden Markov models (HMMs); levels of
   interest; machine learning; universal facial expressions
ID EMOTION; UNIVERSALS
AB This paper presents a spatio-temporal approach in recognizing six universal facial expressions from visual data and using them to compute levels of interest. The classification approach relies on a two-step strategy on the top of projected facial motion vectors obtained from video sequences of facial expression. First a linear classification bank was applied on projected optical flow vectors and decisions made by the linear classifiers were coalesced to produce a characteristic signature for each universal facial expression. The signatures thus computed from the training data set were used to train discrete hidden Markov models (HMMs) to learn the underlying model for each facial expression. The performances of the proposed facial expressions recognition were computed using five fold cross-validation on Cohn-Kanade facial expressions database consisting of 488 video sequences that includes 97 subjects. The proposed approach achieved an average recognition rate of 90.9% on Cohn-Kanade facial expressions database. Recognized facial expressions were mapped to levels of interest using the affect space and the intensity of motion around apex frame. Computed level of interest was subjectively analyzed and was found to be consistent with "ground truth" information in most of the cases. To further illustrate the efficacy of the proposed approach, and also to better understand the effects of a number of factors that are detrimental to the facial expression recognition, a number of experiments were conducted. The first empirical analysis was conducted on a database consisting of 108 facial expressions collected from TV broadcasts and labeled by human coders for subsequent analysis. The second experiment (emotion elicitation) was conducted on facial expressions obtained from 21 subjects by showing the subjects six different movies clips chosen in a manner to arouse spontaneous emotional reactions that Would produce natural facial expressions.
C1 Univ Memphis, Dept Elect & Comp Engn, Memphis, TN 38152 USA.
   Amadeus Inc, Paris 75001, France.
   Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA.
C3 University of Memphis; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); Pennsylvania State University; Pennsylvania State
   University - University Park
RP Yeasin, M (corresponding author), Univ Memphis, Dept Elect & Comp Engn, Memphis, TN 38152 USA.
EM myeasin@memphis.edu
CR [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2000, NEURAL INFORM PROCES
   [Anonymous], SIGGRAPH 94
   [Anonymous], IEEE INT C AUT FAC G
   [Anonymous], 1976, Pictures of facial affect
   [Anonymous], 1976, PICTURES FACIAL AFFE
   AVENT R, 1994, ANN INT C IEEE
   Azoz Y, 2003, MACH VISION APPL, V13, P286, DOI 10.1007/s00138-002-0110-1
   BASSILI JN, 1979, J PERS SOC PSYCHOL, V37, P2049, DOI 10.1037/0022-3514.37.11.2049
   BLACK MJ, 1995, INT C COMP VIS
   BOYLE EA, 1994, LANG SPEECH, V37, P1, DOI 10.1177/002383099403700101
   BREAZAL C, 2000, THESIS MIT CAMBRIDGE
   Breazeal C.L., 1999, Robot in Society: Friend or Appliance?
   Cohen I., 2000, EMOTION RECOGNITION
   COTTRELL G, 2000, APA TALK
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Darrell T., 1995, Advances in Neural Information Processing Systems 7, P909
   DARRELL T, 1995, P INT WORKSH AUT FAC, P135
   EKMAN P, 1992, PHILOS T ROY SOC B, V335, P63, DOI 10.1098/rstb.1992.0008
   EKMAN P, 1964, J ABNORM SOC PSYCH, V68, P295, DOI 10.1037/h0040225
   EKMAN P, 1994, PSYCHOL BULL, V115, P268, DOI 10.1037/0033-2909.115.2.268
   Ekman P, 2003, ANN NY ACAD SCI, V1000, P205, DOI 10.1196/annals.1280.010
   Ekman P, 2003, ANN NY ACAD SCI, V1000, P1, DOI 10.1196/annals.1280.002
   EKMAN P, 1980, SCIENCE, V209, P833, DOI 10.1126/science.7403851
   EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550
   EKMAN P, 1987, J PERS SOC PSYCHOL, V53, P712, DOI 10.1037/0022-3514.53.4.712
   Ekman P., 1994, The nature of emotion: Fundamental questions
   ESSA I, 1995, INT C COMP VIS
   ESSA I, 1995, IEEE T PATTERN ANAL, P757
   Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232
   Franco L, 2001, ISPA 2001: PROCEEDINGS OF THE 2ND INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P628, DOI 10.1109/ISPA.2001.938703
   Haidt J, 1999, COGNITION EMOTION, V13, P225, DOI 10.1080/026999399379267
   Hoey J, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P99, DOI 10.1109/EVENT.2001.938872
   Izard C.E., 1977, HUMAN EMOTION
   KAPOOR A, 2001, AM ASS ART INT
   KUMAR V, 2000, IEEE INT C AUT FAC G
   LEVENSON RW, 1991, PSYCHOL AGING, V6, P28, DOI 10.1037/0882-7974.6.1.28
   LIEN J, 1998, IEEE C COMP VIS PATT
   LIN D, 1999, INT C NEUR INF PROC
   LISETTI C, 1998, INT FLAIRS C
   Lisetti C.L., 2000, Pragmatics cognition, V8, P185, DOI DOI 10.1075/PC.8.1.09LIS
   LUCAS B, 1981, P DARPA IMAGE UNDERS
   LYONS M, INT C AUT FAC GEST R, P98
   Matsumura K, 1997, ELECTRON COMM JPN 3, V80, P36, DOI 10.1002/(SICI)1520-6440(199701)80:1<36::AID-ECJC4>3.0.CO;2-5
   MOSES Y, 1995, INT C COMP VIS
   OLIVER N, 1997, LAFTER LIPS FACE REA
   OLIVER N, 1997, IEEE C COMP VIS PATT
   PADGETT C, 1997, REPRESENTING FACE IM
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Picard R. W., 1997, AFFECTIVE COMPUTING
   PICARD RW, 1998, P IMAGINA
   PICARD RW, 1993, P ICASSP MINNEAPOLIS, V5, P161
   ROSENBLUM M, 1994, IEEE WORKSHOP MOTION
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   STEPHENSON GM, 1976, BRIT J SOC CLIN PSYC, V15, P113, DOI 10.1111/j.2044-8260.1976.tb00016.x
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Yacoob Y, 1996, IEEE T PATTERN ANAL, V18, P636, DOI 10.1109/34.506414
   YEASIN M, 2000, IEEE COMP SOC C COMP
   Yoshitomi Y., 2000, IEEE INT WORKSH ROB
NR 59
TC 158
Z9 177
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 500
EP 508
DI 10.1109/TMM.2006.870737
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000007
DA 2024-07-18
ER

PT J
AU Lie, WN
   Chang, LC
AF Lie, WN
   Chang, LC
TI Robust and high-quality time-domain audio watermarking based on
   low-frequency amplitude modification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio watermarking; data embedding; psychoacoustic model
AB This work proposes a method of embedding digital watermarks into audio signals in the time domain. The proposed algorithm exploits differential average-of-absolute-amplitude relations within each group of audio samples to represent one-bit information. The principle of low-frequency amplitude modification is employed to scale amplitudes in a group manner (unlike the sample-by-sample manner as used in pseudonoise or spread-spectrum techniques) in selected sections of samples so that the time-domain waveform envelope can be almost preserved. Besides, when the frequency-domain characteristics of the watermark signal are controlled by applying absolute hearing thresholds in the psychoacoustic model, the distortion associated with watermarking is hardly perceivable by human ears. The watermark can be blindly extracted without knowledge of the original signal. Subjective and objective tests reveal that the proposed watermarking scheme maintains high audio quality and is simultaneously highly robust to pirate attacks, including MP3 compression, low-pass filtering, amplitude scaling, time scaling, digital-to-analog/analog-to-digital reacquisition, cropping, sampling rate change, and bit resolution transformation. Security of embedded watermarks is enhanced by adopting unequal section lengths determined by a secret key.
C1 Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621, Taiwan.
   Innovat Media Technol Ctr, Networks & Multimedia Inst, Inst Informat Ind, Taipei, Taiwan.
C3 National Chung Cheng University
RP Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621, Taiwan.
EM wnlie@ee.ccu.edu.tw; pasty@iii.org.tw
RI Lie, Wen-Nung/AFP-1266-2022
CR Ambikairajah E, 1997, ELECTRON COMMUN ENG, V9, P165, DOI 10.1049/ecej:19970403
   [Anonymous], 1997, P 5 EUR C SPEECH COM
   Arnold M, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1013, DOI 10.1109/ICME.2000.871531
   Bassia P, 2001, IEEE T MULTIMEDIA, V3, P232, DOI 10.1109/6046.923822
   Boney L, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P473, DOI 10.1109/MMCS.1996.535015
   Changsheng Xu, 1999, ISSPA '99. Proceedings of the Fifth International Symposium on Signal Processing and its Applications (IEEE Cat. No.99EX359), P95, DOI 10.1109/ISSPA.1999.818121
   Çiloglu T, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1017, DOI 10.1109/ICME.2000.871532
   Gruhl D., 1996, Information Hiding. First International Workshop Proceedings, P295
   HAGENAUER J, 1988, IEEE T COMMUN, V36, P389, DOI 10.1109/26.2763
   Hartung F, 2000, IEEE COMMUN MAG, V38, P78, DOI 10.1109/35.883493
   Herre J., 1999, Proceedings of the 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. WASPAA'99 (Cat. No.99TH8452), P27, DOI 10.1109/ASPAA.1999.810841
   IKEDA M, 1999, P IEEE INT C AC SPEE, V4, P2315
   Kim H, 2000, INT CONF ACOUST SPEE, P1971, DOI 10.1109/ICASSP.2000.859217
   Ko BS, 2002, INT CONF ACOUST SPEE, P2001
   Kuo S, 2002, INT CONF ACOUST SPEE, P1753
   Lacy J, 1998, INT CONF ACOUST SPEE, P3725, DOI 10.1109/ICASSP.1998.679693
   Lee HW, 2001, ISIE 2001: IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS PROCEEDINGS, VOLS I-III, P1357, DOI 10.1109/ISIE.2001.931679
   Li A, 2000, IEEE MULTIMEDIA, V7, P4
   Li X, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P397, DOI 10.1109/ICME.2000.869624
   Manikopoulos C, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P355
   Neubauer C, 1998, LECT NOTES COMPUT SC, V1525, P208
   Noll P, 1997, IEEE SIGNAL PROC MAG, V14, P59, DOI 10.1109/79.618009
   Qiao LT, 1999, P SOC PHOTO-OPT INS, V3657, P194, DOI 10.1117/12.344669
   Shin S, 2002, DSP 2002: 14TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING PROCEEDINGS, VOLS 1 AND 2, P701, DOI 10.1109/ICDSP.2002.1028187
   Shu Lin., 1983, ERROR CONTROL CODING
   Swanson MD, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P19, DOI 10.1109/MMCS.1999.779114
   Swanson MD, 1998, SIGNAL PROCESS, V66, P337, DOI 10.1016/S0165-1684(98)00014-0
   TILKI JF, 1997, P SE 97 BLACKSB VA A, P331
   Trappe W, 2003, IEEE T SIGNAL PROCES, V51, P1069, DOI 10.1109/TSP.2003.809378
   Wang Y, 1998, ICSP '98: 1998 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, PROCEEDINGS, VOLS I AND II, P1420, DOI 10.1109/ICOSP.1998.770887
   Wen JT, 2002, IEEE T CIRC SYST VID, V12, P545, DOI 10.1109/TCSVT.2002.800321
   Wu C.P., 1999, P INT S MULT INF PRO, P37
   Xu CS, 1999, J AUDIO ENG SOC, V47, P805
NR 33
TC 145
Z9 168
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 46
EP 59
DI 10.1109/TMM.2005.861292
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000005
DA 2024-07-18
ER

PT J
AU Soderquist, P
   Leeser, M
   Rojas, JC
AF Soderquist, P
   Leeser, M
   Rojas, JC
TI Enabling MPEG-2 video playback in embedded systems through improved data
   cache efficiency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cache memories; multimedia computing; video codecs; video coding; video
   signal processing
AB Digital video decoding, enabled by the MPEG-2 video standard, is an important future application for embedded systems, particularly personal digital assistants and other information appliances. Many such systems require portability and wireless communication capabilities, and thus face severe limitations in size and power consumption. This places a premium on integration and efficiency, and favors software solutions for video functionality over specialized hardware. Apart from computation, an equally important problem in video decoding is the data bandwidth and the need to insure adequate data supply. MPEG data sets are very large, and generate significant amounts of excess memory traffic for standard data caches, up to 100 times the amount required for decoding. Yet MPEG data has locality which caches can exploit if properly optimized, providing fast, flexible, and automatic data supply. We propose a set of enhancements which target the specific needs of the heterogeneous types within the MPEG decoder working set. These optimizations significantly improve the efficiency of small caches, reducing cache-memory traffic by almost 70%, and can make an enhanced 4-kB cache perform better than a standard 1 MB cache. This performance improvement can enable high-resolution, full frame rate video playback in cheaper, smaller systems than would otherwise be possible.
C1 Intel Corp, Hudson, MA 01749 USA.
   Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
   2Wire Inc, San Jose, CA 95131 USA.
C3 Intel Corporation; Northeastern University
RP Intel Corp, Hudson, MA 01749 USA.
EM peter.soderquist@intel.com; mel@coe.neu.edu; jrojas@alumni.neu.edu
OI Leeser, Miriam/0000-0002-5624-056X
CR Berg SG, 1998, P SOC PHOTO-OPT INS, V3655, P147, DOI 10.1117/12.334760
   Bhatt B, 1997, IEEE SPECTRUM, V34, P19, DOI 10.1109/6.625222
   Burger D, 1997, IEEE MICRO, V17, P55, DOI 10.1109/40.641597
   BURGER D, 1995, 1261 U WISC COMP SCI
   ECKART S, 1995, P DIG VID COMPR ALG, P100
   FENG W, 1996, P DIG VID COMPR ALG, P94
   GALL DL, 1991, COMMUN ACM, V34, P46
   *HIT LTD STMICROEL, 1999, SH 5 ARCH TECHN MARK
   *INT CORP, 2000, INT XSCALE MICR TECH
   *ISO IEC, 1996, JTCJSC29WCU13818 ISO
   JOHNSON EE, 1994, P IEEE INT PHOEN C C, P213
   Kuroda I, 1998, P IEEE, V86, P1203, DOI 10.1109/5.687835
   LARUS JR, 1993, IEEE COMPUT, V26, P52
   MULITONOVIC VM, 1996, P 5 INT WORKSH SCI B
   SANCHEZ J, 1997, IEEE TECH COMMIT MAR
   Soderquist P, 1997, PR IEEE COMP DESIGN, P417, DOI 10.1109/ICCD.1997.628903
   SODERQUIST P, 1998, THESIS CORNELL U ITH
   TOMASKO M, 1997, IEEE TECH COMM C MAR
   TURLEY J, 1997, MICROPROCESSOR REPOR, V11, P22
   ZUCKER DF, 1996, COMPCON 96, P327
NR 20
TC 1
Z9 1
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 81
EP 89
DI 10.1109/TMM.2005.861289
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000008
DA 2024-07-18
ER

PT J
AU Fang, T
   Chau, LP
AF Fang, T
   Chau, LP
TI Efficient content-based resynchronization approach for wireless video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based segmentation; resynchronization marker; robust video
   transmission
ID MARKER POSITIONING METHOD; TRANSMISSION
AB Recent advances in technology have caused a significant growth in wireless communications, which have resulted in a strong demand for reliable transmission of video data. The challenge of robust video transmission is to protect the compressed data against hostile channel conditions while bringing little impact on bandwidth efficiency. In this paper, using results from a simplified macroblock-based segmentation algorithm, we propose a framework called content-based resynchronization for the effective positioning of resynchronization markers such that the image quality of foreground can be improved at the expense of sacrificing unimportant background. We do this because, in applications such as video telephony and video conferencing, foreground is typically the most important image region for viewers. Experimental results demonstrate that this scheme significantly improve the perceptual quality of video sequences for robust video transmission.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Fang, T (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM tfang@pmail.ntu.edu.sg; elpchau@ntu.edu.sg
RI Chau, Lap-Pui/A-5149-2011
OI Chau, Lap-Pui/0000-0003-4932-0593
CR BARBOSA LO, 2000, IEEE T BROADCAST, V46, P134
   Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   Gao SS, 2003, IEEE T CIRC SYST VID, V13, P182, DOI 10.1109/TCSVT.2002.808434
   Jeong JH, 2002, SIGNAL PROCESS-IMAGE, V17, P799, DOI 10.1016/S0923-5965(02)00059-0
   Lee SH, 2001, ELECTRON LETT, V37, P348, DOI 10.1049/el:20010255
   Moccagatta I, 2000, IEEE J SEL AREA COMM, V18, P899, DOI 10.1109/49.848245
   VANDERSCHAAR M, 2001, P IEEE INT C IM PROC, V2, P977
   Villasenor JD, 1999, P IEEE, V87, P1724, DOI 10.1109/5.790633
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   YANG KH, 2001, P 2001 INT C IM PROC, V1, P938
   Yoo KY, 1998, ELECTRON LETT, V34, P2084, DOI 10.1049/el:19981458
NR 12
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1021
EP 1027
DI 10.1109/TMM.2005.858378
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200003
DA 2024-07-18
ER

PT J
AU Kang, SH
   Zakhor, A
AF Kang, SH
   Zakhor, A
TI Effective bandwidth based scheduling for streaming media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia; rate-distortion optimization; scalability; scheduling;
   streaming
AB We propose a class of rate-distortion optimized packet scheduling algorithms for streaming media by generating a number of nested substreams, with more important streams embedding less important ones in a progressive manner. Our goal is to determine the optimum substream to send at any moment in time, using feedback information from the receiver and statistical characteristics of the video. To do so, we model the streaming system as a queueing system, compute the run-time decoding failure probability of a group of picture in each substream based on effective bandwidth approach, and determine the optimum substream to be sent at that moment in time. We evaluate our scheduling scheme with various video traffic models featuring short-range dependency (SRD), long-range dependency (LRD), and/or multifractal properties. From experiments with real video data, we show that our proposed scheduling scheme outperforms the conventional sequential sending scheme.
C1 Univ Seoul, Dept Elect & Comp Engn, Seoul 130743, South Korea.
   Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
C3 University of Seoul; University of California System; University of
   California Berkeley
RP Univ Seoul, Dept Elect & Comp Engn, Seoul 130743, South Korea.
EM shkang@uos.ac.kr; avz@cccs.berkeley.edu
RI Zakhor, Avideh/GYA-1602-2022
OI Zakhor, Avideh/0000-0003-4770-6353
CR BERAN J, 1995, IEEE T COMMUN, V43, P1566, DOI 10.1109/26.380206
   Courcoubetis C., 1995, Probability in Engineering and Information Sciences, V9, P285
   Erramilli A, 2001, J COMMUN NETW-S KOR, V3, P383, DOI 10.1109/JCN.2001.6596970
   FONSECA NLS, 2000, ACM T MODEL COMPUT S, V10, P104, DOI DOI 10.1145/364996.365003
   Gao JB, 2001, INT J COMMUN SYST, V14, P783, DOI 10.1002/dac.509
   Kang SH, 2002, IEEE T COMMUN, V50, P633, DOI 10.1109/26.996078
   Krunz MM, 1998, IEEE J SEL AREA COMM, V16, P733, DOI 10.1109/49.700909
   Liu H, 1997, IEEE J SEL AREA COMM, V15, P1775, DOI 10.1109/49.650050
   MELO CAV, 2004, P IEEE INT C COMM IC, V4, P2168
   MIAO Z, 2002, INT PACK WORKSH 2002
   Nagai K., 2001, Sharp Technical Journal, P74
   Nagarajan K, 2000, CONF REC ASILOMAR C, P1245, DOI 10.1109/ACSSC.2000.910762
   NORROS I, 1995, IEEE J SEL AREA COMM, V13, P953, DOI 10.1109/49.400651
   NORROS I, 2003, P IEEE ICIP 2003, V3, P633
   Parulekar M, 1997, IEEE INFOCOM SER, P419, DOI 10.1109/INFCOM.1997.644490
   Zhang QQ, 1999, IEEE J SEL AREA COMM, V17, P867, DOI 10.1109/49.768201
NR 16
TC 11
Z9 15
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1139
EP 1148
DI 10.1109/TMM.2005.858401
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200015
DA 2024-07-18
ER

PT J
AU Xue, XH
   Wu, XL
AF Xue, XH
   Wu, XL
TI Directly operable image representation of multiscale primal sketch
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based retrieval; edge-based scene analysis and classification;
   image compression; image representation; multiresolution analysis
ID ZERO-CROSSINGS
AB In this paper, we propose a versatile semantics-driven image representation that can support many common operations in visual computing and communications, in addition to being itself an efficient image coding scheme. The proposed image representation is based on a semantically meaningful construct called multiscale primal sketch (NIPS). The NIPS consists of edges that are extracted and organized successively from fine to coarse scales. The edges are further classified into two types: pulse edge and step edge. NIPS is an intermediate-level image representation, which is between pixel-based (low-level) and object-based (high-level) descriptions. The NIPS image representation reaches a good compromise between its construction cost and descriptive power. It has a compact form, and hence is amenable to image compression. Furthermore, because the new representation consists of semantically meaningful primitives-edges of different scales and types, and background-many common image operations, such as classification, restoration, detection, and content-based retrieval, can be performed directly in the NIPS framework, without first converting the coded image back to the spatial domain.
C1 Wavelet Imaging Technol, London, ON N5Y 3J7, Canada.
   McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4M2, Canada.
C3 McMaster University
RP Xue, XH (corresponding author), Wavelet Imaging Technol, London, ON N5Y 3J7, Canada.
CR [Anonymous], 1986, COMPUTATIONAL APPROA, DOI DOI 10.1109/TPAMI.1986.4767851
   Antonini M, 1992, IEEE T IMAGE PROCESS, V1, P205, DOI 10.1109/83.136597
   CARLSSON S, 1988, SIGNAL PROCESS, V15, P57, DOI 10.1016/0165-1684(88)90028-X
   COHEN A, 1992, COMMUN PUR APPL MATH, V45, P485, DOI 10.1002/cpa.3160450502
   DAUBECHIES I, 1988, COMMUN PURE APPL MAT, V41, P906
   Do MN, 2002, IEEE T IMAGE PROCESS, V11, P146, DOI 10.1109/83.982822
   EDEN M, 1985, SIGNAL PROCESS, V8, P381, DOI 10.1016/0165-1684(85)90001-5
   Faloutsos C., 1994, Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, V3, P231, DOI 10.1007/BF00962238
   Fan GL, 2000, IEEE T CIRC SYST VID, V10, P120, DOI 10.1109/76.825866
   HUMMEL R, 1989, IEEE T ACOUST SPEECH, V37, P2111, DOI 10.1109/29.45555
   IKONOMOPOULOS A, 1985, SIGNAL PROCESS, V8, P179, DOI 10.1016/0165-1684(85)90073-8
   KUNT M, 1987, IEEE T CIRCUITS SYST, V34, P1306, DOI 10.1109/TCS.1987.1086071
   KUNT M, 1985, P IEEE, V73, P549, DOI 10.1109/PROC.1985.13184
   LOGAN BF, 1977, AT&T TECH J, V56, P487, DOI 10.1002/j.1538-7305.1977.tb00522.x
   MALLAT S, 1991, IEEE T INFORM THEORY, V37, P1019, DOI 10.1109/18.86995
   MALLAT S, 1992, IEEE T PATTERN ANAL, V14, P7
   MALLAT S, 1989, T AM MATH SOC, V315, P68
   Marr D., 1982, Visual perception
   PASS G, 1996, P 4 ACM C MULT BOST
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   SHUKLA R, 2002, P IEEE INT C MULT LA
   VAILAYA A, 1998, P IEEE WORKSH CONT B
   Witkin A.P., 1983, P INT JOINT C ART IN, P1019, DOI DOI 10.1007/978-3-8348-9190-729
   WU XL, 1992, IEEE T INFORM THEORY, V38, P1755, DOI 10.1109/18.165448
   Zhong S., 1990, THESIS NEW YORK U
NR 26
TC 8
Z9 8
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 805
EP 816
DI 10.1109/TMM.2005.854471
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900002
DA 2024-07-18
ER

PT J
AU Matrawy, A
   Lambadaris, I
AF Matrawy, A
   Lambadaris, I
TI A real-time video multicast architecture for assured forwarding services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE assured forwarding; differentiated services (diffserv); MPEG; multicast;
   multimedia; real-time; RED; video
AB This paper presents our work on developing an architecture for multicasting real-time MPEG4 over IP networks that provide service differentiation. In particular, this work is targeted at assured forwarding (AF) style services. This work is an attempt to find a simple solution to the problem of multicast congestion control of real-time traffic by exploiting the service differentiation capabilities of AF networks. Our architecture assumes loss differentiation in the network and assumes the network's ability to provide explicit congestion notification messages to the sender. We do not consider policing/shaping at the edge routers. Rather, we consider a more general case where packet marking and flow control are provided at the senders. For this network model, we built an end-to-end architecture and developed a rate-adaptation algorithm that can operate in both unicast and multicast applications with a minor modification. The simulation results show how the rate-adaptation algorithm accommodates different receivers with different networking capabilities and provides receivers with different levels of quality by taking advantage of the queue management capabilities of the AF service. We test how the architecture scales to a large number of receivers, how multiple multicast sessions interact, and how it interacts with TCP.
C1 Carleton Univ, Broadband Networks Lab, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
C3 Carleton University
RP Carleton Univ, Broadband Networks Lab, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
EM amatrawy@sce.carleton.ca; ioannis@sce.carleton.ca
CR BOLOT JC, 1994, P ACM SIGCOMM LOND U
   BYERS J, 2001, P INFOCOM ANCH AK MA
   Byers J., 1998, P SIGCOMM VANC BC CA
   Clark DD, 1998, IEEE ACM T NETWORK, V6, P362, DOI 10.1109/90.720870
   CLERGET A, 1999, 3728 INRIA
   DELUCIA D, 1997, P IEEE INFOCOM KOB J
   Floyd S., 1994, Computer Communication Review, V24, P8, DOI 10.1145/205511.205512
   Floyd S, 1993, IEEE ACM T NETWORK, V1, P397, DOI 10.1109/90.251892
   GOPALAKRISHNAN R, 1999, P NOSSDAV
   GUIRGUIS RM, 2000, P IEEE ICC NEW ORL L
   HEINANEN J, 1999, 2597 RFC
   ISMAIL MR, 1995, P INFOCOM BOST MA AP
   KOENEN R, 1999, IEEE SPECTRUM    FEB
   Legout A, 2000, PERF E R SI, V28, P13, DOI 10.1145/345063.339340
   LEGOUT A, 2000, P NOSSDAV
   Li X, 1998, IEEE INFOCOM SER, P1062, DOI 10.1109/INFCOM.1998.662916
   LUBY M, 1999, RMRG M PIS IT JUN
   MATRAWY A, 2002, P 4 IEEE INT WORKSH
   MATRAWY A, MPEG4 GENERATOR CODE
   MATRAWY A, 2002, THESIS CARLETON U OT
   MATRAWY A, 2001, P IEEE GLOBECOM SAN
   MCCANNE SR, 1996, THESIS U CALIFORNIA
   MELAMED B, 1993, PERF EVAL COMPUT COM
   NAKAUCHI K, 2001, P IEEE ICC HELS FINL
   REININGER D, 1994, P 14 INT TEL C ITC 1
   RIZZO L, 2000, P SIGCOMM STOCKH SWE
   VICISANO L, 1998, P INFOCOM SAN FRANC
   WIDMER J, 2001, P SIGCOMM SAN DIEG C
   WU L, 1997, P NOSSDAV ST LOUIS M
   ZHANG Z, 2002, P IEEE ICC NEW YORK
   1997, CISCO WHITE PAPER
NR 31
TC 4
Z9 4
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 688
EP 699
DI 10.1109/TMM.2005.846778
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chan, SP
   Kok, CW
   Wong, AK
AF Chan, SP
   Kok, CW
   Wong, AK
TI Multimedia streaming gateway with Jitter Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE DropTail; Jitter Detection; multimedia streaming; RED; TCP-friendliness
AB This paper investigates a novel active buffer management scheme, "Jitter Detection" (JD) for gateway-based congestion control to stream multimedia traffics in packet-switched networks. The quality of multimedia presentation can be greatly degraded due to network delay variation or jitter when transported over a packet-switched network. Jitter degrades the timing relationship among packets in a single media stream and between packets from different media streams and, hence, creates multimedia synchronization problems. Moreover, too much jitter will also degrade the performance of the streaming buffer in the client. Packets received by the client will be rendered useless if they have accumulated enough jitter. The proposed active buffer management scheme will improve the quality of service in multimedia networking by detecting and discarding packets that accumulated enough jitter, such as to maintain a high bandwidth for packets within the multimedia stream's jitter tolerance. Simulation results have shown that the proposed scheme can effectively lower the average received packet jitter and increase the goodput of the received packets when compared to random early detection (RED) and DropTail used in gateway-based congestion control. Furthermore, simulation results have also revealed that the proposed scheme can maintain the same TCP friendliness when compared to that of RED and DropTail used for multimedia streams.
C1 Hong Kong Univ Sci & Technol, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Hong Kong Univ Sci & Technol, Dept Elect Engn, Clear Water Bay, Kowloon, Hong Kong, Peoples R China.
RI Kok, C.W./AAM-6407-2020
CR [Anonymous], 1989, Requirements for Internet Hosts -
   CHAN SP, 2002, P IEEE ICC MAY, P2503
   CHAN SP, 2001, P IEEE PCM OCT, P1010
   Chaudhry S., 1995, Proceedings 1995 International Conference on Network Protocols (Cat. No.95TB8122), P47, DOI 10.1109/ICNP.1995.524818
   De Cnodder S, 2000, IEEE SYMP COMP COMMU, P793, DOI 10.1109/ISCC.2000.860741
   FLOYD S, 2000, CONTROLLING HIGH BAN
   Floyd S, 1993, IEEE ACM T NETWORK, V1, P397, DOI 10.1109/90.251892
   PIPPAS JB, 2000, P ICC2000, V1, P475, DOI DOI 10.1109/ICC.2000.853364
   Shuaib K, 2000, MULTIMEDIA SYST, V8, P231, DOI 10.1007/s005309900039
NR 9
TC 15
Z9 17
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 585
EP 592
DI 10.1109/TMM.2005.843338
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200020
DA 2024-07-18
ER

PT J
AU Lucey, S
   Chen, TH
   Sridharan, S
   Chandran, V
AF Lucey, S
   Chen, TH
   Sridharan, S
   Chandran, V
TI Integration strategies for audio-visual speech processing: Applied to
   text-dependent speaker recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio-visual speech processing (AVSP); classifier combination;
   integration strategies; multistream hidden Markov model (HMM); speaker
   recognition
AB In this paper, an in-depth analysis is undertaken into effective strategies for integrating the audio-visual speech modalities with respect to two major questions. Firstly, at what level should integration occur? Secondly, given a level of integration how should this integration be implemented? Our work is based around the well-known hidden Markov model (HMM) classifier framework for modeling speech. A novel framework for modeling the mismatch between train and test observation sets is proposed, so as to provide effective classifier combination performance between the acoustic and visual HMM classifiers. From this framework, it can be shown that strategies for combining independent classifiers, such as the weighted product or sum rules, naturally emerge depending on the influence of the mismatch. Based on the assumption that poor performance in most audio-visual speech processing applications can be attributed to train/test mismatches we propose that the main impetus of practical audio-visual integration is to dampen the independent errors, resulting from the mismatch, rather than trying to model any bimodal speech dependencies. To this end a strategy is recommended, based on theory and empirical evidence, using a hybrid between the weighted product and weighted sum rules in the presence of varying acoustic noise for the task of text-dependent speaker recognition.
C1 Carnegie Mellon Univ, Dept Elect & Comp Engn, Adv Multimedia Proc Lab, Pittsburgh, PA 15213 USA.
   Queensland Univ Technol, Sch Elect & Elect Syst Engn, RCSAVT, Speech Res Lab, Brisbane, Qld 4001, Australia.
C3 Carnegie Mellon University; Queensland University of Technology (QUT)
RP Carnegie Mellon Univ, Dept Elect & Comp Engn, Adv Multimedia Proc Lab, Pittsburgh, PA 15213 USA.
EM slucey@ieee.org; tsuhan@cmu.edu; s.sridharan@qut.edu.au;
   v.chandran@qut.edu.au
RI Lucey, Simon/HDO-1716-2022; Lucey, Simon M/B-7556-2011; Chandran,
   Vinod/I-9691-2012
OI Chandran, Vinod/0000-0003-3185-0852; Chen, Tsuhan/0000-0003-3951-7931;
   Lucey, Simon/0000-0002-6326-042X; Sridharan, Sridha/0000-0003-4316-9001
CR ADJONDANI A, 1995, P EUR 95 C MADR SPAI, P1563
   [Anonymous], 1997, P EUR C SPEECH COMM
   Chen T, 1998, P IEEE, V86, P837, DOI 10.1109/5.664274
   Chibelushi CC, 2002, IEEE T MULTIMEDIA, V4, P23, DOI 10.1109/6046.985551
   CHIBELUSHI CC, 1993, P EUR C SPEECH COMM, P157
   COX S, 1997, AUD VIS SPEECH PROC
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Jourlin P, 1997, PATTERN RECOGN LETT, V18, P853, DOI 10.1016/S0167-8655(97)00070-6
   Kamel MS, 2003, LECT NOTES COMPUT SC, V2709, P1
   Kittler J, 1998, PATTERN ANAL APPL, V1, P18, DOI 10.1007/BF01238023
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   LAVAGETTO F, 1995, IEEE T REHABIL ENG, V3, P1
   Lucey S, 2003, EURASIP J APPL SIG P, V2003, P264, DOI 10.1155/S1110865703209045
   Lucey S., 2002, P INT C SPOK LANG PR, P1961
   LUCEY S, 2001, P EUR C SPEECH COMM, P1185
   Luettin J, 2001, INT CONF ACOUST SPEE, P169, DOI 10.1109/ICASSP.2001.940794
   MANSOUR D, 1989, IEEE T ACOUST SPEECH, V37, P1659, DOI 10.1109/29.46548
   MCGRATH M, 1985, J ACOUST SOC AM, V77, P678, DOI 10.1121/1.392336
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   MOVELLAN JR, 1997, 9701 USCD DEP COGN S
   PIGEON S, 1997, P INT C AUD VID BAS
   Potamianos G, 1998, INT CONF ACOUST SPEE, P3733, DOI 10.1109/ICASSP.1998.679695
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   STORK DG, 1996, NATO ASI F, V150
   Tomlinson MJ, 1996, INT CONF ACOUST SPEE, P821, DOI 10.1109/ICASSP.1996.543247
   VARGA AP, 1990, INT CONF ACOUST SPEE, P845, DOI 10.1109/ICASSP.1990.115970
   Wark T, 1999, INT CONF ACOUST SPEE, P3061, DOI 10.1109/ICASSP.1999.757487
   YOUNG SJ, 1999, HTK BOOK HTK VERSION
NR 31
TC 20
Z9 22
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 495
EP 506
DI 10.1109/TMM.2005.846777
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200012
DA 2024-07-18
ER

PT J
AU Wang, X
   DeMartini, T
   Wragg, B
   Paramasivam, M
   Barlas, C
AF Wang, X
   DeMartini, T
   Wragg, B
   Paramasivam, M
   Barlas, C
TI The MPEG-21 Rights Expression Language and Rights Data Dictionary
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE authorization; digital rights management; protection; rights; trust
   management
AB The MPEG-21 Rights Expression Language (REL) is an XML-based language for digital rights management (DRM), providing a universal method for specifying rights and conditions associated with the distribution and use of assets like content, resources and services. Evolved from the eXtensible rights Markup language (XrML), the REL facilitates the creation of an open DRM architecture for managing and protecting these assets. As a general-purpose rights expression language, the REL is agnostic to types of assets, platforms and media, and expressive enough to support applications that can be even beyond DRM, including protecting privacy. It also contains additional capabilities in the areas of extensibility, security, trust management, and life cycle management of rights. This article provides an overview of the REL in terms of its data model, expressiveness, authorization model, structure for extensibility and profiling, and usages in digital media, trust management, and web services. To support the REL and provide extensive semantics for the management of rights, MPEG-21 also defined a Rights Data Dictionary (RDD). Based on original work conducted by (indecs), the MPEG-21 RDD specifies a methodology and structure for the RDD dictionary. The specification defines a core set of terms and provides a mechanism for the introduction of further terms through a registration authority. The RDD also supports the mapping of terms from different namespaces.
C1 ContentGuard Inc, El Segundo, CA 90245 USA.
   Universal Music Int, London SW1Y 4JU, England.
   Microsoft Corp, Redmond, WA 98052 USA.
   Rightscom Ltd, London SE1 7HS, England.
C3 Microsoft
RP Wang, X (corresponding author), ContentGuard Inc, El Segundo, CA 90245 USA.
EM xin.wang@contentguard.com; thomas.demartini@contentguard.com;
   barney.wragg@umusic.com; parama@exchange.microsoft.com;
   chris.barlas@rightscom.com
CR Bormans J, 2003, IEEE SIGNAL PROC MAG, V20, P53, DOI 10.1109/MSP.2003.1184339
   Burnett I, 2003, IEEE MULTIMEDIA, V10, P60, DOI 10.1109/MMUL.2003.1237551
   *CONT INC, 2001, EXT RIGHTS MARK LAND
   *IEEE, 2004, IEEE SIGN PROC MAG S, V21
   *ISO IEC, 2004, 2100062004 ISO IEC
   *ISO IEC, 2003, 2100022003 ISO IEC
   *ISO IEC, 2003, 2100032003 ISO IEC
   ROSENBLATT B, 2001, DIGITAL RIGHTS MANAG
   RUST G, 2000, FINAL REPORT
   WANG X, P 2002 ACM WORKSH XM, P71
   Wang YQ, 2003, CHINESE EDUC SOC, V36, P74
   2004, P IEEE SPEC ISS EN S, V92
   2003, INFORMATION TECHNOLO
NR 13
TC 40
Z9 46
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 408
EP 417
DI 10.1109/TMM.2005.846788
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200003
DA 2024-07-18
ER

PT J
AU Cheng, SC
AF Cheng, SC
TI Visual pattern matching in motion estimation for object-based very low
   bit-rate coding using moment-preserving edge detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE low bit-rate video coding; edge feature; moment-preserving technique;
   shape representation
ID IMAGE; VIDEO; SEGMENTATION; COMPRESSION
AB This paper proposes an object-based coding method for very low bit-rate channels, using a method based on motion estimation with a block-based moment-preserving edge detector. In most existing object-based coding methods, only the global motion components are transmitted. However, the global motion prediction error is large, even after motion compensation using the discrete cosine transform (DCT), when images contain rapid moving objects and noise. Furthermore, the global motion-compensating method cannot result in small prediction error if the segmented objects consist of subobjects that move along different directions. The technique proposed in this paper involves segmenting moving objects front video sequences and representing objects compactly by visual-pattern approximations of the boundary. A visual pattern is obtained by detecting the line edge from a square block using the moment-preserving edge detector. Since high computational complexity is required for motion estimation using block matching, a fast block-matching method based on the visual patterns is proposed to reduce the burden of the overall coder complexity. Computer simulation results show that the proposed method gives good performance in terms of the subjective quality, the peak signal-to-noise ratio, and the compression ratio.
C1 Natl Kaohsiung First Univ Sci & Technol, Dept Comp & Commun Engn, Kaohsiung 824, Taiwan.
C3 National Kaohsiung University of Science & Technology
RP Natl Kaohsiung First Univ Sci & Technol, Dept Comp & Commun Engn, Kaohsiung 824, Taiwan.
EM csc@ccms.nkfust.edu.tw
CR Chen MJ, 2000, IEEE T CONSUM ELECTR, V46, P505, DOI 10.1109/30.883402
   CHENG SC, 2002, P INT COMP S HAWL TA
   Cheung WF, 2001, IEE P-VIS IMAGE SIGN, V148, P194, DOI 10.1049/ip-vis:20010310
   Cho DS, 1999, J VIS COMMUN IMAGE R, V10, P291, DOI 10.1006/jvci.1999.0421
   Cho DS, 1998, IEEE T CIRC SYST VID, V8, P316, DOI 10.1109/76.678628
   CLARKS RJ, 1995, DIGITAL COMPRESSION
   EBRAHIMI T, 1995, P IEEE, V83, P877, DOI 10.1109/5.387090
   Francois E, 1997, IEEE T CIRC SYST VID, V7, P97, DOI 10.1109/76.554421
   GERKEN P, 1994, IEEE T CIRC SYST VID, V4, P228, DOI 10.1109/76.305868
   *ISO IEC JTC SC2 W, 1990, MPEG90041 ISOIEC JTC
   Kompatsiaris I, 2000, IEEE T CIRC SYST VID, V10, P1388, DOI 10.1109/76.889030
   KUNT M, 1985, P IEEE, V73, P549, DOI 10.1109/PROC.1985.13184
   Li HL, 1996, PATTERN RECOGN, V29, P1245, DOI 10.1016/0031-3203(95)00170-0
   Malo J, 2001, IEEE T IMAGE PROCESS, V10, P1411, DOI 10.1109/83.951528
   Pei SC, 1999, IEEE T IMAGE PROCESS, V8, P614, DOI 10.1109/83.760310
   ROSENFIELD A, 1982, DIGITAL PICTURE PROC, V2
   Salembier P, 1999, IEEE T CIRC SYST VID, V9, P1147, DOI 10.1109/76.809153
   Schiller H., 1993, Signal Processing: Image Communication, V5, P319, DOI 10.1016/0923-5965(93)90054-W
   Talluri R, 1997, IEEE T CIRC SYST VID, V7, P221, DOI 10.1109/76.554433
   TSAI WH, 1984, COMPUT VIS GRAPH IMA, P377
   Watson AB, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P41, DOI 10.1109/ICIP.2002.1038898
   WELLBORN M, 1994, IEEE T CIRC SYST VID, V4, P236
   Winkler S, 1999, SIGNAL PROCESS, V78, P231, DOI 10.1016/S0165-1684(99)00062-6
   2000, ISOIECJTC1SC29WG11N3
NR 24
TC 6
Z9 9
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 189
EP 200
DI 10.1109/TMM.2005.843358
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400001
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Wang, GJ
   Xiong, ZX
   Zhou, JP
   Zhu, WW
AF Zhang, Q
   Wang, GJ
   Xiong, ZX
   Zhou, JP
   Zhu, WW
TI Error robust scalable audio streaming over wireless IP networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bit allocation; error resilience; error robustness; rate-distortion
   optimization; scalable audio streaming; unequal error protection;
   wireless IP networks
ID BIT ALLOCATION; VIDEO; TRANSMISSION; MPEG-4
AB Streaming high-fidelity audio over wireless Internet protocol (IP) networks is a challenging task because the networks present not only packet losses, but also residual bit errors. These losses and errors have severe adverse effect on the compressed audio bitstream. To solve this problem, this paper introduces error resilience in conjunction with error protection for scalable audio streaming over wireless networks. Specifically, error resilience is achieved by performing bitstream data partitioning and reversible variable length coding in the audio coder. Error protection is provided by layered product channel code to simultaneously handle packet losses and residual bit errors. Both the row and column codes of the product code provide unequal error protection for different layers of the audio bitstream by considering the characteristics of the scalable audio. Rate-distortion optimization is performed to determine the best source-channel coding tradeoff that minimizes the average expected end-to-end distortion. Simulation results demonstrate the effectiveness of our proposed approach.
C1 Microsoft Res Asia, Beijing 100080, Peoples R China.
   Tsing Hua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   Texas A&M Univ, Dept Elect Engn, College Stn, TX 77843 USA.
C3 Microsoft; Microsoft Research Asia; Tsinghua University; Texas A&M
   University System; Texas A&M University College Station
RP Microsoft Res Asia, Haidian Dist, Beijing 100080, Peoples R China.
EM qianz@microsoft.com; wanggj98@mails.tsinghua.edu.cn; zx@lena.tumu.edu;
   jzhou2@uiuc.edu
RI Zhang, Qian/B-9058-2009
OI Zhang, Qian/0000-0001-9205-1881
CR *3GPP2, 2000, REQ 3G NETW BAS INT
   *3GPP2, 23107 3GPP TS 0
   [Anonymous], 1996, WIRELESS COMMUNICATI
   [Anonymous], 3003 UMTS
   ARAZAKI SI, 1991, IEEE T CONSUMER ELEC, V37
   BEATON R, 1996, DIGITAL AUDIO BIT RA, P126
   BOLOT J, P ACM MULT SYST 97
   BOLOT JC, 1999, P IEEE INF 99 NEW YO
   CAI H, 1995, P IEEE INT C IM PROC
   Cheung G, 2000, IEEE T IMAGE PROCESS, V9, P340, DOI 10.1109/83.826773
   COSMAN P, 1998, P 32 AS C SIGN SYST
   FLETCHER R., 1987, PRACTICAL METHODS OP
   HAGENAUER J, 1999, IEEE J SEL AREA COMM, V10, P1764
   KUDUMAKIS P, 1996, P ISCAS 96, V2, P41
   Lin S., 1983, Error Control Coding: Fundamentals and Applications
   Moccagatta I, 2000, IEEE J SEL AREA COMM, V18, P899, DOI 10.1109/49.848245
   MORIYA T, 2000, P ICASSP 00, V2, P889
   *MPEG, 1999, W3075 MPEG
   Painter T, 2000, P IEEE, V88, P451, DOI 10.1109/5.842996
   RAMCHANDRAN K, 1994, IEEE T IMAGE PROCESS, V3, P533, DOI 10.1109/83.334987
   SACHS DG, P DCC 2000 SNOWB UT
   TAKISHIMA Y, 1995, IEEE T COMMUN, V43, P158, DOI 10.1109/26.380026
   Talluri R, 1998, IEEE COMMUN MAG, V36, P112, DOI 10.1109/35.685373
   TEUHOLA J, 1978, INFORM PROCESS LETT, V7, P308, DOI 10.1016/0020-0190(78)90024-8
   VARSA V, 2001, ITUT STUDY GROUP, V16
   WEN G, 1997, P ICIP 97 SANT BARB, V2, P65
   Woller Joy., 1996, BASICS MONTE CARLO S
   YUNG C, 1999, P 1999 IEEE INT S CI, V6
   Zhang Q, 2004, IEEE T CIRC SYST VID, V14, P1049, DOI 10.1109/TCSVT.2004.831966
   ZHOU J, 2001, IEEE WORKSH MULT SIG
   Zorzi M, 1998, IEEE T COMMUN, V46, P1468, DOI 10.1109/26.729391
   ZORZI M, 1995, P IEEE ICUPC 95 TOK
NR 32
TC 12
Z9 14
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 897
EP 909
DI 10.1109/TMM.2004.837249
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, HL
   Liu, GZ
   Zhang, ZW
   Li, YL
AF Li, HL
   Liu, GZ
   Zhang, ZW
   Li, YL
TI Adaptive scene-detection algorithm for VBR video stream
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dynamic threshold model (DTM); multimedia communication; scene-change
   detection; scene duration; variable bit rate (VBR) video
ID LONG-RANGE DEPENDENCE; MODELS
AB Several scene-detection algorithms, which are only based on bit rate fluctuations, have been proposed. All of them are presented on the fixed thresholds, which are obtained by the empirical records of the video characteristics. Due to the sensitivity of these methods to the accuracy of the records, which are generally obtained by testing several values repeatedly, bad performance evaluation might be observed for the actual scene detection, especially for real-time video traffic. In this paper, we review the previous works in this area, and study the correlation between the scene duration and the scene change at the frame level, and simultaneously investigate the local statistical characteristics of scenes such as variance and peak bit rate etc. Based on this analysis, an effective decision function is first constructed for the scene segmentation. Then, we propose a scene-detection algorithm using the defined dynamic threshold model, which can capture the statistical properties of the scene changes. Experimental results using 15 variable bit rate MPEG video traces indicate good performances of the proposed algorithm with significantly improved scene-detection accuracy.
C1 Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM hlli@mailst.xjtu.edu.cn; liugz@mail.xjtu.edu.cn;
   zhweizh@mailst.xjtu.edu.cn; lyl_xjtu@sina.com
OI Li, Hongliang/0000-0002-7481-095X
CR BERAN J, 1995, IEEE T COMMUN, V43, P1566, DOI 10.1109/26.380206
   Casilari E, 1999, ELECTRON LETT, V35, P134, DOI 10.1049/el:19990102
   Chiruvolu G, 1998, ICC 98 - 1998 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS VOLS 1-3, P554, DOI 10.1109/ICC.1998.682940
   Dawood AM, 1999, IEEE T MULTIMEDIA, V1, P77, DOI 10.1109/6046.748173
   FRATER MR, 1994, IEEE T CIRC SYST VID, V4, P521, DOI 10.1109/76.340193
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   Heyman DP, 1992, IEEE T CIRC SYST VID, V2, P49, DOI 10.1109/76.134371
   Heyman DP, 1996, IEEE ACM T NETWORK, V4, P40, DOI 10.1109/90.503760
   Heyman DP, 1996, IEEE ACM T NETWORK, V4, P301, DOI 10.1109/90.502230
   Jelenkovic PR, 1998, J APPL PROBAB, V35, P325, DOI 10.1017/S0021900200014984
   Jelenkovic PR, 1997, IEEE J SEL AREA COMM, V15, P1052, DOI 10.1109/49.611159
   KRUNZ M, SIGM 95 OTT ON CAN
   KRUNZ M, 1997, P ACM SIGMETRICS 97
   KRUNZ M, 1997, SIGMETRICS 97 SEATTL
   Krunz MM, 2000, IEEE T MULTIMEDIA, V2, P27, DOI 10.1109/6046.825792
   LAZAR A, 1994, ACM MULTIMEDIA SYSTE, V1, P253
   Lee SW, 2000, IEEE T MULTIMEDIA, V2, P240, DOI 10.1109/6046.890059
   LIU D, 2000, P IEEE ICCN 2000, P258
   Manzoni P, 1999, IEEE ACM T NETWORK, V7, P387, DOI 10.1109/90.779207
   Melamed B, 1998, IEEE J SEL AREA COMM, V16, P600, DOI 10.1109/49.700899
   ROSE O, 1995, 101 U WUERZBURG WUER
   Vasconcelos N, 2000, IEEE T IMAGE PROCESS, V9, P3, DOI 10.1109/83.817595
   Yoo SJ, 2002, IEEE T BROADCAST, V48, P10, DOI 10.1109/11.992849
   Yoo SJ, 1998, ELECTRON LETT, V34, P1484, DOI 10.1049/el:19981050
   1997, IEEE SIGNAL PROCESSI, V82
NR 25
TC 27
Z9 32
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 624
EP 633
DI 10.1109/tmm.2004.830812
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800010
DA 2024-07-18
ER

PT J
AU Abdel-Mottaleb, M
   Krishnamachari, S
AF Abdel-Mottaleb, M
   Krishnamachari, S
TI Multimedia descriptions based on MPEG-7: Extraction and applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE compact color descriptor; edge descriptor; group of frames descriptor;
   image indexing; image retrieval; MPEG-7; video retrieval
AB The amount of digital multimedia content available to consumers is growing because of the existence of digital capturing devices such as digital cameras, camcorders and the advent of digital video broadcast. With this increase in content, it becomes important for users to be able to browse and search for content in a timely manner. Descriptions and annotations of the content are needed to enable searching and browsing of content. MPEG-7 is a recent ISO standard for multimedia content description. In this paper, we present three descriptors, which we had proposed to MPEG-7, and have been accepted to the standard. In addition, we describe algorithms for automatically extracting these descriptors. We also present the indexing and retrieval algorithms that we developed for these descriptors; these algorithms are fast and scalable for large databases. The results presented in the paper for image and video segment matching using these descriptors show their usefulness in real applications.
C1 Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.
   EG Technol Inc, Atlanta, GA 30318 USA.
C3 University of Miami
RP Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.
EM mottaleb@miami.edu; skrishn@egtinc.com
OI AbdelMottaleb, Mohamed/0000-0002-4163-7230
CR Abdel-Mottaleb M., 1996, Proceedings ACM Multimedia 96, P427, DOI 10.1145/244130.244454
   AGNIHOTRI L, 2000, 4 INT C ADV VIS INF
   BROWN GJ, 1994, COMPUT SPEECH LANG, V8, P297, DOI 10.1006/csla.1994.1016
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   CHEN JY, 1999, P SPIE STOR RETR IM, V7, P144
   FERMAN AM, 2000, P INT C IM PROC VANC
   Flickner M., 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146
   GONG Y, 1995, IEEE INT C MULT COMP, P167
   Hampapur A., 1994, Proceedings ACM Multimedia '94, P357, DOI 10.1145/192593.192699
   HIRATA K, 1992, K3 INT C EXT DAT TEC
   Jain AK, 1996, PATTERN RECOGN, V29, P1233, DOI 10.1016/0031-3203(95)00160-3
   Krishnamachari S, 1999, IEEE SYMP COMP COMMU, P301, DOI 10.1109/ISCC.1999.780837
   KRISHNAMACHARI S, 1999, JTCISC29WG11 ISOIEC
   Ma WY, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P568, DOI 10.1109/ICIP.1997.647976
   Martin K., 1999, DISSERTATION
   Qi W, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P520, DOI 10.1109/ICIP.2000.899482
   Toklu C, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1333, DOI 10.1109/ICME.2000.871012
   2001, MULTIMEDIA CONTENT 5
   2001, TEXT ISOIEC 159383 M
NR 20
TC 17
Z9 18
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 459
EP 468
DI 10.1109/TMM.2004.827500
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200009
DA 2024-07-18
ER

PT J
AU Fisher, JW
   Darrell, T
AF Fisher, JW
   Darrell, T
TI Speaker association with signal-level audiovisual fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audiovisual correspondence; multimodal data association; mutual
   information
AB Audio and visual signals arriving from a common source are detected using a signal-level fusion technique. A probabilistic multimodal generation model is introduced and used to derive an information theoretic measure of cross-modal correspondence. Nonparametric statistical density modeling techniques can characterize the mutual information between signals from different domains. By comparing the mutual information between different pairs of signals, it is possible to identify which person is speaking a given utterance and discount errant motion or audio from other utterances or nonspeech events.
C1 MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.
C3 Massachusetts Institute of Technology (MIT)
RP MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM trevor@ai.mit.edu
OI Fisher, John/0000-0003-4844-3495
CR BECKER S, 1992, THESIS U TORONTO TOR
   Cover Thomas A., 1991, ELEM INF THEORY, DOI 10.1002/047174882X
   CUTLER R, IEEE INT C MULT EXP
   DECO G, 1996, INFORMATION THEORETI
   Fisher III John W, 2000, Advances in neural information processing systems, V13
   FISHER J, 1997, P IEEE WORKSH NEUR N, V7, P14
   Fisher JW, 2002, LECT NOTES COMPUT SC, V2352, P592
   FISHER JW, 1998, P IJCNN 98 INT JOINT, V3, P1712
   Hershey J, 2000, ADV NEUR IN, V12, P813
   Ihler AT, 2003, LECT NOTES COMPUT SC, V2634, P239
   IHLER AT, IN PRESS IEEE T SIGN
   KULLBACK S, 1959, INFORMATION THEORY S
   MAHALANOBIS A, 1987, APPL OPTICS, V26, P3633, DOI 10.1364/AO.26.003633
   MEIER U, 1999, 2 INT C MULT INF ICM
   Nock HarrietJ., 2002, Proceedings of the tenth ACM international conference on Multimedia, P303
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   PLUMBLEY MD, 1988, P 1988 CONN MOD SUMM, P239
   PLUMBLEY MD, 1991, CUEDFINFENGTR78
   Slaney Malcolm, 2000, ADV NEURAL INFORM PR, V13, P1
   WOLFF G, 1994, P NEURAL INFORMATION, P1027
NR 20
TC 64
Z9 72
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 406
EP 413
DI 10.1109/TMM.2004.827503
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rangaswami, R
   Dimitrijevic, Z
   Chang, E
   Chan, SHG
AF Rangaswami, R
   Dimitrijevic, Z
   Chang, E
   Chan, SHG
TI Fine-grained device management in an interactive media server
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data placement; disk profiling; interactive; IO scheduling; media
   server; streaming; video-on-demand
ID ON-DEMAND SYSTEMS; MULTICAST DELIVERY
AB The use of interactive media has already gained considerable popularity. Interactivity gives viewers VCR controls like slow-motion, pause, fast-forward, and instant replay. However, traditional server-based or client-based approaches for supporting interactivity either consume too much network bandwidth or require large client buffering; and hence they are economically unattractive. In this paper, we propose the architecture and design of an interactive media proxy (IMP) server that transforms noninteractive broadcast or multicast streams into interactive ones for servicing a large number of end users. For IMP to work cost-effectively, it must carefully manage its storage devices, which are needed for caching voluminous media data. In this regard, we propose a fine-grained device management strategy consisting of three complementary components: disk profiler, data placement, and IO scheduler. Through quantitative analysis and experiments, we show that these fine-grained strategies considerably improve device throughput under various workload scenarios.
C1 Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
   Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
   Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 University of California System; University of California Santa Barbara;
   University of California System; University of California Santa Barbara;
   Hong Kong University of Science & Technology
RP Rangaswami, R (corresponding author), Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
OI Chan, Gary Shueng Han/0000-0003-4207-764X; Rangaswami,
   Raju/0009-0000-5243-9451
CR Almeroth KC, 1996, IEEE J SEL AREA COMM, V14, P1110, DOI 10.1109/49.508282
   *ANSI, 1995, SCSI 2 SPEC X3T9 2 3
   Chan SHG, 2001, IEEE ACM T NETWORK, V9, P125, DOI 10.1109/90.917070
   Chang E, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P496
   Chen MS, 1996, IEEE MULTIMEDIA, V3, P51, DOI 10.1109/93.502294
   DAN A, 1995, J PARALLEL DISTR COM, V30, P168, DOI 10.1006/jpdc.1995.1135
   DIMITRIJEVIC Z, 2000, SCSIBENCH
   FENG WC, 1996, ICMCS JUN
   FOUAD FA, 1995, IEEE MULTIMEDIA  SPR, P90
   HUA KA, 1997, ACM SIGCOMM 97 CANN, P89
   JOHNSON T, 1996, P 4 IEEE C MULT COMP, P169
   Li VOK, 1997, P IEEE, V85, P1063, DOI 10.1109/5.611116
   NG R, 1994, P 20 VLDB C, P451
   Poon WWF, 1999, IEEE T BROADCAST, V45, P141, DOI 10.1109/11.754993
   RANGASWAMI R, 2001, FINE GRAINED DEVICE
   RUEMMLER C, 1994, COMPUTER, V2, P17
   SHENOY P, 1999, ACM MULTIMEDIA SYST, V7
   SHENOY PJ, 1995, P ACM MULT C NOV, P131
   Talagala Nisha., 1999, MICROBENCHMARK BASED
   TAVANAPONG W, 1997, P 5 ACM MULT C NOV
   WANG C, 2001, IWORLD C ED MULT HYP
   WORTHINGTON BL, 1995, P 1995 ACM SIGMETRIC, P146
   YU PS, 1993, MULTIMEDIA SYSTEMS, V1, P99
   [No title captured]
NR 24
TC 8
Z9 8
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2003
VL 5
IS 4
BP 558
EP 569
DI 10.1109/TTM.2003.814722
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 742VA
UT WOS:000186537700006
DA 2024-07-18
ER

PT J
AU Roy, D
AF Roy, D
TI Grounded spoken language acquisition: Experiments in word learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cross-modal; language learning; multimodal; semantic grounding
ID INFANTS; PERCEPTION
AB Language is grounded in sensory-motor experience. Grounding connects concepts to the physical world enabling humans to acquire and use words and sentences in context. Currently most machines which process language are not grounded. Instead, semantic representations are abstract, pre-specified, and have meaning only when interpreted by humans. We are interested in developing computational systems which represent words, utterances, and underlying concepts in terms of sensory-motor experiences leading to richer levels of machine understanding. A key element of this work is the development of effective architectures for processing multisensory data. Inspired by theories of infant cognition, we present a computational model which learns words from untranscribed acoustic and video input. Channels of input derived from different sensors are integrated in an information-theoretic framework. Acquired words are represented in terms of associations between acoustic and visual sensory experience. The model has been implemented in a real-time robotic system which performs interactive language learning and understanding. Successful learning has also been demonstrated using infant-directed speech and images.
C1 MIT, Media Lab, Cambridge, MA 02139 USA.
C3 Massachusetts Institute of Technology (MIT)
RP Roy, D (corresponding author), MIT, Media Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
CR [Anonymous], 2009, Language learnability and language development: with new commentary by the author
   Barsalou LW, 1999, BEHAV BRAIN SCI, V22, P577, DOI 10.1017/S0140525X99532147
   BECKER S, 1992, NATURE, V355, P161, DOI 10.1038/355161a0
   BORNSTEIN MH, 1976, J EXP PSYCHOL HUMAN, V2, P115, DOI 10.1037/0096-1523.2.1.115
   Brent MR, 1999, MACH LEARN, V34, P71, DOI 10.1023/A:1007541817488
   Brooks R. A., 1990, Robotics and Autonomous Systems, V6, P3, DOI 10.1016/S0921-8890(05)80025-9
   Cover Thomas A., 1991, ELEM INF THEORY, DOI 10.1002/047174882X
   de Marcken C.G., 1996, THESIS MIT CAMBRIDGE
   DESA VR, 1998, NEURAL COMPUT, V10
   GORIN A, 1995, J ACOUST SOC AM, V97, P3441, DOI 10.1121/1.412431
   Grimshaw J., 1981, The Logical Problem of Language Acquisition, P165
   HARNAD S, 1990, PHYSICA D, V42, P335, DOI 10.1016/0167-2789(90)90087-6
   Hermansky H, 1994, IEEE T SPEECH AUDI P, V2, P578, DOI 10.1109/89.326616
   HUTTENLOCHER J, 1994, LANGUAGE ACQUISITION, P222
   Johnson Mark, 1987, The Body in the Mind
   KUHL PK, 1992, SCIENCE, V255, P606, DOI 10.1126/science.1736364
   Lakoff G., 1987, Women, Fire, and Dangerous Things: What Categories Reveal About the Mind
   MILEWSKI AE, 1976, J EXP CHILD PSYCHOL, V22, P229, DOI 10.1016/0022-0965(76)90004-7
   PETROVSKADELACR.D, 2000, P INT C SPOK LANG PR
   Pinker Stephen, 1994, LANGUAGE INSTINCT, P16
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Regier T., 1996, The human semantic potential: Spatial language and constrained connectionism
   ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192
   Rose R. C., 1996, AUTOMATIC SPEECH SPE, P303
   Roy D. K., 1999, THESIS MIT CAMBRIDGE
   ROY DK, 2000, P ICASSP IST TURK 20
   ROY DK, 1997, VISUAL P SIGGRAPH
   Sankar A., 1993, Artificial neural networks for speech and vision, P324
   SCHIELE B, 1996, P 13 INT C PATT REC, VB, P50
   Seneff S., 1988, GETTING STARTED DARP
   SISKIND J, 1992, THESIS MIT CAMBRIDGE
   Snow C. E., 1977, TALKING CHILDREN LAN
   STEELS L, 1997, P 4 EUR C ART LIF 19
   WERKER JF, 1988, DEV PSYCHOL, V24, P672, DOI 10.1037/0012-1649.24.5.672
   WRIGHT JH, 1996, P IEEE INT C AC SPEE, P307
   [No title captured]
NR 36
TC 38
Z9 46
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 197
EP 209
DI 10.1109/TMM.2003.811618
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lelescu, D
   Schonfeld, D
AF Lelescu, D
   Schonfeld, D
TI Statistical sequential analysis for real-time video scene change
   detection on compressed multimedia bitstream
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE abrupt and gradual scene changes; scene change detection; video
   segmentation; video shots
AB The increased availability and usage of multimedia information have created a critical need for efficient multimedia processing algorithms. These algorithms must offer capabilities related to browsing, indexing, and retrieval of relevant data. A crucial step in multimedia processing is that of reliable video segmentation into visually coherent video shots through scene change detection. Video segmentation enables subsequent processing operations on video shots, such as video indexing, semantic representation, or tracking of selected video information. Since video sequences generally contain both abrupt and gradual scene changes, video segmentation algorithms must be able to detect a large variety of changes. While existing algorithms perform relatively well for detecting abrupt transitions (video cuts), reliable detection of gradual changes is much more difficult. In this paper, a novel one-pass, real-time approach to video scene change detection based on statistical sequential analysis and operating on Compressed multimedia bitstream is proposed. Our approach models video sequences as stochastic processes, with scene changes being reflected by changes in the characteristics (parameters) of the process. Statistical sequential analysis is used to provide an unified framework for the. detection of both abrupt and gradual scene changes.
C1 DoCoMo Commun Labs, San Jose, CA 95110 USA.
   Univ Illinois, Dept Elect & Comp Engn, Multimedia Commun Lab, Chicago, IL 60607 USA.
C3 NTT Docomo; University of Illinois System; University of Illinois
   Chicago; University of Illinois Chicago Hospital
RP DoCoMo Commun Labs, San Jose, CA 95110 USA.
EM lelescu@docomolabs-usa.com; ds@ece.uic.edu
CR [Anonymous], 1995, P ACM MULT, DOI DOI 10.1145/217279.215266
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Basseville M., 1993, DETECTION ABRUPT CHA
   Boreczky JS, 1996, P SOC PHOTO-OPT INS, V2670, P170, DOI 10.1117/12.234794
   CHUA TS, 2000, P INT WORKSH ADV IM, P119
   FERNANDO WAC, 1999, P SPIE C STOR RETR I, V3656, P687
   FORD RM, 2000, ACM MULTIMEDIA SYST
   Gargi U, 1998, PROC CVPR IEEE, P559, DOI 10.1109/CVPR.1998.698661
   Kobla V, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P335, DOI 10.1145/266180.266384
   Kobla V, 1996, P SOC PHOTO-OPT INS, V2916, P78, DOI 10.1117/12.257312
   KOBLA V, 1999, P SPIE C STOR RETR I, V7, P302
   Lienhart R, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P509, DOI 10.1109/MMCS.1997.609763
   Lienhart R., 1999, P SOC PHOTO-OPT INS, P290
   MENG JH, 1995, P SOC PHOTO-OPT INS, V2419, P14, DOI 10.1117/12.206359
   MURAKAMI H, 1982, IEEE T PATTERN ANAL, V4, P511, DOI 10.1109/TPAMI.1982.4767295
   Patel NV, 1997, PATTERN RECOGN, V30, P583, DOI 10.1016/S0031-3203(96)00114-8
   Patel NV, 1996, IEE P-VIS IMAGE SIGN, V143, P315, DOI 10.1049/ip-vis:19960778
   Poor HV, 1998, ANN STAT, V26, P2179
   Schönsleben P, 2000, PROD PLAN CONTROL, V11, P2, DOI 10.1080/095372800232432
   Shen K., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P252, DOI 10.1109/ICIP.1995.537462
   Shen WZ, 1997, AEROSP SCI TECHNOL, V1, P97, DOI 10.1016/S1270-9638(97)90040-7
   SONG SM, 1998, P SOC PHOTO-OPT INS, V3312, P404
   YEO BL, 1995, P INT C MULT COMP SY, P2
   YEO BL, 1995, P IEEE INT C IM PROC, V2, P260
NR 24
TC 70
Z9 94
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 106
EP 117
DI 10.1109/TMM.2003.808819
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200009
DA 2024-07-18
ER

PT J
AU Ak, A
   Goswami, A
   Hauser, W
   Le Callet, P
   Dufaux, F
AF Ak, Ali
   Goswami, Abhishek
   Hauser, Wolf
   Le Callet, Patrick
   Dufaux, Frederic
TI RV-TMO: Large-Scale Dataset for Subjective Quality Assessment of Tone
   Mapped Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; image quality evaluation; pairwise comparison; tone
   mapping operators
ID REPRODUCTION
AB Tone mapping operators (TMO) are functions that map high dynamic range (HDR) images to a standard dynamic range (SDR), while aiming to preserve the perceptual cues of a scene that govern its visual quality. Despite the increasing number of studies on quality assessment of tone mapped images, current subjective quality datasets have relatively small numbers of images and subjective opinions. Moreover, existing challenges in transferring laboratory experiments to crowdsourcing platforms put a barrier for collecting large-scale datasets through crowdsourcing. In this work, we address these challenges and propose the RealVision-TMO (RV-TMO), a large-scale tone mapped image quality dataset. RV-TMO contains 250 unique HDR images, their tone mapped versions obtained using four TMOs and pairwise comparison results from seventy unique observers for each pair. To the best of our knowledge, this is the largest dataset available in the literature for quality evaluation of TMOs by the number of tone mapped images and number of annotations. Furthermore, we provide a content selection strategy to identify interesting and challenging HDR images. We also propose a novel methodology for observer screening in pairwise experiments. Our work does not only provide annotated data to benchmark existing objective quality metrics, but also paves the path to building new metrics for tone mapping quality evaluation.
C1 [Ak, Ali; Le Callet, Patrick] Nantes Univ, Ecole Cent Nantes, CNRS, LS2N,UMR 6004, F-44000 Nantes, France.
   [Goswami, Abhishek] Univ Warwick, Warwick Mfg Grp, Coventry CV4 7AL, W Midlands, England.
   [Hauser, Wolf] DxO Labs, F-92100 Paris, France.
   [Dufaux, Frederic] Univ Paris Saclay, CNRS, Lab Signaux & Syst, Cent Supelec, F-91190 Gif Sur Yvette, France.
C3 Centre National de la Recherche Scientifique (CNRS); Nantes Universite;
   Ecole Centrale de Nantes; University of Warwick; Centre National de la
   Recherche Scientifique (CNRS); Universite Paris Saclay; Universite Paris
   Cite
RP Ak, A (corresponding author), Nantes Univ, Ecole Cent Nantes, CNRS, LS2N,UMR 6004, F-44000 Nantes, France.
EM ali.ak@univ-nantes.fr; abhishek.goswami@warwick.ac.uk; whauser@dxo.com;
   patrick.lecallet@univ-nantes.fr; frederic.dufaux@l2s.centralesupelec.fr
RI Dufaux, Frederic/HJJ-1496-2023
OI Dufaux, Frederic/0000-0001-6388-4112; Le callet,
   Patrick/0000-0002-2143-7063; Ak, Ali/0000-0002-8572-3739
FU European Union [765911]
FX This work was supported by the European Union's Horizon 2020 Research
   and Innovation programme through Marie Sklodowska-Curie under Grant
   765911 (RealVision).
CR Ak A, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455992
   [Anonymous], Amazon mechanical turk
   Artusi A, 2019, J REAL-TIME IMAGE PR, V16, P413, DOI 10.1007/s11554-015-0547-x
   BARNARD GA, 1945, NATURE, V156, P177, DOI 10.1038/156177a0
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Cerda-Company X, 2018, J OPT SOC AM A, V35, P626, DOI 10.1364/JOSAA.35.000626
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Fisher RA, 1922, J R STAT SOC, V85, P87, DOI 10.2307/2340521
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Goswami A, 2021, IEEE INT WORKSH MULT, DOI 10.1109/MMSP53017.2021.9733707
   Goswami A, 2020, IEEE INT CONF MULTI
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   HANLEY JA, 1983, RADIOLOGY, V148, P839, DOI 10.1148/radiology.148.3.6878708
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   ITU-R, 2019, ITU-R Recommendation BT.500-14
   Jaccard P., 1901, B SOC VAUD SCI NAT, V37, P547, DOI DOI 10.5169/SEALS-266450
   Kim Min H., 2008, Proceedings of the Tenth IASTED International Conference on Computer Graphics and Imaging, P152
   Krasula Lukas, 2016, 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), DOI 10.1109/QoMEX.2016.7498936
   Krasula L, 2020, IEEE T MULTIMEDIA, V22, P2038, DOI 10.1109/TMM.2019.2952256
   Krasula L, 2017, IEEE J-STSP, V11, P64, DOI 10.1109/JSTSP.2016.2637168
   Krasula L, 2015, INT WORK QUAL MULTIM
   Krawczyk G, 2005, COMPUT GRAPH FORUM, V24, P635, DOI 10.1111/j.1467-8659.2005.00888.x
   Kuang JT, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P315
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P4725, DOI 10.1109/TIP.2017.2713945
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Mehta C. R., 2003, Cytel Softw. Corporation, V675, P1
   Mok RKP, 2017, IEEE T MULTIMEDIA, V19, P530, DOI 10.1109/TMM.2016.2619901
   Palan S, 2018, J BEHAV EXP FINANC, V17, P22, DOI 10.1016/j.jbef.2017.12.004
   Peer E, 2017, J EXP SOC PSYCHOL, V70, P153, DOI 10.1016/j.jesp.2017.01.006
   Petit J, 2013, J VIS COMMUN IMAGE R, V24, P1020, DOI 10.1016/j.jvcir.2013.06.014
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Prolific, US
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Ribeiro F, 2011, IEEE IMAGE PROC
   Swets J. A., 1999, Med. Decis. Mak., V19, P217, DOI [10.1177/0272989X9901900216, DOI 10.1177/0272989X9901900216]
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Zerman E., 2018, P HUM VIS EL IM, V2018, P1
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang W, 2018, IEEE T CIRC SYST VID, V28, P1462, DOI 10.1109/TCSVT.2017.2650910
NR 42
TC 6
Z9 6
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6013
EP 6025
DI 10.1109/TMM.2022.3203211
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500027
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, YY
   Liu, YM
   Chen, ML
   Wang, ZR
   Yang, WM
   Liao, QM
AF Chen, Yiyun
   Liu, Yunmeng
   Chen, Mingliang
   Wang, Zirui
   Yang, Wenming
   Liao, Qingmin
TI Blind JPEG Compression Artifacts Removal by Integrating Channel
   Regulation With Exit Strategy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Indexes; Transform coding; Regulation; Estimation;
   Automobiles; Image restoration; Compression artifacts removal;
   compression index estimation; adaptive channel regulation; exit strategy
ID QUALITY ASSESSMENT; IMAGE; REDUCTION; NETWORK
AB Compression artifacts removal methods based on convolutional neural networks have attracted great attention. However, most existing methods require a specific trained model for a specific compression quality factor (QF), which inevitably leads to resource-consuming. Unfortunately, the QF is unknown in most practical applications, so it is intractable to choose a suitable model. In this work, we experimentally analyze the relationship between compression index estimation and compression artifacts removal. Based on the connection between them, we couple compression index estimation with compression artifacts removal into a unified network. A network named CRESNet is proposed, working for a wide range of QFs by integrating channel regulation with an exit strategy. Specifically, CRESNet adopts a multi-stage progressive structure with an exit strategy embedded to automatically select the optimal exit stage according to the estimated compression index reflecting the difficulty of the input sample. Benefiting from the exit strategy, CRESNet removes artifacts from slightly compressed images through a simple process while doing an elaborate process for severely compressed images. Furthermore, a compression-information-guided channel regulation (CICR) mechanism is developed to adaptively regulate feature maps based on the estimated compression index. CRESNet achieves a more elegant trade-off between artifacts removal and detail preservation in a resource-efficient manner. Experiments demonstrate that CRESNet achieves state-of-the-art performance.
C1 [Chen, Yiyun; Wang, Zirui; Yang, Wenming; Liao, Qingmin] Tsinghua Univ, Shenzhen Int Grad Sch, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Liu, Yunmeng] Chinese Acad Sci, Shanghai Inst Tech Phys, Shanghai 200083, Peoples R China.
   [Chen, Mingliang] Tencent Video Cloud Tencent, Shenzhen 518057, Peoples R China.
C3 Tsinghua University; Chinese Academy of Sciences; Shanghai Institute of
   Technical Physics, CAS
RP Yang, WM (corresponding author), Tsinghua Univ, Shenzhen Int Grad Sch, Dept Elect Engn, Beijing 100084, Peoples R China.
EM chenyy18@mails.tsinghua.edu.cn; lym_sitp@163.com;
   christmas.chen.34@gmail.com; wangzr18@mails.tsinghua.edu.cn;
   yangelwm@163.com; liaoqm@tsinghua.edu.cn
RI Chen, Mingliang/M-1021-2016
OI Yang, Wenming/0000-0002-2506-1286
FU National Natural Science Foundation of China [62171251]; Natural Science
   Foundation of Guangdong Province [2020A1515010711]; Special Foundations
   for the Development of Strategic Emerging Industries of Shenzhen
   [JCYJ20200109143010272]; Oversea Cooperation Foundation of Tsinghua
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62171251, in part by the Natural Science
   Foundation of Guangdong Province under Grant 2020A1515010711, in part by
   the Special Foundations for the Development of Strategic Emerging
   Industries of Shenzhen under Grant JCYJ20200109143010272, and in part by
   Oversea Cooperation Foundation of Tsinghua.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2016, Journal of WSCG
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chang HB, 2014, IEEE T SIGNAL PROCES, V62, P718, DOI 10.1109/TSP.2013.2290508
   Chen HG, 2018, IEEE COMPUT SOC CONF, P824, DOI 10.1109/CVPRW.2018.00114
   Chen T, 2001, IEEE T CIRC SYST VID, V11, P594, DOI 10.1109/76.920189
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Ehrlich Max, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P293, DOI 10.1007/978-3-030-58598-3_18
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo J, 2017, PROC CVPR IEEE, P4867, DOI 10.1109/CVPR.2017.517
   Guo J, 2016, LECT NOTES COMPUT SC, V9905, P628, DOI 10.1007/978-3-319-46448-0_38
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Hang YC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2562, DOI 10.1145/3394171.3413564
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jiang JX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4977, DOI 10.1109/ICCV48922.2021.00495
   Jin Z, 2021, IEEE T CIRC SYST VID, V31, P467, DOI 10.1109/TCSVT.2020.2982174
   Jingwen He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P53, DOI 10.1007/978-3-030-58565-5_4
   Jung C, 2012, SIGNAL PROCESS-IMAGE, V27, P663, DOI 10.1016/j.image.2012.03.002
   Kim T, 2019, IEEE IMAGE PROC, P3606, DOI [10.1109/icip.2019.8803503, 10.1109/ICIP.2019.8803503]
   Kim Y, 2020, IEEE T CIRC SYST VID, V30, P1121, DOI 10.1109/TCSVT.2019.2901919
   Kim Y, 2020, IEEE ACCESS, V8, P20160, DOI 10.1109/ACCESS.2020.2968944
   Kingma D. P., 2014, arXiv
   Li JW, 2020, IEEE SIGNAL PROC LET, V27, P2134, DOI 10.1109/LSP.2020.3039932
   Li JW, 2020, IEEE T IMAGE PROCESS, V29, P8842, DOI 10.1109/TIP.2020.3020389
   Liu JY, 2020, IEEE T IMAGE PROCESS, V29, P7845, DOI 10.1109/TIP.2020.3007828
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Marcellin M. W., 2000, Proceedings DCC 2000. Data Compression Conference, P523, DOI 10.1109/DCC.2000.838192
   Qunliang Xing, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P275, DOI 10.1007/978-3-030-58517-4_17
   REEVE HC, 1984, OPT ENG, V23, P34, DOI 10.1117/12.7973248
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Si ZJ, 2016, LECT NOTES ELECTR EN, V369, P271, DOI 10.1007/978-981-10-0072-0_35
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wang ML, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P566
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yim C, 2011, IEEE T IMAGE PROCESS, V20, P88, DOI 10.1109/TIP.2010.2061859
   Yu S, 2019, IEEE ACCESS, V7, P138032, DOI 10.1109/ACCESS.2019.2943155
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XS, 2018, IEEE IMAGE PROC, P390, DOI 10.1109/ICIP.2018.8451694
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zheng BL, 2020, IEEE T CIRC SYST VID, V30, P3982, DOI 10.1109/TCSVT.2019.2931045
NR 53
TC 0
Z9 0
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7274
EP 7286
DI 10.1109/TMM.2022.3219679
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000041
DA 2024-07-18
ER

PT J
AU Chen, ZY
   Liu, H
   Zhang, LL
   Liao, X
AF Chen, Zhengyan
   Liu, Hong
   Zhang, Linlin
   Liao, Xin
TI Multi-Dimensional Attention With Similarity Constraint for
   Weakly-Supervised Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Location awareness; Feature extraction; Proposals; Optical flow;
   Task analysis; Annotations; Multi-dimensional attention; temporal action
   localization; video analysis; weakly supervised learning
AB Weakly-supervised temporal action localization (WTAL) is a challenging task in understanding untrimmed videos, in which no frame-wise annotation is provided during training, only the video-level category label is available. Current methods mainly adopt temporal attention branches to conduct foreground-background separation with RGB and optical flow features simply concatenated, regardless of the discriminative spacial features and the complementarity between different modalities. In this work, we propose a Multi-Dimensional Attention (MDA) method to explore attention mechanism across three dimensions in weakly supervised action localization, i.e., 1) temporal attention that focuses on segments containing action instances, 2) channel attention that discovers the most relevant cues for action description, and 3) modal attention that fuses RGB and flow information adaptively based on feature magnitudes during background modeling. In addition, we introduce a similarity constraint loss to refine the action segment representation in feature space, which helps the network to detect less discriminative frames of an action to capture the full action boundaries. The proposed MDA with similarity constraints can be easily applied to existing action detection frameworks with few parameters. Extensive experiments on THUMOS'14 and ActivityNet v1.2 datasets show that the proposed method outperforms the current state-of-the-art WTAL approaches, and achieves comparable results with some advanced fully-supervised methods.
C1 [Chen, Zhengyan; Liu, Hong; Zhang, Linlin] Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing 100871, Peoples R China.
   [Liao, Xin] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
C3 Peking University; Hunan University
RP Liu, H (corresponding author), Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing 100871, Peoples R China.
EM chenzhengyan@pku.edu.cn; hongliu@pku.edu.cn; catherinezll@pku.edu.cn;
   xinliao@hnu.edu.cn
RI Liao, Xin/ITT-1021-2023; Zhang, Linlin/AAV-7748-2021
OI Liao, Xin/0000-0002-9131-0578; Zhang, Linlin/0000-0003-4553-448X; Liu,
   Hong/0000-0002-7498-6541
FU National Natural Science Foundation of China [62073004, 61972142];
   Shenzhen Fundamental Research Program [JCYJ20200109140410340,
   GXWD20201231165807007-20200807164903001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62073004 and 61972142, and in part by
   Shenzhen Fundamental Research Program under Grants JCYJ20200109140410340
   and GXWD20201231165807007-20200807164903001.
CR Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen Y., 2020, ARXIV200500754
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Huang LJ, 2020, AAAI CONF ARTIF INTE, V34, P11053
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Islam A, 2021, AAAI CONF ARTIF INTE, V35, P1637
   Kingma D. P., 2014, arXiv
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu HJ, 2020, IEEE T MULTIMEDIA, V22, P337, DOI 10.1109/TMM.2019.2929923
   Liu K, 2018, AAAI CONF ARTIF INTE, P7138
   Liu Y, 2021, PROC CVPR IEEE, P6172, DOI 10.1109/CVPR46437.2021.00611
   Liu YF, 2019, PROC CVPR IEEE, P7099, DOI [10.1109/CVPR.2019.00726, 10.1109/CVPR.2019.00372]
   Liu ZY, 2021, AAAI CONF ARTIF INTE, V35, P2233
   Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Luo W, 2021, PROC CVPR IEEE, P9964, DOI 10.1109/CVPR46437.2021.00984
   Moniruzzaman M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2166, DOI 10.1145/3394171.3413687
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Paszke A, 2019, ADV NEUR IN, V32
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Su HS, 2021, AAAI CONF ARTIF INTE, V35, P2602
   Su HS, 2021, IEEE T MULTIMEDIA, V23, P1503, DOI 10.1109/TMM.2020.2999184
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vaswani A, 2017, ADV NEUR IN, V30
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu M., 2020, CVPR, P10156
   Xu YL, 2019, AAAI CONF ARTIF INTE, P9070
   Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562
   Yuan Y., 2019, P INT C LEARN REPR
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhekun Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P729, DOI 10.1007/978-3-030-58526-6_43
NR 69
TC 2
Z9 2
U1 7
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4349
EP 4360
DI 10.1109/TMM.2022.3174344
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200020
DA 2024-07-18
ER

PT J
AU Feng, YJ
   Ji, YM
   Wu, F
   Gao, GW
   Gao, Y
   Liu, TL
   Liu, SD
   Jing, XY
   Luo, JB
AF Feng, Yujian
   Ji, Yimu
   Wu, Fei
   Gao, Guangwei
   Gao, Yang
   Liu, Tianliang
   Liu, Shangdong
   Jing, Xiao-Yuan
   Luo, Jiebo
TI Occluded Visible-Infrared Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Cameras; Transformers; Task analysis; Semantics;
   Probes; Visualization; Occluded Visible-infrared Person
   Re-identification; Visual Transformer; Co-attention mechanism; Occluded
   VI-ReID datasets
AB Visible-infrared person re-identification (VI-ReID) aims to match person images between the visible and near-infrared modalities. Previous VI-ReID methods are based on holistic pedestrian images and achieve excellent performance. However, in real-world scenarios, images captured by visible and near-infrared cameras usually contain occlusions. The performance of these methods degrades significantly due to the loss of information of discriminative features from the occlusion of the images. We define visible-infrared person re-identification in this occlusion scene as Occluded VI-ReID, where only partial content information of pedestrian images can be used to match images of different modalities from different cameras. In this paper, we propose a matching framework for occlusion scenes, which contains a local feature enhance module (LFEM) and a modality information fusion module (MIFM). LFEM adopts Transformer to learn features of each modality, and adjusts the importance of patches to enhance the representation ability of local features of the non-occluded areas. MIFM utilizes a co-attention mechanism to infer the correlation between each image for reducing the difference between modalities. We construct two occluded VI-ReID datasets, namely Occluded-SYSU-MM01 and Occluded-RegDB datasets. Our approach outperforms existing state-of-the-art methods on two occlusion datasets, while remains top performance on two holistic datasets.
C1 [Feng, Yujian] Nanjing Univ Posts & Telecommun, Sch Internet Things, Nanjing 210003, Peoples R China.
   [Ji, Yimu; Liu, Shangdong] NJUPT, Sch Comp Sci & Technol, Nanjing 210003, Peoples R China.
   [Wu, Fei; Gao, Guangwei; Gao, Yang] NJUPT, Coll Automat, Nanjing 210023, Peoples R China.
   [Liu, Tianliang] NJUPT, Sch Commun & Informat Engn, Nanjing 210003, Peoples R China.
   [Jing, Xiao-Yuan] Guangdong Univ Petrochem Technol, Guangdong Prov Key Lab Petrochem Equipment Fault D, Maoming 525000, Peoples R China.
   [Jing, Xiao-Yuan] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester 14627, NY USA.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications; Nanjing University of Posts &
   Telecommunications; Nanjing University of Posts & Telecommunications;
   Guangdong University of Petrochemical Technology; Wuhan University;
   University of Rochester
RP Ji, YM (corresponding author), NJUPT, Sch Comp Sci & Technol, Nanjing 210003, Peoples R China.; Wu, F; Gao, GW (corresponding author), NJUPT, Coll Automat, Nanjing 210023, Peoples R China.
EM fengyujian_904@163.com; jiym@njupt.edu.cn; wufei_8888@126.com;
   csggao@gmail.com; gy731222@163.com; liutl@njupt.edu.cn;
   lsd@njupt.edu.cn; jingxy_2000@126.com; jiebo.luo@gmail.com
RI Luo, Jiebo/AAI-7549-2020; He, Chen/JLM-5059-2023; YE, Chen/KFR-3858-2024
OI Luo, Jiebo/0000-0002-4516-9729; Liu, Tianliang/0000-0001-9800-1981; Wu,
   Fei/0000-0001-5498-4947; Liu, Shangdong/0000-0002-8511-7544; Feng,
   Yujian/0000-0003-1051-7217; ji, yimu/0000-0001-7019-3942
FU National Natural Science Foundation of China [61902194, 62076139,
   62176069, 61933013, 61972212]; National Key Research and Development
   Program of China [2018AAA0103302]; Natural Science Foundation of Jiangsu
   Province (Higher Education Institutions) [BK20170900, 19KJB520046,
   20KJA520001, BK20190089]; 14th Five-Year Plan Project of Equipment
   Development Department [315107402]; Innovative and Entrepreneurial
   talents projects of Jiangsu Province; Jiangsu Planned Projects for
   Post-doctoral Research Funds [2019K024]; Postgraduate Research and
   Practice Innovation Program of Jiangsu Province [KYCX19_0921,
   KYCX19_0906]; Open Research Project of Zhejiang Lab [2021KF0AB05];
   Future Network Scientific Research Fund Project [FNSRFP-2021-YB-15];
   1311 Talent Program of Nanjing University of Posts and
   Telecommunications
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61902194, 62076139, 62176069, 61933013,
   and 61972212, in part by the National Key Research and Development
   Program of China under Grant 2018AAA0103302, in part by the Natural
   Science Foundation of Jiangsu Province (Higher Education Institutions)
   under Grants BK20170900, 19KJB520046, and 20KJA520001, and BK20190089,
   in part by the 14th Five-Year Plan Project of Equipment Development
   Department under Grant 315107402, in part by the Innovative and
   Entrepreneurial talents projects of Jiangsu Province, in part by the
   Jiangsu Planned Projects for Post-doctoral Research Funds under Grant
   2019K024, in part by the Postgraduate Research and Practice Innovation
   Program of Jiangsu Province under Grants KYCX19_0921 and KYCX19_0906, in
   part by the Open Research Project of Zhejiang Lab under Grant
   2021KF0AB05, in part by Future Network Scientific Research Fund Project
   under Grant FNSRFP-2021-YB-15, and in part by the 1311 Talent Program of
   Nanjing University of Posts and Telecommunications.
CR Bhojanapalli S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10211, DOI 10.1109/ICCV48922.2021.01007
   Cao ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15437, DOI 10.1109/ICCV48922.2021.01517
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Fu CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11803, DOI 10.1109/ICCV48922.2021.01161
   Gao LS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3771, DOI 10.1145/3394171.3413833
   Gildenblat J., 2021, PyTorch Library for CAM Methods
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   He TY, 2021, PROC CVPR IEEE, P9101, DOI 10.1109/CVPR46437.2021.00899
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Hu RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1419, DOI 10.1109/ICCV48922.2021.00147
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Huang Y, 2022, IEEE T MULTIMEDIA, V24, P1570, DOI 10.1109/TMM.2021.3067760
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jia MX, 2021, AAAI CONF ARTIF INTE, V35, P1673
   Li H, 2022, IEEE T CIRC SYST VID, V32, P1624, DOI 10.1109/TCSVT.2021.3073718
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Miao JX, 2022, IEEE T NEUR NET LEAR, V33, P4624, DOI 10.1109/TNNLS.2021.3059515
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Shen D, 2021, IEEE T IMAGE PROCESS, V30, P1676, DOI 10.1109/TIP.2020.3046904
   Sun MJ, 2021, IEEE T PATTERN ANAL, V43, P4189, DOI 10.1109/TPAMI.2021.3058684
   Sun ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3591, DOI 10.1109/ICCV48922.2021.00359
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang PY, 2021, IEEE T MULTIMEDIA, V23, P1474, DOI 10.1109/TMM.2020.2999180
   Wu AC, 2020, INT J COMPUT VISION, V128, P1765, DOI 10.1007/s11263-019-01290-1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Yan C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11855, DOI 10.1109/ICCV48922.2021.01166
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Ye HR, 2021, IEEE T IMAGE PROCESS, V30, P1583, DOI 10.1109/TIP.2020.3045261
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zhao ZW, 2021, AAAI CONF ARTIF INTE, V35, P3520
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhuo JX, 2019, Arxiv, DOI arXiv:1907.03253
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 59
TC 3
Z9 3
U1 5
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1401
EP 1413
DI 10.1109/TMM.2022.3229969
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000001
DA 2024-07-18
ER

PT J
AU Fu, LL
   Chen, ZL
   Chen, YY
   Wang, SP
AF Fu, Lele
   Chen, Zhaoliang
   Chen, Yongyong
   Wang, Shiping
TI Unified Low-Rank Tensor Learning and Spectral Embedding for Multi-View
   Subspace Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-view subspace clustering; spectral embedding; low-rank tensor;
   t-SVD
ID ALGORITHM; NORM
AB Multi-view subspace clustering aims to utilize the comprehensive information of multi-source features to aggregate data into multiple subspaces. Recently, low-rank tensor learning has been applied to multi-view subspace clustering, which explores high-order correlations of multi-view data and has achieved remarkable results. However, these existing methods have certain limitations: 1) The learning processes of low-rank tensor and label indicator matrix are independent. 2) Variable contributions of different views to the consistent clustering results are not discriminated. To handle these issues, we propose a unified framework that integrates low-rank tensor learning and spectral embedding (ULTLSE) for multi-view subspace clustering. Specifically, the proposed model adopts the tensor singular value decomposition (t-SVD) based tensor nuclear norm to encode the low-rank property of the self-representation tensor, and a label indicator matrix via spectral embedding is simultaneously exploited. To distinguish the importance of various views, we learn a quantifiable weighting coefficient for each view. An effective recursion optimization algorithm is also developed to address the proposed model. Finally, we conduct comprehensive experiments on eight real-world datasets with three categories. The experimental results indicate that the proposed ULTLSE is advanced over existing state-of-the-art clustering methods.
C1 [Fu, Lele; Chen, Zhaoliang; Wang, Shiping] Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350116, Peoples R China.
   [Chen, Zhaoliang] Fuzhou Univ, Key Lab Network Comp & Intelligent Informat Proc, Fuzhou 350116, Peoples R China.
   [Chen, Yongyong] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
C3 Fuzhou University; Fuzhou University; Harbin Institute of Technology
RP Wang, SP (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350116, Peoples R China.
EM lawrencefzu@gmail.com; chenzl23@outlook.com; yongyongchen.cn@gmail.com;
   shipingwangphd@163.com
RI Fu, Lele/GLV-0220-2022; Chen, Zhaoliang/KGL-0282-2024; Chen,
   yongyong/P-3801-2016
OI Chen, Zhaoliang/0000-0002-7832-908X; Chen, yongyong/0000-0003-1970-1993
FU National Natural Science Foundation of China [U21A20472, 62106063];
   Shenzhen College Stability Support Plan
   [GXWD20201230155427003-20200824113231001]
FX This work was partially supported by the National Natural Science
   Foundation of China under Grants U21A20472 and 62106063, and in part by
   the Shenzhen College Stability Support Plan under Grant
   GXWD20201230155427003-20200824113231001.
CR Cai JY, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108386
   Chang SY, 2014, IEEE DATA MINING, P60, DOI 10.1109/ICDM.2014.115
   Chao GQ, 2019, INFORM SCIENCES, V494, P278, DOI 10.1016/j.ins.2019.04.039
   Chaudhuri K., 2009, P 26 ANN INT C MACH, P129
   Chen MS, 2020, AAAI CONF ARTIF INTE, V34, P3513
   Chen YY, 2022, IEEE T CIRC SYST VID, V32, P92, DOI 10.1109/TCSVT.2021.3055625
   Chen YY, 2021, IEEE T IMAGE PROCESS, V30, P4022, DOI 10.1109/TIP.2021.3068646
   Chen YY, 2022, IEEE T NEUR NET LEAR, V33, P4712, DOI 10.1109/TNNLS.2021.3059874
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Cheng MM, 2019, IEEE T IMAGE PROCESS, V28, P2399, DOI 10.1109/TIP.2018.2877937
   Djelouah A, 2015, IEEE T PATTERN ANAL, V37, P1890, DOI 10.1109/TPAMI.2014.2385704
   Djelouah A, 2013, IEEE I CONF COMP VIS, P2640, DOI 10.1109/ICCV.2013.328
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   El Hajjar S, 2022, INFORM FUSION, V78, P209, DOI 10.1016/j.inffus.2021.09.009
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fu Lele, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME51207.2021.9428291
   Fu LL, 2020, NEUROCOMPUTING, V402, P148, DOI 10.1016/j.neucom.2020.02.104
   Gao QX, 2020, AAAI CONF ARTIF INTE, V34, P3930
   Houthuys L, 2018, INFORM FUSION, V44, P46, DOI 10.1016/j.inffus.2017.12.002
   Hu WR, 2017, IEEE T NEUR NET LEAR, V28, P2961, DOI 10.1109/TNNLS.2016.2611525
   Hu ZX, 2020, INFORM FUSION, V55, P251, DOI 10.1016/j.inffus.2019.09.005
   Jiangwen Sun, 2013, 2013 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P316, DOI 10.1109/BIBM.2013.6732509
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kim YM, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P821
   Kriegel HP, 2012, WIRES DATA MIN KNOWL, V2, P351, DOI 10.1002/widm.1057
   Kumar P., 2011, Adv. Neural Inf. Process. Syst., P1413, DOI DOI 10.5555/2986459.2986617
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Lu CY, 2016, PROC CVPR IEEE, P5249, DOI 10.1109/CVPR.2016.567
   Luo SR, 2018, AAAI CONF ARTIF INTE, P3730
   Luo Y, 2015, IEEE T KNOWL DATA EN, V27, P3111, DOI 10.1109/TKDE.2015.2445757
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Parsons L., 2004, SIGKDD Explor Newsl, V6, P90, DOI 10.1145/1007730.1007731
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Semerci O, 2014, IEEE T IMAGE PROCESS, V23, P1678, DOI 10.1109/TIP.2014.2305840
   Shu XB, 2018, IEEE T CIRC SYST VID, V28, P454, DOI 10.1109/TCSVT.2016.2607345
   Tan JP, 2021, IEEE T MULTIMEDIA, V23, P2943, DOI 10.1109/TMM.2020.3019683
   Tang C, 2022, IEEE T KNOWL DATA EN, V34, P4705, DOI 10.1109/TKDE.2020.3048678
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P5924
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tang C, 2018, KNOWL-BASED SYST, V160, P49, DOI 10.1016/j.knosys.2018.06.016
   Tang JH, 2019, IEEE T PATTERN ANAL, V41, P2027, DOI 10.1109/TPAMI.2019.2906603
   Tang JH, 2017, IEEE T PATTERN ANAL, V39, P1662, DOI 10.1109/TPAMI.2016.2608882
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Wang QQ, 2021, IEEE T MULTIMEDIA, V23, P3483, DOI 10.1109/TMM.2020.3025666
   Wang SP, 2017, IEEE T MULTIMEDIA, V19, P1454, DOI 10.1109/TMM.2017.2663324
   Wang XB, 2019, PATTERN RECOGN, V88, P50, DOI 10.1016/j.patcog.2018.09.009
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2018, NEURAL NETWORKS, V103, P1, DOI 10.1016/j.neunet.2018.03.006
   Wang Y, 2017, IEEE T NEUR NET LEAR, V28, P57, DOI 10.1109/TNNLS.2015.2498149
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Wu L, 2017, IMAGE VISION COMPUT, V57, P58, DOI 10.1016/j.imavis.2016.11.008
   Xiaobo Wang, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P1, DOI 10.1109/CVPR.2017.8
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Yang ZY, 2019, IEEE T IMAGE PROCESS, V28, P5147, DOI 10.1109/TIP.2019.2913096
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang XB, 2021, COMPUT METH PROG BIO, V199, DOI 10.1016/j.cmpb.2020.105895
   Zhang XQ, 2019, INFORM SCIENCES, V477, P430, DOI 10.1016/j.ins.2018.10.049
NR 62
TC 14
Z9 14
U1 6
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4972
EP 4985
DI 10.1109/TMM.2022.3185886
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300027
DA 2024-07-18
ER

PT J
AU Ji, Z
   Hu, JH
   Liu, DY
   Wu, LYB
   Zhao, Y
AF Ji, Zhong
   Hu, Junhua
   Liu, Deyin
   Wu, Lin Yuanbo
   Zhao, Ye
TI Asymmetric Cross-Scale Alignment for Text-Based Person Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Text-based person search; Transformer; Crossmodal matching.
AB Text-based person search (TBPS) is of significant importance in intelligent surveillance, which aims to retrieve pedestrian images with high semantic relevance to a given text description. This retrieval task is characterized with both modal heterogeneity and fine-grained matching. To implement this task, one needs to extract multi-scale features from both image and text domains, and then perform the cross-modal alignment. However, most existing approaches only consider the alignment confined at their individual scales, e.g., an image-sentence or a region-phrase scale. Such a strategy adopts the presumable alignment in feature extraction, while overlooking the cross-scale alignment, e.g., image-phrase. In this paper, we present a transformer-based model to extract multi-scale representations, and perform Asymmetric Cross-Scale Alignment (ACSA) to precisely align the two modalities. Specifically, ACSA consists of a global-level alignment module and an asymmetric cross-attention module, where the former aligns an image and texts on a global scale, and the latter applies the cross-attention mechanism to dynamically align the cross-modal entities in region/image-phrase scales. Extensive experiments on two benchmark datasets CUHK-PEDES and RSTPReid demonstrate the effectiveness of our approach.
C1 [Ji, Zhong; Hu, Junhua] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Liu, Deyin] Anhui Univ, Sch Artificial Intelligence, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230039, Peoples R China.
   [Wu, Lin Yuanbo] Swansea Univ, Dept Comp Sci, Swansea SA1 8EN, W Glam, Wales.
   [Zhao, Ye] Hefei Univ Technol, TheKey Lab Knowledge Engn Big Data, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
C3 Tianjin University; Anhui University; Swansea University; Hefei
   University of Technology
RP Wu, LYB (corresponding author), Swansea Univ, Dept Comp Sci, Swansea SA1 8EN, W Glam, Wales.
EM jizhong@tju.edu.cn; hujunhua@tju.edu.cn; iedyzzu@outlook.com;
   jolin.lwu@gmail.com; zhaoye@hfut.edu.cn
OI Ji, Zhong/0000-0002-2197-3739; Zhao, Ye/0000-0002-8180-4697; Liu,
   Deyin/0000-0002-0371-9921
FU National Natural Science Foundation of China [62176178, U19A2073,
   62002096]; Natural Science Foundation of Tianjin [19JCYBJC16000]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62176178, U19A2073, and 62002096, and
   in part by the Natural Science Foundation of Tianjin under Grant
   19JCYBJC16000.
CR Aggarwal S, 2020, IEEE WINT CONF APPL, P2606, DOI [10.1109/WACV45572.2020.9093640, 10.1109/wacv45572.2020.9093640]
   Bird S., 2004, P ASS COMP LING, P1
   Brown T., 2020, P ADV NEUR INF PROC, V33, P1877
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen DP, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3291, DOI 10.1145/3503161.3548195
   Chen DP, 2018, LECT NOTES COMPUT SC, V11220, P56, DOI 10.1007/978-3-030-01270-0_4
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen TL, 2018, IEEE WINT CONF APPL, P1879, DOI 10.1109/WACV.2018.00208
   Chen YH, 2022, NEUROCOMPUTING, V494, P171, DOI 10.1016/j.neucom.2022.04.081
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding ZF, 2021, Arxiv, DOI arXiv:2107.12666
   Dosovitskiy A., 2021, INT C LEARN REPRESEN, P1
   Fan HN, 2022, IEEE T MULTIMEDIA, V24, P49, DOI 10.1109/TMM.2020.3045286
   Farooq A., 2021, arXiv
   Gao CY, 2021, Arxiv, DOI arXiv:2101.03036
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Ji Z, 2020, IEEE INTERNET THINGS, V7, P11147, DOI 10.1109/JIOT.2020.2995148
   Ji Z, 2020, PATTERN RECOGN LETT, V138, P170, DOI 10.1016/j.patrec.2020.07.018
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Kingma D, 2014, ICLR P, V2014, P1
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li H, 2022, IEEE T CIRC SYST VID, V32, P1624, DOI 10.1109/TCSVT.2021.3073718
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu DY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3522714
   Liu DY, 2023, IEEE T NEUR NET LEAR, V34, P8589, DOI 10.1109/TNNLS.2022.3151631
   Liu YH, 2019, Arxiv, DOI arXiv:1907.11692
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Shi YX, 2021, IEEE T MULTIMEDIA, V23, P4376, DOI 10.1109/TMM.2020.3042068
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C., 2021, P 30 INT JOINT C ART, P1068
   Wang Y., 2020, ACM Trans. Multimedia Comput. Commun. Appl., V17, P1
   Wang YY, 2019, INT CONF ACOUST SPEE, P2057, DOI 10.1109/ICASSP.2019.8682456
   Wei XS, 2019, LECT NOTES COMPUT SC, V11362, P575, DOI 10.1007/978-3-030-20890-5_37
   Wu L., 2022, P 31 INT JOINT C ART, P1465, DOI [10.24963/ijcai.2022/204, DOI 10.24963/IJCAI.2022/204]
   Wu L, 2022, IEEE T IMAGE PROCESS, V31, P4803, DOI 10.1109/TIP.2022.3186746
   Wu L, 2021, IEEE T NEUR NET LEAR, V32, P722, DOI 10.1109/TNNLS.2020.2979190
   Wu L, 2020, IEEE T CIRC SYST VID, V30, P2081, DOI 10.1109/TCSVT.2019.2909549
   Wu L, 2020, IEEE T IMAGE PROCESS, V29, P1233, DOI 10.1109/TIP.2019.2940684
   Wu L, 2018, PATTERN RECOGN, V76, P727, DOI 10.1016/j.patcog.2017.10.004
   Yang J., 2021, arXiv, DOI DOI 10.48550/ARXIV.2107.00641
   Yang ZL, 2019, ADV NEUR IN, V32
   Zhang Y., 2018, P EUR C COMP VIS ECC, P686
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3441, DOI 10.1145/3394171.3413864
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
NR 53
TC 14
Z9 14
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7699
EP 7709
DI 10.1109/TMM.2022.3225754
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jin, L
   Wang, XJ
   Nie, XC
   Liu, LQ
   Guo, YD
   Zhao, J
AF Jin, Lei
   Wang, Xiaojuan
   Nie, Xuecheng
   Liu, Luoqi
   Guo, Yandong
   Zhao, Jian
TI Grouping by Center: Predicting Centripetal Offsets for the Bottom-up
   Human Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bottom-up; centripetal offset; grouping methods; human pose estimation
AB We introduce Grouping by Center, a novel grouping approach for the bottom-up human pose estimation, which detects human joint first and then does grouping. The grouping strategy is the critical factor for the bottom-up pose estimation. To increase the conciseness and accuracy, we propose to use the center of the body as a grouping clue. More concretely, we predict the offsets from the keypoints to the body centers. Keypoints with aligned shifted results will be grouped as one person. However, the multi-scale variance of people can affect the prediction of the grouping clue, which has been neglected in previous research. To resolve the scale variance of the offset, we put forward a Multi-scale Translation Layer and an iterative refinement. Furthermore, we scheme a greedy grouping strategy with a dynamic threshold due to the various scales of instances. Through a comprehensive comparison, our framework is validated to be effective and practical. We also lay out the state-of-the-art performance revolving the bottom-up multi-person pose estimation on the MS-COCO dataset and the CrowdPose dataset.
C1 [Jin, Lei] Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100876, Peoples R China.
   [Wang, Xiaojuan] Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing 100876, Peoples R China.
   [Nie, Xuecheng; Liu, Luoqi] Meitu Inc, MT Lab, Xiamen, Peoples R China.
   [Nie, Xuecheng] YITU Technol, Beijing 201103, Peoples R China.
   [Liu, Luoqi] Qihoo 360 Artificial Intelligence Inst, Beijing 100015, Peoples R China.
   [Guo, Yandong] Intelligent Percept, Shanghai 201203, Peoples R China.
   [Zhao, Jian] Inst North Elect Equipment, Beijing 100070, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications
RP Wang, XJ (corresponding author), Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing 100876, Peoples R China.; Zhao, J (corresponding author), Inst North Elect Equipment, Beijing 100070, Peoples R China.
EM jinlei@bupt.edu.cn; wj2718@bupt.edu.cn; niexuecheng@u.nus.edu;
   llq5@meitu.com; guoyandong@oppo.com; zhaojian90@u.nus.edu
OI Jin, Lei/0000-0003-4855-2464; Nie, Xuecheng/0000-0003-2433-5983; Zhao,
   Jian/0000-0002-3508-756X
FU National Nature Fund [62102039, 62071056, 62006244]; Young Elite
   Scientist Sponsorship Program of China Association for Science and
   Technology [YESS20200140]
FX This work was supported in part by the National Nature Fund under Grants
   62102039, 62071056, and 62006244, and in part by the Young Elite
   Scientist Sponsorship Program of China Association for Science and
   Technology YESS20200140.& nbsp;
CR Arnab A, 2017, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2017.100
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hidalgo G, 2019, IEEE I CONF COMP VIS, P6981, DOI 10.1109/ICCV.2019.00708
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Insafutdinov E, 2017, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2017.142
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Jin S., 2020, ECCV, P718
   Kingma D. P., 2014, arXiv
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Li J, 2020, AAAI CONF ARTIF INTE, V34, P11354
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HL, 2016, IEEE T MULTIMEDIA, V18, P1233, DOI 10.1109/TMM.2016.2556859
   Liu W., 2018, P NAT C ART INT
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie X., 2017, P EUR C COMP VIS
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P705, DOI 10.1007/978-3-030-01228-1_42
   Nie XC, 2019, IEEE I CONF COMP VIS, P6950, DOI 10.1109/ICCV.2019.00705
   Nie XC, 2018, PROC CVPR IEEE, P2100, DOI 10.1109/CVPR.2018.00224
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun K, 2020, Arxiv, DOI arXiv:2006.15480
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tian Z, 2019, Arxiv, DOI arXiv:1911.07451
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiao Yabo, 2022, P AAAI C ART INT
   Yu F., 2015, ARXIV
   Zhiwei Dong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10516, DOI 10.1109/CVPR42600.2020.01053
   Zhou X., 2019, PROC IEEE C COMPUT V
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
NR 46
TC 9
Z9 9
U1 5
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3364
EP 3374
DI 10.1109/TMM.2022.3159111
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200032
DA 2024-07-18
ER

PT J
AU Ju, C
   Zhao, PS
   Chen, SH
   Zhang, Y
   Zhang, XY
   Wang, YF
   Tian, Q
AF Ju, Chen
   Zhao, Peisen
   Chen, Siheng
   Zhang, Ya
   Zhang, Xiaoyun
   Wang, Yanfeng
   Tian, Qi
TI Adaptive Mutual Supervision for Weakly-Supervised Temporal Action
   Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Location awareness; Videos; Proposals; Task analysis; Annotations;
   Adaptive systems; Optimization; Temporal action localization; weak
   supervision; adaptive sampling strategy; mutual location supervision
AB Weakly-supervised temporal action localization aims to localize actions from untrimmed long videos with only video-level category labels. Most previous methods ignore the incompleteness issue of Class Activation Sequences (CAS), suffering from trivial detection results. To tackle this issue, we propose a novel Adaptive Mutual Supervision (AMS) framework with two branches, where the base branch detects the most discriminative action regions, while the supplementary branch localizes the less discriminative action regions through an adaptive sampler. The sampler dynamically updates the inputs for the supplementary branch using a sampling weight sequence negatively correlated with the CAS from the base branch, thus encouraging the supplementary branch to localize the action regions underestimated by the base branch. To promote mutual enhancement between two branches, we further construct mutual location supervision. Each branch adopts the location pseudo-labels generated from the other branch as the localization supervision. By alternately optimizing two branches for multiple iterations, we progressively complete action regions. Extensive experiments on THUMOS14 and ActivityNet1.2 demonstrate that the proposed AMS method significantly outperforms state-of-the-art methods.
C1 [Ju, Chen; Chen, Siheng; Zhang, Ya; Zhang, Xiaoyun; Wang, Yanfeng] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Zhao, Peisen; Tian, Qi] Huawei Cloud & AI, Shenzhen 518129, Guangdong, Peoples R China.
C3 Shanghai Jiao Tong University; Huawei Technologies
RP Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM ju_chen@sjtu.edu.cn; pszhao@sjtu.edu.cn; sihengc@sjtu.edu.cn;
   ya_zhang@sjtu.edu.cn; xiaoyun.zhang@sjtu.edu.cn;
   wangyanfeng@sjtu.edu.cn; qitian@cs.utsa.edu
RI wang, yi/HOF-6668-2023
OI Zhang, Ya/0000-0002-5390-9053; Ju, Chen/0000-0001-8472-7677
FU National Key Research and Development Program of China [2020YFB1406801];
   111 plan [BP0719010]; STCSM [18DZ2270700]; State Key Laboratory of UHD
   Video and Audio Production and Presentation
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2020YFB1406801, in part by 111 plan under
   Grant BP0719010, in part by STCSM under Grant 18DZ2270700, and in part
   by the State Key Laboratory of UHD Video and Audio Production and
   Presentation. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xinxiao Wu.
CR Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen G, 2021, IEEE T MULTIMEDIA, V23, P2672, DOI 10.1109/TMM.2020.3014555
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Devroye L., 1986, 1986 Winter Simulation Conference Proceedings, P260, DOI 10.1145/318242.318443
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao JL, 2020, AAAI CONF ARTIF INTE, V34, P10810
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Hong FT, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1591, DOI 10.1145/3474085.3475298
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Ju C., 2022, PROC EUR C COMPUT VI, P1
   Ju C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13435, DOI 10.1109/ICCV48922.2021.01320
   Kahatapitiya K, 2021, PROC CVPR IEEE, P8381, DOI 10.1109/CVPR46437.2021.00828
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Kingma D.P., 2014, ARXIV14126980
   Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633
   Lee P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13628, DOI 10.1109/ICCV48922.2021.01339
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu HJ, 2020, IEEE T MULTIMEDIA, V22, P337, DOI 10.1109/TMM.2019.2929923
   Liu QY, 2020, AAAI CONF ARTIF INTE, V34, P11612
   Liu YF, 2019, PROC CVPR IEEE, P7099, DOI [10.1109/CVPR.2019.00726, 10.1109/CVPR.2019.00372]
   Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Ma F, 2020, J MACH LEARN RES, V21
   Mengmeng Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10153, DOI 10.1109/CVPR42600.2020.01017
   Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Pardo A, 2021, IEEE WINT CONF APPL, P3318, DOI 10.1109/WACV48630.2021.00336
   Paszke Adam, 2017, NIPS W
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Polikar R, 2012, ENSEMBLE MACHINE LEARNING: METHODS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-9326-7_1
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Recasens A, 2018, LECT NOTES COMPUT SC, V11213, P52, DOI 10.1007/978-3-030-01240-3_4
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Su H., 2018, P AS C COMP VIS, P558
   Su HS, 2021, IEEE T MULTIMEDIA, V23, P1503, DOI 10.1109/TMM.2020.2999184
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wedel A, 2009, LECT NOTES COMPUT SC, V5604, P23, DOI 10.1007/978-3-642-03061-1_2
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu YL, 2019, AAAI CONF ARTIF INTE, P9070
   Yang L, 2021, IEEE Trans. Pattern Anal. Mach. Intell., V1, P1
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Yueran Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P121, DOI 10.1007/978-3-030-58604-1_8
   Zeng RH, 2022, IEEE T PATTERN ANAL, V44, P6209, DOI 10.1109/TPAMI.2021.3090167
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108
   Zhang CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P738, DOI 10.1145/3343031.3351044
   Zhao PS, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108741
   Zhao PS, 2022, IEEE SIGNAL PROC LET, V29, P194, DOI 10.1109/LSP.2021.3132287
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhekun Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P729, DOI 10.1007/978-3-030-58526-6_43
   Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou Y, 2021, IEEE T MULTIMEDIA, V23, P4363, DOI 10.1109/TMM.2020.3042077
NR 79
TC 2
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6688
EP 6701
DI 10.1109/TMM.2022.3213478
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500076
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kim, NJ
   Kim, H
AF Kim, Nam Joon
   Kim, Hyun
TI FP-AGL: Filter Pruning With Adaptive Gradient Learning for Accelerating
   Deep Convolutional Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive gradient learning; convolutional neural networks; filter
   pruning; light-weight technique; taylor expansion
ID CNN
AB Filter pruning is a technique that reduces computational complexity, inference time, and memory footprint by removing unnecessary filters in convolutional neural networks (CNNs) with an acceptable drop in accuracy, consequently accelerating the network. Unlike traditional filter pruning methods utilizing zeroing-out filters, we propose two techniques to achieve the effect of pruning more filters with less performance degradation, inspired by the existing research on centripetal stochastic gradient descent (C-SGD), wherein the filters are removed only when the ones that need to be pruned have the same value. First, to minimize the negative effect of centripetal vectors that gradually make filters come closer to each other, we redesign the vectors by considering the effect of each vector on the loss-function using the Taylor-based method. Second, we propose an adaptive gradient learning (AGL) technique that updates weights while adaptively changing the gradients. Through AGL, performance degradation can be mitigated because some gradients maintain their original direction, and AGL also minimizes the accuracy loss by perfectly converging the filters, which require pruning, to a single point. Finally, we demonstrate the superiority of the proposed method on various datasets and networks. In particular, on the ILSVRC-2012 dataset, our method removed 52.09% FLOPs with a negligible 0.15% top-1 accuracy drop on ResNet-50. As a result, we achieve the most outstanding performance compared to those reported in previous studies in terms of the trade-off between accuracy and computational complexity.
C1 [Kim, Nam Joon; Kim, Hyun] Seoul Natl Univ Sci & Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.
   [Kim, Nam Joon; Kim, Hyun] Seoul Natl Univ Sci & Technol, Res Ctr Elect & Informat Technol, Seoul 01811, South Korea.
C3 Seoul National University of Science & Technology; Seoul National
   University of Science & Technology
RP Kim, H (corresponding author), Seoul Natl Univ Sci & Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.; Kim, H (corresponding author), Seoul Natl Univ Sci & Technol, Res Ctr Elect & Informat Technol, Seoul 01811, South Korea.
EM rlarla2626@seoultech.ac.kr; hyunkim@seoultech.ac.kr
FU Industrial Fundamental Technology Development Program [20019367];
   Development of Low Power AI Architecture for AIoT; Ministry of Trade,
   Industry & Energy of Korea; Basic Science Research Program through the
   National Research Foundation of Korea; Ministry of Education
   [NRF-2019R1A6A1A03032119]
FX This work was supported in part by the Industrial Fundamental Technology
   Development Program under Grant 20019367, in part by the Development of
   Low Power AI Architecture for AIoT, in part by the Ministry of Trade,
   Industry & Energy of Korea, in part by the Basic Science Research
   Program through the National Research Foundation of Korea, and in part
   by the Ministry of Education under Grant NRF-2019R1A6A1A03032119.
CR Aflalo Y., 2020, arXiv
   Ahmed W, 2021, INT C PATT RECOG, P6554, DOI 10.1109/ICPR48806.2021.9413006
   [Anonymous], 1990, Adv Neural Inform Process Syst.
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Cai Han, 2020, P 8 INT C LEARN REPR
   Chen XZ, 2023, IEEE T COMPUT AID D, V42, P644, DOI 10.1109/TCAD.2022.3178047
   Chin TW, 2020, PROC CVPR IEEE, P1515, DOI 10.1109/CVPR42600.2020.00159
   Choi J, 2019, IEEE I CONF COMP VIS, P502, DOI 10.1109/ICCV.2019.00059
   Choi J, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P16, DOI [10.1109/aicas48895.2020.9073907, 10.1109/AICAS48895.2020.9073907]
   Ding GG, 2021, IEEE T IMAGE PROCESS, V30, P293, DOI 10.1109/TIP.2020.3035028
   Ding X., 2019, Advances in Neural Information Processing Systems, P6379
   Ding XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4490, DOI 10.1109/ICCV48922.2021.00447
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Ding XH, 2019, PROC CVPR IEEE, P4938, DOI 10.1109/CVPR.2019.00508
   Ding XH, 2018, AAAI CONF ARTIF INTE, P6797
   Ding Xiaohan, 2019, INT C MACHINE LEARNI, P1607
   Nguyen DT, 2019, IEEE T VLSI SYST, V27, P1861, DOI 10.1109/TVLSI.2019.2905242
   Nguyen DT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351021
   Everingham Everingham M. M., Int. J. Comput. Vis., V88 88, P303, DOI 10.1007/s11263-009-0275-4 10.1007/s11263-009-0275-4
   Franco J, 2021, PHYSIOTHER THEOR PR, V37, P1419, DOI 10.1080/09593985.2019.1709234
   Gao SQ, 2020, PROC CVPR IEEE, P1896, DOI 10.1109/CVPR42600.2020.00197
   Gao X. T., 2019, P 7 INT C LEARN REPR
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   Han S., 2016, ARXIV151000149
   Han S., 2017, P 5 INT C LEARN REPR, P1
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   Hassibi B., 1993, Proceedings of the 6th International Conference on Neural Information Processing Systems. NIPS'93, P263
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234
   He Y, 2020, PROC CVPR IEEE, P2006, DOI 10.1109/CVPR42600.2020.00208
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu Hengyuan, 2016, Network trimming: A data-driven neuron pruning approach towards efficient deep architectures
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kang M., 2020, PR MACH LEARN RES, P5122
   Kim NJ, 2020, INT SOC DESIGN CONF, P316, DOI 10.1109/ISOCC50952.2020.9333054
   Kim S, 2021, IEEE ACCESS, V9, P20828, DOI 10.1109/ACCESS.2021.3054879
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Li YC, 2019, PROC CVPR IEEE, P2795, DOI 10.1109/CVPR.2019.00291
   Lin MB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P673
   Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160
   Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Misra D., 2020, Paper #928
   Molchanov P., 2017, INT C LEARN REPR ICL, P1, DOI DOI 10.1002/9781118786352.WBIEG1156
   Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152
   Morcos A., 2019, NEURIPS, P4933
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi J, 2021, IEEE T CIRC SYST VID, V31, P2008, DOI 10.1109/TCSVT.2020.3013170
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang ZZ, 2020, IEEE T MULTIMEDIA, V22, P2126, DOI 10.1109/TMM.2019.2950523
   Xu YH, 2020, IEEE T MULTIMEDIA, V22, P1874, DOI 10.1109/TMM.2019.2949857
   Yawei Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8015, DOI 10.1109/CVPR42600.2020.00804
   Ye Jianbo, 2018, P INT C LEARN REPR
   You ZH, 2019, ADV NEUR IN, V32
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao CL, 2019, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2019.00289
NR 66
TC 8
Z9 8
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5279
EP 5290
DI 10.1109/TMM.2022.3189496
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300047
DA 2024-07-18
ER

PT J
AU Li, J
   Zhang, C
   Liu, Z
   Hong, RC
   Hu, H
AF Li, Jie
   Zhang, Cong
   Liu, Zhi
   Hong, Richang
   Hu, Han
TI Optimal Volumetric Video Streaming With Hybrid Saliency Based Tiling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Volumetric video; point cloud; tiling; adaptive video streaming;
   saliency detection; resource allocation
ID POINT; CHALLENGES; HISTOGRAMS; SURFACE
AB Volumetric video enables a six-degree-of-freedom (6DoF) immersive viewing experience and has a wide range of applications in entertainment and education, among others. Most existing approaches to volumetric video streaming are extensions of VR video streaming solutions that do not take into account user behavior and the properties of the video during the tiling process, and the complexity of decoding is high. To this end, we study volumetric video streaming in this paper and address the research questions mentioned above. In particular, we first propose a hybrid visual saliency and hierarchical clustering empowered 3D tiling scheme that better matches the user's field of view (FoV). Then, we build a quality of experience (QoE) model considering the volumetric video features as the optimization objective. In addition to the usual encoded version, we introduce the reconstructed version (i.e., decoded version, which allows the user to skip the decoding process and thus reduces the decoding overhead) and propose a joint computational and communication resource allocation scheme to achieve a trade-off between communication and computational resources to maximize the QoE. We perform exhaustive simulations and build a prototype system to verify the performance of the proposed tiling and transmission scheme. The results show that the proposed tiling and transmission scheme performs significantly better than the comparison schemes.
C1 [Li, Jie; Zhang, Cong; Hong, Richang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
   [Liu, Zhi] Univ Electrocommun, Chofu 1828585, Japan.
   [Hong, Richang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei, Peoples R China.
   [Hu, Han] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
C3 Hefei University of Technology; University of Electro-Communications -
   Japan; Hefei University of Technology; Beijing Institute of Technology
RP Liu, Z (corresponding author), Univ Electrocommun, Chofu 1828585, Japan.
EM lijie@hfut.edu.cn; zhangcong@mail.hfut.edu.cn; liu@ieee.org;
   hongrc@hfut.edu.cn; hhu@bit.edu.cn
RI Wu, Celimuge/P-1232-2019; Liu, Zhi/AAE-5698-2020
OI Wu, Celimuge/0000-0001-6853-5878; Liu, Zhi/0000-0003-0537-4522; LI,
   Jie/0000-0001-8483-6240; Hu, Han/0000-0001-7532-0496
FU National Natural Science Foundation of China (NSFC) [52077049,
   61971457]; Anhui Provincial Natural Science Foundation [2008085UD04];
   Cooperative Innovation Project of Colleges in Anhui [GXXT-2019-025];
   National Key Research and Development Program of China [2021YFC3300200];
   Grants-in-Aid for Scientific Research [20H04174] Funding Source: KAKEN
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants 52077049 and 61971457, in part
   by the Anhui Provincial Natural Science Foundation under Grant
   2008085UD04, in part by the Cooperative Innovation Project of Colleges
   in Anhui under Grant GXXT-2019-025, and in part by the National Key
   Research and Development Program of China under Grant 2021YFC3300200.
CR Abid M, 2020, IEEE IMAGE PROC, P3448, DOI 10.1109/ICIP40778.2020.9191064
   Behl A, 2019, PROC CVPR IEEE, P7954, DOI 10.1109/CVPR.2019.00815
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Clemm A, 2020, IEEE COMMUN MAG, V58, P93, DOI 10.1109/MCOM.001.1900272
   dEon E., 2017, P ISO IEC JTC1 SC29
   Dewan A, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1765, DOI 10.1109/IROS.2016.7759282
   Ding XY, 2019, IEEE T IMAGE PROCESS, V28, P5379, DOI 10.1109/TIP.2019.2918735
   Dorea C, 2019, IEEE IMAGE PROC, P3721, DOI [10.1109/icip.2019.8803690, 10.1109/ICIP.2019.8803690]
   Figueiredo V. F., 2020, P IEEE 22 INT WORKSH, P1
   Fujihashi T, 2022, IEEE T MULTIMEDIA, V24, P2179, DOI 10.1109/TMM.2021.3077772
   Google, 2018, DRAC 3D DAT COMPR
   Guarda AFR, 2021, IEEE T MULTIMEDIA, V23, P77, DOI 10.1109/TMM.2020.2974325
   Gul Serhan, 2020, NOSSDAV '20: Proceedings of the 30th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P27, DOI 10.1145/3386290.3396933
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han B, 2020, MOBICOM '20: PROCEEDINGS OF THE 26TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2020), P137, DOI 10.1145/3372224.3380888
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hosseini M, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P25, DOI 10.1145/3210424.3210429
   Javaheri A, 2021, IEEE T MULTIMEDIA, V23, P4049, DOI 10.1109/TMM.2020.3037481
   Kolpakov RM, 2010, DISCRET MATH APPL, V20, P95, DOI 10.1515/DMA.2010.006
   Li J., 2022, ARXIV
   Li J., 2020, PROC ICC IEEE INT C, P1
   Liu LN, 2020, J NETW COMPUT APPL, V155, DOI 10.1016/j.jnca.2020.102535
   Liu XY, 2019, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2019.00062
   Liu Z, 2021, IEEE NETWORK, V35, P202, DOI 10.1109/MNET.101.2000364
   Liu Z, 2020, IEEE OPEN J COMP SOC, V1, P121, DOI 10.1109/OJCS.2020.3006205
   Long KX, 2021, IEEE T MULTIMEDIA, V23, P3670, DOI 10.1109/TMM.2020.3029880
   Maximo A, 2011, GRAPH MODELS, V73, P231, DOI 10.1016/j.gmod.2011.05.002
   Meynet G, 2019, INT WORK QUAL MULTIM
   Park J, 2019, IEEE J EM SEL TOP C, V9, P149, DOI 10.1109/JETCAS.2019.2898622
   Qian F, 2019, HOTMOBILE '19 - PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, P135, DOI 10.1145/3301293.3302358
   Raca D, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P303, DOI 10.1145/3339825.3394938
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Shtrom E, 2013, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2013.446
   Souto AL, 2020, Arxiv, DOI arXiv:2008.08438
   Subramanyam S, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3669, DOI 10.1145/3394171.3413535
   Tasse FP, 2015, IEEE I CONF COMP VIS, P163, DOI 10.1109/ICCV.2015.27
   van der Hooft J, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123081
   van der Hooft J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2405, DOI 10.1145/3343031.3350917
   Wang LS, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1930, DOI 10.1109/ICASSP39728.2021.9414121
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Yun JS, 2016, IEEE IMAGE PROC, P4062, DOI 10.1109/ICIP.2016.7533123
   Zhang HZ, 2020, IEEE T MULTIMEDIA, V22, P3210, DOI 10.1109/TMM.2020.2973828
   Zheng J. H., 2013, 2013 5th Computer Science and Electronic Engineering Conference (CEEC), P87, DOI 10.1109/CEEC.2013.6659451
NR 45
TC 27
Z9 27
U1 3
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2939
EP 2953
DI 10.1109/TMM.2022.3153208
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600038
DA 2024-07-18
ER

PT J
AU Li, J
   Song, Q
   Yan, XH
   Chen, YQ
   Huang, R
AF Li, Jie
   Song, Qi
   Yan, Xiaohu
   Chen, Yongquan
   Huang, Rui
TI From Front to Rear: 3D Semantic Scene Completion Through Planar
   Convolution and Attention-Based Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Three-dimensional displays; Convolution; Feature extraction;
   Semantic segmentation; Task analysis; Surface treatment; Semantic scene
   completion; planar convolution; planar attention; context perception;
   RGB-D fusion
ID FUSION NETWORK
AB Semantic Scene Completion (SSC) aims to reconstruct complete 3D scenes with precise voxel-wise semantics from the single-view incomplete input data, a crucial but highly challenging problem for scene understanding. Although SSC has seen significant progress due to the introduction of 2D semantic priors in recent years, the occluded parts, especially the rear-view of the scenes, are still poorly completed and segmented. To ameliorate this issue, we propose a novel deep learning framework for 3D SSC, named Planar Convolution and Attention-based Network (PCANet), to effectively extend high-precision predictions of the front-view surface to the rear-view occluded areas. Specifically, we decompose the traditional convolutional layer into three successive planar convolutions to form a Planar Convolution Residual (PCR) block, which maintains the planar features of the 3D scene. Afterward, the Planar Attention Module (PAM) is proposed to capture three different planar attentions and harvest the global context from the front surface to the rear occluded areas to improve the overall accuracy. Extensive experiments on the real NYU and NYUCAD datasets and the synthetic SUNCG-RGBD dataset demonstrate that our proposed framework can generate high-quality SSC results in both front and rear views and outperforms the state-of-the-art approaches trained in an end-to-end manner without additional data.
C1 [Li, Jie; Song, Qi; Huang, Rui] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Guangdong, Peoples R China.
   [Li, Jie; Yan, Xiaohu] Shenzhen Polytech, Sch Artificial Intelligence, Shenzhen 518055, Guangdong, Peoples R China.
   [Chen, Yongquan] Chinese Univ Hong Kong, Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518172, Guangdong, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Shenzhen Polytechnic
   University; The Chinese University of Hong Kong, Shenzhen; Shenzhen
   Institute of Artificial Intelligence & Robotics for Society
RP Huang, R (corresponding author), Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Guangdong, Peoples R China.
EM jieli1@szpt.edu.cn; qisong@link.cuhk.edu.cn; yanxiaohu@szpt.edu.cn;
   yqchen@cuhk.edu.cn; ruihuang@cuhk.edu.cn
RI Huang, Rui/AAC-8207-2022
OI Huang, Rui/0000-0002-7950-1662; CHEN, Yongquan/0000-0003-2493-6033
FU Natural Science Foundation of Shenzhen Municipality
FX No Statement Available
CR Behley J, 2021, INT J ROBOT RES, V40, P959, DOI 10.1177/02783649211006735
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Bi Q, 2020, IEEE T IMAGE PROCESS, V29, P4911, DOI 10.1109/TIP.2020.2975718
   Cai YJ, 2021, PROC CVPR IEEE, P324, DOI 10.1109/CVPR46437.2021.00039
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XK, 2020, PROC CVPR IEEE, P4192, DOI 10.1109/CVPR42600.2020.00425
   Chen XK, 2020, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP40778.2020.9191318
   Chen YT, 2019, IEEE IMAGE PROC, P1835, DOI [10.1109/ICIP.2019.8803174, 10.1109/icip.2019.8803174]
   Cheng Ran, 2021, C ROBOT LEARNING, P2148
   Chorowski J, 2015, ADV NEUR IN, V28
   Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481
   Dourado A., 2022, P IEEECVF WINTER C A, P3781
   Dourado A, 2021, INT C PATT RECOG, P503, DOI 10.1109/ICPR48806.2021.9413252
   Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao CL, 2021, KNOWL-BASED SYST, V214, DOI 10.1016/j.knosys.2021.106754
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Garbade M, 2019, IEEE COMPUT SOC CONF, P416, DOI 10.1109/CVPRW.2019.00055
   Gillsjo D, 2021, INT C PATT RECOG, P6335, DOI 10.1109/ICPR48806.2021.9412403
   Guo RQ, 2017, Arxiv, DOI arXiv:1504.02437
   Guo YX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P726
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/ICIP.2019.8803025, 10.1109/icip.2019.8803025]
   Ilse M, 2018, PR MACH LEARN RES, V80
   Li J., 2021, P 30 INT JOINT C ART, P793
   Li J, 2020, PROC CVPR IEEE, P3348, DOI 10.1109/CVPR42600.2020.00341
   Li J, 2019, PROC CVPR IEEE, P7685, DOI 10.1109/CVPR.2019.00788
   Li J, 2020, IEEE ROBOT AUTOM LET, V5, P219, DOI 10.1109/LRA.2019.2953639
   Li SQ, 2020, AAAI CONF ARTIF INTE, V34, P11402
   Liang YQ, 2021, IEEE INT CONF ROBOT, P13194, DOI 10.1109/ICRA48506.2021.9560925
   Liu SC, 2018, ADV NEUR IN, V31
   Liu Y, 2020, Arxiv, DOI arXiv:2002.07269
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Paszke A., 2017, PROC ADVNEURAL INF P
   Rist C, 2022, IEEE T PATTERN ANAL, V44, P7205, DOI 10.1109/TPAMI.2021.3095302
   Roldao L, 2022, INT J COMPUT VISION, V130, P1978, DOI 10.1007/s11263-021-01504-5
   Roldao L, 2020, INT CONF 3D VISION, P111, DOI 10.1109/3DV50981.2020.00021
   Rong Chen, 2019, 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), P1192, DOI 10.1109/ROBIO49542.2019.8961556
   Shen T, 2018, AAAI CONF ARTIF INTE, P5446
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song Q, 2024, IEEE T NEUR NET LEAR, V35, P7162, DOI 10.1109/TNNLS.2022.3214216
   Song Q, 2022, AAAI CONF ARTIF INTE, P2280
   Song Q, 2021, AAAI CONF ARTIF INTE, V35, P2567
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Tang JX, 2022, AAAI CONF ARTIF INTE, P2352
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XZ, 2022, AAAI CONF ARTIF INTE, P2550
   Wang YD, 2019, IEEE I CONF COMP VIS, P8607, DOI 10.1109/ICCV.2019.00870
   Wang YD, 2018, INT CONF 3D VISION, P426, DOI 10.1109/3DV.2018.00056
   Wu SC, 2020, INT CONF 3D VISION, P801, DOI 10.1109/3DV50981.2020.00090
   Xiaokang Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P561, DOI 10.1007/978-3-030-58621-8_33
   Yan X, 2021, AAAI CONF ARTIF INTE, V35, P3101
   Yang GL, 2023, IEEE T MULTIMEDIA, V25, P3841, DOI 10.1109/TMM.2022.3167555
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Zhang JH, 2018, LECT NOTES COMPUT SC, V11216, P749, DOI 10.1007/978-3-030-01258-8_45
   Zhang L, 2018, NEUROCOMPUTING, V318, P182, DOI 10.1016/j.neucom.2018.08.052
   Zhang PP, 2019, IEEE I CONF COMP VIS, P7800, DOI 10.1109/ICCV.2019.00789
   Zhang SL, 2021, AAAI CONF ARTIF INTE, V35, P3385
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng B, 2013, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2013.402
   Zhong M, 2020, FRONT ARTIF INTEL AP, V325, P2824, DOI 10.3233/FAIA200424
   Zou H, 2021, IEEE INT C INT ROBOT, P16, DOI 10.1109/IROS51168.2021.9635888
NR 63
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8294
EP 8307
DI 10.1109/TMM.2023.3234441
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000041
DA 2024-07-18
ER

PT J
AU Liu, XQ
   Zeng, HQ
   Shi, YF
   Zhu, JQ
   Hsia, CH
   Ma, KK
AF Liu, Xiaoqing
   Zeng, Huanqiang
   Shi, Yifan
   Zhu, Jianqing
   Hsia, Chih-Hsien
   Ma, Kai-Kuang
TI Deep Cross-Modal Hashing Based on Semantic Consistent Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal hashing; rank learning; heterogeneous gap; intra-modal
   similarity
ID IMAGE RETRIEVAL; REPRESENTATIONS; NETWORK; CODES
AB The amount of multi-modal data available on the Internet is enormous. Cross-modal hash retrieval maps heterogeneous cross-modal data into a single Hamming space to offer fast and flexible retrieval services. However, existing cross-modal methods mainly rely on the feature-level similarity between multi-modal data and ignore the relationship between relative rankings and label-level fine-grained similarity of neighboring instances. To overcome these issues, we propose a novel Deep Cross-modal Hashing based on Semantic Consistent Ranking (DCH-SCR) that comprehensively investigates the intra-modal semantic similarity relationship. Firstly, to the best of our knowledge, it is an early attempt to preserve semantic similarity for cross-modal hashing retrieval by combining label-level and feature-level information. Secondly, the inherent gap between modalities is narrowed by developing a ranking alignment loss function. Thirdly, the compact and efficient hash codes are optimized based on the common semantic space. Finally, we use the gradient to specify the optimization direction and introduce the Normalized Discounted Cumulative Gain (NDCG) to achieve varying optimization strengths for data pairs with different similarities. Extensive experiments on three real-world image-text retrieval datasets demonstrate the superiority of DCH-SCR over several state-of-the-art cross-modal retrieval methods.
C1 [Liu, Xiaoqing; Zeng, Huanqiang] Huaqiao Univ, Sch Informat Sci & Engn, Quanzhou 362021, Peoples R China.
   [Zeng, Huanqiang; Shi, Yifan; Zhu, Jianqing] Huaqiao Univ, Sch Engn, Quanzhou 362021, Peoples R China.
   [Hsia, Chih-Hsien] Ilan Univ, Dept Comp Sci & Informat Engn, Yilan City 260, Taiwan.
   [Ma, Kai-Kuang] Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang 639798, Singapore.
C3 Huaqiao University; Huaqiao University; National Ilan University;
   Nanyang Technological University
RP Zeng, HQ (corresponding author), Huaqiao Univ, Sch Informat Sci & Engn, Quanzhou 362021, Peoples R China.; Zeng, HQ (corresponding author), Huaqiao Univ, Sch Engn, Quanzhou 362021, Peoples R China.
EM liuxiaoqing@stu.hqu.edu.cn; zeng0043@hqu.edu.cn; shiyifan@hqu.edu.cn;
   jqzhu@hqu.edu.cn; hsiach@niu.edu.tw; ekkma@ntu.edu.sg
RI Zeng, Huanqiang/U-2017-2018; SHI, YIFAN/KIE-3322-2024; Ma,
   Kai-Kuang/KBA-9411-2024
OI Shi, Yifan/0000-0001-6961-5888; Liu, Xiaoqing/0000-0003-4256-4771
FU National Key Ramp;D Program of China
FX No Statement Available
CR Bogolin SV, 2022, PROC CVPR IEEE, P5184, DOI 10.1109/CVPR52688.2022.00513
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Brunelli R, 2000, IEEE T MULTIMEDIA, V2, P164, DOI 10.1109/6046.865481
   Burges Christopher J. C., 2010, Learning, V11, P81
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Chen YH, 2022, IEEE T IND INFORM, V18, P1458, DOI 10.1109/TII.2021.3091435
   Chen Y, 2020, IEEE T IMAGE PROCESS, V29, P3596, DOI 10.1109/TIP.2020.2963952
   Chen ZD, 2020, IEEE T CIRC SYST VID, V30, P2262, DOI 10.1109/TCSVT.2019.2911359
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Fang YX, 2019, KNOWL-BASED SYST, V171, P69, DOI 10.1016/j.knosys.2019.02.004
   He R, 2015, IEEE T IMAGE PROCESS, V24, P5543, DOI 10.1109/TIP.2015.2466106
   Hoang T, 2023, IEEE T NEUR NET LEAR, V34, P6289, DOI 10.1109/TNNLS.2021.3135420
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Jrvelin Kalervo, 2017, ACM SIGIR Forum, V51, P243, DOI 10.1145/3130348.3130374
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li HX, 2023, IEEE T KNOWL DATA EN, V35, P1185, DOI 10.1109/TKDE.2021.3102119
   Lin QB, 2021, IEEE T MULTIMEDIA, V23, P550, DOI 10.1109/TMM.2020.2984081
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1379, DOI 10.1145/3397271.3401086
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu XQ, 2022, INT CONF ACOUST SPEE, P4828, DOI 10.1109/ICASSP43922.2022.9746965
   Liu XW, 2019, AAAI CONF ARTIF INTE, P4400
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Luan Y, 2021, T ASSOC COMPUT LING, V9, P329, DOI 10.1162/tacl_a_00369
   Luo X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2518
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mo DM, 2022, INT J MACH LEARN CYB, V13, P1571, DOI 10.1007/s13042-021-01466-7
   Nie J, 2023, IEEE T MULTIMEDIA, V25, P868, DOI 10.1109/TMM.2021.3134161
   Nie XS, 2021, IEEE T CIRC SYST VID, V31, P3669, DOI 10.1109/TCSVT.2020.3042972
   Nie XS, 2021, IEEE T CIRC SYST VID, V31, P401, DOI 10.1109/TCSVT.2020.2974877
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song DJ, 2015, IEEE I CONF COMP VIS, P1922, DOI 10.1109/ICCV.2015.223
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang D, 2018, IEEE T CIRC SYST VID, V28, P2703, DOI 10.1109/TCSVT.2017.2723302
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang QQ, 2021, IEEE T IMAGE PROCESS, V30, P305, DOI 10.1109/TIP.2020.3036717
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu X., 2015, P IEEE INT C MULT EX, P1
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang KX, 2023, IEEE T IND INFORM, V19, P251, DOI 10.1109/TII.2022.3157727
   Yang KX, 2023, IEEE T KNOWL DATA EN, V35, P3885, DOI 10.1109/TKDE.2021.3137792
   Yang KX, 2020, IEEE T NEUR NET LEAR, V31, P1387, DOI 10.1109/TNNLS.2019.2920246
   Yu E, 2022, NEUROCOMPUTING, V486, P215, DOI 10.1016/j.neucom.2021.11.035
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Yu J, 2021, AAAI CONF ARTIF INTE, V35, P4626
   Zhang C, 2018, AAAI CONF ARTIF INTE, P531
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang L, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108217
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 66
TC 12
Z9 12
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9530
EP 9542
DI 10.1109/TMM.2023.3254199
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200007
DA 2024-07-18
ER

PT J
AU Liu, YH
   Wei, W
   Peng, DW
   Mao, XL
   He, ZY
   Zhou, P
AF Liu, Yuhang
   Wei, Wei
   Peng, Daowan
   Mao, Xian-Ling
   He, Zhiyong
   Zhou, Pan
TI Depth-Aware and Semantic Guided Relational Attention Network for Visual
   Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth estimation; multi-modal representation; relational reasoning;
   visual question answering
AB Visual relationship understanding plays an indispensable role in grounded language tasks like visual question answering (VQA), which often requires precisely reasoning about relations among objects depicted in the given question. However, prior works generally suffer from the deficiencies as follows, (1) spatial-relation inference<bold> </bold>ambiguity, it is challenging to accurately estimate the distance of a pair of visual objects in 2D space if there is a visual-overlap between their 2D bounding-boxes, and (2) language-visual relational alignment missing, it is insufficient to generate a high-quality answer to the question if there is a lack of alignment in the language-visual relations of objects during fusion, even using a powerful fusion model like Transformer. To this end, we first model the spatial relation of a pair of objects in 3D space by augmenting the original 2D bounding-box with 1D depth information, and then propose a novel model named <bold>D</bold>epth-aware <bold>S</bold>emantic <bold>G</bold>uided Relational <bold>A</bold>ttention Network (DSGANet), to explicitly exploit the formed 3D spatial relations of objects in an intra-/inter-modality manner for precise relational alignment. Extensive experiments conducted on the benchmarks (VQA v2.0 and GQA) demonstrate DSGANet achieves competitive performance compared to pretrained and non-pretrained models, such as 72.7% vs. 74.6% based on the learned grid features on VQA v2.0.
C1 [Liu, Yuhang; Wei, Wei; Peng, Daowan] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Cognit Comp & Intelligent Informat Proc CCIIP Lab, Wuhan 430074, Peoples R China.
   [Mao, Xian-Ling] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100811, Peoples R China.
   [He, Zhiyong] Naval Univ Engn, Wuhan 430030, Peoples R China.
   [Zhou, Pan] Huazhong Univ Sci & Technol, Hubei Engn Res Ctr Big Data Secur, Sch Cyber Sci & Engn, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Beijing Institute of
   Technology; Wuhan Naval University of Engineering; Huazhong University
   of Science & Technology
RP Wei, W (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Cognit Comp & Intelligent Informat Proc CCIIP Lab, Wuhan 430074, Peoples R China.
EM lyuhang@hust.edu.cn; weiw@hust.edu.cn; pengdw@hust.edu.cn;
   maoxl@bit.edu.cn; moonmon_pub@outlook.com; panzhou@hust.edu.cn
RI He, Zhiyong/ABE-6427-2020
OI He, Zhiyong/0000-0001-7976-9928; Wei, Wei/0000-0003-4488-0102
FU National Natural Science Foundation of China [61602197, L1924068,
   61772076]; CCF-AFSG Research Fund [RF20210005]; HUST; Pingan Property &
   Casualty Research
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61602197, L1924068, and 61772076, in
   part by CCF-AFSG Research Fund under Grant RF20210005, and in part by
   the fund of Joint Laboratory of HUST and Pingan Property & Casualty
   Research (HPL).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2012, LECT NOTES COMPUT SC, DOI [10.1007/978-3-642-33715-4_54, DOI 10.1007/978-3-642-33715-4_54]
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Banerjee P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1888, DOI 10.1109/ICCV48922.2021.00192
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Cao QX, 2021, IEEE T PATTERN ANAL, V43, P887, DOI 10.1109/TPAMI.2019.2943456
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   Hoyer L, 2021, PROC CVPR IEEE, P11125, DOI 10.1109/CVPR46437.2021.01098
   Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4
   Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039
   Huang PP, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3595
   Huang Q., 2020, P 58 ANN M ASS COMP, P7166
   Hudson D., 2019, P ADV NEUR INF PROC, P5903
   Hudson D.A., 2018, INT C LEARN REPR ICL
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Jiang H., 2020, IEEE CVF C COMP VIS, DOI 10.1109/CVPR42600.2020.01028
   Kant Yash, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P715, DOI 10.1007/978-3-030-58545-7_41
   Khademi M, 2020, P 58 ANN M ASS COMP, P7177, DOI 10.18653/v1/2020.acl-main.643
   Kim JH, 2018, ADV NEUR IN, V31
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee Jin Han, 2019, arXiv
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Liang YZ, 2019, IEEE I CONF COMP VIS, P10402, DOI 10.1109/ICCV.2019.01050
   Liao ZM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5074, DOI 10.1145/3474085.3475712
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Malinowski M, 2017, INT J COMPUT VISION, V125, P110, DOI 10.1007/s11263-017-1038-2
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Patro B. N., 2022, SSRN, P1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruixue Tang, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12307), P89, DOI 10.1007/978-3-030-60636-7_8
   Su WJ, 2019, ANN NUTR METAB, V75, P31, DOI 10.1159/000501710
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Xiaofeng Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P414, DOI 10.1007/978-3-030-58589-1_25
   Yang ZQ, 2020, IEEE IMAGE PROC, P1411, DOI 10.1109/ICIP40778.2020.9190771
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zheng C., 2020, PROC 58 ANN M ASS CO, P7642
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 53
TC 2
Z9 2
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5344
EP 5357
DI 10.1109/TMM.2022.3190686
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300052
DA 2024-07-18
ER

PT J
AU Ma, CJ
   Zhuo, L
   Li, JF
   Zhang, YT
   Zhang, J
AF Ma, Chunjie
   Zhuo, Li
   Li, Jiafeng
   Zhang, Yutong
   Zhang, Jing
TI Cascade Transformer Decoder Based Occluded Pedestrian Detection With
   Dynamic Deformable Convolution and Gaussian Projection Channel Attention
   Mechanism
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Convolution; Feature extraction; Kernel; Decoding; Object
   detection; Task analysis; Cascade transformer decoder; dynamic
   deformable convolution; Gaussian project channel attention mechanism;
   occluded pedestrian detection
AB Occluded pedestrian detection is very challenging in computer vision, because the pedestrians are frequently occluded by various obstacles or persons, especially in crowded scenarios. In this article, an occluded pedestrian detection method is proposed under a basic DEtection TRansformer (DETR) framework. Firstly, Dynamic Deformable Convolution (DyDC) and Gaussian Projection Channel Attention (GPCA) mechanism are proposed and embedded into the low layer and high layer of ResNet50 respectively, to improve the representation capability of features. Secondly, Cascade Transformer Decoder (CTD) is proposed, which aims to generate high-score queries, avoiding the influence of low-score queries in the decoder stage, further improving the detection accuracy. The proposed method is verified on three challenging datasets, namely CrowdHuman, WiderPerson, and TJU-DHD-pedestrian. The experimental results show that, compared with the state-of-the-art methods, it can obtain a superior detection performance.
C1 [Ma, Chunjie; Zhuo, Li; Li, Jiafeng; Zhang, Yutong; Zhang, Jing] Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Zhuo, L (corresponding author), Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing 100124, Peoples R China.
EM mcj@machunjie.com; zhuoli@bjut.edu.cn; lijiafeng@bjut.edu.cn;
   zyt0203@emails.bjut.edu.cn; zhj@bjut.edu.cn
RI li, jiafeng/KVY-4468-2024
OI ZHANG, JING/0000-0003-1290-0738; Li, Jiafeng/0000-0001-6976-7275; Ma,
   Chunjie/0000-0002-6348-671X
FU Beijing Natural Science Foundation [L211017]; General Program of Beijing
   Municipal Education Commission [KM202110005027]
FX This work was supported in part by the Beijing Natural Science
   Foundation under Grant L211017 and in part by the General Program of
   Beijing Municipal Education Commission under Grant KM202110005027.
CR Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chi C, 2020, AAAI CONF ARTIF INTE, V34, P10639
   Dai J., 2021, ICLR
   Ge ZH, 2020, 2020 IEEE 3RD INTERNATIONAL CONFERENCE ON MECHATRONICS, ROBOTICS AND AUTOMATION (ICMRA 2020), P1, DOI 10.1109/ICMRA51221.2020.9398345
   He Y, 2022, IEEE T INTELL TRANSP, V23, P10514, DOI 10.1109/TITS.2021.3094800
   He YZ, 2022, MULTIMEDIA SYST, V28, P1135, DOI 10.1007/s00530-022-00891-0
   Hsu WY, 2021, IEEE T IMAGE PROCESS, V30, P934, DOI 10.1109/TIP.2020.3039574
   Huang X., 2020, P IEEE CVF C COMP VI, P10747, DOI DOI 10.1109/CVPR42600.2020.01076
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Khan AH, 2022, INT C PATT RECOG, P4658, DOI 10.1109/ICPR56361.2022.9956732
   Li F, 2022, IEEE ACCESS, V10, P19937, DOI 10.1109/ACCESS.2022.3150988
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li QM, 2022, IEEE T INTELL TRANSP, V23, P21291, DOI 10.1109/TITS.2022.3171250
   Liang TJ, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22134833
   Lin MTU, 2021, Arxiv, DOI arXiv:2012.06785
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu CQ, 2022, MOB INF SYST, V2022, DOI 10.1155/2022/4012252
   Liu ST, 2019, PROC CVPR IEEE, P6452, DOI 10.1109/CVPR.2019.00662
   Mao JY, 2017, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2017.639
   Pang YW, 2021, IEEE T IMAGE PROCESS, V30, P207, DOI 10.1109/TIP.2020.3034487
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Rukhovich D, 2021, LECT NOTES COMPUT SC, V12644, P344, DOI 10.1007/978-3-030-73973-7_33
   Shao S, 2018, Arxiv, DOI [arXiv:1805.00123, DOI 10.48550/ARXIV.1805.00123]
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Sun ZM, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155667
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P8483, DOI 10.1109/TIP.2021.3115672
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2022, PATTERN RECOGN, V128, DOI 10.1016/j.patcog.2022.108605
   Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811
   Xia H, 2022, APPL INTELL, V52, P7686, DOI 10.1007/s10489-021-02796-3
   Yuan J, 2022, IEEE IMAGE PROC, P2906, DOI 10.1109/ICIP46576.2022.9897361
   Zhang SF, 2020, IEEE T MULTIMEDIA, V22, P380, DOI 10.1109/TMM.2019.2929005
   Zheng AL, 2022, PROC CVPR IEEE, P847, DOI 10.1109/CVPR52688.2022.00093
   Zhou CL, 2018, LECT NOTES COMPUT SC, V11205, P138, DOI 10.1007/978-3-030-01246-5_9
   Zhou CL, 2019, IEEE I CONF COMP VIS, P9556, DOI 10.1109/ICCV.2019.00965
NR 41
TC 2
Z9 2
U1 14
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1529
EP 1537
DI 10.1109/TMM.2023.3251100
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000012
DA 2024-07-18
ER

PT J
AU Sheng, CC
   Liu, L
   Deng, WX
   Bai, L
   Liu, Z
   Lao, SY
   Kuang, GY
   Pietikainen, M
AF Sheng, Changchong
   Liu, Li
   Deng, Wanxia
   Bai, Liang
   Liu, Zhong
   Lao, Songyang
   Kuang, Gangyao
   Pietikainen, Matti
TI Importance-Aware Information Bottleneck Learning Paradigm for Lip
   Reading
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lips; Visualization; Task analysis; Feature extraction; Speech
   recognition; Hidden Markov models; Noise measurement; Deep learning;
   information bottleneck; lip reading; visual speech recognition
ID NETWORK; FEATURES
AB Lip reading is the task of decoding text from speakers' mouth movements. Numerous deep learning-based methods have been proposed to address this task. However, these existing deep lip reading models suffer from poor generalization due to overfitting the training data. To resolve this issue, we present a novel learning paradigm that aims to improve the interpretability and generalization of lip reading models. In specific, aVariationalTemporalMask (VTM) module is customized to automatically analyze the importance of frame-level features. Furthermore, the prediction consistency constraints of global information and local temporal important features are introduced to strengthen the model generalization. We evaluate the novel learning paradigm with multiple lip reading baseline models on the LRW and LRW-1000 datasets. Experiments show that the proposed framework significantly improves the generalization performance and interpretability of lip reading models.
C1 [Sheng, Changchong; Deng, Wanxia; Kuang, Gangyao] Natl Univ Def Technol, Coll Elect Sci & Technol, Changsha 410073, Peoples R China.
   [Liu, Li; Bai, Liang; Liu, Zhong; Lao, Songyang] Natl Univ Def Technol, Coll Syst Engn, Lab Big Data & Decis, Changsha 410073, Peoples R China.
   [Pietikainen, Matti] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90570, Finland.
C3 National University of Defense Technology - China; National University
   of Defense Technology - China; University of Oulu
RP Liu, L; Liu, Z (corresponding author), Natl Univ Def Technol, Coll Syst Engn, Lab Big Data & Decis, Changsha 410073, Peoples R China.
EM shengcc_nudt@163.com; li.liu@oulu.fi; wanxiadeng@163.com;
   bailiang_nudt@163.com; zhongliu@nudt.edu.cn; laosongyang@vip.sina.com;
   kuanggangyao@nudt.edu.cn; matti.pietikainen@oulu.fi
RI Liu, Li/JQW-6992-2023
OI Sheng, Changchong/0000-0001-6255-754X; Pietikainen,
   Matti/0000-0003-2263-6731; Liu, li/0000-0002-2011-2873
FU National Key R&D Program of China [2021YFB3100800]; Academy of Finland
   [331883]; National Natural Science Foundation of China [61872379]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021YFB3100800, in part by the Academy of Finland under
   Grant 331883, and in part by the National Natural Science Foundation of
   China under Grant 61872379. The Associate Editor coordinating the
   reviewof this manuscript and approving it for publication was Prof.
   Ichiro Ide.
CR Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2018, Arxiv, DOI arXiv:1809.00496
   Afouras T, 2018, INTERSPEECH, P3244
   Akhtar Z, 2015, IEEE SECUR PRIV, V13, P63, DOI 10.1109/MSP.2015.116
   Alemi A. A., 2017, ICLR, DOI DOI 10.48550/ARXIV.1612.00410
   Almajai I, 2016, INT CONF ACOUST SPEE, P2722, DOI 10.1109/ICASSP.2016.7472172
   Çetingül HE, 2006, IEEE T IMAGE PROCESS, V15, P2879, DOI 10.1109/TIP.2006.877528
   Chen JW, 2022, IEEE T MED IMAGING, V41, P595, DOI 10.1109/TMI.2021.3117996
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Cox S., 2008, The challenge of multispeaker lip-reading
   Dai B, 2018, PR MACH LEARN RES, V80
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Feng DL, 2020, Arxiv, DOI arXiv:2011.07557
   Fischer I, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22090999
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jang E., 2017, P ICLR, P1
   Jeon I, 2021, AAAI CONF ARTIF INTE, V35, P7926
   Jia Y, 2018, ADV NEUR IN, V31
   Kolchinsky A., 2019, P INT C LEARN REPR
   Kolchinsky A, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21121181
   KROGH A, 1992, ADV NEUR IN, V4, P950
   Kummerer M., 2015, P INT C LEARN REPR, P1
   Liu H, 2020, INTERSPEECH, P3520, DOI 10.21437/Interspeech.2020-3146
   Lorenzen S. S., 2022, P INT C LEARN REPR
   Lucey P., 2007, P 8 ANN C INT SPEECH, P650
   Luettin J, 1997, COMPUT VIS IMAGE UND, V65, P163, DOI 10.1006/cviu.1996.0570
   Luo YW, 2022, IEEE T PATTERN ANAL, V44, P3940, DOI 10.1109/TPAMI.2021.3064379
   Luo YW, 2019, IEEE I CONF COMP VIS, P6777, DOI 10.1109/ICCV.2019.00688
   Ma PC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7608, DOI 10.1109/ICASSP39728.2021.9415063
   Mardaoui D, 2021, PR MACH LEARN RES, V130
   Martinez B, 2020, INT CONF ACOUST SPEE, P6319, DOI [10.1109/ICASSP40776.2020.9053841, 10.1109/icassp40776.2020.9053841]
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   Michelsanti D, 2021, IEEE-ACM T AUDIO SPE, V29, P1368, DOI 10.1109/TASLP.2021.3066303
   Nguyen D, 2018, P 2018 C N AM CHAPTE, V1, P1069, DOI [10.18653/v1/N18-1097, DOI 10.18653/V1/N18-1097]
   Pan ZQ, 2021, AAAI CONF ARTIF INTE, V35, P9285
   Pei YR, 2013, IEEE I CONF COMP VIS, P129, DOI 10.1109/ICCV.2013.23
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Prajwal KR, 2022, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR52688.2022.00510
   Rekik A, 2015, LECT NOTES COMPUT SC, V9386, P566, DOI 10.1007/978-3-319-25903-1_49
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Saenko K, 2005, IEEE I CONF COMP VIS, P1424, DOI 10.1109/ICCV.2005.251
   Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820
   Schulz K., 2020, P INT C LEARN REPR
   Sheng CC, 2021, IEEE T MULTIMEDIA, V24, P3545, DOI 10.1109/TMM.2021.3102433
   Shwartz-Ziv R, 2017, Arxiv, DOI arXiv:1703.00810
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stafylakis T, 2017, INTERSPEECH, P3652, DOI 10.21437/Interspeech.2017-85
   Strouse DJ, 2017, NEURAL COMPUT, V29, P1611, DOI 10.1162/NECO_a_00961
   Sun K, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P581, DOI 10.1145/3242587.3242599
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tishby N., 2000, arXiv
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Tye-Murray N, 2007, EAR HEARING, V28, P656, DOI 10.1097/AUD.0b013e31812f7185
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C., 2019, arXiv
   Weng XS, 2019, Arxiv, DOI arXiv:1905.02540
   Wu T., 2020, ADV NEURAL INF PROCE, V33, P20437
   Xiao JY, 2020, IEEE INT CONF AUTOMA, P364, DOI 10.1109/FG47880.2020.00132
   Yamada M, 2020, AS C MACH LEARN ACML
   Yang S, 2019, IEEE INT CONF AUTOMA, P222, DOI 10.1109/fg.2019.8756582
   Yu JC, 2024, IEEE T PATTERN ANAL, V46, P1650, DOI 10.1109/TPAMI.2021.3112205
   Yun Fu, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P325
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang XX, 2019, IEEE I CONF COMP VIS, P713, DOI 10.1109/ICCV.2019.00080
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhao X, 2020, IEEE INT CONF AUTOMA, P420, DOI 10.1109/FG47880.2020.00133
NR 68
TC 2
Z9 2
U1 7
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6563
EP 6574
DI 10.1109/TMM.2022.3210761
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500066
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Su, YT
   Zhao, W
   Jing, PG
   Nie, LQ
AF Su, Yuting
   Zhao, Wei
   Jing, Peiguang
   Nie, Liqiang
TI Exploiting Low-Rank Latent Gaussian Graphical Model Estimation for
   Visual Sentiment Distributions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Visualization; Graphical models; Covariance matrices;
   Multivariate regression; Estimation; Sentiment analysis; Gaussian
   graphical model; low-rank representation; visual sentiment analysis
ID INVERSE COVARIANCE ESTIMATION; THRESHOLDING ALGORITHM; REGRESSION
AB Currently, an increasing number of applications and services has encouraged users to openly express their emotions via images. Unlike visual sentiment classification, visual sentiment distribution learning exploits the overall distribution to represent the relative importance of sentiment labels. Considering that most relevant studies have failed to completely model correlation structures or explicitly apply them to unknown instances, in this paper, we proposed a low-rank latent Gaussian graphical model estimation (LGGME) method for visual sentiment distribution learning tasks. There are three main characteristics of LGGME: 1) an integrated inverse covariance matrix whose parameters characterize the latent correlation structures between and within features and sentiments is estimated based on the sparse Gaussian graphical model; 2) a multivariate normal assumption is assigned on the concatenated latent feature representations and the estimated sentiment distributions instead of the original observations for a reasonable surrogate; and 3) the latent feature representations are projected from a low-rank subspace, which is also available for unseen instances, and the estimated sentiment distributions are evaluated by KL divergence to ensure a suitable setting for distribution learning. We further developed an effective optimization algorithm based on the alternating direction method of multipliers (ADMM) for our objective function. The experimental results obtained on three publicly available datasets demonstrate the superiority of our proposed method.
C1 [Su, Yuting; Zhao, Wei; Jing, Peiguang] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
C3 Tianjin University; Shandong University
RP Jing, PG (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM ytsu@tju.edu.cn; echozhaowei@gmail.com; pgjing@tju.edu.cn;
   nieliqiang@gmail.com
OI Jing, Peiguang/0000-0003-2648-7358
FU National Natural Science Foundation of China [61802277]; Tianjin
   Municipal Natural Science Foundation [20JCQNJC01210]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61802277 and in part by Tianjin
   Municipal Natural Science Foundation under Grant 20JCQNJC01210.The
   associate editor coordinating the review of this manuscript and
   approving itfor publication was Dr. Xavier Giro-i-Nieto
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen T, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P367, DOI 10.1145/2647868.2654935
   Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Ekman P., 1982, Emotion in the human face, P39
   Farzaneh AH, 2020, IEEE COMPUT SOC CONF, P1631, DOI 10.1109/CVPRW50498.2020.00211
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Gao BB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P712
   Geng X, 2022, IEEE T PATTERN ANAL, V44, P1974, DOI 10.1109/TPAMI.2020.3029585
   Geng X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3511
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Hou P, 2017, AAAI CONF ARTIF INTE, P2015
   Huang FR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3388861
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Jia XY, 2021, IEEE T KNOWL DATA EN, V33, P1619, DOI 10.1109/TKDE.2019.2943337
   Jia XY, 2019, PROC CVPR IEEE, P9833, DOI 10.1109/CVPR.2019.01007
   Jia XY, 2018, AAAI CONF ARTIF INTE, P3310
   Jian MW, 2020, IEEE T MULTIMEDIA, V22, P970, DOI 10.1109/TMM.2019.2937187
   Jian MW, 2015, IEEE T CYBERNETICS, V45, P1575, DOI 10.1109/TCYB.2014.2356200
   Jordan MI, 2004, STAT SCI, V19, P140, DOI 10.1214/088342304000000026
   Kim S, 2009, PLOS GENET, V5, DOI 10.1371/journal.pgen.1000587
   Kim Seyoung, 2010, ICML, P543
   Li ZC, 2018, IEEE T NEUR NET LEAR, V29, P1947, DOI 10.1109/TNNLS.2017.2691725
   Li ZC, 2017, AAAI CONF ARTIF INTE, P4154
   Li Zheng, 2010, Proceedings of the SICE 2010 - 49th Annual Conference of the Society of Instrument and Control Engineers of Japan, P1187, DOI 10.1145/1873951.1874183
   Lin C, 2020, AAAI CONF ARTIF INTE, V34, P2661
   Ling MG, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2922818
   Liu G., 2010, P INT C MACH LEARN, P663
   Obozinski G, 2008, ANN ALLERTON CONF, P21, DOI 10.1109/ALLERTON.2008.4797530
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Plutchik R., 1984, APPROACHES EMOTION, P197, DOI DOI 10.1016/B978-0-12-558701-3.50007-7
   Ren TT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3325
   Ren TT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3318
   Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Witten DM, 2009, J R STAT SOC B, V71, P615, DOI 10.1111/j.1467-9868.2009.00699.x
   Xiong HT, 2019, AAAI CONF ARTIF INTE, P363
   Xu M, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3175
   Xu ZW, 2023, IEEE T AFFECT COMPUT, V14, P357, DOI 10.1109/TAFFC.2021.3071131
   Yang B, 2020, NEUROCOMPUTING, V390, P207, DOI 10.1016/j.neucom.2019.02.071
   Yang JY, 2021, PROC CVPR IEEE, P4235, DOI 10.1109/CVPR46437.2021.00422
   Yang JY, 2021, IEEE T IMAGE PROCESS, V30, P8686, DOI 10.1109/TIP.2021.3118983
   Yang JY, 2021, IEEE T IMAGE PROCESS, V30, P7432, DOI 10.1109/TIP.2021.3106813
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang JF, 2017, AAAI CONF ARTIF INTE, P224
   Yao XX, 2019, IEEE I CONF COMP VIS, P1140, DOI 10.1109/ICCV.2019.00123
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhao SC, 2022, IEEE T PATTERN ANAL, V44, P6729, DOI 10.1109/TPAMI.2021.3094362
   Zhao SC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P192, DOI 10.1145/3343031.3351062
   Zhou LY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2964, DOI 10.1145/3394171.3413515
   Zhou Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1247, DOI 10.1145/2733373.2806328
NR 56
TC 2
Z9 2
U1 9
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1243
EP 1255
DI 10.1109/TMM.2022.3140892
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100017
DA 2024-07-18
ER

PT J
AU Sun, C
   Zheng, ZD
   Wang, XH
   Xu, ML
   Yang, Y
AF Sun, Chao
   Zheng, Zhedong
   Wang, Xiaohan
   Xu, Mingliang
   Yang, Yi
TI Self-Supervised Point Cloud Representation Learning via Separating Mixed
   Shapes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Task analysis; Training; Three-dimensional
   displays; Decoding; Shape; Solid modeling; Point cloud Pre-training;
   Self-supervised learning; Graph Neural Network; Representation learning
ID SEGMENTATION
AB The manual annotation for large-scale point clouds costs a lot of time and is usually unavailable in harsh real-world scenarios. Inspired by the great success of the pre-training and fine-tuning paradigm in both vision and language tasks, we argue that pre-training is one potential solution for obtaining a scalable model to 3D point cloud downstream tasks as well. In this paper, we, therefore, explore a new self-supervised learning method, called Mixing and Disentangling (MD), for 3D point cloud representation learning. As the name implies, we mix two input shapes and demand the model learning to separate the inputs from the mixed shape. We leverage this reconstruction task as the pretext optimization objective for self-supervised learning. There are two primary advantages: 1) Compared to prevailing image datasets, e.g., ImageNet, point cloud datasets are de facto small. The mixing process can provide a much larger online training sample pool. 2) On the other hand, the disentangling process motivates the model to mine the geometric prior knowledge, e.g., key points. To verify the effectiveness of the proposed pretext task, we build one baseline network, which is composed of one encoder and one decoder. During pre-training, we mix two original shapes and obtain the geometry-aware embedding from the encoder, then an instance adaptive decoder is applied to recover the original shapes from the embedding. Albeit simple, the pre-trained encoder can capture the key points of an unseen point cloud and surpasses the encoder trained from scratch on downstream tasks. The proposed method has improved the empirical performance on both Model Net-40 and ShapeNet-Part datasets in terms of point cloud classification and segmentation tasks. We further conduct ablation studies to explore the effect of each component and verify the generalization of our proposed strategy by harnessing different backbones.
C1 [Sun, Chao; Wang, Xiaohan; Yang, Yi] Zhejiang Univ, Sch Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zheng, Zhedong] Natl Univ Singapore, Sea NExT Joint Lab, Sch Comp, Singapore 118404, Singapore.
   [Xu, Mingliang] Zhengzhou Univ, Zhengzhou 450000, Henan, Peoples R China.
C3 Zhejiang University; National University of Singapore; Zhengzhou
   University
RP Zheng, ZD (corresponding author), Natl Univ Singapore, Sea NExT Joint Lab, Sch Comp, Singapore 118404, Singapore.
EM c_sun@zju.edu.cn; zhedong.zheng@student.uts.edu.au;
   xiaohan.wang@zju.edu.cn; iexumingliang@zzu.edu.cn; yi.yang@uts.edu.au
RI Zhang, Jinfan/JPK-7588-2023; zhang, xueying/JMB-7808-2023; wang,
   wenjuan/JGD-0428-2023; zhang, yimeng/JLL-7337-2023; 王, 娅冰/JGE-0541-2023;
   LI, SHA/JNR-9956-2023; wu, meng/JPK-1930-2023; Wang,
   Xiaohan/JKI-4414-2023; Zhou, heng/JCN-6493-2023; Zheng,
   Zhedong/R-5314-2019; Jiang, Yuan/JED-3759-2023
OI wang, wenjuan/0000-0002-4220-8817; Wang, Xiaohan/0000-0001-5273-4223;
   Zheng, Zhedong/0000-0002-2434-9050; 
FU National Key R&D Program of China [2020AAA0108800]; Fundamental Research
   Funds for the Central Universities [226-2022-00087]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2020AAA0108800 and in part by the Fundamental Research Funds
   for the Central Universities under Grant 226-2022-00087.
CR Armeni I., 2017, arXiv
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen YR, 2022, IEEE T NEUR NET LEAR, V33, P762, DOI 10.1109/TNNLS.2020.3028964
   Cui YD, 2022, IEEE T INTELL TRANSP, V23, P722, DOI 10.1109/TITS.2020.3023541
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Gidaris S., 2018, P 6 INT C LEARNING R
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Grill J.-B., 2020, arXiv, V33, P21271
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han Tengda, 2020, Adv. Neural Inf. Process. Syst., NIPS, V33, P5679
   Han ZZ, 2019, IEEE I CONF COMP VIS, P10441, DOI 10.1109/ICCV.2019.01054
   Hu QY, 2022, IEEE T PATTERN ANAL, V44, P8338, DOI 10.1109/TPAMI.2021.3083288
   Huang XS, 2021, Arxiv, DOI arXiv:2103.02690
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee D, 2021, PROC CVPR IEEE, P15895, DOI 10.1109/CVPR46437.2021.01564
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li Y, 2021, IEEE T NEUR NET LEAR, V32, P3412, DOI 10.1109/TNNLS.2020.3015992
   Li Z., 2020, arXiv, DOI [DOI 10.48550/ARXIV.2012.05493, 10.48550/arXiv.2012.05493]
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loshchilov I., 2017, P INT C LEARN REPR
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Meng QH, 2022, IEEE T PATTERN ANAL, V44, P4454, DOI 10.1109/TPAMI.2021.3063611
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Radford A, 2021, PR MACH LEARN RES, V139
   Rao YM, 2020, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR42600.2020.00542
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Sauder J., 2019, Adv. Neural Inf. Process. Syst, V32, P12962
   Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tampuu A, 2022, IEEE T NEUR NET LEAR, V33, P1364, DOI 10.1109/TNNLS.2020.3043505
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wan T, 2022, IEEE T NEUR NET LEAR, V33, P3547, DOI 10.1109/TNNLS.2021.3053274
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang PS, 2021, AAAI CONF ARTIF INTE, V35, P2773
   Wang PY, 2018, COMPUT GRAPH-UK, V76, P182, DOI 10.1016/j.cag.2018.07.011
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9598, DOI 10.1109/CVPR42600.2020.00962
   Yao HT, 2017, IEEE INT CON MULTI, P1440, DOI 10.1109/ICME.2017.8019485
   Yi L, 2017, Arxiv, DOI arXiv:1710.06104
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yunlu Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P330, DOI 10.1007/978-3-030-58580-8_20
   Zbontar J, 2021, PR MACH LEARN RES, V139
   Zhang JL, 2022, NEUROCOMPUTING, V505, P58, DOI 10.1016/j.neucom.2022.07.049
   Zhang JZ, 2021, PROC CVPR IEEE, P1768, DOI 10.1109/CVPR46437.2021.00181
   Zhang L, 2019, INT CONF 3D VISION, P395, DOI 10.1109/3DV.2019.00051
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang ZW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10232, DOI 10.1109/ICCV48922.2021.01009
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng ZD, 2021, Arxiv, DOI arXiv:2006.04569
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503
NR 84
TC 8
Z9 8
U1 9
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6207
EP 6218
DI 10.1109/TMM.2022.3206664
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500040
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sun, YF
   Li, L
   Li, Z
   Wang, SZ
   Liu, S
   Li, G
AF Sun, Yangfan
   Li, Li
   Li, Zhu
   Wang, Shizheng
   Liu, Shan
   Li, Ge
TI Learning a Compact Spatial-Angular Representation for Light Field
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Electronic mail; Superresolution; Sun; feature extraction; image
   reconstruction; spatial resolution; super-resolution; task analysis
ID IMAGE COMPRESSION
AB The recent emergence of light field technology has led to new opportunities for immersive visual communication that has a need for high spatial and angular resolution, both of which contribute to a large image storage footprint and high-latency transmission. Task-driven downsampling methods have been proposed as a solution, and have shown improvements in single-image restoration. However, they are inevitable to disregard light field's intrinsic properties in the corresponding tasks. In this paper, we propose a light-field-specific task-driven downsampling framework, called the LFCrNet. The LFCrNet operates on a learning-based decreasing and increasing resolution in an end-to-end manner in order to utilize a cross-view asymmetric sampling technique. In detail, it separates raw data into disparity and non-disparity patterns by measuring pixel-wise residuals between the sub-aperture central view and auxiliary views. Then, a chain of 3-D deformable residual blocks (DRBs) is used to extract disparity features and manage these features regard of their intrinsic property individually. Afterwards, they are compacted into spatio-angular domains through a 3-D deformable downsampler (3-DDS). The non-disparity information is integrated into a separate pipeline that leverages spatial similarity across multiple light field views. This technique is capable of preserving specific occlusion components, and subsequently, restoring them using a learning-based upscaling method to generate a high-quality reconstruction. In general, our method has shown superior performance on multiple open-source datasets by a significant margin.
C1 [Sun, Yangfan; Li, Zhu] Univ Missouri, Univ Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
   [Li, Li] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
   [Wang, Shizheng] Chinese Acad Sci, Inst Microelect, Beijing, Peoples R China.
   [Wang, Shizheng] Chinese Acad Sci, R&D Ctr Internet Things, Wuxi 214111, Peoples R China.
   [Liu, Shan] Tencent, Palo Alto, CA 94301 USA.
C3 University of Missouri System; University of Missouri Kansas City;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Microelectronics,
   CAS; Chinese Academy of Sciences
RP Li, Z (corresponding author), Univ Missouri, Univ Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
EM ysb5b@umsystem.edu; lil1@ustc.edu.cn; lizhu@umsystem.edu;
   shizheng.wang@foxmail.com; shanl@tencent.com; geli@ece.pku.edu.cn
OI , Shan/0000-0002-1442-1207
FU National Science Foundation (NSF) [1747751, 2148382]
FX This work was supported by National Science Foundation (NSF) under
   Grants 1747751 and 2148382.
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   [Anonymous], 2018, P EUROPEAN C COMPUTE
   Bakir N, 2021, IEEE T MULTIMEDIA, V23, P2972, DOI 10.1109/TMM.2021.3068563
   BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525
   Bradski G, 2000, DR DOBBS J, V25, P120
   Bruckstein AM, 2003, IEEE T IMAGE PROCESS, V12, P1132, DOI 10.1109/TIP.2003.816023
   Cheng Z, 2020, IEEE T CIRC SYST VID, V30, P2604, DOI 10.1109/TCSVT.2019.2921660
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137
   Gershun Andrei, 1939, J MATH PHYS, V18, P51, DOI [10.1002/sapm193918151, DOI 10.1002/SAPM193918151]
   Ghassab VK, 2020, IEEE T MULTIMEDIA, V22, P1447, DOI 10.1109/TMM.2019.2946094
   Gul MSK, 2018, IEEE T IMAGE PROCESS, V27, P2146, DOI 10.1109/TIP.2018.2794181
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Huang XP, 2022, IEEE T MULTIMEDIA, V24, P152, DOI 10.1109/TMM.2020.3046860
   Jin J, 2020, AAAI CONF ARTIF INTE, V34, P11141
   Jin J, 2020, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR42600.2020.00233
   Jung H, 2020, IEEE T MULTIMEDIA, V22, P980, DOI 10.1109/TMM.2019.2934819
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kim B, 2021, INT J COMPUT VISION, V129, P579, DOI 10.1007/s11263-020-01386-z
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Lee S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925971
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li L, 2017, IEEE J-STSP, V11, P1107, DOI 10.1109/JSTSP.2017.2725198
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P1092, DOI 10.1109/TIP.2018.2872876
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin WS, 2006, IEEE T IMAGE PROCESS, V15, P2513, DOI 10.1109/TIP.2006.877415
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Meng N, 2020, AAAI CONF ARTIF INTE, V34, P11757
   Mingqing Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P126, DOI 10.1007/978-3-030-58452-8_8
   Mitra Kaushik, 2012, 2012 IEEE COMPUTER S, P22
   Peng JY, 2020, IEEE T COMPUT IMAG, V6, P682, DOI 10.1109/TCI.2020.2967148
   Raj A. S., Stanford lytro light field archive
   Rerabek M., 2016, P 8 INT C QUAL MULT, P6
   Rossi M, 2018, IEEE T IMAGE PROCESS, V27, P4207, DOI 10.1109/TIP.2018.2828983
   Shannon C. E., 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Shi JL, 2020, PROC CVPR IEEE, P2552, DOI 10.1109/CVPR42600.2020.00263
   Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun WJ, 2020, IEEE T IMAGE PROCESS, V29, P4027, DOI 10.1109/TIP.2020.2970248
   Wang S., 2015, P SID S, V46, P1440
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang YQ, 2021, IEEE T IMAGE PROCESS, V30, P1057, DOI 10.1109/TIP.2020.3042059
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wetzstein G, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185576
   WU G, 2017, P IEEE C COMP VIS PA, P6319
   Wu XL, 2009, IEEE T IMAGE PROCESS, V18, P552, DOI 10.1109/TIP.2008.2010638
   Xie SL, 2016, OPT EXPRESS, V24, P1483, DOI 10.1364/OE.24.011483
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Yingqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P290, DOI 10.1007/978-3-030-58592-1_18
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Yu JY, 2017, IEEE MULTIMEDIA, V24, P104, DOI 10.1109/MMUL.2017.24
   Zhang S, 2021, IEEE T IMAGE PROCESS, V30, P5956, DOI 10.1109/TIP.2021.3079805
   Zhang S, 2019, PROC CVPR IEEE, P11038, DOI 10.1109/CVPR.2019.01130
NR 57
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7262
EP 7273
DI 10.1109/TMM.2022.3219671
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000040
DA 2024-07-18
ER

PT J
AU Wang, HB
   Jiang, GQ
   Peng, JJ
   Deng, RX
   Fu, XP
AF Wang, Huibing
   Jiang, Guangqi
   Peng, Jinjia
   Deng, Ruoxi
   Fu, Xianping
TI Towards Adaptive Consensus Graph: Multi-View Clustering via Graph
   Collaboration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaboration; Correlation; Task analysis; Learning systems; Data
   models; Clustering methods; Clustering algorithms; Multi-view
   clustering; consensus graph; hilbert-schmidt independence criterion;
   graph collaboration
AB Multi-view clustering is a long-standing important task, however, it remains challenging to exploit valuable information from the complex multi-view data located in diverse high-dimensional spaces. The core issue is the effective collaboration of multiple views to holistically uncover the essential correlations between multi-view data through graph learning. Furthermore, it is indispensable for most existing methods to introduce an additional clustering step to produce the final clusters, which evidently reduces the uniform relationship between graph learning and clustering. Based on the above considerations, in this paper, we present a novel method named multi-view clustering via graph collaboration (MCGC). Based on the low-dimensional representation space developed by MCGC, it first perceives the correlations between samples in each individual view under the supervision of the Hilbert-Schmidt independence criterion (HSIC). Then, MCGC proposes learning a consensus graph by adaptively collaborating between all the views, which is able to uncover the essential structure of the multi-view data. Meanwhile, by imposing the rank constraint on the Laplacian matrix of the consensus graph to partition the multi-view data naturally into the required number of clusters, the optimal clustering results can be obtained directly without any postprocessing steps. Finally, the resulting optimization problem is solved by an alternating optimization scheme with guaranteed fast convergence. Extensive experiments on 5 benchmark multi-view datasets demonstrate that MCGC markedly outperforms the state-of-the-art baselines.
C1 [Wang, Huibing; Fu, Xianping] Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116026, Liaoning, Peoples R China.
   [Jiang, Guangqi] Changzhou Univ, Sch Comp & Artificial Intelligence, Changzhou 213164, Jiangsu, Peoples R China.
   [Peng, Jinjia] Hebei Univ, Sch Cyber Secur & Comp, Baoding 071002, Hebei, Peoples R China.
   [Deng, Ruoxi] Wenzhou Univ, Coll Comp Sci & Artificial Intelligence, Wenzhou 325035, Peoples R China.
C3 Dalian Maritime University; Changzhou University; Hebei University;
   Wenzhou University
RP Jiang, GQ (corresponding author), Changzhou Univ, Sch Comp & Artificial Intelligence, Changzhou 213164, Jiangsu, Peoples R China.
EM huibing.wang@dlmu.edu.cn; jiangguangqi.dlmu@gmail.com;
   pengjinjia@hbu.edu.cn; ruoxii.deng@gmail.com; fxp@dlmu.edu.cn
FU National Natural Science Foundation of China [62002041, 62176037];
   Fundamental Research Funds for the Central Universities [3132022250];
   Liaoning Fundamental Research Funds for Universities [LJKQZ2021010];
   Liaoning Doctoral Research Startup Fund Project [2021-BS-075]; Dalian
   Science and Technology Innovation Fund [2021JJ12GX028]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62002041 and 62176037, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   3132022250, in part by the Liaoning Fundamental Research Funds for
   Universities under Grant LJKQZ2021010, in part by Liaoning Doctoral
   Research Startup Fund Project under Grant 2021-BS-075 and in part by the
   Dalian Science and Technology Innovation Fund under Grant 2021JJ12GX028.
   The Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Ramanathan Subramanian. (Huibing
   Wang and Guangqi Jiang contributed equally to this work.)
CR Asuncion A., 2007, Uci machine learning repository
   BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chen D.-p, 2022, ACM Multimedia
   Chen MS, 2020, AAAI CONF ARTIF INTE, V34, P3513
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P4054, DOI 10.1109/TMM.2021.3112230
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Cheng MM, 2019, IEEE T IMAGE PROCESS, V28, P2399, DOI 10.1109/TIP.2018.2877937
   FAN K, 1949, P NATL ACAD SCI USA, V35, P652, DOI 10.1073/pnas.35.11.652
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Gu SH, 2014, ADV NEUR IN, V27
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Jiang GQ, 2022, IEEE T CIRC SYST VID, V32, P5307, DOI 10.1109/TCSVT.2022.3143848
   Kang Z, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107627
   Kang Z, 2020, KNOWL-BASED SYST, V189, DOI 10.1016/j.knosys.2019.105102
   Kang Z, 2020, IEEE T CYBERNETICS, V50, P1833, DOI 10.1109/TCYB.2018.2887094
   Kang Z, 2017, AAAI CONF ARTIF INTE, P2080
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A., 2011, P 28 INT C MACHINE L, P393
   Kumar P., 2011, Adv. Neural Inf. Process. Syst., P1413, DOI DOI 10.5555/2986459.2986617
   Li JH, 2018, PATTERN RECOGN, V75, P199, DOI 10.1016/j.patcog.2017.06.012
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Liang YW, 2024, IEEE T NEUR NET LEAR, V35, P2848, DOI 10.1109/TNNLS.2022.3192445
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu DY, 2023, IEEE T NEUR NET LEAR, V34, P8589, DOI 10.1109/TNNLS.2022.3151631
   Liu Deyin, 2022, ACM Trans. Multimedia Comput., Commun., Appl., V1, P1, DOI 10.1145/3522714
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Luo MN, 2019, IEEE T IMAGE PROCESS, V28, P4701, DOI 10.1109/TIP.2019.2913081
   Nene S. A., 1996, Tech. Rep. CUCS-005-96
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Peng X, 2019, PR MACH LEARN RES, V97
   Shi SJ, 2023, IEEE T KNOWL DATA EN, V35, P443, DOI 10.1109/TKDE.2021.3078728
   Shirian A, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6284, DOI 10.1109/ICASSP39728.2021.9413876
   Siagian C, 2007, IEEE T PATTERN ANAL, V29, P300, DOI 10.1109/TPAMI.2007.40
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P5924
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang HB, 2021, IEEE T MULTIMEDIA, V23, P3828, DOI 10.1109/TMM.2020.3032023
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang Y., 2016, P 25 INT JOINT C ART, P2153
   Wang Y, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3383-y
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3408317
   Wang Y, 2017, IEEE T NEUR NET LEAR, V28, P57, DOI 10.1109/TNNLS.2015.2498149
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wei Q, 2015, IEEE T IMAGE PROCESS, V24, P4109, DOI 10.1109/TIP.2015.2458572
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Wu JC, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P726, DOI 10.1145/3404835.3462862
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Wu L., 2022, P 31 INT JOINT C ART, P1465, DOI [10.24963/ijcai.2022/204, DOI 10.24963/IJCAI.2022/204]
   Wu L, 2022, IEEE T IMAGE PROCESS, V31, P4803, DOI 10.1109/TIP.2022.3186746
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Yang MX, 2023, IEEE T PATTERN ANAL, V45, P1055, DOI 10.1109/TPAMI.2022.3155499
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CQ, 2022, IEEE T PATTERN ANAL, V44, P2402, DOI 10.1109/TPAMI.2020.3037734
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang YF, 2009, IEEE T MULTIMEDIA, V11, P1276, DOI 10.1109/TMM.2009.2030629
   Zhang Z, 2020, IEEE T CIRC SYST VID, V30, P2430, DOI 10.1109/TCSVT.2019.2923007
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
NR 68
TC 26
Z9 26
U1 5
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6629
EP 6641
DI 10.1109/TMM.2022.3212270
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500071
DA 2024-07-18
ER

PT J
AU Wang, JK
   Chen, SX
   Wu, ZX
   Jiang, YG
AF Wang, Junke
   Chen, Shaoxiang
   Wu, Zuxuan
   Jiang, Yu-Gang
TI FT-TDR: Frequency-Guided Transformer and Top-Down Refinement Network for
   Blind Face Inpainting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind inpainting; face inpainting; top-down refinement network; visual
   transformer
AB Blind face inpainting refers to the task of reconstructing visual contents without explicitly indicating the corrupted regions in a face image. Inherently, this task faces two challenges: (1) how to detect various mask patterns of different shapes and contents; (2) how to restore visually plausible and pleasing contents in the masked regions. In this paper, we propose a novel two-stage blind face inpainting method named Frequency-guided Transformer and Top-Down Refinement Network (FT-TDR) to tackle these challenges. Specifically, we first use a transformer-based network to detect the corrupted regions to be inpainted as masks by modeling the relation among different patches. For improved detection results, we also exploit the frequency modality as complementary information and capture the local contextual incoherence to enhance boundary consistency. Then a top-down refinement network is proposed to hierarchically restore features at different levels and generate contents that are semantically consistent with the unmasked face regions. Extensive experiments demonstrate that our method outperforms current state-of-the-art blind and non-blind face inpainting methods qualitatively and quantitatively.
C1 [Wang, Junke; Chen, Shaoxiang; Wu, Zuxuan; Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
   [Wang, Junke; Chen, Shaoxiang; Wu, Zuxuan; Jiang, Yu-Gang] Shanghai Collaborat Innovat Ctr Intelligent Visual, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.; Jiang, YG (corresponding author), Shanghai Collaborat Innovat Ctr Intelligent Visual, Shanghai 200433, Peoples R China.
EM 21110240026@m.fudan.edu.cn; sxchen13@fudan.edu.cn; zxwu@fudan.edu.cn;
   ygj@fudan.edu.cn
RI li, fei/JYP-3334-2024
FU NSFC [62032006]
FX This work was supported by NSFC under Project 62032006.
CR Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cai N, 2017, VISUAL COMPUT, V33, P249, DOI 10.1007/s00371-015-1190-z
   Cai WW, 2020, IEEE ACCESS, V8, P48451, DOI 10.1109/ACCESS.2020.2979348
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Durall R, 2020, Arxiv, DOI arXiv:1911.00686
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Hongyu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P725, DOI 10.1007/978-3-030-58536-5_43
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Lahiri Avisek, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13693, DOI 10.1109/CVPR42600.2020.01371
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Liu Y, 2019, LECT NOTES COMPUT SC, V11935, P128, DOI 10.1007/978-3-030-36189-1_11
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Miyato T., 2018, 6 INT C LEARNING REP
   Nazeri K., 2019, ARXIV190100212
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Song LS, 2019, AAAI CONF ARTIF INTE, P2506
   Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI [10.1007/978-3-030-01216-8_1, 10.1109/APCAP.2017.8420330]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JK, 2022, Arxiv, DOI arXiv:2111.11591
   Wang JK, 2022, Arxiv, DOI arXiv:2104.09770
   Wang Q, 2021, IEEE T MULTIMEDIA, V23, P429, DOI 10.1109/TMM.2020.2978633
   Wang Q, 2019, PATTERN RECOGN, V88, P493, DOI 10.1016/j.patcog.2018.11.020
   Xu L., 2012, ACM Transactions on Graphics (TOG), V31, P1, DOI DOI 10.1145/2366145.2366158
   Yang Yang, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12305), P14, DOI 10.1007/978-3-030-60633-6_2
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P752, DOI 10.1007/978-3-030-58595-2_45
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu N, 2019, IEEE I CONF COMP VIS, P7555, DOI 10.1109/ICCV.2019.00765
   Yu T, 2020, AAAI CONF ARTIF INTE, V34, P12733
   Zhao L, 2020, PROC CVPR IEEE, P5740, DOI 10.1109/CVPR42600.2020.00578
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu X., 2021, 9 INT C LEARN REPR I
NR 47
TC 3
Z9 4
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2382
EP 2392
DI 10.1109/TMM.2022.3146774
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100060
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, LX
   Li, HL
   Hu, WZ
   Zhang, XL
   Qiu, HQ
   Meng, FM
   Wu, QB
AF Wang, Lanxiao
   Li, Hongliang
   Hu, Wenzhe
   Zhang, Xiaoliang
   Qiu, Heqian
   Meng, Fanman
   Wu, Qingbo
TI What Happens in Crowd Scenes: A New Dataset About Crowd Scenes for Image
   Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CrowdCaption; crowd scenes; image captioning; multimodal understanding
ID NETWORK; MAPS
AB Making machines endowed with eyes and brains to effectively understand and analyze crowd scenes is of paramount importance for building a smart city to serve people. This is of far-reaching significance for the guidance of dense crowds and accident prevention, such as crowding and stampedes. As a typical multimodal scene understanding task, image captioning has always attracted widespread attention. However, crowd scene understanding captioning is rarely studied due to the unobtainability of related datasets. Therefore, it is difficult to know what happens in crowd scenes. In order to fill this research gap, we propose a crowd scenes caption dataset named CrowdCaption which has the advantages of crowd-topic scenes, comprehensive and complex caption descriptions, typical relationships and detailed grounding annotations. The complexity and diversity of the descriptions and the specificity of the crowd scenes make this dataset extremely challenging to most current methods. Thus, we propose a Multi-hierarchical Attribute Guided Crowd Caption Network (MAGC) based on crowd objects, actions, and status (such as position, dress, posture, etc.) aiming to generate crowd-specific detailed descriptions. We conduct extensive experiments on our CrowdCaption dataset, and our proposed method reaches the state-of-the-art (SoTA) performance. We hope the CrowdCaption dataset can assist future studies related to crowd scenes in the multimodal domain.
C1 [Wang, Lanxiao; Li, Hongliang; Hu, Wenzhe; Zhang, Xiaoliang; Qiu, Heqian; Meng, Fanman; Wu, Qingbo] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, HL; Meng, FM (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM lanxiao.wang@std.uestc.edu.cn; hlli@uestc.edu.cn;
   wenzhe-hu@std.uestc.edu.cn; xlzhang@std.uestc.edu.cn;
   hqqiu@std.uestc.edu.cn; fmmeng@uestc.edu.cn; qbwu@uestc.edu.cn
RI Zhang, Xiaoliang/HTL-3296-2023; Wu, Qingbo/M-5065-2015
OI Zhang, Xiaoliang/0000-0001-9313-8428; Hu, Wenzhe/0000-0002-7941-231X;
   Wu, Qingbo/0000-0003-2936-6340; Qiu, Heqian/0000-0002-0963-0311
FU National Key R&D Program of China [2021ZD0112001]; National Natural
   Science Foundation of China [61831005, 61871087, 61971095]
FX This work was supported in part by the National Key R & D Program of
   China under Grant 2021ZD0112001 and in part by the National Natural
   Science Foundation of China under Grants 61831005, 61871087, and
   61971095.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Deshpande A, 2019, PROC CVPR IEEE, P10687, DOI 10.1109/CVPR.2019.01095
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   Hu R., 2020, COMPUTER VISION ECCV, P742
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang X., 2020, P IEEE CVF C COMP VI, P10747, DOI DOI 10.1109/CVPR42600.2020.01076
   Jia Q, 2021, AAAI CONF ARTIF INTE, V35, P13125
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li NJ, 2021, IEEE T MULTIMEDIA, V23, P203, DOI 10.1109/TMM.2020.2984093
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3799, DOI 10.1145/3474085.3478331
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SL, 2021, AAAI CONF ARTIF INTE, V35, P2163
   Liu Xinyan, 2021, P IEEECVF INT C COMP, P3215
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mathews A, 2016, AAAI CONF ARTIF INTE, P3574
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Plummer BA, 2017, INT J COMPUT VISION, V123, P74, DOI 10.1007/s11263-016-0965-7
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren WH, 2021, IEEE T IMAGE PROCESS, V30, P1439, DOI 10.1109/TIP.2020.3044219
   Shao S, 2018, Arxiv, DOI [arXiv:1805.00123, DOI 10.48550/ARXIV.1805.00123]
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wen LY, 2021, PROC CVPR IEEE, P7808, DOI 10.1109/CVPR46437.2021.00772
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang LY, 2021, IEEE T MULTIMEDIA, V23, P835, DOI 10.1109/TMM.2020.2990074
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang X., 2020, COMPUTER VISION ECCV, P1
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhang JL, 2021, IEEE T MULTIMEDIA, V23, P3085, DOI 10.1109/TMM.2020.3020691
   Zhang ZJ, 2022, IEEE T MULTIMEDIA, V24, P3101, DOI 10.1109/TMM.2021.3093725
   Zheng Y, 2019, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2019.00859
NR 50
TC 3
Z9 3
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5400
EP 5412
DI 10.1109/TMM.2022.3192729
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300056
DA 2024-07-18
ER

PT J
AU Wang, WQ
   Chang, FL
   Liu, CS
   Li, GX
   Wang, B
AF Wang, Wenqian
   Chang, Faliang
   Liu, Chunsheng
   Li, Guangxin
   Wang, Bin
TI GA-Net: A Guidance Aware Network for Skeleton-Based Early Activity
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Skeleton; Dams; Feature extraction; Task analysis;
   Three-dimensional displays; Spatiotemporal phenomena; Early activity
   recognition; guidance awareness; metric learning
AB Early activity prediction, which aims to recognize class labels before actions are fully performed, is a very challenging task since partially observed action sequences contain insufficient class-discrimination information, and thus, many partial action sequences belonging to different categories may look very similar. Therefore, in this paper, we propose a novel guidance aware network (GA-Net) to boost the ability to distinguish different activities in diversified partially observed action sequences via metric learning. To mitigate the similarity problem of action segments at very early stages, the proposed guided metric learning module (GMLM) is able to encourage the feature extractor to mine class-discriminative information given partially observed sequences. Specifically, the GMLM is able to minimize the intraclass distance with a full-length guided direction approach and maximize the difference between interclass categories with different observation ratios. To enhance the similarities between the partial- and full-length sequences in the same action categories, we further introduce a distribution alignment module (DAM) that employs full-length guidance to pull the partially observed features closer to the global features. We evaluate our proposed method on three public human activity datasets and achieve competitive results compared with the state-of-the-art approaches.
C1 [Wang, Wenqian; Chang, Faliang; Liu, Chunsheng; Li, Guangxin; Wang, Bin] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
C3 Shandong University
RP Chang, FL (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM wqwang@mail.sdu.edu.cn; flchang@sdu.edu.cn; liuchunsheng@sdu.edu.cn;
   liguangxin@mail.sdu.edu.cn; sducvwangb@gmail.com
OI Wang, Wenqian/0000-0003-0285-9786; Li, Guangxin/0000-0002-3911-8255
FU National Key R&D Program of China [2018YFB1305300]; National Natural
   Science Foundation of China [62176138, 62176136]; Shandong Provincial
   Key Research, and Development Program [2019JZZY010130, 2020CXGC010207]
FX The work was supported in part by the National Key R & D Program of
   China under Grant 2018YFB1305300, in part by the National Natural
   Science Foundation of China under Grants 62176138 and 62176136, in part
   by the Shandong Provincial Key Research, and Devel-opment Program
   through the Major Scientific, and Technological Innovation Project under
   Grants 2019JZZY010130 and 2020CXGC010207. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publi-cation was Dr. Palaiahnakote Shivakumara.
CR Aliakbarian MS, 2017, IEEE I CONF COMP VIS, P280, DOI 10.1109/ICCV.2017.39
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Cai YJ, 2019, AAAI CONF ARTIF INTE, P8118
   Cao Y, 2013, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2013.343
   Chéron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368
   Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Hu J. -F., 2015, PROC IEEE C COMPUT V, P2186
   Hu JF, 2019, IEEE T PATTERN ANAL, V41, P2568, DOI 10.1109/TPAMI.2018.2863279
   Hussein, 2013, INT JOINT C ART INT
   Jain A, 2016, IEEE INT CONF ROBOT, P3118, DOI 10.1109/ICRA.2016.7487478
   Ji YL, 2021, IEEE T CIRC SYST VID, V31, P289, DOI 10.1109/TCSVT.2020.2975845
   Ke QH, 2020, IEEE T IMAGE PROCESS, V29, P959, DOI 10.1109/TIP.2019.2937757
   Ke Q, 2018, IEEE T MULTIMEDIA, V20, P1712, DOI 10.1109/TMM.2017.2778559
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kingma D. P., 2014, arXiv
   Ko B, 2020, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR42600.2020.00728
   Koch G., 2015, P DEEP LEARN WORKSH
   Kong Y, 2022, Arxiv, DOI [arXiv:1806.11230, DOI 10.1007/S11263-022-01594-9]
   Kong Y, 2018, AAAI CONF ARTIF INTE, P7000
   Kong Y, 2017, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2017.390
   Kong Y, 2014, LECT NOTES COMPUT SC, V8693, P596, DOI 10.1007/978-3-319-10602-1_39
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li C, 2017, IEEE T IMAGE PROCESS, V26, P2149, DOI 10.1109/TIP.2017.2670782
   Li GX, 2022, IEEE T COGN DEV SYST, V14, P1258, DOI 10.1109/TCDS.2021.3103960
   Li K, 2014, IEEE T PATTERN ANAL, V36, P1644, DOI 10.1109/TPAMI.2013.2297321
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li NJ, 2021, IEEE T MULTIMEDIA, V23, P203, DOI 10.1109/TMM.2020.2984093
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P1453, DOI 10.1109/TPAMI.2019.2898954
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214
   Milbich T., 2020, PROC EUR C COMPUT V, P590
   Mohan Deen Dayal, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14579, DOI 10.1109/CVPR42600.2020.01460
   Pang GL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P897
   Pei MT, 2011, IEEE I CONF COMP VIS, P487, DOI 10.1109/ICCV.2011.6126279
   Ren B, 2024, Arxiv, DOI arXiv:2002.05907
   Ryoo MS, 2015, ACMIEEE INT CONF HUM, P295, DOI 10.1145/2696454.2696462
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Ryoo MS, 2011, INT J COMPUT VISION, V93, P183, DOI 10.1007/s11263-010-0355-5
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   Wang XH, 2019, PROC CVPR IEEE, P3551, DOI 10.1109/CVPR.2019.00367
   Weng JW, 2020, IEEE T CIRC SYST VID, V30, P4626, DOI 10.1109/TCSVT.2020.2976789
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
NR 60
TC 6
Z9 6
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1061
EP 1073
DI 10.1109/TMM.2021.3137745
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100003
DA 2024-07-18
ER

PT J
AU Wang, XS
   Jin, K
   Yu, K
   Cheng, YH
AF Wang, Xuesong
   Jin, Ke
   Yu, Kun
   Cheng, Yuhu
TI Asymmetric Training in RealnessGAN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE RealnessGAN; asymmetric training; informationentropy; sampling
   transmission; floating anchor distribution
AB Generative adversarial networks (GANs) have demonstrated superior performances in image generation. In recent years, various improvements of network structure and learning theory related to GANs have undergone numerous advancement. Among these improvement techniques, the asymmetric training on the generator and discriminator networks has been widely adopted. For example, the batch normalization is used in generator while the spectral normalization is used in discriminator, or using different learning rates for the generator and discriminator. However, the asymmetric training on the real and generated samples has not been taken into consideration till now. In this paper, we proposed a novel asymmetric training-based RealnessGAN (ATRGAN) which applies the idea of asymmetric training on both samples and networks. Specifically, the asymmetric training on samples refers to performing the differential learning on the real and generated samples by controlling the information entropies of real and fake anchor distributions. The asymmetric training on networks is realized via the sampling transmission $G2D$, which abandons the commonly used independent random sampling. With the help of $G2D$, the discriminator can obtain a dominant training position than the generator, so as to ensure that the discriminator can guide the generator more effectively during training. In addition, we proposed the floating anchor distribution technique and constructed the objective function of generator for ATRGAN. Through comparative experiments, we demonstrated ATRGAN's ability of achieving better generation performance than various SOTA GANs on CIFAR-10, CAT, and CelebA-HQ datasets.
C1 [Wang, Xuesong; Jin, Ke; Yu, Kun; Cheng, Yuhu] China Univ Min & Technol, Engn Res Ctr Intelligent Control Underground Space, Minist Educ, Xuzhou 221116, Peoples R China.
   [Wang, Xuesong; Jin, Ke; Yu, Kun; Cheng, Yuhu] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
C3 China University of Mining & Technology; China University of Mining &
   Technology
RP Cheng, YH (corresponding author), China Univ Min & Technol, Engn Res Ctr Intelligent Control Underground Space, Minist Educ, Xuzhou 221116, Peoples R China.; Cheng, YH (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
EM wangxuesongcumt@163.com; 584483121@qq.com; kunyu9198@126.com;
   chengyuhu@163.com
OI Ke, Jin/0000-0001-6513-3996
FU National Natural Science Foundation of China
FX No Statement Available
CR Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bau D, 2019, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2019.00460
   Brock A., 2019, INT C LEARN REPR
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gui J, 2023, IEEE T KNOWL DATA EN, V35, P3313, DOI 10.1109/TKDE.2021.3130191
   Hensel M, 2017, ADV NEUR IN, V30
   Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Gulrajani I, 2017, ADV NEUR IN, V30
   Ishida T, 2020, PR MACH LEARN RES, V119
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang Liming, 2021, ADV NEURAL INF PROCE, V34, P21655
   Jolicoeur-Martineau Alexia, 2018, The relativistic discriminator: A key element missing from standard GAN
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Li SY, 2019, PROC CVPR IEEE, P3833, DOI 10.1109/CVPR.2019.00396
   LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115
   Liu YH, 2023, IEEE T MULTIMEDIA, V25, P3343, DOI 10.1109/TMM.2022.3159115
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Martin A., 2017, P INT C LEARN REPR
   Miyato T., 2017, ICLR
   Pan XG, 2022, IEEE T PATTERN ANAL, V44, P7474, DOI 10.1109/TPAMI.2021.3115428
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Radford A., 2015, ARXIV
   Samangouei P., 2018, 6 INT C LEARN REPR I
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Song JK, 2022, IEEE T MULTIMEDIA, V24, P791, DOI 10.1109/TMM.2021.3059336
   Thanh-Tung H., 2019, P INT C LEARN REPR
   Tianyu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8382, DOI 10.1109/CVPR42600.2020.00841
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wei XX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P954
   Xiangli Y., 2020, P INT C LEARN REPR
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang WW, 2008, LECT NOTES COMPUT SC, V5305, P802, DOI 10.1007/978-3-540-88693-8_59
   Zhang XC, 2021, PROC CVPR IEEE, P6515, DOI 10.1109/CVPR46437.2021.00645
   Zhu JC, 2023, IEEE T PATTERN ANAL, V45, P3311, DOI 10.1109/TPAMI.2022.3186752
NR 41
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8157
EP 8169
DI 10.1109/TMM.2022.3233307
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000036
DA 2024-07-18
ER

PT J
AU Wang, ZY
   Liu, GH
   Zhang, DC
   Hua, XH
   Xu, LM
   Gao, P
   Jiang, T
AF Wang, Zhaoyang
   Liu, Guanghua
   Zhang, Dongchen
   Hua, Xinhai
   Xu, Lingmin
   Gao, Peng
   Jiang, Tao
TI Edge-Assisted Massive Video Delivery Over Cell-Free Massive MIMO
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Latency; cell-free massive MIMO; user grouping; adaptive bitrate
ID IMPROVED APPROXIMATION ALGORITHMS; USER SELECTION; PERFORMANCE;
   NETWORKS; DOWNLINK; CUT
AB Massive Multiple-Input Multiple-Output (MIMO), with its spatial multiplexing and channel hardening, has the potential to provide high capacity and reliability for massive video services. In spite of this, massive MIMO in the physical layer does not fully showcase its abilities in video applications unless it is specifically designed to do so. In this paper, we consider a cell-free massive MIMO system as an edge node for scheduling massive streams, and thus the standard server-to-client transmission is split into server-to-edge and edge-to-client. When the server-to-edge transmission is ideal, edge schedules massive streams to alleviate user conflicts in cell-free massive MIMO. Moreover, we propose a novel edge-to-client grouping algorithm for assigning the streams with severe interference to different time slots. The proposed algorithm achieves an innovative. 7-approximation ratio while keeping the group sizes within a certain range. When the server-to-edge transmission is non-ideal, we design a special transmission framework called Aggressive, wherein the server sends the next video chunk when the current chunk reaches the edge rather than the client. Thus, the proposed framework saves considerable server-to-edge latency compared with the traditional framework. Simulation results show that the proposed user grouping algorithm improves the achievable rate by around 18% and the Aggressive framework improves the average bitrate.
C1 [Wang, Zhaoyang; Liu, Guanghua; Jiang, Tao] Huazhong Univ Sci & Technol HUST, Res Ctr Mobile Commun 6G, Sch Cyber Sci & Engn, Wuhan 430074, Peoples R China.
   [Wang, Zhaoyang] Huazhong Univ Sci & Technol HUST, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Zhang, Dongchen; Gao, Peng] China Mobile Grp Design Inst Co Ltd, Beijing 100080, Peoples R China.
   [Hua, Xinhai; Xu, Lingmin] ZTE Corp, Dept Cloud Video Prod, Nanjing 210012, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; China Mobile; ZTE
RP Jiang, T (corresponding author), Huazhong Univ Sci & Technol HUST, Res Ctr Mobile Commun 6G, Sch Cyber Sci & Engn, Wuhan 430074, Peoples R China.
EM zywang1117@hust.edu.cn; guanghualiu@hust.edu.cn;
   zhangdongchen@cmdi.chinamobile.com; hua.xinhai@zte.com.cn;
   xu.lingmin@zte.com.cn; gaopeng@cmdi.chinamobile.com; Tao.Jiang@ieee.org
RI Liu, Guanghua/AFK-5493-2022; Gao, Peng/KBQ-3239-2024; wang,
   zhaoyang/HOA-8620-2023
OI Liu, Guanghua/0000-0003-0460-4453; 
FU National Key Ramp;D Program of China
FX No Statement Available
CR Polegre AA, 2020, IEEE INT CONF COMM, DOI 10.1109/iccworkshops49005.2020.9145215
   Andersson G, 1999, LECT NOTES COMPUT SC, V1563, P237
   [Anonymous], 2021, Dash Industry Forum, dash.js
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen Y, 2020, DIGIT COMMUN NETW, V6, P312, DOI 10.1016/j.dcan.2020.07.002
   Cisco, 2019, Cisco public white paper
   Denis J, 2021, IEEE T WIREL COMMUN, V20, P7360, DOI 10.1109/TWC.2021.3083139
   Dimic G, 2005, IEEE T SIGNAL PROCES, V53, P3857, DOI 10.1109/TSP.2005.855401
   Doumiati S, 2019, IEEE T COMMUN, V67, P7856, DOI 10.1109/TCOMM.2019.2931319
   Frieze A, 1997, ALGORITHMICA, V18, P67, DOI 10.1007/BF02523688
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Halder D, 2021, IEEE IC COMP COM NET, DOI 10.1109/ICCCN52240.2021.9522198
   Hao WM, 2018, IEEE T WIREL COMMUN, V17, P7691, DOI 10.1109/TWC.2018.2869749
   He QY, 2017, IEEE T MULTIMEDIA, V19, P1365, DOI 10.1109/TMM.2017.2652061
   Ngo HQ, 2018, IEEE T GREEN COMMUN, V2, P25, DOI 10.1109/TGCN.2017.2770215
   Ngo HQ, 2017, IEEE T WIREL COMMUN, V16, P1834, DOI 10.1109/TWC.2017.2655515
   Ngo HQ, 2013, IEEE T COMMUN, V61, P1436, DOI 10.1109/TCOMM.2013.020413.110848
   Lan Q, 2020, IEEE T WIREL COMMUN, V19, P5716, DOI 10.1109/TWC.2020.2995944
   Larsson EG, 2014, IEEE COMMUN MAG, V52, P186, DOI 10.1109/MCOM.2014.6736761
   Luo ZQ, 2010, IEEE SIGNAL PROC MAG, V27, P20, DOI 10.1109/MSP.2010.936019
   Ni W, 2014, IEEE T WIREL COMMUN, V13, P5382, DOI 10.1109/TWC.2014.2347973
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wang ZY, 2021, IEEE T VEH TECHNOL, V70, P11101, DOI 10.1109/TVT.2021.3110726
   Wang ZY, 2019, IEEE GLOBE WORK, DOI 10.1109/gcwkshps45667.2019.9024447
   Wei-Liang Shen, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1975, DOI 10.1109/INFOCOM.2015.7218581
   Wu XL, 2015, IEEE COMMUN LETT, V19, P1822, DOI 10.1109/LCOMM.2015.2458861
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Yoo T, 2006, IEEE J SEL AREA COMM, V24, P528, DOI 10.1109/JSAC.2005.862421
   YouTube, 2020, Recommended upload encoder, bitrate and resolution suggestions
   Yu Y., 2017, PROC IEEE GLOB COMMU, P1
   Zeng WB, 2021, IEEE T VEH TECHNOL, V70, P6190, DOI 10.1109/TVT.2021.3076440
NR 31
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8566
EP 8579
DI 10.1109/TMM.2023.3238551
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000005
DA 2024-07-18
ER

PT J
AU Wu, JJ
   Xia, CQ
   Yu, TS
   Li, J
AF Wu, Junjie
   Xia, Changqun
   Yu, Tianshu
   Li, Jia
TI View-Aware Salient Object Detection for 360° Omnidirectional Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Annotations; Object detection; Image edge detection; Image
   resolution; Transformers; Image segmentation; Salient object detection;
   panoramic dataset; view transformer; distortion
ID QUALITY ASSESSMENT; NETWORK; COMPRESSION; PREDICTION; MODEL
AB Image-based salient object detection (ISOD) in 360. scenarios is significant for understanding and applying panoramic information. However, research on 360. ISOD has not been widely explored due to the lack of large, complex, high-resolution, and well-labeled datasets. Towards this end, we construct a large scale 360. ISOD dataset with object-level pixel-wise annotation on equirectangular projection (ERP), which contains rich panoramic scenes with not less than 2K resolution and is the largest dataset for 360. ISOD by far to our best knowledge. By observing the data, we find current methods face three significant challenges in panoramic scenarios: diverse distortion degrees, discontinuous edge effects and changeable object scales. Inspired by humans' observing process, we propose a view-aware salient object detection method based on a Sample Adaptive View Transformer (SAVT) module with two sub-modules to mitigate these issues. Specifically, the sub-module View Transformer (VT) contains three transform branches based on different kinds of transformations to learn various features under different views and heighten the model's feature toleration of distortion, edge effects and object scales. Moreover, the sub-module Sample Adaptive Fusion (SAF) is to adjust the weights of different transform branches based on various sample features and make transformed enhanced features fuse more appropriately. The benchmark results of 20 state-of-the-art ISOD methods reveal the constructed dataset is very challenging. Moreover, exhaustive experiments verify the proposed approach is practical and outperforms the state-of-the-art methods.
C1 [Wu, Junjie; Yu, Tianshu] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Jia] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Xia, Changqun; Li, Jia] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Beihang University; Beihang University; Peng Cheng Laboratory
RP Li, J (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Xia, CQ (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM wujunjie@buaa.edu.cn; xiachq@pcl.ac.cn; yuts@buaa.edu.cn;
   jiali@buaa.edu.cn
RI Li, Jia/AAB-6431-2019; Yu, Tianshu/JEZ-7159-2023
OI Li, Jia/0000-0002-4346-8696; Yu, Tianshu/0000-0002-0250-2181; Wu,
   Junjie/0000-0002-9274-0836
FU National Natural Science Foundation of China [62132002, 62102206]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62132002 and 62102206. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Mr. Yuming Fang.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   BLANCHARD P, 1984, B AM MATH SOC, V11, P85, DOI 10.1090/S0273-0979-1984-15240-6
   Chao FY, 2021, IEEE T MULTIMEDIA, V23, P1811, DOI 10.1109/TMM.2020.3003642
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cheng MM, 2022, IEEE T PATTERN ANAL, V44, P8006, DOI 10.1109/TPAMI.2021.3107956
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   De Abreu A., 2017, PROC IEEE 9 INT C QU, P1, DOI 10.1109/QoMEX.2017.7965634
   De Simone F, 2016, PICT COD SYMP
   Fan D.-P., 2021, Scientia Sinica Informationis, V6, P6
   Fan DP, 2023, IEEE T PATTERN ANAL, V45, P2344, DOI 10.1109/TPAMI.2022.3166451
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR42600.2020.00299
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fang YM, 2022, AAAI CONF ARTIF INTE, P580
   Fang YM, 2018, SIGNAL PROCESS-IMAGE, V69, P1, DOI 10.1016/j.image.2018.07.009
   Fernandez-Labrador C, 2020, IEEE ROBOT AUTOM LET, V5, P1255, DOI 10.1109/LRA.2020.2967274
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang MK, 2020, IEEE SIGNAL PROC LET, V27, P1819, DOI 10.1109/LSP.2020.3028192
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Ke YY, 2022, IEEE WINT CONF APPL, P1360, DOI 10.1109/WACV51458.2022.00143
   Lee MS, 2022, Arxiv, DOI arXiv:2112.07380
   Li GB, 2021, COMPUT VIS IMAGE UND, V207, DOI 10.1016/j.cviu.2021.103207
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li J, 2021, IEEE T IMAGE PROCESS, V30, P6855, DOI 10.1109/TIP.2021.3099405
   Li J, 2020, IEEE J-STSP, V14, P38, DOI 10.1109/JSTSP.2019.2957982
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu JJ, 2020, IEEE T IMAGE PROCESS, V29, P8652, DOI 10.1109/TIP.2020.3017352
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P3804, DOI 10.1109/TIP.2021.3065239
   Lv HR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P682, DOI 10.1145/3394171.3413733
   Ma GX, 2020, IEEE T VIS COMPUT GR, V26, P3535, DOI 10.1109/TVCG.2020.3023636
   Ma MC, 2021, AAAI CONF ARTIF INTE, V35, P2311
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   Olsen J., 2010, The geometry of mobius transformations
   Pang YW, 2022, PROC CVPR IEEE, P2150, DOI 10.1109/CVPR52688.2022.00220
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Siris A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4136, DOI 10.1109/ICCV48922.2021.00412
   Su YC, 2017, ADV NEUR IN, V30
   Tateno K, 2018, LECT NOTES COMPUT SC, V11220, P732, DOI 10.1007/978-3-030-01270-0_43
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wen W, 2024, Arxiv, DOI arXiv:2206.08751
   Wikipedia contributors, 2022, Field of view-Wikipedia, the free encyclopedia
   Wilkins David R., 2017, Mobius transformations and stereographic projection
   Wu Z, 2021, IEEE T IMAGE PROCESS, V30, P6226, DOI 10.1109/TIP.2021.3093380
   Xia CQ, 2017, PROC CVPR IEEE, P4399, DOI 10.1109/CVPR.2017.468
   Xu BW, 2021, AAAI CONF ARTIF INTE, V35, P3004
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang KL, 2021, IEEE T IMAGE PROCESS, V30, P1866, DOI 10.1109/TIP.2020.3048682
   Yang S, 2021, IEEE T IMAGE PROCESS, V30, P8426, DOI 10.1109/TIP.2021.3113794
   Yang WY, 2018, INT C PATT RECOG, P2190, DOI 10.1109/ICPR.2018.8546070
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zhang Y, 2021, Arxiv, DOI [arXiv:2107.11629, 10.48550/arXiv.2107.11629]
   Zhang Y, 2021, Arxiv, DOI arXiv:2105.11578
   Zhang Y, 2020, IEEE IMAGE PROC, P3458, DOI 10.1109/ICIP40778.2020.9191158
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhao PY, 2020, AAAI CONF ARTIF INTE, V34, P12959
   Zhao ZR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4967, DOI 10.1145/3474085.3475494
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou Y, 2022, IEEE T CIRC SYST VID, V32, P1767, DOI 10.1109/TCSVT.2021.3081162
NR 78
TC 3
Z9 3
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6471
EP 6484
DI 10.1109/TMM.2022.3209015
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500059
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, N
   Wang, JY
   Tian, Y
   Zhang, RK
   Mao, WJ
AF Xu, Nan
   Wang, Junyan
   Tian, Yuan
   Zhang, Ruike
   Mao, Wenji
TI AnANet: Association and Alignment Network for Modeling Implicit
   Relevance in Cross-Modal Correlation Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Association and alignment network; classification scheme; cross-modal
   correlation; implicit relevance
AB With the explosive increase of multimodal data, cross-modal correlation classification has become an important research topic and is in great demand in many cross-modal applications. A variety of classification schemes and predictive models have been built based on the existing cross-modal correlation categorization. However, these classification schemes typically follow the prior assumption that the paired cross-modal samples are strictly related, and thus pay great attention to the fine-grained relevant types of cross-modal correlation, ignoring the high volume of implicitly relevant data which are often wrongly classified into irrelevant types. Even more, previous predictive models fall short of reflecting the essence of cross-modal correlation according to their definitions, especially in the modeling of network structure. Thus in this paper, by comprehensively investigating the current image-text correlation classification research, we redefine a new classification scheme for cross-modal correlation based on the implicit and explicit relevance. To predict the types of image-text correlation based on our proposed definition, we further devise the Association and Alignment Network (namely AnANet) to model the implicit and explicit relevance, which captures both the implicit association of global discrepancy and commonality between image and text and explicit alignment of cross-modal local relevance. Experimental studies on our constructed new image-text correlation dataset verify the effectiveness of our proposed model.
C1 [Xu, Nan; Tian, Yuan; Zhang, Ruike; Mao, Wenji] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Xu, Nan; Wang, Junyan] Beijing Wenge Technol Co Ltd, Beijing 100190, Peoples R China.
   [Tian, Yuan; Zhang, Ruike; Mao, Wenji] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Mao, WJ (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.; Mao, WJ (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
EM xunan2015@ia.ac.cn; junyan.wang@wenge.com; tianyuan2021@ia.ac.cn;
   zhangruike2020@ia.ac.cn; wenji.mao@ia.ac.cn
RI XU, nan/KDP-0628-2024
OI Mao, Wenji/0000-0003-2323-5091
FU Ministry of Science and Technology of China [2020AAA0108405]; National
   Natural Science Foundation of China [62206287, 11832001, 71621002]
FX This work was supported in part by the Ministry of Science and
   Technology of China under Grant #2020AAA0108405, and in part by the
   National Natural Science Foundation of China under Grants #62206287,
   #11832001, and #71621002.
CR Alikhani M., 2020, P 58 ANN M ASS COMP, P6525, DOI DOI 10.18653/V1/2020.ACL-MAIN.583
   Alikhani M, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P570
   Botach A, 2022, PROC CVPR IEEE, P4975, DOI 10.1109/CVPR52688.2022.00493
   Chen T., 2013, P 21 ACM INT C MULT, P781, DOI [DOI 10.1145/2502081.2502203, 10.1145/2502081, DOI 10.1145/2502081]
   Chen T, 2015, AAAI CONF ARTIF INTE, P30
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Feng Yansong., 2008, P ACL 08 HLT, P272
   Henning C, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P14, DOI 10.1145/3078971.3078991
   Huang T.-H., 2016, NAACL HLT, P1233
   Hussain Z, 2017, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.2017.123
   Jas M, 2015, PROC CVPR IEEE, P2727, DOI 10.1109/CVPR.2015.7298889
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Kruk J, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4622
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Liang B, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4707, DOI 10.1145/3474085.3475190
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ma RF, 2021, IEEE T KNOWL DATA EN, V33, P388, DOI 10.1109/TKDE.2019.2932406
   Marsh EE, 2003, J DOC, V59, P647, DOI 10.1108/00220410310506303
   Martinec R., 2005, VISUAL COMMUNICATION, V4, P337, DOI DOI 10.1177/1470357205055928
   McCloudandADManning Scott, 1998, IEEE T PROFESSIONAL, V41, P66, DOI DOI 10.1109/TPC.1998.661632
   Otto C, 2020, INT J MULTIMED INF R, V9, P31, DOI 10.1007/s13735-019-00187-6
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Sun L, 2021, AAAI CONF ARTIF INTE, V35, P13860
   Vaswani A, 2017, ADV NEUR IN, V30
   Vempala A, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2830
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang L, 2020, IEEE T CYBERNETICS, V50, P3330, DOI 10.1109/TCYB.2019.2894498
   Wang Z., 2014, ACM Trans. Multimedia Comput. Commun. Appl. (TOMM), V10, P1
   Wu S.X., 2014, Theory and Practice in Language Studies, V4, P1415, DOI [10.4304/tpls.4.7.1415- 1420, DOI 10.4304/TPLS.4.7.1415-1420]
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2019, AAAI CONF ARTIF INTE, P371
   Xu N, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3777
   Xu N, 2018, ACM/SIGIR PROCEEDINGS 2018, P929, DOI 10.1145/3209978.3210093
   Xue F, 2020, IEEE T MULTIMEDIA, V22, P2098, DOI 10.1109/TMM.2019.2951194
   Yang Erkun, 2022, CVPR, P7551
   Yang Muli, 2020, P IEEECVF CVPR, P10248
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P2839, DOI 10.1109/TIP.2022.3161832
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zeng ZX, 2022, IEEE INTELL SYST, V37, P45, DOI 10.1109/MIS.2022.3169884
   Zhai HJ, 2021, IEEE T CIRC SYST VID, V31, P742, DOI 10.1109/TCSVT.2020.2991171
   [Zhang Haihui 张海辉], 2005, [兰州大学学报. 自然科学版, Journal of Lanzhou University.Natural Science], V41, P93
   Zhang L., 2020, P IEEE INT C MULT EX, P1
   Zhang L, 2022, IEEE T MULTIMEDIA, V24, P1830, DOI 10.1109/TMM.2021.3073267
   Zhang Q, 2018, AAAI CONF ARTIF INTE, P5674
   Zhu JN, 2020, AAAI CONF ARTIF INTE, V34, P9749
   Zhu JN, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4154
NR 53
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7867
EP 7880
DI 10.1109/TMM.2022.3229960
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400020
DA 2024-07-18
ER

PT J
AU Yang, YL
   He, HJ
   Chen, F
   Yuan, Y
   Mao, NX
AF Yang, Yaolin
   He, Hongjie
   Chen, Fan
   Yuan, Yuan
   Mao, Ningxiong
TI Reversible Data Hiding in Encrypted Images Based on Time-Varying Huffman
   Coding Table
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Huffman coding; Encryption; Complexity theory; Indexes;
   Redundancy; Resists; Encrypted images; huffman coding; index class
   scrambling; reversible data hiding; time-varying
ID PERMUTATION; CRYPTANALYSIS; SECURITY
AB Image privacy protection and management face many challenges, such as privacy disclosure, copyright dispute, and traceability difficulties, with the development of big data. Reversible data hiding in encrypted images (RDHEI) has been widely considered as an effective means to tackle these challenges. In this paper, a RDHEI based on time-varying Huffman coding table (TV-HCT) method is proposed to improve the security, embedding rate (ER) and efficiency. First, the initial HCT is generated according to the prediction errors of an image, which can improve compression performance. And then, the TV-HCT is obtained by scrambling equal-length codewords in the initial HCT using timestamps. This realizes the time variability of compression coding stream (CCS) of an image in that the image TV-HCT has large change space. Analysis shows that the average change space of TV-HCT in UCID is 3.97x10(327), and the average ER of three databases is more than 0.44 bpp higher than the existing algorithms. Finally, the CCS is encrypted using the designed index class scrambling method to balance complexity and security. The proposed method not only strengthens the security against brute force attack and differential attack, but also improves ER and efficiency of the RDHEI technique. Experimental results and performance analysis demonstrate that the proposed algorithm outperforms the state-of-the-art RDHEI algorithms in terms of the security, ER and complexity.
C1 [Yang, Yaolin; He, Hongjie; Yuan, Yuan; Mao, Ningxiong] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 611756, Peoples R China.
   [Chen, Fan] Southwest Jiaotong Univ, Sch Comp & Artificial Intelligence, Chengdu 611756, Peoples R China.
C3 Southwest Jiaotong University; Southwest Jiaotong University
RP Chen, F (corresponding author), Southwest Jiaotong Univ, Sch Comp & Artificial Intelligence, Chengdu 611756, Peoples R China.
EM ylyangwr@foxmail.com; hjhe@swjtu.edu.cn; fchen@swjtu.edu.cn;
   ytuanyuan@my.swjtu.edu.cn; 502441062@qq.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas P., 2017, Image database of BOWS-2
   Chen F, 2023, IEEE T MULTIMEDIA, V25, P2864, DOI 10.1109/TMM.2021.3132168
   Chen F, 2021, IEEE T CIRC SYST VID, V31, P905, DOI 10.1109/TCSVT.2020.2992817
   Dhall S, 2022, J KING SAUD UNIV-COM, V34, P1533, DOI 10.1016/j.jksuci.2018.09.015
   Fu YJ, 2019, INFORM SCIENCES, V494, P21, DOI 10.1016/j.ins.2019.04.043
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Jolfaei A, 2016, IEEE T INF FOREN SEC, V11, P235, DOI 10.1109/TIFS.2015.2489178
   Keshk M, 2022, WIREL NETW, V28, P1241, DOI 10.1007/s11276-018-01912-5
   Khelifi F, 2018, SIGNAL PROCESS, V148, P91, DOI 10.1016/j.sigpro.2018.02.016
   Khelifi F, 2018, SIGNAL PROCESS, V143, P336, DOI 10.1016/j.sigpro.2017.09.020
   Li SJ, 2008, SIGNAL PROCESS-IMAGE, V23, P212, DOI 10.1016/j.image.2008.01.003
   Liao X, 2015, J VIS COMMUN IMAGE R, V28, P21, DOI 10.1016/j.jvcir.2014.12.007
   Liu ZL, 2018, INFORM SCIENCES, V433, P188, DOI 10.1016/j.ins.2017.12.044
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2018, IEEE INT WORKS INFOR
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Puyang Y, 2018, IEEE INT WORKS INFOR
   Qian ZX, 2012, J SYST SOFTWARE, V85, P309, DOI 10.1016/j.jss.2011.08.015
   Qiu YQ, 2021, IEEE T CIRC SYST VID, V31, P1380, DOI 10.1109/TCSVT.2020.3006494
   Qu LF, 2022, IEEE T MULTIMEDIA, V24, P2924, DOI 10.1109/TMM.2021.3090588
   Qu LF, 2022, IEEE T CIRC SYST VID, V32, P920, DOI 10.1109/TCSVT.2021.3069811
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Singh KN, 2022, COGN COMPUT, DOI 10.1007/s12559-022-10040-4
   Singh OP, 2022, COMPUT COMMUN, V191, P368, DOI 10.1016/j.comcom.2022.05.010
   Wang P, 2020, IEEE ACCESS, V8, P28902, DOI 10.1109/ACCESS.2020.2972622
   Wang X, 2021, INFORM SCIENCES, V567, P375, DOI 10.1016/j.ins.2021.02.079
   Wang YM, 2022, IEEE T MULTIMEDIA, V24, P1288, DOI 10.1109/TMM.2021.3062699
   [吴友情 Wu Youqing], 2021, [计算机学报, Chinese Journal of Computers], V44, P846
   Wu YQ, 2020, IEEE T MULTIMEDIA, V22, P1929, DOI 10.1109/TMM.2019.2952979
   Yahi A, 2022, OPTIK, V249, DOI 10.1016/j.ijleo.2021.168290
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yi S, 2018, SIGNAL PROCESS, V150, P171, DOI 10.1016/j.sigpro.2018.04.016
   Yin ZX, 2021, SIGNAL PROCESS, V187, DOI 10.1016/j.sigpro.2021.108146
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Yin ZX, 2014, SCI WORLD J, DOI 10.1155/2014/604876
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
NR 41
TC 1
Z9 1
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8607
EP 8619
DI 10.1109/TMM.2023.3238549
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000022
DA 2024-07-18
ER

PT J
AU Yang, YR
   Sun, X
   Diao, WH
   Rong, XE
   Yan, SY
   Yin, DS
   Li, XM
AF Yang, Yiran
   Sun, Xian
   Diao, Wenhui
   Rong, Xuee
   Yan, Shiyao
   Yin, Dongshuo
   Li, Xinming
TI Optimal Partition Assignment for Universal Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Label assignment; object detection
AB The label assignment problem is a core task in object detection, which mainly focuses on how to define the positive/negative samples during the training phase. Recent works have proved that label assignment is significant for performance improvement of the detector. In this article, we propose an exquisite strategy that can dynamically assign labels according samples' joint scores (classification and location). Moreover, our strategy can apply to both 2D and 3D monocular detectors. In our strategy, we formulate label assignment as an optimization problem. Concretely, we first calculate the classification and location costs of each sample, which are treated as points in a 2-D coordinate system. Then an optimal divider line that minimizes the sum of point-to-line distances is designed to separate the positive/negative samples. An iterative Genetic Algorithm is employed in acquiring the optimal solution. Furthermore, a GIoU auxiliary branch is devised to keep sample selection consistent during the training and testing phase. Benefitting from the non-maximum suppression (NMS) that utilizes the joint scores of classification and location, excellent detection performance is achieved. Extensive experiments conducted on MS COCO, PASCAL VOC (2D object detection), and KITTI (3D object detection) verify the effectiveness and universality of our proposed Optimal Partition Assignment (OPA).
C1 [Yang, Yiran; Sun, Xian; Diao, Wenhui; Rong, Xuee; Yan, Shiyao; Yin, Dongshuo; Li, Xinming] Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100190, Peoples R China.
   [Yang, Yiran; Sun, Xian; Diao, Wenhui; Rong, Xuee; Yan, Shiyao; Yin, Dongshuo; Li, Xinming] Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Network Informat Syst Technol NIST, Beijing 100190, Peoples R China.
   [Yang, Yiran; Sun, Xian; Rong, Xuee; Yan, Shiyao; Yin, Dongshuo] Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Yang, Yiran; Sun, Xian; Rong, Xuee; Yan, Shiyao; Yin, Dongshuo] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Yang, Yiran; Sun, Xian; Rong, Xuee; Yan, Shiyao; Yin, Dongshuo] Univ Chinese Acad Sci, Sch Elect Elect & Commun Eng, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Aerospace Information Research Institute,
   CAS; Chinese Academy of Sciences; Aerospace Information Research
   Institute, CAS; Chinese Academy of Sciences; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Sun, X (corresponding author), Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100190, Peoples R China.
EM yangyiran19@mails.ucas.ac.cn; sunxian@mail.ie.ac.cn;
   diaowh@aircas.ac.cn; rongxuee19@mails.ucas.ac.cn;
   yanshiyao19@mails.ucas.ac.cn; yindongshuo19@mails.ucas.ac.cn;
   13911729321@139.com
OI Sun, Xian/0000-0002-0038-9816; Yin, Dongshuo/0000-0003-4666-4438; Yang,
   Yiran/0000-0003-2688-6340; Xuee, Rong/0000-0002-7681-7020
FU National Natural Science Foundation of China [62171436]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62171436
CR Brazil G, 2019, IEEE I CONF COMP VIS, P9286, DOI 10.1109/ICCV.2019.00938
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen L, 2021, IEEE T MULTIMEDIA, V23, P4297, DOI 10.1109/TMM.2020.3040539
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P1558, DOI 10.1109/TMM.2021.3067439
   Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236
   Chen XZ, 2015, ADV NEUR IN, V28
   Chen ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4939, DOI 10.1145/3474085.3475351
   Chenchen Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P91, DOI 10.1007/978-3-030-58545-7_6
   Dai J., 2021, ICLR
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Everingham M., 2011, PATTERN ANAL STAT MO, V8, P5
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   He T, 2019, AAAI CONF ARTIF INTE, P8409
   Hengduo Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10585, DOI 10.1109/CVPR42600.2020.01060
   Huang T. S., 2016, P 24 ACM INT C MULT, P516, DOI 10.1145/2964284.2967274
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li Peixuan, 2020, EUROPEAN C COMPUTER, P644
   Li S, 2022, PROC CVPR IEEE, P9377, DOI 10.1109/CVPR52688.2022.00917
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu LJ, 2019, PROC CVPR IEEE, P1057, DOI 10.1109/CVPR.2019.00115
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Simonelli A, 2019, IEEE I CONF COMP VIS, P1991, DOI 10.1109/ICCV.2019.00208
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang T., 2021, P CORL, P1475
   Wang T, 2021, IEEE INT CONF COMP V, P913, DOI 10.1109/ICCVW54120.2021.00107
   Wang XL, 2020, AAAI CONF ARTIF INTE, V34, P12257
   Wang Y, 2019, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2019.00864
   Wu S, 2022, IEEE T MULTIMEDIA, V24, P2058, DOI 10.1109/TMM.2021.3075323
   Xie J, 2023, IEEE T MULTIMEDIA, V25, P2153, DOI 10.1109/TMM.2022.3143707
   Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249
   Xu YJ, 2021, IEEE T IMAGE PROCESS, V30, P5782, DOI 10.1109/TIP.2021.3085208
   Ye SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6423, DOI 10.1109/ICCV48922.2021.00638
   Yuhang Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11580, DOI 10.1109/CVPR42600.2020.01160
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhu BJ, 2020, Arxiv, DOI arXiv:2007.03496
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
NR 47
TC 1
Z9 1
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7582
EP 7593
DI 10.1109/TMM.2022.3223780
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000061
DA 2024-07-18
ER

PT J
AU Ye, XL
   Zhao, JY
AF Ye, Xulun
   Zhao, Jieyu
TI Graph Convolutional Network With Unknown Class Number
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clustering; dirichlet process; graph convolutional network; open set
   learning; variational inference
ID DIRICHLET PROCESS MIXTURES; CLASSIFICATION; INFERENCE; MULTIPLE; SPACE
AB The graph convolutional network (GCN), as a powerful tool in graph data processing, is widely exploited in many machine learning and computer vision tasks. However, existing GCNs usually assume that the network has fixed outputs, which is usually contrary to the real-world class number being unknown and incremental, leading to an open set classification problem in which the finite training dataset cannot contain all labels in the infinite testing data. To overcome these issues, a novel Bayesian model is proposed, in which we couple GCN and a deep generative clustering model in a unified framework. In our model, the GCN model is used to detect the known classes, the deep generative clustering model is designed to generate the novel classes, and a two-level label generative process is constructed to extend the finite GCN outputs to infinity and fuse the label generated by the GCN model and the deep generative model. Although posterior inference is difficult, our model leads to an efficient variational inference-based optimization method. Experiments on various datasets validate our theoretical analysis and demonstrate that our model can achieve state-of-the-art performance. Our source code has been released on the website.
C1 [Ye, Xulun; Zhao, Jieyu] Ningbo Univ, Inst Comp Sci & Technol, Ningbo 315211, Zhejiang, Peoples R China.
C3 Ningbo University
RP Zhao, JY (corresponding author), Ningbo Univ, Inst Comp Sci & Technol, Ningbo 315211, Zhejiang, Peoples R China.
EM yexlwh@163.com; zhao_jieyu@nbu.edu.cn
OI Zhao, Jieyu/0000-0002-1013-557X
FU National Natural Science Foundation of China [62006131, 62071260];
   National Natural Science Foundation of Zhejiang Province [LQ21F020009,
   LQ18F020001]; Open Project Program of the State Key Laboratory of CAD&CG
   in Zhejiang University [A2007]; K. C. Wong Magna Fund in Ningbo
   University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62006131 and 62071260,in part by the
   National Natural Science Foundation of Zhejiang Province under Grants
   LQ21F020009 and LQ18F020001, in part by the Open Project Program of the
   State Key Laboratory of CAD&CG in Zhejiang University under Grant A2007,
   and in part by the K. C. Wong Magna Fund in Ningbo University.
CR Akoglu L, 2015, DATA MIN KNOWL DISC, V29, P626, DOI 10.1007/s10618-014-0365-y
   Basu S, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P979
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Chen JT, 2020, PROC CVPR IEEE, P389, DOI 10.1109/CVPR42600.2020.00047
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng C., 2007, P C COMP VIS PATT RE, P1
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gholami B, 2017, PROC CVPR IEEE, P4313, DOI 10.1109/CVPR.2017.459
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Guo R, 2022, IEEE T MULTIMEDIA, V24, P1583, DOI 10.1109/TMM.2021.3068609
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hannah LA, 2011, J MACH LEARN RES, V12, P1923
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ji P, 2017, ADV NEUR IN, V30
   Jiang B, 2019, PROC CVPR IEEE, P11305, DOI 10.1109/CVPR.2019.01157
   Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965
   Kingma D. P., 2014, arXiv
   Kipf TN, 2016, ARXIV
   Law M. T., 2017, P INT C MACH LEARN, P1985
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li TP, 2022, IEEE T MULTIMEDIA, V24, P492, DOI 10.1109/TMM.2021.3054526
   Li X, 2017, IEEE I CONF COMP VIS, P5775, DOI 10.1109/ICCV.2017.615
   Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Mesgaran M, 2021, IEEE T MULTIMEDIA, V23, P3931, DOI 10.1109/TMM.2020.3034530
   Mu X, 2017, IEEE T KNOWL DATA EN, V29, P1605, DOI 10.1109/TKDE.2017.2691702
   Nene S. A., 1996, Tech. Rep. CUCS-005-96
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Nie FP, 2011, IEEE T NEURAL NETWOR, V22, P1796, DOI 10.1109/TNN.2011.2162000
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P3793, DOI 10.1109/TMM.2020.3032037
   Oza P, 2019, PROC CVPR IEEE, P2302, DOI 10.1109/CVPR.2019.00241
   Peng X, 2017, AAAI CONF ARTIF INTE, P2478
   Pham AT, 2015, PR MACH LEARN RES, V37, P2427
   Pu YC, 2016, ADV NEUR IN, V29
   Qi G.-J., 2013, Proceedings of the sixth ACM international conference on Web search and data mining, P617
   Qi G.-J., 2012, Proc. of the 5th ACM Intl. Conf. on Web Search and Data Mining (WSDM), P553, DOI DOI 10.1145/2124295.2124363
   Rasmussen C. E., 2002, Advances in Neural Information Processing Systems, P881
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Shaham U., 2018, ARXIV180101587, P1, DOI DOI 10.1109/PUNECON.2018.8745417
   Shahbaba B, 2009, J MACH LEARN RES, V10, P1829
   Simo-Serra E, 2017, INT J COMPUT VISION, V122, P388, DOI 10.1007/s11263-016-0941-2
   Straub J, 2018, IEEE T PATTERN ANAL, V40, P235, DOI 10.1109/TPAMI.2017.2662686
   Straub J, 2017, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2017.258
   Sun X., 2020, P IEEE CVF C COMP VI, P13480
   Tao ZQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3562
   Tapaswi M, 2019, IEEE I CONF COMP VIS, P5026, DOI 10.1109/ICCV.2019.00513
   van den Berg R, 2017, Arxiv, DOI arXiv:1706.02263
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang YN, 2014, AAAI CONF ARTIF INTE, P2135
   Wang YN, 2015, PR MACH LEARN RES, V37, P862
   Wang Y, 2011, IEEE T NEURAL NETWOR, V22, P1149, DOI 10.1109/TNN.2011.2147798
   Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu TF, 2004, J MACH LEARN RES, V5, P975
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xue Tianfan, 2016, Advances in Neural Information Processing Systems, P91
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang L, 2019, PROC CVPR IEEE, P2293, DOI 10.1109/CVPR.2019.00240
   Ye XL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2133, DOI 10.1145/3343031.3350979
   Yoshihashi R, 2019, PROC CVPR IEEE, P4011, DOI 10.1109/CVPR.2019.00414
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
   Zhou P, 2018, PROC CVPR IEEE, P1596, DOI 10.1109/CVPR.2018.00172
   Zhu Y, 2017, AAAI CONF ARTIF INTE, P2977
NR 66
TC 1
Z9 1
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4800
EP 4813
DI 10.1109/TMM.2022.3183401
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300014
DA 2024-07-18
ER

PT J
AU Ye, YL
   Pan, TJ
   Luo, T
   Li, JJ
   Shen, HT
AF Ye, Yalan
   Pan, Tongjie
   Luo, Tonghoujun
   Li, Jingjing
   Shen, Heng Tao
TI Learning MLatent Representations for Generalized Zero-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Mutual information; Image color analysis;
   Generative adversarial networks; Task analysis; Cows; Generative
   adversarial network; latent representations; mutual information;
   semantic-relevant; semantic-irrelevant; zero-shot learning
AB In generative adversarial network (GAN) based zero-shot learning (ZSL) approaches, the synthesized unseen visual features are inevitably prone to seen classes since the feature generator is merely trained on seen references, which causes the inconsistency between visual features and their corresponding semantic attributes. This visual-semantic inconsistency is primarily induced by the non-preserved semantic-relevant components and the non-rectified semantic-irrelevant low-level visual details. Existing generative models generally tackle the issue by aligning the distribution of the two modalities with an additional visual-to-semantic embedding, which tends to cause the hubness problem and ruin the diversity of visual modality. In this paper, we propose a novel generative model named learning modality-consistent latent representations GAN (LCR-GAN) to address the problem via embedding the visual features and their semantic attributes into a shared latent space. Specifically, to preserve the semantic-relevant components, the distributions of the two modalities are aligned by maximizing the mutual information between them. And to rectify the semantic-irrelevant visual details, the mutual information between original visual features and their latent representations is confined within an appropriate range. Meanwhile, the latent representations are decoded back to both modalities to further preserve the semantic-relevant components. Extensive evaluations on four public ZSL benchmarks validate the superiority of our method over other state-of-the-art methods.
C1 [Ye, Yalan; Pan, Tongjie; Luo, Tonghoujun; Li, Jingjing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, JJ (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM yalanye@uestc.edu.cn; tongjiepan@foxmail.com;
   tonghoujun.luo@outlook.com; lijin117@yeah.net; shenhengtao@hotmail.com
RI Li, Jingjing/T-6522-2019; ye, yalan/JWP-9553-2024; Shen, Heng
   Tao/ABD-5331-2021
OI Pan, Tongjie/0000-0002-1538-5789
FU National Natural Science Foundation of China [61976047]; Science &
   Technology Department of Sichuan Province of China [22ZDYF2694,
   2020YFG0080]; Fundamental Research Funds for the Central Universities
   [ZYGX2021YGLH016]; Sichuan University West China Nursing Discipline
   Development Special Fund Project [HXHL21004]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61976047, in part by the Science &
   Technology Department of Sichuan Province of China under Grants
   22ZDYF2694 and 2020YFG0080, in part by the Fundamental Research Funds
   for the Central Universities under Grant ZYGX2021YGLH016, and in part by
   the Sichuan University West China Nursing Discipline Development Special
   Fund Project under Grant HXHL21004.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Chen Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3413, DOI 10.1145/3394171.3413813
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia Z, 2020, IEEE T IMAGE PROCESS, V29, P1958, DOI 10.1109/TIP.2019.2947780
   Kingma D. P., 2014, arXiv
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li JJ, 2022, IEEE T KNOWL DATA EN, V34, P5770, DOI 10.1109/TKDE.2021.3060473
   Li JJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1348, DOI 10.1145/3394171.3413503
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2022, IEEE T CYBERNETICS, V52, P8167, DOI 10.1109/TCYB.2021.3050803
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1587, DOI 10.1145/3343031.3350901
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Liu B, 2020, AAAI CONF ARTIF INTE, V34, P11547
   Liu Y, 2021, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR46437.2021.00379
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Lu Z., 2020, PROC IEEECVF C COMPU, P682
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Pambala AK, 2020, IEEE WINT CONF APPL, P1226, DOI [10.1109/WACV45572.2020.9093625, 10.1109/wacv45572.2020.9093625]
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Peng X. B., 2018, PROC INT C LEARN REP
   Radovanovic M, 2010, J MACH LEARN RES, V11, P2487
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Romera-Paredes Bernardino, 2015, ICML
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Shermin T, 2022, IEEE T IMAGE PROCESS, V31, P721, DOI 10.1109/TIP.2021.3135480
   Vyas Maunil R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P70, DOI 10.1007/978-3-030-58577-8_5
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang CQ, 2021, AAAI CONF ARTIF INTE, V35, P2710
   Wang ZY, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P2080, DOI 10.1145/3459637.3482471
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328
   Xing Y, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102961
   Ye YL, 2022, IEEE T MULTIMEDIA, V24, P1325, DOI 10.1109/TMM.2021.3063616
   Ye ZH, 2019, IEEE INT CON MULTI, P85, DOI 10.1109/ICME.2019.00023
   Yunlong Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14032, DOI 10.1109/CVPR42600.2020.01405
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
   Zongyan Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12862, DOI 10.1109/CVPR42600.2020.01288
NR 47
TC 5
Z9 5
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2252
EP 2265
DI 10.1109/TMM.2022.3145237
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100051
DA 2024-07-18
ER

PT J
AU Yue, GH
   Cheng, D
   Li, LD
   Zhou, TW
   Liu, HT
   Wang, TF
AF Yue, Guanghui
   Cheng, Di
   Li, Leida
   Zhou, Tianwei
   Liu, Hantao
   Wang, Tianfu
TI Semi-Supervised Authentically Distorted Image Quality Assessment With
   Consistency-Preserving Dual-Branch Convolutional Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Feature extraction; Training; Image quality; Task analysis;
   Social networking (online); Semantics; Image quality assessment;
   authentical distortion; consistency-preserving; semi-supervised
ID SCREEN CONTENT; PREDICTION
AB Recently, convolutional neural networks (CNNs) have provided a favoured prospect for authentically distorted image quality assessment (IQA). For good performance, most existing CNN-based methods rely on a large amount of labeled data for training, which is time-consuming and cumbersome to collect. By simultaneously exploiting few labeled data and many unlabeled data, we make a pioneering attempt to propose a semi-supervised framework (termed SSLIQA) with consistency-preserving dual-branch CNN for authentically distorted IQA in this paper. The proposed SSLIQA introduces a consistency-preserving strategy and transfers two kinds of consistency knowledge from the teacher branch to the student branch. Concretely, SSLIQA utilizes the sample prediction consistency to train the student to mimic output activations of individual examples represented by the teacher. Considering that subjects often refer to previous analogous cases to make scoring decisions, SSLIQA computes the semantic relation among different samples in a batch and encourages the consistency of sample semantic relation between two branches to explore extra quality-related information. Benefiting from the consistency-preserving strategy, we can exploit numerous unlabeled data to improve network's effectiveness and generalization. Experimental results on three authentically distorted IQA databases show that the proposed SSLIQA is stably effective under different student-teacher combinations and different labeled-to-unlabeled data ratios. In addition, it points out a new way on how to achieve higher performance with a smaller network.
C1 [Yue, Guanghui; Cheng, Di; Wang, Tianfu] Shenzhen Univ, Hlth Sci Ctr,Sch Biomed Engn, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasoun, Shenzhen 518060, Peoples R China.
   [Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Zhou, Tianwei] Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF243AA, Wales.
C3 Shenzhen University; Xidian University; Shenzhen University; Cardiff
   University
RP Zhou, TW (corresponding author), Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China.
EM yueguanghui@szu.edu.cn; chengdigogogo@outlook.com; ldli@xidian.edu.cn;
   tianwei@szu.edu.cn; liuh35@cardiff.ac.uk; tfwang@szu.edu.cn
RI Zhou, Tianwei/GSI-8460-2022
FU National Natural Science Foundation of China [62001302, 62103286];
   Guangdong Basic and Applied Basic Research Foundation [2019A1515111205,
   2021A1515011348, 2019A1515110401]; Natural Science Foundation of
   Shenzhen [JCYJ20190808145011259]; Shenzhen Science and Technology
   Program [RCBS20200714114920379]; Tencent Rhinoceros Birds-Scientific
   Research Foundation for Young Teachers of Shenzhen University; Open
   Project Program of StateKey Laboratory of Virtual Reality Technology and
   Systems, Beihang University [VRLAB2021C05]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 62001302 and 62103286, in part by Guangdong Basic
   and Applied Basic Research Foundation under Grants 2019A1515111205,
   2021A1515011348 and 2019A1515110401, in part by Natural Science
   Foundation of Shenzhen under Grant JCYJ20190808145011259, in part by
   Shenzhen Science and Technology Program under Grant
   RCBS20200714114920379, in part by the Tencent Rhinoceros
   Birds-Scientific Research Foundation for Young Teachers of Shenzhen
   University, and in part by the Open Project Program of StateKey
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   under Grant VRLAB2021C05. The Associate Editor coordinating the review
   of this manuscript and approving it for publication was Dr. Mai Xu.
CR Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bovik AC, 2013, P IEEE, V101, P2008, DOI 10.1109/JPROC.2013.2257632
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Golestaneh S Alireza, 2022, P IEEE CVF WINT C AP, P1220
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2019, IEEE SIGNAL PROC LET, V26, P1867, DOI 10.1109/LSP.2019.2951533
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Jiang XH, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116181
   Jiang XH, 2020, J VIS COMMUN IMAGE R, V67, DOI 10.1016/j.jvcir.2019.102745
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laine S, 2017, Arxiv, DOI [arXiv:1610.02242, DOI 10.48550/ARXIV.1610.02242]
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li LD, 2022, IEEE T CIRC SYST VID, V32, P8512, DOI 10.1109/TCSVT.2021.3112197
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P122, DOI 10.1109/LSP.2013.2294333
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Li SM, 2019, IEEE INT CON MULTI, P448, DOI 10.1109/ICME.2019.00084
   Lin HH, 2018, Arxiv, DOI arXiv:1803.08489
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Nafchi HZ, 2018, IEEE T BROADCAST, V64, P518, DOI 10.1109/TBC.2018.2818402
   Ou FZ, 2022, IEEE T MULTIMEDIA, V24, P4197, DOI 10.1109/TMM.2021.3114551
   Pan Zhaoqing, 2023, IEEE Transactions on Artificial Intelligence, P148, DOI 10.1109/TAI.2022.3146804
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Saad MA, 2010, IEEE SIGNAL PROC LET, V17, P583, DOI 10.1109/LSP.2010.2045550
   Shen LL, 2021, NEUROCOMPUTING, V424, P132, DOI 10.1016/j.neucom.2020.10.024
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan MX, 2019, PR MACH LEARN RES, V97
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang SG, 2016, NEUROCOMPUTING, V174, P310, DOI 10.1016/j.neucom.2014.12.117
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xiang T, 2020, IEEE T MULTIMEDIA, V22, P1259, DOI 10.1109/TMM.2019.2938612
   Xu JH, 2021, IEEE T CIRC SYST VID, V31, P1724, DOI 10.1109/TCSVT.2020.3015186
   Jiang XH, 2020, NEUROCOMPUTING, V386, P30, DOI 10.1016/j.neucom.2019.12.027
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   Yue GH, 2019, IEEE T MULTIMEDIA, V21, P2184, DOI 10.1109/TMM.2019.2913315
   Yue GH, 2019, DIGIT SIGNAL PROCESS, V91, P21, DOI 10.1016/j.dsp.2018.12.007
   Yue GH, 2019, IEEE T IMAGE PROCESS, V28, P2075, DOI 10.1109/TIP.2018.2875913
   Yue GH, 2017, J VIS COMMUN IMAGE R, V49, P382, DOI 10.1016/j.jvcir.2017.09.011
   Zhou W, 2020, INFORM SCIENCES, V528, P205, DOI 10.1016/j.ins.2020.04.030
   Zhou WJ, 2017, INFORM SCIENCES, V397, P1, DOI 10.1016/j.ins.2017.02.049
   Zhou WJ, 2018, IEEE T IMAGE PROCESS, V27, P2086, DOI 10.1109/TIP.2018.2794207
   Zhou Y, 2022, IEEE T CIRC SYST VID, V32, P1767, DOI 10.1109/TCSVT.2021.3081162
NR 68
TC 12
Z9 12
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6499
EP 6511
DI 10.1109/TMM.2022.3209889
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500061
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhang, HM
   Xu, M
AF Zhang, Haimin
   Xu, Min
TI Recognition of Emotions in User-Generated Videos through Frame-Level
   Adaptation and Emotion Intensity Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Feature extraction; Emotion recognition; Task analysis; Computer
   architecture; Semantics; Adaptation models; Adversarial domain
   adaptation; emotion intensity learning; video emotion recognition
AB Recognition of emotions in user-generated videos has attracted considerable research attention. Most existing approaches focus on learning frame-level features and fail to consider frame-level emotion intensities which are critical for video representation. In this research, we aim to extract frame-level features and emotion intensities through transferring emotional information from an image emotion dataset. To achieve this goal, we propose an end-to-end network for joint emotion recognition and intensity learning with unsupervised adversarial adaptation. The proposed network consists of a classification stream, an intensity learning stream and an adversarial adaptation module. The classification stream is used to generate pseudo intensity maps with the class activation mapping method to train the intensity learning subnetwork. The intensity learning stream is built upon an improved feature pyramid network in which features from different scales are cross-connected. The adversarial adaptation module is employed to reduce the domain difference between the source dataset and target video frames. By aligning cross domain features, we enable our network to learn on the source data while generalizing to video frames. Finally, we apply a weighted sum pooling method to frame-level features and emotion intensities to generate video-level features. We evaluate the proposed method on two benchmark datasets, i.e., VideoEmotion-8 and Ekman-6. The experimental results show that the proposed method achieves improved performance compared to previous state-of-the-art methods.
C1 [Zhang, Haimin; Xu, Min] Univ Technol Sydney, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.
C3 University of Technology Sydney
RP Xu, M (corresponding author), Univ Technol Sydney, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.
EM Haimin.Zhang@uts.edu.au; Min.Xu@uts.edu.au
OI Zhang, Haimin/0000-0002-0021-3634
CR Abu-El-Haija Sami, 2016, arXiv, DOI [DOI 10.48550/ARXIV.1609.08675, DOI 10.48550/-ARXIV.1609.08675]
   [Anonymous], 2010, P NIPS
   [Anonymous], 2016, Proceedings of the 24th ACM international conference on Multimedia, DOI DOI 10.1145/2964284.2967196
   Baker B., 2017, ICLR
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Chen WF, 2017, IEEE I CONF COMP VIS, P1566, DOI 10.1109/ICCV.2017.173
   Colson B, 2007, ANN OPER RES, V153, P235, DOI 10.1007/s10479-007-0176-2
   Cui JQ, 2019, IEEE I CONF COMP VIS, P6211, DOI 10.1109/ICCV.2019.00661
   Donahue J, 2014, PR MACH LEARN RES, V32
   Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186
   Eigen D, 2014, ADV NEUR IN, V27
   Ekman P., 1982, EMOTION HUMAN FACE
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Hu SH, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3328997
   Lim JH, 2017, Arxiv, DOI arXiv:1705.02894
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jiang YG, 2014, AAAI CONF ARTIF INTE, P73
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma D. P., 2014, arXiv
   Lang PJ, 1998, BIOL PSYCHIAT, V44, P1248, DOI 10.1016/S0006-3223(98)00275-3
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu H., 2019, PROC INT C LEARN REP
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Panda R, 2018, LECT NOTES COMPUT SC, V11206, P594, DOI 10.1007/978-3-030-01216-8_36
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Plutchik R., 1986, EMOTION THEORY RES E, V3, P320
   Pramono RRA, 2019, IEEE I CONF COMP VIS, P61, DOI 10.1109/ICCV.2019.00015
   Rasheed Z, 2005, IEEE T CIRC SYST VID, V15, P52, DOI 10.1109/TCSVT.2004.839993
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136
   Sobel I., 1968, TALK STANFORD ARTIFI, P271
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Xu BH, 2019, IEEE INT CON MULTI, P406, DOI 10.1109/ICME.2019.00077
   Xu BH, 2018, IEEE T AFFECT COMPUT, V9, P255, DOI 10.1109/TAFFC.2016.2622690
   Xu BH, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P15, DOI 10.1145/2911996.2912006
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   Zhang HM, 2018, IEEE T MULTIMEDIA, V20, P2824, DOI 10.1109/TMM.2018.2808760
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhao SC, 2020, AAAI CONF ARTIF INTE, V34, P303
   Zhou BL, 2014, ADV NEUR IN, V27
   Zoph B., 2017, ICLR, P1, DOI DOI 10.1109/ICAIIC48513.2020.9065031
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 49
TC 0
Z9 0
U1 7
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 881
EP 891
DI 10.1109/TMM.2021.3134167
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900016
DA 2024-07-18
ER

PT J
AU Zhang, L
   Qiao, T
   Xu, M
   Zheng, N
   Xie, SC
AF Zhang, Li
   Qiao, Tong
   Xu, Ming
   Zheng, Ning
   Xie, Shichuang
TI Unsupervised Learning-Based Framework for Deepfake Video Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deepfake detection; unsupervised learning; video clustering; PRNU;
   noiseprint
ID SOURCE IDENTIFICATION; FACE MANIPULATION; NORMALIZED CUTS; CAMERA;
   MODEL; CNN
AB With the continuous development of computer hardware equipment and deep learning technology, it is easier for people to swap faces in videos by currently-emerging multimedia tampering tools, such as the most popular deepfake. It would bring a series of new threats of security. Although many forensic researches have focused on this new type of manipulation and achieved high detection accuracy, most of which are based on supervised learning mechanism with requiring a large number of labeled samples for training. In this paper, we first develop a novel unsupervised detection manner for identifying deepfake videos. The main fundamental behind our proposed method is that the face region in the real video is taken by the camera while its counterpart in the deepfake video is usually generated by the computer; the provenance of two videos is totally different. Specifically, our method includes two clustering stages based on Photo-Response Non-Uniformity (PRNU) and noiseprint feature. Firstly, the PRNU fingerprint of each video frame is extracted, which is used to cluster the full-size identical source video (regardless of its real or fake). Secondly, we extract the noiseprint from the face region of the video, which is used to identify (re-cluster for the task of binary classification) the deepfake sample in each cluster. Numerical experiments verify our proposed unsupervised method performs very well on our own dataset and the benchmark FF++ dataset. More importantly, its performance rivals that of the supervised-based state-of-the-art detectors.
C1 [Zhang, Li; Qiao, Tong; Xu, Ming; Zheng, Ning; Xie, Shichuang] Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou 310005, Peoples R China.
   [Zhang, Li; Qiao, Tong] Key Lab Blockchain & Cyberspace Governance Zhejian, Hangzhou 310005, Peoples R China.
   [Qiao, Tong] Henan Key Lab Cyberspace Situat Awareness, Zhengzhou 450001, Peoples R China.
C3 Hangzhou Dianzi University
RP Qiao, T; Xu, M (corresponding author), Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou 310005, Peoples R China.
EM zlmadein1996@gmail.com; tong.qiao@hdu.edu.cn; mxu@hdu.edu.cn;
   nzheng@hdu.edu.cn; shichuang_xie@163.com
OI Xu, Ming/0000-0001-9332-5258; Qiao, Tong/0000-0003-4912-2132; Xie,
   shichuang/0000-0002-3142-3968
FU Fundamental Research Funds for the Provincial Universities of Zhejiang
   [GK219909299001-007]; Open Projects Program of National Laboratory of
   Pattern Recognition
FX This work was supported in part by the Fundamental Research Funds for
   the Provincial Universities of Zhejiang under Grant GK219909299001-007,
   and in part by the Open Projects Program of National Laboratory of
   Pattern Recognition.
CR Abu-El-Haija Sami, 2016, arXiv, DOI [DOI 10.48550/ARXIV.1609.08675, DOI 10.48550/-ARXIV.1609.08675]
   Afchar D, 2018, IEEE INT WORKS INFOR
   Agarwal S., 2019, P IEEE C COMP VIS PA, P38, DOI DOI 10.1109/ICCV.2015.425
   Amerini I, 2014, SIGNAL PROCESS-IMAGE, V29, P831, DOI 10.1016/j.image.2014.07.003
   Amerini I, 2017, SIGNAL PROCESS-IMAGE, V57, P1, DOI 10.1016/j.image.2017.04.009
   Bing Han, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P320, DOI 10.1109/TBIOM.2021.3065735
   Bonettini N, 2021, INT C PATT RECOG, P5012, DOI 10.1109/ICPR48806.2021.9412711
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Cozzolino D, 2019, P IEEE C COMPUTER VI, P130, DOI DOI 10.1109/CVPRW47913.2019
   Cozzolino D, 2020, EURASIP J INF SECUR, V2020, DOI 10.1186/s13635-020-0101-7
   Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Du Y., 2021, Secur. Commun. Netw., V2021, P1
   Dufour N., 2019, Google AI Blog, V1, P1
   Fernandes S, 2019, IEEE INT CONF COMP V, P1721, DOI 10.1109/ICCVW.2019.00213
   Fung SO, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534089
   Gloe T, 2009, LECT NOTES COMPUT SC, V5806, P262, DOI 10.1007/978-3-642-04431-1_19
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   Hu J, 2022, IEEE T CIRC SYST VID, V32, P1089, DOI 10.1109/TCSVT.2021.3074259
   Iuliani M, 2021, IEEE ACCESS, V9, P52455, DOI 10.1109/ACCESS.2021.3070478
   Jiang X, 2021, IEEE T MULTIMEDIA, V23, P2602, DOI 10.1109/TMM.2020.3013449
   Khodabakhsh A., 2018, PROC IEEE INT C BIOM, P1
   Kodovsky J, 2012, PROC SPIE, V8303, DOI 10.1117/12.907495
   Koopman M., 2018, 20 IR MACH VIS IM PR, P133
   Korshunov P, 2018, Arxiv, DOI arXiv:1812.08685
   Kumar A, 2020, I W BIOMETRIC FORENS, DOI [10.1109/IDEA49133.2020.9170705, 10.1109/iwbf49977.2020.9107962]
   Li DZ, 2021, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR46437.2021.00573
   [李艳歌 Li Yange], 2018, [高分子通报, Polymer Bulletin], P46
   Li YZ, 2018, IEEE INT WORKS INFOR
   Lin XF, 2017, IEEE T INF FOREN SEC, V12, P793, DOI 10.1109/TIFS.2016.2636086
   Lukás J, 2006, PROC SPIE, V6072, DOI 10.1117/12.640109
   Lukás J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602
   Mandelli S, 2020, IEEE T INF FOREN SEC, V15, P14, DOI 10.1109/TIFS.2019.2918644
   Marra F, 2017, IEEE T INF FOREN SEC, V12, P2197, DOI 10.1109/TIFS.2017.2701335
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Nguyen TT, 2022, Arxiv, DOI arXiv:1909.11573
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Qiao T, 2019, IEEE T MULTIMEDIA, V21, P1077, DOI 10.1109/TMM.2018.2872863
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Sabir E., 2019, INTERFACES GUI, V3, P80
   Saito S, 2017, IEEE T INF FOREN SEC, V12, P2026, DOI 10.1109/TIFS.2017.2692683
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Singh RD, 2018, MULTIMEDIA SYST, V24, P211, DOI 10.1007/s00530-017-0538-9
   Sun ZK, 2021, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR46437.2021.00361
   Tan MX, 2019, PR MACH LEARN RES, V97
   Taspinar S, 2020, IEEE T INF FOREN SEC, V15, P3270, DOI 10.1109/TIFS.2020.2985544
   Tolosana R, 2020, INFORM FUSION, V64, P131, DOI 10.1016/j.inffus.2020.06.014
   Valsesia D, 2015, IEEE T MULTIMEDIA, V17, P1439, DOI 10.1109/TMM.2015.2455417
   Verdoliva L, 2020, IEEE J-STSP, V14, P910, DOI 10.1109/JSTSP.2020.3002101
   Vo NN, 2016, LECT NOTES COMPUT SC, V9905, P494, DOI 10.1007/978-3-319-46448-0_30
   Wang JW, 2022, IEEE T MULTIMEDIA, V24, P230, DOI 10.1109/TMM.2021.3050057
   Wodajo D, 2021, Arxiv, DOI arXiv:2102.11126
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yao HW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20226668
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhao TC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15003, DOI 10.1109/ICCV48922.2021.01475
   Zhao YH, 2019, MULTIMED TOOLS APPL, V78, P8247, DOI 10.1007/s11042-018-6809-4
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
   Zhou P, 2017, IEEE COMPUT SOC CONF, P1831, DOI 10.1109/CVPRW.2017.229
NR 62
TC 10
Z9 10
U1 11
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4785
EP 4799
DI 10.1109/TMM.2022.3182509
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300013
DA 2024-07-18
ER

PT J
AU Zhang, MR
   Li, MD
   Yu, JH
   Chen, L
AF Zhang, Mingrui
   Li, Mading
   Yu, Jiahao
   Chen, Li
TI Aesthetic Photo Collage With Deep Reinforcement Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic assessment; photo collage; reinforcement learning
AB Photo collage aims to automatically arrange multiple photos on a given canvas with high aesthetic quality. Existing methods are based mainly on handcrafted feature optimization, which cannot adequately capture high-level human aesthetic senses. Deep learning provides a promising way, but owing to the complexity of collage and lack of training data, a solution has yet to be found. In this paper, we propose a novel pipeline for automatic generation of aspect ratio specified collage and the reinforcement learning technique is introduced in non-content-preserving collage. Inspired by manual collages, we model the collage generation as a sequential decision process to adjust spatial positions, orientation angles, placement order and the global layout. To instruct the agent to improve both the overall layout and local details, the reward function is specially designed for collage, considering subjective and objective factors. To overcome the lack of training data, we pretrain our deep aesthetic network on a large scale image aesthetic dataset (CPC) for general aesthetic feature extraction and propose an attention fusion module for structural collage feature representation. We test our model against competing methods on movie and image datasets and our results outperform others in several quality evaluations. Further user studies are also conducted to demonstrate the effectiveness.
C1 [Zhang, Mingrui; Yu, Jiahao; Chen, Li] Tsinghua Univ, Sch Software & BNRist, Beijing 100084, Peoples R China.
   [Li, Mading] Kuaishou, Video Technol Team, Beijing 100085, Peoples R China.
C3 Tsinghua University
RP Chen, L (corresponding author), Tsinghua Univ, Sch Software & BNRist, Beijing 100084, Peoples R China.
EM zmr20@mails.tsinghua.edu.cn; limading@kuaishou.com; chyjh@foxmail.com;
   chenlee@tsinghua.edu.cn
FU Tsinghua-Kuaishou Institute of Future Media Data; National Natural
   Science Foundation of China [61972221, 62021002, 61572274]
FX This work was supported in part by the Tsinghua-Kuaishou Institute of
   Future Media Data and in part by the National Natural Science Foundation
   of China under Grants 61972221, 62021002, and 61572274.
CR [Anonymous], 2006, P CVPR
   Atkins C.B., 2008, P 16 ACM INT C MULTI, P821
   Bianco S, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2801126
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Cheung V., 2022, Shape collage
   Geigel J, 2001, P SOC PHOTO-OPT INS, V4311, P79
   Goferman S, 2010, COMPUT GRAPH FORUM, V29, P459, DOI 10.1111/j.1467-8659.2009.01615.x
   Instagram Inc, 2018, Instagram layout
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Kingma D. P., 2014, arXiv
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li DB, 2019, IEEE T IMAGE PROCESS, V28, P5105, DOI 10.1109/TIP.2019.2914360
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Liang Y, 2018, IEEE T VIS COMPUT GR, V24, P2728, DOI 10.1109/TVCG.2017.2764895
   Liu LJ, 2018, IEEE T VIS COMPUT GR, V24, P1956, DOI 10.1109/TVCG.2017.2703853
   Liu T, 2009, IEEE T MULTIMEDIA, V11, P1225, DOI 10.1109/TMM.2009.2030741
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557
   Pan XJ, 2021, IEEE T VIS COMPUT GR, V27, P2298, DOI 10.1109/TVCG.2019.2948611
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Rohrbach A, 2017, INT J COMPUT VISION, V123, P94, DOI 10.1007/s11263-016-0987-1
   Rother C, 2005, PROC CVPR IEEE, P589
   Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Song Y, 2023, IEEE T VIS COMPUT GR, V29, P1330, DOI 10.1109/TVCG.2021.3113031
   Tan L, 2012, IEEE COMPUT GRAPH, V32, P46, DOI 10.1109/MCG.2011.89
   Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240
   Wei Y., 2009, MSRTR-2009-59
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Wu Z., 2013, P IEEE C AS PAC SIGN, P1
   Yu K, 2018, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2018.00259
   Yu ZQ, 2014, IEEE T VIS COMPUT GR, V20, P182, DOI 10.1109/TVCG.2013.106
   Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610
   Zeng H, 2020, IEEE T IMAGE PROCESS, V29, P1548, DOI 10.1109/TIP.2019.2941778
   Zhong BN, 2019, IEEE T IMAGE PROCESS, V28, P2331, DOI 10.1109/TIP.2018.2885238
NR 38
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4653
EP 4664
DI 10.1109/TMM.2022.3180217
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, NS
   Huang, J
   Zhao, F
   Fu, XY
   Wu, F
AF Zheng, Naishan
   Huang, Jie
   Zhao, Feng
   Fu, Xueyang
   Wu, Feng
TI Unsupervised Underexposed Image Enhancement via Self-Illuminated and
   Perceptual Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Perceptual guidance; self-illuminated; underexposed image enhancement;
   unsupervised learning
ID QUALITY ASSESSMENT; FRAMEWORK; NETWORK
AB Underexposed images inevitably suffer severe degradation due to light distortion and noise corruption. Motivated by the limited samples of paired datasets, several unsupervised enhancement methods have been developed. However, these techniques heavily rely on pre-defined fixed lightness and noise removal constraints. Correspondingly, they cannot match the image-specific lightness when performing enhancement and can only refine details in a non-perceptual way. In this paper, we propose an Unsupervised Underexposed Image Enhancement Network (U2IENet) with self-illuminated and perceptual guidance. Specifically, to adjust the illumination for matching the image-specific lightness adaptively, we utilize the bright area of the underexposed image as the self-illuminated guidance to constrain the training process and modulate the features. Meanwhile, we introduce the perceptual guidance as a constraint to remove the noise based on illumination distribution, thus refining the details perceptually. Experiments on both underexposed datasets and public low-light datasets demonstrate the superiority of the proposed approach with higher flexibility over state-of- the-art solutions. In addition, our U2IENet also provides a side function that enables users to adjust the lightness via interactive tuning of a single parameter.
C1 [Zheng, Naishan; Huang, Jie; Zhao, Feng; Fu, Xueyang; Wu, Feng] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhao, F (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM nszheng@mail.ustc.edu.cn; hj0117@mail.ustc.edu.cn; fzhao956@ustc.edu.cn;
   xyfu@ustc.edu.cn; fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024; Zhao, Feng/C-8367-2009
OI Zhao, Feng/0000-0001-6767-8105; Huang, Jie/0000-0002-3518-3404; Fu,
   Xueyang/0000-0001-8036-4071
FU Anhui Provincial Natural Science Foundation [2108085UD12]; MCC
   Laboratory of Information Science and Technology Institution, USTC
FX This work was supported by Anhui Provincial Natural Science Foundation
   under Grant 2108085UD12, and in part by GPU cluster built by the MCC
   Laboratory of Information Science and Technology Institution, USTC.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chen XY, 2021, IEEE COMPUT SOC CONF, P354, DOI 10.1109/CVPRW53098.2021.00045
   Dong X, 2011, IEEE INT CON MULTI
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Grossberg MD, 2004, IEEE T PATTERN ANAL, V26, P1272, DOI 10.1109/TPAMI.2004.88
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Huang T, 2021, PROC CVPR IEEE, P14776, DOI 10.1109/CVPR46437.2021.01454
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Lv F., 2018, P BMVC, V220, P4
   Lv FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1450, DOI 10.1145/3394171.3413925
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Nakai K, 2013, I S INTELL SIG PROC, P445, DOI 10.1109/ISPACS.2013.6704591
   Quan YH, 2020, PROC CVPR IEEE, P1887, DOI 10.1109/CVPR42600.2020.00196
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tian QC, 2017, IEEE INT CONF COMP V, P3023, DOI 10.1109/ICCVW.2017.357
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C., 2018, P BRIT MACH VIS C, P155
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xi Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P265, DOI 10.1007/978-3-030-58520-4_16
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Zhang L, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/1325705
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao ZJ, 2022, IEEE T CIRC SYST VID, V32, P1076, DOI 10.1109/TCSVT.2021.3073371
   Zhu AQ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102962
   Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106
NR 54
TC 9
Z9 9
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5469
EP 5484
DI 10.1109/TMM.2022.3193059
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300060
DA 2024-07-18
ER

PT J
AU Zhou, SW
   Deng, XN
   Li, CQ
   Liu, YH
   Jiang, HB
AF Zhou, Siwang
   Deng, Xiaoning
   Li, Chengqing
   Liu, Yonghe
   Jiang, Hongbo
TI Recognition-Oriented Image Compressive Sensing With Deep Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adversarial sample; compressive sensing; deep neural network; image
   recovery; machine recognition
ID NETWORKS
AB A number of image compressive sensing (CS) algorithms were proposed in the past two decades, aiming at yielding recovered images with the best possible visual effect. However, it is quite difficult to further improve the image quality for human eyes. For example, in the low-rate sampling scenarios, CS algorithms always suffer degraded performance and can only recover less visually appealing images. We notice that what human beings concern with is the visual quality of an image, while machine users care much more about its latent metrics, such as recognition accuracy, rather than the subjective visual effect. Inspired by this point, we develop a machine recognition-oriented image CS with an adversarial learning strategy. Some adversarial models are investigated to make the recognition accuracy as an additional optimization goal of the CS reconstruction network. Through end-to-end training, CS reconstruction network automatically learns an image recognition pattern, and produce recovered images owning extra recognition metric, which makes them become more suited for machine users. Experimental results indicate that the images recovered with the proposed adversarial learning strategy can be recognized with significantly higher accuracy compared to that with the existing CS algorithms.
C1 [Zhou, Siwang; Deng, Xiaoning; Jiang, Hongbo] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Li, Chengqing] Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.
   [Li, Chengqing] Xiangtan Univ, Minist Educ, Key Lab Intelligent Comp & Informat Proc, Xiangtan 411105, Peoples R China.
   [Liu, Yonghe] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
C3 Hunan University; Xiangtan University; Xiangtan University; University
   of Texas System; University of Texas Arlington
RP Li, CQ (corresponding author), Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.; Li, CQ (corresponding author), Xiangtan Univ, Minist Educ, Key Lab Intelligent Comp & Informat Proc, Xiangtan 411105, Peoples R China.
EM swzhou@hnu.edu.cn; xndeng@hnu.edu.cn; chengqingg@gmail.com;
   yonghe@cse.uta.edu; hongbojiang@hnu.edu.cn
RI Li, Chengqing/B-9388-2008
FU National Natural Science Foundation of China [61772447, 62172153];
   Changsha Municipal Natural Science Foundation [kq2014057]; National
   Natural Science Foundation of China [61772447, 62172153]; Changsha
   Municipal Natural Science Foundation [kq2014057]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61772447 and 62172153 and in part by
   the Changsha Municipal Natural Science Foundation under Grant kq2014057.
   The Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vladan Velisavljevic.(Corresponding
   author: ChengqingLi.).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Agarwal A, 2020, IEEE COMPUT SOC CONF, P3354, DOI 10.1109/CVPRW50498.2020.00395
   [Anonymous], 2012, 2012 IEEE INT C COMP, DOI DOI 10.1109/ICCPHOT.2012.6215212
   [Anonymous], 2015, P 3 INT C LEARN REPR
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Dong X., 2020, P IEEE C COMPUTER VI, P12895
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fowler JE, 2010, FOUND TRENDS SIGNAL, V4, P297, DOI 10.1561/2000000033
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang QR, 2020, IEEE T MULTIMEDIA, V22, P594, DOI 10.1109/TMM.2019.2931400
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lohit S, 2018, IEEE T COMPUT IMAG, V4, P326, DOI 10.1109/TCI.2018.2846413
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Rakotonirina NC, 2020, INT CONF ACOUST SPEE, P3637, DOI 10.1109/ICASSP40776.2020.9054071
   Shi WZ, 2021, IEEE T CIRC SYST VID, V31, P425, DOI 10.1109/TCSVT.2020.2978703
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wakin MB, 2006, IEEE IMAGE PROC, P1273, DOI 10.1109/ICIP.2006.312577
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Yang MR, 2015, IEEE T SIGNAL PROCES, V63, P5479, DOI 10.1109/TSP.2015.2453137
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zheng S, 2019, IEEE T MULTIMEDIA, V21, P1905, DOI 10.1109/TMM.2019.2891415
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
NR 35
TC 30
Z9 31
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2022
EP 2032
DI 10.1109/TMM.2022.3142952
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100034
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhu, JY
   Ma, HM
   Chen, JS
   Yuan, J
AF Zhu, Jingyuan
   Ma, Huimin
   Chen, Jiansheng
   Yuan, Jian
TI MotionVideoGAN: A Novel Video Generator Based on the Motion Space
   Learned From Image Pairs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video generation; motion space; image pairs; content consistency; fast
   convergence
AB Video generation has achieved rapid progress benefiting from high-quality renderings provided by powerful image generators. We regard the video synthesis task as generating a sequence of images sharing the same contents but varying in motions. However, most previous video synthesis frameworks based on pre-trained image generators treat content and motion generation separately, leading to unrealistic generated videos. Therefore, we design a novel framework to build the motion space, aiming to achieve content consistency and fast convergence for video generation. We present MotionVideoGAN, a novel video generator synthesizing videos based on the motion space learned by pre-trained image pair generators. Firstly, we propose an image pair generator named MotionStyleGAN to generate image pairs sharing the same contents and producing various motions. Then we manage to acquire motion codes to edit one image in the generated image pairs and keep the other unchanged. The motion codes help us edit images within the motion space since the edited image shares the same contents with the other unchanged one in image pairs. Finally, we introduce a latent code generator to produce latent code sequences using motion codes for video generation. Our approach achieves state-of-the-art performance on the most complex video dataset ever used for unconditional video generation evaluation, UCF101.
C1 [Zhu, Jingyuan; Yuan, Jian] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Ma, Huimin; Chen, Jiansheng] Univ Sci & Technol, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
C3 Tsinghua University; University of Science & Technology Beijing
RP Ma, HM (corresponding author), Univ Sci & Technol, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
EM jy-zhu20@mails.tsinghua.edu.cn; mhmpub@ustb.edu.cn; jschen@ustb.edu.cn;
   jyuan@tsinghua.edu.cn
OI Zhu, JingYuan/0000-0003-3995-8382
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Acharya D., 2018, arXiv
   Adari SK, 2019, I C DEPENDABLE SYST, P24, DOI 10.1109/DSN-W.2019.00012
   Aich A, 2020, PROC CVPR IEEE, P6089, DOI 10.1109/CVPR42600.2020.00613
   Aila T, 2020, P ADV NEUR INF PROC, P12104, DOI DOI 10.48550/ARXIV.2006.06676
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bau D., 2019, 7 INT C LEARNING REP
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Brock A., 2019, INT C LEARN REPR
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Clark A, 2019, Arxiv, DOI arXiv:1907.06571
   Donaldson J, 2019, AM J ROENTGENOL, V213, P1381, DOI 10.2214/AJR.18.20915
   Fox G., 2021, P BRIT MACH VIS C, P1
   Goetschalckx L, 2019, IEEE I CONF COMP VIS, P5743, DOI 10.1109/ICCV.2019.00584
   Gong XY, 2019, IEEE I CONF COMP VIS, P3223, DOI 10.1109/ICCV.2019.00332
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu J., 2022, P INT C LEARN REPR
   Hensel M, 2017, ADV NEUR IN, V30
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hyun S, 2021, PROC CVPR IEEE, P10821, DOI 10.1109/CVPR46437.2021.01068
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jahanian Ali, 2020, ICLR
   Kahembwe E, 2020, NEURAL NETWORKS, V132, P506, DOI 10.1016/j.neunet.2020.09.016
   Kaisiyuan Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P700, DOI 10.1007/978-3-030-58589-1_42
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, INT C LEARN REPR, P1, Patent No. 171010196
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kuybeda O, 2013, J STRUCT BIOL, V181, P116, DOI 10.1016/j.jsb.2012.10.010
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li DQ, 2021, PROC CVPR IEEE, P8296, DOI 10.1109/CVPR46437.2021.00820
   Ling Huan, 2021, Advances in Neural Information Processing Systems (NeurIPS)
   Ma L, 2019, P INT C LEARN REPR
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Muñoz A, 2021, IEEE WINT CONF APPL, P3178, DOI 10.1109/WACV48630.2021.00322
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Plumerault Antoine, 2020, P INT C LEARN REPR
   Radford A., 2015, ARXIV
   R”ssler A, 2018, Arxiv, DOI arXiv:1803.09179
   Saito M, 2020, INT J COMPUT VISION, V128, P2586, DOI 10.1007/s11263-020-01333-y
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Salimans T, 2016, ADV NEUR IN, V29
   Schonfeld E., 2021, P INT C LEARN REPR, P1
   Schwarz Katja, 2020, NEURIPS
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Skorokhodov I, 2022, PROC CVPR IEEE, P3616, DOI 10.1109/CVPR52688.2022.00361
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tian XY, 2022, IEEE T CIRC SYST VID, V32, P4804, DOI 10.1109/TCSVT.2021.3121987
   Tian Y., 2021, P INT C LEARN REPR
   Tolstikhin I.O., 2017, P ADV NEUR INF PROC, V30, P1
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Turkoglu MO, 2019, AAAI CONF ARTIF INTE, P8901
   Unterthiner T, 2019, Arxiv, DOI arXiv:1812.01717
   Vaishnavi J, 2022, 2022 SECOND INTERNATIONAL CONFERENCE ON ADVANCES IN ELECTRICAL, COMPUTING, COMMUNICATION AND SUSTAINABLE TECHNOLOGIES (ICAECT), DOI 10.1109/ICAECT54875.2022.9807935
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Voynov Andrey, 2020, ICML
   Wu HK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2487, DOI 10.1145/3343031.3350944
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Xiong W, 2018, PROC CVPR IEEE, P2364, DOI 10.1109/CVPR.2018.00251
   Yan W., 2021, arXiv, DOI DOI 10.48550/ARXIV.2104.10157
   Yu S., 2022, P INT C LEARNING REP
   Yujun Shen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9240, DOI 10.1109/CVPR42600.2020.00926
   Yushchenko V, 2019, IEEE INT CONF COMP V, P1523, DOI 10.1109/ICCVW.2019.00190
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang YX, 2021, PROC CVPR IEEE, P10140, DOI 10.1109/CVPR46437.2021.01001
   Zheng Ziqiang, 2022, IEEE Trans. Multimedia
   Zhu Jiapeng, 2021, Advances in Neural Information Processing Systems, V34, P16648
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 72
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9370
EP 9382
DI 10.1109/TMM.2023.3251095
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, J
   Zheng, WS
   Yang, Q
   Meng, JKM
   Hong, RC
   Tian, Q
AF Chen, Jiaxing
   Zheng, Wei-Shi
   Yang, Qize
   Meng, Jingke Meng
   Hong, Richang
   Tian, Qi
TI Deep Shape-Aware Person Re-Identification for Overcoming Moderate
   Clothing Changes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Feature extraction; Clothing; Visualization; Image color
   analysis; Context modeling; Reliability; Person re-identification;
   visual surveillance
ID FEATURES
AB Although person re-identification (person re-id) has advanced substantially in recent years, most methods are based on the assumption that the identities would not change clothes. This assumption might not hold in practice considering criminals who intentionally change clothes. In this work, we attempt to solve person re-id under moderate clothing change. Since the human body shape is considered as relatively more invariant under moderate clothing changes, we propose to learn a reliable shape-aware feature representation by mutually learning both colorful images and contour images. Instead of directly extracting shape features from contour images, we utilize contour feature learning as regularization and excavate more effective shape-aware feature representations from colorful images. We propose a multi-scale appearance and contour deep infomax (MAC-DIM) to maximize mutual information between colorful appearance features and contour shape features, and in this way, the extracted appearance features are constrained to be shape-aware in terms of both low-level visual properties and high-level semantics. To better model the long-range human body shape and explicitly capture contour segment relations, we introduce hierarchical graph modeling as aggregation headers, propagating structural context through graph convolutional networks (GCNs). The extensive results on benchmarks under clothing changes demonstrate the effectiveness of our shape-aware feature learning scheme.
C1 [Chen, Jiaxing; Zheng, Wei-Shi; Yang, Qize; Meng, Jingke Meng] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510000, Peoples R China.
   [Hong, Richang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230000, Peoples R China.
   [Tian, Qi] Cloud & AI BU, Shenzhen 518000, Peoples R China.
C3 Sun Yat Sen University; Hefei University of Technology
RP Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510000, Peoples R China.
EM chenjx228@mail2.sysu.edu.cn; wszheng@ieee.org; yangqz@mail2.sysu.edu.cn;
   mengjk3@mail.sysu.edu.cn; hongrc.hfut@gmail.com; tian.qi1@huawei.com
RI zheng, wei/IQT-9639-2023
FU NSFC [U21A20471, U1911401, U1811461]; Guangdong NSF Project
   [2020B1515120085, 2018B030312002]; Guangzhou Research Project
   [201902010037]; Key-Area Research and Development Program of Guangzhou
   [202007030004]
FX This work was supported in part by the NSFC
   (U21A20471,U1911401,U1811461), Guangdong NSF Project (No.
   2020B1515120085, 2018B030312002), Guangzhou Research Project
   (201902010037), and the Key-Area Research and Development Program of
   Guangzhou (202007030004).
CR Almeida LB, 2004, J MACH LEARN RES, V4, P1297
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J., 2009, IEEE C COMP VIS PATT
   Fan L., 2020, P IEEE C COMP VIS PA, P10696, DOI 10.1109/CVPR42600.2020.01071
   Fangbin Wan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   Hamilton WL, 2017, ADV NEUR IN, V30
   Haque A, 2016, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2016.138
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hjelm R. Devon, 2018, LEARNING DEEP REPRES
   Hu Fenyu, 2019, IJCAI, P4532
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Kipf T. N., 2016, arXiv preprint arXiv:1609.02907
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Li RY, 2018, AAAI CONF ARTIF INTE, P3546
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YJ, 2020, Arxiv, DOI arXiv:2003.07340
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36
   Liu Y, 2017, PROC CVPR IEEE, P5872, DOI 10.1109/CVPR.2017.622
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Munaro M, 2014, ADV COMPUT VIS PATT, P161, DOI 10.1007/978-1-4471-6296-4_8
   Nowozin S, 2016, ADV NEUR IN, V29
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Pala P, 2019, COMPUT GRAPH-UK, V79, P69, DOI 10.1016/j.cag.2019.01.003
   Qian X., 2020, P AS C COMP VIS NOV, P71
   Reddi S. J., 2019, ARXIV PREPRINT ARXIV
   Seddati O., 2017, MULTIMEDIA TOOLS APP, V76, p22 333
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201
   Wu S., 2016, P IEEE WINT C APPL C, P1, DOI [DOI 10.1080/10298436.2016.1248204, DOI 10.1109/WACV.2016.7477681]
   Xie XH, 2010, PATTERN RECOGN, V43, P4177, DOI 10.1016/j.patcog.2010.06.019
   Xue J, 2018, IEEE COMPUT SOC CONF, P2193, DOI 10.1109/CVPRW.2018.00285
   Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P347, DOI 10.1145/3343031.3351043
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Yu SJ, 2020, PROC CVPR IEEE, P3397, DOI 10.1109/CVPR42600.2020.00346
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhang XY, 2020, PATTERN RECOGN LETT, V130, P73, DOI 10.1016/j.patrec.2019.01.006
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng Wei-Shi, 2009, P BRIT MACH VIS C, P23, DOI DOI 10.5244/C.23.23
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou JH, 2020, PROC CVPR IEEE, P2906, DOI 10.1109/CVPR42600.2020.00298
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhu ZH, 2020, AAAI CONF ARTIF INTE, V34, P13114
NR 72
TC 13
Z9 14
U1 3
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4285
EP 4300
DI 10.1109/TMM.2021.3114539
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 5D6ZS
UT WOS:000865087600001
DA 2024-07-18
ER

PT J
AU Chen, X
   Gao, CQ
   Li, CY
   Yang, Y
   Meng, DY
AF Chen, Xu
   Gao, Chenqiang
   Li, Chaoyu
   Yang, Yi
   Meng, Deyu
TI Infrared Action Detection in the Dark via Cross-Stream Attention
   Mechanism
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optical imaging; Streaming media; Feature extraction; Proposals; Task
   analysis; Image recognition; Three-dimensional displays; Infrared video;
   selective cross-stream attention; temporal action detection
ID ACTION RECOGNITION; NETWORKS
AB Action detection plays an important role in video understanding and attracts considerable attention in the last decade. However, current action detection methods are mainly based on visible videos, and few of them consider scenes with low-light, where actions are difficult to be detected by existing methods, or even by human eyes. Compared with visible videos, infrared videos are more suitable for the dark environment and resistant to background clutter. In this paper, we investigate the temporal action detection problem in the dark by using infrared videos, which is, to the best of our knowledge, the first attempt in the action detection community. Our model takes the whole video as input, a Flow Estimation Network (FEN) is employed to generate the optical flow for infrared data, and it is optimized with the whole network to obtain action-related motion representations. After feature extraction, the infrared stream and flow stream are fed into a Selective Cross-stream Attention (SCA) module to narrow the performance gap between infrared and visible videos. The SCA emphasizes informative snippets and focuses on the more discriminative stream automatically. Then we adopt a snippet-level classifier to obtain action scores for all snippets and link continuous snippets into final detections. All these modules are trained in an end-to-end manner. We collect an Infrared action Detection (InfDet) dataset obtained in the dark and conduct extensive experiments to verify the effectiveness of the proposed method. Experimental results show that our proposed method surpasses state-of-the-art temporal action detection methods designed for visible videos, and it also achieves the best performance compared with other infrared action recognition methods on both InfAR and Infrared-Visible datasets.
C1 [Chen, Xu; Gao, Chenqiang] Chongqing Univ Posts & Telecommunt, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Chen, Xu; Gao, Chenqiang] Chongqing Key Lab Signal & Informat Proc, Chongqing 400065, Peoples R China.
   [Li, Chaoyu] Chongqing Univ Posts & Telecommun, Sch Automat, Chongqing 400065, Peoples R China.
   [Yang, Yi] Univ Technol Sydney, Ctr Artificial Intelligence, Ultimo, NSW 2007, Australia.
   [Meng, Deyu] Macau Univ Sci & Technol, Macau Inst Syst Engn, Taipa 999078, Macao, Peoples R China.
   [Meng, Deyu] Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Shanxi, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; University of
   Technology Sydney; Macau University of Science & Technology; Xi'an
   Jiaotong University
RP Gao, CQ (corresponding author), Chongqing Univ Posts & Telecommunt, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
EM lann9601@foxmail.com; gaocq@cqupt.edu.cn; lichaoyu1997@gmail.com;
   yi.yang@uts.edu.au; dymeng@mail.xjtu.edu.cn
RI Yang, Yi/B-9273-2017; yang, yang/HGT-7999-2022
OI Yang, Yi/0000-0002-0512-880X; Gao, Chenqiang/0000-0003-4174-4148; Li,
   Chaoyu/0000-0003-0788-0189; Chen, Xu/0000-0002-1805-5435
FU National Natural Science Foundation of China [61571071, 61906025,
   11690011, U1811461]; Chongqing Research Program of Basic Research and
   Frontier Technology [cstc2018jcyjAX0227]; Science and Technology
   Research Program of Chongqing Municipal Education Commission
   [KJQN201900607, KJQN202000647]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61571071, 61906025, 11690011, and
   U1811461, in part by the Chongqing Research Program of Basic Research
   and Frontier Technology under Grant cstc2018jcyjAX0227, and in part by
   the Science and Technology Research Program of Chongqing Municipal
   Education Commission under Grants KJQN201900607 and KJQN202000647.
CR Black M.J, 2018, GCPR
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duarte K, 2018, ADV NEUR IN, V31
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Gao CQ, 2016, NEUROCOMPUTING, V212, P36, DOI 10.1016/j.neucom.2016.05.094
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Huang JJ, 2020, IEEE T CIRC SYST VID, V30, P2650, DOI 10.1109/TCSVT.2019.2923712
   Hui TW, 2021, IEEE T PATTERN ANAL, V43, P2555, DOI 10.1109/TPAMI.2020.2976928
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Imran J, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.103014
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Jiang ZL, 2017, IEEE COMPUT SOC CONF, P309, DOI 10.1109/CVPRW.2017.44
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2018, IEEE SIGNAL PROC LET, V25, P848, DOI 10.1109/LSP.2018.2823910
   Liu Y, 2018, COMPLEXITY, DOI 10.1155/2018/5345241
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Ma XX, 2019, IEEE INT SYMP CIRC S
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Piergiovanni AJ, 2019, PR MACH LEARN RES, V97
   Piergiovanni AJ, 2018, PROC CVPR IEEE, P5304, DOI 10.1109/CVPR.2018.00556
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sabour S., 2017, P C NIPS, P3856
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shou Z., 2020, ARXIV200607976
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Tak-Wai Hui, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P169, DOI 10.1007/978-3-030-58565-5_11
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tsai DM, 2015, SIGNAL IMAGE VIDEO P, V9, P1897, DOI 10.1007/s11760-014-0677-9
   Ulhaq A, 2018, 2018 IEEE THIRD INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, APPLICATIONS AND SYSTEMS (IPAS), P131, DOI 10.1109/IPAS.2018.8708903
   Ulyanov Dmitry, 2016, arXiv
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L, 2018, LECT NOTES COMPUT SC, V11210, P389, DOI 10.1007/978-3-030-01231-1_24
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei Xiangyu, 2019, ARXIV191106644
   Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yong HW, 2018, IEEE T PATTERN ANAL, V40, P1726, DOI 10.1109/TPAMI.2017.2732350
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang S., 2020, ARXIV200207442
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
NR 64
TC 13
Z9 14
U1 3
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 288
EP 300
DI 10.1109/TMM.2021.3050069
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300022
DA 2024-07-18
ER

PT J
AU Hao, JY
   Wang, CJ
   Yang, G
   Gao, ZF
   Zhang, JL
   Zhang, HY
AF Hao, Jingyu
   Wang, Chengjia
   Yang, Guang
   Gao, Zhifan
   Zhang, Jinglin
   Zhang, Heye
TI Annealing Genetic GAN for Imbalanced Web Data Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Generators; Genetic algorithms; Annealing; Simulated
   annealing; Generative adversarial networks; Optimization; Generative
   adversarial networks; Class imbalance problem; evolutionary computation;
   data augmentation
ID CLASSIFICATION; ALGORITHMS; SMOTE
AB Class imbalance is one of the most basic and important problems of web data. The key to overcoming the class imbalance problems is to increase the effective instances of the minority, that is, data augmentation. Generative Adversarial Networks (GANs), which have recently been successfully applied in the field of image generation, can be used for data augmentation because they can learn the data distribution given ample training data instances and generate more data. However, learning the distributions from the imbalanced data can make GANs easily get stuck in a local optimum. In this work, we propose a new training strategy called Annealing Genetic GAN (AGGAN), which incorporates simulated annealing genetic algorithm into the training process of GANs. And this can help GANs avoid the local optimum trapping problem, which easily occurs when the training set is imbalanced. Unlike existing GANs, which use a fixed adversarial learning objective alternately training a generator, we use multiple adversarial learning objectives to train a set of generators and use the Metropolis criterion in simulated annealing to decide whether the generator should update. More specifically, the Metropolis criterion accepts worse solutions with a certain probability, so it can make our AGGAN escape from the local optimum and find a better solution. Theory and mathematical analysis provide strong theoretical support for the proposed training strategy. And experiments on several datasets demonstrate that AGGAN achieves convincing ability to solve the class imbalanced problem and reduces the training problems inherent in existing GANs.
C1 [Hao, Jingyu; Gao, Zhifan; Zhang, Heye] Sun Yat Sen Univ, Sch Biomed Engn, Shenzhen 518107, Peoples R China.
   [Wang, Chengjia] Univ Edinburgh, Ctr Cardiovasc Sci, Edinburgh EH16 4TJ, Midlothian, Scotland.
   [Yang, Guang] Imperial Coll London, Natl Heart & Lung Inst, Fac Med, London SW7 2AZ, England.
   [Zhang, Jinglin] Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
C3 Sun Yat Sen University; University of Edinburgh; Imperial College
   London; Hebei University of Technology
RP Zhang, JL (corresponding author), Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
EM haojy5@mail2.sysu.edu.cn; chengjia.wang@ed.ac.uk; g.yang@imperial.ac.uk;
   gaozhifan@mail.sysu.edu.cn; jinglin.zhang37@gmail.com;
   zhangheye@mail.sysu.edu.cn
RI Gao, Zhifan/O-9082-2019; HAO, Jingyu/KPB-3465-2024; Yang,
   Guang/S-5032-2016; Zhang, Heye/JPK-4651-2023
OI Gao, Zhifan/0000-0002-1576-4439; Yang, Guang/0000-0001-7344-7733; Zhang,
   Heye/0000-0001-7334-3037; Hao, Jingyu/0000-0002-3301-4439
FU National Key Research and Development Program of China [2018YFE0126100];
   Key Research and Development Program of Jiangsu Province [BE2021093];
   Key Program for International Cooperation Projects of Guangdong Province
   [2018A050506031]; British Heart Foundation [TG/18/5/34111,
   PG/16/78/32402]; European Research Council Innovative Medicines
   Initiative [H2020-JTI-IMI2 101005122]; AI for Health Imaging Award
   [H2020-SC1-FA-DTS-2019-1 952172]; U.K. Research and Innovation Future
   Leaders Fellowship [MR/V023799/1]; FLF [MR/V023799/1] Funding Source:
   UKRI
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFE0126100, in part by the
   Key Research and Development Program of Jiangsu Province under Grant
   BE2021093, in part by the Key Program for International Cooperation
   Projects of Guangdong Province under Grant 2018A050506031, in part by
   British Heart Foundation under Grants TG/18/5/34111 and PG/16/78/32402,
   in part by European Research Council Innovative Medicines Initiative
   under Grant DRAGON, H2020-JTI-IMI2 101005122, in part by AI for Health
   Imaging Award under Grant CHAIMELEON, H2020-SC1-FA-DTS-2019-1 952172,
   and in part by U.K. Research and Innovation Future Leaders Fellowship
   under Grant MR/V023799/1. The Guest Editor coordinating the review of
   this manuscript and approving it for publication was Prof. Yazhou Yao.
CR Aarts E. H. L., 1989, COMPUT SCI NOTES, V8908
   ADLER D, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P1104, DOI 10.1109/ICNN.1993.298712
   Ando S, 2017, LECT NOTES ARTIF INT, V10534, P770, DOI 10.1007/978-3-319-71249-9_46
   [Anonymous], 2012, WORLD ACAD SCI ENG T
   [Anonymous], 2017, P 5 INT C LEARN REPR
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Blagus R, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-106
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen C, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION (IRI), P384, DOI 10.1109/IRI.2011.6009578
   Chen H, 1994, LECT NOTES COMPUT SC, V866, P428
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Douzas G, 2018, EXPERT SYST APPL, V91, P464, DOI 10.1016/j.eswa.2017.09.030
   Galar M, 2012, IEEE T SYST MAN CY C, V42, P463, DOI 10.1109/TSMCC.2011.2161285
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   Hao J., 2020, P 31 BRIT MACHINE VI
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   INGBER L, 1992, MATH COMPUT MODEL, V16, P87, DOI 10.1016/0895-7177(92)90108-W
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jerant AF, 2000, AM FAM PHYSICIAN, V62, P357
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Kubat M., 1997, ICML, P179
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liu HL, 2018, IEEE T EVOLUT COMPUT, V22, P433, DOI 10.1109/TEVC.2017.2725902
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mariani G., 2018, Bagan: Data augmentation with balancing gan
   Mazurowski MA, 2008, NEURAL NETWORKS, V21, P427, DOI 10.1016/j.neunet.2007.12.031
   Mitchell M., 1998, INTRO GENETIC ALGORI
   Mullick SS, 2019, IEEE I CONF COMP VIS, P1695, DOI 10.1109/ICCV.2019.00178
   Nagarajan Vaishnavh, 2017, Advances in neural information processing systems, P5585
   Netzer Y, 2011, PROC NIPS WORKSHOP D, P1
   Odena A, 2017, PR MACH LEARN RES, V70
   Powers DMW, 2020, J MACH LEARN TECHNOL, P37, DOI DOI 10.9735/2229-3981
   Rere LMR, 2015, PROCEDIA COMPUT SCI, V72, P137, DOI 10.1016/j.procs.2015.12.114
   Rotemberg V, 2021, SCI DATA, V8, DOI 10.1038/s41597-021-00815-z
   Shi B., 2019, ADV NEUR IN
   Sun YM, 2007, PATTERN RECOGN, V40, P3358, DOI 10.1016/j.patcog.2007.04.009
   Tao C., 2019, INT C MACHINE LEARNI, P6176
   Wang CY, 2019, IEEE T EVOLUT COMPUT, V23, P921, DOI 10.1109/TEVC.2019.2895748
   Xiao H., 2017, arXiv
   Yu Fisher, 2015, ARXIV150603365
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
NR 46
TC 13
Z9 13
U1 7
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1164
EP 1174
DI 10.1109/TMM.2021.3120642
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800013
DA 2024-07-18
ER

PT J
AU Jia, W
   Li, L
   Akhtar, A
   Li, Z
   Liu, S
AF Jia, Wei
   Li, Li
   Akhtar, Anique
   Li, Zhu
   Liu, Shan
TI Convolutional Neural Network-Based Occupancy Map Accuracy Improvement
   for Video-Based Point Cloud Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Videos; Geometry; Heuristic algorithms;
   Noise measurement; Bit rate; Software algorithms; Convolutional neural
   network; high efficiency video coding; occupancy map; segmentation;
   video-based point cloud compression
ID MPEG
AB In video-based point cloud compression (V-PCC), a dynamic point cloud is projected onto geometry and attribute videos patch by patch for compression. In addition to the geometry and attribute videos, an occupancy map video is compressed into a V-PCC bitstream to indicate whether a two-dimensional (2D) point in the projected geometry video corresponds to any point in three-dimensional (3D) space. The occupancy map video is usually downsampled before compression to obtain a tradeoff between the bitrate and the reconstructed point cloud quality. Due to the accuracy loss in the downsampling process, some noisy points are generated, which leads to severe objective and subjective quality degradation of the reconstructed point cloud. To improve the quality of the reconstructed point cloud, we propose using a convolutional neural network (CNN) to improve the accuracy of the occupancy map video. We mainly make the following contributions. First, we improve the accuracy of the occupancy map video by formulating the problem as a binary segmentation problem since the pixel values of the occupancy map video are either 0 or 1. Second, in addition to the downsampled occupancy map video, we introduce a reconstructed geometry video as the other input of the CNN to provide more useful information in order to indicate the occupancy map video. To the best of our knowledge, this is the first learning-based work to improve the performance of V-PCC. Compared to state-of-the-art schemes, our proposed CNN-based approach achieves much more accurate occupancy map videos and significant bitrate savings.
C1 [Jia, Wei; Akhtar, Anique; Li, Zhu] Univ Missouri, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
   [Li, Li] Univ Sci & Technol, Dept Elect Engn & Informance Sci, Hefei 230027, Anhui, Peoples R China.
   [Liu, Shan] Tencent Media Lab, Palo Alto, CA 94306 USA.
C3 University of Missouri System; University of Missouri Kansas City;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Li, Z (corresponding author), Univ Missouri, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
EM wj3wr@umsystem.edu; lil1@ustc.edu.cn; aniqueakhtar@mail.umkc.edu;
   zhu.li@ieee.org; shanl@tencent.com
RI Li, Zhu/AAD-8182-2021; huang, shan/JVN-1240-2024
OI Li, Zhu/0000-0002-8246-177X; Jia, Wei/0000-0003-0053-6959; ,
   Shan/0000-0002-1442-1207; Akhtar, Anique/0000-0003-2701-6611
FU NSF I/UCRC grant [CNS-1747751]
FX This paper is supported in part by NSF I/UCRC grant under CNS-1747751.
   The associate editor coordinating the review of this manuscript and
   approving it for publicationwas Dr. S. Mehrotra.
CR Andrivon P, 2019, DOCU MENT ISOIEC JTC
   [Anonymous], POINT CLOUD COMPRESS
   [Anonymous], MOBILE MAPPING SYSTE
   [Anonymous], 2017, ACM Transactions on Graphics (TOG), DOI DOI 10.1145/3083722
   [Anonymous], CULTURE 3D CLOUD
   Bjontegaard G., 2001, CALCULATION AVERAGE
   Bruder G, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P161, DOI 10.1109/3DUI.2014.6798870
   Cai K., 2019, DOCUMENT ISOIEC JTC1
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Chen X., 2016, 2016405 NCES US DEP
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807
   Fuchs H, 2014, COMPUTER, V47, P46, DOI 10.1109/MC.2014.185
   Guede C., 2018, Document ISO/IEC JTC1/SC29/WG11 MPEG2018/ M44779
   Guede C, 2017, document ISO/IEC JTC1/SC29/WG11 m41822
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LY, 2017, ASIA-PAC CONF COMMUN, P345
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jang ES, 2019, IEEE SIGNAL PROC MAG, V36, P118, DOI 10.1109/MSP.2019.2900721
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Kingma D, 2014, ICLR P, V2014, P1
   Lee Y.-H, 2019, JTC1SC29WG11 ISOIEC
   Li L, 2021, IEEE T CIRC SYST VID, V31, P326, DOI 10.1109/TCSVT.2020.2966118
   M. Committee, 2020, DOCUMENT ISOIEC JTC1
   Ma L., 2020, IEEE T MULTIMEDIA, P1
   Mammou K., 2017, Document ISO/IEC JTC1/SC29/WG11 m41649
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Najaf-Zadeh H., 2019, document ISO/IEC JTC1/SC29/WG11 m49591
   Oh R. J. Youngho, 2019, JTC1SC29WG11 ISOIEC
   Preda M, 2017, DOCUMENT ISO IEC JTC
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schwarz S., 2018, Document ISO/IEC JTC1/SC29/WG11 w17766
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sportillo D., 2017, Proceedings of the 9th International Conference on Computer and Automation Engineering. S, P6, DOI [DOI 10.1145/3057039.3057079, 10.1145/3057039]
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Tulvan C., 2016, JTC1SC29WG11N16331 I
   Vosoughi A., 2019, DOC UMENT ISOIEC JTC
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang S.-P, 2019, DOCUMENT ISOIEC JTC1
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
NR 43
TC 11
Z9 13
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2352
EP 2365
DI 10.1109/TMM.2021.3079698
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600010
OA Bronze
DA 2024-07-18
ER

PT J
AU Jo, SY
   Lee, S
   Ahn, N
   Kang, SJ
AF Jo, So Yeon
   Lee, Siyeong
   Ahn, Namhyun
   Kang, Suk-Ju
TI Deep Arbitrary HDRI: Inverse Tone Mapping With Controllable Exposure
   Changes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image restoration; Dynamic range; Generators; High frequency;
   Brightness; Training; Process control; High dynamic range imaging;
   inverse tone mapping; image restoration; deep learning
ID DYNAMIC-RANGE EXPANSION; IMAGE
AB Deep convolutional neural networks (CNNs) have recently made significant advances in the inverse tone mapping technique, which generates a high dynamic range (HDR) image from a single low dynamic range (LDR) image that has lost information in over- and under-exposed regions. The end-to-end inverse tone mapping approach specifies the dynamic range in advance, thereby limiting dynamic range expansion. In contrast, the method of generating multiple exposure LDR images from a single LDR image and subsequently merging them into an HDR image enables flexible dynamic range expansion. However, existing methods for generating multiple exposure LDR images require an additional network for each exposure value to be changed or a process of recursively inferring images that have different exposure values. Therefore, the number of parameters increases significantly due to the use of additional networks, and an error accumulation problem arises due to recursive inference. To solve this problem, we propose a novel network architecture that can control arbitrary exposure values without adding networks or applying recursive inference. The training method of the auxiliary classifier-generative adversarial network structure is employed to generate the image conditioned on the specified exposure. The proposed network uses a newly designed spatially-adaptive normalization to address the limitation of existing methods that cannot sufficiently restore image detail due to the spatially equivariant nature of the convolution. Spatially-adaptive normalization facilitates restoration of the high frequency component by applying different normalization parameters to each element in the feature map according to the characteristics of the input image. Experimental results show that the proposed method outperforms state-of-the-art methods, yielding a 5.48dB higher average peak signal-to-noise ratio, a 0.05 higher average structure similarity index, a 0.28 higher average multi-scale structure similarity index, and a 7.36 higher average HDR-VDP-2 for various datasets.
C1 [Jo, So Yeon] LG Uplus, Seoul 07795, South Korea.
   [Lee, Siyeong] NAVER LABS, Seongnam Si 13638, Gyeonggi Do, South Korea.
   [Ahn, Namhyun; Kang, Suk-Ju] Sogang Univ, Vis & Display Syst Lab Elect Engn, Seoul 04017, South Korea.
C3 Naver; Sogang University
RP Kang, SJ (corresponding author), Sogang Univ, Vis & Display Syst Lab Elect Engn, Seoul 04017, South Korea.
EM soyeonjo@sogang.ac.kr; siyeong.lee@naverlabs.com;
   neition503@sogang.ac.kr; sjkang@sogang.ac.kr
OI Lee, Siyeong/0000-0002-5165-1335
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [2021R1A2C1004208, 2020M3H4A1A02084899]
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) grant funded by the Korea government (MSIT) (No.
   2021R1A2C1004208) and (No. 2020M3H4A1A02084899).
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374
   [Anonymous], 1999, P 1999 IEEE COMP SOC
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], HDR EYE DATASET HIGH
   [Anonymous], 2005, P ACM SIGGRAPH
   [Anonymous], P INT C MACH LEARN L
   [Anonymous], 2017, Quality and User Experience, DOI DOI 10.1007/S41233-017-0007-4
   Ayd~n T. O., 2008, HUMAN VISION ELECT I, P6806
   Banterle F., 2006, Computer Graphics and Interactive Techniques in Australasia and Southeast Asia Association for Computing Machinery, P349, DOI 10.1145/1174429.1174489
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Chen X, 2016, ADV NEUR IN, V29
   Debevec Paul E, 2008, ACM SIGGRAPH 2008 CL, P1, DOI DOI 10.1145/1401132.1401174
   Eilertsen G., 2018, The High Dynamic Range Imaging Pipeline
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Huo YQ, 2014, VISUAL COMPUT, V30, P507, DOI 10.1007/s00371-013-0875-4
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kim Min H., 2008, Proceedings of the Tenth IASTED International Conference on Computer Graphics and Imaging, P152
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2013, ARXIV13126114
   Kovaleski RP, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P49, DOI 10.1109/SIBGRAPI.2014.29
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee S, 2021, IEEE T MULTIMEDIA, V23, P2561, DOI 10.1109/TMM.2020.3013378
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Lee S, 2018, IEEE ACCESS, V6, P49913, DOI 10.1109/ACCESS.2018.2868246
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Masia B, 2017, MULTIMED TOOLS APPL, V76, P631, DOI 10.1007/s11042-015-3036-0
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Odena A, 2017, PR MACH LEARN RES, V70
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Reed S, 2016, PR MACH LEARN RES, V48
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Robertson MA, 2003, J ELECTRON IMAGING, V12, P219, DOI 10.1117/1.1557695
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sohn K, 2015, ADV NEUR IN, V28
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu B., 2015, Empirical evaluation of rectified activations in convolutional network, DOI DOI 10.48550/ARXIV.1505.00853
   Xu Long., 2015, Visual quality assessment by machine learning
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang JS, 2017, IEEE I CONF COMP VIS, P4529, DOI 10.1109/ICCV.2017.484
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
NR 60
TC 3
Z9 3
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2713
EP 2726
DI 10.1109/TMM.2021.3087034
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000002
DA 2024-07-18
ER

PT J
AU Leng, JX
   Liu, Y
   Wang, ZH
   Hu, HB
   Gao, XB
AF Leng, Jiaxu
   Liu, Ying
   Wang, Zhihui
   Hu, Haibo
   Gao, Xinbo
TI CrossNet: Detecting Objects as Crosses
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; Costs; Convolution; Estimation; Object detection;
   Prediction methods; Detectors; Keypoint localization; object detection;
   cascade framework; size regression
AB With the use of deep learning, object detection has achieved great breakthroughs. However, existing object detection methods still can not cope with challenging environments, such as dense objects, small objects, and object scale variations. To address these issues, this paper proposes a novel keypoint-based detection framework, called CrossNet, which significantly improves detection performance with minimal costs. In our approach, an object is modeled as a cross that consists of a center keypoint and a specific size, which eliminates the need of hand-craft anchor design. The proposed CrossNet outputs three types of maps: the center map, size map, and offset map, where both center map and offset map are to predict the center keypoints of objects and the size map is to estimate the sizes (width and height) of objects. Specifically, we first design a cascaded center prediction method that introduces a coarse-to-fine idea to improve center prediction. Furthermore, since center prediction considered as a classification task is easier than size regression relatively, we design a center-attention size regression module that uses the detection results of centers to assist the size prediction. In addition, a slightly modified hourglass network is designed to enhance the quality of feature maps for center and size prediction. Extensive experiments are conducted to demonstrate the effectiveness of CrossNet on the challenging PASCAL VOC, COCO, KITTI, and WiderFace datasets. Empirical studies show that CrossNet achieves competitive results with top-ranked one-stage and two-stage detectors while being time-efficient.
C1 [Leng, Jiaxu; Gao, Xinbo] Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, Chongqing 400065, Peoples R China.
   [Leng, Jiaxu; Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Liu, Ying] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101400, Peoples R China.
   [Wang, Zhihui] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266510, Peoples R China.
   [Hu, Haibo] Chongqing Univ, Sch Big Data & Software Engn, Chongqing 401331, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Chongqing University
   of Posts & Telecommunications; Chinese Academy of Sciences; University
   of Chinese Academy of Sciences, CAS; Shandong University of Science &
   Technology; Chongqing University
RP Leng, JX (corresponding author), Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, Chongqing 400065, Peoples R China.
EM lengjx@cqupt.edu.cn; yingliu@mails.ucas.ac.cn; zh_wang@sdustedu.cn;
   haibo.hu@cqu.edu.cn; gaoxb@cqupt.edu.cn
RI leng, jiaxu/V-8212-2019; Hu, Haibo/AAS-5704-2020; Wang,
   Zhihui/D-2915-2015
OI leng, jiaxu/0000-0003-2802-8139; Hu, Haibo/0000-0002-9008-2112; Wang,
   Zhihui/0000-0001-8140-1882
FU National Natural Science Foundation of China [62036007, 62050175,
   71671178]; Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62036007, 62050175, and 71671178, and
   also in part by the Fundamental Research Funds for the Central
   Universities. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Wanqing Li.
CR [Anonymous], IEEE Transactions on Pattern Analysis and
   Bai YC, 2018, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2018.00010
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Bowen Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12472, DOI 10.1109/CVPR42600.2020.01249
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Chen Qi, 2019, ARXIV191212791
   Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Chi C, 2019, AAAI CONF ARTIF INTE, P8231
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Du XX, 2018, IEEE INT CONF ROBOT, P3194
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Kaiwen Duan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P399, DOI 10.1007/978-3-030-58580-8_24
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Leng J., 2020, NEURAL COMPUT APPL, P1
   Leng JX, 2019, NEURAL COMPUT APPL, V31, P6549, DOI 10.1007/s00521-018-3486-1
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YD, 2020, AAAI CONF ARTIF INTE, V34, P11653
   Liu ZC, 2020, IEEE COMPUT SOC CONF, P4289, DOI 10.1109/CVPRW50498.2020.00506
   Misra D., 2019, ARXIV190808681
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
   Samet N., 2020, ARXIV200702355
   Simon M, 2019, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW.2019.00158
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang Y., 2020, P DAGM GERM C PATT R, P289
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tychsen-Smith L, 2018, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR.2018.00719
   Tychsen-Smith L, 2017, IEEE I CONF COMP VIS, P428, DOI 10.1109/ICCV.2017.54
   Wang C.Y., 2020, ARXIV201108036
   Wang J., 2017, ARXIV171107246
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang X, 2017, Point linking network for object detection
   Wang Y., 2017, Detecting Faces Using Region-based fully convolutional networks
   Xiang Y, 2017, IEEE WINT CONF APPL, P924, DOI 10.1109/WACV.2017.108
   Yang F, 2016, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2016.234
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yang T., 2018, Advances in Neural Information Processing Systems
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yi HW, 2020, IEEE INT CONF ROBOT, P2274, DOI [10.1109/icra40945.2020.9196556, 10.1109/ICRA40945.2020.9196556]
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   ZHANG C, 2018, ARXIV180202142
   Zhang S, 2019, ARXIV190106651
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30
   Zhang Y, 2019, ARXIV190102350
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou X., 2019, arXiv
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu CC, 2017, ADV COMPUT VIS PATT, P57, DOI 10.1007/978-3-319-61657-5_3
   Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444
NR 82
TC 4
Z9 4
U1 2
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 861
EP 875
DI 10.1109/TMM.2021.3060278
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100027
DA 2024-07-18
ER

PT J
AU Lin, JR
   Chen, MJ
   Yeh, CH
   Chen, YC
   Kau, LJ
   Chang, CY
   Lin, MH
AF Lin, Jie-Ru
   Chen, Mei-Juan
   Yeh, Chia-Hung
   Chen, Yong-Ci
   Kau, Lih-Jen
   Chang, Chuan-Yu
   Lin, Min-Hui
TI Visual Perception Based Algorithm for Fast Depth Intra Coding of 3D-HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Three-dimensional displays; Image color analysis; Image edge
   detection; Feature extraction; Visualization; Prediction algorithms;
   3D-HEVC; depth map; intra coding; perceptual coding; intra prediction
ID DECISION
AB 3D-HEVC (The 3D Extension of High Efficiency Video Coding) is the newest 3D video coding standard, which enriches multimedia applications with the video format of multi-view plus depth. For the depth map coding in 3D-HEVC, the advanced coding tools enhance the coding efficiency of the depth map and the quality of the synthesized view. However, the time consumption and complexity of 3D-HEVC also increase significantly. This paper utilizes the characteristics of human visual system to propose a fast algorithm based on visual perception for the acceleration of the depth intra coding of 3D-HEVC. The depth map is segmented into different regions by Otsu's auto-thresholding. The dominate edge direction is categorized for each prediction unit. We detect the perceptual edge based on just noticeable depth difference model to extract the area that may affect the visual perception. According to depth map segmentation and edge distribution, we reduce the corresponding intra angular modes and determine whether to perform depth modelling mode. We also incorporate the boundary continuity and rate-distortion cost thresholding to propose the fast coding unit decision. The experimental results show that the proposed algorithm eliminates 53.09% of the depth coding time with only 0.15% BD-BR on average. The coding performance of the proposed algorithm outperforms the previous works significantly.
C1 [Lin, Jie-Ru; Chen, Mei-Juan; Chen, Yong-Ci] Natl Dong Hwa Univ, Dept Elect Engn, Hualien 97401, Taiwan.
   [Yeh, Chia-Hung] Natl Taiwan Normal Univ, Dept Elect Engn, Taipei 10610, Taiwan.
   [Yeh, Chia-Hung; Lin, Min-Hui] Natl Sun Yat Sen Univ, Dept Elect Engn, Kaohsiung 80424, Taiwan.
   [Kau, Lih-Jen] Natl Taipei Univ Technol, Dept Elect Engn, Taipei 10608, Taiwan.
   [Chang, Chuan-Yu] Ind Technol Res Inst, Serv Syst Technol Ctr, Zhudong Township, Taiwan.
   [Chang, Chuan-Yu] Natl Yunlin Univ Sci & Technol, Dept Comp Sci & Informat Engn, Touliu 64002, Yunlin, Taiwan.
C3 National Dong Hwa University; National Taiwan Normal University;
   National Sun Yat Sen University; National Taipei University of
   Technology; Industrial Technology Research Institute - Taiwan; National
   Yunlin University Science & Technology
RP Yeh, CH (corresponding author), Natl Taiwan Normal Univ, Dept Elect Engn, Taipei 10610, Taiwan.
EM 810523001@gms.ndhu.edu.tw; cmj@gms.ndhu.edu.tw; chyeh@ntnu.edu.tw;
   410623020@gms.ndhu.edu.tw; ljkau@ntut.edu.tw; chuanyu@yuntech.edu.tw;
   sylvia821120@gmail.com
RI Chang, Chuan-Yu/X-9186-2019
OI Chang, Chuan-Yu/0000-0001-9476-8130; Kau, Lih-Jen/0000-0001-8115-3751
FU Ministry of Science and Technology, Taiwan [MOST 106-2221-E-259-009-MY3]
FX This work was supported by Ministry of Science and Technology, Taiwan,
   under Grant MOST 106-2221-E-259-009-MY3. The associate editor
   coordinating the reviewof this manuscript and approving it for
   publication was Dr. S. Mehrotra.
CR Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Bjontegaard G., 2008, VCEGAI11
   Bjotegaard G., 2001, VCEGM33
   Burdea G. C., 2003, Virtual reality technology
   Chen J, 2017, J VIS COMMUN IMAGE R, V48, P329, DOI 10.1016/j.jvcir.2017.05.006
   Chen Y., 2015, JCT3VJ1003
   Chen Y, 2016, IEEE T MULTIMEDIA, V18, P576, DOI 10.1109/TMM.2016.2525010
   De Silva DVSX, 2011, IEEE J-STSP, V5, P335, DOI 10.1109/JSTSP.2011.2108113
   De Silva DVSX, 2010, IEEE IMAGE PROC, P4013, DOI 10.1109/ICIP.2010.5653353
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fu CH, 2019, IEEE ACCESS, V7, P173138, DOI 10.1109/ACCESS.2019.2956994
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Hamout H, 2020, J REAL-TIME IMAGE PR, V17, P1285, DOI 10.1007/s11554-019-00890-x
   Hamout H, 2020, IEEE T CIRC SYST VID, V30, P1933, DOI 10.1109/TCSVT.2019.2918770
   Hamout H, 2020, SIGNAL IMAGE VIDEO P, V14, P1301, DOI 10.1007/s11760-020-01669-5
   Hsu Y. C, 2019, THESIS NATL DONG HWA
   Hsu YC, 2018, 2018 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS 2018), P127, DOI 10.1109/APCCAS.2018.8605601
   Jing RH, 2019, SIGNAL IMAGE VIDEO P, V13, P209, DOI 10.1007/s11760-018-1347-0
   Jung SW, 2012, IEEE T IMAGE PROCESS, V21, P3624, DOI 10.1109/TIP.2012.2191569
   Liu C, 2020, IEEE DATA COMPR CONF, P381, DOI 10.1109/DCC47342.2020.00067
   Muller K., 2014, JCT3VG1100 ISOIEC JT
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Saldanha M, 2020, IEEE T CIRC SYST VID, V30, P850, DOI 10.1109/TCSVT.2019.2898122
   Sanchez G, 2018, J VIS COMMUN IMAGE R, V54, P193, DOI 10.1016/j.jvcir.2018.05.003
   Smolic A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2161, DOI 10.1109/ICME.2006.262683
   Stankiewicz O, 2018, IEEE T MULTIMEDIA, V20, P2182, DOI 10.1109/TMM.2018.2790162
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang HB, 2018, IEEE T CIRC SYST VID, V28, P513, DOI 10.1109/TCSVT.2016.2612693
   Zhang Q, 2019, IEEE ACCESS, V7, P155448, DOI 10.1109/ACCESS.2019.2940706
   Zhang RY, 2020, J REAL-TIME IMAGE PR, V17, P1637, DOI 10.1007/s11554-019-00920-8
   Zuo JB, 2019, IEEE ACCESS, V7, P34265, DOI 10.1109/ACCESS.2019.2897161
   2001, N4063 ISOIECJTC1SC29
NR 35
TC 12
Z9 12
U1 2
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1707
EP 1720
DI 10.1109/TMM.2021.3070106
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200035
DA 2024-07-18
ER

PT J
AU Zhang, RH
   Xu, LX
   Yu, ZY
   Shi, Y
   Mu, CP
   Xu, M
AF Zhang, Ruiheng
   Xu, Lixin
   Yu, Zhengyu
   Shi, Ye
   Mu, Chengpo
   Xu, Min
TI Deep-IRTarget: An Automatic Target Detector in Infrared Imagery Using
   Dual-Domain Feature Extraction and Allocation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks; feature extraction; infrared imagery;
   object detection
ID RECOGNITION
AB Recently, convolutional neural networks (CNNs) have brought impressive improvements for object detection. However, detecting targets in infrared images still remains challenging, because the poor texture information, low resolution and high noise levels of the thermal imagery restrict the feature extraction ability of CNNs. In order to deal with these difficulties in the feature extraction, we propose a novel backbone network named Deep-IRTarget, composing of a frequency feature extractor, a spatial feature extractor and a dual-domain feature resource allocation model. Hypercomplex Infrared Fourier Transform is developed to calculate the infrared intensity saliency by designing hypercomplex representations in the frequency domain, while a convolutional neural network is invoked to extract feature maps in the spatial domain. Features from the frequency domain and spatial domain are stacked to construct Dual-domain features. To efficiently integrate and recalibrate them, we propose a Resource Allocation model for Features (RAF). The well-designed channel attention block and position attention block are used in RAF to respectively extract interdependent relationships among channel and position dimensions, and capture channel-wise and position-wise contextual information. Extensive experiments are conducted on three challenging infrared imagery databases. We achieve 10.14%, 9.1% and 8.05% improvement on mAP scores, compared to the current state of the art method on MWIR, BITIR and WCIR respectively.
C1 [Zhang, Ruiheng; Xu, Lixin; Mu, Chengpo] Beijing Inst Technol, Sch Mechatron Engn, Beijing 100081, Peoples R China.
   [Shi, Ye] Shanghaitech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
   [Yu, Zhengyu; Xu, Min] Univ Technol Sydney, Fac Elect & Informat Technol, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.
C3 Beijing Institute of Technology; ShanghaiTech University; University of
   Technology Sydney
RP Xu, M (corresponding author), Univ Technol Sydney, Fac Elect & Informat Technol, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.
EM ruiheng.zhang@bit.edu.cn; lxxu@bit.edu.cn; zhengyu.yu@uts.edu.au;
   ye.shi@alumni.uts.edu.au; muchengpo@bit.edu.cn; min.xu@uts.edu.au
RI Shi, Ye/AAE-4306-2022; xu, li/GNH-3667-2022
OI Shi, Ye/0000-0003-2302-4980; Yu, Zhengyu/0000-0002-8888-4570; Zhang,
   Ruiheng/0000-0002-5460-7196
FU Beijing Institute of Technology Research Fund Program for Young Scholars
FX This work was supported by the Beijing Institute of Technology Research
   Fund Program for Young Scholars. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. P.
   K. Atrey.
CR Andreone L, 2002, IEEE 5TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P141, DOI 10.1109/ITSC.2002.1041203
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bertozzi M, 2007, COMPUT VIS IMAGE UND, V106, P194, DOI 10.1016/j.cviu.2006.07.016
   Besbes Bassem, 2010, 2010 13th International IEEE Conference on Intelligent Transportation Systems (ITSC 2010), P1869, DOI 10.1109/ITSC.2010.5625285
   Braga-Neto U, 2004, J ELECTRON IMAGING, V13, P802, DOI 10.1117/1.1789982
   Cai YF, 2017, IEEE ACCESS, V5, P5013, DOI 10.1109/ACCESS.2017.2695721
   Chen Y., 2013, MIPPR 2013: Automatic Target Recognition and Navigation, V8918, P174
   Ding M, 2019, J AEROSP INFORM SYST, V16, P94, DOI 10.2514/1.I010655
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   ELL TA, 1993, PROCEEDINGS OF THE 32ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P1830, DOI 10.1109/CDC.1993.325510
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Han J, 2007, PATTERN RECOGN, V40, P1771, DOI 10.1016/j.patcog.2006.11.010
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huimin Lu, 2010, Proceedings of the 2010 International Conference on Intelligent Control and Information Processing (ICICIP 2010), P79, DOI 10.1109/ICICIP.2010.5564346
   Khan MNA, 2014, IEEE COMPUT SOC CONF, P293, DOI 10.1109/CVPRW.2014.52
   Kingma D. P., 2014, arXiv
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Li XL, 2020, IEEE T GEOSCI REMOTE, V58, P6776, DOI 10.1109/TGRS.2020.2973969
   Li Z, 2021, IEEE T MULTIMEDIA, V23, P306, DOI 10.1109/TMM.2020.2978640
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Q, 2020, IEEE T MULTIMEDIA, V22, P666, DOI 10.1109/TMM.2019.2932615
   Maurer T, 2005, P SOC PHOTO-OPT INS, V5784, P201, DOI 10.1117/12.604745
   Nasrabadi NM, 2019, IEEE T AERO ELEC SYS, V55, P2687, DOI 10.1109/TAES.2019.2894050
   Paszke A, 2019, ADV NEUR IN, V32
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Popescu M., 2012, Computational Intelligence for Security and Defence Applications (CISDA), 2012 IEEE Symposium on, P1
   Ramachandran P, 2019, ADV NEUR IN, V32
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shunyong Zhou, 2011, 2011 International Conference on Multimedia Technology, P5421
   Sun SG, 2001, OPT ENG, V40, P2638, DOI 10.1117/1.1409563
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2016, J SENSORS, V2016, DOI 10.1155/2016/3403451
   Wang L, 2008, PATTERN RECOGN, V41, P920, DOI 10.1016/j.patcog.2007.07.012
   Yang H, 2021, IEEE T MULTIMEDIA, V23, P572, DOI 10.1109/TMM.2020.2985536
   Yu LJ, 2015, SENSORS-BASEL, V15, P10118, DOI 10.3390/s150510118
   Zhang BY, 2007, OPT ENG, V46, DOI 10.1117/1.2799509
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang RH, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107260
   Zhang RH, 2019, IEEE ACCESS, V7, P153734, DOI 10.1109/ACCESS.2019.2947657
   Zhang RH, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.1.013012
   Zhang RH, 2016, PROC SPIE, V10033, DOI 10.1117/12.2245095
   Zheng GD, 2019, CHIN CONTR CONF, P8649, DOI [10.23919/chicc.2019.8866344, 10.23919/ChiCC.2019.8866344]
   Zhou K, 2016, DESTECH TRANS COMP
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
NR 57
TC 67
Z9 67
U1 15
U2 78
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1735
EP 1749
DI 10.1109/TMM.2021.3070138
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0I7PA
UT WOS:000779607000001
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Li, CH
   Zhang, PY
   Wang, CB
AF Li, Chenhui
   Zhang, Peiying
   Wang, Changbo
TI Harmonious Textual Layout Generation Over Natural Images via Deep
   Aesthetics Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layout; Visualization; Graphics; Saliency detection; Proposals; Feature
   extraction; Task analysis; Textual layout; saliency detection; image
   aesthetics; graphics design; deep learning
ID VISUAL-ATTENTION
AB Automatic typography is important because it helps designers avoid highly repetitive tasks and amateur users achieve high-quality textual layout designs. However, there are often many parameters and complicated aesthetic rules that need to be adjusted in automatic typography work. In this paper, we propose an efficient deep aesthetics learning approach to generate harmonious textual layout over natural images, which can be decomposed into two stages, saliency-aware text region proposal and aesthetics-based textual layout selection. Our method incorporates both semantic features and visual perception principles. First, we propose a semantic visual saliency detection network combined with a text region proposal algorithm to generate candidate text anchors with various positions and sizes. Second, a discriminative deep aesthetics scoring model is developed to assess the aesthetic quality of the candidate textual layouts. We build a new Textual Layout Aesthetics dataset with dense annotations of each image and design a reasonable evaluation metric to compare our method with richer baselines. The results demonstrate that our method can generate harmonious textual layouts in various actual scenarios with better performance.
C1 [Li, Chenhui; Zhang, Peiying; Wang, Changbo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
C3 East China Normal University
RP Li, CH (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
EM chli@sei.ecnu.edu.cn; zhangpeiying17@gmail.com; cbwang@cs.ecnu.edu.cn
RI Li, Chenhui/AAR-3682-2020
OI Li, Chenhui/0000-0001-9835-2650; Zhang, Peiying/0000-0003-3517-808X;
   Wang, Changbo/0000-0001-8940-6418
FU National Natural Science Foundation of China [61802128, 62072183]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61802128 and 62072183. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xavier Alameda-Pineda.(
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   [Anonymous], 2007, DIGITAL WATER MARKIN
   ARKIE, 2019, ARK IMPR DES EFF
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Bylinskii Z, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P57, DOI 10.1145/3126594.3126653
   Canva, 2020, CANV DES COLL SHAR
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cui C., 2020, ACM T MULTIM COMPUT, V16, P1
   Cui CR, 2020, INFORM SCIENCES, V512, P780, DOI 10.1016/j.ins.2019.10.011
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Cui WW, 2020, IEEE T VIS COMPUT GR, V26, P906, DOI 10.1109/TVCG.2019.2934785
   Damera-Venkata N, 2011, DOCENG 2011: PROCEEDINGS OF THE 2011 ACM SYMPOSIUM ON DOCUMENT ENGINEERING, P3
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hsin-Ying Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P491, DOI 10.1007/978-3-030-58580-8_29
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jahanian Ali, 2013, P 2013 INT C INTELLI, P95
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Li JN, 2021, IEEE T PATTERN ANAL, V43, P2388, DOI 10.1109/TPAMI.2019.2963663
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Micallef L, 2017, IEEE T VIS COMPUT GR, V23, P1588, DOI 10.1109/TVCG.2017.2674978
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   O'Donovan P, 2014, IEEE T VIS COMPUT GR, V20, P1200, DOI 10.1109/TVCG.2014.48
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Pan J., 2017, PROC IEEE C COMPUT V
   Paszke A, 2019, ADV NEUR IN, V32
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Qiang YT, 2016, AAAI CONF ARTIF INTE, P51
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Shuyang Gu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P369, DOI 10.1007/978-3-030-58621-8_22
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tu Y, 2020, AAAI CONF ARTIF INTE, V34, P12104
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   WC3, 2008, WEB CONTENT ACCESSIB
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Yang XY, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818709
   Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610
   Zhang P., 2020, PROC IEEE INT C MULT, P1
   Zhang PY, 2021, IEEE T VIS COMPUT GR, V27, P326, DOI 10.1109/TVCG.2020.3030343
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhao NX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201355
   Zheng XR, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322971
NR 58
TC 6
Z9 6
U1 6
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 30
PY 2021
VL 24
BP 3416
EP 3428
DI 10.1109/TMM.2021.3097900
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NC
UT WOS:000824706100001
DA 2024-07-18
ER

PT J
AU Delmotte, A
   Tanaka, K
   Kubo, H
   Funatomi, T
   Mukaigawa, Y
AF Delmotte, Arnaud
   Tanaka, Kenichiro
   Kubo, Hiroyuki
   Funatomi, Takuya
   Mukaigawa, Yasuhiro
TI Blind 3D-Printing Watermarking Using Moment Alignment and Surface Norm
   Distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Watermarking; Three-dimensional displays; Printers; Three-dimensional
   printing; Robustness; Histograms; 3D moments; 3D printing; 3D scanning;
   blind watermarking; surface norms
ID MESH; ROBUST
AB The recent development of 3D printing technology has brought concerns about its potential misuse, such as in copyright infringement, and crimes. Although there have been many studies on blind 3D mesh watermarking for the copyright protection of digital objects, methods applicable to 3D printed objects are rare. In this paper, we propose a novel blind watermarking algorithm for 3D printed objects with applications for copyright protection, traitor tracing, object identification, and crime investigation. Our method allows us to embed a few bits of data into a 3D-printed object, and retrieve it by 3D scanning without requiring any information about the original mesh. The payload is embedded on the object's surface by slightly modifying the distribution of surface norms, that is, the distance between the surface, and the center of gravity. It is robust to resampling and can work with any 3D printer, and scanner technology. In addition, our method increases the capacity, and resistance by subdividing the mesh into a set of bins, and spreading the data over the entire surface to negate the effect of local printing artifacts. The method's novelties include extending the vertex norm histogram to a continuous surface, and the use of 3D moments to synchronize a watermark signal in a 3D-printing context. In the experiments, our method was evaluated using a public dataset against center, orientation, minimum, and maximum norm misalignments; a printing simulation; and actual print/scan experiments using a standard 3D printer, and scanner.
C1 [Delmotte, Arnaud; Tanaka, Kenichiro; Funatomi, Takuya; Mukaigawa, Yasuhiro] Nara Inst Sci & Technol, Opt Media Interface Lab, 8916-5 Takayama Cho, Ikoma, Nara 6300192, Japan.
   [Kubo, Hiroyuki] Tokai Univ, Minato Ku, 2-3-23 Takanawa, Tokyo 1088619, Japan.
C3 Nara Institute of Science & Technology; Tokai University
RP Delmotte, A (corresponding author), Nara Inst Sci & Technol, Opt Media Interface Lab, 8916-5 Takayama Cho, Ikoma, Nara 6300192, Japan.
EM delmotte_ar@yahoo.fr; ktanaka@is.naist.jp; hkubo@tsc.u-tokai.ac.jp;
   funatomi@is.naist.jp; mukaigawa@is.naist.jp
RI Funatomi, Takuya/K-5919-2018
OI Funatomi, Takuya/0000-0001-5588-5932; Kubo,
   Hiroyuki/0000-0002-7061-7941; Mukaigawa, Yasuhiro/0000-0001-8689-3724;
   delmotte, arnaud/0000-0002-2536-1887
FU JSPS KAKEN [JP17K19979]
FX This work was supported by JSPS KAKEN JP17K19979. The associate editor
   coordinating the reviewof this manuscript and approving it for
   publication was Prof. Xiaochun Cao.
CR [Anonymous], 2008, MESHLAB OPEN SOURCE, DOI DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136
   Beekhof F. P., 2008, P 1 INT C FOR APPL T, P28
   Cho JW, 2007, IEEE T SIGNAL PROCES, V55, P142, DOI 10.1109/TSP.2006.882111
   Delmotte A, 2020, IEEE T MULTIMEDIA, V22, P2780, DOI 10.1109/TMM.2019.2962306
   Delmotte A, 2018, 2018 JOINT 7TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2018 2ND INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR), P282, DOI 10.1109/ICIEV.2018.8640986
   Electronic frontier foundation, IS YOUR PRINT SPYING
   Greeff GP, 2017, ADDIT MANUF, V14, P31, DOI 10.1016/j.addma.2016.12.005
   Harrison C, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P563
   Hodgson G., 2011, SLIC3R MANUAL FLOW M
   Hou J.U., 2015, P 3 ACM WORKSHOP INF, P115
   Hou JU, 2018, IEEE ACCESS, V6, P44082, DOI 10.1109/ACCESS.2018.2864331
   Hou JU, 2017, IEEE T INF FOREN SEC, V12, P2712, DOI 10.1109/TIFS.2017.2718482
   Hu R, 2009, INT CONF ACOUST SPEE, P1501, DOI 10.1109/ICASSP.2009.4959880
   Jae-Won Cho, 2004, Digital Watermarking. Third International Workshop, IWDW 2004. Revised Selected Papers (Lecture Notes in Computer Science Vol. 3304), P283
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Kai Wang, 2010, Proceedings of the Shape Modeling International (SMI 2010), P231, DOI 10.1109/SMI.2010.33
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kazemi R, 2016, IEEE T MULTIMEDIA, V18, P2345, DOI 10.1109/TMM.2016.2599149
   Konstantinides JM, 2009, IEEE T MULTIMEDIA, V11, P23, DOI 10.1109/TMM.2008.2008913
   Kumar A., 2016, uS Patent, Patent No. [9,400,910, 9400910]
   Lavoué G, 2006, PROC SPIE, V6312, DOI 10.1117/12.686964
   Lee GW, 2005, IEICE T FUND ELECTR, VE88A, P1512, DOI 10.1093/ietfec/e88-a.6.1512
   Lee SH, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P105
   Li DZY, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P449, DOI 10.1145/3126594.3126635
   Li ZX, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P1306, DOI 10.1145/3243734.3243735
   Lipson H., 2013, Fabricated: The new world of 3D printing, P65
   Liu Y, 2012, IEEE T INF FOREN SEC, V7, P1459, DOI 10.1109/TIFS.2012.2204251
   Liu Y, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P43
   Macq B, 2015, WEB3D 2015, P89, DOI 10.1145/2775292.2775313
   Maia HT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322960
   McNeight D. L., 2015, U.S. Patent App., Patent No. [14/250,533, 14250533]
   Medimegh N., 2015, Int J Multimed, V1
   Molitch-Hou M, 2016, FUTURE HPS MULTIJET
   Nieves Javier, 2010, 2010 21st International Conference on Database and Expert Systems Applications, P247, DOI 10.1109/DEXA.2010.86
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Okada A, 2015, PROC SPIE, V9599, DOI 10.1117/12.2189486
   Stütz T, 2014, IEEE T MULTIMEDIA, V16, P1337, DOI 10.1109/TMM.2014.2310595
   Voris J., 2017, U.S. Patent, Patent No. [9,656,428, 9656428]
   Walther G, 2015, SCI ENG ETHICS, V21, P1435, DOI 10.1007/s11948-014-9617-x
   Wang K, 2008, IEEE T MULTIMEDIA, V10, P1513, DOI 10.1109/TMM.2008.2007350
   Wee J. Y. S., 2015, U.S. Patent App., Patent No. [14/485,880, 14485880]
   Willis KDD, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461936
   Wu DZ, 2015, COMPUT AIDED DESIGN, V59, P1, DOI 10.1016/j.cad.2014.07.006
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981
   Yamazaki S, 2014, INT C PATT RECOG, P4576, DOI 10.1109/ICPR.2014.783
   Yusuf B., 2018, DIGITALLY AUGMENTED
   Zhang C, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P935, DOI 10.1109/ICIP.2001.958278
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang X., 2018, EUROGRAPHICS ASS
   Zhekun L., 2004, P DETC, V4, P1
   Zhu XS, 2014, IEEE T MULTIMEDIA, V16, P1888, DOI 10.1109/TMM.2014.2340695
NR 51
TC 7
Z9 7
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3467
EP 3482
DI 10.1109/TMM.2020.3025660
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100003
OA hybrid
DA 2024-07-18
ER

PT J
AU Du, Y
   Han, GQ
   Tan, YJ
   Xiao, CF
   He, SF
AF Du, Yong
   Han, Guoqiang
   Tan, Yinjie
   Xiao, Chufeng
   He, Shengfeng
TI Blind Image Denoising via Dynamic Dual Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise level; Image denoising; Noise reduction; Task analysis; Noise
   measurement; Estimation; Optimization; Blind image denoising;
   convolutional neural networks; dual learning; Gaussian noise
ID SPARSE; FRAMEWORK; ALGORITHM; CNN
AB Existing discriminative learning methods for image denoising use either a single residual learning or a nonresidual learning design. However, we observe that these two schemes perform differently with the same noise level, and yet, there have been no explorations regarding whether residual or nonresidual designs are better suited for denoising. Additionally, many discriminative denoisers are designed to learn a model that corresponds to a fixed noise level, which means that multiple models are required to recover corrupted images with noise at different levels. In this paper, we propose a dynamic dual learning network for blind image denoising, namely, DualBDNet. Instead of modeling a sole task prediction network, the proposed DualBDNet investigates the inherent relations between the residual estimation and the nonresidual estimation. In particular, DualBDNet produces task-dependent feature maps, and each part of the features is devoted to one specific task (residual/nonresidual mapping). To address different noise levels with a single network or even cases where the statistics of noise are unknown, we further introduce an embedded subnetwork into DualBDNet. One output of the subnetwork is the learning of a dynamic compositional attention to highlight the more significant task-dependent feature maps, adaptively coinciding with the extent of corruption. The other output is the learning of a weight used for fusion of the results to ensure an end-to-end manner. Extensive experiments demonstrate that the proposed DualBDNet outperforms the state-of-the-art methods on both synthetic and real noisy images without estimating the noise levels as input.
C1 [Du, Yong] Ocean Univ China, Dept Comp Sci & Technol, Qingdao 266100, Peoples R China.
   [Han, Guoqiang; Tan, Yinjie; He, Shengfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China.
   [Xiao, Chufeng] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
C3 Ocean University of China; South China University of Technology; City
   University of Hong Kong
RP He, SF (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China.
EM csyongdu@ouc.edu.cn; csgqhan@scut.edu.cn; 3qty3q@gmail.com;
   chufengxiao@outlook.com; hesfe@scut.edu.cn
RI Du, Yong/AAS-3535-2020; He, Shengfeng/E-5682-2016
OI Du, Yong/0000-0003-1864-8326; He, Shengfeng/0000-0002-3802-4644; Xiao,
   Chufeng/0000-0001-6749-0161
FU National Natural Science Foundation of China [61472145, 61972162,
   61702194]; Special Fund of Science and Technology Research and
   Development of Applications From Guangdong Province (SF-STRDA-GD)
   [2016B010127003]; Guangzhou Key Industrial Technology Research fund
   [201802010036]; Guangdong Natural Science Foundation [2017A030312008];
   CCF-Tencent Open Research fund (CCF-Tencent) [RAGR20190112]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61472145, 61972162, and 61702194, in
   part by the Special Fund of Science and Technology Research and
   Development of Applications From Guangdong Province (SF-STRDA-GD) under
   Grant 2016B010127003, in part by the Guangzhou Key Industrial Technology
   Research fund under Grant 201802010036, in part by the Guangdong Natural
   Science Foundation under Grant 2017A030312008, and in part by the
   CCF-Tencent Open Research fund (CCF-Tencent RAGR20190112).
CR [Anonymous], 2018, P 35 INT C MACH LEAR
   [Anonymous], 2015, P INT C LEARN REP
   [Anonymous], 2016, P ADV NEUR INF PROC
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chen MJ, 2011, IEEE T MULTIMEDIA, V13, P1195, DOI 10.1109/TMM.2011.2166538
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Cho SI, 2019, IEEE T MULTIMEDIA, V21, P484, DOI 10.1109/TMM.2018.2859791
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Dabov Kostadin, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P145
   Dabov K, 2007, IEEE IMAGE PROC, P313, DOI 10.1109/icip.2007.4378954
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Franzen Rich, 1999, KODAK LOSSLESS TRUE, V4
   Fu Y, 2019, INT C LEARN REPR
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jakhetiya V, 2017, IEEE T MULTIMEDIA, V19, P93, DOI 10.1109/TMM.2016.2609419
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kligvasser I, 2018, PROC CVPR IEEE, P2433, DOI 10.1109/CVPR.2018.00258
   Kong ZM, 2018, IEEE T MED IMAGING, V37, P941, DOI 10.1109/TMI.2017.2778230
   Lebrun M, 2015, IMAGE PROCESS ON LIN, V5, P1, DOI 10.5201/ipol.2015.125
   LEE JS, 1981, COMPUT VISION GRAPH, V15, P380, DOI 10.1016/S0146-664X(81)80018-4
   Liu D., 2018, Advances in Neural Information Processing Systems, P1673
   Liu P., 2018, P IEEE C COMP VIS PA, P773
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Maiti S, 2019, INT CONF ACOUST SPEE, P6995, DOI 10.1109/ICASSP.2019.8683130
   Mäkitalo M, 2014, IEEE T IMAGE PROCESS, V23, P5348, DOI 10.1109/TIP.2014.2363735
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Peng Y, 2014, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2014.377
   Plotz T., 2017, CVPR
   Plotz Tobias, 2018, ADV NEURAL INFORM PR, P1087
   Roth S, 2005, PROC CVPR IEEE, P860
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wang HY, 2017, IEEE T MULTIMEDIA, V19, P969, DOI 10.1109/TMM.2016.2638624
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Xu J., 2018, ECCV, P20
   Xu J., 2017, P IEEE INT C COMP VI, P1096
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P1625, DOI 10.1109/TMM.2017.2672198
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P822, DOI 10.1109/TMM.2016.2626969
   Yang D, 2018, IEEE SIGNAL PROC LET, V25, P55, DOI 10.1109/LSP.2017.2768660
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhu YC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P130, DOI 10.1145/3343031.3350997
NR 57
TC 13
Z9 13
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2139
EP 2152
DI 10.1109/TMM.2020.3008057
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100023
DA 2024-07-18
ER

PT J
AU Fujii, K
   Sugimura, D
   Hamamoto, T
AF Fujii, Katsuya
   Sugimura, Daisuke
   Hamamoto, Takayuki
TI Hierarchical Group-Level Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Faces; Facial features; Visualization; Semantics;
   Face recognition; Feature extraction; Group-level emotion recognition;
   hierarchical classification; facial expression; scene features
AB Group-level emotion recognition is a technique for estimating the emotion of a group of people. In this paper, we propose a novel method for group-level emotion recognition. Our method lies in the two-fold contributions: (1) recognition of group-level emotion using a hierarchical classification approach; (2) incorporation of novel features to contribute to the description of the group-level emotion. We consider that the use of facial expressions of people will only be effective in differentiating images labeled as "Positive" because those labeled as "Neutral" or "Negative" are likely to include similar facial expressions. Therefore, we first perform binary classification based on facial expression recognition to distinguish "Positive" labels that include discriminative facial expressions (e.g., smile) from the others. We evaluate outcomes that are not classified as "Positive" during the first classification by exploiting scene features that describe what type of events (e.g., demonstration or funeral) are shown in the image. The other novelty of our method lies in two-fold. The first is the exploitation of visual attention for the first classification. It allows us to estimate which faces are the main subjects in the target image, thereby suppressing the influences of faces in the background that contribute less to group-level emotion. The second is the exploitation of object-wise semantic information (labels) for the second classification. This allows a more detailed description of the scene context in the image and enables performance enhancement in the second classification. We demonstrate the effectiveness of our method through experiments using public datasets.
C1 [Fujii, Katsuya; Hamamoto, Takayuki] Tokyo Univ Sci, Dept Elect Engn, Tokyo 1258585, Japan.
   [Sugimura, Daisuke] Tsuda Univ, Dept Comp Sci, Tokyo 1878577, Japan.
C3 Tokyo University of Science
RP Sugimura, D (corresponding author), Tsuda Univ, Dept Comp Sci, Tokyo 1878577, Japan.
EM srnty5566@icloud.com; sugimura@tsuda.ac.jp; hamamoto@rs.tus.ac.jp
OI Hamamoto, Takayuki/0000-0001-8246-8325; Sugimura,
   Daisuke/0000-0002-8305-9790
CR Abbas A., 2017, P 19 ACM INT C MULT, P561, DOI [10.1145/3136755.3143010, DOI 10.1145/3136755.3143010]
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2012, P IEEE INT C COMPL S
   [Anonymous], 2017, P 19 ACM INT C MULT
   Basavaraju S, 2020, IEEE T COMPUT SOC SY, V7, P600, DOI 10.1109/TCSS.2020.2973208
   Bawa V. S., 2018, NEURAL COMPUT APPL, P1
   Beal DJ, 2003, J APPL PSYCHOL, V88, P989, DOI 10.1037/0021-9010.88.6.989
   Bradley MM, 2000, PSYCHOPHYSIOLOGY, V37, P204, DOI 10.1017/S0048577200990012
   Cao P, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), P173, DOI 10.1109/ICIVC.2018.8492827
   Cerekovic A, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P437, DOI 10.1145/2993148.2997628
   Tien DX, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P572, DOI 10.1145/3340555.3355715
   Dejian L., 2020, P 11 INT C GRAPH IM, V1373, P46
   Dhall Abhinav, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163151
   Dhall A., 2017, P 19 ACM INT C MULT, P524, DOI [10.1145/3136755.3143004, DOI 10.1145/3136755.3143004]
   Dhall A, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P653
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Garg S., ARXIV190501118, V2019
   GELADI P, 1986, ANAL CHIM ACTA, V185, P1, DOI 10.1016/0003-2670(86)80028-9
   Ghosh S., 2018, PROC EUR C COMPUT VI, P518
   Ghosh S., 2019, PROC IEEE INT JOINT, P1
   Ghosh S, 2018, IEEE IMAGE PROC, P1967, DOI 10.1109/ICIP.2018.8451242
   Guo X, 2020, IEEE WINT CONF APPL, P2910, DOI [10.1109/WACV45572.2020.9093547, 10.1109/wacv45572.2020.9093547]
   Guo X, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P635, DOI 10.1145/3242969.3264990
   Guo Xin., 2017, P 19 ACM INT C MULTI, P603
   Gupta A, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P611, DOI 10.1145/3242969.3264985
   Hackman J.R., 1992, HDB IND ORG PSYCHOL, V3
   Hamamoto T., 2019, PROC IEEE INT C AUTO, P1
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Hsu LK, 2013, IEEE INT CON MULTI
   Huang X., IN PRESS, DOI [10.1109/TAFFC.2019.2953664, DOI 10.1109/TAFFC.2019.2953664]
   Huang XH, 2018, IEEE T MULTIMEDIA, V20, P2706, DOI 10.1109/TMM.2018.2818015
   Jiang M, 2014, LECT NOTES COMPUT SC, V8695, P17, DOI 10.1007/978-3-319-10584-0_2
   Khan AS, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P623, DOI 10.1145/3242969.3264987
   Khosla A, 2015, IEEE I CONF COMP VIS, P2390, DOI 10.1109/ICCV.2015.275
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kuang HF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P48, DOI 10.1145/3343031.3351001
   Li JS, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P487, DOI 10.1145/2993148.2997636
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Liu NJ, 2018, LECT NOTES COMPUT SC, V11165, P24, DOI 10.1007/978-3-030-00767-6_3
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Mehmood T, 2012, CHEMOMETR INTELL LAB, V118, P62, DOI 10.1016/j.chemolab.2012.07.010
   Mou W., 2019, PROC IEEE INT C AUTO, P1
   Nagarajan B., 2019, PROC IEEE INT C AUTO, P1
   Park C., 2007, P IEEE INT S CONS EL, P1
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pittaras N, 2017, LECT NOTES COMPUT SC, V10132, P102, DOI 10.1007/978-3-319-51811-4_9
   Quach K. G., 2018, ARXIV181111849
   Rassadin A., 2017, ACM INT C MULTIMODAL, P544
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Savran A, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P75, DOI 10.1109/ICCVW.2013.17
   Shamir O., 2013, INT C MACH LEARN, P71
   Shamsi S. N., 2018, P IEEE WINT APPL COM, P77
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shifeng Zhang, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P182, DOI 10.1109/TBIOM.2020.2973001
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sokolov D, 2018, IEEE INT CONF AUTOMA, P787, DOI 10.1109/FG.2018.00124
   Sturm D, 2016, IEEE INT CONF SERIOU
   Sun B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P451, DOI 10.1145/2993148.2997640
   Surace Luca, 2017, P 19 ACM INT C MULT, P593
   Tan Lianzhi., 2017, P 19 ACM INT C MULT, P549
   Tarasov A. V., 2018, P AN IM SOC NETW TEX, P1967
   Valderas MT, 2015, IEEE ENG MED BIO, P6134, DOI 10.1109/EMBC.2015.7319792
   Vaswani A, 2017, ADV NEUR IN, V30
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Vonikakis V, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P479, DOI 10.1145/2993148.2997633
   Vyas AS, 2019, INT CONF ADVAN COMPU, P102, DOI [10.1109/ICACCS.2019.8728330, 10.1109/icaccs.2019.8728330]
   Wang K, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P640, DOI 10.1145/3242969.3264991
   Wei Q., 2017, ACM INT C MULTIMODAL, P587, DOI DOI 10.1145/3136755.3143014
   Wold H, 1985, ENCY STAT SCI, P581, DOI DOI 10.1002/0471667196.ESS1914.PUB2
   WOLD S, 1978, TECHNOMETRICS, V20, P397, DOI 10.2307/1267639
   Yildirim N, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P1132, DOI 10.1109/UBMK.2017.8093523
   Yu D., 2019, IEEE ACCESS, V7, P111617
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
NR 75
TC 10
Z9 10
U1 3
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3892
EP 3906
DI 10.1109/TMM.2020.3033125
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100036
DA 2024-07-18
ER

PT J
AU Ghose, S
   Prevost, JJ
AF Ghose, Sanchita
   Prevost, John Jeffrey
TI AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent
   Videos With Deep Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Motion pictures; Visualization; Synchronization; Neural
   networks; Numerical models; Interpolation; Foley; convolutional neural
   network; recurrent network; interpolation; multi-scale temporal
   relational network; cross-modal learning; sound synthesis
ID AIDED VISUAL-SEARCH
AB In movie productions, the Foley artist is responsible for creating an overlay soundtrack that helps the movie come alive for the audience. This requires the artist identify sounds that enhance the experience for the listener, reinforcing the director's intention for the scene. The artist must decide what artificial sound captures the essence of the sound and action depicted in the scene. In this paper, we present AutoFoley, an automated deep-learning tool that is used to synthesize a representative audio track for videos. AutoFoley is used to associate audio files with soundless video or to identify critical scenarios and provide a synthesized, reinforced and time-synchronized soundtrack. Our algorithm is capable of precise recognition of actions as well as interframe relations in fast- moving video clips through incorporating interpolation technique and temporal relational networks (TRN). We employ a robust multiscale recurrent neural network (RNN) and a convolutional neural network (CNN) for better understanding of the intricate input-to-output associations. To evaluate AutoFoley, we create an audio-video dataset containing a variety of sounds frequently used as Foley effects in movies. While the Foley dataset was limited to short-duration videos off the representative activities, this dataset demonstrates the capabilities of our proposed system. We show the synthesized sounds are portrayed with accurate temporal synchronization of the associated visual inputs. Human qualitative testing of AutoFoley shows more than 73% of the test subjects considered the generated soundtrack as original, which is a noteworthy improvement in comparable cross-modal research.
C1 [Ghose, Sanchita; Prevost, John Jeffrey] Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA.
C3 University of Texas System; University of Texas at San Antonio (UTSA)
RP Prevost, JJ (corresponding author), Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA.
EM sanchita.ghose@my.utsa.edu; jeff.prevost@utsa.edu
OI Prevost, John/0000-0002-2303-599X; Ghose, Sanchita/0000-0003-0883-5718
FU Open Cloud Institute at UTSA
FX Manuscript received November 22, 2019; revised April 5, 2020, May 29,
   2020, and June 11, 2020; accepted June 11, 2020. Date of publication
   June 25, 2020; date of current version June 25, 2021. This work was
   supported by Open Cloud Institute at UTSA. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. YeWang.
CR [Anonymous], 2011, P ICML
   [Anonymous], 2016, P ADV NEUR INF PROC
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Ba J. L., 2016, P NIPS DEEP LEARN SM
   Ba LJ, 2014, ADV NEUR IN, V27
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bolia RS, 1999, HUM FACTORS, V41, P664, DOI 10.1518/001872099779656789
   Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   CROCHIERE RE, 1980, IEEE T ACOUST SPEECH, V28, P99, DOI 10.1109/TASSP.1980.1163353
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Ghose Sanchita, AUTOFOLEY SOUND EXAM
   GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Hahn JK, 1998, PRESENCE-TELEOP VIRT, V7, P67, DOI 10.1162/105474698565532
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Huang J, 2013, INT CONF ACOUST SPEE, P7596, DOI 10.1109/ICASSP.2013.6639140
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma D. P., 2015, INT C LEARNING REPRE
   Krueger D., 2017, ICLR
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Majdak P, 2010, ATTEN PERCEPT PSYCHO, V72, P454, DOI 10.3758/APP.72.2.454
   McDermott JH, 2011, NEURON, V71, P926, DOI 10.1016/j.neuron.2011.06.032
   Mujika A., 2017, ADV NEURAL INFORM PR
   MUTHUSAMY YK, 1990, INT CONF ACOUST SPEE, P533, DOI 10.1109/ICASSP.1990.115767
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niwa K, 2018, IEEE T MULTIMEDIA, V20, P2871, DOI 10.1109/TMM.2018.2829187
   Owens A., 2018, COMPUTER VISION ECCV, P631
   Owens A, 2018, INT J COMPUT VISION, V126, P1120, DOI 10.1007/s11263-018-1083-5
   Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Perrott DR, 1996, HUM FACTORS, V38, P702, DOI 10.1518/001872096778827260
   Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458
   SHELTON BR, 1980, PERCEPT PSYCHOPHYS, V28, P589, DOI 10.3758/BF03198830
   Slaney M., 1995, Advances in Neural Information Processing Systems 7, P827
   Soomro K., 2012, ARXIV12120402CS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   van den Doel K, 2001, COMP GRAPH, P537, DOI 10.1145/383259.383322
   Wang FZ, 2017, IEEE T MULTIMEDIA, V19, P418, DOI 10.1109/TMM.2016.2613641
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yu TZ, 2019, IEEE T MULTIMEDIA, V21, P2504, DOI 10.1109/TMM.2019.2907060
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Zaremba W., 2014, ARXIV
   Zhang C., 2018, P EUR C COMP VIS WOR, P560
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
NR 50
TC 13
Z9 16
U1 2
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1895
EP 1907
DI 10.1109/TMM.2020.3005033
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, L
   Li, Z
   Liu, S
   Li, HQ
AF Li, Li
   Li, Zhu
   Liu, Shan
   Li, Houqiang
TI Efficient Projected Frame Padding for Video-Based Point Cloud
   Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Geometry; Software; Two dimensional
   displays; Color; Software algorithms; Transform coding; Frame padding;
   high efficiency video coding; occupancy map; point cloud compression;
   video-based point cloud compression
AB The state-of-the-art 2D-based dynamic point cloud (DPC) compression algorithm is the video-based point cloud compression (V-PCC) developed by the Moving Pictures Experts Group (MPEG). It first projects the DPC patch by patch from 3D to 2D and organizes the projected patches into a video. The video is then efficiently compressed by High Efficiency Video Coding. However, there are many unoccupied pixels that may have a significant influence on the coding efficiency. These unoccupied pixels are currently padded using either the average of 4-neighbors for the geometry or the push-pull algorithm for the color attribute. While these algorithms are simple, the unoccupied pixels are not handled in the most efficient way. In this paper, we divide the unoccupied pixels into two groups: those that should be occupied and those that should not be occupied according to the occupancy map. We then design padding algorithms tailored to each group to improve the rate-distortion performance of the V-PCC reference software, for both the geometry and the color attribute. The first group is the unoccupied pixels that should be occupied according to the block-based occupancy map. We attempt to pad those pixels using the real points in the original DPC to improve the quality of the reconstructed DPC. Additionally, we attempt to maintain the smoothness of each block so as not to negatively influence the video compression efficiency. The second group is the unoccupied pixels that were correctly identified as unoccupied according to the block-based occupancy map. These pixels are useless for improving the reconstructed quality of the DPC. Therefore, we attempt to minimize the bit cost of these pixels without considering their reconstruction qualities. The bit cost is determined by the residue of these pixels obtained by subtracting the prediction pixels from the original pixels. Therefore, we propose padding the residue using the average residue of the occupied pixels in order to minimize the bit cost. The proposed algorithms are implemented in the V-PCC and the corresponding HEVC reference software. The experimental results show the proposed algorithms can bring significant bitrate savings compared with the V-PCC.
C1 [Li, Li; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Li, Zhu] Univ Missouri Kansas City, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
   [Liu, Shan] Tencent Amer, 661 Bryant St, Palo Alto, CA 94301 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Missouri System; University of Missouri Kansas
   City
RP Li, L (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM lil1@ieee.org; zhu.li@ieee.org; shan1@tencent.com; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
FU National Key Research and Development Plan [2017YFB1002401]; NSF I/UCRC
   on Big Learning [1747751]; Tencent Media Lab
FX Manuscript received July 11, 2019; revisedMarch 7, 2020 and July 15,
   2020; accepted August 9, 2020. Date of publication August 17, 2020; date
   of current version August 24, 2021. This work was supported in part by
   the National Key Research and Development Plan under Grant
   2017YFB1002401 and in part by NSF I/UCRC on Big Learning under Grant
   1747751 and Tencent Media Lab. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Sen-Ching Samson Cheung. (Corresponding author: Li Li.)
CR [Anonymous], 2019, HM1618SCM87
   [Anonymous], 2016, document ISO/IEC JTC1/SC29/WG1 N16331
   [Anonymous], 2019, Point Cloud Compression Category 2 Reference SoftwareTMC2
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Bjotegaard G., 2001, VCEGM33
   Bruder G, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P161, DOI 10.1109/3DUI.2014.6798870
   Budagavi M., 2017, Document m41808
   Champel M.-L., 2017, P SPIE OPT ENG APPL, V10396
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807
   dEon Eugene, 2017, Document ISO/IEC JTC1/SC29/WG11 m40059 and ISO/IEC JTC1/SC29/WG1 M74006
   Faramarzi E., 2019, JTC1SC29WG11 ISOIEC
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Graziosi D, V PCC GORTLER
   Graziosi D., 2019, JTC1SC29WG11 ISOIEC
   Graziosi D., 2019, JTC1SC28WG11 ISOIEC
   He LY, 2017, ASIA-PAC CONF COMMUN, P345
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Ke E.-C., 2019, JTC1SC29WG11 ISOIEC
   Lasserre S., 2017, JTC1SC29WG11 ISOIEC
   Li L, 2019, IEEE IMAGE PROC, P3167, DOI [10.1109/icip.2019.8803233, 10.1109/ICIP.2019.8803233]
   Li L, 2019, IEEE T IMAGE PROCESS, V28, P2342, DOI 10.1109/TIP.2018.2885482
   Li LF, 2020, IEEE T COMPUT SOC SY, V7, P556, DOI 10.1109/TCSS.2020.2980007
   Li M, 2010, IEEE T CIRC SYST VID, V20, P1233, DOI 10.1109/TCSVT.2010.2058475
   Mammou K., 2018, document ISO/IEC JTC1/SC29/WG11 m42640
   Mammou K., 2017, JTC15C29WG11 ISOIEG
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Preda M, 2017, JTC1SC29WG11 ISOIEC
   Schwarz S., 2018, ISO/IEC JTC1/SC29/WG11
   Schwarz S., 2017, JTC1SC29WG11 ISOIEC
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
NR 33
TC 24
Z9 28
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2806
EP 2819
DI 10.1109/TMM.2020.3016894
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600021
DA 2024-07-18
ER

PT J
AU Li, Y
   Zhao, ZQ
   Sun, H
   Cen, YG
   He, ZH
AF Li, Yang
   Zhao, Zhiqun
   Sun, Hao
   Cen, Yigang
   He, Zhihai
TI Snowball: Iterative Model Evolution and Confident Sample Discovery for
   Semi-Supervised Learning on Very Small Labeled Datasets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Semisupervised learning; Predictive models; Error analysis;
   Task analysis; Entropy; Knowledge engineering; Semi-supervised learning;
   classification
ID NEURAL-NETWORKS
AB In this work, we develop a joint sample discovery and iterative model evolution method for semi-supervised learning on very small labeled training sets. We propose a master-teacher-student model framework to provide multi-layer guidance during the model evolution process with multiple iterations and generations. The teacher model is constructed by performing an exponential moving average of the student models obtained from past training steps. The master network combines the knowledge of the student and teacher models with additional access to newly discovered samples. The master and teacher models are then used to guide the training of the student network by enforcing the consistency between their predictions of unlabeled samples and evolve all models when more and more samples are discovered. Our extensive experiments demonstrate that the process of discovering confident samples from the unlabeled dataset, once coupled with the master-teacher-student network evolution, can significantly improve the overall semi-supervised learning performance. For example, on the CIFAR-10 dataset, with a small set of 250 labeled samples, our method achieves an error rate of 11.58%, more than 38% lower than Mean-Teacher (49.91%). When coupled with the MixMatch augmentation and loss function, the improvements are also significant.
C1 [Li, Yang; Zhao, Zhiqun; Sun, Hao; He, Zhihai] Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65211 USA.
   [Cen, Yigang] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
C3 University of Missouri System; University of Missouri Columbia; Beijing
   Jiaotong University
RP He, ZH (corresponding author), Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65211 USA.
EM yltb5@mail.missouri.edu; zzhv7@mail.missouri.edu;
   hshq7@mail.missouri.edu; ygcen@bjtu.edu.cn; hezhi@missouri.edu
RI Sun, Haoyang/KHD-3534-2024; Cen, Yigang/AAC-1999-2019; sun,
   hao/HMD-2991-2023; sun, hao/GRS-7732-2022
OI Li, Yang/0000-0002-8372-1481
FU National Science Foundation [1647213, 1646065]; Direct For Computer &
   Info Scie & Enginr; Division Of Computer and Network Systems [1647213]
   Funding Source: National Science Foundation; Directorate For
   Engineering; Div Of Civil, Mechanical, & Manufact Inn [1646065] Funding
   Source: National Science Foundation
FX This work was supported in part by National Science Foundation under
   Grants 1647213 and 1646065. The associate editor coordinating the review
   of this manuscript and approving it for publication was Chang-Su Kim.
CR [Anonymous], 2014, Advances in neural information processing systems
   [Anonymous], 2001, Proceedings of the 18th International Conference on Machine Learning, ICML '01
   Athiwaratkun B., 2019, P 7 INT C LEARN REPR
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Berthelot David, 2019, ARXIV190502249, P2
   BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Blum A., 2004, Proceedings of the Twenty-first International Conference on Machine Learning, ICML '04, P13
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Chapelle O., 2005, Proceedings of the tenth international workshop on artificial intelligence and statistics, P57
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Grandvalet Y., 2005, CAP, V367, P281
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hoffer E., 2017, P WORKSH 5 INT C LEA
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Jian M, 2016, IEEE T MULTIMEDIA, V18, P458, DOI 10.1109/TMM.2016.2515367
   Ke ZH, 2019, IEEE I CONF COMP VIS, P6727, DOI 10.1109/ICCV.2019.00683
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Laine Samuli, 2017, 5 INT C LEARNING REP, DOI DOI 10.48550/ARXIV.1610.02242
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Li JC, 2020, IEEE T IMAGE PROCESS, V29, P538, DOI 10.1109/TIP.2019.2933724
   Liang JQ, 2017, IEEE T MULTIMEDIA, V19, P1077, DOI 10.1109/TMM.2016.2644862
   Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927
   Mitchell V.W., 1999, EUR J MARKETING, V33, P163, DOI DOI 10.1108/03090569910249229
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Oliver A, 2018, ADV NEUR IN, V31
   Qiao SY, 2018, LECT NOTES COMPUT SC, V11219, P142, DOI 10.1007/978-3-030-01267-0_9
   Rasmus A., 2015, ADV NEURAL INFORM PR, DOI DOI 10.1186/1477-5956-9-S1-S5
   Reed R., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P147, DOI 10.1109/IJCNN.1992.227178
   Riloff E, 2003, PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P105
   Rosenberg C, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P29
   Sajjadi Mehdi, 2016, Advances in Neural Information Processing Systems, P1163
   Salimans T, 2016, ADV NEUR IN, V29
   SIETSMA J, 1991, NEURAL NETWORKS, V4, P67, DOI 10.1016/0893-6080(91)90033-2
   Springenberg J. T., 2016, P 4 INT C LEARN REPR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Tarvainen Antti, 2017, ADV NEURAL INFORM PR, P2, DOI DOI 10.1137/0330046
   Triguero I, 2015, KNOWL INF SYST, V42, P245, DOI 10.1007/s10115-013-0706-y
   Verma Vikas, 2019, P 28 INT JOINT C ART, P2
   Whitney M., 2012, P 51TH ANN M ASS COM, V1, P620
   Wu S, 2019, PROC CVPR IEEE, P10083, DOI [10.1109/CVPR.2019.00666, 10.1109/CVPR.2019.01033]
   Wu S, 2018, IEEE T MULTIMEDIA, V20, P851, DOI 10.1109/TMM.2017.2758522
   Yarowsky D., 1995, 33 ANN M ASS COMPUTA, P189, DOI DOI 10.3115/981658.981684
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhu X., CMUCALD02107 AUT LEA
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
   Zhu X., 2006, Computer Science, University of Wisconsin-Madison
NR 50
TC 8
Z9 8
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1354
EP 1366
DI 10.1109/TMM.2020.2997185
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200014
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, J
   Pu, YY
   Nie, RC
   Xu, D
   Zhao, ZP
   Qian, WH
AF Xu, Jun
   Pu, Yuanyuan
   Nie, Rencan
   Xu, Dan
   Zhao, Zhengpeng
   Qian, Wenhua
TI Virtual Try-on Network With Attribute Transformation and Local Rendering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Rendering (computer graphics); Three-dimensional displays;
   Task analysis; Two dimensional displays; Shape; Gallium nitride;
   Attribute transformation; computer vision; generative adversarial
   networks; local rendering; semantic segmentation; virtual try-on
ID JOINT IMAGE SEGMENTATION
AB A virtual try-on network has gradually become a popular topic in recent years. It aims to transfer images of in-shop clothes onto the image of a target person. Owing to the diversity of clothing attributes, developing an image-based virtual try-on network is a complicated task for computers to perform and requires significant effort. Existing methods are unsatisfactory as they cannot preserve the characteristics of the clothes or the target person's identity well, thereby affecting the perception of the generated images; therefore, further research is required. To address this problem, we propose a novel try-on method that combines attribute transformation and local rendering. First, we employ pixel-level semantic segmentation to identify the try-on area and provide implementation conditions for local rendering. Second, we construct a learnable attribute transformation module to complete the try-on task for different attributes. Third, we use a learnable clothing warping module to fit the pose and figure of the target person well and establish a novel loss function, called modified style loss (M-SL), to handle clothes with rich details. Finally, we adopt a local rendering strategy, using which only renders the clothing area to ensure that the details of the non-target area are not lost. Extensive experiments are performed to test our method. The results demonstrate that our method outperforms other state-of-the-art methods.
C1 [Xu, Jun; Pu, Yuanyuan; Nie, Rencan; Xu, Dan; Zhao, Zhengpeng; Qian, Wenhua] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650500, Yunnan, Peoples R China.
   [Pu, Yuanyuan] Key Lab Internet Things Technol & Applicat Yunnan, Kunming 650500, Yunnan, Peoples R China.
   [Nie, Rencan; Qian, Wenhua] Southeast Univ, Sch Automat, Nanjing 210096, Peoples R China.
C3 Yunnan University; Southeast University - China
RP Pu, YY (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650500, Yunnan, Peoples R China.
EM ujun951020@163.com; yuanyuanpu@ynu.edu.cn; rcnie@ynu.edu.cn;
   danxu@ynu.edu.cn; 517295759@qq.com; qwhua003@sina.com
RI Xu, Dan/KPA-7396-2024
OI Xu, Dan/0000-0003-4602-3550; Nie, Rencan/0000-0003-0568-1231
FU National Natural Science Foundation of China [61163019, 61271361,
   61761046, U1802271, 61662087, 62061049]; Yunnan Science and Technology
   Department Project [2014FA021, 2018FB100]; Key Program of the Applied
   Basic Research Programs of Yunnan [202001BB050043, 2019FA044]; Major
   Special Science and Technology of Yunnan [202002AD080001]; Reserve
   Talents for Yunnan Young and Middle-aged Academic and Technical Leaders
   [2019HB121]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61163019, 61271361, 61761046, U1802271, 61662087,
   and 62061049; in part by Yunnan Science and Technology Department
   Project under Grants 2014FA021 and 2018FB100; in part by Key Program of
   the Applied Basic Research Programs of Yunnan under Grants
   202001BB050043 and 2019FA044; in part by Major Special Science and
   Technology of Yunnan under Grant 202002AD080001; and in part by the
   Reserve Talents for Yunnan Young and Middle-aged Academic and Technical
   Leaders under Grant 2019HB121.
CR [Anonymous], 1977, Construction theory of functions of several variables, DOI [DOI 10.1007/BFB0086566, 10.1007/BFb0086566]
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dong H., 2019, FASHION EDITING MULT
   Gu SY, 2018, PROC CVPR IEEE, P8222, DOI 10.1109/CVPR.2018.00858
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hauswiesner S, 2013, IEEE T VIS COMPUT GR, V19, P1552, DOI 10.1109/TVCG.2013.67
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hsiao WL, 2019, IEEE I CONF COMP VIS, P5046, DOI 10.1109/ICCV.2019.00515
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Issenhuth Thibaut, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P619, DOI 10.1007/978-3-030-58565-5_37
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim J, 2008, J INTERACT MARK, V22, P45, DOI 10.1002/dir.20113
   Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Leibe B., 2004, WORKSHOP STAT LEARN, V2
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liu L, 2019, IEEE TNNLS
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Minar M. R., 2020, P IEEECVF C COMPUTER
   Mir A., 2020, IEEE C COMPUT VIS PA
   Mo S., 2018, Instagan: Instance-aware image-to-image translation
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Patel C., 2020, IEEE C COMPUT VIS PA
   PONSMOLL G, 2017, ACM T GRAPHIC, V36, P1, DOI DOI 10.1145/3072959.3073711
   Preuer K, 2018, J CHEM INF MODEL, V58, P1736, DOI 10.1021/acs.jcim.8b00234
   Reed S, 2016, PR MACH LEARN RES, V48
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sekine M., 2014, Int. Conf. on 3D Body Scanning Technologies, P406
   Simonyan K., 2014, 14091556 ARXIV
   Song D, 2020, MULTIMED TOOLS APPL, V79, P33757, DOI 10.1007/s11042-019-08363-w
   Tangseng P., 2017, LOOKING OUTFIT PARSE
   Targ S., 2016, Resnet in Resnet: Generalizing Residual Architectures
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang Jiayun, 2019, ARXIV PREPRINT ARXIV
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yihang Bo, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2265, DOI 10.1109/CVPR.2011.5995609
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 49
TC 8
Z9 9
U1 3
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2222
EP 2234
DI 10.1109/TMM.2021.3070972
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800005
DA 2024-07-18
ER

PT J
AU Xu, J
   Liu, ZA
   Hou, YK
   Zhen, XT
   Shao, L
   Cheng, MM
AF Xu, Jun
   Liu, Zhi-Ang
   Hou, Ying-Kun
   Zhen, Xian-Tong
   Shao, Ling
   Cheng, Ming-Ming
TI Pixel-Level Non-local Image Smoothing With Objective Evaluation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Smoothing methods; Task analysis; Measurement; Benchmark testing; Image
   edge detection; Visualization; Optimization; Image smoothing; benchmark
   dataset; performance evaluation; pixel-level non-local self similarity
AB Recently, imagesmoothing has gained increasing attention due to its prerequisite role in other image processing tasks, e.g., image enhancement and editing. However, the evaluation of image smoothing algorithms is usually performed by subjective observation on images without corresponding ground truths. To promote the development of image smoothing algorithms, in this paper, we construct a novel Nankai Smoothing (NKS) dataset containing 200 images blended by versatile structure images and natural textures. The structure images are inherently smooth and naturally taken as ground truths. On our NKS dataset, we comprehensively evaluate 14 popular image smoothing algorithms. Moreover, we propose a Pixel-level Non-Local Smoothing (PNLS) method to well preserve the structure of the smoothed images, by exploiting the pixel-level non-local self-similarity prior of natural images. Extensive experiments on several benchmark datasets demonstrate that our PNLS outperforms previous algorithms on the image smoothing task. Ablation studies also reveal the work mechanism of our PNLS on image smoothing. To further show its effectiveness, we apply our PNLS on several applications such as semantic region smoothing, detail/edge enhancement, and image abstraction. The dataset and code are available at https://github.com/zal0302/PNLS.
C1 [Xu, Jun; Liu, Zhi-Ang; Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, TKL NDST, Tianjin 300071, Peoples R China.
   [Hou, Ying-Kun] Taishan Univ, Sch Informat Sci & Technol, Tai An 271500, Shandong, Peoples R China.
   [Zhen, Xian-Tong; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Zhen, Xian-Tong; Shao, Ling] Mohamed Bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Nankai University; Taishan University; Mohamed Bin Zayed University of
   Artificial Intelligence
RP Cheng, MM (corresponding author), Nankai Univ, Coll Comp Sci, TKL NDST, Tianjin 300071, Peoples R China.
EM nankaimathxujun@gmail.com; liuzhiang@mail.nankai.edu.cn;
   njusthyk@163.com; zhenxt@gmail.com; ling.shao@ieee.org;
   cmm@nankai.edu.cn
RI Hou, Yingkun/HSE-7936-2023; Cheng, Ming-Ming/A-2527-2009; Shao,
   Ling/D-3535-2011
OI Hou, Yingkun/0000-0003-2153-9040; Cheng, Ming-Ming/0000-0001-5550-8758;
   Liu, Zhiang/0000-0002-3319-4492
FU Major Project for New Generation of AI [2018AAA0100400]; Fundamental
   Research Funds for the Central Universities, Nankai University
   [63201168, 92022104]; National Natural Science Foundation of China
   [61922046]; Tianjin Natural Science Foundation [18ZXZNGX00110]
FX This work was supported in part by the Major Project for New Generation
   of AI under Grant 2018AAA0100400, in part by the Fundamental Research
   Funds for the Central Universities, Nankai University (63201168) and
   Project (92022104), in part by National Natural Science Foundation of
   China (61922046), and in part by Tianjin Natural Science Foundation
   (18ZXZNGX00110).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2006, 2006 IEEE COMP SOC C, DOI DOI 10.1109/CVPR.2006.207
   [Anonymous], Clker-Free-Vector-Images
   Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Daubechies I, 1998, J FOURIER ANAL APPL, V4, P247, DOI 10.1007/BF02476026
   Du YJ, 2020, IEEE WINT CONF APPL, P2395, DOI 10.1109/WACV45572.2020.9093393
   Du YJ, 2020, IEEE T IMAGE PROCESS, V29, P6288, DOI 10.1109/TIP.2020.2990606
   Fan QN, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275081
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Gavaskar RG, 2019, IEEE T IMAGE PROCESS, V28, P779, DOI 10.1109/TIP.2018.2871597
   Haar A, 1910, MATH ANN, V69, P331, DOI 10.1007/BF01456326
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hou YK, 2020, IEEE T IMAGE PROCESS, V29, P5121, DOI 10.1109/TIP.2020.2980116
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Liang ZT, 2018, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2018.00500
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu D, 2020, IEEE T IMAGE PROCESS, V29, P3695, DOI 10.1109/TIP.2020.2964518
   Liu D, 2018, ADV NEUR IN, V31
   Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34
   Liu W, 2017, IEEE I CONF COMP VIS, pCP32, DOI 10.1109/ICCV.2017.624
   Lu Cewu., 2012, Proc. NPAR, P65
   Lu KY, 2018, LECT NOTES COMPUT SC, V11208, P229, DOI 10.1007/978-3-030-01225-0_14
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Ren DW, 2018, IEEE T IMAGE PROCESS, V27, P511, DOI 10.1109/TIP.2017.2764261
   Shen X., 2017, ARXIV170402071
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Sweldens W, 1996, APPL COMPUT HARMON A, V3, P186, DOI 10.1006/acha.1996.0015
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZQ, 2019, IEEE I CONF COMP VIS, P3977, DOI 10.1109/ICCV.2019.00408
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Xu J, 2018, THESIS HONG KONG POL
   Xu JL, 2018, CAMB CHINA LIBR, P1
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P9316, DOI 10.1109/TIP.2020.3026622
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xu J, 2018, LECT NOTES COMPUT SC, V11212, P21, DOI 10.1007/978-3-030-01237-3_2
   Xu J, 2018, IEEE T IMAGE PROCESS, V27, P2996, DOI 10.1109/TIP.2018.2811546
   Xu J, 2017, IEEE I CONF COMP VIS, P1105, DOI 10.1109/ICCV.2017.125
   Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zhang FH, 2015, IEEE I CONF COMP VIS, P361, DOI 10.1109/ICCV.2015.49
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhou ZQ, 2018, IEEE T MULTIMEDIA, V20, P1392, DOI 10.1109/TMM.2017.2772438
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
NR 59
TC 28
Z9 28
U1 4
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4065
EP 4078
DI 10.1109/TMM.2020.3037535
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900012
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Li, ZP
   Jiang, JM
AF Zhang, Xiaoyan
   Li, Zhuopeng
   Jiang, Jianmin
TI Emotion Attention-Aware Collaborative Deep Reinforcement Learning for
   Image Cropping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color; Feature extraction; Collaboration; Visualization; Microsoft
   Windows; History; Learning (artificial intelligence); Photo composition;
   image cropping; reinforcement learning; emotion attention
AB This paper proposes a collaborative deep reinforcement learning model for automatic image cropping (called CDRL-IC). By modeling image cropping as a decision-making process of reinforcement learning, our model could generate optimal cropping result in a few moving and zooming steps. An image with good composition is a comprehensive result by considering the relative importance of objects and also the spatial organization of visual elements. Therefore, emotion attention information which indicates the relationship and importance between objects is applied together with contextual information of color image for image cropping. In order to sufficiently use the emotion attention map and the color image, they are processed by two collaborative agents. The two agents make their primary learning separately and then share information through an information interaction module for making joint action prediction. In order to efficiently evaluate the cropping quality in the reward function, weighted Intersection Over Union (WIoU) is designed by integrating emotion attention map in the traditional IoU. Our CDRL-IC model is tested on a variety of datasets for both image cropping and thumbnail generation. The experiments show that our CDRL-IC model outperforms state-of-the-art methods on these benchmark datasets.
C1 [Zhang, Xiaoyan; Li, Zhuopeng; Jiang, Jianmin] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518000, Peoples R China.
   [Zhang, Xiaoyan; Li, Zhuopeng; Jiang, Jianmin] Shenzhen Univ, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518000, Peoples R China.
C3 Shenzhen University; Guangming Laboratory; Shenzhen University
RP Zhang, XY (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518000, Peoples R China.; Zhang, XY (corresponding author), Shenzhen Univ, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518000, Peoples R China.
EM xyzhang15@szu.edu.cn; lizhuopeng2017@email.sm.edu.cn;
   jianmin.jiang@szu.edu.cn
FU Chinese Natural Science Foundation [61602313]; National Engineering
   Laboratory for Big Data System Computing Technology
FX This work was supported in part by the Chinese Natural Science
   Foundation under Grant 61602313 and in part by the National Engineering
   Laboratory for Big Data System Computing Technology. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Elisa Ricci.
CR [Anonymous], 2010, ACM MULTIMEDIA
   BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Chen YL, 2017, IEEE WINT CONF APPL, P226, DOI 10.1109/WACV.2017.32
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Esmaeili SA, 2017, PROC CVPR IEEE, P4178, DOI 10.1109/CVPR.2017.445
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Farquhar G., 2018, P INT C MACHINE LEAR, P1
   Foerster JN, 2016, ADV NEUR IN, V29
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974
   Hung W.-C., 2018, P EUR C COMP VIS ECC, P70
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Jie ZQ, 2016, ADV NEUR IN, V29
   Kao YY, 2017, INT CONF ACOUST SPEE, P1982, DOI 10.1109/ICASSP.2017.7952503
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Li ZP, 2019, IEEE INT CON MULTI, P254, DOI 10.1109/ICME.2019.00052
   Lowe R, 2017, ADV NEUR IN, V30
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Sukhbaatar S, 2016, ADV NEUR IN, V29
   Sun J, 2013, INT J COMPUT VISION, V104, P135, DOI 10.1007/s11263-013-0618-z
   Sutskever I, 2014, ADV NEUR IN, V27
   Tsitsiklis JN, 1997, ADV NEUR IN, V9, P1075
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang W., 2017, Proceedings of the IEEE International Conference on Computer Vision, P2186
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Yu K, 2018, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2018.00259
   Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
NR 45
TC 9
Z9 10
U1 3
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2545
EP 2560
DI 10.1109/TMM.2020.3013350
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600001
DA 2024-07-18
ER

PT J
AU Zhou, SW
   He, Y
   Liu, YH
   Li, CQ
   Zhang, JM
AF Zhou, Siwang
   He, Yan
   Liu, Yonghe
   Li, Chengqing
   Zhang, Jianming
TI Multi-Channel Deep Networks for Block-Based Image Compressive Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Sensors; Correlation; Approximation algorithms;
   Smoothing methods; Neural networks; Visualization; Block partition;
   blocking artifact; compressive sensing; deep network; image recovery
ID SPARSE REPRESENTATION; NEURAL-NETWORKS; RECONSTRUCTION
AB Incorporating deep neural networks in image compressive sensing (CS) receives intensive attentions in multimedia technology and applications recently. As deep network approaches learn the inverse mapping directly from the CS measurements, the reconstruction speed is significantly faster than the conventional CS algorithms. However, for existing network-based approaches, a CS sampling procedure has to map a separate network model. This may potentially degrade the performance of image CS with block-wise sampling because of blocking artifacts, especially when multiple sampling rates are assigned to different blocks within an image. In this paper, we develop a multi-channel deep network for block-based image CS by exploiting inter-block correlation with performance significantly exceeding the current state-of-the-art methods. The significant performance improvement is attributed to block-wise approximation but full-image removal of blocking artifacts. Specifically, with our multi-channel structure, the image blocks with a variety of sampling rates can be reconstructed in a single model. The initially reconstructed blocks are then capable of being reassembled into a full image to improve the recovered images by unrolling a hand-designed block-based CS recovery algorithm. Experimental results demonstrate that the proposed method outperforms the state-of-the-art CS methods by a large margin in terms of objective metrics and subjective visual image quality. Our source codes are available at https://github.com/siwangzhou/DeepBCS.
C1 [Zhou, Siwang; He, Yan; Li, Chengqing] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
   [Zhou, Siwang; He, Yan; Li, Chengqing] Xiangtan Univ, Key Lab Intelligent Comp & Informat Proc, Minist Educ, Xiangtan 411105, Peoples R China.
   [Liu, Yonghe] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
   [Zhang, Jianming] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Peoples R China.
C3 Hunan University; Xiangtan University; University of Texas System;
   University of Texas Arlington; Changsha University of Science &
   Technology
RP Zhou, SW (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
EM swzhou@hnu.edu.cn; heyan@hnu.edu.cn; yonghe@cse.uta.edu;
   chengqingli@hnu.edu.cn; jmzhang@csust.edu.cn
RI Lu, Wang/JVO-0416-2024; Zhang, Jianming/AAD-1000-2019; Li,
   Chengqing/B-9388-2008
OI Zhang, Jianming/0000-0002-4278-0805; 
FU National Science Foundation of China [61772447, 61972056, 2019JJ40047]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61772447 and 61972056 and in part by theHunan
   Provincial National Science Foundation underGrant 2019JJ40047.
CR Akbari A, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574688
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bai XL, 2014, IEEE T IND INFORM, V10, P2135, DOI 10.1109/TII.2014.2359416
   Baraniuk RG, 2017, IEEE SIGNAL PROC MAG, V34, P52, DOI 10.1109/MSP.2016.2602099
   Bigot J, 2016, IEEE T INFORM THEORY, V62, P2125, DOI 10.1109/TIT.2016.2524628
   Chen J, 2017, CIRC SYST SIGNAL PR, V36, P1621, DOI 10.1007/s00034-016-0432-2
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Chen Z, 2018, IEEE T MULTIMEDIA, V20, P1610, DOI 10.1109/TMM.2017.2774004
   Cheng Y, 2018, IEEE SIGNAL PROC MAG, V35, P126, DOI 10.1109/MSP.2017.2765695
   Christopher M., 2017, P NIPS, P1770
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   Fowler JE, 2010, FOUND TRENDS SIGNAL, V4, P297, DOI 10.1561/2000000033
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Jiang QR, 2020, IEEE T MULTIMEDIA, V22, P594, DOI 10.1109/TMM.2019.2931400
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Li CQ, 2019, J INF SECUR APPL, V48, DOI 10.1016/j.jisa.2019.102361
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lohit S, 2018, IEEE T COMPUT IMAG, V4, P326, DOI 10.1109/TCI.2018.2846413
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Shi WZ, 2018, IEEE IMAGE PROC, P46, DOI 10.1109/ICIP.2018.8451352
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Xie XM, 2018, IEEE INT CON MULTI
   Yang MR, 2015, IEEE T SIGNAL PROCES, V63, P5479, DOI 10.1109/TSP.2015.2453137
   Yu Y, 2010, IEEE SIGNAL PROC LET, V17, P973, DOI 10.1109/LSP.2010.2080673
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P2057, DOI 10.1109/TCSVT.2016.2580399
   Zheng S, 2019, MULTIMED TOOLS APPL, V78, P25101, DOI 10.1007/s11042-019-07746-3
   Zheng S, 2019, IEEE T MULTIMEDIA, V21, P1905, DOI 10.1109/TMM.2019.2891415
   Zhou SW, 2018, IEEE INT CON MULTI
   Zhu ZH, 2018, SIAM J IMAGING SCI, V11, P1717, DOI 10.1137/17M1148426
NR 41
TC 58
Z9 59
U1 7
U2 55
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2627
EP 2640
DI 10.1109/TMM.2020.3014561
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zeng, ZL
   Wang, ZX
   Wang, Z
   Zheng, YQ
   Chuang, YY
   Satoh, S
AF Zeng, Zelong
   Wang, Zhixiang
   Wang, Zheng
   Zheng, Yinqiang
   Chuang, Yung-Yu
   Satoh, Shin'ichi
TI Illumination-Adaptive Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person re-identification; illumination-adaptive; feature disentanglement
ID RANKING
AB Most person re-identification (ReID) approaches assume that person images are captured under relatively similar illumination conditions. In reality, long-term person retrieval is common, and person images are often captured under different illumination conditions at different times across a day. In this situation, the performances of existing ReID models often degrade dramatically. This paper addresses the ReID problem with illumination variations and names it as Illumination-Adaptive Person Re-identification (IA-ReID). We propose an Illumination-Identity Disentanglement (IID) network to dispel different scales of illuminations away while preserving individuals' identity information. To demonstrate the illumination issue and to evaluate our model, we construct two large-scale simulated datasets with a wide range of illumination variations. Experimental results on the simulated datasets and real-world images demonstrate the effectiveness of the proposed framework.
C1 [Zeng, Zelong] Univ Tokyo, Grad Sch Informat Sci & Technol, Bunkyo Ku, Tokyo 1138654, Japan.
   [Wang, Zhixiang; Chuang, Yung-Yu] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   [Wang, Zheng; Zheng, Yinqiang; Satoh, Shin'ichi] Natl Inst Informat, Digital Content & Media Sci Res Div, Chiyoda Ku, Tokyo 1018430, Japan.
C3 University of Tokyo; National Taiwan University; Research Organization
   of Information & Systems (ROIS); National Institute of Informatics (NII)
   - Japan
EM zzlbz@nii.ac.jp; wangzx1994@gmail.com; wangz@nii.ac.jp;
   yqzheng@nii.ac.jp; cyy@csie.ntu.edu.tw; satoh@nii.ac.jp
RI Wang, Zheng/ABC-6029-2020; Zeng, Zelong/ITU-9384-2023; Wang,
   Zhixiang/AAC-7106-2021
OI Wang, Zheng/0000-0003-3846-9157; Satoh, Shin'ichi/0000-0001-6995-6447;
   Zheng, Yinqiang/0000-0001-7434-5069; Wang, Zhixiang/0000-0002-5016-587X
FU JST CREST [JPMJCR1686]; Honda RD; Microsoft Research Asia;  [18F18378]
FX This work was supported in part by the JST CREST underGrant JPMJCR1686,
   in part by theGrant-in-Aid for JSPS Fellows under Grant 18F18378, in
   part by the Honda R&D, and in part by the Microsoft Research Asia.
   Zelong Zeng and Zhixiang Wang are the co-first authors of this article.
   The associate editor coordinating the review of this manuscript and
   approving it for publicationwas Dr. Fatih Porikli.
CR [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   Bhuiyan A, 2015, IEEE IMAGE PROC, P2329, DOI 10.1109/ICIP.2015.7351218
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Cui JS, 2013, IEEE T SYST MAN CY-S, V43, P996, DOI 10.1109/TSMCA.2012.2223670
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Ge Y., 2018, ADV NEUR IN, P1222
   Gonzalezgarcia A., 2018, ADV NEURAL INFORM PR, P1287
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kansal K, 2020, IEEE T CIRC SYST VID, V30, P3422, DOI 10.1109/TCSVT.2019.2963721
   Kviatkovsky Igor, 2013, IEEE Trans Pattern Anal Mach Intell, V35, P1622, DOI 10.1109/TPAMI.2012.246
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li M., 2016, ARXIV161005586
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu L, 2016, AAAI CONF ARTIF INTE, P1266
   Liu Y, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1617
   Liu Y, 2018, PROC CVPR IEEE, P2080, DOI 10.1109/CVPR.2018.00222
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu BY, 2019, PROC CVPR IEEE, P10217, DOI 10.1109/CVPR.2019.01047
   Luo YW, 2018, LECT NOTES COMPUT SC, V11213, P424, DOI 10.1007/978-3-030-01240-3_26
   Ma F, 2019, MULTIMED TOOLS APPL, V78, P337, DOI 10.1007/s11042-018-6239-3
   Rasmus A, 2015, ADV NEUR IN, V28
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Wang J, 2017, IEEE T CIRC SYST VID, V27, P513, DOI 10.1109/TCSVT.2016.2586851
   Wang YM, 2014, IEEE T CIRC SYST VID, V24, P1350, DOI 10.1109/TCSVT.2014.2305519
   Wang Z., 2016, P IICAI, V2, P6
   Wang Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3891
   Wang Z, 2020, IEEE T IMAGE PROCESS, V29, P2013, DOI 10.1109/TIP.2019.2946975
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2018, IEEE T CYBERNETICS, V48, P3006, DOI 10.1109/TCYB.2017.2755044
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wang Zheng, 2019, ARXIV190510048
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Zhang HC, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3620, DOI 10.1145/3308558.3314139
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
NR 52
TC 62
Z9 65
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3064
EP 3074
DI 10.1109/TMM.2020.2969782
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Min, WQ
   Jiang, SQ
   Jain, R
AF Min, Weiqing
   Jiang, Shuqiang
   Jain, Ramesh
TI Food Recommendation: Framework, Existing Solutions, and Challenges
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Artificial intelligence; knowledge based systems; image recognition;
   data mining; health information management
ID INFORMATION; VALIDATION; SYSTEM; INDEX; TOOL
AB A growing proportion of the global population is becoming overweight or obese, leading to various diseases (e.g., diabetes, ischemic heart disease and even cancer) due to unhealthy eating patterns, such as increased intake of food with high energy and high fat. Food recommendation is of paramount importance to alleviate this problem. Unfortunately, modern multimedia research has enhanced the performance and experience of multimedia recommendation in many fields such asmovies and POI, yet largely lags in the food domain. This article proposes a unified framework for food recommendation, and identifies main issues affecting food recommendation including incorporating various context and domain knowledge, building the personal model, and analyzing unique food characteristics. We then review existing solutions for these issues, and finally elaborate research challenges and future directions in this field. To our knowledge, this is the first survey that targets the study of food recommendation in the multimedia field and offers a collection of research studies and technologies to benefit researchers in this field.
C1 [Min, Weiqing; Jiang, Shuqiang] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Min, Weiqing] Chinese Acad Sci, State Key Lab Robot, Shenyang Inst Automat, Shenyang 110016, Peoples R China.
   [Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Jain, Ramesh] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; University of California System; University of California Irvine
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.; Jiang, SQ (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM minweiqing@ict.ac.cn; sqjiang@ict.ac.cn; jain@ics.uci.edu
FU National Natural Science Foundation of China [61532018, 61972378];
   Beijing Natural Science Foundation [L182054]; National Program for
   Special Support of Eminent Professionals; National Program for Support
   of Top-Notch Young Professionals; StateKey Laboratory of Robotics
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61532018 and 61972378, in part by the
   Beijing Natural Science Foundation under Grant L182054, and in part by
   the National Program for Special Support of Eminent Professionals and
   National Program for Support of Top-Notch Young Professionals, and in
   part by the StateKey Laboratory of Robotics.
CR Aizawa K, 2015, IEEE MULTIMEDIA, V22, P4, DOI 10.1109/MMUL.2015.39
   Aizawa K, 2013, IEEE T MULTIMEDIA, V15, P2176, DOI 10.1109/TMM.2013.2271474
   [Anonymous], 2015, P 9 ACM C REC SYST N
   [Anonymous], 2011, P 5 ACM C REC SYST, DOI 10.1145/2043932.2043979
   [Anonymous], 2018, IEEE SPECTR
   [Anonymous], 2017, arXiv
   Bianchini Devis, 2015, Web Information Systems Engineering - WISE 2015. 16th International Conference. Proceedings: LNCS 9419, P32, DOI 10.1007/978-3-319-26187-4_3
   Boll S, 2018, IEEE MULTIMEDIA, V25, P51, DOI 10.1109/MMUL.2018.011921235
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Burke R, 2002, USER MODEL USER-ADAP, V12, P331, DOI 10.1023/A:1021240730564
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chia-Jen Lin, 2014, Advances in Knowledge Discovery and Data Mining. 18th Pacific-Asia Conference, PAKDD 2014. Proceedings: LNCS 8444, P560, DOI 10.1007/978-3-319-06605-9_46
   Chu WT, 2017, WORLD WIDE WEB, V20, P1313, DOI 10.1007/s11280-017-0437-1
   Cordeiro F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3207, DOI 10.1145/2702123.2702154
   Dehais J, 2017, IEEE T MULTIMEDIA, V19, P1090, DOI 10.1109/TMM.2016.2642792
   Delarue J, 2004, FOOD QUAL PREFER, V15, P771, DOI 10.1016/j.foodqual.2003.11.005
   Elsweiler D, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P575, DOI 10.1145/3077136.3080826
   Elsweiler David, 2015, P 9 ACM C REC SYST, P313, DOI DOI 10.1145/2792838.2799665
   Freyne J, 2010, IUI 2010, P321
   Fu Yanjie., 2014, P 2014 SIAM INT C DA, P470
   Fulgoni VL, 2009, J NUTR, V139, P1549, DOI 10.3945/jn.108.101360
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   Ge M., 2015, P 5 INT C DIGITAL HL, P105, DOI DOI 10.1145/2750511.2750528
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Huayang Wang, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9916, P520, DOI 10.1007/978-3-319-48890-5_51
   I. D. Federation, 2015, IDF DIABETES ATLAS
   Jain R, 2014, IEEE MULTIMEDIA, V21, P100, DOI 10.1109/MMUL.2014.63
   James SL, 2018, LANCET, V392, P1789, DOI [10.1016/s0140-6736(18)32335-3, 10.1016/S0140-6736(18)32335-3]
   Jiang SQ, 2020, IEEE T IMAGE PROCESS, V29, P265, DOI 10.1109/TIP.2019.2929447
   Jin MX, 2012, FPGA 12: PROCEEDINGS OF THE 2012 ACM-SIGDA INTERNATIONAL SYMPOSIUM ON FIELD PROGRAMMABLE GATE ARRAYS, P21
   Julia C, 2016, EUR J NUTR, V55, P1901, DOI 10.1007/s00394-015-1006-y
   Kawano Y, 2013, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW.2013.5
   Kim SJ, 2018, SCI REP-UK, V8, DOI [10.1038/s41598-018-21621-5, 10.1038/s41598-018-26895-3]
   Lin C.-J., 2014, ADV KNOWLEDGE DISCOV, P560
   Lou Z, 2017, SMALL, V13, DOI 10.1002/smll.201701791
   Meyer J, 2014, IEEE PERVAS COMPUT, V13, P10, DOI 10.1109/MPRV.2014.25
   Min WQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1331, DOI 10.1145/3343031.3350948
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Min WQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P402, DOI 10.1145/3123266.3123272
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Min WQ, 2017, MULTIMEDIA SYST, V23, P647, DOI 10.1007/s00530-016-0523-8
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Ming ZY, 2018, LECT NOTES COMPUT SC, V10705, P129, DOI 10.1007/978-3-319-73600-6_12
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Nag N, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P99, DOI 10.1145/3078971.3080545
   Nag Nitish, 2017, MMHealth17 (2017), V2017, P61, DOI 10.1145/3132635.3132643
   Nag N, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1993, DOI 10.1145/3240508.3241913
   Nag N, 2017, LECT NOTES COMPUT SC, V10590, P444, DOI 10.1007/978-3-319-70742-6_43
   Ng YK, 2017, 9TH INTERNATIONAL CONFERENCE ON MANAGEMENT OF EMERGENT DIGITAL ECOSYSTEMS (MEDES 2017), P243, DOI 10.1145/3167020.3167057
   Oh H, 2018, PROCEEDINGS OF THE 3RD INTERNATIONAL WORKSHOP ON MULTIMEDIA FOR PERSONAL HEALTH AND HEALTH CARE (HEALTHMEDIA'18), P39, DOI 10.1145/3264996.3265000
   Oliveira MJ, 2018, V CONGRESO IBEROAMERICANO DE HORMIGON AUTOCOMPACTANTE Y HORMIGONES ESPECIALES, P107, DOI 10.4995/HAC2018.2018.5191
   Rokicki Markus, 2018, 12 INT AAAI C WEB SO
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Sanjo S, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2279, DOI 10.1145/3132847.3133137
   Schäfer H, 2017, ADJUNCT PUBLICATION OF THE 25TH CONFERENCE ON USER MODELING, ADAPTATION AND PERSONALIZATION (UMAP'17), P93, DOI 10.1145/3099023.3099108
   Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973
   Teng CY, 2012, PROCEEDINGS OF THE 3RD ANNUAL ACM WEB SCIENCE CONFERENCE, 2012, P298
   Tirosh A, 2019, SCI TRANSL MED, V11, DOI 10.1126/scitranslmed.aav0120
   Verhagen JV, 2006, NEUROSCI BIOBEHAV R, V30, P613, DOI 10.1016/j.neubiorev.2005.11.003
   Wang HW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1835, DOI 10.1145/3178876.3186175
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yang LQ, 2017, ACM T INFORM SYST, V36, DOI 10.1145/3072614
   Yang Longqi., 2015, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, P183, DOI [DOI 10.1145/2806416, 10.1145/2806416]
   Zeng J, 2018, 3RD WORKSHOP ON MULTISENSORY APPROACHES TO HUMAN-FOOD INTERACTION (MHFI), DOI 10.1145/3279954.3279959
   Zhang FZ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P353, DOI 10.1145/2939672.2939673
   Zulaika U., 2018, P MULT DIG PUBL I, V2, P1218
NR 67
TC 51
Z9 54
U1 9
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2659
EP 2671
DI 10.1109/TMM.2019.2958761
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Song, JR
   Yang, FZ
   Zhang, W
   Zou, WJ
   Fan, YQ
   Di, PY
AF Song, Jiarun
   Yang, Fuzheng
   Zhang, Wei
   Zou, Wenjie
   Fan, Yuqun
   Di, Peiyun
TI A Fast FoV-Switching DASH System Based on Tiling Mechanism for Practical
   Omnidirectional Video Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Bandwidth; Decoding; Bit rate; Quality of experience;
   Encoding; Estimation; Omnidirectional video; 360-degree video; DASH;
   tile-based; fast-switching
ID COST
AB With the development of multimedia technologies and virtual reality display devices, omnidirectional videos have gained popularity nowadays. To reduce the bandwidth requirement for omnidirectional video transmission, tile-based viewport adaptive streaming methods have been proposed in the literatures. Challenges related to decoding the tiles simultaneously with limited number of decoders, and ensuring user's viewing experience during the viewport switch are still to be solved. In this paper, a two-layer fast viewport switching dynamic adaptive streaming over HTTP (DASH) system based on tiling mechanism is proposed, which incorporates the viewing trajectory of end users. To deal with the simultaneously decoding problem, an open group of picture (GOP) technique is proposed to enable merging different types of tiles into a composite stream at the client side. To reduce the quality recovery duration after the viewport change, a fast-switching strategy is also proposed. Moreover, considering the priorities of different types of chunks, a download strategy is further proposed to adapt the bandwidth fluctuations and viewport changes. Experimental results showed that the proposed system can significantly reduce the recovery duration of high quality video by approximately 90%, which can provide a better viewing experience to end users.
C1 [Song, Jiarun; Yang, Fuzheng; Zhang, Wei; Zou, Wenjie] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Fan, Yuqun; Di, Peiyun] Huawei Technol Co Ltd, Media Technol Lab, Shenzhen 518129, Peoples R China.
C3 Xidian University; Huawei Technologies
RP Zhang, W (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM jrsong@xidian.edu.cn; fzhyang@mail.xidian.edu.cn; wzhang@xidian.edu.cn;
   wjzou@xidian.edu.cn; fanyuqun@huawei.com; dipeiyun@huawei.com
OI Song, Jiarun/0000-0001-6718-4201
FU National Natural Science Foundation of China [61601349, 61571337,
   61801364]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61601349, 61571337, and 61801364. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Shiwen Mao.
CR [Anonymous], 2015, EURASIP J ADV S 1117, DOI DOI 10.1186/S13634-015-0279-X
   [Anonymous], 2015, POPULATION PYRAMIDS
   [Anonymous], 2017, I S INTELL SIG PROC
   [Anonymous], 2017, 2017 4 NAF C INF
   [Anonymous], 2017, IEEE IMAGE PROC
   Belshe M., 2017, HYPERTEXT TRANSFER P
   Bross B., 2012, P 11 JCT VC M OCT, P105
   Cisco, 2017, CISC VIS NETW IND GL
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Duanmu F, 2017, IEEE INT SYMP CIRC S
   Espelien J., 2016, FUTURE CONSUMER VR I
   Fan Y., 2018, ARXIV180808469V1
   Feldmann C., 2013, P INT C ADV MULT, P130
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Go Y, 2015, IEEE T MULTIMEDIA, V17, P1646, DOI 10.1109/TMM.2015.2451951
   Graf M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P261, DOI 10.1145/3083187.3084016
   He J, 2018, MOBISYS'18: PROCEEDINGS OF THE 16TH ACM INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P482, DOI 10.1145/3210240.3210323
   Hosseini M, 2017, P IEEE VIRT REAL ANN, P423, DOI 10.1109/VR.2017.7892357
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Hou X., 2018, P ACM MORN WORKSH VI, P20
   Inoue M., 2010, P 18 ACM INT C MULT, P1191
   Kammachi-Sreedhar K, 2016, IEEE INT SYM MULTIM, P583, DOI [10.1109/ISM.2016.0126, 10.1109/ISM.2016.143]
   Kenesi Z, 2005, IEEE SYMP COMP COMMU, P631, DOI 10.1109/ISCC.2005.111
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Luthra A., 2014, JTC1SC29WG11 ISOIEC, P14510
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Niamut OA, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P46, DOI 10.1145/2910017.2910606
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Qian F., 2016, P 5 ACM WORKSH ALL T, P1
   Rainer B, 2017, IEEE T MULTIMEDIA, V19, P849, DOI 10.1109/TMM.2016.2629761
   Reese W., 2008, LINUX J, V2
   Rizzo L., 1997, Computer Communication Review, V27, P31, DOI 10.1145/251007.251012
   Skupin R, 2016, IEEE INT SYM MULTIM, P399, DOI [10.1109/ISM.2016.0089, 10.1109/ISM.2016.137]
   Stenberg D, 2014, ACM SIGCOMM COMP COM, V44, P120, DOI 10.1145/2656877.2656896
   Stockhammer T., 2011, P ACM MMSYS, P133
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun LY, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P162, DOI 10.1145/3204949.3204978
   Tian GB, 2016, IEEE ACM T NETWORK, V24, P2386, DOI 10.1109/TNET.2015.2464700
   Thang TC, 2014, IEEE J SEL AREA COMM, V32, P693, DOI 10.1109/JSAC.2014.140403
   Thang TC, 2012, IEEE T CONSUM ELECTR, V58, P78, DOI 10.1109/TCE.2012.6170058
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   You D, 2018, IEEE ACCESS, V6, P40439, DOI 10.1109/ACCESS.2018.2829194
   You D, 2018, INT CONF UBIQ FUTUR, P676, DOI 10.1109/ICUFN.2018.8437012
   Zare A., 2016, P 24 ACM INT C MULT, P601
   Zhang X., 2018, IEEE C COMP VIS PATT
   Zhou C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209660
   Zhou C, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P27, DOI 10.1145/3083187.3083190
NR 49
TC 20
Z9 22
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2366
EP 2381
DI 10.1109/TMM.2019.2957976
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200012
DA 2024-07-18
ER

PT J
AU Wang, ZZ
   Hong, WX
   Tan, YP
   Yuan, JS
AF Wang, Zhenzhen
   Hong, Weixiang
   Tan, Yap-Peng
   Yuan, Junsong
TI Pruning 3D Filters For Accelerating 3D ConvNets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Two dimensional displays; Acceleration;
   Feature extraction; Task analysis; Computational efficiency; Redundancy;
   3D ConvNets; Filter Pruning; DPPs; Maximum Abs; of Filters (MAF)
ID DEEP; NETWORKS
AB Many methods have been proposed to accelerate 2D ConvNets by removing redundant parameters. However, few efforts are devoted to the problem of accelerating 3D Convolutional Networks. The 3D ConvNets, which are mainly designed for extracting spatiotemporal features, have been widely used in many video analytics tasks, such as action recognition and scene analysis. In this paper, we focus on accelerating 3D ConvNets for two motivations: (1) Fast video processing techniques are in dire need due to the explosive growth of video data; (2) Compared with individual images, video data consist of consecutively similar frames, thus are inherently more redundant. In this paper, we present a novel algorithm to dramatically accelerate 3D ConvNets by pruning redundant convolutional filters, while preserving the discriminative power of the networks. Specifically, we formulate the filter pruning from 3D ConvNets as a subset selection problem where each filter is regarded as a candidate. Determinantal Point Processes (DPPs) are employed to discriminatively select the filter candidates which are informative and yet diverse. We evaluate our method using two popular 3D networks, C3D and Pseudo-3D, on Sports-1 M dataset for video classification. Extensive experimental results demonstrate both the efficiency and performance advantages of our method. We also show that the proposed method can be easily generalized to 2D ConvNets pruning with promising experimental results on VGGnet and ResNet.
C1 [Wang, Zhenzhen; Tan, Yap-Peng] Nanyang Technol Univ NTU, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Hong, Weixiang] Ant Financial Serv Grp, Hangzhou 310000, Peoples R China.
   [Yuan, Junsong] Univ Buffalo State Univ New York, Dept Comp Sci & Engn, Clayton, NY 13624 USA.
C3 Nanyang Technological University
RP Wang, ZZ (corresponding author), Nanyang Technol Univ NTU, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM zwang033@e.ntu.edu.sg; hwx229374@antfin.com; eyptan@ntu.edu.sg;
   jsyuan@buffalo.edu
RI Hong, Weixiang/Q-7636-2019; Yuan, Junsong/A-5171-2011; Tan,
   Yap-Peng/A-5158-2011
OI Yuan, Junsong/0000-0002-7901-8793
FU University at Buffalo
FX This work is supported in part by the start-up funds at University at
   Buffalo.
CR [Anonymous], 2018, IJCAI
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], IEEE T NEURAL NETW L
   [Anonymous], 2016, CoRR
   [Anonymous], 1990, Adv Neural Inform Process Syst.
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Buko B., 2022, ABS151203385 CORR, V22, P8878
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Han S, 2015, P ADV NEUR INF PROC, V2015, P1135
   Hassibi B., 1993, P ADV NEUR INF PROC, P164
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Hong WX, 2018, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2018.00145
   Hong WX, 2017, PROC CVPR IEEE, P6221, DOI 10.1109/CVPR.2017.659
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Howard A. G., 2017, PREPRINT
   Hu Yiming, 2018, CORR
   Huang QG, 2018, IEEE WINT CONF APPL, P709, DOI 10.1109/WACV.2018.00083
   Jiang YH, 2019, PROCEEDINGS OF 2019 THE 3RD INTERNATIONAL CONFERENCE ON CRYPTOGRAPHY, SECURITY AND PRIVACY (ICCSP 2019) WITH WORKSHOP 2019 THE 4TH INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP 2019), P28, DOI 10.1145/3309074.3309094
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lebedev V, 2016, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR.2016.280
   Li CT, 2016, PR MACH LEARN RES, V48
   Li CT, 2016, JMLR WORKSH CONF PRO, V51, P1328
   Li HF, 2017, ADV SOC SCI EDUC HUM, V179, P1
   Li YC, 2019, PROC CVPR IEEE, P2795, DOI 10.1109/CVPR.2019.00291
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Luo JH, 2019, IEEE T PATTERN ANAL, V41, P2525, DOI 10.1109/TPAMI.2018.2858232
   Luo JH, 2017, IEEE I CONF COMP VIS, P5068, DOI 10.1109/ICCV.2017.541
   Mariet Z., 2016, INT C LEARN REPR ICL
   Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152
   Polyak A, 2015, IEEE ACCESS, V3, P2163, DOI 10.1109/ACCESS.2015.2494536
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HF, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004792
   Suau Xavier, 2018, ARXIV PREPRINT ARXIV
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Tolias G., 2016, Conference Track Proceedings,
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wen W., 2016, P 30 INT C NEUR INF, P2082
   Xu YH, 2018, AAAI CONF ARTIF INTE, P4335
   Ye JJ, 2019, PROG HUM GEOG, V43, P478, DOI 10.1177/0309132518768405
   Yu T, 2017, IEEE I CONF COMP VIS, P726, DOI 10.1109/ICCV.2017.85
   Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579
   Zhao CL, 2019, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2019.00289
   Zhou H, 2016, LECT NOTES COMPUT SC, V9908, P662, DOI 10.1007/978-3-319-46493-0_40
NR 52
TC 10
Z9 10
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2126
EP 2137
DI 10.1109/TMM.2019.2950523
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500017
DA 2024-07-18
ER

PT J
AU Zhu, WW
   Wang, X
   Gao, W
AF Zhu, Wenwu
   Wang, Xin
   Gao, Wen
TI Multimedia Intelligence: When Multimedia Meets Artificial Intelligence
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Machine learning; Hidden Markov models; Cognition;
   Videos; Data models; Multimedia artificial intelligence; reasoning in
   multimedia
ID RECOGNITION; VIDEO
AB Owing to the rich emerging multimedia applications and services in the past decade, super large amount of multimedia data has been produced for the purpose of advanced research in multimedia. Furthermore, multimedia research has made great progress on image/video content analysis, multimedia search and recommendation, multimedia streaming, multimedia content delivery etc. At the same time, Artificial Intelligence (AI) has undergone a "new" wave of development since being officially regarded as an academic discipline in 1950s, which should give credits to the extreme success of deep learning. Thus, one question naturally arises: What happens when multimedia meets Artificial Intelligence? To answer this question, we introduce the concept of Multimedia Intelligence through investigating the mutual-influence between multimedia and Artificial Intelligence. We explore the mutual influences between multimedia and Artificial Intelligence from two aspects: i) multimedia drives Artificial Intelligence to experience a paradigm shift towards more explainability and ii) Artificial Intelligence in turn injects new ways of thinking for multimedia research. As such, these two aspects form a loop in which multimedia and Artificial Intelligence interactively enhance each other. In this paper, we discuss what and how efforts have been done in literature and share our insights on research directions that deserve further study to produce potentially profound impact on multimedia intelligence.
C1 [Zhu, Wenwu; Wang, Xin] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Tsinghua University; Peking University
RP Wang, X (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.; Gao, W (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM wwzhu@tsinghua.edu.cn; xin_wang@tsinghua.edu.cn; wgao@pku.edu.cn
RI li, lan/KCJ-5061-2024; Yuan, Fang/JQV-7426-2023
OI Wang, Xin/0000-0002-0351-2939
FU National Natural Science Foundation of China [U1611461]
FX This work is supported by the National Natural Science Foundation of
   China Major Project No. U1611461. This is an invited paper in the
   special issue of Multimedia Computing with Interpretable Machine
   Learning.
CR Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Agrawal R., 1993, Foundations of Data Organization and Algorithms. 4th International Conference. FODO '93 Proceedings, P69
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Andreas J., 2016, P 2016 C N AM CHAPT, P1545, DOI DOI 10.18653/V1/N16-1181
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   [Anonymous], 2013, EMNLP
   [Anonymous], 2018, Advances in neural information processing systems
   [Anonymous], 2012, INT C MACH LEARN WOR
   [Anonymous], 2010, P 23ND ANN ACM S USE
   [Anonymous], 2015, P 2015 C N AM CHAPT
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Baroni M, 2016, LANG LINGUIST COMPAS, V10, P3, DOI 10.1111/lnc3.12170
   Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Cao QX, 2018, PROC CVPR IEEE, P7249, DOI 10.1109/CVPR.2018.00757
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Chen D, 2019, I IEEE EMBS C NEUR E, P433, DOI [10.1109/NER.2019.8717063, 10.1109/ner.2019.8717063]
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   Cord M, 2008, COGN TECHNOL, P1, DOI 10.1007/978-3-540-75171-7
   Das R, 2019, SPR SER POLYM COMPOS, P1, DOI 10.1007/978-3-030-00743-0_1
   dAvila Artur S, 2012, Neural-symbolic Learning Systems: Foundations and Applications
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Duan X., 2018, ADV NEURAL INFORM PR, P3059
   Duan XG, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1543, DOI 10.1145/3343031.3351094
   Duan YQ, 2019, PROC CVPR IEEE, P949, DOI 10.1109/CVPR.2019.00104
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Escalera S, 2015, LECT NOTES COMPUT SC, V8925, P459, DOI 10.1007/978-3-319-16178-5_32
   Fan HQ, 2018, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2018.00118
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Garg A, 2003, P IEEE, V91, P1355, DOI 10.1109/JPROC.2003.817119
   Ghahramani Z, 1996, ADV NEUR IN, V8, P472
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Gurban M., 2008, P 10 INT C MULT INT, P237
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Huang R, 2017, ALIMENT PHARM THER, V46, P769, DOI 10.1111/apt.14266
   Hudson D. A., 2019, 2019 IEEE CVF C COMP, P6700, DOI DOI 10.48550/ARXIV.1902.09506
   Hudson D.A., 2018, INT C LEARN REPR ICL
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Johnson O, 2017, IEEE INT SYMP INFO, P898, DOI 10.1109/ISIT.2017.8006658
   Kahou SE, 2016, J MULTIMODAL USER IN, V10, P99, DOI 10.1007/s12193-015-0195-2
   Khapra M. M., 2010, HUM LANG TECHN 2010, P420
   Kuznetsova P., 2012, Long Papers, P359
   Lafferty John, 2001, INT C MACH LEARN ICM
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   Li GH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P530, DOI 10.1145/3343031.3350922
   Lian DZ, 2019, IEEE T NEUR NET LEAR, V30, P3010, DOI 10.1109/TNNLS.2018.2865525
   Liu D., 2019, ARXIV190603561
   Liu XC, 2019, PROC CVPR IEEE, P3561, DOI 10.1109/CVPR.2019.00368
   Liu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1239, DOI 10.1145/3343031.3350986
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Manhaeve R, 2018, ADV NEURAL INFORM PR, P3753
   Mansimov Elman, 2015, ARXIV151102793
   Mascharka D, 2018, PROC CVPR IEEE, P4942, DOI 10.1109/CVPR.2018.00519
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Nakov P, 2012, J ARTIF INTELL RES, V44, P179, DOI 10.1613/jair.3540
   Narasimhan M, 2018, ADV NEUR IN, V31
   Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814
   Nefian A. V., 2002, P IEEE INT C AC SPEE, V2
   Neverova N, 2016, IEEE T PATTERN ANAL, V38, P1692, DOI 10.1109/TPAMI.2015.2461544
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999
   Oord A., 2016, ARXIV160903499
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264
   Pan YW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1789, DOI 10.1145/3123266.3127905
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Rajendran J., 2016, 2016 C N AM CHAPT AS, P171, DOI [DOI 10.18653/V1/N16-1021, 10.18653/v1/N16-1021]
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Reed S, 2016, PR MACH LEARN RES, V48
   Santoro A., 2017, Advances in Neural Information Processing Systems
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schultz P. T., 2016, U.S. Patent, Patent No. [9,323,912, 9323912]
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Sitová Z, 2016, IEEE T INF FOREN SEC, V11, P877, DOI 10.1109/TIFS.2015.2506542
   Socher R., 2013, P 2013 C EMP METH NA, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   SONG YL, 2015, PROC CVPR IEEE, P5179, DOI [DOI 10.1109/CVPR.2015.7299154, 10.1109/CVPR.2015.7299154]
   Sutskever I., 2014, ADV NEURAL INFORM PR, V4, P3104, DOI DOI 10.5555/2969033.2969173
   Tsai YHH, 2019, PROC CVPR IEEE, P10416, DOI 10.1109/CVPR.2019.01067
   Van den Oord A., 2013, ADV NEURAL INFORM PR, P2643, DOI [DOI 10.1109/MMUL.2011.34.VAN, 10.5555/2999792.2999907]
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang K., 2016, A comprehensive survey on cross-modal retrieval
   Wang X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1730, DOI 10.1145/3343031.3350934
   Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1518, DOI 10.1145/3292500.3330939
   Wang X, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1601, DOI 10.1145/3038912.3052556
   Wang X, 2016, AAAI CONF ARTIF INTE, P1331
   Wang X, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P5, DOI 10.1145/2983323.2983701
   Wöllmer M, 2013, IMAGE VISION COMPUT, V31, P153, DOI 10.1016/j.imavis.2012.03.001
   Wu CF, 2018, ADV NEUR IN, V31
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Xia XY, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1241, DOI [10.1109/VR.2019.8797791, 10.1109/vr.2019.8797791]
   Xiong PX, 2019, PROC CVPR IEEE, P8349, DOI 10.1109/CVPR.2019.00855
   Xu H, 2019, INT CONF ACOUST SPEE, P1264, DOI 10.1109/ICASSP.2019.8682825
   Xu YL, 2018, PROC CVPR IEEE, P2178, DOI 10.1109/CVPR.2018.00232
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yao Ting, 2016, PROC CVPR IEEE, P982, DOI DOI 10.1109/CVPR.2016.112
   Yi KX, 2018, ADV NEUR IN, V31
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yu SZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4635
   Yu WJ, 2019, PROC CVPR IEEE, P2932, DOI 10.1109/CVPR.2019.00305
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Yuan YT, 2019, IEEE T CIRC SYST VID, V29, P226, DOI 10.1109/TCSVT.2017.2771247
   YUHAS BP, 1989, IEEE COMMUN MAG, V27, P65, DOI 10.1109/35.41402
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zhang L, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490823
   Zhang W, 2015, PATTERN RECOGN, V48, P3191, DOI 10.1016/j.patcog.2015.04.012
   Zhao JJ, 2019, PROCEEDINGS OF THE 2ND ACM SIGSOFT INTERNATIONAL WORKSHOP ON SOFTWARE QUALITIES AND THEIR DEPENDENCIES (SQUADE' 19), P1, DOI 10.1145/3340495.3342749
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhu WW, 2020, IEEE T CIRC SYST VID, V30, P3740, DOI 10.1109/TCSVT.2019.2940647
NR 133
TC 40
Z9 41
U1 13
U2 120
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1823
EP 1835
DI 10.1109/TMM.2020.2969791
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500014
DA 2024-07-18
ER

PT J
AU Kristoffersen, MS
   Shepstone, SE
   Tan, ZH
AF Kristoffersen, Miklas Strom
   Shepstone, Sven Ewan
   Tan, Zheng-Hua
TI The Importance of Context When Recommending TV Content: Dataset and
   Algorithms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE TV; Meters; Recommender systems; Automobiles; Context modeling;
   Complexity theory; Multimedia systems; Context awareness; recommender
   systems; data collection; TV
ID TELEVISION
AB Home entertainment systems feature in a variety of usage scenarios with one or more simultaneous users, for whom the complexity of choosing media to consume has increased rapidly over the last decade. Users' decision processes are complex and highly influenced by contextual settings, but data supporting the development and evaluation of context-aware recommender systems are scarce. In this paper we present a dataset of self-reported TV consumption enriched with contextual information of viewing situations. We show how choice of genre associates with, among others, the number of present users and users' attention levels. Furthermore, we evaluate the performance of predicting chosen genres given different configurations of contextual information, and compare the results to contextless predictions. The results suggest that including contextual features in the prediction cause notable improvements, and both temporal and social context show significant contributions.
C1 [Kristoffersen, Miklas Strom; Shepstone, Sven Ewan] Bang & Olufsen AS, Res Dept, DK-7600 Struer, Denmark.
   [Kristoffersen, Miklas Strom; Tan, Zheng-Hua] Aalborg Univ, Dept Elect Syst, Signal & Informat Proc Sect, DK-9220 Aalborg, Denmark.
C3 Aalborg University
RP Kristoffersen, MS (corresponding author), Bang & Olufsen AS, Res Dept, DK-7600 Struer, Denmark.
EM mko@bang-olufsen.dk; ssh@bang-olufsen.dk; zt@es.aau.dk
RI Tan, Zheng-Hua/B-6889-2015
OI Tan, Zheng-Hua/0000-0001-6856-8928; Kristoffersen, Miklas
   Strom/0000-0002-1409-2618
FU Bang & Olufsen A/S, Denmark; Innovation Fund Denmark (IFD) [5189-00009B]
FX This work was supported in part by Bang & Olufsen A/S, Denmark, and in
   part by the Innovation Fund Denmark (IFD) under File 5189-00009B.
CR Abreu Jorge., 2013, PROCS 11 EUROPEAN C, P5, DOI DOI 10.1145/2465958.2465970
   Adomavicius G, 2011, RECOMMENDER SYSTEMS HANDBOOK, P217, DOI 10.1007/978-0-387-85820-3_7
   Aharon M, 2015, LECT NOTES ARTIF INT, V9286, P180, DOI 10.1007/978-3-319-23461-8_12
   [Anonymous], P 1 WORKSH REC SYST
   [Anonymous], 2016, P 1 WORKSH DEEP LEAR, DOI [DOI 10.1145/2988450.2988454, 10.1145/2988450.2988454]
   Ardissono L, 2004, HUM-COMPUT INT-SPRIN, P3
   Bambia M, 2016, 2016 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE (WI 2016), P272, DOI [10.1109/WI.2016.45, 10.1109/WI.2016.0046]
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Bertini M., 2013, P 21 ACM INT C MULTI, P451
   Coifman A, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P249, DOI 10.5220/0007311402490256
   Cremonesi P, 2015, LECT NOTES BUS INF P, V239, P57, DOI 10.1007/978-3-319-27729-5_5
   CSIKSZENTMIHALYI M, 1987, J NERV MENT DIS, V175, P526, DOI 10.1097/00005053-198709000-00004
   Cui P, 2016, IEEE MULTIMEDIA, V23, P92, DOI 10.1109/MMUL.2016.8
   Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Nguyen DT, 2016, PATTERN RECOGN, V51, P148, DOI 10.1016/j.patcog.2015.08.027
   Feng YA, 2019, IEEE T MULTIMEDIA, V21, P1762, DOI 10.1109/TMM.2018.2885237
   Frolov E, 2017, WIRES DATA MIN KNOWL, V7, DOI 10.1002/widm.1201
   Gomez-Uribe CA, 2016, ACM TRANS MANAG INF, V6, DOI 10.1145/2843948
   Hsu SH, 2007, LECT NOTES COMPUT SC, V4471, P166
   Jardine B, 2016, EUR J MARKETING, V50, P1290, DOI 10.1108/EJM-03-2015-0137
   Kim M, 2018, TVX 2018: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE EXPERIENCES FOR TV AND ONLINE VIDEO, P89, DOI 10.1145/3210825.3210829
   Kristoffersen MS, 2018, PROCEEDINGS OF THE 26TH CONFERENCE ON USER MODELING, ADAPTATION AND PERSONALIZATION (UMAP'18), P367, DOI 10.1145/3209219.3209263
   Larson R., 2014, New directions for methodology of social & behavioral science, P21, DOI [DOI 10.1007/978-94-017-9088-8_2, 10.1007/978-94-017-9088-82, DOI 10.1007/978-94-017-9088-82]
   Lorenz Felix., 2017, P 2017 ACM INT C INT, P21
   Masthoff J., 2015, recommender systems handbook, P743, DOI 10.1007/978-1-4899-7637-6_22
   Mercer K, 2014, PERS UBIQUIT COMPUT, V18, P723, DOI 10.1007/s00779-013-0702-y
   Pagano R, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P249, DOI 10.1145/2959100.2959136
   Park Y, 2017, INFORM SCIENCES, V409, P1, DOI 10.1016/j.ins.2017.04.038
   Pedregosa F, 2011, J. Mach. Learn. Res., V12, P2825
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P635
   Saxbe D, 2011, COMMUN RES REP, V28, P180, DOI 10.1080/08824096.2011.566104
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Shi Y, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2556270
   Song SB, 2012, IEEE T MULTIMEDIA, V14, P1528, DOI 10.1109/TMM.2012.2217118
   Unger M, 2017, PROCEEDINGS OF THE 25TH CONFERENCE ON USER MODELING, ADAPTATION AND PERSONALIZATION (UMAP'17), P221, DOI 10.1145/3079628.3079666
   Vanattenhoven J, 2015, PERS UBIQUIT COMPUT, V19, P761, DOI 10.1007/s00779-015-0861-0
   Véras D, 2015, EXPERT SYST APPL, V42, P9046, DOI 10.1016/j.eswa.2015.06.052
   Vildjiounaite E, 2009, MULTIMEDIA SYST, V15, P143, DOI 10.1007/s00530-009-0157-1
   Xiao J, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2933232
NR 41
TC 5
Z9 5
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1531
EP 1541
DI 10.1109/TMM.2019.2944214
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, HY
   Liu, AA
   Nie, WZ
   Nie, J
AF Zhou, He-Yu
   Liu, An-An
   Nie, Wei-Zhi
   Nie, Jie
TI Multi-View Saliency Guided Deep Neural Network for 3-D Object Retrieval
   and Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Visualization; Cameras;
   Feature extraction; Computational modeling; Shape; 3D object retrieval;
   3D object classification; multi-view learning; saliency analysis
ID 3D MODEL RETRIEVAL; DESCRIPTORS; RECOGNITION
AB In this paper, we propose the multi-view saliency guided deep neural network (MVSG-DNN) for 3D object retrieval and classification. This method mainly consists of three key modules. First, the module of model projection rendering is employed to capture the multiple views of one 3D object. Second, the module of visual context learning applies the basic Convolutional Neural Networks for visual feature extraction of individual views and then employs the saliency LSTM to adaptively select the representative views based on multi-view context. Finally, with these information, the module of multi-view representation learning can generate the compile 3D object descriptors with the designed classification LSTM for 3D object retrieval and classification. The proposed MVSG-DNN has two main contributions: 1) It can jointly realize the selection of representative views and the similarity measure by fully exploiting multi-view context; 2) It can discover the discriminative structure of multi-view sequence without constraints of specific camera settings. Consequently, it can support flexible 3D object retrieval and classification for real applications by avoiding the required camera settings. Extensive comparison experiments on ModelNet10, ModelNet40, and ShapeNetCore55 demonstrate the superiority of MVSG-DNN against the state-of-art methods.
C1 [Zhou, He-Yu; Liu, An-An; Nie, Wei-Zhi] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Jie] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
C3 Tianjin University; Ocean University of China
RP Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Nie, J (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
EM zhy_std@163.com; anan0422@gmail.com; weizhinie@tju.edu.cn;
   niejie@ouc.edu.cn
RI Nie, Weizhi/ABF-5316-2021; Nie, Jie/ABG-9228-2021
OI Nie, Jie/0000-0003-4952-7666; , Heyu Zhou/0000-0001-9451-5600; nie,
   weizhi/0000-0002-0578-8138
FU National Nature Science Foundation ofChina [61772359, 61872267,
   61702471]; 2019 Tianjin New Generation Artificial Intelligence Major
   Progam; 2018 Tianjin New Generation Artificial Intelligence Major
   Program [18ZXZNGX00150]; Open Project Program of the State Key Lab of
   CAD & CG, Zhejiang University [A1907]; grant of Elite Scholar Program of
   Tianjin University [2019XRX-0035]
FX This work was supported in part by the National Nature Science
   Foundation ofChina underGrants 61772359, 61872267, and 61702471, in part
   by the grant of 2019 Tianjin New Generation Artificial Intelligence
   Major Progam, in part by the grant of 2018 Tianjin New Generation
   Artificial Intelligence Major Program 18ZXZNGX00150, in part by the Open
   Project Program of the State Key Lab of CAD & CG, Zhejiang University
   under Grant A1907, and in part by the grant of Elite Scholar Program of
   Tianjin University 2019XRX-0035. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr Zhu
   Liu.
CR Abdul-Rashid H., 2018, P EUR WORKSH 3D OBJ, P37
   [Anonymous], 2017, P EUR WORKSH 3D OBJ
   [Anonymous], 2017, P EUR WORKSH 3D OBJ
   [Anonymous], ARXIV171110108
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Dominguez M, 2018, IEEE WINT CONF APPL, P1972, DOI 10.1109/WACV.2018.00218
   Funkhouser T.A., 2015, P COMP VIS PATT REC
   Gao Y, 2014, IEEE T IND ELECTRON, V61, P2088, DOI 10.1109/TIE.2013.2262760
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gao Y, 2011, SIGNAL PROCESS-IMAGE, V26, P39, DOI 10.1016/j.image.2010.10.006
   Garcia-Garcia A, 2016, IEEE IJCNN, P1578, DOI 10.1109/IJCNN.2016.7727386
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kim WY, 2000, SIGNAL PROCESS-IMAGE, V16, P95, DOI 10.1016/S0923-5965(00)00019-9
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Liu AA, 2016, IEEE T IMAGE PROCESS, V25, P2103, DOI 10.1109/TIP.2016.2540802
   Liu A, 2015, INFORM SCIENCES, V320, P429, DOI 10.1016/j.ins.2015.04.042
   Liu SK, 2018, INT CONF 3D VISION, P542, DOI 10.1109/3DV.2018.00068
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma C, 2017, PROCEEDINGS OF THE ASME TURBO EXPO: TURBINE TECHNICAL CONFERENCE AND EXPOSITION, 2017, VOL 2C
   Matsuda T, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P100, DOI 10.1109/BigMM.2015.66
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Pham Q., 2018, P EUR WORKSH 3D OBJ, P45
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Schneider RG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661231
   Shi CJ, 2017, IEEE T CIRC SYST VID, V27, P1947, DOI 10.1109/TCSVT.2016.2576919
   Shih JL, 2007, PATTERN RECOGN, V40, P283, DOI 10.1016/j.patcog.2006.04.034
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Wang D, 2019, IEEE T MULTIMEDIA, V21, P2071, DOI 10.1109/TMM.2019.2892004
   Wang XY, 2015, NEUROCOMPUTING, V151, P620, DOI 10.1016/j.neucom.2014.03.091
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Zanuttigh P, 2017, IEEE IMAGE PROC, P3615, DOI 10.1109/ICIP.2017.8296956
NR 47
TC 49
Z9 51
U1 4
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1496
EP 1506
DI 10.1109/TMM.2019.2943740
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100010
DA 2024-07-18
ER

PT J
AU Wu, YL
   Wang, SH
   Huang, QM
AF Wu, Yiling
   Wang, Shuhui
   Huang, Qingming
TI Online Fast Adaptive Low-Rank Similarity Learning for Cross-Modal
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Correlation; Training; Data models; Visualization; Adaptation
   models; Fasteners; Cross-modality learning; similarity function
   learning; online learning; low-rank matrix
ID IMAGES
AB The semantic similarity among cross-modal data objects, e.g., similarities between images and texts, are recognized as the bottleneck of cross-modal retrieval. However, existing batch-style correlation learning methods suffer from prohibitive time complexity and extra memory consumption in handling large-scale high dimensional cross-modal data. In this paper, we propose a Cross-Modal Online Low-Rank Similarity function learning (CMOLRS) method, which learns a low-rank bilinear similarity measurement for cross-modal retrieval. We model the cross-modal relations by relative similarities on the training data triplets and formulate the relative relations as convex hinge loss. By adapting the margin in hinge loss with pair-wise distances in feature space and label space, CMOLRS effectively captures the multi-level semantic correlation and adapts to the content divergence among cross-modal data. Imposed with a low-rank constraint, the similarity function is trained by online learning in the manifold of low-rank matrices. The low-rank constraint not only endows the model learning process with faster speed and better scalability, but also improves the model generality. We further propose fast-CMOLRS combining multiple triplets for each query instead of standard process using single triplet at each model update step, which further reduces the times of gradient updates and retractions. Extensive experiments are conducted on four public datasets, and comparisons with state-of-the-art methods show the effectiveness and efficiency of our approach.
C1 [Wu, Yiling; Wang, Shuhui; Huang, Qingming] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Wang, SH (corresponding author), Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
EM yiling.wu@vipl.ict.ac.cn; wangshuhui@ict.ac.cn; qmhuang@ucas.ac.cn
FU National Natural Science Foundation of China [61672497, 61931008,
   61620106009, U1636214, 61836002]; National Basic Research Program of
   China under 973 Program [2015CB351802]; Key Research Program of Frontier
   Sciences of CAS [QYZDJ-SSW-SYS013]
FX This work was supported in part by the National Natural Science
   Foundation of China underGrants 61672497, 61931008, 61620106009,
   U1636214, and 61836002, in part by National Basic Research Program of
   China under 973 Program 2015CB351802, and in part by Key Research
   Program of Frontier Sciences of CAS underGrant QYZDJ-SSW-SYS013.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2015, P INT C INF KNOWL MA
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Bai B, 2010, INFORM RETRIEVAL, V13, P291, DOI 10.1007/s10791-009-9117-9
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Daras P, 2012, IEEE T MULTIMEDIA, V14, P734, DOI 10.1109/TMM.2011.2181343
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI 10.1109/TP'AMI.2007.1115
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He JF, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P227, DOI 10.1145/2964284.2967216
   He XF, 2004, ADV NEUR IN, V16, P153
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huang X, 2018, PROC CVPR IEEE, P8837, DOI 10.1109/CVPR.2018.00921
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Hwang SY, 2011, PURE APPL CHEM, V83, P233, DOI 10.1351/PAC-CON-10-09-35
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kuang ZH, 2015, INT J COMPUT VISION, V113, P176, DOI 10.1007/s11263-014-0783-8
   Kulis B, 2009, J MACH LEARN RES, V10, P341
   Lazaridis M, 2013, SIGNAL PROCESS-IMAGE, V28, P351, DOI 10.1016/j.image.2012.04.001
   Ma SQ, 2011, MATH PROGRAM, V128, P321, DOI 10.1007/s10107-009-0306-5
   MARTINETZ T, 1994, NEURAL NETWORKS, V7, P507, DOI 10.1016/0893-6080(94)90109-0
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   MEYER CD, 1973, SIAM J APPL MATH, V24, P315
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Shalit U, 2012, J MACH LEARN RES, V13, P429
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Song GL, 2017, IEEE T IMAGE PROCESS, V26, P4168, DOI 10.1109/TIP.2017.2713045
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Tran TQN, 2016, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2016.225
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wu F., 2013, P ACM INT C MULT, P877
   Wu XX, 2016, IEEE T MULTIMEDIA, V18, P1305, DOI 10.1109/TMM.2016.2557722
   Wu YL, 2017, IEEE INT CON MULTI, P823, DOI 10.1109/ICME.2017.8019528
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Xie L, 2016, AAAI CONF ARTIF INTE, P294
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang Yi., 2009, Proceedings of the 17th ACM international conference on Multimedia, P175
   Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1355, DOI 10.1145/2964284.2964336
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 61
TC 12
Z9 13
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1310
EP 1322
DI 10.1109/TMM.2019.2942494
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200016
DA 2024-07-18
ER

PT J
AU Liu, TL
   Wan, JW
   Dai, XB
   Liu, F
   You, QZ
   Luo, JB
AF Liu, Tianliang
   Wan, Junwei
   Dai, Xiubin
   Liu, Feng
   You, Quanzeng
   Luo, Jiebo
TI Sentiment Recognition for Short Annotated GIFs Using Visual-Textual
   Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sentiment analysis; Visualization; Feature extraction; Streaming media;
   Semantics; Twitter; GIFs Sentiment; 3-D Convolution; Convolutional
   Long-Short-Term-Memory; SentiWordNet3; 0; Grid Searching
ID PREDICTION
AB With the rapid development of social media, visual sentiment analysis from image or video has become a hot spot in visual understanding researches. In this work, we propose an effective approach using visual and textual fusion for sentiment analysis of short GIF videos with textual descriptions. We extract both sequence-level and frame-level visual features for each given GIF video. Next, we build a visual sentiment classifier by using the extracted features. We also define a mapping function, which converts the sentiment probability from the classifier to a sentiment score used in our fusion function. At the same time, for the accompanied textual annotations, we employ the Synset forest to extract the sets of the meaningful sentiment words and utilize the SentiWordNet3.0 model to obtain the textual sentiment score. Then, we design a joint visual-textual sentiment score function weighted with visual sentiment component and textual sentiment one. To make the function more robust, we introduce a noticeable difference threshold to further process the fused sentiment score. Finally, we adopt a grid search technique to obtain relevant model hyper-parameters by optimizing a sentiment aware score function. Experimental results and analysis extensively demonstrate the effectiveness of the proposed sentiment recognition scheme on three benchmark datasets including T-GIF dataset, GSO-2016 dataset and Adjusted-GIFGIF dataset.
C1 [Liu, Tianliang; Wan, Junwei; Dai, Xiubin; Liu, Feng] Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Jiangsu Prov Key Lab Image Proc & Image Commun,Na, Minist Educ,Jiangsu Prov Engn Res Ctr High Perfor, Nanjing 210003, Peoples R China.
   [You, Quanzeng] Microsoft Cloud A1, Comp Vis Team, Redmond, WA 98052 USA.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 Nanjing University of Posts & Telecommunications; University of
   Rochester
RP Liu, TL (corresponding author), Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Jiangsu Prov Key Lab Image Proc & Image Commun,Na, Minist Educ,Jiangsu Prov Engn Res Ctr High Perfor, Nanjing 210003, Peoples R China.
EM liutl@njupt.edu.cn; w_j_wan@163.com; daixb@njupt.edu.cn;
   liuf@njupt.edu.cn; quanzeng.you@microsoft.com; jluo@cs.rochester.edu
RI Luo, Jiebo/AAI-7549-2020
OI Liu, Tianliang/0000-0001-9800-1981; Luo, Jiebo/0000-0002-4516-9729
FU National Natural Science Foundation of China [61001152, 61071091,
   31671006, 61572503, 61772286, 61872199, 61872424, 6193000388]; Natural
   Science Foundation of Jiangsu Province of China [BK2012437]; China
   Scholarship Council; "333" project of Jiangsu Province [BRA2017401]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61001152, Grant 61071091, Grant
   31671006, Grant 61572503, Grant 61772286, Grant 61872199, Grant
   61872424, and Grant 6193000388, in part by the Natural Science
   Foundation of Jiangsu Province of China under Grant BK2012437, in part
   by the China Scholarship Council, and in part by "333" project of
   Jiangsu Province under Grant BRA2017401.
CR [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   Baccianella S, 2010, LREC 2010 - SEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION
   Bakhshi S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P575, DOI 10.1145/2858036.2858532
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Cai SQ, 2013, TWELFTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, P1
   Cai Z, 2016, IEEE C EVOL COMPUTAT, P4860, DOI 10.1109/CEC.2016.7744413
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chen WX, 2017, INT CONF AFFECT, P510
   Chikersal P, 2015, LECT NOTES COMPUT SC, V9042, P49, DOI 10.1007/978-3-319-18117-2_4
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   Islam J, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCES ON BIG DATA AND CLOUD COMPUTING (BDCLOUD 2016) SOCIAL COMPUTING AND NETWORKING (SOCIALCOM 2016) SUSTAINABLE COMPUTING AND COMMUNICATIONS (SUSTAINCOM 2016) (BDCLOUD-SOCIALCOM-SUSTAINCOM 2016), P124, DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.29
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Ji RR, 2016, FRONT COMPUT SCI-CHI, V10, P602, DOI 10.1007/s11704-016-5453-2
   Jindal S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING (ICIP), P447, DOI 10.1109/INFOP.2015.7489424
   Jou B, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P213, DOI 10.1145/2647868.2656408
   Kim Y, 2014, IEEE ASME INT C ADV, P1747, DOI 10.1109/AIM.2014.6878336
   Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502
   Li ZH, 2019, MULTIMED TOOLS APPL, V78, P6939, DOI 10.1007/s11042-018-6445-z
   Li ZH, 2018, MULTIMED TOOLS APPL, V77, P1115, DOI 10.1007/s11042-016-4310-5
   Naaman M, 2012, MULTIMED TOOLS APPL, V56, P9, DOI 10.1007/s11042-010-0538-7
   Pereira MHR, 2016, P INT AAAI C WEB SOC, P659
   Poria S, 2018, IEEE INTELL SYST, V33, P17, DOI 10.1109/MIS.2018.2882362
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556
   Wang XX, 2013, PROCEEDINGS OF THE TWELFTH INTERNATIONAL SYMPOSIUM - MANAGEMENT SCIENCE & ENGINEERING (2013), P210
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1008, DOI 10.1145/2964284.2964288
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zhang XY, 2009, IEEE T MULTIMEDIA, V11, P272, DOI 10.1109/TMM.2008.2009689
   Zhang XY, 2016, INT SYMP PARA DISTR, P230, DOI 10.1109/ISPDC.2016.39
   Zhu GM, 2017, IEEE ACCESS, V5, P4517, DOI 10.1109/ACCESS.2017.2684186
NR 44
TC 19
Z9 20
U1 2
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 1098
EP 1110
DI 10.1109/TMM.2019.2936805
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400022
DA 2024-07-18
ER

PT J
AU Qian, YQ
   Yang, M
   Zhao, X
   Wang, CX
   Wang, B
AF Qian, Yeqiang
   Yang, Ming
   Zhao, Xu
   Wang, Chunxiang
   Wang, Bing
TI Oriented Spatial Transformer Network for Pedestrian Detection Using
   Fish-Eye Camera
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Detectors; Feature extraction; Distortion; Deep learning;
   Lenses; Training; Pedestrian detection; deep learning; Projective Model
   Transformation; Oriented Spatial Transformer Network; fish-eye image
   dataset
ID SYSTEM
AB Pedestrian detection using fish-eye cameras is a principal research focus in computer vision. Lack of pedestrian datasets of fish-eye images and pedestrian distortion in fish-eye images are two primary challenges. In this paper, two approaches are proposed to deal with these two challenges, respectively. On the one hand, the projective model transformation (PMT) algorithm is proposed, which can transform normal images into fish-eye images. The PMT can be applied to most of the pedestrian datasets and generates corresponding fish-eye image datasets. In this way, enough training data can be provided through the PMT. On the other hand, the oriented spatial transformer network (OSTN) is designed to rectify warped pedestrian features using CNNs, so that pedestrians in fish-eye images are easier for detectors to recognize. The OSTN can be embedded into universal deep learning based detectors easily. Moreover, the new pedestrian detector, where the OSTN is embedded, can be trained end to end. Finally, the OSTN based fish-eye pedestrian detectors can be trained using fish-eye images, which are generated using the PMT. Experiments on ETH, KITTI, Citypersons, and real pedestrian datasets show the effectiveness of the PMT and accuracy improvement of pedestrian detection in fish-eye images using the OSTN.
C1 [Qian, Yeqiang; Yang, Ming; Zhao, Xu; Wang, Chunxiang; Wang, Bing] Shanghai Jiao Tong Univ, Dept Automat, Key Lab Syst Control & Informat Proc, Minist Educ China, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Yang, M (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Key Lab Syst Control & Informat Proc, Minist Educ China, Shanghai 200240, Peoples R China.
EM qianyeqiang@sjtu.edu.cn; MingYang@sjtu.edu.cn; zhaoxu@sjtu.edu.cn;
   wangcx@sjtu.edu.cn; bingwang@sjtu.edu.cn
RI Yang, Ming/K-8245-2012
OI Yang, Ming/0000-0002-8679-9137
FU National Natural Science Foundation of China [U1764264/61873165]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U1764264/61873165 and in part by the
   International Chair on Automated Driving of Ground Vehicle. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Elisa Ricci.
CR [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], P IEEE RSJ INT C INT
   Bertozzi M, 2015, IEEE INT VEH SYM, P132, DOI 10.1109/IVS.2015.7225675
   Broggi A, 2014, IEEE INT VEH SYM, P918
   Bui MT, 2014, I C CONT AUTOMAT ROB, P389, DOI 10.1109/ICARCV.2014.7064337
   Bülow T, 2004, IEEE T PATTERN ANAL, V26, P1650, DOI 10.1109/TPAMI.2004.129
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Courbon J, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P1689
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Daniilidis K, 2002, THIRD WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P3, DOI 10.1109/OMNVIS.2002.1044483
   Deng LF, 2016, 2016 INTERNATIONAL CONFERENCE ON MODERN ECONOMIC DEVELOPMENT AND ENVIRONMENT PROTECTION (ICMED 2016), P113
   Deng LY, 2017, IEEE INT VEH SYM, P231, DOI 10.1109/IVS.2017.7995725
   Dupuis Y., 2011, 2011 IEEE International Symposium on Robotic and Sensors Environments (ROSE 2011), P243, DOI 10.1109/ROSE.2011.6058532
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Fremont V, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010128
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gressmann M, 2011, IEEE INT C INTELL TR, P1317, DOI 10.1109/ITSC.2011.6082895
   Heng L, 2013, IEEE INT C INT ROBOT, P1793, DOI 10.1109/IROS.2013.6696592
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153
   Levi D, 2015, IEEE INT C INTELL TR, P664, DOI 10.1109/ITSC.2015.114
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Qian YQ, 2017, IEEE INT VEH SYM, P33, DOI 10.1109/IVS.2017.7995695
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schulz W, 2007, LECT NOTES COMPUT SC, V4713, P456
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Suhr JK, 2018, IEEE T INTELL TRANSP, V19, P1122, DOI 10.1109/TITS.2017.2709797
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Wang M, 2018, IEEE T MULTIMEDIA, V20, P620, DOI 10.1109/TMM.2017.2748459
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Zhang SS, 2017, PROC CVPR IEEE, P4457, DOI 10.1109/CVPR.2017.474
   Zhang ZC, 2016, IEEE INT CONF ROBOT, P801, DOI 10.1109/ICRA.2016.7487210
   Zhou K, 2016, DESTECH TRANS COMP
NR 36
TC 23
Z9 25
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 421
EP 431
DI 10.1109/TMM.2019.2929949
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300011
DA 2024-07-18
ER

PT J
AU Luo, L
   Yang, TH
   Zhu, C
   Jin, Z
   Tang, S
AF Luo, Lei
   Yang, Taihai
   Zhu, Ce
   Jin, Zhi
   Tang, Shu
TI Joint Texture/Depth Power Allocation for 3-D Video SoftCast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D video; multi-view video plus depth; SoftCast; power allocation;
   wireless; power-distortion
ID BIT ALLOCATION; MULTIVIEW VIDEO; VIEW SYNTHESIS; DISTORTION;
   TRANSMISSION; COMMUNICATION; OPTIMIZATION; MODEL; DIBR
AB Recently, a novel uncoded (pseudoanalog) scheme called SoftCast is proposed for wireless video transmission, which eliminates the cliff effect of the state-of-the-art source-channel coding based schemes and achieves linear quality transition within a wide range of channel signal-to-noise ratio. Therefore, SoftCast-like uncoded and hybrid transmission has become an attractive research issue for natural 2-D video. However, very few studies focus on the SoftCast-based wireless transmission of the 3-D video (3DV) currently. One critical issue of 3DV SoftCast is how to allocate the limited power budget of the transmitter to the texture videos and depth maps of the 3DV to achieve the optimal overall quality on the receiver side, including the transmission quality of the reference views and the synthesis quality of the virtual views. This paper attempts to solve the optimal joint power allocation problem in an efficient way. First, we formulate the target problem as a constrained power-distortion optimization (PDO) problem mathematically. Then, each part of the distortion is analyzed and formulated in a closed form. Finally, the PDO problem is mapped to an unconstrained convex optimization problem and solved by the Lagrangian multiplier method. Simulation results demonstrate that the performance of the proposed method is close to that of the full search method, which can provide the best performance theoretically. Nevertheless, the complexity of the proposed method is negligible compared with that of the full search method. In addition, as compared with the fixed ratio (e.g., 1:1) power allocation between texture and depth, the proposed method can achieve a PNSR gain up to 1.8 dB.
C1 [Luo, Lei] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Luo, Lei; Zhu, Ce] Univ Elect Sci & Technol China, Sch Informat & Commun, Chengdu 610051, Peoples R China.
   [Yang, Taihai; Tang, Shu] Chongqing Univ Posts & Telecommun, Chongqing 400065, Peoples R China.
   [Zhu, Ce] Univ Elect Sci & Technol China, Ctr Robot, Chengdu 610051, Peoples R China.
   [Jin, Zhi] Shenzhen Univ, Coll Informat Engn, Shenzhen 518060, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; University of
   Electronic Science & Technology of China; Chongqing University of Posts
   & Telecommunications; University of Electronic Science & Technology of
   China; Shenzhen University
RP Zhu, C (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun, Chengdu 610051, Peoples R China.; Zhu, C (corresponding author), Univ Elect Sci & Technol China, Ctr Robot, Chengdu 610051, Peoples R China.
EM luolei@cqupt.edu.cn; yangth@cqupt.edu.cn; eczhu@uestc.edu.cn;
   jinzhi_126@163.com; shutang@cqupt.edu.cn
RI Zhu, Ce/AEN-1875-2022; Jin, Zhi/AAB-2440-2022
OI Jin, Zhi/0000-0001-9670-7366; Luo, Lei/0000-0002-7008-4276
FU National Natural Science Foundation of China [61501074, 61701313,
   61571102, 61601070]; Fundamental and Advanced Technology Research
   Project of Chongqing [cstc2015jcyjA40012, cstc2016jcyjA0455]; Key
   Project of Sichuan Provincial Department of Science and Technology
   [2018JY0035]; Science and Technology Research Project of Chongqing
   Municipal Education Commission [KJ1500430, KJ1600411]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61501074, 61701313, 61571102, and
   61601070, in part by the Fundamental and Advanced Technology Research
   Project of Chongqing under Grants cstc2015jcyjA40012 and
   cstc2016jcyjA0455, in part by the Key Project of Sichuan Provincial
   Department of Science and Technology underGrant 2018JY0035, and in part
   by Science and Technology Research Project of Chongqing Municipal
   Education Commission under Grants KJ1500430 and KJ1600411. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Honggang Wang.
CR [Anonymous], IEEE ICC
   [Anonymous], M16090 ISOIEC JTC1SC
   [Anonymous], 2009, MITCSAILTR2009005
   BERROU C, 1993, IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS 93 : TECHNICAL PROGRAM, CONFERENCE RECORD, VOLS 1-3, P1064, DOI 10.1109/ICC.1993.397441
   Cao B., 2019, IEEE T MOBILE COMPUT
   Cui H, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2338279
   Fan XP, 2013, IEEE T CIRC SYST VID, V23, P1040, DOI 10.1109/TCSVT.2013.2249019
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   He CF, 2018, IEEE T IMAGE PROCESS, V27, P3599, DOI 10.1109/TIP.2018.2818019
   He DL, 2015, ACM S MODEL ANAL SIM, P327, DOI 10.1145/2811587.2811601
   Jakubczak S., 2011, PROC MOBICOM, P289
   Jakubczak S, 2010, ACM SIGCOMM COMP COM, V40, P449, DOI 10.1145/1851275.1851257
   Lawson C., 1974, SOLVING LEAST SQUARE, V77, P673
   Li CL, 2018, IEEE T CIRC SYST VID, V28, P1648, DOI 10.1109/TCSVT.2017.2681024
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P361, DOI 10.1109/TMM.2017.2745709
   Li CL, 2014, IEEE T CIRC SYST VID, V24, P1170, DOI 10.1109/TCSVT.2014.2302517
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Li Y, 2018, IEEE T MULTIMEDIA, V20, P2427, DOI 10.1109/TMM.2018.2796246
   Lin XC, 2015, IEEE WCNC, P1996, DOI 10.1109/WCNC.2015.7127774
   Liu D, 2018, IEEE T MULTIMEDIA, V20, P3097, DOI 10.1109/TMM.2018.2823903
   Liu HF, 2019, IEEE T CIRC SYST VID, V29, P1832, DOI 10.1109/TCSVT.2018.2842818
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Oh BT, 2011, IEEE J-STSP, V5, P1344, DOI 10.1109/JSTSP.2011.2164893
   Schnell M., 1994, IEEE ISSSTA '94. IEEE Third International Symposium on Spread Spectrum Techniques and Applications (Cat. No.94TH0604-9), P505, DOI 10.1109/ISSSTA.1994.379534
   Shannon C. E., 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Shen J, 2018, IEEE T MULTIMEDIA, V20, P2788, DOI 10.1109/TMM.2018.2811622
   SONG XD, 2017, IEEE T CIRCUITS SYST, V19, P1351, DOI DOI 10.1109/TMM.2017.2654123
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan B, 2017, IEEE T MULTIMEDIA, V19, P2293, DOI 10.1109/TMM.2017.2733303
   VEMBU S, 1995, IEEE T INFORM THEORY, V41, P44, DOI 10.1109/18.370119
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiao JM, 2015, IEEE T CIRC SYST VID, V25, P139, DOI 10.1109/TCSVT.2014.2334011
   Xiao LY, 2015, 2ND INTERNATIONAL CONFERENCE ON COMMUNICATION AND TECHNOLOGY (ICCT 2015), P353, DOI 10.1109/ICT.2015.7124710
   Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2621478
   Xiong RQ, 2016, IEEE T IMAGE PROCESS, V25, P1820, DOI 10.1109/TIP.2016.2535288
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P863, DOI 10.1109/TMM.2018.2870540
   Yu L, 2015, IEEE T CIRC SYST VID, V25, P436, DOI 10.1109/TCSVT.2014.2347532
   Yuan H, 2016, IEEE T BROADCAST, V62, P134, DOI 10.1109/TBC.2015.2492461
   Yuan H, 2014, IEEE T BROADCAST, V60, P614, DOI 10.1109/TBC.2014.2361964
   Yuan H, 2012, IEEE T BROADCAST, V58, P558, DOI 10.1109/TBC.2012.2187612
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   Zhao X, 2016, IEEE T CIRC SYST VID, V26, P1117, DOI 10.1109/TCSVT.2015.2444753
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
NR 45
TC 9
Z9 10
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 2973
EP 2984
DI 10.1109/TMM.2019.2919474
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200001
DA 2024-07-18
ER

PT J
AU Yu, TZ
   Wang, LF
   Da, C
   Gu, HX
   Xiang, SM
   Pan, CH
AF Yu, Tingzhao
   Wang, Lingfeng
   Da, Cheng
   Gu, Huxiang
   Xiang, Shiming
   Pan, Chunhong
TI Weakly Semantic Guided Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic guided module; action recognition; cross domain; 3D
   convolution; attention model
AB Action recognition plays a fundamental role in computer vision and video analysis. Nevertheless, extracting effective spatial-temporal features remains a challenging task. This paper proposes three simple but effective weakly semantic guided modules (SGMs) for both environment-constrained and cross-domain action recognition. The SGMs are composed of total 3-D convolution and element-wise gated operations; thus, they are efficient and easy to implement. The semantic guidance is obtained in a weakly supervised manner, in which each video clip is labeled with only an action class instead of pixel-level semantics. Benefitting from the semantic guidance, the network [called semantic guided network (SGN)] can focus on the salient parts of the video clips. Consequently, the redundant information can be reduced and the model is more robust to noise. Besides, benefitting from the intrinsic property of SGMs, SGN is totally end-to-end trainable. Quantities of experiments on both environment-constrained (e.g., Penn, HMDB-51, and UCF-101) and cross-domain (e.g., ODAR) action recognition datasets demonstrate its effectiveness. Specifically, SGN gets improvements of 3.7%, 2.1%, and 5.2% for Penn, HMDB-51, and UCF-101 than the baseline ResNet3D, respectively, and SGN ranked third place in the ODAR 2017 challenge.
C1 [Yu, Tingzhao; Wang, Lingfeng; Da, Cheng; Gu, Huxiang; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yu, Tingzhao; Da, Cheng] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 101408, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Yu, TZ (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM tingzhao.yu@nlpr.ia.ac.cn; wanglingfenglf@tom.com; yu152zhang@163.com;
   fhcqayqb@163.com; sm_xiang@sohu.com; chh.pan@yahoo.com
OI yu, ting zhao/0000-0002-7558-4448
FU National Natural Science Foundation of China [61773377, 61573352,
   91646207, 91438105]
FX This work was supported by the National Natural Science Foundation of
   China under Grants #61773377, #61573352, #91646207, and #91438105.
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2016, P 33 INT C MACHINE L
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], ARXIV160508140
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2017, KINETICS HUMAN ACTIO
   [Anonymous], 2008, PROC BRIT MACH VIS C
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, ARXIV150702159
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], ARXIV160508140
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2015, ACTION RECOGNITION U
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.55
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Bahdanau Dzmitry, 2014, NEURAL MACHINE TRANS
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Canzian L, 2015, IEEE T SIGNAL PROCES, V63, P1282, DOI 10.1109/TSP.2015.2389765
   Cao C., 2016, INT JOINT C ART INT, V1, P3
   Cao CQ, 2018, IEEE T CYBERNETICS, V48, P1095, DOI 10.1109/TCYB.2017.2756840
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen M, 2011, INT CONF CLOUD COMPU, P316, DOI 10.1109/CCIS.2011.6045082
   Chorowski J, 2015, ADV NEUR IN, V28
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dimitropoulos K, 2017, IEEE T CIRC SYST VID, V27, P1143, DOI 10.1109/TCSVT.2016.2527340
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C., 2016, P INT C NEUR INF PRO, P3468
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao SH, 2016, IEEE T CIRC SYST VID, V26, P494, DOI 10.1109/TCSVT.2015.2389413
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Iqbal U, 2017, IEEE INT CONF AUTOMA, P438, DOI 10.1109/FG.2017.61
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kar A, 2017, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR.2017.604
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Li C, 2017, IEEE T IMAGE PROCESS, V26, P2149, DOI 10.1109/TIP.2017.2670782
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Nguyen TV, 2015, IEEE T CIRC SYST VID, V25, P77, DOI 10.1109/TCSVT.2014.2333151
   Ning F, 2005, IEEE T IMAGE PROCESS, V14, P1360, DOI 10.1109/TIP.2005.852470
   Pei WJ, 2017, PROC CVPR IEEE, P820, DOI 10.1109/CVPR.2017.94
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Soomro K., 2012, CoRR, V2
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vezhnevets A, 2011, IEEE I CONF COMP VIS, P643, DOI 10.1109/ICCV.2011.6126299
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Y., 2016, BIG DATA ANAL, V1, P12, DOI [10.1186/s41044-016-0010-4, DOI 10.1186/S41044-016-0010-4]
   Wang Z, 2017, IEEE T IMAGE PROCESS, V26, P2028, DOI 10.1109/TIP.2017.2666739
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yang Y, 2017, IEEE IMAGE PROC, P2164, DOI 10.1109/ICIP.2017.8296665
   Yao HX, 2017, IEEE T CIRC SYST VID, V27, P405, DOI 10.1109/TCSVT.2017.2669658
   Yu TZ, 2017, IEEE IMAGE PROC, P1552, DOI 10.1109/ICIP.2017.8296542
   Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280
NR 77
TC 22
Z9 22
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2504
EP 2517
DI 10.1109/TMM.2019.2907060
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400007
DA 2024-07-18
ER

PT J
AU Li, XR
   Xu, CX
   Wang, XX
   Lan, WY
   Jia, ZX
   Yang, G
   Xu, JP
AF Li, Xirong
   Xu, Chaoxi
   Wang, Xiaoxu
   Lan, Weiyu
   Jia, Zhengxiong
   Yang, Gang
   Xu, Jieping
TI COCO-CN for Cross-Lingual Image Tagging, Captioning, and Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE COCO-CN; Chinese language; cross-lingual learning; image tagging; image
   captioning; image retrieval
ID FUSION
AB This paper contributes to cross-lingual image annotation and retrieval in terms of data and baseline methods. We propose COCO-CN, a novel dataset enriching MS-COCO with manually written Chinese sentences and tags. For effective annotation acquisition, we develop a recommendation-assisted collective annotation system, automatically providing an annotator with several tags and sentences deemed to be relevant with respect to the pictorial content. Having 20 342 images annotated with 27 218 Chinese sentences and 70 993 tags, COCO-CN is currently the largest Chinese-English dataset that provides a unified and challenging platform for cross-lingual image tagging, captioning, and retrieval. We develop conceptually simple yet effective methods per task for learning from cross-lingual resources. Extensive experiments on the three tasks justify the viability of the proposed dataset and methods. Data and code are publicly available at https://github.com/li-xirong/coco-cn.
C1 [Li, Xirong; Xu, Chaoxi; Wang, Xiaoxu; Lan, Weiyu; Jia, Zhengxiong; Yang, Gang; Xu, Jieping] Renmin Univ China, Sch Informat, AI & Media Comp Lab, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China.
   [Li, Xirong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Anhui, Peoples R China.
C3 Renmin University of China; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS
RP Xu, JP (corresponding author), Renmin Univ China, Sch Informat, AI & Media Comp Lab, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China.
EM xirong@ruc.edu.cn; xcx@ruc.edu.cn; wangxiaoxuinfo@ruc.edu.cn;
   bbluey@126.com; jcloud@ruc.edu.cn; yanggang@ruc.edu.cn;
   xjieping@ruc.edu.cn
RI Li, Xirong/AAD-3347-2019
OI Li, Xirong/0000-0002-0220-8310
FU National Natural Science Foundation of China [61672523, 61771468,
   61773385]; Fundamental Research Funds for the Central Universities;
   Research Funds of Renmin University of China [18XNLG19]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672523, 61771468, and 61773385, and
   in part by the Fundamental Research Funds for the Central Universities
   and the Research Funds of Renmin University of China under Grant
   18XNLG19. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Wolfgang Hurst.
CR [Anonymous], IEEE T NEURAL NETW L
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   [Anonymous], 2014, TOIS
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Caputo B, 2013, LECT NOTES COMPUT SC, V8138, P250, DOI 10.1007/978-3-642-40802-1_26
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Chua Tat-Seng, 2009, CIVR
   Clarifai, 2018, CLAR IM VID REC API
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Desmond Elliott, 2017, P 8 INT JOINT C NATU, P130
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Elliott D., 2016, P ACL 2016, P70
   Fu JL, 2017, APSIPA TRANS SIGNAL, V6, DOI 10.1017/ATSIP.2017.12
   Funaki R., 2015, P 2015 C EMPIRICAL M, P585
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gella Spandana, 2017, P 2017 C EMPIRICAL M, P2839
   Grubinger M., 2006, Language Resources and Evaluation, P13
   Hitschler J, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2399
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Koochali A., 2016, P ACM WORKSH MULT CO, P35, DOI DOI 10.1145/2983554.2983560
   Lan WY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1549, DOI 10.1145/3123266.3123366
   Levenshtein V. I., 1966, SOV PHYS DOKL, V10, P707
   Li XR, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P271, DOI 10.1145/2911996.2912049
   Li XR, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2906152
   Li X, 2018, IEEE T MULTIMEDIA, V20, P1169, DOI 10.1109/TMM.2017.2761985
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Min KR, 2015, LECT NOTES ARTIF INT, V9362, P520, DOI 10.1007/978-3-319-25207-0_48
   Miyazaki T, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1780
   Peng Y., IEEE T CIRCUITS SYST, V28, P2372
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Rajendran J., 2016, 2016 C N AM CHAPT AS, P171, DOI [DOI 10.18653/V1/N16-1021, 10.18653/v1/N16-1021]
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Shen Z., 2018, P 26 ACM INT C MULT, P441
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Tsutsui S., 2017, P C COMP VIS PATT RE
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang ZY, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2611388
   Wei QJ, 2017, PROCEEDINGS OF THE 15TH INTERNATIONAL WORKSHOP ON CONTENT-BASED MULTIMEDIA INDEXING (CBMI), DOI 10.1145/3095713.3095751
   Wu J., 2017, ARXIV171106475
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yao JC, 2018, IEEE T MULTIMEDIA, V20, P224, DOI 10.1109/TMM.2017.2716829
   Yoshikawa Y, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P417, DOI 10.18653/v1/P17-2066
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
NR 49
TC 51
Z9 54
U1 2
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2347
EP 2360
DI 10.1109/TMM.2019.2896494
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, YH
   Wang, XL
   Zhang, GQ
   Xiao, BH
   Xiao, F
   Zhang, JW
AF Zheng, Yuhui
   Wang, Xilong
   Zhang, Guoqing
   Xiao, Baihua
   Xiao, Fu
   Zhang, Jianwei
TI Multi-Kernel Coupled Projections for Domain Adaptive Dictionary Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dictionary learning; multiple kernel learning; discriminative
   projections; domain adaptation
ID KERNEL; DEEP; TRANSPORT
AB Dictionary learning has produced state-of-the-art results in various classification tasks. However, if the training data have a different distribution than the testing data, the learned sparse representation might not be optimal. Recently, several domain-adaptive dictionary learning (DADE.) methods and kernels have been proposed and have achieved impressive performance. However, the performance of these single kernel-based methods heavily depends heavily on the choice of the kernel, and the question of how to combine multiple kernel learning (MKL) with the DADL framework has not been well studied. Motivated by these concerns, in this paper, we propose a multi-kernel domain-adaptive sparse representation-based classification (MK-DASRC) and then use it as a criterion to design a multi-kernel sparse representation-based domain-adaptive discriminative projection method, in which the discriminative features of the data in the two domains are simultaneously learned with the dictionary. The purpose of this method is to maximize the between-class sparse reconstruction residuals of data from both domains, and minimize the within-class sparse reconstruction residuals of data in the low-dimensional subspace. Thus, the resulting representations can satisfactorily fit MK-DASRC and simultaneously display discriminability. Extensive experimental results on a series of benchmark databases show that our method performs better than the state-of-the-art methods.
C1 [Zheng, Yuhui; Wang, Xilong; Zhang, Guoqing] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Xiao, Baihua] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Xiao, Fu] Nanjing Univ Posts & Telecommun, Sch Comp Sci, Nanjing 210003, Jiangsu, Peoples R China.
   [Zhang, Jianwei] Nanjing Univ Informat Sci & Technol, Sch Math & Stat, Nanjing 210044, Jiangsu, Peoples R China.
C3 Nanjing University of Information Science & Technology; Chinese Academy
   of Sciences; Institute of Automation, CAS; Nanjing University of Posts &
   Telecommunications; Nanjing University of Information Science &
   Technology
RP Zhang, GQ (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
EM zheng_yuhui@nuist.edu.cn; wangxilong1150@gmail.com;
   xiayang14551@163.com; baihua.xiao@ia.ac.cn; xiaof@njupt.edu.cn;
   zhangjw@nuist.edu.cn
RI wang, long/IZE-1764-2023; Zheng, Yuhui/AAF-2420-2019; zhang,
   guoqing/GXG-4800-2022
OI xiao, bai hua/0000-0003-3941-1141; guoqing, zhang/0000-0002-8741-8607
FU Natural Science Foundation of China [61806099, 61672293]; Natural
   Science Foundation of Jiangsu Province, China [BK20180790]; Talent Start
   Foundation of Nanjing University of Information Science and Technology
   [2243141701077]; PAPD (priority academic program development of Jiangsu
   Higher Education Institutions)
FX This work was support in part by the Natural Science Foundation of China
   undere Grants 61806099 and 61672293, in part by the Natural Science
   Foundation of Jiangsu Province, China under Grant BK20180790, in part by
   the Talent Start Foundation of Nanjing University of Information Science
   and Technology (2243141701077), and in part by the PAPD (a project
   funded by the priority academic program development of Jiangsu Higher
   Education Institutions). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Zixiang
   Xiong.
CR Abdelwahab M, 2015, INT CONF ACOUST SPEE, P5058, DOI 10.1109/ICASSP.2015.7178934
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Bucak SS, 2014, IEEE T PATTERN ANAL, V36, P1354, DOI 10.1109/TPAMI.2013.212
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen J, 2017, CHIN CONTR CONF, P7327, DOI 10.23919/ChiCC.2017.8028514
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Donahue J, 2014, PR MACH LEARN RES, V32
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151
   Ghifary M, 2014, LECT NOTES ARTIF INT, V8862, P898, DOI 10.1007/978-3-319-13560-1_76
   Glorot X., 2011, P 28 INT C INT C MAC, P513
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gong M., 2016, P 33 INT C MACH LEAR, V48, P2839
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Griffin G., 2007, CALTECH 256 OBJECT C
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Han YH, 2014, IEEE T MULTIMEDIA, V16, P1115, DOI 10.1109/TMM.2014.2306092
   Heng Zhang, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163133
   Herath S, 2017, PROC CVPR IEEE, P3956, DOI 10.1109/CVPR.2017.421
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167
   Liu X, 2016, ADV COND MATTER PHYS, V2016, DOI 10.1155/2016/8528617
   Lu B., 2015, P BRIT MACH VIS C
   Lu H, 2017, IEEE I CONF COMP VIS, P599, DOI 10.1109/ICCV.2017.72
   Monga V., 2005, HALFTONING TOOLBOX M
   Nguyen HV, 2015, IEEE T IMAGE PROCESS, V24, P5479, DOI 10.1109/TIP.2015.2479405
   Ni J, 2013, PROC CVPR IEEE, P692, DOI 10.1109/CVPR.2013.95
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Qian SS, 2018, IEEE T MULTIMEDIA, V20, P2086, DOI 10.1109/TMM.2017.2785227
   Qiu Q, 2012, LECT NOTES COMPUT SC, V7575, P631, DOI 10.1007/978-3-642-33765-9_45
   Saenko K., 2013, ARXIV PREPRINT ARXIV
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Shekhar S, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2431440
   Shekhar S, 2013, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.2013.53
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Shrivastava A, 2015, PATTERN RECOGN, V48, P2667, DOI 10.1016/j.patcog.2015.03.005
   Shrivastava A, 2014, IEEE WINT CONF APPL, P277, DOI 10.1109/WACV.2014.6836088
   Shrivastava A, 2014, IEEE T IMAGE PROCESS, V23, P3013, DOI 10.1109/TIP.2014.2324290
   Sun B, 2016, IEEE IND ELEC, P2052, DOI 10.1109/IECON.2016.7793405
   Sun C, 2015, IEEE T MULTIMEDIA, V17, P1747, DOI 10.1109/TMM.2015.2463218
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1
   Wu X, 2017, PATTERN RECOGN, V66, P404, DOI 10.1016/j.patcog.2016.12.001
   Xu H., 2018, ARXIV180404687
   Xu Z., 2010, P 27 INT C MACHINE L, P1175
   Yang BY, 2018, AAAI CONF ARTIF INTE, P7453
   Yang Q, 2012, IEEE SYS MAN CYBERN, P1, DOI 10.1109/ICSMC.2012.6377667
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yao T, 2015, PROC CVPR IEEE, P2142, DOI 10.1109/CVPR.2015.7298826
   Yuan Shi, 2012, P INT C MACH LEARN I, P1275
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang GQ, 2017, IEEE T IMAGE PROCESS, V26, P5922, DOI 10.1109/TIP.2017.2745684
   Zhang GQ, 2016, J VIS COMMUN IMAGE R, V40, P470, DOI 10.1016/j.jvcir.2016.07.015
   Zhang GQ, 2016, NEUROCOMPUTING, V207, P300, DOI 10.1016/j.neucom.2016.04.044
   Zhang GQ, 2016, IEEE T IMAGE PROCESS, V25, P4271, DOI 10.1109/TIP.2016.2587119
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
   Zhang LH, 2015, INT CONF SOFTW ENG, P931, DOI 10.1109/ICSESS.2015.7339207
NR 65
TC 31
Z9 31
U1 4
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2292
EP 2304
DI 10.1109/TMM.2019.2900166
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200011
DA 2024-07-18
ER

PT J
AU Zhu, WH
   Zhai, GT
   Min, XK
   Hu, MH
   Liu, J
   Guo, GD
   Yang, XK
AF Zhu, Wenhan
   Zhai, Guangtao
   Min, Xiongkuo
   Hu, Menghan
   Liu, Jing
   Guo, Guodong
   Yang, Xiaokang
TI Multi-Channel Decomposition in Tandem With Free-Energy Principle for
   Reduced-Reference Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment (IQA); reduced-reference (RR); multi-channel
   decomposition; free-energy principle; human visual system
ID INFORMATION
AB The visual quality of perceptions is highly correlated with the mechanisms of the human brain and visual system. Recently, the free-energy principle, which has been widely researched in brain theory and neuroscience, is introduced to quantize the perception, action, and learning in human brain. In the field of image quality assessment (IQA), on one hand, the free-energy principle can resort to the internal generative model to simulate the visual stimulus of the human beings. On the other hand, abundant psychological and neurobiological studies reveal that different frequency and orientation components of one visual stimulus arouse different neurons in the striate cortex, and the striate cortex processes visual information in the cerebral cortex. Motivated by these two aspects, a novel reduce-reference IQA metric called the multi-channel free-energy based reduced-reference quality metric is proposed in this paper. First, a two-level discrete Haar wavelet transform is used to decompose the input reference and distorted images. Next, to simulate the generative model in the human brain, the sparse representation is leveraged to extract the free-energy-based features in subband images. Finally, the overall quality metric is obtained through the support vector regressor. Extensive experimental comparisons on four benchmark image quality databases (LIVE, CSIQ, TID2008, and TID2013) demonstrate that the proposed method is highly competitive with the representative reduced-reference and classical full-reference models.
C1 [Zhu, Wenhan; Zhai, Guangtao; Min, Xiongkuo; Yang, Xiaokang] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, Artificial Intelligence Inst, Shanghai 200240, Peoples R China.
   [Zhu, Wenhan; Zhai, Guangtao; Min, Xiongkuo; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
   [Hu, Menghan] East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200240, Peoples R China.
   [Liu, Jing] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing 100094, Peoples R China.
   [Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing 100094, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; East China
   Normal University; Tianjin University; Baidu
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, Artificial Intelligence Inst, Shanghai 200240, Peoples R China.; Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
EM zhuwenhan823@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn;
   minxiongkuo@sjtu.edu.cn; mhhu@ce.ecnu.edu.cn; jliu_tju@tju.edu.cn;
   guoguodong01@baidu.com; xkyang@sjtu.edu.cn
RI Liu-Zeng, Jing/F-8582-2011; Hu, Menghan/AAK-7153-2021; Zhai,
   Guangtao/X-5949-2019; Yang, Xiaokang/C-6137-2009; Min,
   Xiongkuo/A-7097-2019
OI Hu, Menghan/0000-0002-8557-8930; Zhai, Guangtao/0000-0001-8165-9322;
   Yang, Xiaokang/0000-0003-4029-3322; Min, Xiongkuo/0000-0001-5693-0416
FU National Science Foundation of China [61831015, 61521062, 61527804];
   National Key Research and Development Program of China [2016YFB1001003];
   NSFC [61527804]; STCSM [18DZ1112300]; Equipment Pre-research Joint
   Research Program of Ministry of Education [6141A020223]; China
   Postdoctoral Science Foundation [BX20180197]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61831015, 61521062, and 61527804, in part by the
   National Key Research and Development Program of China (2016YFB1001003),
   NSFC (61527804), STCSM (18DZ1112300), and Equipment Pre-research Joint
   Research Program of Ministry of Education 6141A020223, and in part by
   the China Postdoctoral Science Foundation under Grant BX20180197. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Manoranjan Paul.
CR [Anonymous], 2006, J MACH LEARN RES
   [Anonymous], IEEE T INTELL TRANSP
   [Anonymous], 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment
   Bampis CG, 2017, IEEE SIGNAL PROC LET, V24, P1333, DOI 10.1109/LSP.2017.2726542
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Feynman R. P., 1998, Statistical Mechanics: A Set of Lectures, Advanced Books Classics
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Goldstein E.B., 2016, Sensation and perception
   Golestaneh S, 2016, IEEE T IMAGE PROCESS, V25, P5293, DOI 10.1109/TIP.2016.2601821
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   HEITGER F, 1992, VISION RES, V32, P963, DOI 10.1016/0042-6989(92)90039-L
   HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229, DOI 10.1152/jn.1965.28.2.229
   Kauffman Stuart A., 1993, The Origins of Order
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Liu M, 2017, IEEE T BROADCAST, V63, P71, DOI 10.1109/TBC.2016.2597545
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu YT, 2016, IEEE INT SYMP CIRC S, P1586, DOI 10.1109/ISCAS.2016.7538867
   MALLAT SG, 1989, IEEE T ACOUST SPEECH, V37, P2091, DOI 10.1109/29.45554
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2018, SIGNAL PROCESS, V145, P127, DOI 10.1016/j.sigpro.2017.10.025
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Narwaria M, 2018, IEEE T BROADCAST, V64, P446, DOI 10.1109/TBC.2018.2832441
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Redi JA, 2010, IEEE T CIRC SYST VID, V20, P1757, DOI 10.1109/TCSVT.2010.2087456
   Schölkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Sheskin J.D., 2004, Handbook of Parametric and Nonparametric Statistical Procedures, VThird
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2006, IEEE T IMAGE PROCESS, V15, P1680, DOI 10.1109/TIP.2005.864165
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
NR 49
TC 30
Z9 33
U1 2
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2334
EP 2346
DI 10.1109/TMM.2019.2902484
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200014
DA 2024-07-18
ER

PT J
AU Li, XY
   Jiang, SQ
AF Li, Xiangyang
   Jiang, Shuqiang
TI Know More Say Less: Image Captioning Based on Scene Graphs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image captioning; scene graph; relationship; long short-term network;
   attention mechanism; vision-language
ID LANGUAGE
AB Automatically describing the content of an image has been attracting considerable research attention in the multimedia field. To represent the content of an image, many approaches directly utilize convolutional neural networks (CNNs) to extract visual representations, which are fed into recurrent neural networks to generate natural language. Recently, some approaches have detected semantic concepts from images and then encoded them into high-level representations. Although substantial progress has been achieved, most of the previous methods treat entities in images individually, thus lacking structured information that provides important cues for image captioning. In this paper, we propose a framework based on scene graphs for image captioning. Scene graphs contain abundant structured information because they not only depict object entities in images but also present pairwise relationships. To leverage both visual features and semantic knowledge in structured scene graphs, we extract CNN features from the bounding box offsets of object entities for visual representations, and extract semantic relationship features from triples (e.g., man riding bike) for semantic representations. After obtaining these features, we introduce a hierarchical-attention-based module to learn discriminative features for word generation at each time step. The experimental results on benchmark datasets demonstrate the superiority of our method compared with several state-of-the-art methods.
C1 [Li, Xiangyang; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Li, Xiangyang; Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM xiangyang.li@vipl.ict.ac.cn; sqjiang@ict.ac.cn
RI Li, Xiangyang/AAF-1260-2019
OI Li, Xiangyang/0000-0002-3944-4704
FU National Natural Science Foundation of China [61532018]; Beijing Natural
   Science Foundation [L182054]; National Program for Special Support of
   Eminent Professionals; National Program for Support of Top-notch Young
   Professionals
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61532018, in part by the Beijing Natural
   Science Foundation under Grant L182054, in part by the National Program
   for Special Support of Eminent Professionals, and in part by the
   National Program for Support of Top-notch Young Professionals. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Engin Erzin. (Corresponding
   author: Shuqiang Jiang.)
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Aneja J, 2018, PROC CVPR IEEE, P5561, DOI 10.1109/CVPR.2018.00583
   [Anonymous], 2016, P 2016 ACM MULT C
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, FINITE ELEMENT METHO
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2017, P 2017 ACM MULT C, DOI DOI 10.1145/3123266.3123275
   [Anonymous], 2013, P 26 INT C NEUR INF, DOI DOI 10.5555/2999792.2999923
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2811621
   [Anonymous], 2013, P 51 ANN M ASS COMP
   [Anonymous], P 15 C EUR CHAPT ASS
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Bin Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1345, DOI 10.1145/3123266.3123391
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cho K., 2014, P 8 WORKSH SYNT SEM, P103
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Choi WG, 2013, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2013.12
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352
   Desai C, 2012, LECT NOTES COMPUT SC, V7575, P158, DOI 10.1007/978-3-642-33765-9_12
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Fried Daniel, 2018, ADV NEURAL INFORM PR, V31, P3318
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Klawonn M, 2018, AAAI CONF ARTIF INTE, P6992
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lan WY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1549, DOI 10.1145/3123266.3123366
   Lavie A., 2007, P 2 WORKSH STAT MACH, P228
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766
   Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin Kevin, 2017, NIPS
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F, 2017, I NAVIG SAT DIV INT, P4176
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao Junhua, 2015, P INT C LEARN REPR
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Ordonez V, 2016, INT J COMPUT VISION, V119, P46, DOI 10.1007/s11263-015-0840-y
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Plummer BA, 2017, IEEE I CONF COMP VIS, P1946, DOI 10.1109/ICCV.2017.213
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang XY, 2018, PROCEEDINGS 2018 IEEE/ACM 40TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE), P37, DOI 10.1145/3180155.3180196
   Wang YZ, 2018, ADV SOC SCI EDUC HUM, V195, P397
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu K., 2015, COMPUTER SCI, P2048
   Yang ZL, 2016, ADV NEUR IN, V29
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang Hanwang, 2017, P IEEE C COMP VIS PA, P5532, DOI DOI 10.48550/ARXIV.1702.08319
   Zhao N, 2016, PROCEEDINGS OF 2016 8TH INTERNATIONAL CONFERENCE ON MODELLING, IDENTIFICATION & CONTROL (ICMIC 2016), P989, DOI 10.1109/ICMIC.2016.7804258
NR 80
TC 113
Z9 121
U1 0
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2117
EP 2130
DI 10.1109/TMM.2019.2896516
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700018
DA 2024-07-18
ER

PT J
AU Kang, C
   Zhu, L
   Qian, XM
   Han, JW
   Wang, M
   Tang, YY
AF Kang, Chen
   Zhu, Li
   Qian, Xueming
   Han, Junwei
   Wang, Meng
   Tang, Yuan Yan
TI Geometry and Topology Preserving Hashing for SIFT Feature
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CBIR; geometric information; GTPH; hashing; SIFT
ID IMAGE RETRIEVAL; SEARCH
AB In recent years, content-based image retrieval has been of concern because of practical needs on Internet services, especially methods that can improve retrieving speed and accuracy. The SIFT feature is a well-designed local feature. It has mature applications in feature matching and retrieval, whereas the raw SIFT feature is high dimensional, with high storage cost as well as computational cost in feature similarity measurements. Thus, we propose a hashing scheme for fast SIFT feature-based image matching and retrieval. First, a training process of the hashing function involves geometric and topological information being introduced; second, a geometry-enhanced similarity evaluation that considers both the global and details of images in evaluation is explained. Compared with state-of-the-art methods, our method achieves better performance.
C1 [Kang, Chen] Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Shaanxi, Peoples R China.
   [Kang, Chen] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Shaanxi, Peoples R China.
   [Kang, Chen] Univ Paris Saclay, Univ Paris Sud, Lab Signaux & Syst, CNRS,Cent Supelec, F-91192 Gif Sur Yvette, France.
   [Zhu, Li] Xi An Jiao Tong Univ, Sch Software, Xian 710049, Shaanxi, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Shaanxi, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES Lab, Sch Elect & Informat Engn, Xian 710049, Shaanxi, Peoples R China.
   [Qian, Xueming] Zhibian Technol Co Ltd, Taizhou 317000, Peoples R China.
   [Han, Junwei] Northwestern Polytech Univ, Sch Automat & Informat Engn, Xian 710049, Shaanxi, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Hefei 230011, Anhui, Peoples R China.
   [Tang, Yuan Yan] Macau Univ, Taipa 999078, Macau, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Universite Paris
   Saclay; Universite Paris Cite; Centre National de la Recherche
   Scientifique (CNRS); Xi'an Jiaotong University; Xi'an Jiaotong
   University; Xi'an Jiaotong University; Northwestern Polytechnical
   University; Hefei University of Technology; University of Macau
RP Zhu, L (corresponding author), Xi An Jiao Tong Univ, Sch Software, Xian 710049, Shaanxi, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Shaanxi, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, SMILES Lab, Sch Elect & Informat Engn, Xian 710049, Shaanxi, Peoples R China.
EM kangchen@stu.xjtu.edu.cn; zhuli@mail.xjtu.edu.cn;
   qianxm@mail.xjtu.edu.cn; junweihan2010@gmail.com;
   eric.mengwang@gmail.com; yytang@umac.edu.cn
RI Wang, Meng/ITR-8699-2023
FU NSFC [61732008, 61772407, 61332018, 1531141]; National Key R&D Program
   of China [2017YFF0107700]; World-Class Universities; Characteristic
   Development Guidance Funds for the Central Universities [PY3A022]
FX This work was supported in part by the NSFC under Grant 61732008, Grant
   61772407, Grant 61332018, and Grant 1531141); in part by the National
   Key R&D Program of China under Grant 2017YFF0107700; and in part by the
   World-Class Universities (Disciplines) and the Characteristic
   Development Guidance Funds for the Central Universities (PY3A022). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Benoit Huet.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], P ACM INT C INF KNOW
   [Anonymous], HASHING SIMILARITY S
   [Anonymous], 2009, NIPS
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Ji J., 2012, 25 INT C NEUR INF PR, P108
   Jian M, 2016, IEEE T MULTIMEDIA, V18, P458, DOI 10.1109/TMM.2016.2515367
   Jiang Q Y, 2015, P INT C ART INT, P331
   Kordelas GA, 2016, IEEE T MULTIMEDIA, V18, P155, DOI 10.1109/TMM.2015.2505905
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lei Z, 2015, LECT NOTES ENG COMP, P27
   Li WJ, 2016, IJCAI, P1711
   Li YS, 2018, IEEE T GEOSCI REMOTE, V56, P950, DOI 10.1109/TGRS.2017.2756911
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu D, 2016, IEEE T MULTIMEDIA, V18, P1628, DOI 10.1109/TMM.2016.2568099
   Luo J., 2009, INT J IMAGE PROCESSI, V3, P143, DOI DOI 10.1007/S11270-006-2859-8
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Qian XM, 2018, IEEE T IMAGE PROCESS, V27, P1178, DOI 10.1109/TIP.2017.2769454
   Qian XM, 2017, P IEEE, V105, P1937, DOI 10.1109/JPROC.2017.2731600
   Qian XM, 2017, IEEE T MULTIMEDIA, V19, P813, DOI 10.1109/TMM.2016.2638207
   Qian XM, 2016, IEEE T IMAGE PROCESS, V25, P195, DOI 10.1109/TIP.2015.2497145
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang M, 2018, IEEE T IMAGE PROCESS, V27, P907, DOI 10.1109/TIP.2017.2751150
   Wang YX, 2018, IEEE T IMAGE PROCESS, V27, P4437, DOI 10.1109/TIP.2018.2837219
   Xia R, 2014, 2014 INTERNATIONAL CONFERENCE ON POWER SYSTEM TECHNOLOGY (POWERCON), P1521, DOI 10.1109/POWERCON.2014.6993796
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   Yang X., 2014, P 23 ACM INT C INF K, P1903
   Yang XP, 2015, MULTIMEDIA SYST, V21, P217, DOI 10.1007/s00530-014-0379-8
   Yang XY, 2015, PATTERN RECOGN, V48, P3093, DOI 10.1016/j.patcog.2014.12.017
   Yang XY, 2015, IEEE T IMAGE PROCESS, V24, P1709, DOI 10.1109/TIP.2015.2411433
   Yu X, 2013, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2013.66
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3025, DOI 10.1109/TIP.2014.2326010
   Zhang L, 2013, PROCEEDINGS OF THE EIGHTH INTERNATIONAL SYMPOSIUM ON VITICULTURE AND ENOLOGY (2013), P123
   Zhang SL, 2014, IEEE J EM SEL TOP C, V4, P130, DOI 10.1109/JETCAS.2014.2298272
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhang YD, 2014, IEEE T MULTIMEDIA, V16, P1127, DOI 10.1109/TMM.2014.2306392
   Zhao Y., 2015, MULTIMEDIA MODELING
   Zhou W., 2012, Proceedings of the second ACM conference on Data and Application Security and Privacy, P1
   Zhou W., 2013, ACM T MULTIM COMPUT, V9, P319
NR 49
TC 15
Z9 15
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1563
EP 1576
DI 10.1109/TMM.2018.2883868
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400018
DA 2024-07-18
ER

PT J
AU Zhang, H
   Ngo, CW
AF Zhang, Hao
   Ngo, Chong-Wah
TI A Fine Granularity Object-Level Representation for Event Detection and
   Recounting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia event detection and recounting; object encoding; search
   result reasoning
ID SEMANTIC GAP; VIDEO RETRIEVAL; RECOGNITION; CLASSIFICATION
AB Multimedia events such as "birthday party" usually involve the complex interaction between humans and objects. Unlike actions and sports, these events rarely contain unique motion patterns to be vividly explored for recognition. To encode rich objects in the events, a common practice is to tag an individual video frame with object labels, represented as a vector signifying probabilities of object appearances. These vectors are then pooled across frames to obtain a video-level representation. The current practices suffer from two deficiencies due to the direct employment of deep convolutional neural network (DCNN) and standard feature pooling techniques. First, the use of max-pooling and softmax layers in DCNN overemphasize the primary object or scene in a frame, producing a sparse vector that overlooks the existence of secondary or small-size objects. Second, feature pooling by max or average operator over sparse vectors makes the video-level feature unpredictable in modeling the object composition of an event. To address these problems, this paper proposes a new video representation, named Object-VLAD, which treats each object equally and encodes them into a vector for multimedia event detection. Furthermore, the vector can be flexibly decoded to identify evidences such as key objects to recount the reason why a video is retrieved for an event of interest. Experiments conducted on MED13 and MED14 datasets verify the merit of Object-VLAD by consistently outperforming several state-of-the-arts in both event detection and recounting.
C1 [Zhang, Hao; Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Zhang, H (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM hzhang57-c@my.cityu.edu.hk; cwngo@cs.cityu.edu.hk
RI Zhang, Hao/ABF-5434-2021
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 120213]
FX This work was supported by the Research Grants Council of the Hong Kong
   Special Administrative Region, China, under Grant CityU 120213. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Xilin Chen.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], CORR
   [Anonymous], P TREC VID RETR EV
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P TREC VID RETR EV
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P TREC VID RETR EV
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], INT C LEARN REPR APR
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P TREC VID RETR EV
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], P BMVC
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Chang XJ, 2015, PR MACH LEARN RES, V37, P1348
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Ding L, 2012, BIOMED ENG LETT, V2, P2, DOI 10.1007/s13534-012-0047-x
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gan C, 2017, AAAI CONF ARTIF INTE, P4032
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Han XT, 2017, IEEE T MULTIMEDIA, V19, P1583, DOI 10.1109/TMM.2017.2671414
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hou J., 2016, PROC IEEE INT C MULT, P1
   Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jhuo IH, 2014, INT C PATT RECOG, P666, DOI 10.1109/ICPR.2014.125
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Liu J, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS IEEE-ROBIO 2014, P297, DOI 10.1109/ROBIO.2014.7090346
   Liu JE, 2013, IEEE WORK APP COMP, P339, DOI 10.1109/WACV.2013.6475038
   Lu YJ, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P127, DOI 10.1145/2911996.2912015
   Ma H, 2010, IEEE T MULTIMEDIA, V12, P462, DOI 10.1109/TMM.2010.2051360
   Ma ZG, 2017, IEEE T MULTIMEDIA, V19, P1558, DOI 10.1109/TMM.2017.2659221
   Mazloom M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P123, DOI 10.1145/2671188.2749402
   Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Mettes P, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P427, DOI 10.1145/2671188.2749404
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Over P., 2014, Proceedings of TRECVID, P52
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV
   Sun C, 2014, IEEE INT CONF CON AU, P249, DOI 10.1109/ICCA.2014.6870928
   Sun C, 2014, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2014.329
   Tan CC, 2016, INT J MULTIMED INF R, V5, P73, DOI 10.1007/s13735-015-0090-3
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang F, 2014, IEEE T MULTIMEDIA, V16, P1303, DOI 10.1109/TMM.2014.2315780
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zhang ZH, 2016, ANN TRANSL MED, V4, DOI 10.21037/atm.2016.03.37
   Zhao R, 2002, IEEE T MULTIMEDIA, V4, P189, DOI 10.1109/TMM.2002.1017733
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 71
TC 13
Z9 13
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1450
EP 1463
DI 10.1109/TMM.2018.2884478
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400009
DA 2024-07-18
ER

PT J
AU Cheng, H
   Zhang, J
   Wu, Q
   An, P
AF Cheng, Hao
   Zhang, Jian
   Wu, Qiang
   An, Ping
TI A Computational Model for Stereoscopic Visual Saliency Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pop-out effect; comfort zone; background effect; multi-feature saliency
   prediction
ID OBJECT DETECTION; ATTENTION MODEL; SEGMENTATION; IMAGES
AB Depth information plays an important role in human vision as it provides additional cues that distinguish objects from their backgrounds. This paper explores depth information for analyzing stereoscopic saliency and presents a computational model that predicts stereoscopic visual saliency based on three aspects of human vision: 1) the pop-out effect; 2) comfort zones; and 3) background effects. Through an analysis of these three phenomena, we find that most of the stereoscopic saliency region can be explained. Our model comprises three modules, each describing one aspect of saliency distribution, and a control function that can be used to adjust the three models independently. The relationship between the three models is not mutually exclusive. One, two, or three phenomena may appear in one image. Therefore, to accurately determine which phenomena the image conforms to, we have devised a selection strategy that chooses the appropriate combination of models based on the content of the image. Our approach is implemented within a framework based on the multifeature analysis. The framework considers surrounding regions, color/depth contrast, and points of interest. The selection strategy can improve the performance of the framework. A series of experiments on two recent eye-tracking datasets shows that our proposed method outperforms several state-of-the-art saliency models.
C1 [Cheng, Hao; An, Ping] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Cheng, Hao] Univ Technol Sydney, Sch Elect & Data Engn, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Zhang, Jian; Wu, Qiang] Univ Technol Sydney, Sch Comp & Commun, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [An, Ping] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Adv & Syst Applicat, Minist Educ, Shanghai 200444, Peoples R China.
C3 Shanghai University; University of Technology Sydney; University of
   Technology Sydney; Shanghai University
RP Cheng, H (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM hao.cheng@student.uts.edu.au; Jian.Zhang@uts.edu.au;
   Qiang.Wu@uts.edu.au; anping@shu.edu.cn
OI Zhang, Jian/0000-0002-7240-3541; Wu, Qiang/0000-0001-5641-2483
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], TECH REP
   [Anonymous], 2D 3D VIDEO PROCESSI
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], J EYE MOVEMENT RES
   [Anonymous], 2009, ADV NEURAL INFORM PR
   [Anonymous], 2013, 2013 47th Annual Conference on Information Sciences and Systems (CISS)
   [Anonymous], 2010, ACM T GRAPH P SIGGRA
   Awh E, 2000, J EXP PSYCHOL HUMAN, V26, P834, DOI 10.1037/0096-1523.26.2.834
   Aziz MZ, 2010, LECT NOTES COMPUT SC, V6474, P367, DOI 10.1007/978-3-642-17688-3_35
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bruce N., 2005, ADV NEURAL INFORM PR, V18, P155
   Bruce NDB, 2005, 2nd Canadian Conference on Computer and Robot Vision, Proceedings, P88, DOI 10.1109/CRV.2005.13
   Chamaret C, 2010, P SOC PHOTO-OPT INS, V7524, DOI 10.1117/12.837532
   Chang YS, 2016, TECHNOL HEALTH CARE, V24, pS231, DOI 10.3233/THC-151079
   Cheng H, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0210-5
   Cheng H, 2015, 2015 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P54
   De Silva V, 2011, IEEE T MULTIMEDIA, V13, P498, DOI 10.1109/TMM.2011.2129500
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Fan XX, 2014, INT CONF DIGIT SIG, P454, DOI 10.1109/ICDSP.2014.6900706
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100
   Frintrop S, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658355
   Häkkinen J, 2010, PROC SPIE, V7524, DOI 10.1117/12.838857
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Huang LQ, 2007, PSYCHOL REV, V114, P599, DOI 10.1037/0033-295X.114.3.599
   Huynh-Thu Q, 2011, IEEE T BROADCAST, V57, P421, DOI 10.1109/TBC.2011.2128250
   Iatsun I, 2015, SIGNAL PROCESS-IMAGE, V38, P70, DOI 10.1016/j.image.2015.05.009
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jang H., 2017, U.S. Patent, Patent No. 9674501
   Jiang QP, 2015, SIGNAL PROCESS-IMAGE, V38, P57, DOI 10.1016/j.image.2015.04.007
   Kim H, 2015, IEEE T MULTIMEDIA, V17, P2198, DOI 10.1109/TMM.2015.2493367
   Kobyshev N, 2016, INT CONF 3D VISION, P267, DOI 10.1109/3DV.2016.35
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Lang CY, 2012, IEEE T IMAGE PROCESS, V21, P1327, DOI 10.1109/TIP.2011.2169274
   Lin HY, 2015, LECT NOTES COMPUT SC, V9314, P664, DOI 10.1007/978-3-319-24075-6_64
   Lu HC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2524198
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Nauge M, 2012, PROC SPIE, V8291, DOI 10.1117/12.912089
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Palmer S.E., 1999, VISION SCI PHOTONS P, V1
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   POGGIO GF, 1984, ANNU REV NEUROSCI, V7, P379, DOI 10.1146/annurev.ne.07.030184.002115
   Rensink RA, 1997, PSYCHOL SCI, V8, P368, DOI 10.1111/j.1467-9280.1997.tb00427.x
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shao F, 2016, IEEE T MULTIMEDIA, V18, P2104, DOI 10.1109/TMM.2016.2594142
   Sheng H, 2016, INT CONF ACOUST SPEE, P1347, DOI 10.1109/ICASSP.2016.7471896
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Wang JL, 2013, IEEE T IMAGE PROCESS, V22, P2151, DOI 10.1109/TIP.2013.2246176
   Yarbus A. L., 1967, Eye Movements and Vision
   Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844
   Zhang Y, 2010, LECT NOTES COMPUT SC, V5916, P314, DOI 10.1007/978-3-642-11301-7_33
NR 56
TC 9
Z9 9
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 678
EP 689
DI 10.1109/TMM.2018.2864613
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800013
DA 2024-07-18
ER

PT J
AU Fan, ZX
   Zhao, X
   Lin, TW
   Su, HS
AF Fan, Zhaoxuan
   Zhao, Xu
   Lin, Tianwei
   Su, Haisheng
TI Attention-Based Multiview Re-Observation Fusion Network for Skeletal
   Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; multiple views; attention mechanism; skeleton; long
   short-term memory (LSTM)
ID DYNAMIC MEMORY NETWORKS; MODEL
AB Action recognition is an important and popular area in computer vision. Because of the helpfulness of action recognition of the skeleton and the development of related pose estimation techniques, action recognition based on skeleton data has drawn considerable attention and has been widely studied in recent years. In this paper, we propose an attention-based multiview re-observation fusion model for skeletal action recognition. The proposed model focuses on the factor of observation view of actions, which greatly influences action recognition. The model utilizes action information from multiple observation views to improve the recognition performance. In this method, we reobserve input skeleton data from several possible viewpoints, process these augmented observation data with a long short-term memory (LSTM) network separately, and, finally, fuse the outputs to generate the final recognition result. In the multiview fusion process, an attention mechanism is applied to regulate the fusion operation according to the helpfulness for the recognition of all views. In this way, the model can fuse information from multiple viewpoints to recognize actions and can learn to evaluate observation views to improve fusion performance. We also propose a multilayer feature attention method to improve the performance of the LSTM in our model. We utilize an attention mechanism to enhance the feature expression by finding and focusing on informative feature dimensions according to contextual action information. Moreover, we propose stacking multiple layers of attention operation in a multilayer LSTM network to further improve network performance. The final model is integrated into an end-to-end trainable network. Experiments conducted on two popular datasets, NTU RGB+D and SBU Kinect interaction, show that our model achieves state-of-the-art performance.
C1 [Fan, Zhaoxuan; Zhao, Xu; Lin, Tianwei; Su, Haisheng] Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200000, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhao, X (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200000, Peoples R China.
EM fzx92@sjtu.edu.cn; zhaoxu@sjtu.edu.cn; wzmsltw@sjtu.edu.cn;
   suhaisheng@sjtu.edu.cn
OI Su, Haisheng/0000-0002-4228-7439
FU National Natural Science Foundation of China [61673269, 61273285];
   Cooperative Medianet Innovation Center
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61673269 and 61273285 and in part by
   the Cooperative Medianet Innovation Center. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Abdulmotaleb El Saddik.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], ICLR
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], 2017, INT JOINT C ART INT
   [Anonymous], P COMP VIS PATT REC
   Aydin R, 2014, IN C IND ENG ENG MAN, P1, DOI 10.1109/IEEM.2014.7058588
   Chorowski J, 2015, ADV NEUR IN, V28
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Du Y, 2016, IEEE T IMAGE PROCESS, V25, P3010, DOI 10.1109/TIP.2016.2552404
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772
   Gowayyed M.A., 2013, Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, P1351
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Hussein, 2013, INT JOINT C ART INT
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Ke QH, 2017, IEEE SIGNAL PROC LET, V24, P731, DOI 10.1109/LSP.2017.2690339
   Kumar A, 2016, PR MACH LEARN RES, V48
   Li CL, 2018, IEEE T IMAGE PROCESS, V27, P3657, DOI 10.1109/TIP.2018.2815744
   Li CK, 2017, IEEE SIGNAL PROC LET, V24, P624, DOI 10.1109/LSP.2017.2678539
   Li W, 2017, IEEE I CONF COMP VIS, P1453, DOI 10.1109/ICCV.2017.161
   Li WB, 2015, IEEE I CONF COMP VIS, P4444, DOI 10.1109/ICCV.2015.505
   Li YH, 2016, LECT NOTES COMPUT SC, V9911, P203, DOI 10.1007/978-3-319-46478-7_13
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sharma S, 2016, ACTION RECOGNITION U
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   Weng JW, 2019, IEEE T CIRC SYST VID, V29, P1077, DOI 10.1109/TCSVT.2018.2818151
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu K., 2015, COMPUTER SCI, P2048
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 47
TC 49
Z9 51
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 363
EP 374
DI 10.1109/TMM.2018.2859620
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400008
DA 2024-07-18
ER

PT J
AU Chen, K
   Tao, WB
AF Chen, Kai
   Tao, Wenbing
TI Learning Linear Regression via Single-Convolutional Layer for Visual
   Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual tracking; linear regression; gradient descent; convolutional
   neural networks
ID FILTER
AB Learning a large-scale regression model has proven to be one of the most successful approaches for visual tracking as in recent correlation filter (CF)- based trackers. Different from the conventional CF-based algorithms in which the regression model is solved based on circulant training samples, we propose learning linear regression models via a single-convolutional layer with the gradient descent (GD) technique. In our convolution-based approach, the samples are cropped from an image in a sliding window manner rather than being circularly shifted from one base sample. As a result, the abundant background context in the images can be fully exploited to learn a robust tracker. The proposed tracker is based on two independent regression models: a holistic regression model and a texture regression model. The holistic regression model is trained based on the entire object patch to predict the object location, whereas the texture regression model is trained based on the local object textures. The foreground map outputted by the texture regression model is not only helpful to boost the location prediction in the case of large variations, but is also an important clue for estimating the object size. With the foreground map outputted by the texture regression model, we are able to estimate the object size by optimizing a novel objective function based on object-background contrast. Our extensive experiments on four popular visual tracking datasets OTB-50, OTB-100, VOT-2016, and Temple Color have proved that the proposed algorithm achieves outstanding performance and outperforms most CF-based trackers.
C1 [Chen, Kai; Tao, Wenbing] Huazhong Univ Sci & Technol, Sch Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology
RP Tao, WB (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Hubei, Peoples R China.
EM chkap@hust.edu.cn; wenbingtao@hust.edu.cn
OI Chen, Kai/0000-0001-8436-6533; Tao, Wenbing/0000-0003-3284-864X
FU National Natural Science Foundation of China [61772213, 91748204]; Wuhan
   Science and Technology Plan [2017010201010121]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61772213 and Grant 91748204 and in part by the Wuhan
   Science and Technology Plan under Grant 2017010201010121. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Lei Zhang.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2016, CVPR
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen K, 2018, IEEE T IMAGE PROCESS, V27, P3611, DOI 10.1109/TIP.2018.2819362
   Chen K, 2017, INFORM SCIENCES, V394, P232, DOI 10.1016/j.ins.2017.02.012
   Cheng K, 2021, IEEE T BIG DATA, V7, P689, DOI 10.1109/TBDATA.2017.2707552
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Gao JY, 2018, IEEE T IMAGE PROCESS, V27, P3074, DOI 10.1109/TIP.2018.2813166
   Gao JY, 2017, IEEE T IMAGE PROCESS, V26, P1845, DOI 10.1109/TIP.2017.2656628
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   King DB, 2015, ACS SYM SER, V1214, P1
   Kristan M, 2016, LECT NOTES COMPUT SC, V9914, P777, DOI 10.1007/978-3-319-48881-3_54
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV
   Song HH, 2017, ELECTRON LETT, V53, P20, DOI 10.1049/el.2016.3011
   Song HH, 2014, ELECTRON LETT, V50, P1931, DOI 10.1049/el.2014.1911
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Wang LJ, 2016, PROC CVPR IEEE, P1373, DOI 10.1109/CVPR.2016.153
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang TZ, 2018, IEEE T IMAGE PROCESS, V27, P2676, DOI 10.1109/TIP.2017.2781304
   Zhang T, 2019, INT J COMPUT MATH, V96, P594, DOI 10.1080/00207160.2018.1455092
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 49
TC 28
Z9 29
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 86
EP 97
DI 10.1109/TMM.2018.2846405
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700008
DA 2024-07-18
ER

PT J
AU Silva, DF
   Yeh, CCM
   Zhu, Y
   Batista, GEAPA
   Keogh, E
AF Silva, Diego F.
   Yeh, Chin-Chia M.
   Zhu, Yan
   Batista, Gustavo E. A. P. A.
   Keogh, Eamonn
TI Fast Similarity Matrix Profile for Music Analysis and Exploration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data analysis; distance measurement; music information retrieval
ID ALIGNMENT; FEATURES; TIME
AB Most algorithms for music data mining and retrieval analyze the similarity between feature sets extracted from the raw audio. A conventional approach to assess similarities within or between recordings is to create similarity matrices. However, this method requires quadratic space for each comparison and typically requires costly post-processing of the matrix. We have recently proposed SiMPle, a powerful representation based on subsequence similarity join, which is applicable in several music analysis tasks. In this paper, we propose SiMPle-Fast a highly efficient method for exact computation of SiMPle that is up to one order of magnitude faster than SiMPle. Furthermore, we demonstrate the utility of SiMPle-Fast in cover music recognition and thumbnailing tasks and show that our method is significantly faster and more accurate than the state-of-the-art.
C1 [Silva, Diego F.; Batista, Gustavo E. A. P. A.] Univ Sao Paulo, Inst Ciencias Matemat & Computacao, BR-13566590 Sao Paulo, Brazil.
   [Silva, Diego F.] Univ Fed Sao Carlos, Dept Computacao, BR-13565905 Sao Paulo, Brazil.
   [Yeh, Chin-Chia M.; Zhu, Yan; Keogh, Eamonn] Univ Calif Riverside, Dept Comp Sci & Engn, Riverside, CA 92521 USA.
C3 Universidade de Sao Paulo; Universidade Federal de Sao Carlos;
   University of California System; University of California Riverside
RP Silva, DF (corresponding author), Univ Sao Paulo, Inst Ciencias Matemat & Computacao, BR-13566590 Sao Paulo, Brazil.
EM diego.fsilva@gmail.com; myeh003@ucr.edu; yzhu015@ucr.edu;
   gbatista@icmc.usp.br; eamonn@cs.ucr.edu
RI Batista, Gustavo EAPA/E-9847-2011; Silva, Diego/GRF-5446-2022; Yeh,
   Michael/J-1738-2019; Furtado Silva, Diego/K-3306-2016
OI Yeh, Michael/0000-0002-9807-2963; Furtado Silva,
   Diego/0000-0002-5184-9413
FU Sao Paulo Research Foundation [2013/26151-5, 2016/04986-6]; Conselho
   Nacional de Desenvolvimento Cientifico e Tecnologico [446330/2014-0,
   306631/2016-4]; National Science Foundation [IIS-1161997 II]; United
   States Agency for International Development [AID-OAA-F-16-00072];
   Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)
   [13/26151-5] Funding Source: FAPESP
FX This work was supported in part by the Sao Paulo Research Foundation
   under Grants #2013/26151-5 and #2016/04986-6, in part by the Conselho
   Nacional de Desenvolvimento Cientifico e Tecnologico under Grants
   446330/2014-0 and 306631/2016-4, in part by the National Science
   Foundation under Grant IIS-1161997 II, and in part by the United States
   Agency for International Development under Grant AID-OAA-F-16-00072.
   This paper was presented in part at the 17th International Society for
   Music Information Retrieval Conference, New York, NY, USA, August 2016.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Martha Larson.
CR Bartsch MA, 2005, IEEE T MULTIMEDIA, V7, P96, DOI 10.1109/TMM.2004.840597
   Bello JP, 2011, IEEE T AUDIO SPEECH, V19, P2013, DOI 10.1109/TASL.2011.2108287
   Chen N, 2018, MULTIMED TOOLS APPL, V77, P2629, DOI 10.1007/s11042-017-4456-9
   Foote J, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P77, DOI 10.1145/319463.319472
   Gomez E, 2006, THESIS
   Grosche P., 2012, Proc of the 13th International Society of Music Information Retrieval, number Ismir, P55
   HaitaoWu Zhenqian Feng, 2010, P 6 INT C CO NEXT 10, DOI DOI 10.1145/1921168.1921186
   Li T, 2009, IEEE T MULTIMEDIA, V11, P477, DOI 10.1109/TMM.2009.2012942
   Liu NH, 2014, IEEE T MULTIMEDIA, V16, P1407, DOI 10.1109/TMM.2014.2311326
   Mueen A., FASTEST SIMILARITY S
   Muller M., 2007, INFORM RETRIEVAL MUS, P70
   Muller M, 2005, P 6 INT C MUSIC INFO, P288
   Salvadora S, 2007, INTELL DATA ANAL, V11, P561, DOI 10.3233/IDA-2007-11508
   Sapp C., THE MAZURKA PROJECT
   Serra J., 2008, IEEE CS Conference on The Use of Symbols to Represent Music and Multimedia Objects, P45
   Serrà J, 2008, IEEE T AUDIO SPEECH, V16, P1138, DOI 10.1109/TASL.2008.924595
   Serrà J, 2014, IEEE T MULTIMEDIA, V16, P1229, DOI 10.1109/TMM.2014.2310701
   Serrà J, 2009, NEW J PHYS, V11, DOI 10.1088/1367-2630/11/9/093017
   Silva D. F., 2015, P 16 INT SOC MUS INF, P441
   Silva D. F., 2013, P 14 INT SOC MUS INF, P95
   Silva DF, 2016, P INT SOC MUS INF RE, P23
   Tsai WH, 2008, J INF SCI ENG, V24, P1669
   Wang SY, 2016, IEEE-ACM T AUDIO SPE, V24, P2132, DOI 10.1109/TASLP.2016.2598318
   Zhu Y, 2016, IEEE DATA MINING, P739, DOI [10.1109/ICDM.2016.126, 10.1109/ICDM.2016.0085]
NR 24
TC 17
Z9 18
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 29
EP 38
DI 10.1109/TMM.2018.2849563
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700003
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Mao, AL
   Huo, SW
   Lei, JJ
   Kung, SY
AF Zhou, Yuan
   Mao, Ailing
   Huo, Shuwei
   Lei, Jianjun
   Kung, Sun-Yuan
TI Salient Object Detection via Fuzzy Theory and Object-Level Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency detection; object proposal; fuzzy sets
ID DRIVEN
AB This paper proposes a bottom-up saliency detection method via effective integration of regional saliency measure and object-level information using fuzzy theory. First, we generate an initial saliency map by fusing multiple prior maps. Second, to emphasize the object-level concept of saliency, we further generate many object proposals of the input image. A fuzzy set theory is then applied to measure the objectness score of the object proposals and integrate them into an objectness map. Third, an optimization framework is proposed to effectively fuse various prior saliency cues and object-level information to produce a clean and uniform saliency map as well as to maintain the salient object completeness. Experimental studies in several benchmark datasets confirmed the superiority of the proposed method over state-of-the-art saliency detection methods.
C1 [Zhou, Yuan; Mao, Ailing; Huo, Shuwei; Lei, Jianjun] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Kung, Sun-Yuan] Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA.
C3 Tianjin University; Princeton University
RP Zhou, Y (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM zhouyuan@tju.edu.cn; maoailing@tju.edu.cn; huosw@tju.edu.cn;
   jjlei@tju.edu.cn; kung@princeton.edu
RI Huo, Shuwei/JAO-1073-2023; Lei, Jianjun/P-2539-2018
OI Huo, Shuwei/0000-0002-7290-7838
FU National Natural Science Foundation of China [61571326, 61520106002];
   Natural Science Foundation of Tianjin [16JCQNJC00900]; Brandeis Program
   of the Defense Advanced Research Project Agency and Space and Naval
   Warfare System Center Pacific [66001-15-C-4068]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61571326, Grant 61520106002, in part by
   Natural Science Foundation of Tianjin under Grant 16JCQNJC00900, and in
   part by the Brandeis Program of the Defense Advanced Research Project
   Agency and Space and Naval Warfare System Center Pacific under Contract
   66001-15-C-4068. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Raouf Hamzaoui.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Huo SW, 2018, IEEE T MULTIMEDIA, V20, P1350, DOI 10.1109/TMM.2017.2769801
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Korytkowski M, 2016, INFORM SCIENCES, V327, P175, DOI 10.1016/j.ins.2015.08.030
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lindblad J, 2014, IEEE T IMAGE PROCESS, V23, P126, DOI 10.1109/TIP.2013.2286904
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1937, DOI 10.1109/TIP.2014.2307434
   Lu HC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2524198
   Lu HC, 2017, IEEE T IMAGE PROCESS, V26, P414, DOI 10.1109/TIP.2016.2627804
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   MAI L, 2013, PROC CVPR IEEE, P1131, DOI DOI 10.1109/CVPR.2013.150
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Navalpakkam V., 2006, P IEEE C COMPUTER VI, P2049
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Ren JR, 2018, J VIS COMMUN IMAGE R, V50, P227, DOI 10.1016/j.jvcir.2017.12.002
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Sun JG, 2015, IEEE T IMAGE PROCESS, V24, P1639, DOI 10.1109/TIP.2015.2403241
   Tasse FP, 2015, IEEE I CONF COMP VIS, P163, DOI 10.1109/ICCV.2015.27
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Werro N, 2015, FUZZ MANAG METH, P1, DOI 10.1007/978-3-319-15970-6
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yang JM, 2017, IEEE T PATTERN ANAL, V39, P576, DOI 10.1109/TPAMI.2016.2547384
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang LH, 2017, IEEE T PATTERN ANAL, V39, P1892, DOI 10.1109/TPAMI.2016.2609426
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhou XF, 2017, MULTIMED TOOLS APPL, V76, P23187, DOI 10.1007/s11042-016-4093-8
   Zhou Y, 2019, IEEE T CYBERNETICS, V49, P1173, DOI 10.1109/TCYB.2018.2793278
   Zhou Y, 2017, IEEE T GEOSCI REMOTE, V55, P5997, DOI 10.1109/TGRS.2017.2718728
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 52
TC 40
Z9 40
U1 2
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 74
EP 85
DI 10.1109/TMM.2018.2845667
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700007
DA 2024-07-18
ER

PT J
AU Freitas, PG
   Akamine, WYL
   Farias, MCQ
AF Freitas, Pedro Garcia
   Akamine, Welington Y. L.
   Farias, Mylene C. Q.
TI No-Reference Image Quality Assessment Using Orthogonal Color Planes
   Patterns
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment; pattern recognition; machine learning;
   orthogonal color plane binary patterns
AB This paper proposes a new general-purpose no-reference image quality assessment (NR-IQA) method based on color texture analysis. Specifically, the proposed method uses the statistics of the orthogonal color planes pattern (OCPP) descriptor to characterize image quality. The OCPP descriptor, proposed in this paper, is an extension of the local binary pattern operator that incorporates color information. To make NR-IQA methods more generic, that is, more sensitivity to different types of degradation (e.g., color and contrast degradation), it is important to take into consideration the color information. In the proposed NR-IQA method, we use the statistics of the OCPP descriptor as an input vector to a regression algorithm, which models the nonlinear relationship between the OCPP statistics and the subjective opinion scores. Experimental results show that proposed IQA method is quite efficient when compared to popular state-of-the-art
   NR-IQA methods.
C1 [Freitas, Pedro Garcia] Univ Brasilia, Dept Comp Sci, BR-70910900 Brasilia, DF, Brazil.
   [Akamine, Welington Y. L.; Farias, Mylene C. Q.] Univ Brasilia, Dept Elect Engn, BR-70853060 Brasilia, DF, Brazil.
C3 Universidade de Brasilia; Universidade de Brasilia
RP Freitas, PG (corresponding author), Univ Brasilia, Dept Comp Sci, BR-70910900 Brasilia, DF, Brazil.
EM sawp@sawp.com.br; welingtonylakamine@gmail.com; mylene@ieee.org
RI Farias, Mylene/C-4900-2015; Garcia Freitas, Pedro/T-5287-2018
OI Farias, Mylene/0000-0002-1957-9943; Garcia Freitas,
   Pedro/0000-0003-0866-658X
CR [Anonymous], P SPIE
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], MULTIMEDIA QUALITY
   Brahnam S., 2014, Local Binary Patterns: New Variants and Applications, P1
   Freitas PG, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Golestaneh S. Alireza, 2014, IEEE Signal Processing Letters, V21, P155, DOI 10.1109/LSP.2013.2296038
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Gu K, 2015, IEEE T CIRC SYST VID, V25, P1480, DOI 10.1109/TCSVT.2014.2372392
   Gu K, 2013, IEEE IMAGE PROC, P383, DOI 10.1109/ICIP.2013.6738079
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Li LD, 2016, NEUROCOMPUTING, V177, P572, DOI 10.1016/j.neucom.2015.11.063
   Li QH, 2017, NEUROCOMPUTING, V236, P93, DOI 10.1016/j.neucom.2016.09.105
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P494, DOI 10.1016/j.image.2014.02.004
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Ortiz-Jaramillo B., 2016, Proc. 8th Int. Conf. on Qual. Multim. Exp, P1
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Seshadrinathan K, 2011, MULTIMED TOOLS APPL, V51, P163, DOI 10.1007/s11042-010-0625-9
   Tang HX, 2011, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2011.5995446
   Wu JJ, 2014, IEEE SIGNAL PROC LET, V21, P437, DOI 10.1109/LSP.2014.2304714
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhang M, 2015, IEEE SIGNAL PROC LET, V22, P207, DOI 10.1109/LSP.2014.2326399
   Zhang Y, 2016, AER ADV ENG RES, V77, P241
NR 27
TC 26
Z9 27
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3353
EP 3360
DI 10.1109/TMM.2018.2839529
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600014
DA 2024-07-18
ER

PT J
AU Niwa, K
   Hioka, Y
   Uematsu, H
AF Niwa, Kenta
   Hioka, Yusuke
   Uematsu, Hisashi
TI Efficient Audio Rendering Using Angular Region-Wise Source Enhancement
   for 360° Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Virtual reality (VR); head-mounted display (HMD); binaural synthesizing;
   auditory localization; angular region-wise source separation; microphone
   array
AB In virtual reality, 360 degrees video services provided through head-mounted displays or smartphones are widely available. Among these, some state-of-the-art devices are able to render varying auditory location of an object perceived by the user when the visual location of the object in the video moves along with the change of the user's looking direction. Nevertheless, an acoustic immersion technology that generates binaural sound to maintain a good match between the auditory and visual localization of an object in 360 degrees video has not been studied sufficiently. This study focuses on an approach that synthesizes semibinaural sound being composed of virtual sources located in each angular region and the representative head related transfer functions of each angular region. To minimize the calculation cost on audio rendering and to reduce latency in downloading data from servers, the number of angular regions should be reduced while maintaining a good match between the auditory and visual localization of an object. In this paper, we investigate the minimum number of angular regions at which it is possible to maintain a good match by conducting subjective tests using a 360 degrees video viewing system composed of virtual images and sound sources. From the subjective tests, it was confirmed that the acoustic field should be divided into more than six equispaced angular regions so as to achieve natural auditory localization that matches an object's location in 360 degrees video.
C1 [Niwa, Kenta; Uematsu, Hisashi] NTT Corp, NTT Media Intelligence Labs, Tokyo 1808585, Japan.
   [Hioka, Yusuke] Univ Auckland, Dept Mech Engn, Auckland 1010, New Zealand.
C3 Nippon Telegraph & Telephone Corporation; University of Auckland
RP Niwa, K (corresponding author), NTT Corp, NTT Media Intelligence Labs, Tokyo 1808585, Japan.
EM niwa.kenta@lab.ntt.co.jp; yusuke.hioka@ieee.org;
   uematsu.hisashi@lab.ntt.co.jp
RI Hioka, Yusuke/AAU-5569-2021
OI Hioka, Yusuke/0000-0003-3380-9677
CR Alais D, 2004, CURR BIOL, V14, P257, DOI 10.1016/j.cub.2004.01.029
   [Anonymous], P 141 AUD ENG SOC CO
   [Anonymous], P INT WORKSH AC SIGN
   [Anonymous], MICROPHONE ARRAYS SI
   [Anonymous], P 24 AUD ENG SOC C M
   [Anonymous], THESIS
   [Anonymous], P 41 AUD ENG SOC C
   [Anonymous], 2003, AUDIO ENG SOC CONVEN
   [Anonymous], 1996, FORCE TOUCH FEEDBACK
   [Anonymous], P 30 AUD ENG SOC C
   [Anonymous], P 6 AUSTR REG CONV A
   [Anonymous], P 7 W PAC REG AC C
   [Anonymous], P INT WORKSH AC SIGN
   Begault D. R., 1994, 3 SOUND VIRTUAL REAL
   Blauert J., 1996, Spatial Hearing: The Psychophysics of Human Sound Localization
   Burdea G. C., 2003, Virtual reality technology
   COOPER DH, 1972, J AUDIO ENG SOC, V20, P346
   Dimoulas CA, 2016, IEEE T MULTIMEDIA, V18, P1969, DOI 10.1109/TMM.2016.2594148
   FROST OL, 1972, PR INST ELECTR ELECT, V60, P926, DOI 10.1109/PROC.1972.8817
   GERZON MA, 1973, J AUDIO ENG SOC, V21, P2
   Haustein B.-G., 1970, Hochfrequenztechnik und Elektroakustik, V79, P96
   Heller F, 2016, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI'16), P278, DOI 10.1145/2935334.2935365
   Hioka Y, 2013, IEEE T AUDIO SPEECH, V21, P1240, DOI 10.1109/TASL.2013.2248715
   Hioka Y, 2012, IEEE T CONSUM ELECTR, V58, P1403, DOI 10.1109/TCE.2012.6415013
   Iida K, 2014, J ACOUST SOC AM, V136, P317, DOI 10.1121/1.4880856
   Johnson D.H., 1993, Array processing: Concepts and techniques
   Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394
   Middlebrooks JC, 1999, J ACOUST SOC AM, V106, P1480, DOI 10.1121/1.427176
   Morimoto M., 1980, Journal of the Acoustical Society of Japan (E), V1, P167, DOI 10.1250/ast.1.167
   Narumi T, 2011, P IEEE VIRT REAL ANN, P127, DOI 10.1109/VR.2011.5759450
   Niwa K, 2016, INT CONF ACOUST SPEE, P2852, DOI 10.1109/ICASSP.2016.7472198
   Niwa K, 2008, INT CONF ACOUST SPEE, P181, DOI 10.1109/ICASSP.2008.4517576
   Noisternig M, 2003, VECIMS'03: 2003 IEEE INTERNATIONAL SYMPOSIUM ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P174
   Ochi D, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P737, DOI 10.1145/2733373.2807963
   Robles-De-La-Torre G, 2006, IEEE MULTIMEDIA, V13, P24, DOI 10.1109/MMUL.2006.69
   Williams E.G., 1999, Fourier Acoustics
   Zelinski R., 1988, ICASSP 88: 1988 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.88CH2561-9), P2578, DOI 10.1109/ICASSP.1988.197172
NR 37
TC 8
Z9 8
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2871
EP 2881
DI 10.1109/TMM.2018.2829187
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800001
OA Green Published
DA 2024-07-18
ER

PT J
AU Torres, C
   Fried, JC
   Rose, K
   Manjunath, BS
AF Torres, Carlos
   Fried, Jeffrey C.
   Rose, Kenneth
   Manjunath, B. S.
TI A Multiview Multimodal System for Monitoring Patient Sleep
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Healthcare; sleep poses; multimodal sensor network; ICU monitoring;
   patient motion analysis; summarization; hidden markov models;
   time-series motion interference; MASH
ID ACTION RECOGNITION; RETRIEVAL; MOTION
AB Clinical observations indicate that during critical care at the hospitals, a patient's sleep positioning and motion have a significant effect on recovery rate. Unfortunately, there is no formal medical protocol to record, quantify, and analyze motion of patients. There are very few clinical studies that use manual analysis of sleep poses and motion recordings to support medical benefits of patient positioning and motion monitoring. Manual processes do not scale, are prone to human errors, and put strain on an already taxed healthcare workforce. This study introduces multimodal, multiview motion analysis and summarization for healthcare (MASH). MASH is an autonomous system, which addresses these issues by monitoring healthcare environments and enabling the recording and analysis of patient sleep-pose patterns. MASH uses three RGB-D cameras to monitor patients in a medical intensive care unit (ICU) room. The proposed algorithms estimate pose direction at different temporal resolutions and use keyframes to efficiently represent pose transition dynamics. MASH combines deep features computed from the data with a modified version of hidden Markov model (HMM) to flexibly model pose duration and summarize patient motion. The performance is evaluated in ideal (BC: bright and clear/occlusion-free) and natural (DO: dark and occluded) scenarios at two motion resolutions and in two environments: a mock-up and a medical ICU. The usage of deep features is evaluated and their performance compared with engineered features. Experimental results using deep features in DO scenes increase performance from 86.7% to 93.6%, while matching the classification performance of engineered features in BC scenes. The performance of MASH is compared with HMM and C3D. The overall overtime tracing and summarization error rate across all methods increased when transitioning from the mock-up to the the medical ICU data. The proposed keyframe estimation helps achieve a 78% transition classification accuracy.
C1 [Torres, Carlos; Rose, Kenneth; Manjunath, B. S.] Univ Calif Santa Barbara, Elect & Comp Engn Dept, Santa Barbara, CA 93106 USA.
   [Fried, Jeffrey C.] Santa Barbara Cottage Hosp, Med ICU, Santa Barbara, CA 93105 USA.
C3 University of California System; University of California Santa Barbara;
   Santa Barbara Cottage Hospital
RP Torres, C (corresponding author), Univ Calif Santa Barbara, Elect & Comp Engn Dept, Santa Barbara, CA 93106 USA.
EM carlostorres@ece.ucsb.edu; jfried@sbch.org; rose@ece.ucsb.edu;
   manj@ece.ucsb.edu
RI Manjunath, B S/AAM-8190-2020; Torres, Carlos/KDO-9548-2024; Torres,
   Carlos/GRS-7240-2022
OI Manjunath, B S/0000-0003-2804-3611; 
FU Army Research Laboratory [W911NF-09-2-0053]
FX This work was supported in part by the Army Research Laboratory under
   Cooperative Agreement Number W911NF-09-2-0053 (the ARL Network Science
   CTA).
CR [Anonymous], BBC NEWS
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2015, Open Source Computer Vision Library
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Ben Amor B, 2016, IEEE T PATTERN ANAL, V38, P1, DOI 10.1109/TPAMI.2015.2439257
   Bihari S, 2012, J CLIN SLEEP MED, V8, P301, DOI 10.5664/jcsm.1920
   Chéron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368
   De Leo C, 2014, ACM T SENSOR NETWORK, V10, DOI 10.1145/2530285
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   GIRAUD T, 1993, CRIT CARE MED, V21, P40, DOI 10.1097/00003246-199301000-00011
   Grimm T, 2016, INT C PATT RECOG, P319, DOI 10.1109/ICPR.2016.7899653
   Harvard School of Medicine, 2016, FIND TOP LIN OPP BOT
   Hoque E., 2012, 2012 6th International Conference on Pervasive Computing Technologies for Healthcare, P139, DOI 10.4108/icst.pervasivehealth.2012.248600
   Liu AA, 2017, IEEE T CYBERNETICS, V47, P1781, DOI 10.1109/TCYB.2016.2582918
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu L, 2016, IEEE T CYBERNETICS, V46, P158, DOI 10.1109/TCYB.2015.2399172
   Liu PY, 2017, IEEE INT CON MULTI, P97, DOI 10.1109/ICME.2017.8019355
   Liu W, 2013, IEEE T CYBERNETICS, V43, P1442, DOI 10.1109/TCYB.2013.2272636
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Morris PE, 2007, CRIT CARE CLIN, V23, P1, DOI 10.1016/j.ccc.2006.11.003
   Obdrzalek Stepan, 2012, Stud Health Technol Inform, V173, P320
   Panda R, 2017, IEEE T MULTIMEDIA, V19, P2010, DOI 10.1109/TMM.2017.2708981
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Raman S, 2011, POSTCOLON LIT STUD, P1
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K., 2014, 14091556 ARXIV
   Song J, 2017, IEEE T MULTIMEDIA, V19, P1965, DOI 10.1109/TMM.2017.2733638
   Song Y, 2011, IEEE T CIRC SYST VID, V21, P1193, DOI 10.1109/TCSVT.2011.2130230
   Soran B, 2015, IEEE I CONF COMP VIS, P4669, DOI 10.1109/ICCV.2015.530
   Sunderrajan S, 2016, IEEE T MULTIMEDIA, V18, P51, DOI 10.1109/TMM.2015.2496139
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   The Raspberry Pi Foundation, 2017, RASPH PI 3 MOD B
   Torres C, 2016, P IEEE WINT C APPL C, P1
   Torres C, 2016, LECT NOTES COMPUT SC, V9914, P178, DOI 10.1007/978-3-319-48881-3_13
   Torres C, 2015, LECT NOTES COMPUT SC, V9163, P56, DOI 10.1007/978-3-319-20904-3_6
   Ulutan O., 2018, P IEEE WINT C APPL C
   Veeriah V., 2015, P IEEE INT C COMP VI, P40
   Weimin Huang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4336, DOI 10.1109/ICPR.2010.1054
   Weinhouse GL, 2006, SLEEP, V29, P707, DOI 10.1093/sleep/29.5.707
   Wu Chun, 2010, Proceedings 2010 International Conference on Challenges in Environmental Science and Computer Engineering (CESCE 2010), P142, DOI 10.1109/CESCE.2010.72
   Wu JX, 2016, IEEE T CYBERNETICS, V46, P2978, DOI 10.1109/TCYB.2015.2493538
   Xu JJ, 2013, IEEE T MULTIMEDIA, V15, P2046, DOI 10.1109/TMM.2013.2281019
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Ye M., 2013, TIME OF FLIGHT DEPTH
   Zhen XT, 2017, IEEE T MULTIMEDIA, V19, P2056, DOI 10.1109/TMM.2017.2700204
NR 48
TC 21
Z9 21
U1 4
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3057
EP 3068
DI 10.1109/TMM.2018.2829162
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800015
OA Green Published
DA 2024-07-18
ER

PT J
AU Hu, P
   Wang, G
   Tan, YP
AF Hu, Ping
   Wang, Gang
   Tan, Yap-Peng
TI Recurrent Spatial Pyramid CNN for Optical Flow Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optical flow estimation; convolutional neural network; coarse-to-fine
   refinement
ID SEGMENTATION; FIELDS; COLOR
AB Optical flow estimation plays an important role in many multimedia and computer vision tasks. Although great progress has been made in applying convolutional neural networks (CNNs) to estimate optical flow in recent works, it is still difficult for CNNs to generate optical flow with the desired effectiveness and efficiency. Compared to CNN-based methods, conventional variational methods normally perform to optimize an energy function and produce optical flow with more precise details. Inspired by the effectiveness of variational methods and deep CNNs, we propose a recurrent spatial pyramid (RecSPy) network for optical flow estimation. To deal with large displacements and to decrease the number of parameters, we formulate the spatial pyramid as a recurrent process, and adopt a CNN to refine optical flow at each spatial scale. Furthermore, to improve the results with more precise details, we propose an energy function that encodes structure and constancy constraints to help refine the optical flow at each spatial scale. The combination of the proposed RecSPy network and the proposed energy-based refinement enables our system to estimate optical flow effectively and efficiently. Experimental results on the benchmarks validate the effectiveness and efficiency of the proposed method.
C1 [Hu, Ping; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Wang, Gang] Alibaba Grp, Hangzhou 310052, Zhejiang, Peoples R China.
C3 Nanyang Technological University; Alibaba Group
RP Hu, P (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM phu005@ntu.edu.sg; gang-wang6@gmail.com; eyptan@ntu.edu.sg
RI Tan, Yap-Peng/A-5158-2011
FU National Research Foundation, Singapore; Infocomm Media Development
   Authority, Singapore
FX This research was carried out at the Rapid-Rich Object Search (ROSE) Lab
   at the Nanyang Technological University, Singapore. The ROSE Lab is
   supported by the National Research Foundation, Singapore, and the
   Infocomm Media Development Authority, Singapore. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Fatih Porikli.
CR [Anonymous], 2016, ENET DEEP NEURAL NET
   [Anonymous], 2017, P IEEE C COMPUTER VI
   [Anonymous], 1971, ITERATIVE SOLUTION L
   Bai M, 2016, LECT NOTES COMPUT SC, V9910, P154, DOI 10.1007/978-3-319-46466-4_10
   Bailer C, 2017, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2017.290
   Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457
   Bao LC, 2014, PROC CVPR IEEE, P3534, DOI 10.1109/CVPR.2014.452
   Black M. J., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P296, DOI 10.1109/CVPR.1991.139705
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Chen QF, 2016, PROC CVPR IEEE, P4706, DOI 10.1109/CVPR.2016.509
   Chen ZY, 2013, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2013.316
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Gadot D, 2016, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2016.459
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Glocker B., 2008, IEEE COMPUTER VISION, P1, DOI [DOI 10.1109/CVPR.2008.4587562, 10.1109/CVPR.2008.4587562]
   He KM, 2012, PROC CVPR IEEE, P111, DOI 10.1109/CVPR.2012.6247665
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu YL, 2016, PROC CVPR IEEE, P5704, DOI 10.1109/CVPR.2016.615
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jaderberg M., 2015, ICLR, DOI DOI 10.48550/ARXIV.1506.02025
   Kingma D. P., 2014, arXiv
   Korman S, 2011, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2011.6126421
   Kroeger T, 2016, LECT NOTES COMPUT SC, V9908, P471, DOI 10.1007/978-3-319-46493-0_29
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Lu SP, 2015, IEEE T MULTIMEDIA, V17, P577, DOI 10.1109/TMM.2015.2412879
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Memin E, 1998, IEEE T IMAGE PROCESS, V7, P703, DOI 10.1109/83.668027
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Mileva Y, 2007, LECT NOTES COMPUT SC, V4713, P152
   Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   Qi GJ, 2015, IEEE T MULTIMEDIA, V17, P1873, DOI 10.1109/TMM.2015.2485538
   Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Schuster T, 2017, PROC CVPR IEEE, P6921, DOI 10.1109/CVPR.2017.732
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Su CW, 2007, IEEE T MULTIMEDIA, V9, P1193, DOI 10.1109/TMM.2007.902875
   Sun DQ, 2008, LECT NOTES COMPUT SC, V5304, P83
   Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wan YL, 2014, IEEE T MULTIMEDIA, V16, P637, DOI 10.1109/TMM.2014.2299515
   Wedel A, 2009, IEEE I CONF COMP VIS, P1663, DOI 10.1109/ICCV.2009.5459375
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wu GL, 2016, IEEE T MULTIMEDIA, V18, P978, DOI 10.1109/TMM.2016.2545401
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Yang JL, 2016, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2016.157
   Yang JL, 2015, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2015.7298704
   Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1
   Zhu Yi, 2017, ARXIV170202295
   Zweig S, 2017, PROC CVPR IEEE, P6363, DOI 10.1109/CVPR.2017.674
NR 59
TC 36
Z9 40
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2814
EP 2823
DI 10.1109/TMM.2018.2815784
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000022
DA 2024-07-18
ER

PT J
AU Yue, GH
   Hou, CP
   Gu, K
   Ling, N
   Li, BC
AF Yue, Guanghui
   Hou, Chunping
   Gu, Ke
   Ling, Nam
   Li, Beichen
TI Analysis of Structural Characteristics for Quality Assessment of
   Multiply Distorted Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment (IQA); local binary pattern (LBP); multiple
   distortions; no reference (NR)
ID SUPPORT VECTOR MACHINES; PERCEPTUAL IMAGE; RANDOM SUBSPACE; STATISTICS
AB Perceptual image quality assessment (IQA) plays an important role in numerous applications, including image restoration, compression, enhancement, and others. Although many works have been conducted on individually distorted IQA problems and have achieved encouraging results, few studies have been conducted on multiple distorted (MD) IQA problems. Thus, limited progress has been made. In this paper, we propose a novel no reference image quality assessment (NR-IQA) method, named improved multiscale local binary pattern (IMLBP), for addressing multiply distorted IQA problems. The image structures are sensitive to image distortions, which motivates us to utilize the structural characteristics for overall image quality prediction. We improved the local binary pattern (LBP) by considering the human visual mechanism to better extract the structural information. The IMLBP contains two parts, the LBP and the radius difference LBP (DLBP). The DLBP reflects the values' changes in the radial direction. Specifically, when the radius value is small, the proposed descriptor is computed to represent microstructural information. Conversely, it represents macrostructural information when the radius becomes large. Moreover, to better mimick the human visual mechanism, the IMLBP is computed with the multiscale strategy and the operation is based on a patch unit whose size is proportional to the radius value. The frequency histogram of feature maps is transformed to feature vectors. Subsequently, a predictable function trained by the support vector regression is used to infer the overall quality score. Experimental results show that the proposed method outperforms most state-of-the-art IQA metrics on publicly available multiply distorted image databases.
C1 [Yue, Guanghui; Hou, Chunping; Li, Beichen] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Ling, Nam] Santa Clara Univ, Santa Clara, CA 95053 USA.
C3 Tianjin University; Beijing University of Technology; Santa Clara
   University
RP Yue, GH (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM yueguanghui@tju.edu.cn; hcp@tju.edu.cn; guke.doctor@gmail.com;
   nling@scu.edu; relidin@126.com
RI Gu, Ke/AAJ-9684-2021
FU National Natural Science Foundation of China [61520106002, 61471262,
   61731003]; China Scholarship Council
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61520106002, 61471262, and 61731003, and in part by
   the China Scholarship Council. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Lingfen Su.
CR [Anonymous], 2013, International Scholarly Research Notices., DOI DOI 10.1155/2013/905685
   BENNETT PJ, 1987, NATURE, V326, P873, DOI 10.1038/326873a0
   Bouboulis P, 2015, IEEE T NEUR NET LEAR, V26, P1260, DOI 10.1109/TNNLS.2014.2336679
   Bovik AC, 2013, P IEEE, V101, P2008, DOI 10.1109/JPROC.2013.2257632
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Du SL, 2016, DIGIT SIGNAL PROCESS, V55, P1, DOI 10.1016/j.dsp.2016.04.006
   Ghadiyaram D., 2015, MASSIVE ONLINE CROWD
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Ghadiyaram D, 2015, IEEE IMAGE PROC, P3851, DOI 10.1109/ICIP.2015.7351526
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu K, 2018, IEEE T VIS COMPUT GR, V24, P2689, DOI 10.1109/TVCG.2017.2771284
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Gu K, 2013, IEEE WORKSHOP SIG, P241, DOI 10.1109/SiPS.2013.6674512
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Guo ZH, 2016, IEEE T IMAGE PROCESS, V25, P687, DOI 10.1109/TIP.2015.2507408
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   Intriligator J, 2001, COGNITIVE PSYCHOL, V43, P171, DOI 10.1006/cogp.2001.0755
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092
   Li CF, 2015, IEEE IMAGE PROC, P4883, DOI 10.1109/ICIP.2015.7351735
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Lu W, 2010, NEUROCOMPUTING, V73, P784, DOI 10.1016/j.neucom.2009.10.012
   Lu YA, 2015, IEEE SIGNAL PROC LET, V22, P1811, DOI 10.1109/LSP.2015.2436908
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Pelli DG, 2008, NAT NEUROSCI, V11, P1129, DOI 10.1038/nn.2187
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   T. V. Q. E. Group, FIN REP VID QUAL EXP
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   TOET A, 1992, VISION RES, V32, P1349, DOI 10.1016/0042-6989(92)90227-A
   Wang SG, 2017, IEEE T CYBERNETICS, V47, P232, DOI 10.1109/TCYB.2015.2512852
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HR, 2013, P IEEE, V101, P2025, DOI 10.1109/JPROC.2013.2262911
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yue GH, 2018, IEEE T IND ELECTRON, V65, P2525, DOI 10.1109/TIE.2017.2739708
   Yue GH, 2017, J VIS COMMUN IMAGE R, V49, P382, DOI 10.1016/j.jvcir.2017.09.011
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang M, 2015, IEEE SIGNAL PROC LET, V22, P207, DOI 10.1109/LSP.2014.2326399
NR 52
TC 32
Z9 33
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2722
EP 2732
DI 10.1109/TMM.2018.2807589
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000015
DA 2024-07-18
ER

PT J
AU Ceulemans, B
   Lu, SP
   Lafruit, G
   Munteanu, A
AF Ceulemans, Beerend
   Lu, Shao-Ping
   Lafruit, Gauthier
   Munteanu, Adrian
TI Robust Multiview Synthesis for Wide-Baseline Camera Arrays
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE View synthesis; multiview 3D; disocclusion inpainting; depth filtering
ID SPATIO-TEMPORALLY CONSISTENT; TEXTURE SYNTHESIS; IMAGE; DISOCCLUSION;
   EXTENSIONS; COLOR
AB In many advanced multimedia systems, multiview content can offer more immersion compared to classical stereoscopy. The feeling of immersiveness is increased substantially by offering motion-parallax, as well as stereopsis. This drives both the so-called free-navigation and super-multiview technologies. However, it is currently still challenging to acquire, store, process, and transmit this type of content. This paper presents a novel multiview-interpolation framework for wide-baseline camera arrays. The proposed method comprises several novel components, including point cloud-based filtering, improved de-ghosting, multireference color blending, and depth-aware MRF-based disocclusion inpainting. The method offers robustness against depth errors caused by quantization and smoothing across object boundaries. Furthermore, the available input color and depth are maximally exploited while preventing propagation of unreliable information to virtual viewpoints. The experimental results show that the proposed method outperforms the state-of-the-art View Synthesis Reference Software (VSRS 4.1) both in objective terms as well as subjectively, based on a visual assessment on a high-end light-field three-dimensional display.
C1 [Ceulemans, Beerend; Lu, Shao-Ping; Munteanu, Adrian] Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
   [Lafruit, Gauthier] Univ Libre Bruxelles, Labs Image Signal Proc & Acoust, B-1050 Brussels, Belgium.
C3 Vrije Universiteit Brussel; Universite Libre de Bruxelles
RP Lu, SP; Munteanu, A (corresponding author), Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
EM bceulema@etro.vub.ac.be; slu@etrovub.be; gauthier.lafruit@ulb.ac.be;
   acmuntea@etrovub.be
RI Li, Mengqi/AAG-6804-2021; Munteanu, Adrian/HKO-9955-2023
OI Munteanu, Adrian/0000-0001-7290-0428
FU Innoviris 3DLicorneA [2015-R-39c, 2015-R-39d]
FX Parts of this research were performed in the scope of the Innoviris
   3DLicorneA 2015-R-39c, 2015-R-39d project. We would also like to thank
   the reviewers for their constructive feedback and suggestions.
CR Ahn I, 2013, IEEE T BROADCAST, V59, P614, DOI 10.1109/TBC.2013.2281658
   [Anonymous], 2012, Methodology for the subjective assessment of the quality of television pictures
   [Anonymous], P INT C COMP VIS THE
   [Anonymous], 2016, IEEE INT CON MULTI
   Badino H., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P3084, DOI 10.1109/ICRA.2011.5980275
   Balogh T, 2006, P SOC PHOTO-OPT INS, V6055, pU550, DOI 10.1117/12.650907
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Buyssens P., 2015, SIGGRAPH ASIA 2015 T
   Cheng CM, 2011, IEEE T BROADCAST, V57, P523, DOI 10.1109/TBC.2011.2139090
   Cignoni P., 2008, P EUR IT CHAPT C, P129, DOI [DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136]
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Daribo Ismael, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P167, DOI 10.1109/MMSP.2010.5662013
   Domanski M, 2013, IEEE T IMAGE PROCESS, V22, P3517, DOI 10.1109/TIP.2013.2266580
   Dricot A, 2015, SIGNAL PROCESS-IMAGE, V39, P369, DOI 10.1016/j.image.2015.04.012
   Dziembowski A, 2016, PICT COD SYMP, DOI 10.1109/PCS.2016.7906380
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Emori T, 2015, PROC SPIE, V9391, DOI 10.1117/12.2078853
   Fecker U, 2008, IEEE T CIRC SYST VID, V18, P1258, DOI 10.1109/TCSVT.2008.926997
   Gallup David, 2007, CVPR
   Gautier J, 2011, 3DTV CONF
   Habigt J, 2013, IEEE IMAGE PROC, P2131, DOI 10.1109/ICIP.2013.6738439
   He KM, 2012, LECT NOTES COMPUT SC, V7573, P16, DOI 10.1007/978-3-642-33709-3_2
   Held RT, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P23
   Huang JB, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601205
   Huynh DQ, 2009, J MATH IMAGING VIS, V35, P155, DOI 10.1007/s10851-009-0161-2
   Jin J, 2016, IEEE T MULTIMEDIA, V18, P953, DOI 10.1109/TMM.2016.2539825
   Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82
   Komodakis N, 2007, IEEE T IMAGE PROCESS, V16, P2649, DOI 10.1109/TIP.2007.906269
   Kopf J, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508369
   Le Meur O., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3401, DOI 10.1109/ICIP.2011.6116441
   Lee BH, 2013, PHYS TODAY, V66, P36, DOI 10.1063/PT.3.1947
   Liang CK, 2011, IEEE T CIRC SYST VID, V21, P525, DOI 10.1109/TCSVT.2011.2125570
   Lu S., 2013, Proceedings of the International Conference on Image Processing, Computer Vision, and Pattern Recognition (IPCV). The Steering Committee of The World Congress in Computer Science, P1
   Lu SP, 2015, IEEE T MULTIMEDIA, V17, P577, DOI 10.1109/TMM.2015.2412879
   Mao Y, 2013, INT CONF ACOUST SPEE, P1859, DOI 10.1109/ICASSP.2013.6637975
   MPEG Requirements, 2015, N15733 ISOIEC JTC1SC
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Plath N, 2013, IEEE T IMAGE PROCESS, V22, P3420, DOI 10.1109/TIP.2013.2268940
   Ruzic T, 2015, IEEE T IMAGE PROCESS, V24, P444, DOI 10.1109/TIP.2014.2372479
   Schenkel A., 2017, THESIS
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stankiewicz O, 2016, LECT NOTES COMPUT SC, V9972, P253, DOI 10.1007/978-3-319-46418-3_23
   Tauber Z, 2007, IEEE T SYST MAN CY C, V37, P527, DOI 10.1109/TSMCC.2006.886967
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Tezuka T, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P124, DOI 10.1109/PCS.2015.7170060
   Urey H, 2011, P IEEE, V99, P540, DOI 10.1109/JPROC.2010.2098351
   Vetro A, 2012, PROC SPIE, V8499, DOI 10.1117/12.945926
   Wegner K., 2013, M31518 ISOIEC JTC1SC
   Wegner K., 2013, M31520 ISOIEC JTC1SC
   Ye SQ, 2017, SIGNAL PROCESS-IMAGE, V53, P40, DOI 10.1016/j.image.2017.01.004
   Yin Z, 2014, 3DTV C TRUE VIS CAPT, P1
   Zhao Y, 2011, IEEE T IMAGE PROCESS, V20, P2221, DOI 10.1109/TIP.2011.2118218
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
   Zou F, 2014, IEEE T CIRC SYST VID, V24, P1696, DOI 10.1109/TCSVT.2014.2313891
NR 57
TC 15
Z9 18
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2235
EP 2248
DI 10.1109/TMM.2018.2802646
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200001
DA 2024-07-18
ER

PT J
AU Usman, MA
   Usman, MR
   Shin, SY
AF Usman, Muhammad Arslan
   Usman, Muhammad Rehan
   Shin, Soo Young
TI A Novel No-Reference Metric for Estimating the Impact of Frame Freezing
   Artifacts on Perceptual Quality of Streamed Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE No reference; motion content; video quality assessment; frame freezing;
   temporal features
ID JERKINESS
AB Online monitoring of multimedia networks is required to ensure seamless and ubiquitous delivery of services to the end users. Quality of multimedia content, such as video streams, often gets degraded due to network losses such as packet loss. Frame freezing artifacts are introduced in a video stream when packet loss or packet delay takes place. Estimating the perceptual impact of these artifacts on quality of experience of end users helps service providers to maintain quality of service. In this paper, we have presented a novel no-reference video quality metric, which measures the impact of frame freezing due to packet loss and delay in video streaming networks. The proposed metric is based on several features that directly impact the quality of experience of end users. These features, including motion characteristics of videos, are calculated using the temporal information between video frames and then combined mathematically to form a video quality metric. Different weights are assigned to different features for better performance of the proposed metric. With detailed experiments, we have shown that our method outperforms other contemporary methods in terms of high accuracy and low computation time in frame freeze detection, low root mean square values, high coefficient of determination, and high correlation between subjective and objective measurements. We have used five video databases for our model's evaluation and validation. Furthermore, we have shown that our method is statistically superior to the other models in comparison.
C1 [Usman, Muhammad Arslan; Usman, Muhammad Rehan; Shin, Soo Young] Kumoh Natl Inst Technol, Wireless & Emerging Network Syst Lab, Dept IT Convergence Engn, Gumi 39177, South Korea.
   [Usman, Muhammad Rehan] Super Coll, Dept Elect Engn, Univ Campus, Lahore, Pakistan.
C3 Kumoh National University Technology
RP Shin, SY (corresponding author), Kumoh Natl Inst Technol, Wireless & Emerging Network Syst Lab, Dept IT Convergence Engn, Gumi 39177, South Korea.
EM arslanus-man@msn.com; rehanusman@gmail.com; wdragon@kumoh.ac.kr
RI Usman, Muhammad/AAF-3895-2019; Shin, Soo Young/ABG-4608-2021; Usman,
   Muhammad Rehan/AAZ-1513-2021
OI Shin, Soo Young/0000-0002-2526-2395; Usman, Muhammad
   Rehan/0000-0002-9586-0833; Usman, Muhammad Arslan/0000-0002-6440-4595
FU Ministry of Science and ICT, South Korea, under the Information
   Technology Research Center support program [IITP-2017-2014-0-00639]
FX This work was supported by the Ministry of Science and ICT, South Korea,
   under the Information Technology Research Center support program
   (IITP-2017-2014-0-00639) supervised by the Institute for Information and
   Communications Technology Promotion.
CR [Anonymous], 2015, IEEE INT WORKSHOP MU
   [Anonymous], 2007, STAT METHODS PSYCHOL
   Borer S., 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P218, DOI 10.1109/QOMEX.2010.5516155
   Chen C, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2337277
   Chen C, 2014, IEEE T IMAGE PROCESS, V23, P2206, DOI 10.1109/TIP.2014.2312613
   Duanmu ZF, 2017, IEEE J-STSP, V11, P154, DOI 10.1109/JSTSP.2016.2608329
   Huynh-Thu Q, 2009, IEEE IMAGE PROC, P2221, DOI 10.1109/ICIP.2009.5413894
   ITU-R, 2012, BT50013 ITUR
   ITU-T, 2008, ITU T P910 SUBJECTIV
   ITU-T, 2012, P12011 ITUT
   Mittal A, 2016, IEEE T IMAGE PROCESS, V25, P289, DOI 10.1109/TIP.2015.2502725
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Rimac-Drlje S, 2010, MULTIMED TOOLS APPL, V49, P425, DOI 10.1007/s11042-009-0442-1
   Rodríguez DZ, 2012, IEEE T CONSUM ELECTR, V58, P985, DOI 10.1109/TCE.2012.6311346
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Scott MJ, 2016, IEEE T MULTIMEDIA, V18, P1796, DOI 10.1109/TMM.2016.2574623
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Shahid M, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-40
   Silva AF, 2016, IEEE T MULTIMEDIA, V18, P2446, DOI 10.1109/TMM.2016.2601027
   Tu JV, 1996, J CLIN EPIDEMIOL, V49, P1225, DOI 10.1016/S0895-4356(96)00002-9
   Usman MA, 2017, COMPUT BIOL MED, V91, P112, DOI 10.1016/j.compbiomed.2017.10.007
   Usman MA, 2017, IETE TECH REV, V34, P309, DOI 10.1080/02564602.2016.1185975
   Usman MA, 2016, INT CONF UBIQ FUTUR, P839, DOI 10.1109/ICUFN.2016.7537155
   Vega MT, 2016, INT J PERVASIVE COMP, V12, P66, DOI 10.1108/IJPCC-01-2016-0008
   VQEG STL, MATLAB COD POP SUBJ
   Vranjes M, 2013, SIGNAL PROCESS-IMAGE, V28, P1, DOI 10.1016/j.image.2012.10.003
   Wang Z, 2015, PROC SPIE, V9599, DOI 10.1117/12.2187740
   Webster A., 2010, TECH REP
   Wolf S., 2009, P INT WORKSH VID PRO
   Wolf S., 2011, TM11482 NAT TEL INF
   Wu YC, 2016, IEEE T MULTIMEDIA, V18, P2135, DOI 10.1109/TMM.2016.2614220
   Xue YY, 2015, IEEE T MULTIMEDIA, V17, P134, DOI 10.1109/TMM.2014.2368272
   Yamagishi K., 2013, 7 INT WORKSHOP VIDEO, P52
   Yammine G, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P341, DOI 10.1109/PCS.2012.6213315
NR 34
TC 12
Z9 14
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2344
EP 2359
DI 10.1109/TMM.2018.2801722
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200009
DA 2024-07-18
ER

PT J
AU Zhao, WD
   Lu, HM
   Wang, D
AF Zhao, Wenda
   Lu, Huimin
   Wang, Dong
TI Multisensor Image Fusion and Enhancement in Spectral Total Variation
   Domain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive gain function; multiscale decomposition; multisensor image
   fusion and enhancement; spectral total variation (TV)
ID CONTOURLET TRANSFORM; CURVELET TRANSFORM; EXTRACTION; DECOMPOSITION
AB Most existing image fusion methods assume that at least one input image contains high-quality information at any place of an observed scene. Thus, these fusion methods will fail if every input image is degraded. To address this issue, this study proposes a novel fusion framework that integrates image fusion based on spectral total variation (TV) method and image enhancement. For spatially varying multiscale decompositions generated by the spectral TV framework, this study verifies that the decomposition components can be modeled efficiently by tailed astable-based random variable distribution (TRD) rather than the commonly used Gaussian distribution. Consequently, salience and match measures based on TRD are proposed to fuse each sub-band decomposition. The spatial intensity information is also adopted to fuse the remainder of the image decomposition components. A sub-band adaptive gain function family based on TV spectrum and space variation is constructed for fused multiscale decompositions to enhance fused image simultaneously. Finally, numerous experiments with various multisensor image pairs are conducted to evaluate the proposed method. Experimental results show that even if the input images are degraded, the fused image obtained by the proposed method achieves significant improvement in terms of edge details and contrast while extracting the main features of the input images, thereby achieving better performance compared with the state-of-the-art methods.
C1 [Zhao, Wenda; Wang, Dong] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
   [Lu, Huimin] Kyushu Inst Technol, Dept Mech & Control Engn, Kitakyushu, Fukuoka 8048550, Japan.
C3 Dalian University of Technology; Kyushu Institute of Technology
RP Wang, D (corresponding author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
EM zhaowenda@dlut.edu.cn; luhuimin@ieee.org; wdice@dlut.edu.cn
RI Wang, Dong/R-9624-2018
FU China Postdoctoral Science Foundation [2017M611221]; Introduction Talent
   Project, Fundamental Research Funds for Central Universities
   [DUT16RC(3)077]
FX This work was supported in part by the Project funded by China
   Postdoctoral Science Foundation under Grant 2017M611221, and in part by
   the Introduction Talent Project, Fundamental Research Funds for Central
   Universities (DUT16RC(3)077). The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Chang-Su Kim.
CR Andreu F., 2001, DIFFERENTIAL INTEGRA, V14, P321
   [Anonymous], 1998, CWI
   [Anonymous], 2002, INFORM FUSION, DOI DOI 10.1016/S1566-2535(01)00037-9
   [Anonymous], 1994, Stable Non-Gaussian Random Processes: Stochastic Models with Infinite Variance
   [Anonymous], ICCV 1993
   Aslantas V, 2014, IET IMAGE PROCESS, V8, P289, DOI 10.1049/iet-ipr.2012.0667
   Asmare MH, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION MANAGEMENT AND ENGINEERING, PROCEEDINGS, P352, DOI 10.1109/ICIME.2009.112
   Bai XZ, 2012, APPL OPTICS, V51, P338, DOI 10.1364/AO.51.000338
   Benning M, 2017, LECT NOTES COMPUT SC, V10302, P41, DOI 10.1007/978-3-319-58771-4_4
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Cho TS, 2010, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2010.5540214
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Do MN, 2002, IEEE T IMAGE PROCESS, V11, P146, DOI 10.1109/83.982822
   Du J, 2016, NEUROCOMPUTING, V194, P326, DOI 10.1016/j.neucom.2016.02.047
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Ghassemian H, 2016, INFORM FUSION, V32, P75, DOI 10.1016/j.inffus.2016.03.003
   Gilboa G, 2014, SIAM J IMAGING SCI, V7, P1937, DOI 10.1137/130930704
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Horesh D, 2016, IEEE T IMAGE PROCESS, V25, P4260, DOI 10.1109/TIP.2016.2587121
   James AP, 2014, INFORM FUSION, V19, P4, DOI 10.1016/j.inffus.2013.12.002
   Jang JH, 2012, IEEE T IMAGE PROCESS, V21, P3479, DOI 10.1109/TIP.2012.2197014
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Kim M, 2016, INFORM FUSION, V27, P198, DOI 10.1016/j.inffus.2015.03.003
   Korus P, 2016, IEEE T IMAGE PROCESS, V25, P1312, DOI 10.1109/TIP.2016.2518870
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Kumar M, 2009, IEEE T IMAGE PROCESS, V18, P2137, DOI 10.1109/TIP.2009.2025006
   Kuruoglu EE, 2004, IEEE T IMAGE PROCESS, V13, P527, DOI 10.1109/TIP.2003.818017
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li JX, 2016, IEEE T IMAGE PROCESS, V25, P4421, DOI 10.1109/TIP.2016.2588331
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liang GZ, 2015, IEEE T IMAGE PROCESS, V24, P4488, DOI 10.1109/TIP.2015.2465169
   Liu Z, 2017, INFORM FUSION, V36, P251, DOI 10.1016/j.inffus.2016.12.007
   Liu Z, 2001, PATTERN RECOGN LETT, V22, P929, DOI 10.1016/S0167-8655(01)00047-2
   Lu QK, 2016, IEEE GEOSCI REMOTE S, V13, P515, DOI 10.1109/LGRS.2016.2521418
   Ma Y, 2016, NEUROCOMPUTING, V202, P12, DOI 10.1016/j.neucom.2016.03.009
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Petrovic V, 2007, INFORM FUSION, V8, P208, DOI 10.1016/j.inffus.2005.05.001
   Petrovic VS, 2004, IEEE T IMAGE PROCESS, V13, P228, DOI 10.1109/tip.2004.823821
   Piella G, 2009, INT J COMPUT VISION, V83, P1, DOI 10.1007/s11263-009-0206-4
   Restaino R, 2016, IEEE T IMAGE PROCESS, V25, P2882, DOI 10.1109/TIP.2016.2556944
   Son CH, 2016, IEEE T IMAGE PROCESS, V25, P2866, DOI 10.1109/TIP.2016.2556618
   Son CH, 2013, J VIS COMMUN IMAGE R, V24, P1303, DOI 10.1016/j.jvcir.2013.09.001
   Starck JL, 2003, IEEE T IMAGE PROCESS, V12, P706, DOI 10.1109/TIP.2003.813140
   Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI [10.1109/TIP.2002.1014998, 10.1117/12.408568]
   Sun J, 2013, INFORM FUSION, V14, P241, DOI 10.1016/j.inffus.2012.07.003
   Tsai HC, 2015, J VIS COMMUN IMAGE R, V33, P165, DOI 10.1016/j.jvcir.2015.09.012
   Upla KP, 2015, IEEE T GEOSCI REMOTE, V53, P3210, DOI 10.1109/TGRS.2014.2371812
   Wan T, 2009, IEEE T MULTIMEDIA, V11, P624, DOI 10.1109/TMM.2009.2017640
   Yan B, 2015, IEEE T CIRC SYST VID, V25, P15, DOI 10.1109/TCSVT.2014.2329374
   Yin HP, 2015, OPT COMMUN, V354, P299, DOI 10.1016/j.optcom.2015.05.020
   Zhou ZQ, 2016, INFORM FUSION, V30, P15, DOI 10.1016/j.inffus.2015.11.003
NR 55
TC 107
Z9 112
U1 0
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 866
EP 879
DI 10.1109/TMM.2017.2760100
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000008
DA 2024-07-18
ER

PT J
AU Xie, Q
   Remil, O
   Guo, YW
   Wang, M
   Wei, MQ
   Wang, J
AF Xie, Qian
   Remil, Oussama
   Guo, Yanwen
   Wang, Meng
   Wei, Mingqiang
   Wang, Jun
TI Object Detection and Tracking Under Occlusion for Object-Level RGB-D
   Video Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE RGB-D video; region clustering; scale-invariant feature transform (SIFT)
   flow; spatio-temporal segmentation
AB RGB-D video segmentation is important for many applications, including scene understanding, object tracking, and robotic grasping. However, to segment RGB-D frames over a long video sequence into globally consistent segmentation is still a challenging problem. Current methods often lose pixel correspondences between frames under occlusion and, thus, fail to generate consistent and continuous segmentation results. To address this problem, we propose a novel spatiotemporal RGB-D video segmentation framework that automatically segments and tracks objects with continuity and consistency over time. Our approach first produces consistent segments in some keyframes by region clustering, and then propagates the segmentation result to a whole video sequence via a mask propagation scheme in bilateral space. Instead of exploiting local optical, flow information to establish correspondences between adjacent frames, we leverage scale-invariant feature transform (SIFT) flow and bilateral representation to solve inconsistency under occlusion. Moreover, our method automatically extracts multiple objects of interest and tracks them without any user input hint. A variety of experiments demonstrates effectiveness and robustness of our proposed method.
C1 [Xie, Qian; Remil, Oussama; Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210016, Jiangsu, Peoples R China.
   [Guo, Yanwen] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210093, Jiangsu, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Anhui, Peoples R China.
   [Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing 210016, Jiangsu, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing University;
   Hefei University of Technology; Nanjing University of Aeronautics &
   Astronautics
RP Wang, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210016, Jiangsu, Peoples R China.; Wei, MQ (corresponding author), Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing 210016, Jiangsu, Peoples R China.
EM qianxie@nuaa.edu.cn; remil.oussama@outlook.com; ywguo@nju.edu.cn;
   eric.mengwang@gmail.com; mqwei@nuaa.edu.cn; junwang@outlook.com
RI Wang, Jun/AAM-6868-2021; Wang, Meng/ITR-8699-2023
OI Wang, Jun/0000-0001-9223-2615; Wei, Mingqiang/0000-0003-0429-490X
FU Natural Science Foundation of Jiangsu Province [BK2014833, BK20150016];
   National Natural Science Foundation of China [61772267, 61402224,
   61772257]; Nanjing University of Aeronautics and Astronautics
   Fundamental Research Funds [NS2015053]; Jiangsu Specially Appointed
   Professorship; Fundamental Research Funds for the Central Universities
   [NE2014402, NE2016004]
FX This work was supported in part by the National Natural Science
   Foundation of China (61772267, 61402224, 61772257), in part by the
   Fundamental Research Funds for the Central Universities (NE2014402,
   NE2016004), in part by the Natural Science Foundation of Jiangsu
   Province (BK2014833, BK20150016), in part by the Nanjing University of
   Aeronautics and Astronautics Fundamental Research Funds (NS2015053), and
   in part by Jiangsu Specially Appointed Professorship.
CR [Anonymous], 2012, EUR C COMP VIS
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Camplani M, 2014, J VIS COMMUN IMAGE R, V25, P122, DOI 10.1016/j.jvcir.2013.03.009
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fossati A., 2012, CONSUMER DEPTH CAMER
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Galasso F, 2014, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2014.14
   Ghafarianzadeh M, 2016, IEEE INT CONF ROBOT, P2310, DOI 10.1109/ICRA.2016.7487380
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Hickson S, 2014, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2014.51
   Hodan T, 2017, IEEE WINT CONF APPL, P880, DOI 10.1109/WACV.2017.103
   Huang H., 2014, ISPRS ANN PHOTOGRAMM, VII, P73
   Husain F, 2015, IEEE T CYBERNETICS, V45, P266, DOI 10.1109/TCYB.2014.2324815
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Khoreva A, 2015, PROC CVPR IEEE, P951, DOI 10.1109/CVPR.2015.7298697
   Koutlemanis P, 2013, LECT NOTES COMPUT SC, V8033, P216, DOI 10.1007/978-3-642-41914-0_22
   Kwak S, 2015, IEEE I CONF COMP VIS, P3173, DOI 10.1109/ICCV.2015.363
   Liu C. H., THESIS
   Märki N, 2016, PROC CVPR IEEE, P743, DOI 10.1109/CVPR.2016.87
   Maier D, 2012, IEEE-RAS INT C HUMAN, P692, DOI 10.1109/HUMANOIDS.2012.6651595
   Nagaraja NS, 2015, IEEE I CONF COMP VIS, P3235, DOI 10.1109/ICCV.2015.370
   Reza M. A., 2014, P ROB SCI SYST C 5 W
   Rubinstein M, 2016, INT J COMPUT VISION, V119, P23, DOI 10.1007/s11263-016-0894-5
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sun D, 2015, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2015.7298653
   Teichman A, 2013, IEEE T AUTOM SCI ENG, V10, P841, DOI 10.1109/TASE.2013.2264286
   Ückermann A, 2012, IEEE-RAS INT C HUMAN, P198, DOI 10.1109/HUMANOIDS.2012.6651520
   Wang C, 2014, IEEE T MULTIMEDIA, V16, P903, DOI 10.1109/TMM.2014.2306393
   Wang T, 2016, IEEE T MULTIMEDIA, V18, P2358, DOI 10.1109/TMM.2016.2600441
   Wang Wenguan, 2015, IEEE Trans Image Process, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Weikersdorfer D, 2013, IEEE IMAGE PROC, P2708, DOI 10.1109/ICIP.2013.6738558
   Yun Jiang, 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P3304, DOI 10.1109/ICRA.2011.5980145
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
NR 35
TC 33
Z9 34
U1 3
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 580
EP 592
DI 10.1109/TMM.2017.2751965
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500006
DA 2024-07-18
ER

PT J
AU Guan, JW
   Yi, S
   Zeng, XY
   Cham, WK
   Wang, XG
AF Guan, Jingwei
   Yi, Shuai
   Zeng, Xingyu
   Cham, Wai-Kuen
   Wang, Xiaogang
TI Visual Importance and Distortion Guided Deep Image Quality Assessment
   Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion sensitive features; image quality assessment (IQA); visual
   importance; visual quality maps
ID NATURAL SCENE STATISTICS; GRADIENT MAGNITUDE; SALIENCY DETECTION; JOINT
   STATISTICS; INFORMATION; SIMILARITY
AB In this paper, we tackle the problem of no-reference image quality assessment (IQA). A learning-based IQA framework "VIDGIQA" is proposed, which extracts quality features from the input image and regresses the visual quality on these features. Since different distortions lead to different visual perceptions in the human visual system, distortion information is adopted to guide the feature learning process together with the human quality scores. Besides, a regression method is proposed to model and estimate the visual importance weights of all local regions, which can effectively improve the performance. More importantly, all these operations are integrated into one deep neural network, so that they can be jointly optimized and well cooperate with each other. Experiments were conducted to demonstrate the power of the proposed method on several datasets, including the LIVE dataset [1], the TID 2013 dataset [2], the LIVE multiply distorted IQA dataset [3], CSIQ [4], and the LIVE in the wild image quality database [5]. The proposed method achieves 0.969 and 0.973 on the LIVE dataset [1] in terms of the spearman rank-order correlation coefficient and the Pearson linear correlation coefficient, respectively, which outperforms the state-of-the-art methods.
C1 [Guan, Jingwei; Yi, Shuai; Zeng, Xingyu; Cham, Wai-Kuen; Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Guan, JW (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM jwguan@ee.cuhk.edu.hk; syi@ee.cuhk.edu.hk; xyzeng@ee.cuhk.edu.hk;
   wkcham@ee.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk
RI zhang, weijie/JQX-1450-2023; Lu, Wang/JVO-0416-2024; yang,
   qing/JBR-8440-2023; Wang, Xiaogang/B-2439-2013; Wang,
   Xiaogang/L-4369-2014
OI Wang, Xiaogang/0000-0002-7929-5889; 
CR Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Berger J., 2010, Proceedings of the Python for Scientific Computing Conference (SciPy), number Scipy, P1
   Blanchet G, 2008, IEEE IMAGE PROC, P1176, DOI 10.1109/ICIP.2008.4711970
   Carandini M, 1997, J NEUROSCI, V17, P8621
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Freitas PG, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Gao XB, 2013, IEEE T NEUR NET LEAR, V24, P2013, DOI 10.1109/TNNLS.2013.2271356
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Ghadiyaram D, 2015, PROC SPIE, V9394, DOI 10.1117/12.2084807
   Ghadiyaram D, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P946, DOI 10.1109/GlobalSIP.2014.7032260
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Gu K, 2014, IEEE IMAGE PROC, P511, DOI 10.1109/ICIP.2014.7025102
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guan JW, 2015, J VIS COMMUN IMAGE R, V29, P1, DOI 10.1016/j.jvcir.2015.01.007
   Hassen R, 2010, INT CONF ACOUST SPEE, P2434, DOI 10.1109/ICASSP.2010.5496297
   Hou WL, 2015, IEEE MULTIMEDIA, V22, P46, DOI 10.1109/MMUL.2014.55
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Kang L, 2014, IEEE IMAGE PROC, P2570, DOI 10.1109/ICIP.2014.7025520
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee D, 2016, IEEE T IMAGE PROCESS, V25, P3875, DOI 10.1109/TIP.2016.2579308
   Li J, 2016, SIGNAL IMAGE VIDEO P, V10, P609, DOI 10.1007/s11760-015-0784-2
   Li LD, 2016, IEEE T IMAGE PROCESS, V25, P3775, DOI 10.1109/TIP.2016.2577891
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Li YM, 2015, NEUROCOMPUTING, V154, P94, DOI 10.1016/j.neucom.2014.12.015
   Liu LX, 2016, SIGNAL PROCESS-IMAGE, V40, P1, DOI 10.1016/j.image.2015.10.005
   Lu ZK, 2005, IEEE T IMAGE PROCESS, V14, P1928, DOI 10.1109/TIP.2005.854478
   Ma L, 2013, SIGNAL PROCESS-IMAGE, V28, P884, DOI 10.1016/j.image.2012.08.001
   Ma Q, 2008, LECT NOTES COMPUT SC, V5226, P1124
   Manap RA, 2016, IEEE MULTIMEDIA, V23, P22, DOI 10.1109/MMUL.2016.2
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Moorthy AK, 2009, IEEE J-STSP, V3, P193, DOI 10.1109/JSTSP.2009.2015374
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Osberger W, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P414, DOI 10.1109/ICIP.1998.727227
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Rohaly A.M., 2000, ITU T STANDARDS CONT, P9
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368
   Tang HX, 2011, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2011.5995446
   Vu CT, 2008, 2008 IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS & INTERPRETATION, P73, DOI 10.1109/SSIAI.2008.4512288
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2006, IEEE IMAGE PROC, P2945, DOI 10.1109/ICIP.2006.313136
   Wu JJ, 2014, IEEE SIGNAL PROC LET, V21, P437, DOI 10.1109/LSP.2014.2304714
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xu Y, 2015, IEEE T IMAGE PROCESS, V24, P2098, DOI 10.1109/TIP.2015.2413298
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye P, 2013, PROC CVPR IEEE, P987, DOI 10.1109/CVPR.2013.132
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhang Hong, 2013, 2013 IEEE International Conference on Green Computing and Communications (GreenCom) and IEEE Internet of Things (iThings) and IEEE Cyber, Physical and Social Computing (CPSCom), P1790, DOI 10.1109/GreenCom-iThings-CPSCom.2013.329
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2013, IEEE IMAGE PROC, P171, DOI 10.1109/ICIP.2013.6738036
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang ML, 2013, PROCEEDINGS OF CHINA-CANADA WORKSHOP ON FINANCIAL ENGINEERING AND ENTERPRISE RISK MANAGEMENT 2013, P1
   Zhang M, 2015, IEEE SIGNAL PROC LET, V22, P207, DOI 10.1109/LSP.2014.2326399
   Zhang P, 2015, PROC CVPR IEEE, P2394, DOI 10.1109/CVPR.2015.7298853
   Zhang YM, 2016, MECHANICAL, CONTROL, ELECTRIC, MECHATRONICS, INFORMATION AND COMPUTER, P241
   Zhao Y, 2016, ELECTRON LETT, V52, P1849, DOI 10.1049/el.2016.1328
NR 76
TC 49
Z9 56
U1 2
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2505
EP 2520
DI 10.1109/TMM.2017.2703148
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200012
DA 2024-07-18
ER

PT J
AU Chen, ZX
   Lu, JW
   Feng, JJ
   Zhou, J
AF Chen, Zhixiang
   Lu, Jiwen
   Feng, Jianjiang
   Zhou, Jie
TI Nonlinear Sparse Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hashing; informative encoding; nonlinear transformation; sparse
ID IMAGE; QUANTIZATION; SELECTION; CODES; TREES; SET
AB To facilitate fast similarity search, this paper proposes to encode the nonlinear similarity and image structure as compact binary codes. Rather than adopting single matrix as projection in the literature, we employ a nonlinear transformation in the form of multilayer neural network to generate binary codes to capture the local structure between data samples. Specifically, we train the network such that the quantization loss is minimized and the variance over all bits is maximized. In addition, we capture the salient structure of image samples at the abstract level with sparsity constraint and inherit the generalization power to unseen samples. Furthermore, we incorporate the supervisory label information into the learning procedure to take advantage of the manual label. To obtain the desired binary codes and the parameterized nonlinear transformation, we optimize the formulated objective problem over each variablewith an iterative alternatingmethod. To validate the efficacy of the proposed hashing approach, we conduct experiments on three widely used datasets, namely CIFAR10, MNIST, and SUN397, by comparing with several recent proposed hashing methods.
C1 [Chen, Zhixiang; Lu, Jiwen; Feng, Jianjiang; Zhou, Jie] Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
   [Chen, Zhixiang; Lu, Jiwen; Feng, Jianjiang; Zhou, Jie] Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Lu, JW (corresponding author), Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
EM chen-zx10@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn;
   jfeng@tsinghua.edu.cn; jzhou@tsinghua.edu.cn
RI Feng, Jianjiang/I-3386-2012; Lu, Jiwen/C-5291-2009
OI Chen, Zhixiang/0000-0002-5636-6082; Lu, Jiwen/0000-0002-6121-5529
FU National Key Research and Development Program of China [2016YFB1001001];
   National Natural Science Foundation of China [61672306, 61572271,
   61527808, 61373074, 61373090]; National 1000 Young Talents Plan Program;
   National Basic Research Program of China [2014CB349304]; Ministry of
   Education of China [20120002110033]; Tsinghua University Initiative
   Scientific Research Program
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1001001, in part by the
   National Natural Science Foundation of China under Grant 61672306, Grant
   61572271, Grant 61527808, Grant 61373074, and Grant 61373090, in part by
   the National 1000 Young Talents Plan Program, in part by the National
   Basic Research Program of China under Grant 2014CB349304, in part by the
   Ministry of Education of China under Grant 20120002110033, and in part
   by the Tsinghua University Initiative Scientific Research Program. The
   guest editor coordinating the review of this manuscript and approving it
   for publication was Mr. Jingkuan Song. (Corresponding author: Jiwen Lu.)
CR [Anonymous], 2007, P 24 INT C MACHINE L, DOI DOI 10.1145/1273496.1273592
   [Anonymous], 2013, Book Linear cross-modal hashing for efficient multimedia search, DOI DOI 10.1145/2502081.2502107
   [Anonymous], 2009, NEURIPS
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Carreira-Perpiñán MA, 2015, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2015.7298654
   Dean T, 2013, PROC CVPR IEEE, P1814, DOI 10.1109/CVPR.2013.237
   Ding C., 2006, P 23 INT C MACH LEAR, P281, DOI DOI 10.1145/1143844.1143880
   Donahue J, 2014, PR MACH LEARN RES, V32
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Ge TZ, 2014, LECT NOTES COMPUT SC, V8695, P250, DOI 10.1007/978-3-319-10584-0_17
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453
   He J., 2010, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P1129
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Ji JQ, 2014, IEEE T PATTERN ANAL, V36, P1963, DOI 10.1109/TPAMI.2014.2315806
   Kan MN, 2014, IEEE T CIRC SYST VID, V24, P704, DOI 10.1109/TCSVT.2013.2276713
   Kim S, 2015, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2015.7298739
   Krizhevsky A., 2009, 001 U TOR TOR ON CAN
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Li Deng, 2012, IEEE Signal Processing Magazine, V29, P141, DOI [10.1109/MSP.2012.2211477, DOI 10.1109/MSP.2012.2211477]
   Li Y, 2015, IEEE I CONF COMP VIS, P3819, DOI 10.1109/ICCV.2015.435
   Li Y, 2015, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2015.7299108
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180
   Liu XL, 2014, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2014.275
   Liu XL, 2013, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2013.206
   Liu XL, 2014, PATTERN RECOGN, V47, P748, DOI 10.1016/j.patcog.2013.08.022
   Mairal J., 2008, NIPS, V21, P1033
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Masci J., 2013, CORR
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Norouzi M.E., 2011, ICML
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Pramanik S, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P865, DOI 10.1109/MMCS.1999.779315
   Rastegari M, 2015, PROC CVPR IEEE, P1501, DOI 10.1109/CVPR.2015.7298757
   Shao J, 2012, PATTERN RECOGN LETT, V33, P271, DOI 10.1016/j.patrec.2011.10.018
   Shen FM, 2015, IEEE I CONF COMP VIS, P4148, DOI 10.1109/ICCV.2015.472
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Shi QF, 2009, J MACH LEARN RES, V10, P2615
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song JK, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P827, DOI 10.1145/2733373.2806341
   Song JK, 2014, IEEE T CYBERNETICS, V44, P1225, DOI 10.1109/TCYB.2013.2289351
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang S, 2016, SIGNAL PROCESS, V120, P746, DOI 10.1016/j.sigpro.2014.12.012
   Wang S, 2012, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2012.6247823
   Weiss Y., 2008, PROC ADV NEURAL INFO, V21, P1753
   Weng L, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P259, DOI 10.1145/2671188.2749291
   XIA Y, 2015, PROC CVPR IEEE, P3332
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xu X, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P305, DOI 10.1145/2911996.2912056
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yang M, 2011, PROC CVPR IEEE, P625, DOI 10.1109/CVPR.2011.5995393
   Ye RZ, 2016, IEEE T CYBERNETICS, V46, P718, DOI 10.1109/TCYB.2015.2414299
   Zhang HW, 2016, AAAI CONF ARTIF INTE, P3669
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu XF, 2014, IEEE T IMAGE PROCESS, V23, P3737, DOI 10.1109/TIP.2014.2332764
   Zhu XF, 2013, ACM T INFORM SYST, V31, DOI 10.1145/2457465.2457469
   Zou FH, 2015, IEEE T MULTIMEDIA, V17, P1006, DOI 10.1109/TMM.2015.2425651
NR 76
TC 8
Z9 8
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 1996
EP 2009
DI 10.1109/TMM.2017.2705918
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200004
DA 2024-07-18
ER

PT J
AU Müller, H
   Unay, D
AF Mueller, Henning
   Unay, Devrim
TI Retrieval From and Understanding of Large-Scale Multi-modal Medical
   Datasets: A Review
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Big data; content-based image retrieval; deep learning; large scale
   datasets; medical images; multi-modality
ID HISTOPATHOLOGICAL IMAGE-ANALYSIS; COMPUTER-AIDED DIAGNOSIS; SYSTEM;
   SEGMENTATION
AB Content-based multimedia retrieval (CBMR) has been an active research domain since the mid 1990s. In medicine visual retrieval started later and has mostly remained a research instrument and less a clinical tool. The limited size of data sets due to privacy constraints is often mentioned as reason for these limitations. Nevertheless, much work has been done in CBMR, including the availability of increasingly large data sets and scientific challenges. Annotated data sets and clinical data for images have now become available and can be combined for multimodal retrieval. Much has been learned on user behavior and application scenarios. This text is motivated by the advances in medical image analysis and the availability of public large data sets that often include clinical data. It is a systematic review of recent work (concentrating on the period 2011-2017) on multimodal CBMR and image understanding in the medical domain, where image understanding includes techniques such as detection, localization, and classification for leveraging visual content. With the objective of summarizing the current state of research for multimedia researchers outside the medical field, the text provides ways to get data sets and identifies current limitations and promising research directions. The text highlights advances in the past six years and a trend to use larger scale training data and deep learning approaches that can replace/complement handcrafted features. Using images alone will likely only work in limited domains but combining multiple sources of data for multi-modal retrieval has the biggest chances of success, particularly for clinical impact.
C1 [Mueller, Henning] HES SO Valais, Informat Syst Inst, CH-3960 Sierre, Switzerland.
   [Unay, Devrim] Izmir Univ Econ, Biomed Engn Dept, TR-35330 Izmir, Turkey.
C3 University of Applied Sciences & Arts Western Switzerland; Izmir Ekonomi
   Universitesi
RP Unay, D (corresponding author), Izmir Univ Econ, Biomed Engn Dept, TR-35330 Izmir, Turkey.
EM henning.mueller@hevs.ch; devrim.unay@ieu.edu.tr
RI Unay, Devrim/AAE-6908-2020; Unay, Devrim/G-6002-2010
OI Unay, Devrim/0000-0003-3478-7318; Muller, Henning/0000-0001-6800-9878
CR Aisen AM, 2003, RADIOLOGY, V228, P265, DOI 10.1148/radiol.2281020126
   Akakin HC, 2012, IEEE T INF TECHNOL B, V16, P758, DOI 10.1109/TITB.2012.2185829
   Akgül CB, 2011, J DIGIT IMAGING, V24, P208, DOI 10.1007/s10278-010-9290-9
   Cruz-Roa AA, 2013, LECT NOTES COMPUT SC, V8150, P403, DOI 10.1007/978-3-642-40763-5_50
   Anavi Y, 2015, IEEE ENG MED BIO, P2940, DOI 10.1109/EMBC.2015.7319008
   [Anonymous], TELEHEALTHCARE COMPU
   [Anonymous], SOCIAL NETW ANAL MIN
   [Anonymous], LECT NOTES COMPUT SC
   [Anonymous], 2013, HLTH MANAGE
   [Anonymous], IEEE J BIOM IN PRESS
   [Anonymous], P SPIE MED IMAGING
   [Anonymous], LECT NOTES COMPUT SC
   [Anonymous], TECH REP
   [Anonymous], 2010, RID WAV EUR CAN GAIN
   [Anonymous], OVERVIEW IMAGECLEF 2
   [Anonymous], INT SERIES INFORM RE
   [Anonymous], CORR
   [Anonymous], 2016, J PATHOL INFORM
   [Anonymous], SOC IMP MOB HLTH
   [Anonymous], CORR
   [Anonymous], LECT NOTES COMPUT SC
   [Anonymous], CORR
   [Anonymous], IEEE T CIRC IN PRESS
   [Anonymous], J NUCL MED S1
   [Anonymous], P TRECVID 2003 C DEC
   [Anonymous], 2017, MED IMAGING 2017 IMA
   [Anonymous], P MEDIAEVAL WORKSH
   [Anonymous], P INT C MACH LEARN I
   Archenaa J, 2015, PROCEDIA COMPUT SCI, V50, P408, DOI 10.1016/j.procs.2015.04.021
   AREVALO JOHN, 2014, rev.fac.med, V22, P79
   Beecks C, 2015, IEEE INT SYM MULTIM, P33, DOI 10.1109/ISM.2015.21
   Belle A, 2015, BIOMED RES INT, V2015, DOI 10.1155/2015/370194
   Cai WD, 2012, 2012 9TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), P1775, DOI 10.1109/ISBI.2012.6235925
   Çamlica Z, 2015, INT CONF IMAG PROC, P550, DOI 10.1109/IPTA.2015.7367208
   Carlos JR, 2015, 2015 13 INT WORKSH C, P1
   Chen JC, 2016, SCI REP-UK, V6, DOI [10.1038/srep24454, 10.1038/srep25671]
   Clough P, 2004, LECT NOTES COMPUT SC, V3115, P243
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Depeursinge A, 2012, IEEE T INF TECHNOL B, V16, P53, DOI 10.1109/TITB.2011.2173585
   Depeursinge A, 2010, INFORM RETRIEVAL SER, V32, P95, DOI 10.1007/978-3-642-15181-1_6
   Doi K, 2007, COMPUT MED IMAG GRAP, V31, P198, DOI 10.1016/j.compmedimag.2007.02.002
   Elger BS, 2010, COMPUT METH PROG BIO, V99, P230, DOI 10.1016/j.cmpb.2009.12.001
   Ferreira JR, 2015, COMP MED SY, P60, DOI 10.1109/CBMS.2015.16
   Foncubierta-Rodriguez A., 2012, P ACM MULT 2012 WORK, DOI [DOI 10.1145/2390803.2390808, 10.1145/2390803.2390808]
   Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401
   Groves Peter., 2016, The Big Data Revolution in Healthcare: Accelerating Value and Innovation
   Heimann T, 2009, IEEE T MED IMAGING, V28, P1251, DOI 10.1109/TMI.2009.2013851
   Huang T, 2015, BIG DATA RES, V2, P2, DOI 10.1016/j.bdr.2015.02.002
   Hughes J., 2010, European textbook on ethics in research
   Ioannidis JPA, 2005, PLOS MED, V2, P696, DOI 10.1371/journal.pmed.0020124
   Jiang ML, 2016, MED IMAGE ANAL, V34, P3, DOI 10.1016/j.media.2016.07.011
   Jiang ML, 2015, IEEE T BIO-MED ENG, V62, P783, DOI 10.1109/TBME.2014.2365494
   Jimenez-del-Toro O, 2016, IEEE T MED IMAGING, V35, P2459, DOI 10.1109/TMI.2016.2578680
   Khan I.U., 2017, BIG DATA MANAGEMENT, P71
   Kitanovski I, 2017, MULTIMED TOOLS APPL, V76, P2955, DOI 10.1007/s11042-016-3261-1
   Krenn M., 2014, P MICCAI WORKSH MED, P163
   Kumar A, 2016, COMPUT MED IMAG GRAP, V49, P37, DOI 10.1016/j.compmedimag.2016.01.001
   Kumar A, 2013, J DIGIT IMAGING, V26, P1025, DOI 10.1007/s10278-013-9619-2
   Kundu MK, 2017, COMPUT METH PROG BIO, V139, P209, DOI 10.1016/j.cmpb.2016.10.023
   Li ZY, 2017, PATTERN RECOGN, V63, P680, DOI 10.1016/j.patcog.2016.09.041
   Liu SD, 2014, I C CONT AUTOMAT ROB, P849, DOI 10.1109/ICARCV.2014.7064415
   Liu XR, 2016, IEEE IJCNN, P2872, DOI 10.1109/IJCNN.2016.7727562
   Maier-Hein L, 2014, LECT NOTES COMPUT SC, V8674, P349, DOI 10.1007/978-3-319-10470-6_44
   Markonis D, 2015, INT J MED INFORM, V84, P774, DOI 10.1016/j.ijmedinf.2015.04.003
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Mourao A, 2015, COMPUT MED IMAG GRAP, V39, P35, DOI 10.1016/j.compmedimag.2014.05.006
   Müller H, 2004, INT J MED INFORM, V73, P1, DOI 10.1016/j.ijmedinf.2003.11.024
   Mustapha A, 2015, BIOMED ENG ONLINE, V14, DOI 10.1186/1475-925X-14-6
   Napel S, 2015, J MED IMAGING, V2, DOI 10.1117/1.JMI.2.4.041001
   Bedo MVN, 2016, J DIGIT IMAGING, V29, P22, DOI 10.1007/s10278-015-9809-1
   Patil HK, 2014, IEEE INT CONGR BIG, P762, DOI 10.1109/BigData.Congress.2014.112
   Quellec G, 2011, IEEE T MED IMAGING, V30, P108, DOI 10.1109/TMI.2010.2063711
   Raghupathi W, 2014, HEALTH INF SCI SYST, V2, DOI 10.1186/2047-2501-2-3
   Ramos J, 2016, IEEE J BIOMED HEALTH, V20, P281, DOI 10.1109/JBHI.2014.2375491
   Ranard BL, 2014, J GEN INTERN MED, V29, P187, DOI 10.1007/s11606-013-2536-8
   Rosset A, 2002, RADIOGRAPHICS, V22, P1567, DOI 10.1148/rg.226025058
   Rutman AM, 2009, EUR J RADIOL, V70, P232, DOI 10.1016/j.ejrad.2009.01.050
   Safran C, 2007, J AM MED INFORM ASSN, V14, P1, DOI 10.1197/jamia.M2273
   Shah A, 2016, PROC SPIE, V9784, DOI 10.1117/12.2217162
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Song Y, 2011, I S BIOMED IMAGING, P1885, DOI 10.1109/ISBI.2011.5872776
   Soydemir M., 2013, Intelligent multimedia technologies for networking applications: Techniques and tools, P434
   Srinivas M, 2015, NEUROCOMPUTING, V168, P880, DOI 10.1016/j.neucom.2015.05.036
   Tang L.H. Y., 1999, Health Informatics Journal, V5, P40, DOI DOI 10.1177/146045829900500107
   Tene Omer, 2013, NW. J. TECH. & INTELL. PROP., V11, P239
   Tsikrika T, 2011, LECT NOTES COMPUT SC, V6941, P95, DOI 10.1007/978-3-642-23708-9_12
   Unay D, 2010, IEEE T INF TECHNOL B, V14, P897, DOI 10.1109/TITB.2009.2038152
   Vannier MW, 2003, RADIOLOGY, V228, P23, DOI 10.1148/radiol.2281021654
   Wei GH, 2016, MED PHYS, V43, P6259, DOI 10.1118/1.4966030
   Wyman BT, 2013, ALZHEIMERS DEMENT, V9, P332, DOI 10.1016/j.jalz.2012.06.004
   Zhang F, 2016, NEUROCOMPUTING, V177, P75, DOI 10.1016/j.neucom.2015.11.008
   Zhang XF, 2016, IEEE J BIOMED HEALTH, V20, DOI 10.1109/JBHI.2015.2461671
   Zhang XF, 2015, IEEE T MED IMAGING, V34, P496, DOI 10.1109/TMI.2014.2361481
   Zhou X, 2012, INT J COMPUT ASS RAD, V7, P401, DOI 10.1007/s11548-011-0643-8
   Zhuo YN, 2016, I S BIOMED IMAGING, P859, DOI 10.1109/ISBI.2016.7493401
NR 97
TC 34
Z9 36
U1 1
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2093
EP 2104
DI 10.1109/TMM.2017.2729400
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhen, XT
   Zheng, F
   Shao, L
   Cao, XB
   Xu, D
AF Zhen, Xiantong
   Zheng, Feng
   Shao, Ling
   Cao, Xianbin
   Xu, Dan
TI Supervised Local Descriptor Learning for Human Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; dimensionality reduction; image-to-class distance;
   large scale local features; manifold regularization; naive Bayes nearest
   neighbor
ID MANIFOLD REGULARIZATION; REGRESSION; EIGENMAPS; FRAMEWORK
AB Local features have been widely used in computer vision tasks, e.g., human action recognition, but it tends to be an extremely challenging task to deal with large-scale local features of high dimensionality with redundant information. In this paper, we propose a novel fully supervised local descriptor learning algorithm called discriminative embedding method based on the image-to-class distance (I2CDDE) to learn compact but highly discriminative local feature descriptors for more accurate and efficient action recognition. By leveraging the advantages of the I2C distance, the proposed I2CDDE incorporates class labels to enable fully supervised learning of local feature descriptors, which achieves highly discriminative but compact local descriptors. The objective of our I2CDDE is to minimize the I2C distances from samples to their corresponding classes while maximizing the I2C distances to the other classes in the low-dimensional space. To further improve the performance, we propose incorporating a manifold regularization based on the graph Laplacian into the objective function, which can enhance the smoothness of the embedding by extracting the local intrinsic geometrical structure. The proposed I2CDDE for the first time achieves fully supervised learning of local feature descriptors. It significantly improves the performance of I2C-based methods by increasing the discriminative ability of local features while greatly reducing the computational burden by dimensionality reduction to handle large-scale data. We apply the proposed I2CDDE algorithm to human action recognition on four widely used benchmark datasets. The results have shown that I2CDDE can significantly improve I2C-based classifiers and achieves state-of-the-art performance.
C1 [Zhen, Xiantong; Zheng, Feng] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
   [Shao, Ling] Univ East Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, England.
   [Cao, Xianbin] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
   [Xu, Dan] Univ Trento, Dept Informat Engn & Comp Sci, I-38122 Trento, Italy.
C3 University of Texas System; University of Texas Arlington; University of
   East Anglia; Beihang University; University of Trento
RP Shao, L (corresponding author), Univ East Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, England.
EM zhenxt@gmail.com; zfeng02@gmail.com; ling.shao@ieee.org;
   xbcao@buaa.edu.cn; dan.xu@unitn.it
RI Zheng, Feng/AAH-5643-2019; Xu, Dan/KPA-7396-2024; Shao, Ling/D-3535-2011
OI Zheng, Feng/0000-0002-1701-9141; Xu, Dan/0000-0003-4602-3550; Shao,
   Ling/0000-0002-8264-6117
FU National Science Foundation of China [61571147, 61425014]
FX This work was supported by the National Science Foundation of China
   under Grant 61571147 and Grant 61425014. The guest editor coordinating
   the review of this manuscript and approving it for publication was Mr.
   Jingkuan Song. (Corresponding author: Ling Shao.)
CR [Anonymous], INTRO HUMAN ACTION R
   [Anonymous], 2009, Advances in neural information processing systems
   [Anonymous], P AS C COMP VIS
   [Anonymous], P BR MACH VIS C
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   [Anonymous], 2012, CoRR
   Behmo R, 2010, LECT NOTES COMPUT SC, V6314, P171, DOI 10.1007/978-3-642-15561-1_13
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Cai HP, 2011, IEEE T PATTERN ANAL, V33, P338, DOI 10.1109/TPAMI.2010.89
   Chung F. R. K., 1997, AM MATH SOC, V92, DOI DOI 10.1090/CBMS/092
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100
   Feng ZH, 2015, IEEE T IMAGE PROCESS, V24, P3425, DOI 10.1109/TIP.2015.2446944
   Fornoni M, 2014, INT C PATT RECOG, P3404, DOI 10.1109/ICPR.2014.586
   Garcia EK, 2010, IEEE T KNOWL DATA EN, V22, P1274, DOI 10.1109/TKDE.2009.159
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2004, ADV NEUR IN, V16, P153
   Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642
   Hua G., 2007, PROC IEEE INT C COMP, P1
   Jain P, 2012, J MACH LEARN RES, V13, P519
   Ke Y, 2004, PROC CVPR IEEE, P506
   Klaser A., 2008, BRIT MACHINE VISION
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kuzborskij I, 2016, PROC CVPR IEEE, P2100, DOI 10.1109/CVPR.2016.231
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   McCann S, 2012, PROC CVPR IEEE, P3650, DOI 10.1109/CVPR.2012.6248111
   Niyogi P, 2013, J MACH LEARN RES, V14, P1229
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shao L., 2010, Proceedings of the ACM International Conference on Image and Video Retrieval, P477
   Shao L, 2014, IEEE T CYBERNETICS, V44, P817, DOI 10.1109/TCYB.2013.2273174
   Simonyan K, 2014, IEEE T PATTERN ANAL, V36, P1573, DOI 10.1109/TPAMI.2014.2301163
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song JK, 2016, IMAGE VISION COMPUT, V55, P101, DOI 10.1016/j.imavis.2016.02.005
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tuytelaars T, 2011, IEEE I CONF COMP VIS, P1824, DOI 10.1109/ICCV.2011.6126449
   Wang H., 2007, PROC IEEE C COMPUT V, P1
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang Y, 2010, PATTERN RECOGN, V43, P1008, DOI 10.1016/j.patcog.2009.08.009
   Wang ZX, 2010, LECT NOTES COMPUT SC, V6311, P706, DOI 10.1007/978-3-642-15549-9_51
   Xie J, 2017, IEEE T PATTERN ANAL, V39, P1335, DOI 10.1109/TPAMI.2016.2596722
   Yu MY, 2016, IEEE T PATTERN ANAL, V38, P1908, DOI 10.1109/TPAMI.2015.2497686
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3241, DOI 10.1109/TIP.2014.2328894
   Zhen XT, 2017, IEEE T NEUR NET LEAR, V28, P2035, DOI 10.1109/TNNLS.2016.2573260
   Zhen XT, 2016, IMAGE VISION COMPUT, V55, P39, DOI 10.1016/j.imavis.2016.10.002
   Zhen XT, 2016, IMAGE VISION COMPUT, V50, P1, DOI 10.1016/j.imavis.2016.02.006
   Zhen X, 2015, PROC CVPR IEEE, P1211, DOI 10.1109/CVPR.2015.7298725
   Zhen XT, 2014, INFORM SCIENCES, V281, P295, DOI 10.1016/j.ins.2014.05.021
   Zhen XT, 2013, PATTERN RECOGN LETT, V34, P1899, DOI 10.1016/j.patrec.2012.10.021
   Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu F, 2016, IMAGE VISION COMPUT, V55, P42, DOI 10.1016/j.imavis.2016.06.007
NR 58
TC 15
Z9 17
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2056
EP 2065
DI 10.1109/TMM.2017.2700204
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200009
DA 2024-07-18
ER

PT J
AU Li, YC
   Cao, LL
   Zhu, J
   Luo, JB
AF Li, Yuncheng
   Cao, Liangliang
   Zhu, Jiang
   Luo, Jiebo
TI Mining Fashion Outfit Composition Using an End-to-End Deep Learning
   Approach on Set Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Big data applications; multilayer neural network; multimedia computing
AB Composing fashion outfits involves deep understanding of fashion standards while incorporating creativity for choosing multiple fashion items (e.g., jewelry, bag, pants, dress). In fashion websites, popular or high-quality fashion outfits are usually designed by fashion experts and followed by large audiences. In this paper, we propose a machine learning system to compose fashion outfits automatically. The core of the proposed automatic composition system is to score fashion outfit candidates based on the appearances and metadata. We propose to leverage outfit popularity on fashion-oriented websites to supervise the scoring component. The scoring component is a multimodal multiinstance deep learning system that evaluates instance aesthetics and set compatibility simultaneously. In order to train and evaluate the proposed composition system, we have collected a large-scale fashion outfit dataset with 195K outfits and 368K fashion items from Polyvore. Although the fashion outfit scoring and composition is rather challenging, we have achieved an AUC of 85% for the scoring component, and an accuracy of 77% for a constrained composition task.
C1 [Li, Yuncheng; Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14611 USA.
   [Cao, Liangliang] Yahoo Res, New York, NY 10036 USA.
   [Zhu, Jiang] Yahoo Inc, Sunnyvale, CA 94089 USA.
C3 University of Rochester; Yahoo! Inc; Yahoo! Inc United States; Yahoo!
   Inc; Yahoo! Inc United States
RP Li, YC (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14611 USA.
EM yli@cs.rochester.edu; liangliang@yahoo-inc.com; jiangzhu@yahoo-inc.com;
   jiebo.luo@gmail.com
RI Luo, Jiebo/AAI-7549-2020
OI Luo, Jiebo/0000-0002-4516-9729
FU New York State through the Goergen Institute for Data Science at the
   University of Rochester; FREP award from Yahoo! Research
FX This work was supported in part by New York State through the Goergen
   Institute for Data Science at the University of Rochester, and in part
   by a FREP award from Yahoo! Research. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   M. Bertini. (Corresponding author: Yuncheng Li.)
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], P ANN C INT SPEECH C
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2013, CORR
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], 2015, Deep Residual Learning for Image Recognition
   [Anonymous], P INT C COMP VIS
   Cao DL, 2016, MULTIMEDIA SYST, V22, P479, DOI 10.1007/s00530-014-0407-8
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen KT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P177, DOI 10.1145/2733373.2809930
   Chen Q, 2015, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR.2015.7299169
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Dos Santos C., 2014, Coling, P69
   Hidayati SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P197, DOI 10.1145/2647868.2656405
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Huang ZW, 2015, PR MACH LEARN RES, V37, P720
   Iwata Tomoharu, 2011, P 22 INT JOINT C ON, V3, P2262
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Li YC, 2015, PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, P591, DOI 10.1109/BigData.2015.7363803
   Lin M., 2014, CORR
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Lu JW, 2015, PROC CVPR IEEE, P1137, DOI 10.1109/CVPR.2015.7298717
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, CORR
   Smith JR, 2015, IBM J RES DEV, V59, DOI 10.1147/JRD.2015.2398590
   Tang Jinhui., 2009, Proceedings of ACM international conference on Multimedia, P223, DOI DOI 10.1145/1631272.1631305
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Yamaguchi K, 2015, IEEE T PATTERN ANAL, V37, P1028, DOI 10.1109/TPAMI.2014.2353624
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
NR 32
TC 114
Z9 127
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1946
EP 1955
DI 10.1109/TMM.2017.2690144
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xia, YJ
   Liu, ZG
   Yan, Y
   Chen, YX
   Zhang, LM
   Zimmermann, R
AF Xia, Yingjie
   Liu, Zhenguang
   Yan, Yan
   Chen, Yanxiang
   Zhang, Luming
   Zimmermann, Roger
TI Media Quality Assessment by Perceptual Gaze-Shift Patterns Discovery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaze-shift; perceptual; probabilistic model; quality model; sparse
   encoding
ID AESTHETIC QUALITY; VISUAL-ATTENTION; SALIENCY; MODEL
AB Quality assessment is an indispensable technique in a large body of media applications, i.e., photo retargeting, scenery rendering, and video summarization. In this paper, a fully automatic framework is proposed to mimic how humans subjectively perceive media quality. The key is a locality-preserved sparse encoding algorithm that accurately discovers human gaze shifting paths from each image or video clip. In particular, we first extract local image descriptors from each image/video, and subsequently project them into the so-called perceptual space. Then, a nonnegative matrix factorization (NMF) algorithm is proposed that represents each graphlet by a linear and sparse combination of the basis ones. Since each graphlet is visually/semantically similar to its neighbors, a locality-preserved constraint is encoded into the NMF algorithm. Mathematically, the saliency of each graphlet is quantified by the norm of its sparse codes. Afterward, we sequentially link them into a path to simulate human gaze allocation. Finally, a probabilistic quality model is learned based on such paths extracted from a collection of photos/videos, which are marked as high quality ones via multiple Flickr users. Comprehensive experiments have demonstrated that: 1) our quality model outperforms many of its competitors significantly, and 2) the learned paths are on average 89.5% consistent with real human gaze shifting paths.
C1 [Xia, Yingjie] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
   [Xia, Yingjie] Hefei Univ Technol, Sch Comp & Informat, Hefei 230000, Anhui, Peoples R China.
   [Liu, Zhenguang; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Yan, Yan] Univ Trento, Dept Informat Engn & Comp Sci, I-38122 Trento, Italy.
   [Chen, Yanxiang; Zhang, Luming] Hefei Univ Technol, Sch Comp & Informat, Hefei 230000, Anhui, Peoples R China.
C3 Zhejiang University; Hefei University of Technology; National University
   of Singapore; University of Trento; Hefei University of Technology
RP Chen, YX (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230000, Anhui, Peoples R China.
EM xiayinjie@gmail.com; zhenguangliu@zju.edu.cn; tom.yan.555@gmail.com;
   yanxiangcheng@hfut.edu.cn; zglumg@nus.edu.sg; rogerz@comp.nus.edu.sg
RI Lei, Ming/JAD-1050-2023; Wang, Yiru/JMB-2281-2023; Zimmermann,
   Roger/D-7944-2015; zhang, lu/GRO-2969-2022
OI Zimmermann, Roger/0000-0002-7410-2590; 
FU National Natural Science Foundation of China [61572169, 61472266,
   61472113]; National University of Singapore (Suzhou) Research Institute,
   Suzhou Industrial Park, Jiang Su, China; Zhejiang Provincial Natural
   Science Foundation of China [LR14F020003]; National Nature Science
   Foundation of China [61672201]; Key Projects of Anhui Province Science
   and Technology Plan [15czz02074]; Natural Science Foundation of China;
   Natural Science Foundation of Anhui Province of China; Postdoctoral
   Science Foundation of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61572169, Grant 61472266, and Grant
   61472113, in part by the National University of Singapore (Suzhou)
   Research Institute, Suzhou Industrial Park, Jiang Su, China, in part by
   Zhejiang Provincial Natural Science Foundation of China under Grant
   LR14F020003, in part by National Nature Science Foundation of China
   (61672201), and in part by Key Projects of Anhui Province Science and
   Technology Plan (15czz02074). The work of Y. Chen was supported by the
   Natural Science Foundation of China, by the Natural Science Foundation
   of Anhui Province of China, and by the Postdoctoral Science Foundation
   of China. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Lingfen Sun.
   (Corresponding author: Yanxiang Chen.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], 2010, ACM MULTIMEDIA
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Avraham T, 2010, IEEE T PATTERN ANAL, V32, P693, DOI 10.1109/TPAMI.2009.53
   Borji A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.85
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, PROC CVPR IEEE, P470, DOI 10.1109/CVPR.2012.6247710
   Bruce N. D. B., 2011, J VIS, V9
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P1327, DOI 10.1109/TIP.2016.2520358
   Chen JX, 2011, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.2011.5995675
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Funes Mora KennethAlberto., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on, P25, DOI DOI 10.1109/CVPRW.2012.6239182
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Guestrin ED, 2006, IEEE T BIO-MED ENG, V53, P1124, DOI 10.1109/TBME.2005.863952
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Lee H., 1996, P ADV NEUR INF PROC, P801
   Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Liu X, 2013, PROC CVPR IEEE, P492, DOI 10.1109/CVPR.2013.70
   Lu F, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.126
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Moorthy AK, 2010, LECT NOTES COMPUT SC, V6315, P1, DOI 10.1007/978-3-642-15555-0_1
   Mora KAF, 2013, IEEE IMAGE PROC, P2787, DOI 10.1109/ICIP.2013.6738574
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nakazawa A, 2012, LECT NOTES COMPUT SC, V7573, P159, DOI 10.1007/978-3-642-33709-3_12
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Park JC, 2008, J VISION, V8, DOI 10.1167/8.10.8
   Parks D, 2014, IEEE IMAGE PROC, P436, DOI 10.1109/ICIP.2014.7025087
   Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3
   Song ML, 2012, IEEE T IMAGE PROCESS, V21, P341, DOI 10.1109/TIP.2011.2157514
   Wang W, 2016, IEEE MULTIMEDIA, V23, P80, DOI 10.1109/MMUL.2016.69
   Wang XC, 2011, IEEE T IMAGE PROCESS, V20, P2627, DOI 10.1109/TIP.2011.2114354
   WANG Z, 2003, HDB VIDEO DATABASES, pCH41
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Xia Y., 2015, IEEE T CYBERNETICS, V47, P566
   Xiang S., 2008, KNOWL INF SYST, V19
   Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang JM, 2012, PROC CVPR IEEE, P2296, DOI 10.1109/CVPR.2012.6247940
   Yao B, 2007, LECT NOTES COMPUT SC, V4679, P169
   Yeh HH, 2013, IEEE T MULTIMEDIA, V15, P1944, DOI 10.1109/TMM.2013.2280250
   Zhang L., 2013, IEEE T IP, V21, P803
   Zhang LY, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818710
   Zhang LM, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P491, DOI 10.1145/2733373.2806255
   Zhang LM, 2016, IEEE T NEUR NET LEAR, V27, P674, DOI 10.1109/TNNLS.2015.2444417
   Zhang LM, 2016, IEEE T CYBERNETICS, V46, P535, DOI 10.1109/TCYB.2015.2408592
   Zhang LM, 2016, IEEE T IMAGE PROCESS, V25, P553, DOI 10.1109/TIP.2015.2502147
   Zhang LM, 2016, IEEE T CYBERNETICS, V46, P258, DOI 10.1109/TCYB.2015.2400821
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, P4150, DOI 10.1109/TIP.2014.2344433
   Zhang LM, 2014, IEEE T CYBERNETICS, V44, P1408, DOI 10.1109/TCYB.2013.2285219
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, P2235, DOI 10.1109/TIP.2014.2311658
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang LM, 2013, PROC CVPR IEEE, P1908, DOI 10.1109/CVPR.2013.249
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P5071, DOI 10.1109/TIP.2013.2278465
NR 68
TC 7
Z9 7
U1 2
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1811
EP 1820
DI 10.1109/TMM.2017.2679900
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400010
DA 2024-07-18
ER

PT J
AU Lin, K
   Dumitrescu, S
AF Lin, Kuan
   Dumitrescu, Sorina
TI Cross-Layer Resource Allocation for Scalable Video Over OFDMA Wireless
   Networks: Tradeoff Between Quality Fairness and Efficiency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer; orthogonal frequency division multiple access (OFDMA);
   resource allocation; scalable video coding; quality fairness/efficiency
   trade-off
ID TRANSMISSION; H.264/AVC; DOWNLINK; CHANNELS; STREAMS
AB This work addresses the tradeoff between quality fairness and system efficiency for scalable video delivery to multiple users over OFDMA wireless networks. We consider a cross-layer optimization framework seeking to maximize the sum-PSNR corresponding to average user rates, subject to relaxed PSNR-fair constraints. More specifically, a pure quality-fairness (PF) problem is solved first to determine the maximum PSNR value obtained by imposing the same PSNR level to all users. Next the constraints in the PF problem are relaxed by allowing the relative difference between the PSNR of each video and the PF PSNR value to be within some range [0, sigma]. Thus, the parameter sigma controls the tradeoff between quality fairness and system efficiency. The PF problem is equivalent to the quality fairness problem proposed by Cicalo and Tralli, which was solved using a vertical decomposition approach. Further, we convert the optimization problem with the relaxed fairness constraints into a convex problem and solve it using established techniques. Our simulation results show that by varying the value of sigma, a wide range, densely populated, of tradeoff points between quality fairness and efficiency can be achieved. Additionally, a subjective quality assessment reveals that while the maximum efficiency scheme (ME), i.e., when sigma = infinity, may compromise the quality of the high demanding videos, the PF scheme may sacrifice the quality of the low demanding videos. On the other hand, by providing a trade-off between PF and ME, the proposed scheme has the potential of finding a middle ground where all users are satisfied.
C1 [Lin, Kuan; Dumitrescu, Sorina] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
C3 McMaster University
RP Lin, K (corresponding author), McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
EM link24@mcmaster.ca; sorina@mail.ece.mcmaster.ca
CR [Anonymous], 2015, THESIS
   [Anonymous], 2006, IEEE Standard 802.16--2005
   [Anonymous], CISC VIS NETW IND GL
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen ZZ, 2010, IEEE SIGNAL PROC LET, V17, P675, DOI 10.1109/LSP.2010.2046193
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Cicalò S, 2012, SIGNAL PROCESS-IMAGE, V27, P800, DOI 10.1016/j.image.2012.01.005
   De Cock J, 2009, IEEE T MULTIMEDIA, V11, P1209, DOI 10.1109/TMM.2009.2030606
   Dumitrescu S, 2007, IEEE T MULTIMEDIA, V9, P1466, DOI 10.1109/TMM.2007.906557
   Ha H, 2008, COMPUT COMMUN, V31, P3553, DOI 10.1016/j.comcom.2008.05.010
   He LJ, 2014, IEEE T WIREL COMMUN, V13, P6768, DOI 10.1109/TWC.2014.2364603
   Khalek AA, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2332304
   Khan N, 2012, IEEE INT WORK SIGN P, P334, DOI 10.1109/SPAWC.2012.6292922
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Lin K., 2017, CROSS LAYER RESOURCE
   Maani E, 2008, IEEE T IMAGE PROCESS, V17, P1663, DOI 10.1109/TIP.2008.2001402
   Mansour H, 2008, IEEE T MULTIMEDIA, V10, P1366, DOI 10.1109/TMM.2008.2004915
   Mazzotti M, 2012, IEEE T COMMUN, V60, P2915, DOI 10.1109/TCOMM.2012.081412.110113
   Munaretto D., 2012, P IEEE WCNC, P2134
   Physical Layer Aspects for Evolved Universal Terrestrial Radio Access (UTRA), 2006, 25814 3GPP TR
   Reichel J., 2007, JOINT SCALABLE VIDEO
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SMG E., 1997, 101112 ETSI
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Su GM, 2006, IEEE T CIRC SYST VID, V16, P1217, DOI 10.1109/TCSVT.2006.883513
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Wang X, 2011, IEEE T INFORM THEORY, V57, P4359, DOI 10.1109/TIT.2011.2145770
   Wong IC, 2008, CONF REC ASILOMAR C, P2203, DOI 10.1109/ACSSC.2008.5074826
NR 28
TC 14
Z9 14
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1654
EP 1669
DI 10.1109/TMM.2017.2678198
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800020
DA 2024-07-18
ER

PT J
AU Ma, ZG
   Chang, XJ
   Yang, Y
   Sebe, N
   Hauptmann, AG
AF Ma, Zhigang
   Chang, Xiaojun
   Yang, Yi
   Sebe, Nicu
   Hauptmann, Alexander G.
TI The Many Shades of Negativity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attribute representation; attribute selection; complex event detection;
   selective fine-grained labeling
ID EVENT DETECTION; RECOGNITION; FUSION
AB Complex event detection has been progressively researched in recent years for the broad interest of video indexing and retrieval. To fulfill the purpose of event detection, one needs to train a classifier using both positive and negative examples. Current classifier training treats the negative videos as equally negative. However, we notice that many negative videos resemble the positive videos in different degrees. Intuitively, we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning. Aiming for this, we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event. Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation. The resulting fine-grained labels may be not optimal to capture the discriminative cues from the negative videos. Hence, we propose to jointly optimize the fine-grained labels with the classifier learning, which brings mutual reciprocality. Meanwhile, the labels of positive examples are supposed to remain unchanged. We thus additionally introduce a constraint for this purpose. On the other hand, the state-of-the-art deep convolutional neural network features are leveraged in our approach for event detection to further boost the performance. Extensive experiments on the challenging TRECVID MED 2014 dataset have validated the efficacy of our proposed approach.
C1 [Ma, Zhigang; Chang, Xiaojun; Hauptmann, Alexander G.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Yang, Yi] Univ Technol Sydney, Ultimo, NSW 2007, Australia.
   [Sebe, Nicu] Univ Trento, I-38122 Trento, Italy.
C3 Carnegie Mellon University; University of Technology Sydney; University
   of Trento
RP Chang, XJ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM kevinma@cs.cmu.edu; uqxchan1@cs.cmu.edu; yi.yang@uts.edu.au;
   sebe@disi.unitn.it; alex@cs.cmu.edu
RI Yang, Yi/B-9273-2017; yang, yang/GWB-9426-2022; Chang,
   Xiaojun/A-2055-2015; yang, yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022;
   Sebe, Niculae/KEC-2000-2024; yang, yang/GVT-5210-2022
OI Yang, Yi/0000-0002-0512-880X; Chang, Xiaojun/0000-0002-7778-8807; Sebe,
   Niculae/0000-0002-6597-7248; 
FU National Science Foundation [IIS-1251187]; Intelligence Advanced
   Research Projects Activity via Department of Interior National Business
   Center [D11PC20068]; Data to Decisions Cooperative Research Centre;
   Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [1251187] Funding Source: National Science
   Foundation
FX This work was supported in part by the National Science Foundation under
   Grant IIS-1251187, in part by the Intelligence Advanced Research
   Projects Activity via Department of Interior National Business Center
   under Contract D11PC20068, and in part by the Data to Decisions
   Cooperative Research Centre (www.d2dcrc.com.au). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Tao Mei. (Corresponding author: Xiaojun Chang.)
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   [Anonymous], CMU INFORM TRECVID 2
   [Anonymous], P 21 ACM INT C MULT
   [Anonymous], 2012, UCF101 DATASET 101 H
   [Anonymous], TRECV MED 2014 EV PL
   [Anonymous], P 23 ACM INT C MULT
   [Anonymous], 2015, CoRR
   [Anonymous], INFORM TRECVID 2014
   Bhattacharya S, 2014, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2014.287
   Chang XJ, 2016, AAAI CONF ARTIF INTE, P3464
   Chang XJ, 2016, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2016.208
   Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheng Y, 2014, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2014.286
   Habibian A, 2013, P 3 ACM C INT C MULT, P89, DOI DOI 10.1145/2461466.2461482
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Jiang L, 2014, ADV NEUR IN, V27
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Ma ZG, 2014, IEEE T PATTERN ANAL, V36, P1789, DOI 10.1109/TPAMI.2014.2306419
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Ricci E, 2013, IEEE T PATTERN ANAL, V35, P513, DOI 10.1109/TPAMI.2012.131
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Simonyan K., 2014, CORR
   Vahdat A, 2013, IEEE I CONF COMP VIS, P1185, DOI 10.1109/ICCV.2013.463
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Xu D, 2008, IEEE T PATTERN ANAL, V30, P1985, DOI 10.1109/TPAMI.2008.129
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang Y., 2011, P 22 INT JOINT C ART, P1589
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
NR 34
TC 61
Z9 61
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1558
EP 1568
DI 10.1109/TMM.2017.2659221
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800013
DA 2024-07-18
ER

PT J
AU Bai, H
   Li, S
   He, XX
AF Bai, Huang
   Li, Sheng
   He, Xiongxiong
TI Sensing Matrix Optimization Based on Equiangular Tight Frames With
   Consideration of Sparse Representation Error
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressed sensing (CS); equiangular tight frames (ETFs); mutual
   coherence; sensing matrix; sparse representation; tightness
ID SIGNAL RECOVERY; DICTIONARIES; PROJECTIONS; DESIGN
AB This paper deals with the sensingmatrix optimization problem for compressed sensing (CS) systems. Traditionally, the optimal sensing matrix is designed such that the Gram of the equivalent dictionary defined as the product of the sensing matrix and the dictionary is as close to a target Gram with some proper properties as possible. In this study, the sensing matrix is designed to make the equivalent dictionary approximate to a certain target frame. In addition, to avoid the sparse representation error (SRE) to be amplified in the measurement domain, a penalty term related to the SRE is included in the design criterion. An alternating minimization algorithm is proposed to solve the optimum sensing matrix problem, where the target frame is taken as the relaxed equiangular tight frame, which is constructed with a new method with the purpose of reducing the mutual coherence and maintaining the tightness of the frame, then the solution of the optimal sensing matrix is derived analytically with the target frame fixed. Experiments are carried out with synthetic data and real images, which demonstrate promising performance of the proposed algorithms and superiority of the CS system designed with the optimized sensing matrix to existing ones in terms of signal reconstruction accuracy.
C1 [Bai, Huang; Li, Sheng; He, Xiongxiong] Zhejiang Univ Technol, Coll Informat Engn, Zhejiang Prov Key Lab Signal Proc, Hangzhou 310023, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology
RP Bai, H (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Zhejiang Prov Key Lab Signal Proc, Hangzhou 310023, Zhejiang, Peoples R China.
EM bh667770@163.com; shengli@zjut.edu.cn; hxx@zjut.edu.cn
RI Li, Sheng/AAA-1540-2022
OI Li, Sheng/0000-0003-2144-958X; Bai, Huang/0000-0003-1875-468X
FU Natural Science Foundation of China [61273195, 61503339, 61473262,
   61503330]; Zhejiang Natural Science Foundation [LY13F010009,
   LQ14F030008]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61273195, Grant 61503339, Grant 61473262, and Grant
   61503330, and in part by the Zhejiang Natural Science Foundation under
   Grant LY13F010009 and Grant LQ14F030008. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shahram Shirani.
CR Abolghasemi V, 2012, SIGNAL PROCESS, V92, P999, DOI 10.1016/j.sigpro.2011.10.012
   Abolghasemi V, 2010, EUR SIGNAL PR CONF, P427
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Benedetto JJ, 2003, ADV COMPUT MATH, V18, P357, DOI 10.1023/A:1021323312367
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chen W, 2012, IEEE SIGNAL PROC LET, V19, P8, DOI 10.1109/LSP.2011.2173675
   Chrétien S, 2010, IEEE SIGNAL PROC LET, V17, P181, DOI 10.1109/LSP.2009.2034554
   Christensen O., 2003, INTRO FRAMES RIESZ B, V2nd
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P278, DOI 10.1109/TMM.2011.2181491
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Duarte MF, 2011, IEEE T SIGNAL PROCES, V59, P4053, DOI 10.1109/TSP.2011.2161982
   Duarte-Carvajalino JM, 2009, IEEE T IMAGE PROCESS, V18, P1395, DOI 10.1109/TIP.2009.2022459
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   Fickus M, 2012, LINEAR ALGEBRA APPL, V436, P1014, DOI 10.1016/j.laa.2011.06.027
   Gunawardana A, 2005, J MACH LEARN RES, V6, P2049
   Horn R. A., 2012, MATRIX ANAL
   Li G, 2013, IEEE T SIGNAL PROCES, V61, P2887, DOI 10.1109/TSP.2013.2253776
   Liu HX, 2014, IEEE T MULTIMEDIA, V16, P1549, DOI 10.1109/TMM.2014.2328324
   Pudlewski S, 2013, IEEE T MULTIMEDIA, V15, P2072, DOI 10.1109/TMM.2013.2280245
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Rusu Cristian, 2015, CORR
   Strohmer T, 2003, APPL COMPUT HARMON A, V14, P257, DOI 10.1016/S1063-5203(03)00023-X
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Tropp JA, 2005, IEEE T INFORM THEORY, V51, P188, DOI 10.1109/TIT.2004.839492
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Tsiligianni EV, 2014, IEEE T INFORM THEORY, V60, P2319, DOI 10.1109/TIT.2014.2308171
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu JX, 2011, ERGOD THEOR DYN SYST, V31, P599, DOI 10.1017/S0143385709001114
   Yaghoobi M, 2009, IEEE T SIGNAL PROCES, V57, P4800, DOI 10.1109/TSP.2009.2026610
   Zelnik-Manor L, 2011, IEEE T SIGNAL PROCES, V59, P4300, DOI 10.1109/TSP.2011.2159211
   Zibulevsky M, 2010, IEEE SIGNAL PROC MAG, V27, P76, DOI 10.1109/MSP.2010.936023
NR 34
TC 22
Z9 24
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2040
EP 2053
DI 10.1109/TMM.2016.2595261
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800011
DA 2024-07-18
ER

PT J
AU Guo, YA
   Tao, DP
   Cheng, J
   Dougherty, A
   Li, YT
   Yue, K
   Zhang, B
AF Guo, Yanan
   Tao, Dapeng
   Cheng, Jun
   Dougherty, Alan
   Li, Yaotang
   Yue, Kun
   Zhang, Bob
TI Tensor Manifold Discriminant Projections for Acceleration-Based Human
   Activity Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Accelerometers; human activity recognition; multimedia big data; rank
   order; tensor representation
ID FRAMEWORK; SPARSE
AB With the rapid development of wearable sensors and pervasive computing technologies including smartphones, acceleration-based human activity recognition is receiving increased attention for medical research applications. Motivated by the "weightlessness" feature, here we apply a bidirectional feature during the feature extraction phase of activity recognition; however, since the bidirectional feature has two components, they cannot simply be concatenated into a long vector, but can be naturally treated as a second-order tensor. Therefore, we propose a new tensor-based feature selection method termed tensor manifold discriminant projections (TMDP). TMDP simultaneously considers: 1) applying an optimization criterion that can directly process the tensor spectral analysis problem, thereby decreasing the computational cost compared to traditional tensor-based feature selection methods; 2) extracting local rank information by finding a tensor subspace that preserves the rank order information of the within-class input samples; and 3) extracting discriminant information by maximizing the sum of distances between every sample and their interclass sample mean. Experiments on the naturalistic mobile devices-based human activity 2.0 dataset are performed to demonstrate the effectiveness and robustness of TMDP.
C1 [Guo, Yanan; Li, Yaotang] Yunnan Univ, Sch Math & Stat, Kunming 650091, Peoples R China.
   [Tao, Dapeng; Yue, Kun] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
   [Cheng, Jun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Lab Human Machine Control, Shenzhen 518055, Peoples R China.
   [Cheng, Jun] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
   [Dougherty, Alan] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
   [Zhang, Bob] Macau Univ, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 Yunnan University; Yunnan University; Chinese Academy of Sciences;
   Shenzhen Institute of Advanced Technology, CAS; Chinese University of
   Hong Kong; Hong Kong Polytechnic University; University of Macau
RP Guo, YA (corresponding author), Yunnan Univ, Sch Math & Stat, Kunming 650091, Peoples R China.
EM yananguo.ynu@qq.com; dtao.scut@gmail.com; jun.cheng@siat.ac.cn;
   alan.dougherty@gmail.com; liyaotang@ynu.edu.cn; kyue@ynu.edu.cn;
   bobzhang@umac.mo
RI Tao, Dapeng/E-8649-2013; Li, Yao/AAE-3550-2019; Zhang,
   Bob/ABD-5926-2021; Zhang, Bob/HIR-3656-2022
OI Zhang, Bob/0000-0003-2497-9519; Zhang, Bob/0000-0001-6512-0474;
   DOUGHERTY, Alan William/0000-0002-9315-6586; guo,
   yanan/0000-0001-8592-6993
FU National Natural Science Foundation of China [61572486, 11361074];
   Natural Science Foundation of Yunnan Province [2016FB105, 2014FA023];
   Guangdong Natural Science Funds [2014A030310252]; Guangdong Science
   Technology Project [2013B010202004]; Shenzhen Technology Project
   [JCYJ20140901003939001, JSGG20160331185256983, JSGG20140703092631382,
   JCYJ20140901003939013]; Program for Excellent Young Talents of Yunnan
   University; State Key Laboratory of Digital Publishing Technology; Hong
   Kong Ph.D. Fellowship Scheme [PF14-19094]; Science and Technology
   Development Fund (FDCT) of Macao SAR [FDCT/128/2013/A,
   FDCT/124/2014/A3]; University of Macau [MRG026/ZYB/2015/FST]; Special
   Program of Guangdong Frontier and Key Technological Innovation
   [2016B010108010]; Key Laboratory of Human-Machine Intelligence-Synergy
   Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of
   Sciences [2014DP173025]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61572486 and Grant 11361074, in part by
   the Natural Science Foundation of Yunnan Province under Grant 2016FB105
   and Grant 2014FA023, in part by the Guangdong Natural Science Funds
   under Grant 2014A030310252, in part by the Guangdong Science Technology
   Project under Grant 2013B010202004, in part by the Shenzhen Technology
   Project under Grant JCYJ20140901003939001, Grant JSGG20160331185256983,
   Grant JSGG20140703092631382, and Grant JCYJ20140901003939013, in part by
   the Program for Excellent Young Talents of Yunnan University, in part by
   the Opening Project of State Key Laboratory of Digital Publishing
   Technology, in part by the Hong Kong Ph.D. Fellowship Scheme with
   reference number PF14-19094, in part by the Science and Technology
   Development Fund (FDCT) of Macao SAR FDCT/128/2013/A and
   FDCT/124/2014/A3, in part by the University of Macau
   MRG026/ZYB/2015/FST, in part by the Special Program of Guangdong
   Frontier and Key Technological Innovation under Grant 2016B010108010,
   and in part by the Key Laboratory of Human-Machine Intelligence-Synergy
   Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of
   Sciences, under Grant 2014DP173025. The guest editor team coordinated
   the review of this manuscript and approved it for publication.
CR Abbate S, 2012, PERVASIVE MOB COMPUT, V8, P883, DOI 10.1016/j.pmcj.2012.08.003
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Bian W, 2008, INT C PATT RECOG, P160, DOI 10.1109/ICPR.2008.4760987
   Chao JS, 2016, IEEE T MULTIMEDIA, V18, P25, DOI 10.1109/TMM.2015.2502552
   Duda RO., 2012, Pattern classificatio
   Farringdon J., 1999, Digest of Papers. Third International Symposium on Wearable Computers, P107, DOI 10.1109/ISWC.1999.806681
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   He DL, 2015, IEEE T MULTIMEDIA, V17, P1658, DOI 10.1109/TMM.2015.2451956
   He X. F., 2005, P ADV NEUR INF PROC
   He ZY, 2008, INT C PATT RECOG, P1401
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Jia Y., 2009, 2009 2 INT C INTELLI, P693, DOI [DOI 10.1109/ICINIS.2009.177, 10.1109/ICINIS.2009.177]
   Kawsar F, 2015, P INT COMP SOFTW APP, P44, DOI 10.1109/COMPSAC.2015.201
   Kwapisz JR., 2011, ACM SIGKDD EXPLORATI, V12, P74, DOI [DOI 10.1145/1964897.1964918, 10.1145/1964897.1964918]
   Lara OD, 2013, IEEE COMMUN SURV TUT, V15, P1192, DOI 10.1109/SURV.2012.110112.00192
   Lathauwer L. D., 1997, THESIS
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lester J., 2006, PERVASIVE, V2006, P1, DOI DOI 10.1007/11748625_
   Lim LH, 2005, IEEE CAMSAP 2005: FIRST INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING, P129
   Liu TL, 2017, IEEE T PATTERN ANAL, V39, P227, DOI 10.1109/TPAMI.2016.2544314
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Livatino S, 2012, IEEE T IND INFORM, V8, P69, DOI 10.1109/TII.2011.2174062
   Lu HP, 2008, IEEE T NEURAL NETWOR, V19, P18, DOI 10.1109/TNN.2007.901277
   Lu XQ, 2013, IEEE T NEUR NET LEAR, V24, P929, DOI 10.1109/TNNLS.2013.2245914
   Lu XQ, 2012, IEEE T SYST MAN CY B, V42, P939, DOI 10.1109/TSMCB.2012.2185490
   Mannini A, 2015, PERVASIVE MOB COMPUT, V21, P62, DOI 10.1016/j.pmcj.2015.06.003
   Maurer U, 2006, BSN 2006: INTERNATIONAL WORKSHOP ON WEARABLE AND IMPLANTABLE BODY SENSOR NETWORKS, PROCEEDINGS, P113
   Nappi M, 2014, IEEE T SYST MAN CY-S, V44, P1457, DOI 10.1109/TSMC.2014.2337851
   Nham B., 2008, MACH LEARN FINAL PRO
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Samanta S, 2014, IEEE T MULTIMEDIA, V16, P1525, DOI 10.1109/TMM.2014.2326734
   Samuel A, 2015, IEEE T MULTIMEDIA, V17, P1484, DOI 10.1109/TMM.2015.2458299
   Shi XS, 2015, IEEE T IMAGE PROCESS, V24, P1341, DOI 10.1109/TIP.2015.2405474
   Si H., 2005, UB MET URB LIF WORKS
   Swartz AM, 2000, MED SCI SPORT EXER, V32, pS450, DOI 10.1097/00005768-200009001-00003
   Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70
   Tao DP, 2016, IEEE T NEUR NET LEAR, V27, P1392, DOI 10.1109/TNNLS.2014.2357794
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Thurau C, 2007, LECT NOTES COMPUT SC, V4814, P299
   Tian DY, 2016, IEEE T IMAGE PROCESS, V25, P961, DOI 10.1109/TIP.2015.2509418
   Wang DM, 2016, BIOMED SIGNAL PROCES, V23, P62, DOI 10.1016/j.bspc.2015.08.002
   Wang RX, 2016, IEEE T IMAGE PROCESS, V25, P2117, DOI 10.1109/TIP.2016.2541318
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Ye J, 2004, ADV NEURAL INFORM PR, P1569
   Yin J, 2008, IEEE T KNOWL DATA EN, V20, P1082, DOI 10.1109/TKDE.2007.1042
   Yu ZW, 2015, IEEE ACM T COMPUT BI, V12, P887, DOI 10.1109/TCBB.2014.2359433
   Yu ZW, 2014, IEEE ACM T COMPUT BI, V11, P727, DOI 10.1109/TCBB.2014.2315996
   Zhang LF, 2015, PATTERN RECOGN, V48, P3102, DOI 10.1016/j.patcog.2014.12.016
   Zhang LF, 2015, SIGNAL PROCESS, V106, P245, DOI 10.1016/j.sigpro.2014.08.005
   Zhang LP, 2013, IEEE T GEOSCI REMOTE, V51, P242, DOI [10.1109/TGRS.2011.2180392, 10.1109/TGRS.2012.2197860]
   Zhang TH, 2009, IEEE T KNOWL DATA EN, V21, P1299, DOI 10.1109/TKDE.2008.212
NR 53
TC 21
Z9 21
U1 1
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 1977
EP 1987
DI 10.1109/TMM.2016.2597007
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800006
DA 2024-07-18
ER

PT J
AU Lu, D
   Liu, XX
   Qian, XM
AF Lu, Dan
   Liu, Xiaoxiao
   Qian, Xueming
TI Tag-Based Image Search by Social Re-ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image search; re-ranking; social clues; social media; tag-based image
   retrieval
ID RETRIEVAL; RELEVANCE; FEATURES
AB Social media sharing websites like Flickr allow users to annotate images with free tags, which significantly contribute to the development of the web image retrieval and organization. Tag-based image search is an important method to find images contributed by social users in such social websites. However, how to make the top ranked result relevant and, with diversity, is challenging. In this paper, we propose a social re-ranking system for tag-based image retrieval with the consideration of an image's relevance and diversity. We aim at re-ranking images according to their visual information, semantic information, and social clues. The initial results include images contributed by different social users. Usually each user contributes several images. First, we sort these images by inter-user re-ranking. Users that have higher contribution to the given query rank higher. Then we sequentially implement intra-user re-ranking on the ranked user's image set, and only the most relevant image from each user's image set is selected. These selected images compose the final retrieved results. We build an inverted index structure for the social image dataset to accelerate the searching process. Experimental results on a Flickr dataset show that our social re-ranking method is effective and efficient.
C1 [Lu, Dan; Liu, Xiaoxiao; Qian, Xueming] Xi An Jiao Tong Univ, SMILES Lab, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Security, Minist Educ, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Lu, D (corresponding author), Xi An Jiao Tong Univ, SMILES Lab, Xian 710049, Peoples R China.
EM ludandan@stu.xjtu.edu.cn; 776905363@qq.com; qianxm@mail.xjtu.edu.cn
RI Liu, Xiaoxiao/HNI-6180-2023
FU Program 973 [2012CB316400]; National Natural Science Foundation of China
   [61173109, 60903121, 61332018]; Microsoft Research
FX This work was supported in part by the Program 973 under Grant
   2012CB316400, in part by the National Natural Science Foundation of
   China under Grant 61173109, Grant 60903121, and Grant 61332018, and in
   part by Microsoft Research. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Zhen Wen.
CR Agrawal G., 2011, 2011 2nd International Conference on Computer and Communication Technology, P169, DOI 10.1109/ICCCT.2011.6075169
   [Anonymous], IEEE T KNOWLEDGE DAT
   [Anonymous], IEEE INT C MULT EXP
   [Anonymous], 2014, IEEE T KNOWL DATA EN
   [Anonymous], P INT C MULT RETR JU
   [Anonymous], IEEE T MULT IN PRESS
   [Anonymous], 2010, IMPROVING SOCIAL TAG
   [Anonymous], IEEE T CIRC IN PRESS
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P 5 ACM INT C MULT R
   [Anonymous], 2011, Proceedings of the 19th ACM international conference on Multimedia
   [Anonymous], ACM MULT C NEW YORK
   [Anonymous], P INT C MULT RETR
   [Anonymous], SCI WORLD J
   Chen L, 2012, IEEE T MULTIMEDIA, V14, P1057, DOI 10.1109/TMM.2012.2187435
   Chen L, 2010, PROC CVPR IEEE, P3440, DOI 10.1109/CVPR.2010.5539988
   Chen LL, 2014, CHIN CONTR CONF, P4620, DOI 10.1109/ChiCC.2014.6895717
   Dupret G., 2010, Proceedings of the third ACM international conference on Web search and data mining, WSDM '10, P181
   Gu Y, 2015, IEEE T IMAGE PROCESS, V24, P3450, DOI 10.1109/TIP.2015.2443501
   Hu Y., 2008, PROC IEEE C COMPUT V, P1
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Ji SH, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P35, DOI 10.1145/1571941.1571950
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ksibi Amel, 2013, Computer Analysis of Images and Patterns. 15th International Conference, CAIP 2013. Proceedings: LNCS 8048, P571, DOI 10.1007/978-3-642-40246-3_71
   Ksibi A, 2014, INT J MULTIMED INF R, V3, P29, DOI 10.1007/s13735-013-0045-5
   Lee S, 2014, MULTIMED TOOLS APPL, V72, P1363, DOI 10.1007/s11042-013-1439-3
   Li X, 2014, SOFT MATER, V12, P1, DOI 10.1080/1539445X.2011.584269
   Li Xirong., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, MIR '08, P180
   Liu D., 2012, ACM MM'12', P659
   Liu D, 2009, IEEE INT CON MULTI, P1636, DOI 10.1109/ICME.2009.5202833
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Mishra D, 2014, INT J COMPUT SCI NET, V14, P50
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Qi Guo-Jun., 2013, Proceedings of the 22Nd International Conference on World Wide Web, WWW '13, P1041
   Qian XM, 2016, IEEE T IMAGE PROCESS, V25, P195, DOI 10.1109/TIP.2015.2497145
   Qian XM, 2015, IEEE T CIRC SYST VID, V25, P1857, DOI 10.1109/TCSVT.2014.2369731
   Qian XM, 2015, IEEE T IMAGE PROCESS, V24, P4348, DOI 10.1109/TIP.2015.2462131
   Qian XM, 2014, IEEE T CYBERNETICS, V44, P2493, DOI 10.1109/TCYB.2014.2309593
   Qian XM, 2014, MULTIMED TOOLS APPL, V69, P897, DOI 10.1007/s11042-012-1151-8
   Qian XM, 2013, NEUROCOMPUTING, V111, P144, DOI 10.1016/j.neucom.2012.12.021
   Qian XM, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P44, DOI 10.1109/ISM.2009.14
   Song K., 2006, ACM Multimedia, P707
   Sun A., 2009, Proc. SIGMM Workshop on Social Media, P19
   Sun FM, 2012, NEUROCOMPUTING, V95, P40, DOI 10.1016/j.neucom.2011.05.040
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Wang B, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P353, DOI 10.1109/ICME.2006.262509
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang SH, 2012, IEEE T MULTIMEDIA, V14, P1259, DOI 10.1109/TMM.2012.2193120
   Weinberger K.Q., 2008, Proceedings of Conference on Multimedia, P111
   Wu D, 2014, INT SYMP PARAL ARCH, P191, DOI 10.1109/PAAP.2014.26
   Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124
   Yang KY, 2010, LECT NOTES COMPUT SC, V5916, P174, DOI 10.1007/978-3-642-11301-7_20
   Yang XY, 2015, IEEE T IMAGE PROCESS, V24, P1709, DOI 10.1109/TIP.2015.2411433
   Yu J, 2015, IEEE T CYBERNETICS, V45, P767, DOI 10.1109/TCYB.2014.2336697
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu XF, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P223, DOI 10.1145/2600428.2609556
NR 59
TC 49
Z9 50
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1628
EP 1639
DI 10.1109/TMM.2016.2568099
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000015
DA 2024-07-18
ER

PT J
AU López-Oller, D
   Gomez, AM
   Pérez-Córdoba, JL
   Sánchez, V
AF Lopez-Oller, Domingo
   Gomez, Angel M.
   Perez-Cordoba, Jose L.
   Sanchez, Victoria
TI An Error Mitigation Technique for Erasure Channels Based on a Wavelet
   Representation of the Speech Excitation Signal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Erasure channels; error concealment; quantization methods; speech
   coding; speech excitation signal
ID RECOVERY TECHNIQUES; PACKET; PARAMETERS; TIME
AB The importance of packet-based speech transmissions has grown since it offers cheaper and efficient communications. However, frame erasures are a common hurdle in these networks and concealment techniques are necessary to ensure a minimum quality of service. In this paper, we propose a mitigation technique focused on the reconstruction of the linear prediction coding (LPC) coefficients and the excitation signal of the lost frame by using a replacement technique. These replacements are obtained by means of a minimum mean square error estimation based on a source model of the speech parameters (LPC coefficients and the excitation signal). As this approach critically relies on the quantization and representation of the excitation signal, we explore the Haar wavelet transform as a novel approach to represent the excitation signal for error mitigation. Thus, this paper describes how optimal codebook and estimates can be computed in a Haar transformed domain. As a result, the excitation signal of a frame can be decomposed in several partitions where each one is independently reconstructed. Objective and subjective tests are conducted in order to assess the quality of the concealed speech signal resulting from our proposal. Both evaluations confirm noticeable improvements over the default mitigation method included in the two tested standard codecs, adaptive multirate, and Internet low bitrate codec.
C1 [Lopez-Oller, Domingo; Gomez, Angel M.; Perez-Cordoba, Jose L.; Sanchez, Victoria] Univ Granada, Dept Signal Theory Networking & Commun, E-18071 Granada, Spain.
C3 University of Granada
RP López-Oller, D (corresponding author), Univ Granada, Dept Signal Theory Networking & Commun, E-18071 Granada, Spain.
EM domingolopez@ugr.es; amgg@ugr.es; jlpc@ugr.es; victoria@ugr.es
RI García, Angel Manuel Gómez/C-6856-2012; Córdoba, José Luis
   Pérez/E-1015-2012
OI García, Angel Manuel Gómez/0000-0002-9995-3068; Córdoba, José Luis
   Pérez/0000-0002-1686-2401; Sanchez Calle, Victoria
   Eugenia/0000-0003-1546-9728
FU Ministerio de Economia y Competitividad; Fondo Europeo de Desarrollo
   Regional FEDER [TEC2013-46690-P]; FPI grant from Spanish Ministry of
   Education
FX This work was supported by Ministerio de Economia y Competitividad, by
   Fondo Europeo de Desarrollo Regional FEDER under Project
   TEC2013-46690-P, and by an FPI grant from the Spanish Ministry of
   Education. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Xiao-Ping Zhang.
CR Agiomyrginanakis Y, 2005, INT CONF ACOUST SPEE, P141
   Andersen SV, 2002, 2002 IEEE SPEECH CODING WORKSHOP PROCEEDINGS, P23, DOI 10.1109/SCW.2002.1215711
   [Anonymous], 2004, 3951 IR
   Aoki N., 2004, TENCON 2004. 2004 IEEE Region 10 Conference (IEEE Cat. No. 04CH37582), P52
   Boubakir C., 2009, INT MULT SYST SIGN D, P1
   Chibani M, 2007, IEEE T AUDIO SPEECH, V15, P2485, DOI 10.1109/TASL.2007.907332
   Devore J.L., 2001, PROBABILITY STAT ENG, V5th
   Feldbauer C, 2009, IEEE T COMMUN, V57, P2309, DOI 10.1109/TCOMM.2009.08.070351
   Garofolo J., 1988, P NAT I STAND TECHN, P1
   Gomez A., 2003, P EUR, P2733
   Gomez A. M., 2004, COST278 ISCA TUT RES
   Gómez AM, 2006, IEEE T WIREL COMMUN, V5, P2555, DOI [10.1109/TWC.2006.1687779, 10.1109/TWC.2006.04478]
   Gómez AM, 2011, IEEE T MULTIMEDIA, V13, P894, DOI 10.1109/TMM.2011.2156773
   Gómez AM, 2010, IEEE T AUDIO SPEECH, V18, P1258, DOI 10.1109/TASL.2009.2031798
   Gournay P, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P108
   Liao WT, 2001, IEEE INFOCOM SER, P815, DOI 10.1109/INFCOM.2001.916272
   Lindblom J, 2002, INT CONF ACOUST SPEE, P173
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   López-Oller D, 2014, EUR SIGNAL PR CONF, P1242
   Ma Z., 2014, IEEE INT C AC SPEECH, P6929
   Mandatory Speech Codec Speech Processing Functions; Adaptive Multi- Rate (AMR) Speech Codec, 1999, 26090 3GPP TS
   Martin R, 2001, INT CONF ACOUST SPEE, P729, DOI 10.1109/ICASSP.2001.941018
   Merazka F., 2013, 2013 CONST INT WORKS, P1
   Merazka F, 2012, IEEE IND ELEC, P1495, DOI 10.1109/IECON.2012.6388521
   Method for the Subjective Assessment of Intermediate, 2001, BS15341 IR
   Moreno A., 2000, ELRA S0089 ALBAYZIN
   Paliwal K.K., 1995, SPEECH SYNTHESIS COD, P433
   Perceptual Evaluation of Speech Quality (PESQ), 2001, P862 IR
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   Rodbro CA, 2006, IEEE T AUDIO SPEECH, V14, P1609, DOI 10.1109/TSA.2005.858561
   Schroeder M., 1985, IEEE International Conference on Acoustics, Speech, and Signal Processing, V10, P937
   Serizawa M, 2002, 2002 IEEE SPEECH CODING WORKSHOP PROCEEDINGS, P68, DOI 10.1109/SCW.2002.1215726
   Serizawa M, 2002, INT CONF ACOUST SPEE, P169
   Soong F., 1984, P IEEE INT C AC SPEE, V9, P37
   SUGAMURA N, 1988, IEEE J SEL AREA COMM, V6, P432, DOI 10.1109/49.618
   TOYOSHIMA M, 2014, P AS PAC C CIRC SYST, P89
   Vaidynathan P., 2008, THEORY LINEAR PREDIC
   Walker J.S., 1999, ST ADV MATH
   Zavarehei E, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING, VOLS 1 AND 2, P14, DOI 10.1109/ASRU.2007.4430076
   Zhang GQ, 2008, INT CONF ACOUST SPEE, P4797
NR 40
TC 1
Z9 1
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1245
EP 1256
DI 10.1109/TMM.2016.2561840
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600002
DA 2024-07-18
ER

PT J
AU Siahaan, E
   Hanjalic, A
   Redi, J
AF Siahaan, Ernestasia
   Hanjalic, Alan
   Redi, Judith
TI A Reliable Methodology to Collect Ground Truth Data of Image Aesthetic
   Appeal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computational aesthetics; crowdsourcing; image aesthetic appeal; quality
   of experience (QoE); subjective quality assessment
ID QUALITY ASSESSMENT; VIDEO
AB Recognizing what makes an image aesthetically pleasing is crucial to the effectiveness of many multimedia systems. Several works have attempted to build image aesthetic appeal predictors, and created their own set of ground truth data for the purpose, either by using rated images from photo sharing websites, or by asking a pool of users to rate images in lab or crowdsourcing experiments. Literature has shown that the way these experiments are conducted can influence their results: poor experimental setup can result in poorly reliable outcomes (i.e., highly imprecise aesthetic appeal measures). A question then arises whether the different choices made to collect ground truth of aesthetic appeal data are appropriate. In this paper, we propose a systematic study that looks into how different experimental environments and rating scales used to collect image aesthetic appeal ground truth data influence the reliability and repeatability of aesthetic appeal assessments. Our findings show that discrete and continuous scales with five-point absolute category rating labels yield more reliable results, with the continuous scale being more reliable for abstract images. We also show that image aesthetic appeal assessments could be repeatable across different experimental environments (i.e., lab and crowdsourcing). We finally formulate concrete recommendations to guide the collection of large sets of ground truth data for training models of aesthetic appeal appreciation.
C1 [Siahaan, Ernestasia; Hanjalic, Alan; Redi, Judith] Delft Univ Technol, Multimedia Comp Grp, Intelligent Syst Dept, NL-2628 CD Delft, Netherlands.
C3 Delft University of Technology
RP Siahaan, E (corresponding author), Delft Univ Technol, Multimedia Comp Grp, Intelligent Syst Dept, NL-2628 CD Delft, Netherlands.
EM E.Siahaan@tudelft.nl; A.Hanjalic@tudelft.nl; J.A.Redi@tudelft.nl
OI Hanjalic, Alan/0000-0002-5771-2549
FU Netherlands Organisation for Scientific Research (NWO) under Veni Grant
   [639.021.230]
FX This work was supported in part by the Netherlands Organisation for
   Scientific Research (NWO) under Veni Grant 639.021.230. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Xiaokang Yang.
CR Agrawal A, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON INFORMATION SYSTEMS AND COMPUTER NETWORKS (ISCON), P24, DOI 10.1109/ICISCON.2013.6524167
   [Anonymous], 2008, Subjective video quality assessment methods for multimedia applications.
   [Anonymous], 2012, Methodology for the subjective assessment of the quality of television pictures
   [Anonymous], 2013, P 21 ACM INT C MULTI
   [Anonymous], 2011, MM 11
   [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   Brunnstrom K., 2013, HAL00977812 QUALINET
   Cerosaletti C. D., 2013, P INT WORKSH QUAL MU, P47
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   de Ridder H, 2001, J ELECTRON IMAGING, V10, P47, DOI 10.1117/1.1335529
   Gescheider G. A., 2013, Psychophysics: the fundamentals
   Hallgren Kevin A, 2012, Tutor Quant Methods Psychol, V8, P23
   Hossfeld T., 2014, 204797 EPFL QUALINET
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Janowski Lucjan, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P251, DOI 10.1109/QoMEX.2014.6982327
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Keelan B., 2002, Handbook of Image Quality: Characterization and Prediction
   Keelan BW, 2004, P SOC PHOTO-OPT INS, V5294, P181
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Pinson MH, 2012, IEEE J-STSP, V6, P640, DOI 10.1109/JSTSP.2012.2215306
   Quan HT, 2011, IEEE T BROADCAST, V57, P1, DOI 10.1109/TBC.2010.2086750
   Redi J., 2013, Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia (CrowdMM '13), P29
   Redi J. A., 2013, P SOC PHOTO-OPT INS, V8651
   Redi J, 2010, PROC SPIE, V7529, DOI 10.1117/12.839195
   Redi JA, 2012, PROC SPIE, V8291, DOI 10.1117/12.911894
   ROUFS JAJ, 1992, PHILIPS J RES, V47, P35
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P1231, DOI 10.1109/TMM.2013.2261481
   Sachs TS, 2011, LECT NOTES COMPUT SC, V6469, P112
   Siahaan Ernestasia, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P245, DOI 10.1109/QoMEX.2014.6982326
   Sijtsma K, 2009, PSYCHOMETRIKA, V74, P107, DOI 10.1007/s11336-008-9101-0
   SIMS K, 1991, COMP GRAPH, V25, P319, DOI 10.1145/127719.122752
   Staelens N, 2010, IEEE T BROADCAST, V56, P458, DOI 10.1109/TBC.2010.2067710
   Stout J., 2009, LIGHTWEIGHT EVOLUTIO
   Tang X., 2009, IEEE T MULTIMEDIA, V15, P1930
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Tingting Zhang, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P81, DOI 10.1109/QoMEX.2014.6982300
   Tkalcic Marko, 2012, Advances in User Modeling. UMAP 2011 Workshops. Revised Selected Papers, P342, DOI 10.1007/978-3-642-28509-7_32
   Tominaga Toshiko, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P82, DOI 10.1109/QOMEX.2010.5517948
   VANDIJK AM, 1995, P SOC PHOTO-OPT INS, V2451, P90, DOI 10.1117/12.201231
   Vessel EA, 2010, J VISION, V10, DOI 10.1167/10.2.18
   Winkler S, 2009, INT WORK QUAL MULTIM, P139, DOI 10.1109/QOMEX.2009.5246961
   Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043
   Yadati K, 2014, IEEE T MULTIMEDIA, V16, P15, DOI 10.1109/TMM.2013.2282128
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
NR 50
TC 22
Z9 23
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1338
EP 1350
DI 10.1109/TMM.2016.2559942
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600010
DA 2024-07-18
ER

PT J
AU Gu, K
   Wang, SQ
   Yang, H
   Lin, WS
   Zhai, GT
   Yang, XK
   Zhang, WJ
AF Gu, Ke
   Wang, Shiqi
   Yang, Huan
   Lin, Weisi
   Zhai, Guangtao
   Yang, Xiaokang
   Zhang, Wenjun
TI Saliency-Guided Quality Assessment of Screen Content Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment (IQA); screen content images (SCI); visual
   saliency
ID VISUAL-ATTENTION; NEURAL MECHANISMS; MODEL; INFORMATION
AB With the widespread adoption of multidevice communication, such as telecommuting, screen content images (SCIs) have become more closely and frequently related to our daily lives. For SCIs, the tasks of accurate visual quality assessment, high-efficiency compression, and suitable contrast enhancement have thus currently attracted increased attention. In particular, the quality evaluation of SCIs is important due to its good ability for instruction and optimization in various processing systems. Hence, in this paper, we develop a new objective metric for research on perceptual quality assessment of distorted SCIs. Compared to the classical MSE, our method, which mainly relies on simple convolution operators, first highlights the degradations in structures caused by different types of distortions and then detects salient areas where the distortions usually attract more attention. A comparison of our algorithm with the most popular and state-of-the-art quality measures is performed on two new SCI databases (SIQAD and SCD). Extensive results are provided to verify the superiority and efficiency of the proposed IQA technique.
C1 [Gu, Ke; Yang, Huan; Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Gu, Ke; Zhai, Guangtao; Yang, Xiaokang; Zhang, Wenjun] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
   [Wang, Shiqi] Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Nanyang Technological University; Shanghai Jiao Tong University; Peking
   University
RP Gu, K; Yang, H; Lin, WS (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.; Zhai, GT; Yang, XK; Zhang, WJ (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.; Wang, SQ (corresponding author), Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM guke.doctor@gmail.com; sqwang1986@gmail.com; hyang3@e.ntu.edu.sg;
   wslin@ntu.edu.sg; zhaiguangtao@gmail.com; xkyang@sjtu.edu.cn;
   zhangwenjun@sjtu.edu.cn
RI Gu, Ke/AAJ-9684-2021; Zhang, Wenjun/GNH-2095-2022; Lin,
   Weisi/A-8011-2012; Lin, Weisi/A-3696-2011; Yang, Xiaokang/C-6137-2009;
   Zhai, Guangtao/X-5949-2019
OI Zhang, Wenjun/0000-0002-5282-3725; Lin, Weisi/0000-0001-9866-1947; Yang,
   Xiaokang/0000-0003-4029-3322; Zhai, Guangtao/0000-0001-8165-9322; Wang,
   Shiqi/0000-0002-3583-959X
FU Singapore MoE Tier 1 Project [M4011379, RG141/14]; National Science
   Foundation of China [61527804, 61305011]
FX This work was supported in part by the Singapore MoE Tier 1 Project
   under Grant M4011379 and Grant RG141/14, and in part by the National
   Science Foundation of China under Grant 61527804 and Grant 61305011. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Maria Martini.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2012, BT50013 ITU
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Engelke U, 2011, IEEE SIGNAL PROC MAG, V28, P50, DOI 10.1109/MSP.2011.942473
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Gu K., NEUROCOMPUT IN PRESS
   Gu K., IEEE T BROA IN PRESS
   Gu K, 2015, IEEE INT SYMP CIRC S, P125, DOI 10.1109/ISCAS.2015.7168586
   Gu K, 2014, IEEE IMAGE PROC, P506, DOI 10.1109/ICIP.2014.7025101
   Gu K, 2015, IEEE SIGNAL PROC LET, V22, P1552, DOI 10.1109/LSP.2015.2413944
   Gu K, 2013, IEEE INT CON MULTI
   Gu K, 2012, IEEE INT WORKSH MULT, P313, DOI 10.1109/MMSP.2012.6343461
   Henderson JM, 2003, TRENDS COGN SCI, V7, P498, DOI 10.1016/j.tics.2003.09.006
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Huan Yang, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P257, DOI 10.1109/QoMEX.2014.6982328
   Huang CY, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2537855
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jahne B., 1999, HDB COMPUTER VISION, VVolume 2, P125
   Kim C, 2013, J VISION, V13, DOI 10.1167/13.4.5
   Lai CF, 2013, IEEE T MULTIMEDIA, V15, P747, DOI 10.1109/TMM.2013.2240270
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Min XK, 2014, IEEE INT SYMP CIRC S, P894, DOI 10.1109/ISCAS.2014.6865280
   Pan ZT, 2013, IEEE T CIRC SYST VID, V23, P949, DOI 10.1109/TCSVT.2013.2243056
   Pelli DG, 2008, NAT NEUROSCI, V11, P1129, DOI 10.1038/nn.2187
   Putnam NM, 2005, J VISION, V5, P632, DOI 10.1167/5.7.3
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shen H. L., 2009, P ISES WORLD C 2007, VI-V, P1
   Shi S, 2015, 2015 Picture Coding Symposium (PCS) with 2015 Packet Video Workshop (PV), P75, DOI 10.1109/PCS.2015.7170050
   Thiele A, 2002, SCIENCE, V295, P2460, DOI 10.1126/science.1068788
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang SX, 2013, IEEE T MULTIMEDIA, V15, P870, DOI 10.1109/TMM.2013.2240674
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
NR 52
TC 227
Z9 237
U1 3
U2 90
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1098
EP 1110
DI 10.1109/TMM.2016.2547343
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100013
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Chen, Y
   Zhao, X
   Zhang, L
   Kang, JW
AF Chen, Ying
   Zhao, Xin
   Zhang, Li
   Kang, Je-Won
TI Multiview and 3D Video Compression Using Neighboring Block Based
   Disparity Vectors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D-AVC; 3D-HEVC; 3D video coding; disparity vector (DV); multiview
   compatibility; multiview video coding; neighboring block based disparity
   vector derivation (NBDV)
ID VIEW INTERPOLATION; DEPTH
AB Compression of the statistical redundancy among different viewpoints, i.e., inter-view redundancy, is a fundamental and critical problem in multiview and three-dimensional (3D) video coding. To exploit the inter-view redundancy, disparity vectors are required to identify pixels of the same objects within two different views; in this way, the enhancement coding tools can be efficiently employed as new modes in block-based video codecs to achieve higher compression efficiency. Although disparity can be converted from depth, it is not possible in multiview video coding since depth information is not considered. Even when depth information is coded, it breaks the so-called multiview compatibility wherein texture views can be decoded without depth information. To resolve this problem, in this paper, a neighboring block-based disparity vector derivation (NBDV) method is proposed. The basic concept of NBDV is to derive a disparity vector (DV) of a current block by utilizing the motion information of spatially and temporally neighboring blocks predicted from another view. Through extensive experiments and analysis, it is shown that the proposed NBDV method achieves efficient DV derivation in the state-of-art video codecs, and it keeps the multiview compatibility with a relatively lower complexity. The proposed method has become an essential part of the 3D video standard extensions of H.264/AVC and HEVC.
C1 [Chen, Ying; Zhao, Xin; Zhang, Li; Kang, Je-Won] Qualcomm Technol Inc, San Diego, CA 92121 USA.
   [Kang, Je-Won] Ewha Womans Univ, Seoul 120750, South Korea.
C3 Qualcomm; Ewha Womans University
RP Zhang, L (corresponding author), Qualcomm Technol Inc, San Diego, CA 92121 USA.
EM cheny@qti.qualcomm.com; xinzhao@qti.qualcomm.com;
   lizhang@qti.qualcomm.com; jewonk@ewha.ac.kr
RI Kang, Jewon/AAU-9722-2020
CR [Anonymous], 2011, N12035 ISOIEC JTC1SC
   [Anonymous], 2011, N12036 ISOIEC JTC1SC
   Bjontegaard G., 2001, CALCULATION AVERAGE
   Chang Y.-L., 2012, 3 M JOINT COLL TEAM
   Chen Y., 2013, 4 M JOINT COLL TEAM
   Chen Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/786015
   Droese M., 2004, P 3D C 2004 TOK JAP, P213
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Hannuksela M. M., 2013, 6 M JOINT COLL TEAM
   Kang J., 2013, 4 M JOINT COLL TEAM
   Kang J., 2013, 3 M JOINT COLL TEAM
   Kang J., 2012, 2 M JOINT COLL TEAM
   Kauff P, 2007, SIGNAL PROCESS-IMAGE, V22, P217, DOI 10.1016/j.image.2006.11.013
   Lin J.-L., 2012, 1 M JOINT COLL TEAM
   Martinian E., 2006, PICT COD S BEIJ CHIN
   Rusanovskyy D., 2013, 3 M JOINT COLL TEAM
   Schwarz H, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P101, DOI 10.1109/PCS.2012.6213296
   Smolic A, 2008, IEEE IMAGE PROC, P2448, DOI 10.1109/ICIP.2008.4712288
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sung J., 2012, 2 M JOINT COLL TEAM
   Sung J., 2012, 1 M JOINT COLL TEAM
   Tanimoto M, 2006, SIGNAL PROCESS-IMAGE, V21, P454, DOI 10.1016/j.image.2006.03.009
   Tech G., 2013, 5 M JOINT COLL TEAM
   Tech G., 2013, 6 M JOINT COLL TEAM
   Tian D., 2012, 3 M JOINT COLL TEAM
   Tourapis AM, 2004, P SOC PHOTO-OPT INS, V5308, P364, DOI 10.1117/12.524578
   VETRO A, 2004, P 23 PICT COD S PCS, P319
   Yamamoto K, 2007, IEEE T CIRC SYST VID, V17, P1436, DOI 10.1109/TCSVT.2007.903802
   Yang HT, 2009, IEEE T CIRC SYST VID, V19, P887, DOI 10.1109/TCSVT.2009.2017410
   Yea S, 2009, SIGNAL PROCESS-IMAGE, V24, P89, DOI 10.1016/j.image.2008.10.007
   Zhang L., 2013, 6 M JOINT COLL TEAM
   Zhang L., 2012, 1 M JOINT COLL TEAM
   Zhang L., 2013, 4 M JOINT COLL TEAM
   Zhang L., 2014, IEEE INT S CIRC SYST
   Zhang L, 2013, IEEE INT SYMP CIRC S, P1632, DOI 10.1109/ISCAS.2013.6572175
   Zhang N., 2013, 3 M JOINT COLL TEAM
   Zhao X., 2013, 4 M JOINT COLL TEAM
   Zhao X., 2013, 5 M JOINT COLL TEAM
NR 38
TC 28
Z9 30
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 576
EP 589
DI 10.1109/TMM.2016.2525010
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300003
DA 2024-07-18
ER

PT J
AU Das Bhattacharjee, S
   Yuan, JS
   Tan, YP
   Duan, LY
AF Das Bhattacharjee, Sreyasee
   Yuan, Junsong
   Tan, Yap-Peng
   Duan, Ling-Yu
TI Query-Adaptive Small Object Search Using Object Proposals and
   Shape-Aware Descriptors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contour-based descriptor; graph-based search; localization; mobile
   visual search
ID FEATURES; CONTEXT
AB While there has been a significant amount of work on object search and image retrieval, the focus has primarily been on establishing effective models for the whole images, scenes, and objects occupying a large portion of an image. In this paper, we propose to leverage object proposals to identify small and smooth-structured objects in a large image database. Unlike popular methods exploring a coarse image-level pairwise similarity, the search is designed to exploit the similarity measures at the proposal level. An effective graph-based query expansion strategy is designed to assess each of these better matched proposals against all its neighbors within the same image for a precise localization. Combined with a shape-aware feature descriptor EdgeBoW, a set of more insightful edge-weights and node-utility measures, the proposed search strategy can handle varying view angles, illumination conditions, deformation, and occlusion efficiently. Experiments performed on a number of other benchmark datasets show the powerful and superior generalization ability of this single integrated framework in dealing with both clutter-intensive real-life images and poor-quality binary document images at equal dexterity.
C1 [Das Bhattacharjee, Sreyasee; Yuan, Junsong; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Duan, Ling-Yu] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Nanyang Technological University; Peking University
RP Das Bhattacharjee, S; Yuan, JS; Tan, YP (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.; Duan, LY (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM dbhattacharjee@ntu.edu.sg; jsyuan@ntu.edu.sg; eyptan@ntu.edu.sg;
   lingyu@pku.edu.cn
RI Yuan, Junsong/R-4352-2019; Tan, Yap-Peng/A-5158-2011; Bhattacharjee,
   Sreyasee Das/AAU-2313-2020; Yuan, Junsong/A-5171-2011
OI Yuan, Junsong/0000-0002-7901-8793
FU National Research Foundation, Prime Ministers Office, Singapore under
   IDM Futures Funding Initiative
FX The authors would like to thank Y. Kawahara for the very helpful
   discussions and sharing the code of [44]. This research was carried out
   at the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological
   University, Singapore. The ROSE Lab was supported by the National
   Research Foundation, Prime Ministers Office, Singapore, under its IDM
   Futures Funding Initiative and administered by the Interactive and
   Digital Media Program Office.
CR [Anonymous], 2010, P 18 ACM INT C MULT
   [Anonymous], C COMP VIS PATT REC
   [Anonymous], 2009, P 17 ACM INT C MULTI, DOI DOI 10.1145/1631272.1631361
   Azencott CA, 2013, BIOINFORMATICS, V29, P171, DOI 10.1093/bioinformatics/btt238
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bronstein AM, 2010, LECT NOTES COMPUT SC, V6312, P197, DOI 10.1007/978-3-642-15552-9_15
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2010, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2010.5539997
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Daras P, 2012, IEEE T MULTIMEDIA, V14, P374, DOI 10.1109/TMM.2011.2176111
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Douze M, 2011, PROC CVPR IEEE, P745, DOI 10.1109/CVPR.2011.5995595
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Gong BQ, 2013, IEEE T MULTIMEDIA, V15, P369, DOI 10.1109/TMM.2012.2231059
   Guangyu Zhu, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P606, DOI 10.1109/ICDAR.2009.60
   Jain R., 2012, Proceedings of the 10th IAPR International Workshop on Document Analysis Systems (DAS 2012), P135, DOI 10.1109/DAS.2012.54
   Jiang YN, 2015, IEEE T IMAGE PROCESS, V24, P1748, DOI 10.1109/TIP.2015.2405337
   Jiang YN, 2012, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2012.6248042
   Jiang YN, 2011, IEEE IMAGE PROC, P113, DOI 10.1109/ICIP.2011.6115629
   Kalantidis Y., 2011, INT C MULT RETR TREN
   Lampert CH, 2009, IEEE I CONF COMP VIS, P987, DOI 10.1109/ICCV.2009.5459359
   Letessier P., 2012, P ACM INT C MULT, P599
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meng J., 2010, P INT C ACM MULT, P1147
   Meng JJ, 2016, IEEE T MULTIMEDIA, V18, P116, DOI 10.1109/TMM.2015.2500734
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220
   Ohgushi Kazumasa., 2009, TENCON 2009 - 2009 IEEE Region 10 Conference, P1, DOI DOI 10.1109/TENCON.2009.5395921
   Phan R, 2010, COMPUT VIS IMAGE UND, V114, P66, DOI 10.1016/j.cviu.2009.07.004
   PHILBIN J, 2008, C COMP VIS PATT REC
   Rusinol M., 2010, P 8 IAPR INT WORKSHO, P215
   Sahbi H, 2013, IEEE T IMAGE PROCESS, V22, P1018, DOI 10.1109/TIP.2012.2226046
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111
   Tao R, 2014, PROC CVPR IEEE, P2099, DOI 10.1109/CVPR.2014.269
   Wang XG, 2014, IEEE T PATTERN ANAL, V36, P810, DOI 10.1109/TPAMI.2013.214
   Wang XG, 2011, PROC CVPR IEEE, P857, DOI 10.1109/CVPR.2011.5995399
   Wei CH, 2009, PATTERN RECOGN, V42, P386, DOI 10.1016/j.patcog.2008.08.019
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Yang F., 2015, WINT C APPL COMP VIS
   Yang LJ, 2011, IEEE T MULTIMEDIA, V13, P1295, DOI 10.1109/TMM.2011.2162399
   Yuan J., 2007, INT C COMP VIS RIO D
   Yuan Zhang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4324, DOI 10.1109/ICASSP.2014.6854418
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhe Li, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2716, DOI 10.1109/ICPR.2010.665
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
   Zhu G., 2007, INT C DOC AN REC COI
   Zitnick C. L., 2014, EUR C COMP VIS ZUR S
NR 48
TC 13
Z9 15
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 726
EP 737
DI 10.1109/TMM.2016.2532601
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300014
DA 2024-07-18
ER

PT J
AU Fang, Q
   Xu, CS
   Sang, JT
   Hossain, MS
   Ghoneim, A
AF Fang, Quan
   Xu, Changsheng
   Sang, Jitao
   Hossain, M. Shamim
   Ghoneim, Ahmed
TI Folksonomy-Based Visual Ontology Construction and Its Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge discovery; ontology; visual recognition
ID SEMANTICS; DETECTORS; KNOWLEDGE; DATABASE
AB An ontology hierarchically encodes concepts and concept relationships, and has a variety of applications such as semantic understanding and information retrieval. Previous work for building ontologies has primarily relied on labor-intensive human contributions or focused on text-based extraction. In this paper, we consider the problem of automatically constructing a folksonomy-based visual ontology (FBVO) from the user-generated annotated images. A systematic framework is proposed consisting of three stages as concept discovery, concept relationship extraction, and concept hierarchy construction. The noisy issues of the user-generated tags are carefully addressed to guarantee the quality of derived FBVO. The constructed FBVO finally consists of 139 825 concept nodes and millions of concept relationships by mining more than 2.4 million Flickr images. Experimental evaluations show that the derived FBVO is of high quality and consistent with human perception. We further demonstrate the utility of the derived FBVO in applications of complex visual recognition and exploratory image search.
C1 [Fang, Quan; Xu, Changsheng; Sang, Jitao] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Hossain, M. Shamim; Ghoneim, Ahmed] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 11543, Saudi Arabia.
   [Ghoneim, Ahmed] Menoufia Univ, Dept Comp Sci, Coll Sci, Menoufia 32721, Egypt.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Saud
   University; Egyptian Knowledge Bank (EKB); Menofia University
RP Fang, Q; Xu, CS; Sang, JT (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.; Hossain, MS; Ghoneim, A (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 11543, Saudi Arabia.; Ghoneim, A (corresponding author), Menoufia Univ, Dept Comp Sci, Coll Sci, Menoufia 32721, Egypt.
EM qfang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn;
   mshossain@ksu.edu.sa; ghoneim@ccis.ksu.edu.sa
RI Hossain, M. Shamim/K-1362-2014; Guizani, Mohsen/AAX-4534-2021; xu,
   cj/HJZ-3488-2023; ghoneim, ahmed/L-3019-2013
OI Hossain, M. Shamim/0000-0001-5906-9422; Guizani,
   Mohsen/0000-0002-8972-8094; Ghoneim, Ahmed/0000-0003-2076-8925
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61432019, 61332016,
   61303176, U1435211]; Beijing Natural Science Foundation [4131004];
   Deanship of Scientific Research, King Saud University, Riyadh, Saudi
   Arabia [RGP-229]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304, in part by the National Natural
   Science Foundation of China under Grant 61225009, Grant 61432019, Grant
   61332016, Grant 61303176, and Grant U1435211, in part by the Beijing
   Natural Science Foundation under Grant 4131004, and in part by the
   Deanship of Scientific Research, King Saud University, Riyadh, Saudi
   Arabia, under the research group project RGP-229. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Chengcui Zhang.
CR Akbani R, 2004, LECT NOTES COMPUT SC, V3201, P39, DOI 10.1007/978-3-540-30115-8_7
   Alani H, 2003, IEEE INTELL SYST, V18, P14, DOI 10.1109/MIS.2003.1179189
   [Anonymous], 2013, EXPLORATORY SEARCH Q, DOI DOI 10.2200/S00174ED1V01Y200901ICR003
   [Anonymous], 2013, P INT ACM C MULTIMED, DOI DOI 10.1145/2502081.2502093
   [Anonymous], 2008, P 2008 IEEE C COMP V
   [Anonymous], 2016, Pattern Recognition and Machine Learning, Softcover Reprint of the Original 1st ed., Information Science and Statistics, DOI DOI 10.1117/1.2819119
   [Anonymous], 2014, CORR
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Bird S., 2004, P ACL INTERACTIVE PO, P214
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Craswell Nick, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P239, DOI 10.1145/1277741.1277784
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Divvala SK, 2014, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2014.412
   Ewerth R, 2012, IEEE T MULTIMEDIA, V14, P1008, DOI 10.1109/TMM.2012.2186956
   Fan JP, 2008, IEEE T IMAGE PROCESS, V17, P407, DOI 10.1109/TIP.2008.916999
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Fan J, 2007, IEEE T MULTIMEDIA, V9, P939, DOI 10.1109/TMM.2007.900143
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fang Q, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2648581
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fensel D., 2001, ONTOLOGIES
   Ferrari V., 2007, P 20 INT C NEUR INF, P433
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Koskela M, 2007, IEEE T MULTIMEDIA, V9, P912, DOI 10.1109/TMM.2007.900137
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li L.J., 2007, PROC IEEE INT C COMP
   Li QN, 2013, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2013.115
   Lu YJ, 2010, IEEE T MULTIMEDIA, V12, P288, DOI 10.1109/TMM.2010.2046292
   Marszalek Marcin, 2007, CVPR '07. IEEE Conference on Computer Vision and Pattern Recognition, P1
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Mylonas P, 2009, IEEE T MULTIMEDIA, V11, P229, DOI 10.1109/TMM.2008.2009681
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Oberle D, 2014, SEMANT WEB, V5, P473, DOI 10.3233/SW-130114
   Over P., 2014, Proceedings of TRECVID, P52
   Park JY, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P985, DOI 10.1145/2702123.2702527
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Plangprasopchok A., 2010, PROCEEDING 16 INT C, P949
   Sanderson M, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P206, DOI 10.1145/312624.312679
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   Wang X.J., 2012, Proceedings of the 20th ACM International Conference on Multimedia, Oct. 29-Nov.2, ACM, P1229
   Wei XY, 2008, IEEE T MULTIMEDIA, V10, P1085, DOI 10.1109/TMM.2008.2001382
   Wu L, 2012, IEEE T PATTERN ANAL, V34, P863, DOI 10.1109/TPAMI.2011.195
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhu SA, 2012, IEEE T MULTIMEDIA, V14, P1068, DOI 10.1109/TMM.2012.2190387
NR 51
TC 31
Z9 34
U1 0
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 702
EP 713
DI 10.1109/TMM.2016.2527602
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300012
DA 2024-07-18
ER

PT J
AU Cai, XY
   Zhou, WG
   Wu, L
   Luo, JB
   Li, HQ
AF Cai, Xingyang
   Zhou, Wengang
   Wu, Lei
   Luo, Jiebo
   Li, Houqiang
TI Effective Active Skeleton Representation for Low Latency Human Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; depth camera; low latency; Markov random field;
   multiple instance learning
ID IMAGE; CODEBOOK; CONTEXT; POSE
AB With the development of depth sensors, low latency 3D human action recognition has become increasingly important in various interaction systems, where response with minimal latency is a critical process. High latency not only significantly degrades the interaction experience of users, but also makes certain interaction systems, e.g., gesture control or electronic gaming, unattractive. In this paper, we propose a novel active skeleton representation towards low latency human action recognition. First, we encode each limb of the human skeleton into a state through a Markov random field. The active skeleton is then represented by aggregating the encoded features of individual limbs. Finally, we propose a multi-channel multiple instance learning with maximum-pattern-margin to further boost the performance of the existing model. Our method is robust in calculating features related to joint positions, and effective in handling the unsegmented sequences. Experiments on the MSR Action3D, the MSR DailyActivity3D, and the Huawei/3DLife-2013 dataset demonstrate the effectiveness of the model with the proposed novel representation, and its superiority over the state-of-the-art low latency recognition approaches.
C1 [Cai, Xingyang; Zhou, Wengang; Wu, Lei; Li, Houqiang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Rochester
RP Cai, XY; Zhou, WG; Wu, L; Li, HQ (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.; Luo, JB (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
EM xycai@mail.ustc.edu.cn; zhwg@ustc.edu.cn; wuleibig@gmail.com;
   jluo@cs.rochester.edu; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013; Luo, Jiebo/AAI-7549-2020
OI Luo, Jiebo/0000-0002-4516-9729
FU 973 Program [2015CB351803]; NSFC [61325009, 61390514, 61472378]; Anhui
   Provincial Natural Science Foundation [1508085MF109]; Fundamental
   Research Funds for the Central Universities [WK2100060011]; Beijing
   University of Technology Beijing Multimedia and Intelligent Software Key
   Laboratory Open Project
FX The work of H. Li was supported in part by the 973 Program under
   Contract 2015CB351803, and in part by the NSFC under Contract 61325009
   and Contract 61390514. The work of W. Zhou was supported in part by the
   NSFC under Contract 61472378, in part by the Anhui Provincial Natural
   Science Foundation under Contract 1508085MF109, and in part by the
   Fundamental Research Funds for the Central Universities under Contract
   WK2100060011. This work was supported in part by the Beijing University
   of Technology Beijing Multimedia and Intelligent Software Key Laboratory
   Open Project. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Ebroul Izquierdo.
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], 2015, ACM T MULTIM COMPUT
   [Anonymous], 2013, P 21 ACM INT C MULT
   [Anonymous], 2013, HUAW 3DLIFE ACM MULT
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Cai XY, 2014, PROC SPIE, V9273, DOI 10.1117/12.2073573
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Ellis C, 2013, INT J COMPUT VISION, V101, P420, DOI 10.1007/s11263-012-0550-7
   Freeman W. T., 2000, NIPS 00, V13, P668
   Gong D, 2012, LECT NOTES COMPUT SC, V7574, P229, DOI 10.1007/978-3-642-33712-3_17
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Ikizler-Cinbis N, 2012, IEEE T MULTIMEDIA, V14, P1031, DOI 10.1109/TMM.2012.2187180
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   JOHANSSON G, 1975, SCI AM, V232, P76, DOI 10.1038/scientificamerican0675-76
   Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Li WX, 2013, PROC CVPR IEEE, P2587, DOI 10.1109/CVPR.2013.334
   Liu Z., IEEE T CIRCUITS SYST
   Luo JJ, 2013, IEEE I CONF COMP VIS, P1809, DOI 10.1109/ICCV.2013.227
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Ni BB, 2012, LECT NOTES COMPUT SC, V7573, P173, DOI 10.1007/978-3-642-33709-3_13
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pang YW, 2014, IEEE T NEUR NET LEAR, V25, P2191, DOI 10.1109/TNNLS.2014.2306844
   Pang YW, 2014, IEEE T CYBERNETICS, V44, P2122, DOI 10.1109/TCYB.2014.2301453
   Pang YW, 2012, IEEE T SYST MAN CY B, V42, P458, DOI 10.1109/TSMCB.2011.2167750
   Pang YW, 2011, SIGNAL PROCESS, V91, P773, DOI 10.1016/j.sigpro.2010.08.010
   Papadopoulos G. T., 2014, MULTIMEDIA MODELING
   Raptis M., 2013, P ACM INT C MULT, P147
   Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807
   Samanta S, 2014, IEEE T MULTIMEDIA, V16, P1525, DOI 10.1109/TMM.2014.2326734
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Song Y, 2013, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR.2013.457
   Song Y, 2014, IEEE T CIRC SYST VID, V24, P952, DOI 10.1109/TCSVT.2014.2302558
   Tang NC, 2014, IEEE T MULTIMEDIA, V16, P47, DOI 10.1109/TMM.2013.2283844
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wu XX, 2011, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2011.5995624
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Yang DC, 2015, WIRELESS PERS COMMUN, V84, P1, DOI 10.1007/s11277-015-2874-4
   Yang DC, 2015, IET COMMUN, V9, P1412, DOI 10.1049/iet-com.2014.1100
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang D, 2011, IEEE T NEURAL NETWOR, V22, P739, DOI 10.1109/TNN.2011.2109011
   Zhang JG, 2010, PATTERN RECOGN, V43, P197, DOI 10.1016/j.patcog.2009.05.015
   Zhao X., 2013, P ACM INT C MULT, P273
   Zhen XT, 2013, IEEE T CIRC SYST VID, V23, P1182, DOI 10.1109/TCSVT.2013.2240916
   Zhou WG, 2016, IEEE T PATTERN ANAL, V38, P159, DOI 10.1109/TPAMI.2015.2430329
   Zhou WG, 2015, IEEE T IMAGE PROCESS, V24, P967, DOI 10.1109/TIP.2015.2389624
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhu J, 2013, IEEE I CONF COMP VIS, P3559, DOI 10.1109/ICCV.2013.442
NR 55
TC 66
Z9 70
U1 3
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 141
EP 154
DI 10.1109/TMM.2015.2505089
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400001
DA 2024-07-18
ER

PT J
AU Traverso, S
   Ahmed, M
   Garetto, M
   Giaccone, P
   Leonardi, E
   Niccolini, S
AF Traverso, Stefano
   Ahmed, Mohamed
   Garetto, Michele
   Giaccone, Paolo
   Leonardi, Emilio
   Niccolini, Saverio
TI Unravelling the Impact of Temporal and Geographical Locality in Content
   Caching Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content distribution networks; content caching; Internet; least recently
   used; locality
AB To assess the performance of caching systems, the definition of a proper process describing the content requests generated by users is required. Starting from the analysis of traces of YouTube video requests collected inside operational networks, we identify the characteristics of real traffic that need to be represented and those that instead can be safely neglected. Based on our observations, we introduce a simple, parsimonious traffic model, named shot noise model (SNM), that allows us to capture temporal and geographical locality of content popularity. The SNM is sufficiently simple to be effectively employed in both analytical and scalable simulative studies of caching systems. We demonstrate this by analytically characterizing the performance of the LRU caching policy under the SNM, for both a single cache and a network of caches. With respect to the standard independent reference model (IRM), some paradigmatic shifts, concerning the impact of various traffic characteristics on cache performance, clearly emerge from our results.
C1 [Traverso, Stefano; Giaccone, Paolo; Leonardi, Emilio] Politecn Torino, Dept Elect & Commun, I-10129 Turin, Italy.
   [Ahmed, Mohamed; Niccolini, Saverio] NEC Labs Europe, Network Res Div, D-69115 Heidelberg, Germany.
   [Garetto, Michele] Univ Turin, Dept Comp Sci, I-10149 Turin, Italy.
C3 Polytechnic University of Turin; NEC Corporation; University of Turin
RP Traverso, S (corresponding author), Politecn Torino, Dept Elect & Commun, I-10129 Turin, Italy.
EM stefano.traverso@polito.it; Mohamed.Ahmed@neclab.eu;
   michele.garetto@unito.it; paolo.giaccone@polito.it; leonardi@polito.it;
   Saverio.Niccolini@neclab.eu
RI Giaccone, Paolo/F-3125-2018; Leonardi, Emilio/A-1700-2011
OI Giaccone, Paolo/0000-0003-4283-7936; 
CR Abrahamsson H., 2012, P 2012 INT MEAS C IM, P199
   Almeida V, 1996, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED INFORMATION SYSTEMS, P92, DOI 10.1109/PDIS.1996.568672
   [Anonymous], P NOMEN WORKSH
   [Anonymous], CORR
   [Anonymous], CISC VIS NETW IND FO
   [Anonymous], P ACM SIGCOMM CCR SE
   [Anonymous], P IEEE INFOCOM
   [Anonymous], 2012, P ITC
   Applegate D., 2010, Proceedings of ACM International Conference on Emerging Networking Experiments and Technologies (CoNEXT), P1
   Borst S, 2010, IEEE INFOCOM SER, DOI 10.1109/infcom.2010.5461964
   Brodersen Anders, 2012, P 21 INT C WORLD WID, P241, DOI DOI 10.1145/2187836.2187870
   Che H, 2002, IEEE J SEL AREA COMM, V20, P1305, DOI 10.1109/JSAC.2002.801752
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   Coffman Jr Edward G., 1973, Operating Systems Theory
   Crane R, 2008, P NATL ACAD SCI USA, V105, P15649, DOI 10.1073/pnas.0803685105
   Gallo Massimo, 2012, Performance Evaluation Review, V40, P395, DOI 10.1145/2318857.2254810
   Huang Q, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P167, DOI 10.1145/2517349.2522722
   Imbrenda C., 2014, Proceedings of the 1st ACM Conference on Information-Centric Networking, P57, DOI DOI 10.1145/2660129.2660146
   Jelenkovic PR, 2008, RANDOM STRUCT ALGOR, V33, P219, DOI 10.1002/rsa.20214
   Jiang W., 2012, Proceedings of ACM International Conference on Emerging Networking Experiments and Technologies (CoNEXT), P133, DOI DOI 10.1145/2413176.2413193
   Jin SD, 2000, 8TH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P28, DOI 10.1109/MASCOT.2000.876426
   Kamath Krishna., 2013, P 22 INT C WORLD WID, P667, DOI DOI 10.1145/2488388.2488447
   Mahanti A, 2000, PERFORM EVALUATION, V42, P187, DOI 10.1016/S0166-5316(00)00032-8
   Moller J, 2003, ADV APPL PROBAB, V35, P614, DOI 10.1239/aap/1059486821
   Nygren E., 2010, SIGOPS OPER SYST REV, V44, P2, DOI [10.1145/1842733.1842736, DOI 10.1145/1842733.1842736]
   Plass M.F., 2009, P 5 INT C EMERGING N, P1, DOI [10.1145/1658939.1658941, DOI 10.1145/1658939.1658941]
   Scellato S., 2011, Proceedings of the 20th International Conference on World Wide Web, P457
   Sun Y, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P363, DOI 10.1145/2674005.2675003
   Wolf R.W., 1989, STOCHASTIC MODELING
   Xie MJ, 2012, IEEE INFOCOM SER, P2426, DOI 10.1109/INFCOM.2012.6195632
NR 31
TC 53
Z9 58
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1839
EP 1854
DI 10.1109/TMM.2015.2458043
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400013
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Dibeklioglu, H
   Salah, AA
   Gevers, T
AF Dibeklioglu, Hamdi
   Salah, Albert Ali
   Gevers, Theo
TI Recognition of Genuine Smiles
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective computing; expression dynamics; expression spontaneity; face
   analysis; genuine smile; human-computer interaction; social signals
ID EXPRESSION RECOGNITION; FACIAL EXPRESSIONS; DUCHENNE SMILE; DELIBERATE;
   MOVEMENT; DATABASE; POLITE; FALSE; FELT
AB Automatic distinction between genuine (spontaneous) and posed expressions is important for visual analysis of social signals. In this paper, we describe an informative set of features for the analysis of face dynamics, and propose a completely automatic system to distinguish between genuine and posed enjoyment smiles. Our system incorporates facial landmarking and tracking, through which features are extracted to describe the dynamics of eyelid, cheek, and lip corner movements. By fusing features over different regions, as well as over different temporal phases of a smile, we obtain a very accurate smile classifier. We systematically investigate age and gender effects, and establish that age-specific classification significantly improves the results, even when the age is automatically estimated. We evaluate our system on the 400-subject UvA-NEMO database we have recently collected, as well as on three other smile databases from the literature. Through an extensive experimental evaluation, we show that our system improves the state of the art in smile classification and provides useful insights in smile psychophysics.
C1 [Dibeklioglu, Hamdi; Gevers, Theo] Univ Amsterdam, Informat Inst, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
   [Dibeklioglu, Hamdi] Delft Univ Technol, Pattern Recognit & Bioinformat Grp, NL-2628 CD Delft, Netherlands.
   [Salah, Albert Ali] Bogazici Univ, Dept Comp Engn, TR-34342 Istanbul, Turkey.
   [Gevers, Theo] Univ Autonoma Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain.
C3 University of Amsterdam; Delft University of Technology; Bogazici
   University; Autonomous University of Barcelona; Centre de Visio per
   Computador (CVC)
RP Dibeklioglu, H (corresponding author), Univ Amsterdam, Informat Inst, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
EM h.dibeklioglu@tudelft.nl; salah@boun.edu.tr; th.gevers@uva.nl
RI Salah, Albert Ali/ABH-5561-2020; Dibeklioglu, Hamdi/AAB-6907-2020;
   Salah, Albert Ali/E-5820-2013
OI Salah, Albert Ali/0000-0001-6342-428X
FU Dutch national program COMMIT; Bogazici University [BAP-6531]
FX This work was supported by the Dutch national program COMMIT and by
   Bogazici University under Project BAP-6531. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jiebo Luo.
CR AM M., 1998, AR FACE DATABASE
   Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5
   [Anonymous], AVBPA
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2010, PROC 3 INT WORKSHOP
   [Anonymous], 2010, P INT C MULT MM, DOI DOI 10.1145/1873951.1874056
   [Anonymous], 2011, International Journal of Wavelets Multiresolution and Information Processing, DOI DOI 10.1142/S021969130400041X
   Borod J C, 1983, Brain Cogn, V2, P165, DOI 10.1016/0278-2626(83)90006-4
   Cohn JF, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P129, DOI 10.1109/AFGR.2004.1301520
   Del Giudice M, 2007, DEV PSYCHOL, V43, P796, DOI 10.1037/0012-1649.43.3.796
   Dibeklioglu H, 2012, IEEE T IMAGE PROCESS, V21, P844, DOI 10.1109/TIP.2011.2163162
   Dibekliolu H., 2012, MM'2012: Proceedings of the 20th ACM International Conference on Multimedia, P209
   Duchenne B., 1990, The mechanism of human facial expression or an electrophysiological analysis of the expression of the emotions
   EKMAN P, 1980, CHILD DEV, V51, P886, DOI 10.1111/j.1467-8624.1980.tb02627.x
   EKMAN P, 1981, PSYCHOPHYSIOLOGY, V18, P101, DOI 10.1111/j.1469-8986.1981.tb02919.x
   Ekman P, 2003, ANN NY ACAD SCI, V1000, P205, DOI 10.1196/annals.1280.010
   EKMAN P, 1988, J PERS SOC PSYCHOL, V54, P414, DOI 10.1037/0022-3514.54.3.414
   EKMAN P, 1982, J NONVERBAL BEHAV, V6, P238, DOI 10.1007/BF00987191
   EKMAN P, 1990, J PERS SOC PSYCHOL, V58, P342, DOI 10.1037/0022-3514.58.2.342
   Ekman P., 1992, Telling lies: Cues to deceit in the marketplace, politics, and marriage
   Ekman P, 1978, FACIAL ACTION CODING
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Gosselin P, 2010, EMOTION, V10, P266, DOI 10.1037/a0017748
   Gunnery SD, 2013, J NONVERBAL BEHAV, V37, P29, DOI 10.1007/s10919-012-0139-4
   Guo GD, 2013, IEEE T AFFECT COMPUT, V4, P291, DOI 10.1109/T-AFFC.2013.13
   Guo GD, 2012, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2012.6247972
   Hai Tao, 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P611, DOI 10.1109/CVPR.1999.787002
   He MH, 2013, INT CONF AFFECT, P79, DOI 10.1109/ACII.2013.20
   Koelstra S., 2008, P IEEE INT C AUT FAC, P1
   Krumhuber EG, 2009, EMOTION, V9, P807, DOI 10.1037/a0017844
   Kuncheva L. I., 2004, COMBINING PATTERN CL, P151
   Littlewort-Ford G., 2001, P JOINT S NEUR COMP
   Manera V, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00143
   Pantic M, 2005, IEEE SYS MAN CYBERN, P3358
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Petridis S, 2013, IMAGE VISION COMPUT, V31, P186, DOI 10.1016/j.imavis.2012.08.014
   Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401
   Ramanathan N, 2009, J VISUAL LANG COMPUT, V20, P131, DOI 10.1016/j.jvlc.2009.01.011
   Saatci Y, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P393
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Schmidt KL, 2009, J NONVERBAL BEHAV, V33, P35, DOI 10.1007/s10919-008-0058-6
   Schmidt KL, 2006, J NONVERBAL BEHAV, V30, P37, DOI 10.1007/s10919-005-0003-x
   Schmidt KL, 2003, BIOL PSYCHOL, V65, P49, DOI 10.1016/S0301-0511(03)00098-X
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Shizhi Chen, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P330, DOI 10.1109/FG.2011.5771419
   Shore DM, 2011, EMOTION, V11, P169, DOI 10.1037/a0022601
   Tang B, 2013, IEEE SYS MAN CYBERN, P1, DOI 10.1109/SMC.2013.8
   Valstar M.F., 2006, International Conference on Multimodal Interfaces, P162, DOI DOI 10.1145/1180995.1181031
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710
   Valstar MF, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P38
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang LG, 2011, LECT NOTES COMPUT SC, V7064, P431, DOI 10.1007/978-3-642-24965-5_49
NR 55
TC 67
Z9 71
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 279
EP 294
DI 10.1109/TMM.2015.2394777
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700002
DA 2024-07-18
ER

PT J
AU Minotto, VP
   Jung, CR
   Lee, B
AF Minotto, Vicente P.
   Jung, Claudio R.
   Lee, Bowon
TI Simultaneous-Speaker Voice Activity Detection and Localization Using
   Mid-Fusion of SVM and HMMs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Beamforming; hidden Markov model; multimodal fusion; optical-flow; sound
   source localization; SRP-PHAT; support vector machine; voice activity
   detection
AB Humans can extract speech signals that they need to understand from amixture of background noise, interfering sound sources, and reverberation for effective communication. Voice Activity Detection (VAD) and Sound Source Localization (SSL) are the key signal processing components that humans perform by processing sound signals received at both ears, sometimes with the help of visual cues by locating and observing the lip movements of the speaker. Both VAD and SSL serve as the crucial design elements for building applications involving human speech. For example, systems with microphone arrays can benefit from these for robust speech capture in video conferencing applications, or for speaker identification and speech recognition in Human Computer Interfaces (HCIs). The design and implementation of robust VAD and SSL algorithms in practical acoustic environments are still challenging problems, particularly when multiple simultaneous speakers exist in the same audiovisual scene. In this work we propose a multimodal approach that uses Support Vector Machines (SVMs) and Hidden Markov Models (HMMs) for assessing the video and audio modalities through an RGB camera and a microphone array. By analyzing the individual speakers' spatio-temporal activities and mouth movements, we propose a mid-fusion approach to perform both VAD and SSL for multiple active and inactive speakers. We tested the proposed algorithm in scenarios with up to three simultaneous speakers, showing an average VAD accuracy of 95.06% with an average error of 10.9 cm when estimating the three-dimensional locations of the speakers.
C1 [Minotto, Vicente P.; Jung, Claudio R.] Univ Fed Rio Grande do Sul, Inst Informat, BR-91501970 Porto Alegre, RS, Brazil.
   [Lee, Bowon] Inha Univ, Dept Elect Engn, Inchon, South Korea.
   [Lee, Bowon] Hewlett Packard Labs, Palo Alto, CA 94304 USA.
C3 Universidade Federal do Rio Grande do Sul; Inha University;
   Hewlett-Packard
RP Lee, B (corresponding author), Inha Univ, Dept Elect Engn, Inchon, South Korea.
EM vpminotto@inf.ufrgs.br; crjung@inf.ufrgs.br; bowon.lee@hp.com
RI Jung, Claudio R/G-2439-2012; Lee, Bowon/GMX-1775-2022
CR Almajai I., 2008, P 16 EUR SIGN PROC C
   [Anonymous], 2005, WILEY SERIES PROBABI
   [Anonymous], 1978, DIGITAL PROCESSING S
   [Anonymous], P 16 INT C DIG SIGN
   [Anonymous], 1981, P 7 INT JOINT C ART
   Asoh H., 2004, Seventh International Conference on Information Fusion, P805
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Aubrey AJ, 2010, IET IMAGE PROCESS, V4, P463, DOI 10.1049/iet-ipr.2009.0042
   Benesty J., 2008, SPRINGER TOP SIGN PR
   Bertrand A, 2010, INT CONF ACOUST SPEE, P85, DOI 10.1109/ICASSP.2010.5496183
   Bins J, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P227, DOI 10.1109/ISM.2009.33
   Blauth DA, 2012, PATTERN RECOGN LETT, V33, P373, DOI 10.1016/j.patrec.2011.09.002
   Bouguet J-Y, 1999, Pyramidal implementation of the Lucas Kanade feature tracker
   Brandstein M, 2001, DIGITAL SIGNAL PROC, P133
   Brutti A, 2008, INT CONF ACOUST SPEE, P4349, DOI 10.1109/ICASSP.2008.4518618
   Butko T, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P123
   BYRD RH, 1988, MATH PROGRAM, V40, P247, DOI 10.1007/BF01580735
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Conn A., 2000, MPS/SIAM Series on Optimization, DOI DOI 10.1137/1.9780898719857
   DiBiase J. H., 2000, Ph.D. thesis
   Do HA, 2008, INT CONF ACOUST SPEE, P301
   Do H, 2010, INT CONF ACOUST SPEE, P125, DOI 10.1109/ICASSP.2010.5496133
   EPHRAIM Y, 1992, P IEEE, V80, P1526, DOI 10.1109/5.168664
   EPHRAIM Y, 1984, IEEE T ACOUST SPEECH, V32, P1109, DOI 10.1109/TASSP.1984.1164453
   Fan RE, 2005, J MACH LEARN RES, V6, P1889
   Farkas L. G., 1994, ANTHROPOMETRY HEAD F, V6
   Gurban M., 2006, PARALLEL COMPUTING E
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Jaimes A, 2007, COMPUT VIS IMAGE UND, V108, P116, DOI 10.1016/j.cviu.2006.10.019
   Johnson D. H., 1993, ARRAY SIGNAL PROCESS
   Lee B., 2010, P 12 INT WORKSH AC E
   Lee B, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P3106
   Lorenzo-Trueba J., 2010, P 2010 INT S INT SIG, P1
   Maraboina S., 2006, P EUR SIGN PROC C EU, P2
   Minotto VP, 2013, IEEE J-STSP, V7, P147, DOI 10.1109/JSTSP.2012.2237379
   Naqvi SM, 2012, IET SIGNAL PROCESS, V6, P466, DOI 10.1049/iet-spr.2011.0124
   Peruffo V. Minotto, 2012, INT J HIGH PERFORM C
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Schapire RE, 1998, ANN STAT, V26, P1651
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Small C., 2003, OXFORD STAT SCI SERI, V29
   Sohn J, 1999, IEEE SIGNAL PROC LET, V6, P1, DOI 10.1109/97.736233
   Taghizadeh M. J., 2011, 2011 Joint Workshop on Hands-Free Speech Communication and Microphone Arrays (HSCMA 2011), P92, DOI 10.1109/HSCMA.2011.5942417
   Takeuchi S., 2009, P INT C AUD VIS SPEE
   Tanyer SG, 2000, IEEE T SPEECH AUDI P, V8, P478, DOI 10.1109/89.848229
   Tiawongsombat P, 2012, PATTERN RECOGN, V45, P783, DOI 10.1016/j.patcog.2011.07.011
   Weiping Cai, 2010, Proceedings of the 2010 International Conference on Electrical and Control Engineering (ICECE 2010), P1246, DOI 10.1109/iCECE.2010.310
   Wu TF, 2004, J MACH LEARN RES, V5, P975
   Yamamoto S, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5333, DOI 10.1109/IROS.2006.282037
   Zhang C, 2007, INT CONF ACOUST SPEE, P125
   Zhang WY, 2010, IEEE T AUDIO SPEECH, V18, P1913, DOI 10.1109/TASL.2010.2040525
NR 51
TC 7
Z9 8
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1032
EP 1044
DI 10.1109/TMM.2014.2305632
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800012
DA 2024-07-18
ER

PT J
AU Xie, HT
   Zhang, YD
   Tan, JL
   Guo, L
   Li, JT
AF Xie, Hongtao
   Zhang, Yongdong
   Tan, Jianlong
   Guo, Li
   Li, Jintao
TI Contextual Query Expansion for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contextual query expansion; common visual patterns; image retrieval
ID EFFICIENT; VIDEOS
AB In this paper, we study the problem of image retrieval by introducing contextual query expansion to address the shortcomings of bag-of-words based frameworks: semantic gap of visual word quantization, and the efficiency and storage loss due to query expansion. Our method is built on common visual patterns (CVPs), which are the distinctive visual structures between two images and have rich contextual information. With CVPs, two contextual query expansions on visual word-level and image-level are explored, respectively. For visual word-level expansion, we find contextual synonymous visual words (CSVWs) and expand a word in the query image with its CSVWs to boost retrieval accuracy. CSVWs are the words that appear in the same CVPs and have same contextual meaning, i.e. similar spatial layout and geometric transformations. For image-level expansion, the database images that have the same CVPs are organized by linked list and the images that have the same CVPs as the query image, but not included in the results are automatically expanded. The main computation of these two expansions is carried out offline, and they can be integrated into the inverted file and efficiently applied to all images in the dataset. Experiments conducted on three reference datasets and a dataset of one million images demonstrate the effectiveness and efficiency of our method.
C1 [Xie, Hongtao; Tan, Jianlong; Guo, Li] Chinese Acad Sci, Natl Engn Lab Informat Secur Technol, Inst Informat Engn, Beijing 100093, Peoples R China.
   [Xie, Hongtao; Zhang, Yongdong; Li, Jintao] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Xie, HT (corresponding author), Chinese Acad Sci, Natl Engn Lab Informat Secur Technol, Inst Informat Engn, Beijing 100093, Peoples R China.
EM xiehongtao@iie.ac.cn; zhyd@ict.ac.cn; tanjianlong@iie.ac.cn;
   guoli@iie.ac.cn; jtli@ict.ac.cn
FU Chinese Academy of Sciences [XDA06030602]; National High Technology
   Research and Development Program [2011AA010705]; National Nature Science
   Foundation of China [61303171, 61100087]; Beijing New Star Project on
   Science Technology [2007B071]; Natural Science Foundation of Beijing
   [4112055]
FX This work was supported in part by the "Strategic Priority Research
   Program" of the Chinese Academy of Sciences (XDA06030602), National High
   Technology Research and Development Program (2011AA010705), National
   Nature Science Foundation of China (61303171, 61100087); Beijing New
   Star Project on Science & Technology (2007B071); Natural Science
   Foundation of Beijing (4112055). The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Nicu Sebe.
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], 2006, P IEEE COMPUTER SOC
   [Anonymous], 2007, CVPR
   [Anonymous], P EUR C COMP VIS
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P INT C COMP VIS
   [Anonymous], P EUR C COMP VIS
   Berg B. T., 2005, P IEEE C COMP VIS PA
   Duan JZ, 2013, IEEE SIGNAL PROC LET, V20, P831, DOI 10.1109/LSP.2013.2268206
   Fernando B., 2012, P EUR C COMP VIS
   Gavves E., 2010, P ACM C MULT
   Holub A., 2008, P COMP VIS PATT REC
   Hsiao JH, 2007, IEEE T IMAGE PROCESS, V16, P2069, DOI 10.1109/TIP.2007.900099
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Joly A., 2009, P ACM C MULT
   Kuo Y.-H., 2011, P IEEE C COMP VIS PA
   Kuo Y. H., 2009, P INT C MULT
   Liu H., 2010, P INT C MACH LEARN I
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Olivares X., 2008, P ACM C MULT
   Pavan M, 2007, IEEE T PATTERN ANAL, V29, P167, DOI 10.1109/TPAMI.2007.250608
   Philbin J, 2010, INT J COMPUT VIS
   Philbin J., 2010, P EUR C COMP VIS
   Qin Danfeng, 2011, P IEEE C COMP VIS PA
   Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111
   Tang JH, 2010, IEEE T MULTIMEDIA, V12, P131, DOI 10.1109/TMM.2009.2037373
   Tang W., 2011, P ACM C MULT
   Tian Q, 2011, MULTIMED TOOLS APPL, V51, P441, DOI 10.1007/s11042-010-0636-6
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   Weibull J. W., 1997, EVOLUTIONARY GAME TH
   Wu W., 2008, P ACM C MULT
   Xie H., 2011, P ACM C MULT
   Xie HT, 2011, IEEE T MULTIMEDIA, V13, P1319, DOI 10.1109/TMM.2011.2167224
   Yuan J., 2007, P INT C COMP VIS
   Zha Z., 2009, P ACM C MULT
   Zhang D.-Q., P ACM C MULT
   Zhang YD, 2014, INFORM SCIENCES, V281, P586, DOI 10.1016/j.ins.2013.12.043
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zhou W., 2010, P ACM C MULT
   Zhou W., 2013, TOMCCAP, V9
   Zhou W., 2012, P ACM C MULT
NR 44
TC 47
Z9 52
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1104
EP 1114
DI 10.1109/TMM.2014.2305909
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800017
DA 2024-07-18
ER

PT J
AU Yu, Y
   Zimmermann, R
   Wang, Y
   Oria, V
AF Yu, Yi
   Zimmermann, Roger
   Wang, Ye
   Oria, Vincent
TI Scalable Content-Based Music Retrieval Using Chord Progression Histogram
   and Tree-Structure LSH
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio computing; chord progression histogram; locality sensitive
   hashing; music-IR; tree-structure.
ID CHROMA FEATURES
AB With more and more multimedia content made available on the Internet, music information retrieval is becoming a critical but challenging research topic, especially for real-time online search of similar songs from websites. In this paper we study how to quickly and reliably retrieve relevant songs from a large-scale dataset of music audio tracks according to melody similarity. Our contributions are two-fold: (i) Compact and accurate representation of audio tracks by exploiting music semantics. Chord progressions are recognized from audio signals based on trained music rules, and the recognition accuracy is improved by multi-probing. A concise chord progression histogram (CPH) is computed from each audio track as a mid-level feature, which retains the discriminative capability in describing audio content. (ii) Efficient organization of audio tracks according to their CPHs by using only one locality sensitive hash table with a tree-structure. A set of dominant chord progressions of each song is used as the hash key. Average degradation of ranks is further defined to estimate the similarity of two songs in terms of their dominant chord progressions, and used to control the number of probing in the retrieval stage. Experimental results on a large dataset with 74,055 music audio tracks confirm the scalability of the proposed retrieval algorithm. Compared to state-of-the-artmethods, our algorithm improves the accuracy of summarization and indexing, and makes a further step towards the optimal performance determined by an exhaustive sequence comparison.
C1 [Yu, Yi; Zimmermann, Roger; Wang, Ye] Natl Univ Singapore, Dept Comp Sci, Singapore 117548, Singapore.
   [Oria, Vincent] New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA.
C3 National University of Singapore; New Jersey Institute of Technology
RP Yu, Y (corresponding author), Natl Univ Singapore, Dept Comp Sci, Singapore 117548, Singapore.
EM yuy@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015; Wang, Ye/KGL-6405-2024
OI Zimmermann, Roger/0000-0002-7410-2590; Wang, Ye/0000-0002-0123-1260
FU Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative
FX This work was supported by the Singapore National Research Foundation
   under its International Research Centre @ Singapore Funding Initiative
   and administered by the IDM Programme Office.
CR Ahonen T., 2010, P ISMIR, P165
   [Anonymous], 2010, P 18 ACM INT C MULTI
   Bawa M, 2005, Proceedings of the 14th International Conference on World Wide Web-WWW'05, P651, DOI [DOI 10.1145/1060745.1060840, 10.1145/1060745.1060840]
   Bertin N., 2005, P ISMIR 05, P238
   Bertsekas D. P., 2002, INTRO PROBABILITY, V1st
   Cai R., 2007, P 15 ACM INT C MULT, P1065
   Casey M, 2008, IEEE T AUDIO SPEECH, V16, P1015, DOI 10.1109/TASL.2008.925883
   Cheng HT, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1505, DOI 10.1109/ICME.2008.4607732
   Cho T., 2010, P SOUND MUS COUMP C
   Cui B., 2006, P ACM MM 06, P634
   Ellis D., 2010, P MIREX
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Fujishima T., 1999, P INT COMP MUS C, P464
   Guo Z., 2012, SIGNAL PROCESS, DOI DOI 10.1016/J.SIGPR0.2012.09.006
   Harte C., 2005, P CONV AUD ENG SOC
   INDYK P, 1998, P ACM STOC
   Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]
   Joly Alexis., 2008, PROCEEDING 16 ACM IN, P209
   Karydis I, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P22, DOI 10.1109/MMMC.2005.22
   Lee K., 2006, P ICMC
   Lv Q, 2007, P 33 INT C VER LARG, P950
   McVicar M., 2011, P ISMIR 11, P639
   Meyer D, 2003, NEUROCOMPUTING, V55, P169, DOI 10.1016/S0925-2312(03)00431-4
   Miotto R., 2007, P ISMIR 07, P239
   Miotto R., 2009, P FDIA, P69
   Müller M, 2009, INT CONF ACOUST SPEE, P1877, DOI 10.1109/ICASSP.2009.4959974
   Muller M, 2005, P 6 INT C MUSIC INFO, P288
   Poullot S., 2008, Proceedings of the ACM International Conference on Multimedia, P61
   Ruiz-Sánchez MA, 2001, IEEE NETWORK, V15, P8, DOI 10.1109/65.912716
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Shen JL, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P455, DOI 10.1145/2348283.2348346
   Shen JL, 2009, IEEE T MULTIMEDIA, V11, P313, DOI 10.1109/TMM.2008.2009719
   Slaney M, 2008, IEEE SIGNAL PROC MAG, V25, P128, DOI 10.1109/MSP.2007.914237
   Tsai Wei-Ho., 2005, Proceeding of the International Symposium on Music Information Retrieval (ISMIR), V5, P183
   Yang Cheng., 2002, MULTIMEDIA 02, P584
   Yu Y., 2008, P ACM MIR, P121
   Yu Y, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P9, DOI 10.1109/ISM.2012.10
NR 37
TC 14
Z9 14
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1969
EP 1981
DI 10.1109/TMM.2013.2269313
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900020
DA 2024-07-18
ER

PT J
AU Jeon, M
   Kim, C
AF Jeon, Mansik
   Kim, Chulhong
TI Multimodal Photoacoustic Tomography
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Photoacoustic tomography; optoacoustic tomography; multiscale imaging;
   multimodal imaging
ID OPTICAL COHERENCE TOMOGRAPHY; SENTINEL LYMPH-NODE; MULTISPECTRAL
   OPTOACOUSTIC TOMOGRAPHY; FREQUENCY-DOMAIN RECONSTRUCTION; REFLECTANCE
   CONFOCAL MICROSCOPY; IN-VIVO; THERMOACOUSTIC TOMOGRAPHY; CONTRAST
   AGENTS; GOLD NANOCAGES; BIOLOGICAL TISSUES
AB Currently available optical microscopic imaging techniques-confocal microscopy, multi-photon (also referred to as two-photon) microscopy, and optical coherence tomography-have revolutionized biological and medical research, based on strong optical contrast and high spatial resolution. Unfortunately, owing to unavoidable strong light scattering in biological tissues, such methods cannot maintain contrast and spatial resolution beyond one optical transport mean free path (mm in tissues). Although model-based diffuse optical tomography is able to operate at greater depths, this technique fails to maintain spatial resolution. Photoacoustic tomography overcomes the fundamental penetration depth problem and achieves high-resolution optical imaging in deep tissues by combing light and ultrasound. In this review article, the multimodal imaging capability of photoacoustic tomography, integrated with existing imaging tools, is contemplated and the potential preclinical and clinical impacts of the combined systems are discussed.
C1 [Jeon, Mansik; Kim, Chulhong] SUNY Buffalo, Dept Biomed Engn, Buffalo, NY 14260 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo
RP Jeon, M (corresponding author), Pohang Univ Sci & Technol, Dept Elect Engn, Pohang, Kyungpook, South Korea.
EM msjeon@postech.ac.kr; chulhong@postech.ac.kr
OI Kim, Chulhong/0000-0001-7249-1257
FU University at Buffalo faculty start-up fund; Roswell Park Alliance
   Foundation
FX This work was supported by the University at Buffalo faculty start-up
   fund and Roswell Park Alliance Foundation. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Jane Wang.
CR Ahlgrimm-Siess V, 2009, SEMIN CUTAN MED SURG, V28, P180, DOI 10.1016/j.sder.2009.06.008
   Akers WJ, 2012, TRANSL RES, V159, P175, DOI 10.1016/j.trsl.2011.09.006
   Akers WJ, 2011, ACS NANO, V5, P173, DOI 10.1021/nn102274q
   Anastasio MA, 2005, IEEE T MED IMAGING, V24, P199, DOI 10.1109/TMI.2004.839682
   Bauer AQ, 2011, J BIOMED OPT, V16, DOI 10.1117/1.3626212
   Beard P, 2011, INTERFACE FOCUS, V1, P602, DOI 10.1098/rsfs.2011.0028
   Becerra L, 2008, NEUROIMAGE, V41, P252, DOI 10.1016/j.neuroimage.2008.01.047
   Bell A G, 1880, Science, V1, P130, DOI 10.1126/science.os-1.12.130
   Boas DA, 2001, IEEE SIGNAL PROC MAG, V18, P57, DOI 10.1109/79.962278
   Boppart SA, 2004, BREAST CANCER RES TR, V84, P85, DOI 10.1023/B:BREA.0000018401.13609.54
   Boppart SA, 1998, NEUROSURGERY, V43, P834, DOI 10.1097/00006123-199810000-00068
   Brecht HP, 2009, J BIOMED OPT, V14, DOI 10.1117/1.3259361
   Cai X, 2011, ACS NANO, V5, P9658, DOI 10.1021/nn203124x
   Chen JY, 2010, ADV FUNCT MATER, V20, P3684, DOI 10.1002/adfm.201001329
   Cho EC, 2009, J PHYS CHEM C, V113, P9023, DOI 10.1021/jp903343p
   Culver JP, 2003, OPT LETT, V28, P2061, DOI 10.1364/OL.28.002061
   Danielli A, 2011, OPT LETT, V36, P769, DOI 10.1364/OL.36.000769
   De La Zerda A, 2008, NAT NANOTECHNOL, V3, P557, DOI 10.1038/nnano.2008.231
   de la Zerda A, 2011, CONTRAST MEDIA MOL I, V6, P346, DOI 10.1002/cmmi.455
   de la Zerda A, 2010, NANO LETT, V10, P2168, DOI 10.1021/nl100890d
   Erpelding TN, 2010, RADIOLOGY, V256, P102, DOI 10.1148/radiol.10091772
   Esenaliev RO, 1999, IEEE J SEL TOP QUANT, V5, P981, DOI 10.1109/2944.796320
   Favazza CP, 2011, J BIOMED OPT, V16, DOI 10.1117/1.3536522
   Feng DZ, 2001, MED PHYS, V28, P2427, DOI 10.1118/1.1418015
   Filonov GS, 2012, ANGEW CHEM INT EDIT, V51, P1448, DOI 10.1002/anie.201107026
   Gamelin J, 2009, OPT EXPRESS, V17, P10489, DOI 10.1364/OE.17.010489
   Gamelin J, 2008, J BIOMED OPT, V13, DOI 10.1117/1.2907157
   Guo H, 2009, REV SCI INSTRUM, V80, DOI 10.1063/1.3069292
   Guo ZJ, 2009, MED PHYS, V36, P4084, DOI 10.1118/1.3187231
   Harrison T, 2009, OPT EXPRESS, V17, P22041, DOI 10.1364/OE.17.022041
   Hoelen CGA, 1998, OPT LETT, V23, P648, DOI 10.1364/OL.23.000648
   Hofmann-Wellenhof R, 2009, SEMIN CUTAN MED SURG, V28, P172, DOI 10.1016/j.sder.2009.06.004
   Homan KA, 2012, ACS NANO, V6, P641, DOI 10.1021/nn204100n
   HUANG D, 1991, SCIENCE, V254, P1178, DOI 10.1126/science.1957169
   Jetzfellner T, 2011, OPT LETT, V36, P4176, DOI 10.1364/OL.36.004176
   Jiao SL, 2010, OPT EXPRESS, V18, P3967, DOI 10.1364/OE.18.003967
   Jiao SL, 2009, OPT LETT, V34, P2961, DOI 10.1364/OL.34.002961
   Jung YR, 2012, J BIOMED OPT, V17, DOI 10.1117/1.JBO.17.1.016015
   Karabutov AA, 1996, APPL PHYS B-LASERS O, V63, P545
   Karpiouk AB, 2008, J BIOMED OPT, V13, DOI 10.1117/1.2992175
   Kim C, 2011, PHILOS T R SOC A, V369, P4644, DOI 10.1098/rsta.2010.0353
   Kim C, 2011, OPT LETT, V36, P3599, DOI 10.1364/OL.36.003599
   Kim C, 2010, BIOMED OPT EXPRESS, V1, P278, DOI 10.1364/BOE.1.000278
   Kim C, 2011, J MATER CHEM, V21, P2841, DOI 10.1039/c0jm04194g
   Kim C, 2010, J BIOMED OPT, V15, DOI 10.1117/1.3469829
   Kim C, 2010, ACS NANO, V4, P4559, DOI 10.1021/nn100736c
   Kim C, 2010, CHEM REV, V110, P2756, DOI 10.1021/cr900266s
   Kim C, 2010, RADIOLOGY, V255, P442, DOI 10.1148/radiol.10090281
   Kim C, 2010, J BIOMED OPT, V15, DOI 10.1117/1.3302808
   Kim G, 2007, J BIOMED OPT, V12, DOI 10.1117/1.2771530
   Kim JW, 2009, NAT NANOTECHNOL, V4, P688, DOI 10.1038/nnano.2009.231
   Kolkman RGM, 2008, J BIOMED OPT, V13, DOI 10.1117/1.3005421
   Kruger RA, 2010, MED PHYS, V37, P6096, DOI 10.1118/1.3497677
   Krumholz A, 2011, J BIOMED OPT, V16, DOI 10.1117/1.3606568
   Ku G, 2005, OPT LETT, V30, P507, DOI 10.1364/OL.30.000507
   Ku G, 2005, APPL OPTICS, V44, P770, DOI 10.1364/AO.44.000770
   Lanza GM, 2011, CONTRAST MEDIA MOL I, V6, P331, DOI 10.1002/cmmi.466
   Li CH, 2010, J BIOMED OPT, V15, DOI 10.1117/1.3302807
   Li L, 2007, J BIOMED OPT, V12, DOI 10.1117/1.2717531
   Li L, 2008, J INNOV OPT HEAL SCI, V1, P207, DOI 10.1142/S1793545808000212
   Li L, 2009, OPT EXPRESS, V17, P16450, DOI 10.1364/OE.17.016450
   Li ML, 2008, P IEEE, V96, P481, DOI 10.1109/JPROC.2007.913515
   Li WY, 2011, CONTRAST MEDIA MOL I, V6, P370, DOI 10.1002/cmmi.439
   Li XQ, 2011, BIOMED OPT EXPRESS, V2, P2348, DOI 10.1364/BOE.2.002348
   Liu T, 2011, BIOMED OPT EXPRESS, V2, P1359, DOI 10.1364/BOE.2.001359
   Lovell JF, 2011, NAT MATER, V10, P324, DOI [10.1038/nmat2986, 10.1038/NMAT2986]
   Luke GP, 2012, ANN BIOMED ENG, V40, P422, DOI 10.1007/s10439-011-0449-4
   Manohar S, 2011, CONTRAST MEDIA MOL I, V6, P389, DOI 10.1002/cmmi.454
   Maslov K, 2008, OPT LETT, V33, P929, DOI 10.1364/OL.33.000929
   Maslov K, 2008, J BIOMED OPT, V13, DOI 10.1117/1.2904965
   NICHOLAS D, 1982, ULTRASOUND MED BIOL, V8, P17, DOI 10.1016/0301-5629(82)90065-5
   Paproski RJ, 2011, BIOMED OPT EXPRESS, V2, P771, DOI 10.1364/BOE.2.000771
   Proskurnin MA, 2011, CYTOM PART A, V79A, P834, DOI 10.1002/cyto.a.21127
   Rajian JR, 2011, OPT EXPRESS, V19, P14335, DOI 10.1364/OE.19.014335
   Razansky D, 2007, MED PHYS, V34, P4293, DOI 10.1118/1.2786866
   Razansky D, 2009, NAT PHOTONICS, V3, P412, DOI 10.1038/NPHOTON.2009.98
   ROSENCWA A, 1973, B AM PHYS SOC, V18, P357
   ROSENCWAIG A, 1980, ANNU REV BIOPHYS BIO, V9, P31, DOI 10.1146/annurev.bb.09.060180.000335
   ROSENCWAIG A, 1975, J ACOUST SOC AM S1, V58, pS52
   Rosenthal A, 2010, IEEE T MED IMAGING, V29, P1275, DOI 10.1109/TMI.2010.2044584
   Shashkov EV, 2008, NANO LETT, V8, P3953, DOI 10.1021/nl802442x
   Song KH, 2008, MED PHYS, V35, P4524, DOI 10.1118/1.2977534
   Song KH, 2007, J BIOMED OPT, V12, DOI 10.1117/1.2818045
   Song KH, 2009, EUR J RADIOL, V70, P227, DOI 10.1016/j.ejrad.2009.01.045
   Song KH, 2009, NANO LETT, V9, P183, DOI 10.1021/nl802746w
   Song L, 2009, MED PHYS, V36, P3724, DOI 10.1118/1.3168598
   Wang L.V., 2007, Biomedical Optics: Principles and Imaging
   Wang LV, 2008, IEEE J SEL TOP QUANT, V14, P171, DOI 10.1109/JSTQE.2007.913398
   Wang LHV, 2012, SCIENCE, V335, P1458, DOI 10.1126/science.1216210
   Wang LV, 2009, NAT PHOTONICS, V3, P503, DOI 10.1038/NPHOTON.2009.157
   Wang XD, 2003, NAT BIOTECHNOL, V21, P803, DOI 10.1038/nbt839
   Wang XD, 2002, MED PHYS, V29, P2799, DOI 10.1118/1.1521720
   Wang Y, 2011, OPT LETT, V36, P1029, DOI 10.1364/OL.36.001029
   Wang Y, 2010, IEEE T BIO-MED ENG, V57, P2576, DOI 10.1109/TBME.2010.2059026
   WONG YH, 1978, APPL PHYS LETT, V32, P538, DOI 10.1063/1.90120
   Xie ZX, 2009, OPT LETT, V34, P1771, DOI 10.1364/OL.34.001771
   Xu MH, 2006, REV SCI INSTRUM, V77, DOI 10.1063/1.2195024
   Xu Y, 2004, MED PHYS, V31, P724, DOI 10.1118/1.1644531
   Xu Y, 2002, IEEE T MED IMAGING, V21, P823, DOI 10.1109/TMI.2002.801172
   Xu Y, 2002, IEEE T MED IMAGING, V21, P829, DOI 10.1109/TMI.2002.801171
   Yang Y, 2011, BIOMED OPT EXPRESS, V2, P2551, DOI 10.1364/BOE.2.002551
   Yao J., 2012, TECHNOL CANC RES TRE
   Yao JJ, 2011, J BIOMED OPT, V16, DOI 10.1117/1.3594786
   Yao JJ, 2010, OPT LETT, V35, P1419, DOI 10.1364/OL.35.001419
   Yao JJ, 2009, J BIOMED OPT, V14, DOI 10.1117/1.3251044
   Zhang C, 2008, PHYS MED BIOL, V53, P4971, DOI 10.1088/0031-9155/53/18/008
   Zhang C, 2010, OPT LETT, V35, P3195, DOI 10.1364/OL.35.003195
   Zhang EZ, 2011, BIOMED OPT EXPRESS, V2, P2202, DOI 10.1364/BOE.2.002202
   Zhang HF, 2007, NAT PROTOC, V2, P797, DOI 10.1038/nprot.2007.108
   Zhang HF, 2006, NAT BIOTECHNOL, V24, P848, DOI 10.1038/nbt1220
   Zhang Hao F, 2011, Ophthalmic Surg Lasers Imaging, V42 Suppl, pS106, DOI 10.3928/15428877-20110627-10
   Zhang HF, 2010, OPT EXPRESS, V18, P1278, DOI 10.1364/OE.18.001278
   Zhang XY, 2012, J BIOMED OPT, V17, DOI 10.1117/1.JBO.17.3.030502
   Zhang XY, 2011, J BIOMED OPT, V16, DOI 10.1117/1.3606569
NR 114
TC 67
Z9 67
U1 3
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 975
EP 982
DI 10.1109/TMM.2013.2244203
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600002
DA 2024-07-18
ER

PT J
AU Jiménez-Moreno, A
   Martínez-Enríquez, E
   Díaz-de-María, F
AF Jimenez-Moreno, Amaya
   Martinez-Enriquez, Eduardo
   Diaz-de-Maria, Fernando
TI Mode Decision-Based Algorithm for Complexity Control in H.264/AVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Complexity control; H.264/AVC; hypothesis testing; mode decision
ID MOTION ESTIMATION; RATE-DISTORTION; PREDICTION; SEARCH
AB The latest H.264/AVC video coding standard achieves high compression rates in exchange for high computational complexity. Nowadays, however, many application scenarios require the encoder to meet some complexity constraints.
   This paper proposes a novel complexity control method that relies on a hypothesis testing that can handle time-variant content and target complexities. Specifically, it is based on a binary hypothesis testing that decides, on a macroblock basis, whether to use a low-or a high-complexity coding model. Gaussian statistics are assumed so that the probability density functions involved in the hypothesis testing can be easily adapted. The decision threshold is also adapted according to the deviation between the actual and the target complexities.
   The proposed method is implemented on the H.264/AVC reference software JM10.2 and compared with a state-of-the-art method. Our experimental results prove that the proposed method achieves a better trade-off between complexity control and coding efficiency. Furthermore, it leads to a lower deviation from the target complexity.
C1 [Jimenez-Moreno, Amaya; Martinez-Enriquez, Eduardo; Diaz-de-Maria, Fernando] Carlos III Univ, Dept Signal Theory & Commun, Leganes 28911, Madrid, Spain.
C3 Universidad Carlos III de Madrid
RP Jiménez-Moreno, A (corresponding author), Carlos III Univ, Dept Signal Theory & Commun, Leganes 28911, Madrid, Spain.
EM ajimenez@tsc.uc3m.es; emenriquez@tsc.uc3m.es; fdiaz@tsc.uc3m.es
RI de María, Fernando Díaz/E-8048-2011; Martinez, Eduardo/ISS-3584-2023;
   Jimenez-Moreno, Amaya/AAA-7450-2021; Martinez-Enriquez,
   Eduardo/L-8332-2014
OI de María, Fernando Díaz/0000-0002-6437-914X; Martinez-Enriquez,
   Eduardo/0000-0001-7097-8846
FU Spanish Ministry of Science and Innovation [TEC2011-26807]
FX This work was supported in part by the National Grant TEC2011-26807 of
   the Spanish Ministry of Science and Innovation. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Yiannis Andreopoulos.
CR [Anonymous], 2001, ITU T VCEG M AUST TE
   Ates HF, 2008, IEEE T CIRC SYST VID, V18, P159, DOI 10.1109/TCSVT.2008.918114
   Choi WI, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P371
   da Fonseca T., 2009, P PCS MAY, P1
   da Fonseca TA, 2011, IEEE INT SYMP CIRC S, P2909
   Gao XJ, 2010, SIGNAL PROCESS, V90, P2468, DOI 10.1016/j.sigpro.2010.01.029
   González-Díaz I, 2008, IEEE T CIRC SYST VID, V18, P1369, DOI 10.1109/TCSVT.2008.2004917
   Grecos C, 2006, IEEE T MULTIMEDIA, V8, P1125, DOI 10.1109/TMM.2006.884631
   Huijbers ERAM, 2011, IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE 2011), P49, DOI 10.1109/ICCE.2011.5722705
   Kannangara CS, 2008, IEEE T CIRC SYST VID, V18, P1191, DOI 10.1109/TCSVT.2008.928881
   Kannangara CS, 2009, IEEE T MULTIMEDIA, V11, P433, DOI 10.1109/TMM.2009.2012937
   Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138
   Kuo TY, 2006, IEEE T CIRC SYST VID, V16, P1185, DOI 10.1109/TCSVT.2006.883512
   Li GL, 2005, IEEE INT SYMP CIRC S, P5481
   Martinez-Enriquez E., 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P325
   Martínez-Enríquez E, 2011, IEEE T CIRC SYST VID, V21, P1719, DOI 10.1109/TCSVT.2011.2134010
   Martínez-Enríquez E, 2010, IEEE T CONSUM ELECTR, V56, P826, DOI 10.1109/TCE.2010.5506008
   Martínez-Enríquez E, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P217
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   Saha A, 2007, IEEE T CONSUM ELECTR, V53, P1153, DOI 10.1109/TCE.2007.4341599
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Su L, 2009, IEEE T CIRC SYST VID, V19, P477, DOI 10.1109/TCSVT.2009.2014017
   Sullivan G., 2001, VCEGN81 ITUT
   Tan YH, 2010, IEEE T CIRC SYST VID, V20, P1271, DOI 10.1109/TCSVT.2010.2058480
   Tourapis HYC, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P517
   Vanam R, 2007, IEEE DATA COMPR CONF, P303
   Vanam R, 2008, IEEE DATA COMPR CONF, P372, DOI 10.1109/DCC.2009.53
   You J, 2006, IEEE T CONSUM ELECTR, V52, P1377, DOI 10.1109/TCE.2006.273159
   Zhang JN, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P888
   Zhou C, 2009, ELECTRON LETT, V45, P974, DOI 10.1049/el.2009.1296
   Zhu C, 2002, IEEE T CIRC SYST VID, V12, P349, DOI 10.1109/TCSVT.2002.1003474
NR 31
TC 10
Z9 10
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1094
EP 1109
DI 10.1109/TMM.2013.2241414
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600013
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Luo, L
   Shen, CH
   Zhang, CY
   van den Hengel, A
AF Luo, Lei
   Shen, Chunhua
   Zhang, Chunyuan
   van den Hengel, Anton
TI Shape Similarity Analysis by Self-Tuning Locally Constrained
   Mixed-Diffusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Locally constrained mixed-diffusion; shape similarity analysis;
   shape/image retrieval
AB Similarity analysis is a powerful tool for shape matching/retrieval and other computer vision tasks. In the literature, various shape (dis) similarity measures have been introduced. Different measures specialize on different aspects of the data. In this paper, we consider the problem of improving retrieval accuracy by systematically fusing several different measures. To this end, we propose the locally constrained mixed-diffusion method, which partly fuses the given measures into one and propagates on the resulted locally dense data space. Furthermore, we advocate the use of self-adaptive neighborhoods to automatically determine the appropriate size of the neighborhoods in the diffusion process, with which the retrieval performance is comparable to the best manually tuned NNs. The superiority of our approach is empirically demonstrated on both shape and image datasets. Our approach achieves a score of 100% in the bull's eye test on the MPEG-7 shape dataset, which is the best reported result to date.
C1 [Luo, Lei; Zhang, Chunyuan] Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China.
   [Luo, Lei; Shen, Chunhua; van den Hengel, Anton] Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.
   [Shen, Chunhua; van den Hengel, Anton] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 National University of Defense Technology - China; University of
   Adelaide; University of Adelaide
RP Luo, L (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China.
EM l.luo@nudt.edu.cn; chunhua.shen@adelaide.edu.au; cyzhang@nudt.edu.cn;
   anton.vandenhengel@adelaide.edu.au
OI van den Hengel, Anton/0000-0003-3027-8364
FU NSFC [61033008, 60903041]; ARC [LP120200485, FT120100969]; Australian
   Research Council [LP120200485] Funding Source: Australian Research
   Council
FX The work of L. Luo and C. Zhang was supported in part by NSFC projects
   61033008 and 60903041. The work of C. Shen and A. van den Hengel was
   supported in part by ARC grant LP120200485. The work of C. Shen was also
   supported by ARC Future Fellowship FT120100969. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2006, PROC IEEE C COMPUTER
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2006, P IEEE COMPUTER SOC
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   Bai X., 2010, P EUR C COMP VIS
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gopalan R., 2010, P EUR C COMP VIS
   Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253
   Guan NY, 2012, IEEE T NEUR NET LEAR, V23, P1087, DOI 10.1109/TNNLS.2012.2197827
   Guan NY, 2011, IEEE T NEURAL NETWOR, V22, P1218, DOI 10.1109/TNN.2011.2157359
   Huang J, 2011, IEEE T MULTIMEDIA, V13, P653, DOI 10.1109/TMM.2011.2127463
   Jegou H, 2010, IEEE T PATTERN ANAL, V32, P2, DOI 10.1109/TPAMI.2008.285
   Kontschieder Peter, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P655
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Liu HR, 2012, INT J COMPUT VISION, V98, P65, DOI 10.1007/s11263-011-0496-1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mezaris V., 2010, EURASIP J ADV SIG PR, V2010, P47
   Olszewska J. I., 2012, Proceedings of the 2012 IEEE 16th International Conference on Intelligent Engineering Systems (INES), P91, DOI 10.1109/INES.2012.6249809
   Pavan M, 2007, IEEE T PATTERN ANAL, V29, P167, DOI 10.1109/TPAMI.2007.250608
   Pedronette D. C. G., 2011, P ACM INT C MULT RET
   Santini S, 1999, IEEE T PATTERN ANAL, V21, P871, DOI 10.1109/34.790428
   Santini S, 2001, IEEE T KNOWL DATA EN, V13, P337, DOI 10.1109/69.929893
   Soderkvist O, 2001, COMPUTER VISION CLAS
   WANG J, 2008, P IEEE C COMP VIS PA
   YANG X, 2008, P EUR C COMP VIS
   Yang XW, 2009, PROC CVPR IEEE, P357, DOI 10.1109/CVPRW.2009.5206844
   Zhou C. J., 2007, P 24 INT C MACH LEAR, P1159, DOI DOI 10.1145/1273496.1273642
NR 33
TC 21
Z9 22
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1174
EP 1183
DI 10.1109/TMM.2013.2242450
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600019
OA Green Published
DA 2024-07-18
ER

PT J
AU Li, HQ
   Wang, Y
   Mei, T
   Wang, JD
   Li, SP
AF Li, Houqiang
   Wang, Yang
   Mei, Tao
   Wang, Jingdong
   Li, Shipeng
TI Interactive Multimodal Visual Search on Mobile Device
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile visual search; multimodal search; interactive search; mobile
   device
ID EFFICIENT
AB This paper describes a novel multimodal interactive image search system on mobile devices. The system, the Joint search with ImaGe, Speech, And Word Plus (JIGSAW), takes full advantage of the multimodal input and natural user interactions of mobile devices. It is designed for users who already have pictures in their minds but have no precise descriptions or names to address them. By describing it using speech and then refining the recognized query by interactively composing a visual query using exemplary images, the user can easily find the desired images through a few natural multimodal interactions with his/her mobile device. Compared with our previous work JIGSAW, the algorithm has been significantly improved in three aspects: 1) segmentation-based image representation is adopted to remove the artificial block partitions; 2) relative position checking replaces the fixed position penalty; and 3) inverted index is constructed instead of brute force matching. The proposed JIGSAW + is able to achieve 5% gain in terms of search performance and is ten times faster.
C1 [Li, Houqiang; Wang, Yang] Univ Sci & Technol China, Hefei 230027, Peoples R China.
   [Mei, Tao; Wang, Jingdong; Li, Shipeng] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft
RP Li, HQ (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM lihq@ustc.edu.cn; wyang1@mail.ustc.edu.cn; tmei@microsoft.com;
   jingdw@microsoft.com; spli@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Li, Houqiang Li/B-6259-2013; Wang,
   Jingdong/E-9920-2017; Li, Shipeng/AAA-3374-2020
OI Mei, Tao/0000-0002-5990-7307; Wang, Jingdong/0000-0002-4888-4445; Li,
   Shipeng/0000-0001-5368-4256
FU NSFC [61272316]; 973 Program [2013CB329004]
FX The work of H. Li was supported in part by the NSFC under Grant 61272316
   and the 973 Program under Grant 2013CB329004. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Pascal Frossard.
CR [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], 2006, 2006 C COMPUTER VISI
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Cao Yang, 2010, P 18 ACM INT C MULT, P1605
   Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733
   Chandrasekhar V, 2010, IEEE IMAGE PROC, P3885, DOI 10.1109/ICIP.2010.5649937
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Church K, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232726
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266
   Eitz M, 2010, COMPUT GRAPH-UK, V34, P482, DOI 10.1016/j.cag.2010.07.002
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gales M, 2007, FOUND TRENDS SIGNAL, V1, P195, DOI 10.1561/2000000004
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Jia M., 2006, P MOB DAT MAN
   Jing F, 2004, IEEE T IMAGE PROCESS, V13, P699, DOI 10.1109/TIP.2004.826125
   Kim J, 2011, PROC CVPR IEEE, P1553, DOI 10.1109/CVPR.2011.5995526
   Li MJ, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P508
   Liu XY, 2011, 2011 SECOND INTERNATIONAL CONFERENCE ON EDUCATION AND SPORTS EDUCATION (ESE 2011), VOL II, P1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   TSAI S.S., 2011, Proceedings of the 19th ACM international conference on Multimedia, P1029
   Tsai SS, 2011, IEEE IMAGE PROC, DOI 10.1109/ICIP.2011.6116198
   Wang Changhu., 2010, proceedings of the International Conference on World Wide Web, P1309
   Wang Y, 2011, PROCEEDINGS OF THE 4TH CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE AND SYSTEMS DYNAMICS, SSMSSD10, VOL 4, P73
   Xu H, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P275
NR 32
TC 19
Z9 133
U1 2
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 594
EP 607
DI 10.1109/TMM.2012.2234730
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900011
DA 2024-07-18
ER

PT J
AU Liao, J
   Chou, PA
   Yuan, C
   Hu, YS
   Zhu, WW
AF Liao, Jun
   Chou, Philip A.
   Yuan, Chun
   Hu, Yusuo
   Zhu, Wenwu
TI Online Allocation of Communication and Computation Resources for
   Real-Time Multimedia Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximation algorithm; congestion pricing; mixing; multimedia
   services; online algorithm; primal-dual algorithm; resource allocation;
   subgraph packing
ID TASKS
AB In a network, the location of the node on which a service is computed is inextricably linked to the locations of the paths through which the service communicates. Hence, service location can have a profound effect on quality of service, especially for communication-centric applications such as real-time multimedia. In this paper, we propose an online algorithm that uses pricing to consider server load, route congestion, and propagation delay jointly when locating servers and routes for real-time multimedia services in a network with fixed computing and communication capacities. The algorithm is online in the sense that it is able to sequentially allocate resources for services with long and unknown duration as demands arrive, without the benefit of looking ahead to later demands. By formulating the problem as one of lowest cost subgraph packing, we prove that our algorithm is nevertheless C-competitive with the optimal algorithm that looks ahead, meaning that our performance is within a constant factor C of optimal, as measured by the total number of service demands satisfied, or total user utility. Using mixing services as an example, we show through experimental results that our algorithm can adapt to cross traffic and automatically route around congestion and failure of nodes and edges, can reduce latency by 40% or more, and can pack 20% more sessions or alternatively can double the number of sessions before significant call rejection, compared with conventional approaches.
C1 [Liao, Jun] China Mobile Commun Corp, Informat Secur Ctr, Beijing 100053, Peoples R China.
   [Chou, Philip A.] Microsoft Res, Redmond, WA 98052 USA.
   [Yuan, Chun; Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Hu, Yusuo] Google, Mountain View, CA 94043 USA.
C3 China Mobile; Microsoft; Tsinghua University; Google Incorporated
RP Liao, J (corresponding author), China Mobile Commun Corp, Informat Secur Ctr, Beijing 100053, Peoples R China.
EM liaojun@chinamobile.com; pachou@microsoft.com; yuanc@tsinghua.edu.cn;
   yusuo.hu@gmail.com; wwzhu@tsinghua.edu.cn
RI Liao, Jun/JUF-2784-2023
CR Andersen D., 2001, Operating Systems Review, V35, P131, DOI 10.1145/502059.502048
   Andrews M., 1996, Computing and Combinatorics. Second Annual International Conference. COCOON '96. Proceedings, P1
   [Anonymous], 1998, Online Algorithms, The State of the Art
   [Anonymous], 1998, Online computation and competitive analysis
   Aspnes J, 1997, J ACM, V44, P486, DOI 10.1145/258128.258201
   Awerbuch B., 1993, Proceedings. 34th Annual Symposium on Foundations of Computer Science (Cat. No.93CH3368-8), P32, DOI 10.1109/SFCS.1993.366884
   Azar Y., 1993, Algorithms and Data Structures, P119
   AZAR Y, 1998, ONLINE ALGORITHMS ST
   Azar Y., 1992, IEEE FOCS
   Buchbinder N, 2007, FOUND TRENDS THEOR C, V3, P93, DOI 10.1561/0400000024
   Chowdhury NMMK, 2009, IEEE INFOCOM SER, P783, DOI 10.1109/INFCOM.2009.5061987
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Garey M.R., 1979, COMPUTERS INTRACTABI
   Huang L, 2004, IEEE T VEH TECHNOL, V53, P547, DOI 10.1109/TVT.2003.823290
   Ibaraki T., 1988, RESOURCE ALLOCATION
   Kamath A, 1996, PROCEEDINGS OF THE SEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P269
   Karger D, 1999, COMPUT NETW, V31, P1203, DOI 10.1016/S1389-1286(99)00055-9
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   Lam TW, 2002, THEOR COMPUT SCI, V270, P325, DOI 10.1016/S0304-3975(00)00392-3
   Low S., 2000, 61600 TR PRINC U COM
   Ma Y, 1997, INFORM PROCESS LETT, V62, P301, DOI 10.1016/S0020-0190(97)00085-9
   Medernach E, 2012, EUR J OPER RES, V218, P339, DOI 10.1016/j.ejor.2011.10.029
   Ross S. M., 1992, INTRO PROBABILITY MO
   Sedgewick R., 2004, ALGORITHMS
   Subramanian L., 2004, P NSDI MAR, V1, P1
   Szymaniak M., 2003, P INT C WWW INT, P435
   van Hentenryck P., 2006, ONLINE STOCHASTIC CO
NR 27
TC 8
Z9 8
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 670
EP 683
DI 10.1109/TMM.2012.2235416
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900017
DA 2024-07-18
ER

PT J
AU Daras, P
   Manolopoulou, S
   Axenopoulos, A
AF Daras, Petros
   Manolopoulou, Stavroula
   Axenopoulos, Apostolos
TI Search and Retrieval of Rich Media Objects Supporting Multiple
   Multimodal Queries
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Manifold learning; multimedia description; multimedia indexing;
   multimodal search and retrieval
ID SEGMENTATION
AB In this paper, a novel framework for rich-media object retrieval is described. The searchable items are media representations consisting of multiple modalities, such as 2-D images, 3-D objects and audio files, which share a common semantic concept. The proposed method utilizes the low-level descriptors of each separate modality to construct a new low-dimensional feature space, where all media objects can be mapped irrespective of their constituting modalities. While most of the existing state-of-the-art approaches support queries of one single modality at a time, the proposed one allows querying with multiple modalities simultaneously, through efficient multimodal query formulation, and retrieves multimodal results of any available type. Finally, a multimedia indexing scheme is adopted to tackle the problem of large scale media retrieval. The present framework proposes significant advances over existing methods and can be easily extended to involve as many heterogeneous modalities as possible. Experiments performed on two multimodal datasets demonstrate the effectiveness of the proposed method in multimodal search and retrieval.
C1 [Daras, Petros; Manolopoulou, Stavroula; Axenopoulos, Apostolos] Ctr Res & Technol Hellas, Informat & Telemat Inst, GR-57001 Thessaloniki, Greece.
C3 Centre for Research & Technology Hellas
RP Daras, P (corresponding author), Ctr Res & Technol Hellas, Informat & Telemat Inst, GR-57001 Thessaloniki, Greece.
EM daras@iti.gr; manolop@iti.gr; axenop@iti.gr
RI Daras, Petros/F-5284-2012
OI Daras, Petros/0000-0003-3814-6710
FU EC
FX This work was supported by the EC-funded project I-SEARCH. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Shin'ichi Satoh.
CR [Anonymous], 2009, WEKA DATA MINING SOF
   [Anonymous], THESIS U LEIPZIG LEI
   Attalla E, 2005, PATTERN RECOGN, V38, P2229, DOI 10.1016/j.patcog.2005.02.009
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Borg I., 2005, Modern Multidimensional Scaling: Theory and Applications
   Bors A., 2001, Online Symp. Electron. Eng, V1, P1
   Chatzichristofis SA, 2008, LECT NOTES COMPUT SC, V5008, P312
   Daras P, 2006, IEEE T MULTIMEDIA, V8, P101, DOI 10.1109/TMM.2005.861287
   Daras P., 2009, P IEEE 7 INT WORKSH
   Daras P., 2004, P 3D DAT PROC VIS TR
   Dutagaci H., 2011, P 4 EUR WORKSH 3D OB
   Geetha P., 2008, Journal of Computer Sciences, V4, P474, DOI 10.3844/jcssp.2008.474.486
   Gennaro C., 2010, P 14 EUR C RES ADV T
   He J. R., 2004, P ACM MM NEW YORK
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kokare M, 2005, IEEE T SYST MAN CY B, V35, P1168, DOI 10.1109/TSMCB.2005.850176
   Lai P. L., 1998, P EUR S ART NEUR NET
   Li D., 2003, P 11 ACM INT C MULT
   Li G., 2000, P ICME
   Li MK, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P473
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   Mademlis A, 2009, IEEE T MULTIMEDIA, V11, P1422, DOI 10.1109/TMM.2009.2032690
   Messina A., 2009, P 18 INT C WORLD WID
   Muja M., 2009, P INT C COMP VIS THE
   Ohbuchi R., 2006, P ACM MIR SANT BARB
   Saul L. K., 2003, J MACH LEARN RES
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   SIVIC J, 2006, THESIS U OXFORD OXFO
   ThiruvadandamPorethi V, 2010, P EUR WORKSH 3D OBJ
   Tong H., 2005, P ACM MM SING
   Tzanetakis G., 2001, P C AC MUS THEOR SEP
   Wan P., 2005, P SOC PHOTO-OPT INS, V6015
   Wichern G, 2010, IEEE T AUDIO SPEECH, V18, P688, DOI 10.1109/TASL.2010.2041384
   Worring M., 2001, International Journal of Image and Graphics, V1, P387
   Wu F., 2006, P IEEE INT C IM PROC
   Yang Y., 2009, P ACM MM BEIJ CHIN
   Zarpalas D, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/23912
   Zhang H., 2006, ADV MULTIMEDIA INFOR
   Zhang H, 2009, LECT NOTES COMPUT SC, V5879, P637, DOI 10.1007/978-3-642-10467-1_56
   Zhaung Y, 2009, LECT NOTES COMPUT SC, V5463, P677, DOI 10.1007/978-3-642-00887-0_59
   Zhou D., 2003, P C ADV NEUR INF PRO
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 42
TC 31
Z9 34
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 734
EP 746
DI 10.1109/TMM.2011.2181343
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700007
DA 2024-07-18
ER

PT J
AU Ofli, F
   Erzin, E
   Yemez, Y
   Tekalp, AM
AF Ofli, Ferda
   Erzin, Engin
   Yemez, Yuecel
   Tekalp, A. Murat
TI Learn2Dance: Learning Statistical Music-to-Dance Mappings for
   Choreography Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic dance choreography creation; multimodal dance modeling;
   music-driven dance performance synthesis and animation; music-to-dance
   mapping; musical measure clustering
ID MOTION GENERATION; BEAT TRACKING; METER
AB We propose a novel framework for learning many-to-many statistical mappings from musical measures to dance figures towards generating plausible music-driven dance choreographies. We obtain music-to-dance mappings through use of four statistical models: 1) musical measure models, representing a many-to-one relation, each of which associates different melody patterns to a given dance figure via a hidden Markov model (HMM); 2) exchangeable figures model, which captures the diversity in a dance performance through a one-to-many relation, extracted by unsupervised clustering of musical measure segments based on melodic similarity; 3) figure transition model, which captures the intrinsic dependencies of dance figure sequences via an n-gram model; 4) dance figure models, capturing the variations in the way particular dance figures are performed, by modeling the motion trajectory of each dance figure via an HMM. Based on the first three of these statistical mappings, we define a discrete HMM and synthesize alternative dance figure sequences by employing a modified Viterbi algorithm. The motion parameters of the dance figures in the synthesized choreography are then computed using the dance figure models. Finally, the generated motion parameters are animated synchronously with the musical audio using a 3-D character model. Objective and subjective evaluation results demonstrate that the proposed framework is able to produce compelling music-driven choreographies.
C1 [Ofli, Ferda] Univ Calif Berkeley, Coll Engn, Tele Immers Grp, Elect Engn & Comp Sci Dept, Berkeley, CA 94720 USA.
   [Erzin, Engin; Yemez, Yuecel; Tekalp, A. Murat] Koc Univ, Multimedia Vis & Graph Lab, Coll Engn, TR-34450 Istanbul, Turkey.
C3 University of California System; University of California Berkeley; Koc
   University
RP Ofli, F (corresponding author), Univ Calif Berkeley, Coll Engn, Tele Immers Grp, Elect Engn & Comp Sci Dept, Berkeley, CA 94720 USA.
EM fofli@eecs.berkeley.edu; eerzin@ku.edu.tr; yyemez@ku.edu.tr;
   mtekalp@ku.edu.tr
RI Ofli, Ferda/G-2027-2017; Tekalp, Murat/AAW-1060-2020; Erzin,
   Engin/H-1716-2011
OI Ofli, Ferda/0000-0003-3918-3230; Tekalp, Ahmet
   Murat/0000-0003-1465-8121; Erzin, Engin/0000-0002-2715-2368
FU TUBITAK [EEEAG-106E201]; COST2102 action
FX This work was supported by TUBITAK under project EEEAG-106E201 and
   COST2102 action. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Daniel Gatica-Perez.
CR Alankus G, 2005, COMPUT ANIMAT VIRT W, V16, P259, DOI 10.1002/cav.99
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   Bregler C., 1998, COMPUTER VISION MAN, P267
   Cardle M, 2002, 20TH EUROGRAPHICS UK CONFERENCE, PROCEEDINGS, P38, DOI 10.1109/EGUK.2002.1011270
   Chen TH, 2001, IEEE SIGNAL PROC MAG, V18, P9
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Ellis DPW, 2007, J NEW MUSIC RES, V36, P51, DOI 10.1080/09298210701653344
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Fujishima T., 1999, Proceedings of the International Computer Music Conference, ICMC, P464
   Gainza M, 2009, INT CONF ACOUST SPEE, P329, DOI 10.1109/ICASSP.2009.4959587
   Gao S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL IV, PROCEEDINGS, P237
   Kim JW, 2009, COMPUT ANIMAT VIRT W, V20, P375, DOI 10.1002/cav.314
   Kim S, 2009, INT CONF ACOUST SPEE, P1961, DOI 10.1109/ICASSP.2009.4959995
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Klapuri AP, 2006, IEEE T AUDIO SPEECH, V14, P342, DOI 10.1109/TSA.2005.854090
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Lee HC, 2005, COMPUT GRAPH FORUM, V24, P353, DOI 10.1111/j.1467-8659.2005.00860.x
   Lee K., 2006, Proceedings of the 1st ACM workshop on Audio and Music Computing Multimedia, P11
   Levine S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778861
   Li Y, 2002, ACM T GRAPHIC, V21, P465
   Li Y, 2006, IEEE T MULTIMEDIA, V8, P542, DOI 10.1109/TMM.2006.870732
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   McKinney MF, 2007, J NEW MUSIC RES, V36, P1, DOI 10.1080/09298210701653252
   Min J., 2010, P 2010 ACM SIGGRAPH, DOI [10.1145/1730804.1730811, DOI 10.1145/1730804.1730811]
   Neff M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330516
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Ofli F, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1703
   Ofli F, 2010, INT CONF ACOUST SPEE, P2466, DOI 10.1109/ICASSP.2010.5494891
   Ofli F., 2011, MUSIC DRIVEN DANCE C
   Ofli F, 2008, J MULTIMODAL USER IN, V2, P93, DOI 10.1007/s12193-008-0009-x
   Reynolds WilliamC., 1974, YB INT FOLK MUSIC CO, V6, P115
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Ruiz AP, 2002, TSI PRESS S, V13, P189, DOI 10.1109/WAC.2002.1049543
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Sargin ME, 2008, IEEE T PATTERN ANAL, V30, P1330, DOI 10.1109/TPAMI.2007.70797
   Shepard R., 1964, J ACOUST SOC AM, V36
   Shiratori T, 2006, COMPUT GRAPH FORUM, V25, P449, DOI 10.1111/j.1467-8659.2006.00964.x
   Xue JX, 2006, 2006 IEEE International Conference on Multimedia and Expo - ICME 2006, Vols 1-5, Proceedings, P1165, DOI 10.1109/ICME.2006.262743
NR 41
TC 39
Z9 42
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 747
EP 759
DI 10.1109/TMM.2011.2181492
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chien, SY
   Lok, KH
   Lu, YC
AF Chien, Shao-Yi
   Lok, Ka-Hang
   Lu, Yen-Chang
TI Low-Decoding-Latency Buffer Compression for Graphics Processing Units
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Buffer compression; mobile graphics; texture compression
AB Power consumption is the key design factor for graphics processing units (GPUs), especially for mobile applications. The increasing bandwidth required to produce more realistic graphics is a major power draw. To address this factor, in this paper, we present a new universal buffer compression method that can handle both color and depth data with the same hardware unit. In contrast to the current state-of-art technologies, which mainly focus on achieving higher and higher compression ratios but discarded the decompression latency, our method reaches a good compromise between the two, which are factors critical to system performance. With spatial prediction and bitstream rearrangement, the data dependencies between different samples are reduced, which enables a parallel decoding process and makes the proposed system have 6.78 times lower decoding latency. Moreover, by adopting a similar concept for the color/depth compression in the DXT5 texture compression method, better quality in terms of PSNR can be achieved without introducing any decoding latency when retrieving a texel.
C1 [Chien, Shao-Yi] Natl Taiwan Univ, Grad Inst Elect Engn, Taipei 106, Taiwan.
   [Chien, Shao-Yi] Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
   [Lok, Ka-Hang] MediaTek Inc, Hsinchu 300, Taiwan.
   [Lu, Yen-Chang] Texas Instruments Inc, New Taipei City 235, Taiwan.
C3 National Taiwan University; National Taiwan University; Mediatek
   Incorporated; Texas Instruments
RP Chien, SY (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, Taipei 106, Taiwan.
EM sychien@ntu.edu.tw; b92901159@ntu.edu.tw; getsby07@gmail.com
OI Chien, Shao-Yi/0000-0002-0634-6294
FU National Science Council [NSC97-2221-E-002-243-MY3,
   NSC100-2221-E-002-090-MY3]
FX Manuscript received November 01, 2010; revised April 07, 2011; accepted
   October 03, 2011. Date of publication October 25, 2011; date of current
   version March 21, 2012. This work was supported in part by the National
   Science Council under Grant NSC97-2221-E-002-243-MY3 and Grant
   NSC100-2221-E-002-090-MY3. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Zicheng Liu.
CR Akenine-Möller T, 2003, ACM T GRAPHIC, V22, P801, DOI 10.1145/882262.882348
   [Anonymous], MICR SYST POW CAL
   Arakida H, 2003, ISSCC DIG TECH PAP I, V46, P42
   Beers A. C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P373, DOI 10.1145/237170.237276
   Campbell G., 1986, Computer Graphics, V20, P215, DOI 10.1145/15886.15910
   DELP EJ, 1979, IEEE T COMMUN, V27, P1335, DOI 10.1109/TCOM.1979.1094560
   Doggett M., 2005, P EUR IND SEM NEW CO
   Fromm R, 1997, ACM COMP AR, P327, DOI 10.1145/384286.264214
   Hamilton E., 2004, JPEG File Interchange Format
   Hasselgren J., 2006, P 21 ACM SIGGRAPHEUR, P103
   Iourcha K., 1999, U.S. Patent, Patent No. 5956431
   Kuo CH, 2002, IEEE T CIRC SYST VID, V12, P850, DOI 10.1109/TCSVT.2002.804878
   Lian CJ, 2007, IEEE CIRC SYST MAG, V7, P26, DOI 10.1109/MCAS.2007.4299440
   LIU D, 1994, IEEE J SOLID-ST CIRC, V29, P663, DOI 10.1109/4.293111
   Malvar H., 2003, JVT1014
   Molnar S. E., 2004, U.S. Patent, Patent No. [6 825 847, 6825847]
   Morein S., 2000, P ACM SIGGRAPH EUR W
   Morein S. L., 2004, U S. Patent, Patent No. [6 762 758, 6762758]
   Nickolls J, 2010, IEEE MICRO, V30, P56, DOI 10.1109/MM.2010.41
   Nishikawa T., 2000, 2000 IEEE International Solid-State Circuits Conference. Digest of Technical Papers (Cat. No.00CH37056), P230, DOI 10.1109/ISSCC.2000.839762
   Pereberin A. V., 1999, P GRAPHICON 99, P195
   Rasmusson J, 2010, VISUAL COMPUT, V26, P17, DOI 10.1007/s00371-009-0372-y
   Rasmusson J, 2007, GRAPHICS HARDWARE 2007: ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P41
   Redmill DW, 1996, IEEE T IMAGE PROCESS, V5, P565, DOI 10.1109/83.491333
   Srinivasan S., 1983, N4183 JPEG WGI
   Stachera J., 2006, P WSCG 2006, P108
   Ström J, 2007, GRAPHICS HARDWARE 2007: ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P49
   Strom J., 2008, Proceedings of the 23rd ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware, GH '08, P75
   Strom Jacob, 2005, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics Hardware, P63, DOI DOI 10.1145/1071866.1071877
   Strom Jacob., 2004, ACM SIGGRAPH 2004 Sketches, SIGGRAPH '04, P66, DOI [DOI 10.1145/1186223.1186306, 10.1145/1186223.1186306.]
   Sun CH, 2009, IEEE T MULTIMEDIA, V11, P589, DOI 10.1109/TMM.2009.2017637
   Taubman D., 2012, JPEG2000: image compression fundamentals, standards and practice, V642
   Weinberger MJ, 1996, DCC '96 - DATA COMPRESSION CONFERENCE, PROCEEDINGS, P140, DOI 10.1109/DCC.1996.488319
NR 33
TC 1
Z9 2
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 250
EP 263
DI 10.1109/TMM.2011.2173476
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500002
DA 2024-07-18
ER

PT J
AU Liu, XQ
   Wang, WQ
AF Liu, Xiaoqian
   Wang, Weiqiang
TI Robustly Extracting Captions in Videos Based on Stroke-Like Edges and
   Spatio-Temporal Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spatio-temporal; stroke-like edges; text detection; text extraction
ID TEXT DETECTION; IMAGES
AB This paper presents an effective and efficient approach to extracting captions from videos. The robustness of our system comes from two aspects of contributions. First, we propose a novel stroke-like edge detection method based on contours, which can effectively remove the interference of non-stroke edges in complex background so as to make the detection and localization of captions much more accurate. Second, our approach highlights the importance of temporal feature, i.e., inter-frame feature, in the task of caption extraction (detection, localization, segmentation). Instead of regarding each video frame as an independent image, through fully utilizing the temporal feature of video together with spatial analysis in the computation of caption localization, segmentation and post-processing, we demonstrate that the use of inter-frame information can effectively improve the accuracy of caption localization and caption segmentation. In the comprehensive our evaluation experiments, the experimental results on two representative datasets have shown the robustness and efficiency of our approach.
C1 [Liu, Xiaoqian] Chinese Acad Sci, Grad Univ, Beijing 100190, Peoples R China.
   [Wang, Weiqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Liu, XQ (corresponding author), Chinese Acad Sci, Grad Univ, Beijing 100190, Peoples R China.
EM xqliu@jdl.ac.cn; wqwang@ict.ac.cn
FU National Natural Science Foundation of China [60873087]
FX Manuscript received April 15, 2011; revised August 05, 2011 and November
   13, 2011; accepted November 17, 2011. Date of publication November 29,
   2011; date of current version March 21, 2012. This work was supported by
   National Natural Science Foundation of China under Grant 60873087. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jia Li.
CR Ar I., 2007, P INT C COMP AN IM P
   Cai M, 2002, IEEE IMAGE PROC, P117
   Dubey P., 2006, P INT C SIGN PROC BE
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Ezaki N, 2004, INT C PATT RECOG, P683, DOI 10.1109/ICPR.2004.1334351
   Garcia C, 2000, INT CONF ACOUST SPEE, P2326, DOI 10.1109/ICASSP.2000.859306
   Gllavata J, 2004, LECT NOTES COMPUT SC, V3115, P216
   Hase H, 2001, PATTERN RECOGN, V34, P1349, DOI 10.1016/S0031-3203(00)00081-9
   Hua X. S., 2001, P ACM WORKSH MULT IN
   Jain A. K., 1992, Machine Vision and Applications, V5, P169, DOI 10.1007/BF02626996
   Jung K, 2004, PATTERN RECOGN, V37, P977, DOI 10.1016/j.patcog.2003.10.012
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Kim KI, 2001, PATTERN RECOGN, V34, P527, DOI 10.1016/S0031-3203(00)00095-9
   Liu Q., 2006, P INT C MULTIMEDIA, P129
   Liu QF, 2006, IEEE IMAGE PROC, P1473, DOI 10.1109/ICIP.2006.312560
   Liu Y, 2004, I C CONT AUTOMAT ROB, P1528
   Mao W., 2002, P INT C PATT REC QUE
   Park SH, 1999, ELECTRON LETT, V35, P1475, DOI 10.1049/el:19990977
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shim JC, 1998, INT C PATT RECOG, P618, DOI 10.1109/ICPR.1998.711219
   Shivakumara P, 2009, IEEE INT CON MULTI, P514, DOI 10.1109/ICME.2009.5202546
   Tang X, 2002, IEEE T NEURAL NETWOR, V13, P961, DOI 10.1109/TNN.2002.1021896
   Wu W, 2005, IEEE T INTELL TRANSP, V6, P378, DOI 10.1109/TITS.2005.858619
   Xiaoqian Liu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3232, DOI 10.1109/ICPR.2010.790
   Ye QX, 2005, IMAGE VISION COMPUT, V23, P565, DOI 10.1016/j.imavis.2005.01.004
NR 25
TC 39
Z9 44
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 482
EP 489
DI 10.1109/TMM.2011.2177646
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500020
DA 2024-07-18
ER

PT J
AU Lo Presti, L
   Sclaroff, S
   La Cascia, M
AF Lo Presti, Liliana
   Sclaroff, Stan
   La Cascia, Marco
TI Path Modeling and Retrieval in Distributed Video Surveillance Databases
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Beam search; camera network; dynamic Bayesian network (DBN); path
   modeling; path retrieval
ID ALGORITHM; TRACKING; OBJECTS
AB We propose a framework for querying a distributed database of video surveillance data in order to retrieve a set of likely paths of a person moving in the area under surveillance. In our framework, each camera of the surveillance system locally processes the data and stores video sequences in a storage unit and the metadata for each detected person in the distributed database. A pedestrian's path is formulated as a dynamic Bayesian network (DBN) to model the dependencies between subsequent observations of the person as he makes his way through the camera network. We propose a tool by which the analyst can pose queries about where a certain person appeared while moving in the site during a specified temporal window. The DBN is used in an algorithm that finds potentially relevant metadata records from the distributed databases and then assembles these into probable paths that the person took in the camera network. Finally, the system presents the analyst with the retrieved set of likely paths in ranked order. The computational complexity for our method is quadratic in the number of camera nodes and linear in the number of moving persons. Experiments were carried out on simulated data to test the system with large distributed databases and in a real setting in which six databases store the data from six video cameras. The simulations confirm that our method provides good results with varying numbers of cameras and persons moving in the network. In a real setting, the method reconstructs paths across the camera network with approximatively 75% accuracy at rank 1.
C1 [Lo Presti, Liliana; Sclaroff, Stan] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.
   [La Cascia, Marco] Univ Palermo, Chem Managerial Comp & Mech Engn Dept DICGIM, I-90128 Palermo, Italy.
C3 Boston University; University of Palermo
RP Lo Presti, L (corresponding author), Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA.
EM loprest@bu.edu; sclaroff@bu.edu; marco.lacascia@unipa.it
RI La Cascia, Marco/E-9612-2012
OI La Cascia, Marco/0000-0002-8766-6395; Lo Presti,
   Liliana/0000-0003-0833-4403
FU Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [0855065] Funding Source: National Science Foundation
CR Anjum N, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P201, DOI 10.1109/AVSS.2009.65
   [Anonymous], IEEE T PATT AN MACH
   [Anonymous], MACH VIS APPL
   [Anonymous], J AMBIENT INTELL HUM
   [Anonymous], M0519 UCBERL
   [Anonymous], 2016, MALAYSIA
   [Anonymous], INT J COMPUTER VISIO
   [Anonymous], 2001, J OPER RES SOC
   [Anonymous], 2009, ACM T INFORM SYST, DOI [DOI 10.1145/1416950.1416952, 10.1145/1416950.1416952rs, DOI 10.1145/1416950.1416952RS]
   [Anonymous], AUEB TAC 2009
   [Anonymous], P ACM INT C IM VID
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], T S3 R1 IBM SMART SU
   [Anonymous], 2008, P CVPR
   [Anonymous], 1993, STAT DISTRIBUTIONS
   [Anonymous], IEEE INTELL IN PRESS
   [Anonymous], P INT J C ART INT
   [Anonymous], P EUR WORKSH ADV VID
   Brander A.W., 1995, P 11 UK PERFORMANCE, P370
   Calderara S., 2006, P 4 ACM INT WORKSH V, P95
   CASTANON DA, 1990, IEEE T AERO ELEC SYS, V26, P405, DOI 10.1109/7.53448
   Del Bimbo A, 2010, COMPUT VIS IMAGE UND, V114, P611, DOI 10.1016/j.cviu.2010.01.007
   Eppstein D, 1998, SIAM J COMPUT, V28, P652, DOI 10.1137/S0097539795290477
   Erdem UM, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P105
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Foresti GL, 2002, IEEE T MULTIMEDIA, V4, P459, DOI 10.1109/TMM.2002.802024
   Fortmann T. E., 1980, Proceedings of the 19th IEEE Conference on Decision & Control Including the Symposium on Adaptive Processes, P807
   Furcy D, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P125
   Gandhi T, 2007, MACH VISION APPL, V18, P207, DOI 10.1007/s00138-006-0063-x
   Ghahramani Z, 1998, LECT NOTES ARTIF INT, V1387, P168, DOI 10.1007/BFb0053999
   Gilbert A, 2008, COMPUT VIS IMAGE UND, V111, P43, DOI 10.1016/j.cviu.2007.06.005
   Hershberger J, 2007, ACM T ALGORITHMS, V3, DOI 10.1145/1290672.1290682
   Hu WM, 2007, IEEE T IMAGE PROCESS, V16, P1168, DOI 10.1109/TIP.2006.891352
   Javed O, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P952
   Jaynes C, 1999, SECOND IEEE WORKSHOP ON VISUAL SURVEILLANCE (VS'99), PROCEEDINGS, P59, DOI 10.1109/VS.1999.780269
   Kettnaker V., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P253, DOI 10.1109/CVPR.1999.784638
   Khan S, 2003, IEEE T PATTERN ANAL, V25, P1355, DOI 10.1109/TPAMI.2003.1233912
   Lagogiannis G, 2009, SIGMOD RECORD, V38, P11, DOI 10.1145/1815933.1815936
   Lee L, 2000, IEEE T PATTERN ANAL, V22, P758, DOI 10.1109/34.868678
   Lo Presti L, 2009, LECT NOTES COMPUT SC, V5716, P547, DOI 10.1007/978-3-642-04146-4_59
   Makris D, 2004, PROC CVPR IEEE, P205
   Makris D, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P183, DOI 10.1109/AVSS.2003.1217920
   Micheloni C, 2010, IEEE SIGNAL PROC MAG, V27, P78, DOI 10.1109/MSP.2010.937333
   Mittal A, 2009, COMPUT VIS IMAGE UND, V113, P63, DOI 10.1016/j.cviu.2008.07.004
   Murphy Kevin Patrick, 2002, Dynamic bayesian networks: representation, inference and learning
   Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   Rodriguez M., 2007, Proceedings of the 15th international conference on Multimedia, MULTIMEDIA '07, P353
   Schwartz WR, 2009, SIBGRAPI, P322, DOI 10.1109/SIBGRAPI.2009.42
   Soto C, 2009, PROC CVPR IEEE, P1486, DOI 10.1109/CVPRW.2009.5206773
   Stauffer C, 2003, PROC CVPR IEEE, P259
   Stauffer C., 2003, IEEE CVPRW, V4, P35
   Thi-Lan Le, 2008, 2008 International Workshop on Content-based Multimedia Indexing - CBMI 2008, P338, DOI 10.1109/CBMI.2008.4564966
   Tieu K, 2005, IEEE I CONF COMP VIS, P1842
   UKKONEN E, 1983, LECT NOTES COMPUT SC, V158, P487
   WOLF JK, 1989, IEEE T AERO ELEC SYS, V25, P287, DOI 10.1109/7.18692
   Zajdel W, 2005, INT J PATTERN RECOGN, V19, P977, DOI 10.1142/S0218001405004423
   Zhao Tao., 2004, P 2004 IEEE COMPUTER, P406
NR 59
TC 17
Z9 20
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 346
EP 360
DI 10.1109/TMM.2011.2173323
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500010
DA 2024-07-18
ER

PT J
AU Stütz, T
   Uhl, A
AF Stuetz, Thomas
   Uhl, Andreas
TI Efficient and Rate-Distortion Optimal Wavelet Packet Basis Selection in
   JPEG2000
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image compression; JPEG2000; rate-distortion optimization; wavelet
   packet bases
ID BASES
AB This paper discusses optimal wavelet packet basis selection within JPEG2000. Algorithms for rate-distortion optimal wavelet packet basis selection in JPEG2000 are presented and compared to more efficient wavelet packet basis selection schemes. Both isotropic and anisotropic wavelet packet bases are considered. For the first time, computationally efficient heuristics are compared to the best bases in the standardized coding framework of JPEG2000. For the first time, the maximum performance gains of custom wavelet packets in JPEG2000 can be assessed. The algorithms are evaluated on a wide range of highly textured image data.
C1 [Stuetz, Thomas] Univ Nantes, PolytechNantes LUNAM Univ, CNRS, IRCCyN,UMR 6597, Nantes, France.
   [Uhl, Andreas] Salzburg Univ, Dept Comp Sci, A-5020 Salzburg, Austria.
C3 Centre National de la Recherche Scientifique (CNRS); Nantes Universite;
   Ecole Centrale de Nantes; Salzburg University
RP Stütz, T (corresponding author), Univ Nantes, PolytechNantes LUNAM Univ, CNRS, IRCCyN,UMR 6597, Nantes, France.
EM thomas.w.stuetz@gmail.com; uhl@cosy.sbg.ac.at
FU Austrian Science Fund [19159]
FX Manuscript received October 31, 2010; revised April 03, 2011 and July
   29, 2011; accepted October 04, 2011. Date of publication December 06,
   2011; date of current version March 21, 2012. This work was supported by
   the Austrian Science Fund under Project 19159. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Oscar C. Au.
CR [Anonymous], 2000, 154441 ISOIEC
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   [Anonymous], 2002, T801 ITUT
   BRADLEY JN, 1993, P SOC PHOTO-OPT INS, V1961, P293, DOI 10.1117/12.150973
   COIFMAN RR, 1992, IEEE T INFORM THEORY, V38, P713, DOI 10.1109/18.119732
   Engel D., 2006, P 31 INT C AC SPEECH, P465
   Information Technology-JPEG, 2000, 154442 ISOIEC
   Kasaei S, 2002, IEEE T IMAGE PROCESS, V11, P1365, DOI 10.1109/TIP.2002.802534
   Khuwaja G. A., 2004, International Journal of Computer Applications in Technology, V19, P51, DOI 10.1504/IJCAT.2004.003668
   KIDD RC, 1995, J ELECTRON IMAGING, V4, P31, DOI 10.1117/12.195010
   Muhlbacher B., 2010, P SPIE VIS COMM IM P, V7744
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Rajpoot NM, 2003, IEEE T IMAGE PROCESS, V12, P1460, DOI 10.1109/TIP.2003.818115
   Ramchandran K, 1993, IEEE T IMAGE PROCESS, V2, P160, DOI 10.1109/83.217221
   Reisecker M., 2004, P 6 IEEE NORD SIGN P
   SAEEDIAN P, 2004, P 2004 PICT COD S PC
   Schell T, 2003, EURASIP J APPL SIG P, V2003, P806, DOI 10.1155/S111086570330407X
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Stütz T, 2010, IEEE INT CON MULTI, P19, DOI 10.1109/ICME.2010.5582573
   Taswell C, 1996, IEEE T SIGNAL PROCES, V44, P2423, DOI 10.1109/78.539028
   Xu D, 2003, P SOC PHOTO-OPT INS, V5207, P619, DOI 10.1117/12.506601
   Yang JY, 2008, IEEE T IMAGE PROCESS, V17, P1555, DOI 10.1109/TIP.2008.926159
   Yang Y., 2006, P IEEE INT S CIRC SY
   Yang YM, 2006, LECT NOTES COMPUT SC, V4141, P480
   Yang Yongming, 2005, P IEEE INT C IM PROC, VIII, P201
NR 25
TC 4
Z9 5
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 264
EP 277
DI 10.1109/TMM.2011.2177644
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500003
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, SH
   Li, XY
   Hu, SM
   Martin, RR
AF Zhang, Song-Hai
   Li, Xian-Ying
   Hu, Shi-Min
   Martin, Ralph R.
TI Online Video Stream Abstraction and Stylization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Abstraction; color scheme replacement; optical flow; segmentation;
   temporal coherence; video stream
AB This paper gives an automatic method for online video stream abstraction, producing a temporally coherent output video stream, in a style with large regions of constant color and highlighted bold edges. Our system includes two novel components. Firstly, to provide coherent and simplified output, we segment frames, and use optical flow to propagate segmentation information from frame to frame; an error control strategy is used to help ensure that the propagated information is reliable. Secondly, to achieve coherent and attractive coloring of the output, we use a color scheme replacement algorithm specifically designed for an online video stream. We demonstrate real-time performance for CIF videos, allowing our approach to be used for live communication and other related applications.
C1 [Zhang, Song-Hai; Li, Xian-Ying; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, S Glam, Wales.
C3 Tsinghua University; Cardiff University
RP Zhang, SH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM shz@tsinghua.edu.cn; shimin@tsinghua.edu.cn;
   Ralph.Martin@cs.cardiff.ac.uk
RI Martin, Ralph R/D-2366-2010; Li, Xian-Ying/E-2142-2011; Hu,
   Shi-Min/AAW-1952-2020
FU National Basic Research Project of China [2011CB302205]; National
   Natural Science Foundation of China [60970100, 61033012]; EPSRC; EPSRC
   [EP/E034357/1] Funding Source: UKRI
FX Manuscript received September 23, 2010; accepted July 29, 2011. Date of
   publication August 18, 2011; date of current version November 18, 2011.
   This work was supported in part by the National Basic Research Project
   of China (Grant No. 2011CB302205), in part by the National Natural
   Science Foundation of China (Grant Nos. 60970100, 61033012), and in part
   by an EPSRC Travel Grant. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Nadia
   Magnenat-Thalmann.
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P584, DOI 10.1145/1015706.1015764
   Agarwala Aseem., 2002, Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering, P139
   Baker Simon, 2007, 2007 11th IEEE International Conference on Computer Vision, P1
   Bousseau A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276507
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chang YH, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P176, DOI 10.1109/CGI.2003.1214463
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Collomosse JP, 2005, IEEE T VIS COMPUT GR, V11, P540, DOI 10.1109/TVCG.2005.85
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   DEMENTHON D, 2002, P STAT METH VID PROC
   Doug DeCarlo AnthonySantella., 2004, NONPHOTOREALISTIC AN, P71
   Fischer J., 2009, INT J VIRTUAL REALIT, V7, P71
   Greenfield GR, 2003, WSCG'2003, VOL 11, NO 1, CONFERENCE PROCEEDINGS, P189
   Hays J., 2004, PROC NPAR 01, P113
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P47, DOI 10.1109/CGI.2001.934657
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   Hertzmann A., 2000, NPAR, P7
   Huang H, 2010, COMPUT GRAPH FORUM, V29, P2055, DOI 10.1111/j.1467-8659.2010.01792.x
   Huang H, 2010, VISUAL COMPUT, V26, P933, DOI 10.1007/s00371-010-0498-y
   Kang H, 2009, IEEE T VIS COMPUT GR, V15, P62, DOI 10.1109/TVCG.2008.81
   Kovács L, 2002, INT C PATT RECOG, P1090, DOI 10.1109/ICPR.2002.1048495
   Kumar MP, 2008, INT J COMPUT VISION, V76, P301, DOI 10.1007/s11263-007-0064-x
   Kyprianidis J. E., 2008, P EG UK THEOR PRACT, P51
   Kyprianidis JE, 2009, COMPUT GRAPH FORUM, V28, P1955, DOI 10.1111/j.1467-8659.2009.01574.x
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   Pang W.-M., 2008, ACM T GRAPHIC, V27, P89
   Paris S, 2008, LECT NOTES COMPUT SC, V5303, P460, DOI 10.1007/978-3-540-88688-4_34
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Sawant N, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P576, DOI 10.1109/ICVGIP.2008.17
   Vanderhaeghe D., 2007, Proc. EGSR '07, P139
   Wang CM, 2006, MATH COMPUT MODEL, V44, P608, DOI 10.1016/j.mcm.2006.01.029
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   Wen F., 2006, PROC INT S NONPHOTOR, P47
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Xiao JJ, 2005, IEEE T PATTERN ANAL, V27, P1644, DOI 10.1109/TPAMI.2005.202
   Yang CK, 2008, VISUAL COMPUT, V24, P303, DOI 10.1007/s00371-007-0183-y
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   Zitnick CL, 2005, IEEE I CONF COMP VIS, P1308
NR 43
TC 25
Z9 28
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1286
EP 1294
DI 10.1109/TMM.2011.2165052
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400009
DA 2024-07-18
ER

PT J
AU Bogdanov, D
   Serrà, J
   Wack, N
   Herrera, P
   Serra, X
AF Bogdanov, Dmitry
   Serra, Joan
   Wack, Nicolas
   Herrera, Perfecto
   Serra, Xavier
TI Unifying Low-Level and High-Level Music Similarity Measures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distance measurement; information retrieval; knowledge acquisition;
   multimedia computing; multimedia databases; music
ID INFORMATION-RETRIEVAL
AB Measuring music similarity is essential for multimedia retrieval. For music items, this task can be regarded as obtaining a suitable distance measurement between songs defined on a certain feature space. In this paper, we propose three of such distance measures based on the audio content: first, a low-level measure based on tempo-related description; second, a high-level semantic measure based on the inference of different musical dimensions by support vector machines. These dimensions include genre, culture, moods, instruments, rhythm, and tempo annotations. Third, a hybrid measure which combines the above-mentioned distance measures with two existing low-level measures: a Euclidean distance based on principal component analysis of timbral, temporal, and tonal descriptors, and a timbral distance based on single Gaussian Mel-frequency cepstral coefficient (MFCC) modeling. We evaluate our proposed measures against a number of baseline measures. We do this objectively based on a comprehensive set of music collections, and subjectively based on listeners' ratings. Results show that the proposed methods achieve accuracies comparable to the baseline approaches in the case of the tempo and classifier-based measures. The highest accuracies are obtained by the hybrid distance. Furthermore, the proposed classifier-based approach opens up the possibility to explore distance measures that are based on semantic notions.
C1 [Bogdanov, Dmitry; Serra, Joan; Wack, Nicolas; Herrera, Perfecto; Serra, Xavier] Univ Pompeu Fabra, Mus Technol Grp, Barcelona 08018, Spain.
C3 Pompeu Fabra University
RP Bogdanov, D (corresponding author), Univ Pompeu Fabra, Mus Technol Grp, Barcelona 08018, Spain.
EM dmitry.bogdanov@upf.edu; joan.serraj@upf.edu; nicolas.wack@upf.edu;
   perfecto.herrera@upf.edu; xavier.serra@upf.edu
RI Serra, Xavier/C-9299-2014; Herrera, Perfecto/C-4658-2012; Serrà,
   Joan/E-3250-2010; Serra, Xavier/JAN-6936-2023; Bogdanov,
   Dmitry/H-7278-2015
OI Serra, Xavier/0000-0003-1395-2345; Herrera,
   Perfecto/0000-0003-2799-7675; Bogdanov, Dmitry/0000-0002-9469-0633
FU Spanish Ministry of Industry, Tourism, and Trade [TSI-070100-2008-318];
   Buscamedia project [CEN-20091026]
FX This work was supported in part by the FI Grant of Generalitat de
   Catalunya (AGAUR); in part by the Music 3.0 project of the Spanish
   Ministry of Industry, Tourism, and Trade (Avanza Contenidos,
   TSI-070100-2008-318); and in part by the Buscamedia project
   (CEN-20091026).
CR ABDULLAH MB, 1990, J ROY STAT SOC D-STA, V39, P455
   Aggarwal C.C., 2005, P 31 INT C VER LARG, P901, DOI [DOI 10.5555/1083592.1083696, 10.5555/1083592.1083696]
   [Anonymous], 2006, THESIS VIENNA U TECH
   [Anonymous], 2000, ISMIR
   [Anonymous], 2009, ISMIR
   Arevalillo-Herráez M, 2008, PATTERN RECOGN LETT, V29, P2174, DOI 10.1016/j.patrec.2008.08.003
   Aucouturier Jean-Julien., 2002, Proceedings of the 3rd International Conference on Music Information Retrieval, ISMIR, P157
   Aucouturier JJ, 2005, IEEE T MULTIMEDIA, V7, P1028, DOI 10.1109/TMM.2005.858380
   Barrington L., 2009, Proceedings of the International Conference on Music Information Retrieval, P357
   BARRINGTON L, 2007, P MUS INF RETR EV EX
   BERENZWEIG A, 2003, P 2003 IEEE INT C MU, V1, P29
   Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217
   Bogdanov D, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P453, DOI 10.1109/ISM.2009.72
   Brossier Paul M., 2007, Automatic Annotation of Musical Audio for Interactive Applications
   Cano P, 2005, P ACM INT C MULT, P211
   Cano P., 2006, ISMIR 2004 Audio Description Contest
   Casey MA, 2008, P IEEE, V96, P668, DOI 10.1109/JPROC.2008.916370
   Celma O., 2006, P ESWC 2006 WORKSH M
   Celma Oscar, 2008, THESIS U P FABRA BAR
   Cripps A, 2006, IEEE INT CONF FUZZY, P323, DOI 10.1109/FUZZY.2006.1681732
   Croft W. B., 2010, SEARCH ENGINES INFOR
   CUPCHIK GC, 1982, SCAND J PSYCHOL, V23, P273, DOI 10.1111/j.1467-9450.1982.tb00441.x
   Downie JS, 2010, STUD COMPUT INTELL, V274, P93
   Downie JS, 2008, ACOUST SCI TECHNOL, V29, P247, DOI 10.1250/ast.29.247
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Fernández M, 2006, LECT NOTES COMPUT SC, V3936, P553
   Flexer Arthur., 2008, ISMIR, P173, DOI DOI 10.5281/ZENODO.1418272
   GOMEZ E, 2008, Patent No. 2009001202
   GOMEZ E, 2006, THESIS U P FABRA BAR
   Gómez E, 2008, EMPIR MUSICOL REV, V3, P140, DOI 10.18061/1811/34105
   Gouyon F., 2005, THESIS U P FABRA BAR
   Gouyon F, 2006, IEEE T AUDIO SPEECH, V14, P1832, DOI 10.1109/TSA.2005.858509
   Gruzd AA, 2007, ACM-IEEE J CONF DIG, P507, DOI 10.1145/1255175.1255307
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Homburg Helge., 2005, ISMIR, P528
   Jensen JH, 2009, IEEE T AUDIO SPEECH, V17, P693, DOI 10.1109/TASL.2008.2012314
   Konstan J., 2001, P 10 INT C WORLD WID, P285
   Korn F, 2001, IEEE T KNOWL DATA EN, V13, P96, DOI 10.1109/69.908983
   LAURIER C, 2009, P INT WORKSH CONT BA
   Laurier C, 2010, MULTIMED TOOLS APPL, V48, P161, DOI 10.1007/s11042-009-0360-2
   Levy M, 2009, IEEE T MULTIMEDIA, V11, P383, DOI 10.1109/TMM.2009.2012913
   Li T, 2006, IEEE T MULTIMEDIA, V8, P564, DOI 10.1109/TMM.2006.870730
   Logan Beth., 2001, IEEE International Conference on Multimedia and Expo, P190
   Lu GJ, 2002, IEEE T MULTIMEDIA, V4, P372, DOI 10.1109/TMM.2002.802831
   MAILLET F, 2009, P INT C MUS INF RETR
   Mandel M., 2005, ISMIR 2005, P594
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Marolt M, 2008, IEEE T MULTIMEDIA, V10, P1617, DOI 10.1109/TMM.2008.2007293
   MCFEE B, 2009, P INT C MUS INF RETR
   McKinney MF, 2006, MUSIC PERCEPT, V24, P155, DOI 10.1525/mp.2006.24.2.155
   NOVELLO A, 2006, P INT C MUS INF RETR
   Pampalk E, 2003, DAFX-03: 6TH INTERNATIONAL CONFERENCE ON DIGITAL AUDIO EFFECTS, PROCEEDINGS, P7
   Pampalk E., 2005, Proceedings of the International Conference on Music Information Retrieval, P628
   Peeters G., 2004, Tech. Rep.
   POHLE T, 2007, P MUS INF RETR EV EX
   Pohle T., 2009, P MUS INF RETR EV EX
   Pohle Tim, 2006, P 1 WORKSH LEARN SEM, P66
   Radlinski F, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P667
   Rentfrow PJ, 2003, J PERS SOC PSYCHOL, V84, P1236, DOI 10.1037/0022-3514.84.6.1236
   Saris WE., 2007, Wiley series in survey methodology. Design, evaluation, DOI DOI 10.1002/9780470165195
   Serrà J, 2009, NEW J PHYS, V11, DOI 10.1088/1367-2630/11/9/093017
   Sethares W. A., 2005, Tuning, Timbre, Spectrum, Scale
   Shental N, 2002, LECT NOTES COMPUT SC, V2353, P776
   Sigurdsson S., 2006, Proceedings of the International Conference on Music Information Retrieval, P286
   SINHA R, 2002, P CHI 02 HUM FACT CO, P831
   Slaney M., 2008, Proc. ISMIR, P313
   Slaney M., 2007, P INT S MUS INF RETR
   SMITH LM, 2010, P INT SOC MUS INF RE
   Song YQ, 2008, IEEE T MULTIMEDIA, V10, P145, DOI 10.1109/TMM.2007.911305
   Turpin A., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P11, DOI 10.1145/1148170.1148176
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   WACK N, 2006, COMP STUDY DIMENSION
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   West K, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/24602
   Witten I. H., 2005, DATA MINING PRACTICA
   Xu CS, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P429
NR 76
TC 29
Z9 30
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 687
EP 701
DI 10.1109/TMM.2011.2125784
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300009
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, D
   Yan, SC
   Hua, XS
   Zhang, HJ
AF Liu, Dong
   Yan, Shuicheng
   Hua, Xian-Sheng
   Zhang, Hong-Jiang
TI Image Retagging Using Collaborative Tag Propagation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retagging; label propagation; multi-graph multi-label learning;
   semantic correlation
ID ANNOTATION; OBJECT; SCALE
AB Photo sharing websites such as Flickr host a massive amount of social images with user-provided tags. However, these tags are often imprecise and incomplete, which essentially limits tag-based image indexing and related applications. To tackle this issue, we propose an image retagging scheme that aims at refining the quality of the tags. The retagging process is formulated as a multiple graph-based multi-label learning problem, which simultaneously explores the visual content of the images, semantic correlation of the tags as well as the prior information provided by users. Different from classical single graph-based multi-label learning algorithms, the proposed algorithm propagates the information of each tag along an individual tag-specific similarity graph, which reflects the particular relationship among the images with respect to the specific tag and at the same time the propagations of different tags interact with each other in a collaborative way with an extra tag similarity graph. In particular, we present a robust tag-specific visual sub-vocabulary learning algorithm for the construction of those tag-specific graphs. Experimental results on two benchmark Flickr image datasets demonstrate the effectiveness of our proposed image retagging scheme. We also show the remarkable performance improvements brought by retagging in the task of image ranking.
C1 [Liu, Dong] Harbin Inst Technol, Harbin 150001, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Singapore 117576, Singapore.
   [Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Zhang, Hong-Jiang] Microsoft Adv Technol Ctr, Beijing 100080, Peoples R China.
C3 Harbin Institute of Technology; National University of Singapore;
   Microsoft Research Asia; Microsoft; Microsoft
RP Liu, D (corresponding author), Harbin Inst Technol, Harbin 150001, Peoples R China.
EM dongliu.hit@gmail.com; eleyans@nus.edu.sg; xshua@microsoft.com;
   hjzhang@microsoft.com
RI Yan, Shuicheng/HCI-1431-2022; Liu, Dong/AAL-8559-2021
FU National Research Foundation (NRF) [CSIDM-200803]
FX This research is completed for CSIDM Project No. CSIDM-200803, which was
   supported in part by a grant from the National Research Foundation (NRF)
   administered by the Media Development Authority (MDA) of Singapore. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Francesco G. B. De Natale.
CR Ames M, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P971
   [Anonymous], P ACM MULT
   [Anonymous], 2006, P 20 ANN C NEUR INF
   [Anonymous], P 24 INT C MACH LEAR
   BERTSEKAS DP, 1976, IEEE T AUTOMAT CONTR, V21, P174, DOI 10.1109/TAC.1976.1101194
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chang E, 2003, IEEE T CIRC SYST VID, V13, P26, DOI 10.1109/TCSVT.2002.808079
   Chen G., 2008, P SIAM INT C DAT MIN, P410, DOI DOI 10.1137/1.9781611972788.37
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Fellbaum C, 1998, LANG SPEECH & COMMUN, P1
   FENG S, 2004, P IEEE COMP SOC C CO, P1063
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jeon J., 2003, Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P119, DOI DOI 10.1145/860435.860459
   Jin Y., 2005, P 13 ANN ACM INT C M, P706
   Johnson R, 2007, J MACH LEARN RES, V8, P1489
   Kang F., 2006, CVPR, V2, P1719
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   LIU D, 2009, P 18 INT C WORLD WID, P351, DOI DOI 10.1145/1526709.1526757
   Liu Dong., 2010, Proceedings of the International Conference on Multimedia, P491
   Liu Y., 2008, P IEEE EN 2030 C ATL, P1, DOI DOI 10.1109/ENERGY.2008.4781069
   Liu Y., 2006, Proc. of Conference on Artificial Intelligence, P421
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Muller H., 2002, PROC INT C IMAGE VID, P38
   Sha F, 2007, NEURAL COMPUT, V19, P2004, DOI 10.1162/neco.2007.19.8.2004
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang L, 2007, 2007 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN SECURITY AND DEFENSE APPLICATIONS, P1
   Zhang H., 2006, 2006 IEEE COMP SOC C, V2, P2126, DOI DOI 10.1109/CVPR.2006.301
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   ZHOU D, 2008, P 17 INT C WORLD WID, P141
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 36
TC 55
Z9 59
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 702
EP 712
DI 10.1109/TMM.2011.2134078
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300010
DA 2024-07-18
ER

PT J
AU Arun, KR
   Yap, X
   Khong, AWH
AF Arun, Kattukandy Rajan
   Yap, XueXin
   Khong, Andy W. H.
TI A Touch Interface Exploiting Time-Frequency Classification Using Zak
   Transform for Source Localization on Solids
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human-computer interface; location template matching; time-frequency
   classification; touch interface
ID DISCRETE; IMPACT
AB We propose a new approach to the development of a touch interface using surface-mounted sensors which allows one to convert a hard surface into a touch pad. This is achieved by using location template matching (LTM), a source localization algorithm that is robust to dispersion and multipath. In this interdisciplinary research, we employ mechanical vibration theories that model wave propagation of the flexural modes of vibration generated by an impact on the surface. We then verify that the amplitude variance across time for each propagating mode frequency is unique to each location on a surface. We show that the Zak transform allows us to faithfully track these amplitude variations and we exploit the uniqueness of this variance as a time-frequency classifier which in turn allows us to localize a finger tap in the context of a human-computer interface. The performance of the proposed algorithm is compared with existing LTM approaches on real surfaces.
C1 [Arun, Kattukandy Rajan; Yap, XueXin; Khong, Andy W. H.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Arun, KR (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM arun4@e.ntu.edu.sg; xxyap@ntu.edu.sg; andykhong@ntu.edu.sg
RI Khong, Andy/A-5169-2011
OI Khong, Andy/0000-0002-0708-4791
FU Singapore National Research Foundation [NRF2008IDM-IDM004-010]
FX This work was supported by the Singapore National Research Foundation
   Interactive Digital Media R&D Program, under research grant
   NRF2008IDM-IDM004-010. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Zhengyou Zhang.
CR ANGELIDIS PA, 1993, J MAGN RESON SER A, V103, P191, DOI 10.1006/jmra.1993.1152
   [Anonymous], P VIRT C MAY
   [Anonymous], P VIRT INT C MAY
   Bolcskei H, 1997, IEEE T SIGNAL PROCES, V45, P851, DOI 10.1109/78.564174
   BORNAND C, 2005, P 2 INT C EN INT GEN
   Cheng C, 2007, IEEE T CIRCUITS-I, V54, P791, DOI 10.1109/TCSI.2006.888772
   COHEN L, 1989, P IEEE, V77, P941, DOI 10.1109/5.30749
   HARRIS FJ, 1978, P IEEE, V66, P51, DOI 10.1109/PROC.1978.10837
   Kim J, 2007, IEEE PACIF, P387
   KIM YW, 2002, P IEEE 2 ANN INT EMB
   Kundu T, 2008, ULTRASONICS, V48, P193, DOI 10.1016/j.ultras.2007.12.001
   OHair JR, 1996, IEEE T SIGNAL PROCES, V44, P1099, DOI 10.1109/78.502324
   Poletkin K, 2010, IEEE INT CON MULTI, P286, DOI 10.1109/ICME.2010.5582570
   Ribay G, 2007, IEEE T ULTRASON FERR, V54, P378, DOI 10.1109/TUFFC.2007.251
   SHINAND G, 2007, P IEEE INT C CONV IN
   Sulaiman A, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P82, DOI 10.1109/CW.2010.72
   Sundararajan D, 1997, IEEE T SIGNAL PROCES, V45, P2010, DOI 10.1109/78.611197
   VENTSELAND E, 2001, THIN PLATES SHELLS T
   Yap X, 2010, INT CONF ACOUST SPEE, P2490, DOI 10.1109/ICASSP.2010.5494897
NR 19
TC 10
Z9 10
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 487
EP 497
DI 10.1109/TMM.2011.2123084
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700008
DA 2024-07-18
ER

PT J
AU Yu, G
   Goussies, NA
   Yuan, JS
   Liu, ZC
AF Yu, Gang
   Goussies, Norberto A.
   Yuan, Junsong
   Liu, Zicheng
TI Fast Action Detection via Discriminative Random Forest Voting and Top-K
   Subvolume Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action detection; branch and bound; random forest; top-K search
ID RECOGNITION
AB Multiclass action detection in complex scenes is a challenging problem because of cluttered backgrounds and the large intra-class variations in each type of actions. To achieve efficient and robust action detection, we characterize a video as a collection of spatio-temporal interest points, and locate actions via finding spatio-temporal video subvolumes of the highest mutual information score towards each action class. A random forest is constructed to efficiently generate discriminative votes from individual interest points, and a fast top-K subvolume search algorithm is developed to find all action instances in a single round of search. Without significantly degrading the performance, such a top-K search can be performed on down-sampled score volumes for more efficient localization. Experiments on a challenging MSR Action Dataset II validate the effectiveness of our proposed multiclass action detection method. The detection speed is several orders of magnitude faster than existing methods.
C1 [Yu, Gang; Yuan, Junsong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Goussies, Norberto A.] Univ Buenos Aires, Image Proc Grp, Buenos Aires, DF, Argentina.
   [Liu, Zicheng] Microsoft Res, Redmond, WA 98052 USA.
C3 Nanyang Technological University; University of Buenos Aires; Microsoft
RP Yu, G (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM gyu1@e.ntu.edu.sg; ngoussie@dc.uba.ar; jsyuan@ntu.edu.sg;
   zliu@microsoft.com
RI Yuan, Junsong/R-4352-2019; Yuan, Junsong/A-5171-2011
OI Yu, Gang/0000-0001-5570-2710; Yuan, Junsong/0000-0002-7901-8793
FU  [SUG M58040015]
FX The work of J. Yuan was supported in part by the Nanyang Assistant
   Professorship SUG M58040015. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Chia-Wen Lin.
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P BMVC
   [Anonymous], 2007, P IEEE INT C COMP VI
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], P ACM MULT WORKSH EV
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P NEUR INF PROC SYST
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2010, P EUR C COMP VIS ECC
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], 2009, P IEEE INT C COMP VI
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   BREITENBACH M, 2003, CUCS95403 U COL BOUL
   Brodal GS, 2007, LECT NOTES COMPUT SC, V4708, P442
   CAO L, 2010, P IEEE P COMP VIS PA
   Cao L, 2010, IEEE INT C INT ROBOT
   Derpanis K., 2010, P COMP VIS PATT REC
   DUAN L, 2010, P COMP VIS PATT REC
   Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755
   Jiang H, 2010, IEEE T CIRC SYST VID, V20, P50, DOI 10.1109/TCSVT.2009.2026947
   Ke Y., 2007, P IEEE INT C COMP VI
   Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167
   Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144
   Lampert Christoph H., 2009, P IEEE INT C COMP VI
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   LEPETIT V, 2005, P COMP VIS PATT REC
   LI Z, 2008, P ACM INT C MULT
   MIKOLAJCZYK K, 2008, P COMP VIS PATT REC
   NORBERT A, 2010, P IEEE C MULT EXP IC
   PRABHAKAR K, 2010, P COMP VIS PATT REC
   Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727
   Schroff F, 2008, P BRIT MACH VIS C, P1
   SCHULDT C, 2004, P IEEE C PATT REC
   Seo HJ, 2010, IEEE T PATTERN ANAL, V32, P1688, DOI 10.1109/TPAMI.2009.153
   Shechtman E., 2005, P IEEE C COMP VIS PA
   Wang H., 2009, BMVC
   Wang P, 2009, P IEEE INT C COMP VI
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Yacoob Y, 1999, COMPUT VIS IMAGE UND, V73, P232, DOI 10.1006/cviu.1998.0726
   YANG M, 2009, P IEEE WORKSH VID OR
   YAO A, 2010, P COMP VIS PATT REC
   Yoon J, 2009, IEEE INTERNATIONAL CONFERENCE ON MICROWAVES, COMMUNICATIONS, ANTENNAS AND ELECTRONICS SYSTEMS (COMCAS 2009)
   Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPRW.2009.5206671, 10.1109/CVPR.2009.5206671]
   Zhu Guangyu., 2009, Proceedings of the 17th acm international conference on multimedia, P165
NR 47
TC 46
Z9 50
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 507
EP 517
DI 10.1109/TMM.2011.2128301
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700010
DA 2024-07-18
ER

PT J
AU Ulges, A
   Worring, M
   Breuel, T
AF Ulges, Adrian
   Worring, Marcel
   Breuel, Thomas
TI Learning Visual Contexts for Image Annotation From Flickr Groups
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based image retrieval; context; image annotation
AB We present an extension of automatic image annotation that takes the context of a picture into account. Our core assumption is that users do not only provide individual images to be tagged, but group their pictures into batches (e. g., all snapshots taken over the same holiday trip), whereas the images within a batch are likely to have a common style. These batches are matched with categories learned from Flickr groups, and an accurate context-specific annotation is performed.
   In quantitative experiments, we demonstrate that Flickr groups, with their user-driven categorization and their rich group space, provide an excellent basis for learning context categories. Our approach-which can be integrated with virtually any annotation model-is demonstrated to give significant improvements of above 100%, compared to standard annotations of individual images.
C1 [Ulges, Adrian] German Res Ctr Artificial Intelligence, D-67663 Kaiserslautern, Germany.
   [Worring, Marcel] Univ Amsterdam, Intelligent Syst Lab, NL-1098 GH Amsterdam, Netherlands.
   [Breuel, Thomas] Univ Kaiserslautern, Dept Comp Sci, D-67663 Kaiserslautern, Germany.
C3 University of Amsterdam; University of Kaiserslautern
RP Ulges, A (corresponding author), German Res Ctr Artificial Intelligence, D-67663 Kaiserslautern, Germany.
EM adrian.ulges@dfki.de; m.worring@uva.nl; tmb@cs.uni-kl.de
RI Worring, Marcel/JRW-7059-2023
OI Worring, Marcel/0000-0003-4097-4136
FU German Research Foundation (DFG) [BR 2517/1-1]
FX This work was supported by the German Research Foundation (DFG) under
   project MOONVID (BR 2517/1-1). The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Svetha Venkatesh.
CR [Anonymous], P ACM MULT
   [Anonymous], 2008, PROC AMERICASCONF IN
   [Anonymous], COMPUTER VISION
   [Anonymous], 2003, INT C INT C MACH LEA, DOI DOI 10.1016/0026-2714(92)90278-S
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   COTO AL, 2008, P SAMT WORKSH CROSS
   CRISTANI M, 2008, P CVPR
   DUAN M, 2009, P CIVR
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   FENG S, 2008, P CIVR, P427
   Feng S.L., 2004, P CVPR
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Hrishikesh A, 2009, P WORKSH INT MULT MI
   Jeon J, 2004, LECT NOTES COMPUT SC, V3115, P24
   KUCK H, 2004, P ECCV, P1
   Lavrenko V., 2004, Advances in Neural Information Processing Systems, V16
   LEON J, 2003, P INT C RES DEV INF, P119
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   LI L.-J., 2007, P CVPR
   Li X., 2006, P ACM MM
   LI X, 2009, P ICASSP
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24
   Marin-Castro H, 2007, LECT NOTES COMPUT SC, V4756, P487
   Metzler D, 2004, LECT NOTES COMPUT SC, V3115, P42
   Monay F., 2004, P 12 ANN ACM INT C M, P348, DOI [10.1145/1027527.1027608, DOI 10.1145/1027527.1027608]
   MORI Y, 1999, P WORKSH MULT INT ST
   Muller H., 2002, PROC INT C IMAGE VID, P38
   Naaman M, 2005, ACM-IEEE J CONF DIG, P178, DOI 10.1145/1065385.1065430
   Negoescu R.-A., 2009, P ACM MM
   Negoescu R.A., 2008, Proc. Content-based Image and Video Retrieval, P417, DOI DOI 10.1145/1386352.1386406
   RENN M, 2008, P INT WORKSH AD MULT
   Sarkar P, 2005, IEEE T PATTERN ANAL, V27, P88, DOI 10.1109/TPAMI.2005.18
   Setz AT, 2009, IEEE INT CON MULTI, P1460, DOI 10.1109/ICME.2009.5202778
   Sivic J, 2006, LECT NOTES COMPUT SC, V4170, P127
   Tang JY, 2007, IEEE T CIRC SYST VID, V17, P384, DOI 10.1109/TCSVT.2006.888941
   ULGES A, 2008, P ICVS
   Ulges A, 2010, COMPUT VIS IMAGE UND, V114, P429, DOI 10.1016/j.cviu.2009.08.002
   VIITANIEMI V, 2007, P VISUAL
   WANG CH, 2006, P ACM MM
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   YANG C, 2005, P ACM MM
   Yang Qingxiong., 2008, CIVR, CIVR'08, P591
   Yavlinsky A, 2005, LECT NOTES COMPUT SC, V3568, P507
NR 48
TC 36
Z9 43
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 330
EP 341
DI 10.1109/TMM.2010.2101051
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shao, J
   Shen, HT
   Huang, Z
   Zhou, XF
AF Shao, Jie
   Shen, Heng Tao
   Huang, Zi
   Zhou, Xiaofang
TI Exploring Distributional Discrepancy for Multidimensional Point Set
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributional discrepancy; hypothesis tests; multidimensional point
   set; nonparametric; similarity measures
ID MULTIVARIATE; SIMILARITY
AB How to effectively and efficiently assess similarity is a long-standing and challenging research problem in various multimedia applications. For ranked retrieval in a collection of objects based on series of multivariate observations (e.g., searching similar video clips to a query example), satisfactory performance cannot be achieved by using many conventional similarity measures that aggregate element-to-element comparison results. Some correlation information among the individual elements has also been investigated to characterize each set of multidimensional points for comparison, but with an unwarranted assumption that the underlying data distribution has a particular parametric form. Motivated by these concerns, measuring the similarity of multidimensional point sets is approached from a novel collective perspective in this paper, by evaluating the probability that they are consistent with a same distribution. We propose to make use of nonparametric hypothesis tests in statistics to compute the distributional discrepancy of samples for assessing the degree of similarity between two ensembles of points. While our proposal is mainly presented in the context of video similarity search, it enjoys great flexibility and is extensible to other applications where multidimensional point set representations are involved, such as motion capture retrieval.
C1 [Shao, Jie] Univ Melbourne, Dept Comp Sci & Software Engn, Parkville, Vic 3010, Australia.
   [Shen, Heng Tao; Huang, Zi; Zhou, Xiaofang] Univ Queensland, Sch Informat Technol & Elect Engn, St Lucia, Qld 4072, Australia.
   [Huang, Zi; Zhou, Xiaofang] Natl ICT Australia, Queensland Res Lab, Sydney, NSW, Australia.
C3 University of Melbourne; University of Queensland; NICTA
RP Shao, J (corresponding author), Univ Melbourne, Dept Comp Sci & Software Engn, Parkville, Vic 3010, Australia.
EM jsh@csse.unimelb.edu.au; shenht@itee.uq.edu.au; huang@itee.uq.edu.au;
   zxf@itee.uq.edu.au
RI Zhou, Xiaofang/C-6169-2013; Shen, Heng Tao/ABD-5331-2021; Zhou,
   Xiangfeng/KDO-8724-2024
OI Zhou, Xiaofang/0000-0001-6343-1455; HUANG, ZI/0000-0002-9738-4949
CR ANDERSON NH, 1994, J MULTIVARIATE ANAL, V50, P41, DOI 10.1006/jmva.1994.1033
   [Anonymous], 2002, Encyclopaedia of mathematics
   [Anonymous], 2006, P ADV NEUR INF PROC
   [Anonymous], 2002, THESIS U NEW S WALES
   [Anonymous], P 16 INT C DAT ENG S
   Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217
   Biau G, 2005, IEEE T INFORM THEORY, V51, P3965, DOI 10.1109/TIT.2005.856979
   Cappelli R, 2001, IEEE T PATTERN ANAL, V23, P977, DOI 10.1109/34.955111
   Chalmond B, 1999, IEEE T PATTERN ANAL, V21, P422, DOI 10.1109/34.765654
   Chen L, 2004, P 30 INT C VER LARG, V30, P792, DOI [DOI 10.1016/B978-012088469-8.50070-X, DOI 10.1016/B978-012088469-8/50070-X]
   Efron B., 1993, INTRO BOOTSTRAP, VVolume 914, DOI DOI 10.1007/978-1-4899-4541-9
   FRIEDMAN JH, 1979, ANN STAT, V7, P697, DOI 10.1214/aos/1176344722
   Goh K., 2002, P ACM INT C MULTIMED, P466
   HENZE N, 1988, ANN STAT, V16, P772, DOI 10.1214/aos/1176350835
   HOI CH, 2003, P INT C IM VID RETR, P373
   Huang Z, 2009, ACM T INFORM SYST, V27, DOI 10.1145/1508850.1508855
   Jacobs DW, 2000, IEEE T PATTERN ANAL, V22, P583, DOI 10.1109/34.862197
   Keogh E., 2002, Proceedings of the Twenty-eighth International Conference on Very Large Data Bases, P406
   Korolyuk VS., 1994, Theory of u-statistics
   Li CJ, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236475
   Pradhan G, 2008, IEEE MULTIMEDIA, V15, P20, DOI 10.1109/MMUL.2008.28
   Puzicha J, 1997, PROC CVPR IEEE, P267, DOI 10.1109/CVPR.1997.609331
   Rubner Y, 2001, COMPUT VIS IMAGE UND, V84, P25, DOI 10.1006/cviu.2001.0934
   SCHILLING MF, 1986, J AM STAT ASSOC, V81, P799, DOI 10.2307/2289012
   Shao J, 2008, P 16 ACM INT C MULT, P429
   SKOPAL T, 2006, P INT C EXT DAT TECH, P718
   Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252
   Theoharatos C, 2006, PATTERN RECOGN, V39, P1892, DOI 10.1016/j.patcog.2006.04.015
   Vlachos M, 2002, PROC INT CONF DATA, P673, DOI 10.1109/ICDE.2002.994784
   Wald A, 1940, ANN MATH STAT, V11, P147, DOI 10.1214/aoms/1177731909
   Xu JX, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P254
   Yang KY, 2007, INFORM COMPUT, V205, P65, DOI 10.1016/j.ic.2006.08.004
   Zezula P., 2006, Similarity Search: The Metric Space Approach, V1st ed.
   Zhao T, 2006, I S BIOMED IMAGING, P562
NR 34
TC 1
Z9 1
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 71
EP 81
DI 10.1109/TMM.2010.2085424
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900008
DA 2024-07-18
ER

PT J
AU Jayagopi, DB
   Gatica-Perez, D
AF Jayagopi, Dinesh Babu
   Gatica-Perez, Daniel
TI Mining Group Nonverbal Conversational Patterns Using Probabilistic Topic
   Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Discovery; group behavior descriptor; meetings; nonverbal cues
ID BEHAVIOR
AB The automatic discovery of group conversational behavior is a relevant problem in social computing. In this paper, we present an approach to address this problem by defining a novel group descriptor called bag of group-nonverbal-patterns (NVPs) defined on brief observations of group interaction, and by using principled probabilistic topic models to discover topics. The proposed bag of group NVPs allows fusion of individual cues and facilitates the eventual comparison of groups of varying sizes. The use of topic models helps to cluster group interactions and to quantify how different they are from each other in a formal probabilistic sense. Results of behavioral topics discovered on the Augmented Multi-Party Interaction (AMI) meeting corpus are shown to be meaningful using human annotation with multiple observers. Our method facilitates "group behavior-based" retrieval of group conversational segments without the need of any previous labeling.
C1 [Jayagopi, Dinesh Babu; Gatica-Perez, Daniel] Idiap Res Inst, Social Comp Grp, Martigny, Switzerland.
   [Jayagopi, Dinesh Babu; Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Jayagopi, DB (corresponding author), Idiap Res Inst, Social Comp Grp, Martigny, Switzerland.
EM djaya@idiap.ch; gatica@idiap.ch
RI Jayagopi, Dinesh Babu/ABE-2546-2021
OI Jayagopi, Dinesh Babu/0000-0003-0080-452X
FU EU; Swiss NCCR [IM2]
FX Manuscript received December 23, 2009; revised July 05, 2010; accepted
   July 12, 2010. Date of publication August 09, 2010; date of current
   version November 17, 2010. This work was supported in part by the EU
   project AMIDA and the Swiss NCCR IM2. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Shrikanth Narayanan.
CR [Anonymous], 2005, PARAMETER ESTIMATION
   [Anonymous], 2008, P ICMI 2008, DOI DOI 10.1145/1452392.1452404
   [Anonymous], GEN EXPECTANCIES INT
   [Anonymous], 2008, HONEST SIGNALS, DOI DOI 10.7551/MITPRESS/8022.001.0001
   [Anonymous], P 10 INT C MULT INT
   [Anonymous], 1950, AM SOCIOL REV
   Bachour K, 2008, LECT NOTES COMPUT SC, V5192, P39
   Basu S., 2001, P IEEE CVPR WORKSH C
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   BOUREAU Y, 2010, P INT C COMP VIS PAT
   BOYDGRABER J, 2009, ADV NEURAL INF PROCE, V31
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Dielmann A, 2007, IEEE T MULTIMEDIA, V9, P25, DOI 10.1109/TMM.2006.886337
   DiMicco J.M., 2006, C HUMAN FACTORS COMP, P706
   DINES J, 2006, P 9 INT C SPOK LANG
   DONG W, IEEE T AUTON M UNPUB
   Dong W, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P271
   DOURISH P, 1992, P 1992 ACM C COMPUTE, P114
   GARG NP, 2008, P ACM INT C MULT VAN
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   GATICAPEREZ D, 2005, P IEEE INT C AC SPEE, V1
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hall JA, 2005, PSYCHOL BULL, V131, P898, DOI 10.1037/0033-2909.131.6.898
   Hassin R., 2005, NEW UNCONSCIOUS
   JAYAGOPI D, 2009, P INT C MULT EXP ICM
   Jayagopi D., 2009, P INT C MULT INT ICM
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   John O. P., 1999, BIG 5 TRAIT TAXONOMY
   Kim T, 2008, CSCW: 2008 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, CONFERENCE PROCEEDINGS, P457
   Knapp M.L., 1978, NONVERBAL COMMUNICAT
   LEVINE JM, 1990, ANNU REV PSYCHOL, V41, P585, DOI 10.1146/annurev.ps.41.020190.003101
   Lewin K, 1939, J SOC PSYCHOL, V10, P271, DOI 10.1080/00224545.1939.9713366
   MacKay D., 2003, INFORM THEORY INFERE
   Manusov V., 2006, The Sage Handbook of Nonverbal Communication
   McGrath J.E., 1984, GROUPS INTERACTION P
   McNeill D., 2000, LANGUAGE GESTURE
   MOORE D, 2002, P IDIAP COM 02, V7
   OTSUKA K, 2006, P CHI 06 HUM FACT CO, P1180
   Otsuka K, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P255
   Pianesi F, 2008, PERS UBIQUIT COMPUT, V12, P181, DOI 10.1007/s00779-007-0144-5
   Poole MS, 2004, SMALL GR RES, V35, P3, DOI 10.1177/1046496403259753
   RADUCANU B, 2009, P IEEE INT C AC SPEE
   REMLAND M, 2006, USES CONSEQUENCES NO
   RIENKS RJ, 2005, P MACH LEARN MULT IN
   SANCHEZCORTES D, 2009, P WORKSH MULT SENS B
   STEYVERS M, 2007, LATENT SEMANTIC ANAL, V427
   Sturm J, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P263
   Vinciarelli A, 2007, IEEE T MULTIMEDIA, V9, P1215, DOI 10.1109/TMM.2007.902882
   WREDE B, 2003, P 8 EUR C SPEECH COM
   Zhang D, 2006, IEEE T MULTIMEDIA, V8, P509, DOI 10.1109/TMM.2006.870735
NR 50
TC 24
Z9 25
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 790
EP 802
DI 10.1109/TMM.2010.2065218
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100002
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, RX
   McKenna, SJ
   Han, JW
   Ward, AA
AF Wang, Ruixuan
   McKenna, Stephen J.
   Han, Junwei
   Ward, Annette A.
TI Visualizing Image Collections Using High-Entropy Layout Distributions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based browsing; high-entropy layout distribution (HELD); image
   layouts; manifold learning; Renyi entropy
ID NONLINEAR DIMENSIONALITY REDUCTION
AB Mechanisms for visualizing image collections are essential for browsing and exploring their content. This is especially true when metadata are ineffective in retrieving items due to the sparsity or esoteric nature of text. An obvious approach is to automatically lay out sets of images in ways that reflect relationships between the items. However, dimensionality reduction methods that map from high-dimensional content-based feature distributions to low-dimensional layout spaces for visualization often result in displays in which many items are occluded whilst large regions are empty or only sparsely populated. Furthermore, such methods do not consider the shape of the region of layout space to be populated. This paper proposes a method, high-entropy layout distributions. that addresses these limitations. Layout distributions with low differential entropy are penalized. An optimization strategy is presented that finds layouts that have high differential entropy and that reflect inter-image similarities. Efficient optimization is obtained using a step-size constraint and an approximation to quadratic (Renyi) entropy. Two image archives of cultural and commercial importance are used to illustrate and evaluate the method. A comparison with related methods demonstrates its effectiveness.
C1 [Wang, Ruixuan; McKenna, Stephen J.; Han, Junwei; Ward, Annette A.] Univ Dundee, Sch Comp, Angus DD1 4HN, Scotland.
C3 University of Dundee
RP Wang, RX (corresponding author), Univ Dundee, Sch Comp, Angus DD1 4HN, Scotland.
EM ruixuanwang@computing.dundee.ac.uk; stephen@computing.dundee.ac.uk;
   junweihan@hotmail.com; award@computing.dundee.ac.uk
RI McKenna, Stephen/AAL-8335-2020
OI McKenna, Stephen/0000-0003-0530-2035
FU U.K. Technology Strategy Board [FABRIC]; Liberty Art Fabrics; System
   Simulation; Victoria and Albert Museum
FX Manuscript received March 18, 2010; revised June 20, 2010; accepted June
   26, 2010. Date of publication July 15, 2010; date of current version
   November 17, 2010. This work was supported by the U.K. Technology
   Strategy Board under Grant FABRIC in collaboration with Liberty Art
   Fabrics, System Simulation, and the Victoria and Albert Museum. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jia Li.
CR [Anonymous], 2002, NEOPLASIA
   [Anonymous], 1991, P 1991 IEEE COMP SOC, DOI DOI 10.1109/CVPR.1991.139758
   BASALAJ W, 2000, THESIS U CAMBRIDGE C
   BEDERSON BB, 2001, P 2001 ACM S US INT, V3, P71
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Cayton L., 2005, CS20080923 UCSD
   Cox M.F., 2001, MULTIDIMENSIONAL SCA
   Donoho D.L., 2005, Proceedings of the National Academy of Sciences, V102, p7,426
   Fabbri R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1322432.1322434
   Fan J., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, ser. MIR'08, Vancouver, British Columbia, P358, DOI [DOI 10.1145/1460096.1460155, 10.1145/1460096.1460155]
   Gomi A, 2008, IEEE INT CONF INF VI, P82, DOI 10.1109/IV.2008.8
   Kang HM, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1539, DOI 10.1109/ICME.2000.871061
   Liu HN, 2004, FIRST INTERNATIONAL CONFERENCE ON QUALITY OF SERVICE IN HETEROGENEOUS WIRED/WIRELESS NETWORKS, PROCEEDINGS, P84
   Moghaddam B, 2004, INT J COMPUT VISION, V56, P109, DOI 10.1023/B:VISI.0000004834.62090.74
   Nadler B, 2006, APPL COMPUT HARMON A, V21, P113, DOI 10.1016/j.acha.2005.07.004
   Nguyen GP, 2008, J VISUAL LANG COMPUT, V19, P203, DOI 10.1016/j.jvlc.2006.09.002
   Paccanaro A, 2002, ADV NEUR IN, V14, P857
   Principe JC, 2000, J VLSI SIG PROCESS S, V26, P61, DOI 10.1023/A:1008143417156
   Renyi A., 1961, P 4 BERK S MATH STAT, V1, P547
   RODDEN K, 2002, THESIS U CAMBRIDGE C
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   SAUL LK, 2006, SPECTRAL METHODS DIM, pCH16
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   van Der Maaten L., 2009, 2009005 TICCTR TILB
   Wang R Q, 2009, P IEEE C DEC CONTR 2, P16
   WEINBERGER KQ, 2005, P INT WORKSH AI STAT
   Zhang ZY, 2004, SIAM J SCI COMPUT, V26, P313, DOI 10.1137/S1064827502419154
   [No title captured]
NR 30
TC 6
Z9 8
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 803
EP 813
DI 10.1109/TMM.2010.2057411
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100003
DA 2024-07-18
ER

PT J
AU Douze, M
   Jégou, H
   Schmid, C
AF Douze, Matthijs
   Jegou, Herve
   Schmid, Cordelia
TI An Image-Based Approach to Video Copy Detection With Spatio-Temporal
   Post-Filtering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Large-scale indexing; spatio-temporal verification; video databases
AB This paper introduces a video copy detection system which efficiently matches individual frames and then verifies their spatio-temporal consistency. The approach for matching frames relies on a recent local feature indexing method, which is at the same time robust to significant video transformations and efficient in terms of memory usage and computation time. We match either keyframes or uniformly sampled frames. To further improve the results, a verification step robustly estimates a spatio-temporal model between the query video and the potentially corresponding video segments.
   Experimental results evaluate the different parameters of our system and measure the trade-off between accuracy and efficiency. We show that our system obtains excellent results for the TRECVID 2008 copy detection task.
C1 [Douze, Matthijs; Jegou, Herve; Schmid, Cordelia] INRIA Rhone Alpes, F-38334 Saint Ismier, France.
RP Douze, M (corresponding author), INRIA Rhone Alpes, F-38334 Saint Ismier, France.
EM matthijs.douze@inria.fr; herve.jegou@inria.fr; cordelia.schmid@inria.fr
FU ANR; Gravit; Quaero
FX This work was supported in part by the ANR project Raffut, in part by
   the Gravit project, and in part by the Quaero project. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Zhu Liu.
CR [Anonymous], P INT C COMP VIS
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], P EUR C COMP VIS
   [Anonymous], P ACM INT WORKSH MUL
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Jegou H., 2009, P C COMP VIS PATT RE
   JOLY A, 2005, P INT C IM PROC
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   KRAAIJ W, 2008, FINAL CBCD EVALUATIO
   LAWTO J, 2007, P C IM VID RETR
   LIENHART R, 2009, P C IM VID RETR
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   MIKOLAJCZYK K, BINARIES AFFINE COVA
   Naturel X, 2008, MULTIMED TOOLS APPL, V38, P233, DOI 10.1007/s11042-007-0180-1
   Saoudi A, 2007, PROC WRLD ACAD SCI E, V22, P45
   Sivic J, 2008, P IEEE, V96, P548, DOI 10.1109/JPROC.2008.916343
   Weiss Y., 2008, ADV NEURAL INF PROCE, V21
NR 21
TC 105
Z9 130
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 257
EP 266
DI 10.1109/TMM.2010.2046265
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100003
OA Green Published
DA 2024-07-18
ER

PT J
AU Qin, M
   Zimmermann, R
AF Qin, Min
   Zimmermann, Roger
TI An Adaptive Strategy for Mobile Ad Hoc Media Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content distribution; media streaming; mobile environments; wireless ad
   hoc communication
ID LINK QUALITY; PREDICTION
AB Mobile devices are increasingly popular and many of them are capable of handling multimedia content. Users enjoy the ability to access their collection of media objects anywhere. Wireless connectivity is often integrated into these handhelds, therefore providing the opportunity to stream multimedia content among mobile and ad hoc peers. An important consideration is to transfer a multimedia object in its entirety. This is often challenging since the transmission time for such an object can be considerable due to an unfavorable combination of a large object size and limited available bandwidth. We previously introduced a novel strategy to improve the probability of success to stream a video sequence based on studying the minimum buffer size. This strategy takes advantage of layered video encoding schemes such as scalable video coding (SVC) or multiple description coding (MDC). The technique adaptively selects the number of layers to be streamed to deliver more frames before the wireless link disconnects while keeping the video quality high. In our current study, we simplify this strategy by using the streaming probability alone to dynamically adjust the number of layers to be delivered. Our proposed technique improves the prediction accuracy by incorporating the 802.11 Auto-Rate Fallback (ARF) scheme along with two popular mobility models: the random waypoint and the random walk mobility models. While ARF-which steps down the sending rate when consecutive transmission errors occur-is implemented in all hardware that follows the popular IEEE 802.11 standard, it is not commonly modeled in existing work. In addition, our approach can retransmit missing layers if peers reconnect after a link break, hence improving the rendering quality. We have performed extensive simulations to validate our technique and the results show an improvement in streaming probability as well as the number of layers that are transmitted.
C1 [Qin, Min] Google Inc, Irvine, CA 92612 USA.
   [Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
C3 Google Incorporated; National University of Singapore
RP Qin, M (corresponding author), Google Inc, Irvine, CA 92612 USA.
EM qinmin@google.com; rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590
FU National Science Foundation (NSF) [EEC-529152, IIS-0534761]; Singapore
   Ministry of Education (MoE)
FX This work was supported in part by the National Science Foundation (NSF)
   under Grants EEC-529152 (Integrated Media Systems Center, Engineering
   Research Center) and IIS-0534761, and in part by a National University
   of Singapore Academic Research Fund (AcRF) grant from the Singapore
   Ministry of Education (MoE). The associate editor coordinating the
   review of the manuscript and approving it for publication was Dr. S.-H.
   Gary Chan.
CR [Anonymous], P IEEE RSJ INT C INT
   [Anonymous], 1996, WIRELESS COMMUNICATI
   [Anonymous], 2000, SPOTON INDOOR 3D LOC
   BACH E, 2003, P 2 INT C MOB UB MUL
   BARARIA S, 2004, P 4 WORKSH APPL SERV
   BOUDEC JYL, 2005, P INFOCOM MAR
   Camp T, 2002, WIREL COMMUN MOB COM, V2, P483, DOI 10.1002/wcm.72
   CHOU PA, 2003, P INT PACK VID WORKS
   Gaertner G, 2004, LECT NOTES COMPUT SC, V3260, P147
   Gaertner G, 2004, IEEE INTERNET COMPUT, V8, P55, DOI 10.1109/MIC.2004.1260704
   Gerharz M, 2003, C LOCAL COMPUT NETW, P130
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   HAN Y, 2004, P 16 ITC SPEC SEM SE
   Jiang SM, 2004, IEEE T COMMUN, V52, P183, DOI 10.1109/TCOMM.2003.822739
   Lacage M. H., 2004, 7 ACM INT S MODEL AN, P126
   Li BC, 2003, IEEE J SEL AREA COMM, V21, P1627, DOI 10.1109/JSAC.2003.815964
   Liang B., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1293, DOI 10.1109/INFCOM.2000.832522
   Liang W, 2007, IEEE MULTIMEDIA, V14, P92, DOI 10.1109/MMUL.2007.47
   *LS COMM, 1999, IEEE STAND WIR LAN M
   MAYERPATEL K, 2007, P 15 INT C MULT MULT, P625
   McDonald AB, 1999, IEEE J SEL AREA COMM, V17, P1466, DOI 10.1109/49.780353
   PRIYANTHA N., 2000, P 6 ANN INT C MOB CO, P32, DOI [10.1145/345910.345917, DOI 10.1145/345910.345917]
   *PROX, PROX ORINOCO 11B CLI
   QIN M, 2007, THESIS U SO CALIFORN
   QIN M, 2006, P 14 ANN ACM INT C M
   QIN M, 2006, 06878 U SO CAL COMP
   QIN M, 2007, P 15 ANN ACM INT C M
   REICHEL HSJ, 2005, SCALABLE VI IN PRESS
   Savvides A., 2001, P 7 ANN INT C MOB CO, P166, DOI DOI 10.1145/381677.381693
   WANG K. H., 2002, P IEEE INT C COMM IC
   Wang KH, 2002, IEEE INFOCOM SER, P1089, DOI 10.1109/INFCOM.2002.1019357
   YOON J, 2003, P INFOCOM MAR, P1293
   [No title captured]
NR 33
TC 19
Z9 20
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 317
EP 329
DI 10.1109/TMM.2010.2046275
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100008
DA 2024-07-18
ER

PT J
AU Khalifeh, A
   Yousefi'zadeh, H
AF Khalifeh, Ala'
   Yousefi'zadeh, Homayoun
TI Optimal Audio Transmission Over Error-Prone Wireless Links
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio transmission; dynamic programming; MPEG-4 bit slice arithmetic
   coding (BSAC); optimal unequal error protection; packet erasures; random
   bit errors; wireless MIMO links
ID TIME BLOCK-CODES; OPTIMIZATION; PERFORMANCE; CHANNELS
AB In this paper, we present an optimization framework for transmitting high quality audio sequences over error-prone wireless links. Our framework introduces apparatus and technique to optimally protect a stored audio sequence transmitted over a wireless link while considering the packetization overhead of audio frames. Utilizing rate compatible punctured Reed-Solomon (RS) codes and dynamic programming, it identifies the optimal assignment of parity to audio frames according to their perceptual importance such that the segmented SNR of the received audio sequence is maximized. Our framework covers two cases. In the first case, a frame grouping technique is proposed to packetize audio frames and protect them against temporarily correlated bit errors introduced by a fading wireless channel. In this case, each packet is treated as a channel coding codeword. In the second case, a one-dimensional RS coder is applied vertically to a sequence of horizontally formed packets associated with an audio sequence in order to protect the sequence against both bit errors introduced by fading wireless channels and packet erasures introduced by network buffering. Our numerical results capture the performance advantage of our framework compared to existing techniques proposed in the literature of audio transmission. We also note that our framework can be generically applied to a variety of audio coders, making it attractive in terms of implementation.
C1 [Khalifeh, Ala'; Yousefi'zadeh, Homayoun] Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
C3 University of California System; University of California Irvine
RP Khalifeh, A (corresponding author), Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
EM ala.khal-ifeh@fulbrightmail.org; hyousefi@uci.edu
RI Khalifeh, Ala/ACU-7567-2022
OI Khalifeh, Ala/0000-0003-3600-8090
CR [Anonymous], 2003, Introduction to Algorithms
   BAYYA A, 1996, P IEEE INT C AC SPEE
   Creusere CD, 2005, IEEE T SPEECH AUDI P, V13, P422, DOI 10.1109/TSA.2005.845817
   DEGERMARK M, 1996, P ACM MOB COMP NETW
   Etemadi F, 2006, IEEE T MULTIMEDIA, V8, P1291, DOI 10.1109/TMM.2006.884606
   HELLERUD E, 2006, P IEEE INT C DIG TEL
   *ITU R, BS13871 ITUR, P1998
   Kabal P., MATLAB IMPLEMENTATIO
   LI Z, 2003, P IEEE INT C 3G MOB
   MARKS SK, 2005, P IEEE INT C MULT EX
   OLAUSSON M, 2004, P IEEE INT C AC SPEE
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   PETRACCA M, 2004, P IEEE INT S CONTR C
   ROYCHOUDHURI L, 2004, P IEEE GLOB COMM C G
   Sklar B., 2003, DIGITAL COMMUNICATIO, V2nd
   Tarokh V, 1999, IEEE T INFORM THEORY, V45, P1456, DOI 10.1109/18.771146
   TOURRILHES J, 1998, P IEEE INT C UN PERS
   Wang W, 2005, IEEE T VEH TECHNOL, V54, P366, DOI 10.1109/TVT.2004.838890
   WANG Y, 2003, P ACM INT C MULT
   Xu AX, 2000, J AUDIO ENG SOC, V48, P627
   YEE JR, 1995, IEEE T COMMUN, V43, P2316, DOI 10.1109/26.403764
   Yousefi'zadeh H, 2004, IEEE T IMAGE PROCESS, V13, P873, DOI 10.1109/TIP.2004.827234
   Yousefi'zadeh H, 2008, IEEE J-STSP, V2, P220, DOI 10.1109/JSTSP.2008.923581
   YUNG CW, 1999, P IEEE INT S CIRC SY
NR 24
TC 11
Z9 12
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2010
VL 12
IS 3
BP 204
EP 214
DI 10.1109/TMM.2010.2041096
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 570IN
UT WOS:000275666900005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, ZL
   Xi, HS
   Wei, G
   Chen, Q
AF Wang, Zilei
   Xi, Hongsheng
   Wei, Guo
   Chen, Qing
TI Generalized PCRTT Offline Bandwidth Smoothing Based on SVM and
   Systematic Video Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth smoothing; piecewise constant rate transmission and transport
   (PCRTT); support vector machine (SVM); video-on-demand (VoD); video
   segmentation
ID ON-DEMAND; ALLOCATION; DELIVERY
AB As a trade-off technique, bandwidth smoothing can reduce the client buffer requirements and simultaneously keep transmission scheme as smooth as possible. In this paper, bandwidth smoothing is formulated into a binary classification problem of the underflow and overflow points. We propose a novel method to solve that problem based on support vector machine (SVM). Our method is proven to be able to achieve the minimum buffer requirements of constant rate transmission and transport. Furthermore, it directly computes the transmission rate without exhaustively searching buffer size and startup delay. Besides this method, this paper provides a systematic video segmentation algorithm, which can intelligently partition the playback curve into some unequal segments to naturally track the trends of playback curve. The smoothing results with the playback curve of demonstrate that this video systematic segmentation requires smaller than half of the buffer of the equal segmentation algorithm. Finally, we construct a generalized piecewise constant rate transmission and transport algorithm with SVM and the systematic video segmentation method. The experiments of some real MPEG4 and H.264 video data confirmed the efficiency of our proposed algorithm.
C1 [Wang, Zilei; Xi, Hongsheng; Chen, Qing] Univ Sci & Technol China, Lab Network Commun Syst & Control, Hefei 230027, Peoples R China.
   [Wei, Guo] Univ Sci & Technol China, Wireless Informat Network Lab, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Wang, ZL (corresponding author), Univ Sci & Technol China, Lab Network Commun Syst & Control, Hefei 230027, Peoples R China.
EM zlwang@ustc.edu.cn; xihs@ustc.edu.cn; wei@ustc.edu.cn; qchen@ustc.edu.cn
RI Guo, Wei/KJM-4941-2024
FU National High-Tech Research and Development Program of China
   [2007AA01Z235]; National Natural Science Foundation of China [60774038];
   National Basic Research Program of China [2007CB310602]
FX This work was supported in part by the National High-Tech Research and
   Development Program of China (863 Program) under Grant No. 2007AA01Z235,
   in part by the National Natural Science Foundation of China under Grant
   No. 60774038, and in part by the National Basic Research Program of
   China (973 Program) under Grant No. 2007CB310602.
CR Cristianini N., 2000, INTRO SUPPORT VECTOR
   DERAUWERA GV, 2007, TRAFFIC QUALITY CHAR
   Feng WC, 1997, IEEE INFOCOM SER, P58, DOI 10.1109/INFCOM.1997.635114
   Feng WC, 1997, MULTIMEDIA SYST, V5, P297, DOI 10.1007/s005300050062
   FENG WC, 1995, COMPUT COMMUN, V18, P709, DOI 10.1016/0140-3664(95)98484-M
   Feng WC, 1999, IEEE T MULTIMEDIA, V1, P302, DOI 10.1109/6046.784468
   Grossglauser M, 1997, IEEE ACM T NETWORK, V5, P741, DOI 10.1109/90.650136
   Hadar O, 2001, REAL-TIME IMAGING, V7, P301, DOI 10.1006/rtim.2001.0229
   HADAR O, 2007, MULTIMED TOOLS APPL, V36, P1
   JUNBIAO Z, 1998, COMPUT COMMUN, V21, P375
   KRUNZ M, 1995, P ACM SIGMETRICS OTT, P47
   McManus J, 1998, TELECOMMUN SYST, V9, P223, DOI 10.1023/A:1019147923657
   McManus JM, 1996, IEEE J SEL AREA COMM, V14, P1087, DOI 10.1109/49.508280
   Rao SG, 1999, MULTIMEDIA SYST, V7, P222, DOI 10.1007/s005300050124
   Salehi JD, 1998, IEEE ACM T NETWORK, V6, P397, DOI [10.1109/90.720873, 10.1142/S0218213097000219]
   Zhang H, 1997, MULTIMEDIA SYST, V5, P164, DOI 10.1007/s005300050053
   Zhang JB, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P3, DOI 10.1109/MMCS.1997.609511
   Zhang L, 2000, COMPUT COMMUN, V23, P133, DOI 10.1016/S0140-3664(99)00161-9
   Zhao Y, 2005, LECT NOTES COMPUT SC, V3610, P1090
NR 19
TC 7
Z9 7
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 998
EP 1009
DI 10.1109/TMM.2009.2021800
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300017
DA 2024-07-18
ER

PT J
AU Levy, M
   Sandler, M
AF Levy, Mark
   Sandler, Mark
TI Music Information Retrieval Using Social Tags and Audio
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio; information retrieval; music; social tags
ID RECOGNITION; MODEL
AB In this paper we describe a novel approach to applying text-based information retrieval techniques to music collections. We represent tracks with a joint vocabulary consisting of both conventional words, drawn from social tags, and audio muswords, representing characteristics of automatically-identified regions of interest within the signal. We build vector space and latent aspect models indexing words and muswords for a collection of tracks, and show experimentally that retrieval with these models is extremely well-behaved. We find in particular that retrieval performance remains good for tracks by artists unseen by our models in training, and even if tags for their tracks are extremely sparse.
C1 [Levy, Mark; Sandler, Mark] Univ London, Dept Elect Engn, Ctr Digital Mus, London E1 4NS, England.
C3 University of London
RP Levy, M (corresponding author), Last Fm, London N1 6DL, England.
EM mark@last.fm; mark.sandler@elec.qmul.ac.uk
OI Sandler, Mark/0000-0002-5691-8107
FU EPSRC [EP/E017614/1]; EPSRC [EP/E017614/1] Funding Source: UKRI
FX Manuscript received May 28, 2008: revised November 13, 2008. Current
   version published March 18, 2009. This work was supported by EPSRC grant
   EP/E017614/1 (Online Music Recognition And Searching). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Lexing Me.
CR AN GG, 2007, P ISMIR
   [Anonymous], 2006, THESIS VIENNA U TECH
   [Anonymous], P ISMIR
   [Anonymous], THESIS U PARIS 6 PAR
   Aucouturier JJ, 2007, J ACOUST SOC AM, V122, P881, DOI 10.1121/1.2750160
   BARNARD K, 2001, P IEEE INT C COMP VI
   Barrington L., 2007, P IEEE INT C AC SPEE
   Bell RE, 2004, J ENDOVASC THER, V11, P6
   BLEI D, 2003, P ACM SIGIR C
   Chai Wei, 2003, P 11 ACM INT C MULT, P223
   DAVIES M, 2008, P EUSIPCO UNPUB
   ECK D, 2007, P NEUR INF PROC SYST
   GILLET O, 2006, P ICASSP
   Golder SA, 2006, J INF SCI, V32, P198, DOI 10.1177/0165551506062337
   Goto M, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P437
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Hofmann T., 1999, UAI
   JEON J, 2003, P ACM SIGIR C
   KNEES P, 2004, THESIS VIENNA U TECH
   KNEES P, 2004, P ISMIR
   KNEES P, 2007, P 30 ANN INT ACM SIG
   LEVY M, 2007, P ISMIR
   LEVY M, J NEW MUSIC RE UNPUB
   LEVY M, 2006, P ICASSP
   LEVY M, 2006, P ACM MULT
   LEVY M, 2006, P EUR SIGN PROC C
   Levy M, 2008, IEEE T AUDIO SPEECH, V16, P318, DOI 10.1109/TASL.2007.910781
   LU L, 2004, P 6 ACM SIGMM INT WO
   MADDAGE N, 2004, P 6 ACM SIGMM INT WO
   Mandel M., 2005, P ISMIR
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Marlin B. M., 2007, P 23 C UNC ART INT
   Monay F, 2007, IEEE T PATTERN ANAL, V29, P1802, DOI 10.1109/TPAMI.2007.1097
   Mori Y, 1999, P INT WORKSH MULT IN
   PAULUS J, 2006, P 1 ACM AUD MUS COMP
   Quelhas P, 2007, IEEE T PATTERN ANAL, V29, P1575, DOI 10.1109/TPAMI.2007.1155
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   SHIU Y, 2006, P 1 ACM AUD MUS COMP
   TURNBULL D, 2007, P INT C MUS INF RETR
   Turnbull D, 2008, IEEE T AUDIO SPEECH, V16, P467, DOI 10.1109/TASL.2007.913750
   Vignoli F, 2005, P ISMIR
   Wang A, 2006, COMMUN ACM, V49, P44, DOI 10.1145/1145287.1145312
   WHITMAN B, 2005, THESIS MIT CAMBRIDGE
   WU X, 2006, P WORLD WID WEB C
   Yoshii K, 2008, IEEE T AUDIO SPEECH, V16, P435, DOI 10.1109/TASL.2007.911503
NR 45
TC 65
Z9 76
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 383
EP 395
DI 10.1109/TMM.2009.2012913
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300005
DA 2024-07-18
ER

PT J
AU Schwalb, M
   Ewerth, R
   Freisleben, B
AF Schwalb, Martin
   Ewerth, Ralph
   Freisleben, Bernd
TI Fast Motion Estimation on Graphics Hardware for H.264 Video Encoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Parallel motion estimation; H.264; GPGPU (general purpose computation on
   GPU); programmable graphics hardware; MPEG-4 part 10/AVC
AB The video coding standard H.264 supports video compression with a higher coding efficiency than previous standards. However, this comes at the expense of an increased encoding complexity, in particular for motion estimation which becomes a very time consuming task even for today's central processing units (CPU). On the other hand, modern graphics hardware includes a powerful graphics processing unit (GPU) whose computing power remains idle most of the time. In this paper, we present a GPU based approach to motion estimation for the purpose of H.264 video encoding. A small diamond search is adapted to the programming model of modern GPUs to exploit their available parallel computing power and memory bandwidth. Experimental results demonstrate a significant reduction of computation time and a competitive encoding quality compared to a CPU UMHexagonS implementation while enabling the CPU to process other encoding tasks in parallel.
C1 [Schwalb, Martin; Ewerth, Ralph; Freisleben, Bernd] Univ Marburg, Dept Math & Comp Sci, D-35032 Marburg, Germany.
C3 Philipps University Marburg
RP Schwalb, M (corresponding author), Univ Marburg, Dept Math & Comp Sci, D-35032 Marburg, Germany.
EM schwalbm@in-formatik.uni-marburg.de; ewerth@informatik.uni-marburg.de;
   freisleb@infor-matik.uni-marburg.de
OI Ewerth, Ralph/0000-0003-0918-6297; Freisleben, Bernd/0000-0002-7205-8389
CR BUCK I, 2004, COURS HELD SIGGRAPH
   Chen MJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P737
   HARRIS M, 2004, TUT HELD EUROGRAPHIC
   Huang YW, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P145
   *ISO IEC, 2003, 14496102003 ISOIEC
   Kelly F, 2004, P SOC PHOTO-OPT INS, V5297, P184, DOI 10.1117/12.526400
   Kelly F, 2004, P SOC PHOTO-OPT INS, V5309, P92, DOI 10.1117/12.526742
   Li X., 2004, P IEEE INT C AC SPEE, V3, P369
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Shen GB, 2005, IEEE T CIRC SYST VID, V15, P685, DOI 10.1109/TCSVT.2005.846440
   Strzodka R, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P545, DOI 10.1109/VISUAL.2004.88
   Zhou Z., 2004, P 2004 IEEE INT S CI, V3, P715
NR 12
TC 22
Z9 31
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 1
EP 10
DI 10.1109/TMM.2008.2008873
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700001
DA 2024-07-18
ER

PT J
AU Yue, ZF
   Chellappa, R
AF Yue, Zhanfeng
   Chellappa, Rama
TI Synthesis of Silhouettes and Visual Hull Reconstruction for Articulated
   Humans
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Articulated humans; body part segmentation; image-based visual hull;
   shape reconstruction; synthesis of silhouettes; visual hull
ID RECOGNITION
AB In this paper, we propose a complete framework for improved synthesis and understanding of the human pose from a limited number of silhouette images. It combines the active image-based visual hull (IBVH) algorithm and a contour-based body part segmentation technique. We derive a simple, approximate algorithm to decide the extrinsic parameters of a virtual camera, and synthesize the turntable image collection of the person using the IBVH algorithm by actively moving the virtual camera on a properly computed circular trajectory around the person. Using the turning function distance as the silhouette similarity measurement, this approach can be used to generate the desired pose-normalized images for recognition applications. In order to overcome the inability of the visual hull (VH) method to reconstruct concave regions, we propose a contour-based human body part localization algorithm to segment the silhouette images into convex body parts. The body parts observed from the virtual view are generated separately from the corresponding body parts observed from the input views and then assembled together for a more accurate VH reconstruction. Furthermore, the obtained turntable image collection helps to improve the body part segmentation and identification process. By using the inner distance shape context (IDSC) measurement, we are able to estimate the body part locations more accurately from a synthesized view where we can localize the body part more precisely. Experiments show that the proposed algorithm can greatly improve body part segmentation and hence shape reconstruction results.
C1 [Yue, Zhanfeng] FastVDO Inc, Columbia, MD 21044 USA.
   [Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Chellappa, Rama] Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park
RP Yue, ZF (corresponding author), FastVDO Inc, Columbia, MD 21044 USA.
EM zyue@fastvdo.com; rama@cfar.umd.edu
RI Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020
FU NSF-ITR [115-03-25715]
FX Manuscript received February 19,2008; revised July 28,2008. First
   published December 10, 2008. This work was supported in part by the
   NSF-ITR Grant 115-03-25715. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Yo-Sung Ho.
CR AHUJA N, 1989, IEEE T PATTERN ANAL, V11, P137, DOI 10.1109/34.16710
   [Anonymous], THESIS CARNEGIE MELL
   [Anonymous], P 23 ANN C COMP GRAP
   [Anonymous], IEEE WORKSH ART NONR
   ARKIN EM, 1991, IEEE T PATTERN ANAL, V13, P209, DOI 10.1109/34.75509
   Avidan S, 1998, IEEE T VIS COMPUT GR, V4, P293, DOI 10.1109/2945.765324
   Barral P., 1999, P INT C GRAPHICON 99
   BEARDSLEY PA, 1996, LNCS, V1065, P683
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BOROVIKOV E, 2002, P 1 INT S 3D DAT PRO
   Brand M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1237, DOI 10.1109/ICCV.1999.790422
   CHEUNG G, 2003, P COMP VIS PATT REC
   DAVIS L, 1999, P 3 INT WORKSH COOP
   Dyer Charles R., 2001, Foundations ofImage Understanding, chapter Volumetric Scene Reconstruction from Multiple Views, P469
   Haritaoglu I, 1998, INT C PATT RECOG, P77, DOI 10.1109/ICPR.1998.711084
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   Howe N. R., 1999, NEURAL INFORM PROCES
   Johansson B., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P54, DOI 10.1109/ICCV.1999.791197
   KANG SB, 1999, P SOC PHOTO-OPT INS, V3641, P2
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   MATUSIK W, 2001, P EUR WORKSH REND
   Mori G, 2002, LECT NOTES COMPUT SC, V2352, P666
   POTMESIL M, 1987, COMPUT VISION GRAPH, V40, P1, DOI 10.1016/0734-189X(87)90053-3
   RONFARD R, 2002, EUR C COMP VIS
   Rosales R, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P19, DOI 10.1109/HUMO.2000.897366
   SCASSELLATI B, 1994, P SOC PHOTO-OPT INS, V2185, P2, DOI 10.1117/12.171777
   Seitz S. M., 1995, Proceedings IEEE Workshop on Representation of Visual Scenes (In Conjunction with ICCV'95) (Cat. No.95TB8126), P18, DOI 10.1109/WVRS.1995.476848
   Shakhnarovich G, 2001, PROC CVPR IEEE, P439
   SHAKHNAROVICH G, 2003, P INT C COMP VIS
   Taylor CJ, 2000, COMPUT VIS IMAGE UND, V80, P349, DOI 10.1006/cviu.2000.0878
   [No title captured]
NR 33
TC 3
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1565
EP 1577
DI 10.1109/TMM.2008.2007321
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600012
DA 2024-07-18
ER

PT J
AU Picard, D
   Cord, M
   Revel, A
AF Picard, David
   Cord, Matthieu
   Revel, Arnaud
TI Image Retrieval Over Networks: Active Learning Using Ant Algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cooperative systems; image databases; information retrieval
AB In this article, we present a framework for distributed content based image retrieval with online learning based on ant-like mobile agents. Mobile agents crawl the network to find images matching a given example query. The images retrieved are shown to the user who labels them, following the classical relevant feedback scheme. The labels are used both to improve the similarity measure used for the retrieval and to learn paths leading to sites containing relevant images. The relevant paths are learned in an ethologically inspired way. We made experiments on the trecvid 2005 keyframe dataset showing that learning both the similarity function and the localization of the relevant images leads to a significant improvement. We also present an extension with the reuse of learned paths for later sessions leading to a further improvement.
C1 [Picard, David] ETIS CNRS UMR 8051, F-95014 Cergy Pontoise, France.
   [Cord, Matthieu] LIP6 UMPC, F-75016 Paris, France.
   [Revel, Arnaud] ENSEA, F-95014 Cergy Pontoise, France.
   [Revel, Arnaud] Ctr Emot CNRS UMR 7593, F-95014 Cergy Pontoise, France.
C3 Centre National de la Recherche Scientifique (CNRS); Sorbonne
   Universite; Centre National de la Recherche Scientifique (CNRS)
RP Picard, D (corresponding author), ETIS CNRS UMR 8051, F-95014 Cergy Pontoise, France.
EM picard@ensea.fr; matthieu.cord@lip6.fr; revel@ensea.fr
RI Picard, David/AAV-8841-2021
OI Picard, David/0000-0002-6296-4222
CR Berretti S, 2004, MULTIMED TOOLS APPL, V24, P215, DOI 10.1023/B:MTAP.0000039388.63801.67
   Bonabeau E, 2000, NATURE, V406, P39, DOI 10.1038/35017500
   Callan Jamie., 2000, DISTRIBUTED INFORM R, P127
   CARSON C, 1991, 3 INT C VIS INF SYST
   CHAPELLE O, 1999, IEEE T NEURAL NETW, V9
   Cho Junghoo, 2004, WWW'04, Proceedings of the 13th international conference on World Wide Web, P20, DOI DOI 10.1145/988672.988676
   Cord M, 2007, IMAGE VISION COMPUT, V25, P14, DOI 10.1016/j.imavis.2006.01.004
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Deneubourg J., 1990, Proceedings of the First International Conference of Simulation of Adaptive Behavior on from Animals to Animats, P356
   Dorigo M, 1996, IEEE T SYST MAN CY B, V26, P29, DOI 10.1109/3477.484436
   Huang TS, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P2, DOI 10.1109/ICIP.2001.958036
   Jiao Y., 2004, J INTERCONNECT NETW, V5, P351
   King I, 2004, ACM T INFORM SYST, V22, P477, DOI 10.1145/1010614.1010619
   Lange DB, 1999, COMMUN ACM, V42, P88, DOI 10.1145/295685.298136
   LEWIS D, 1994, INT C MACH LEARN
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173
   PARK J, 2000, P 2000 IEEE SIGN PRO, V1, P195
   PICARD D, 2006, IEEE INT C IM PROC I
   Revel A, 2003, IEEE/WIC INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P482
   REVEL A, 2005, ACM INT C WEB INT CO
   ROTH V, 2005, SAC 05, P66
   Santini S, 2001, IEEE T KNOWL DATA EN, V13, P337, DOI 10.1109/69.929893
   Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243
   TONG S, 2001, ACM MULTIMEDIA
   VASCONCELOS N, 2000, THESIS MIT CAMBRIDGE
   Veltkamp RemcoC., 2002, Content-based image retrieval systems, DOI 10.1007/978-1-4615-0987-5_5
   Wood M. E. J., 1998, Proceedings ACM Multimedia 98, P13, DOI 10.1145/290747.290750
NR 30
TC 11
Z9 12
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1356
EP 1365
DI 10.1109/TMM.2008.2004913
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, JP
   Chiu, DKW
   Li, Q
   Li, MM
AF Wang, Jianping
   Chiu, Dickson K. W.
   Li, Qing
   Li, Minming
TI Service Sharing for Streaming Video Multicast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Greedy algorithm; NP-hard; service cost optimization; service invocation
   order; service-oriented architecture
AB In a general context, the sharing of intermediate service results among different processes is seldom feasible because parameters are often different and there may be transactional and side effects. However, in a streaming video multicast environment, a large number of users often request various similar processing on the same stream. Therefore, service sharing is feasible, with a large potential of savings in processing cost. In this paper, we study the problem of determining the service invocation orders for multiple service composition requests in a streaming video multicast with the aim of maximizing the service sharing. We first formally define the problem. After proving the problem is NP-Complete, we develop an optimal algorithm for the base case of two requests. Then for the general case, we develop two heuristic algorithms, namely, a global greedy algorithm and a local greedy algorithm using the optimal algorithm for the base case as the building block. The global greed), algorithm is designed for a system where the existing service composition requests can be recomposed with the arrival of a new request. The local greedy algorithm can be used in a system where the existing service composition requests do not change their service composition solutions with the arrival of a new request. We prove that the global greedy algorithm is a 2-approximation algorithm in terms of maximizing service sharing. Simulation results show that the greedy algorithms can save more service costs compared with a naive algorithm, and are effective compared with the cost lower bound.
C1 [Wang, Jianping; Li, Qing; Li, Minming] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Chiu, Dickson K. W.] Dickson Comp Syst, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Wang, JP (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM jianwang@cityu.edu.hk; dicksonchiu@ieee.org; itqli@cityu.edu.hk;
   minmli@cs.cityu.edu.hk
RI Li, Qing/JMH-1365-2023; Wang, Jianping/H-3974-2011; Chiu, Dickson K.
   W./B-9630-2017; Li, Minming/L-9676-2013
OI Li, Qing/0000-0003-3370-471X; Chiu, Dickson K. W./0000-0002-7926-9568;
   Li, Minming/0000-0002-7370-6237; WANG, Jianping/0000-0002-9318-1482
FU Hong Kong Special Administrative Region. China [CityU 121107, CityU
   116907]
FX Manuscript received November 14, 2007; revised July 17, 2008. Current
   version published November 17. 2008. This work was supported in part by
   grants front the Research Grants Council of the Hong Kong Special
   Administrative Region. China (Project No. CityU 121107 and CityU
   116907). The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. S.-H. Gary Chan.
CR ABRAMS Z, 2006, P ICDCS
   [Anonymous], P 1 ACM INT WORKSH M
   BANERJEE S, 2003, P 3 IEEE WORKSH INT
   Basu P., 2002, P 1 ANN MED AD HOC N
   Bunke H, 1998, PATTERN RECOGN LETT, V19, P255, DOI 10.1016/S0167-8655(97)00179-7
   Chakraborty D, 2005, MOBILE NETW APPL, V10, P435, DOI 10.1007/s11036-005-1556-y
   CONTE D, 2003, P 4 IAPR INT WORKSH, P589
   Deelman E., 2005, Scientific Programming, V13, P219
   Garey M.R., 1979, COMPUTERS INTRACTABI
   Goderis A, 2006, ICWS 2006: IEEE INTERNATIONAL CONFERENCE ON WEB SERVICES, PROCEEDINGS, P312
   Gu X., 2003, P 23 IEEE INT C DIST, P19
   Inokuchi A, 2000, LECT NOTES COMPUT<D>, V1910, P13
   Jin J., 2004, P IEEE INT C COMM 20, P20
   Jin JW, 2004, LECT NOTES COMPUT SC, V3231, P115
   Jin JW, 2003, NINTH IEEE WORKSHOP ON FUTURE TRENDS OF DISTRIBUTED COMPUTING SYSTEMS, PROCEEDINGS, P198
   JIN JW, 2003, P IEEE INT C COMM IC
   KALASAPUR S, 2005, P 1 ACM INT WORKSH M, P11
   Kuramochi M, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P313, DOI 10.1109/ICDM.2001.989534
   Lim JH, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P213, DOI 10.1109/ICME.2002.1035756
   Liu Jie., 2005, P C EMBEDDED SOFTWAR, P273
   Nahrstedt K., 2004, Proc. the 12th ACM Int. Conf. Multimedia (MULTIMEDIA '04), P88
   Raman B, 2003, IEEE INFOCOM SER, P1477
   SRIVASTAVA U, 2005, P 24 ACM S PRINC DAT
   Wombacher A, 2006, P IEEE I C SERV COMP, P94, DOI 10.1109/SCC.2006.81
   XU Z, 2003, P NOSSDAV 03 JAN
NR 25
TC 2
Z9 3
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1393
EP 1405
DI 10.1109/TMM.2008.2004935
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700015
DA 2024-07-18
ER

PT J
AU Lin, WH
   Horng, SJ
   Kao, TW
   Fan, PZ
   Lee, CL
   Pan, Y
AF Lin, Wei-Hung
   Horng, Shi-Jinn
   Kao, Tzong-Wann
   Fan, Pingzhi
   Lee, Cheng-Ling
   Pan, Yi
TI An efficient watermarking method based on significant difference of
   wavelet coefficient quantization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE blind; copyright protection; significant coefficient difference;
   watermark; wavelet
AB This paper proposes a blind watermarking algorithm based on the significant difference of wavelet coefficient quantization for copyright protection. Every seven nonoverlap wavelet coefficients of the host image are grouped into a block. The largest two coefficients in a block are called significant coefficients in this paper and their difference is called significant difference. We quantized the local maximum wavelet coefficient in a block by comparing the significant difference value in a block with the average significant difference value in all blocks. The maximum wavelet coefficients are so quantized that their significant difference between watermark bit 0 and watermark bit I exhibits a large energy difference which can be used for watermark extraction. During the extraction, an adaptive threshold value is designed to extract the watermark from the watermarked image under different attacks. We compare the adaptive threshold value to the significant difference which was quantized in a block to determine the watermark bit. The experimental results show that the proposed method is quite effective against JPEG compression, low-pass filtering, and Gaussian noise; the PSNR value of a watermarked image is greater than 40 dB.
C1 [Lin, Wei-Hung; Horng, Shi-Jinn] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
   [Horng, Shi-Jinn] Natl United Univ, Dept Elect Engn, Miaoli, Taiwan.
   [Horng, Shi-Jinn; Fan, Pingzhi] SW Jiatong Univ, Inst Mobile Commun, Chengdu 610031, Peoples R China.
   [Horng, Shi-Jinn; Pan, Yi] Georgia State Univ, Dept Comp Sci, Atlanta, GA 30302 USA.
   [Kao, Tzong-Wann] Inst No Taiwan, Dept Elect Engn Technol & Sci, Taipei, Taiwan.
   [Lee, Cheng-Ling] Natl United Univ, Dept Electroopt Engn, Miaoli, Taiwan.
C3 National Taiwan University of Science & Technology; National United
   University; Southwest Jiaotong University; University System of Georgia;
   Georgia State University; National United University
RP Lin, WH (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
EM D9515013@nnail.must.edu.tw; horngsj@yahoo.com.tw; tkao@tsint.edu.tw;
   p.fan@ieee.org; cherry@nuu.edu.tw; pan@cs.gsu.edu
RI Pan, Yi/AAJ-2341-2021; Lin, Wei-Hung/ABE-1927-2021; Horng,
   Shi-Jinn/GVU-0488-2022
CR Al-Khassaweneh M, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1597, DOI 10.1109/ICME.2006.262851
   [Anonymous], 2006, P 19 C COMP VIS GRAP
   Byun K, 2005, PDCAT 2005: Sixth International Conference on Parallel and Distributed Computing, Applications and Technologies, Proceedings, P689
   Chen DY, 2000, IEEE T CONSUM ELECTR, V46, P404, DOI 10.1109/30.883385
   CHENG MC, 2004, P 2004 INT S INT SIG, P283
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Cox IJ, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P243, DOI 10.1109/ICIP.1996.560429
   Davoine F, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P682, DOI 10.1109/ICIP.2000.899546
   Duan FY, 1998, INT C PATT RECOG, P1589, DOI 10.1109/ICPR.1998.712016
   Dugad R, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P419, DOI 10.1109/ICIP.1998.723406
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Hsieh MS, 2001, IEEE T IND ELECTRON, V48, P875, DOI 10.1109/41.954550
   Huang JY, 2004, IEEE SYS MAN CYBERN, P2977
   JIANG M, 2004, J IEEE ICSP, P857
   Khelifi F, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P588
   KIM JD, 1999, P INT S DAT APPL NON, P226
   Kimpan S, 2004, IEEE INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES 2004 (ISCIT 2004), PROCEEDINGS, VOLS 1 AND 2, P374
   Kundur D, 1998, INT CONF ACOUST SPEE, P2969, DOI 10.1109/ICASSP.1998.678149
   Kwon OH, 1999, IEEE T CONSUM ELECTR, V45, P1221
   Kwon SG, 2001, ISIE 2001: IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS PROCEEDINGS, VOLS I-III, P205, DOI 10.1109/ISIE.2001.931783
   Langelaar GC, 2001, IEEE T IMAGE PROCESS, V10, P148, DOI 10.1109/83.892451
   Lumini A., 2000, Proceedings International Conference on Information Technology: Coding and Computing (Cat. No.PR00540), P122, DOI 10.1109/ITCC.2000.844194
   PETICOLAS FAP, WEAKNESS EXISTING WA
   PICKHOLTZ RL, 1984, IEEE T COMMUN, V32, P211, DOI 10.1109/TCOM.1984.1096025
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Tachibana T, 2004, IEEE INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES 2004 (ISCIT 2004), PROCEEDINGS, VOLS 1 AND 2, P330
   TEMI C, 2005, P IEEE ISCIT, P623
   Thiemert S, 2004, EUROMICRO CONF PROC, P457, DOI 10.1109/EURMIC.2004.1333404
   *UL SYST INC, PHOTOIMPACT 11 SOFTW
   WANG HJ, 1997, P SPIE ANN M APPL DI, P383
   WANG HJ, 1997, P IEEE ICIP SANT BAR, P652
   Wang HJM, 1998, OPT EXPRESS, V3, P491, DOI 10.1364/OE.3.000491
   Wang SH, 2004, IEEE T IMAGE PROCESS, V13, P154, DOI 10.1109/TIP.2004.823822
   Wang YP, 2000, CONF REC ASILOMAR C, P1846, DOI 10.1109/ACSSC.2000.911307
   Wu CF, 2000, IEEE T CONSUM ELECTR, V46, P1, DOI 10.1109/30.826373
   Yang SY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1723, DOI 10.1109/ICME.2004.1394586
   YUAN Y, 2006, P IEEE IMSCCS, P1597
   Yuan Y, 2006, FIRST INTERNATIONAL MULTI-SYMPOSIUMS ON COMPUTER AND COMPUTATIONAL SCIENCES (IMSCCS 2006), PROCEEDINGS, VOL 2, P175, DOI 10.1109/IMSCCS.2006.187
   Zhang GN, 2004, 2004 7TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS 1-3, P2294
   Zhou HT, 2006, INT C COMMUN CIRCUIT, P19
NR 41
TC 160
Z9 177
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 746
EP 757
DI 10.1109/TMM.2008.922795
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800008
DA 2024-07-18
ER

PT J
AU Shiang, HP
   van der Schaar, M
AF Shiang, Hsien-Po
   van der Schaar, Milhaela
TI Queuing-based dynamic channel selection for heterogeneous multimedia
   applications over cognitive radio networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cognitive radio networks; delay-sensitive multimedia applications;
   queuing analysis; resource management for heterogeneous users
AB In this paper, we propose a dynamic channel-selection solution for autonomous wireless users transmitting delay-sensitive multimedia applications over cognitive radio networks. Unlike prior works that seldom consider the requirement of the application layer, our solution explicitly considers various rate requirements and delay deadlines of heterogeneous multimedia users. Note that the users usually possess private utility functions, application requirements, and distinct channel conditions in different frequency channels. To efficiently manage available spectrum resources in a decentralized manner, information exchange among users is necessary. Hence, we propose a novel priority virtual queue interface that determines the required information exchanges and evaluates the expected delays experienced by various priority traffics. Such expected delays are important for multimedia users due to their delay-sensitivity nature. Based on the exchanged information, the interface evaluates the expected delays using priority queuing analysis that considers the wireless environment, traffic characteristics, and the competing users' behaviors in the same frequency channel. We propose a dynamic strategy learning (DSL) algorithm deployed at each user that exploits the expected delay and dynamically adapts the channel selection strategies to maximize the user's utility function. We simulate multiple video users sharing the cognitive radio network and show that our proposed solution significantly reduces the packet loss rate and outperforms the conventional single-channel dynamic resource allocation by almost 2 dB in terms of video quality.
C1 [Shiang, Hsien-Po; van der Schaar, Milhaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Shiang, HP (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM hpshiang@ee.ucla.edu; mihaela@ee.ucla.edu
CR AKYILDIZ IF, 2006, COMPUT NETWORKS INT, V50
   [Anonymous], 80222 IEEE
   [Anonymous], 1998, THEORY LEARNING GAME
   Bertsekas D. P., 1992, Data Networks, V2nd
   Brown TNX, 2005, 2005 1st IEEE International Symposium on New Frontiers in Dynamic Spectrum Access Networks, Conference Record, P11
   CORDEIRO C, 2006, J COMMUN, V1, P17108
   Haykin S, 2005, IEEE J SEL AREA COMM, V23, P201, DOI 10.1109/JSAC.2004.839380
   *IEEE, 2003, 80211ED50 IEEE
   Jiang YM, 2001, IEEE COMMUN LETT, V5, P175, DOI 10.1109/4234.917105
   Julian D, 2002, IEEE INFOCOM SER, P477, DOI 10.1109/INFCOM.2002.1019292
   Kleinrock L., 1975, Queuing Systems, VI
   Koenigsberg Ernest., 1966, Management Science, V12, P412
   KONDYLIS GD, 1999, P IEEE VTC AMST NETH
   KONHEIM AG, 1980, IEEE T COMMUN, V28, P1004, DOI 10.1109/TCOM.1980.1094766
   krishnaswamy D., 2002, PROC 3G WIRELESS C, P165
   Lucky RW, 2006, IEEE SPECTRUM, V43, P88, DOI 10.1109/MSPEC.2006.1572368
   Mitola J, 1999, IEEE PERS COMMUN, V6, P13, DOI 10.1109/98.788210
   SABRI S, 1985, P IEEE, V73, P671, DOI 10.1109/PROC.1985.13192
   SHANKAR S, 2005, P GLOB TEL C ST LOUI
   Staple G, 2004, IEEE SPECTRUM, V41, P48, DOI 10.1109/MSPEC.2004.1270548
   Tan XH, 2003, PARALLEL AND DISTRIBUTED COMPUTING, APPLICATIONS AND TECHNOLOGIES, PDCAT'2003, PROCEEDINGS, P660, DOI 10.1109/PDCAT.2003.1236386
   TEKINAY S, 1991, IEEE COMMUN MAG, V29, P42, DOI 10.1109/35.109664
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Wang CC, 2002, IEEE T COMMUN, V50, P1637, DOI 10.1109/TCOMM.2002.803969
   Wang JF, 2004, IEEE WCNC, P1234, DOI 10.1109/WCNC.2004.1311365
   Zekavat S. A., 2006, Journal of Communications, V1, P60, DOI 10.4304/jcm.1.1.60-67
   Zhao J, 2005, 2005 1ST IEEE INTERNATIONAL SYMPOSIUM ON NEW FRONTIERS IN DYNAMIC SPECTRUM ACCESS NETWORKS, CONFERENCE RECORD, P259
   ZHENG H, 2005, P 40 ANN IEEE INT C
   Zheng HT, 2005, 2005 1st IEEE International Symposium on New Frontiers in Dynamic Spectrum Access Networks, Conference Record, P56
NR 29
TC 136
Z9 151
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 896
EP 909
DI 10.1109/TMM.2008.922851
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800020
DA 2024-07-18
ER

PT J
AU Zavarehei, E
   Vaseghi, S
AF Zavarehei, Esfandiar
   Vaseghi, Saeed
TI Interpolation of lost speech segments using LP-HNM model with codebook
   post-processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID PACKET LOSS CONCEALMENT; ENHANCEMENT; SIGNALS; NOISE
AB This paper presents a method for interpolation of lost speech segments. The interpolation method can be used for packet loss concealment in voice communication over mobile phones, for voice over IP or for restoration of lost segments in speech recordings. The interpolation method employs a combination of a linear prediction (LP) model of the spectral envelope and a harmonic noise model (HNM) of the excitation of speech. The speech interpolation problem is transformed to the modeling and interpolation of the trajectories of LP parameters and the amplitude, phase and harmonicity of HNM tracks of speech excitation. In particular, the interpolation of harmonicity results in a smooth transition from voiced to unvoiced speech and vice versa. Crucially, the proposed interpolation method does not suffer from the consequences of zero-excitation of conventional autoregressive (AR) interpolation. Different combinations of linear and autoregressive interpolation methods are evaluated for the estimation of the time-varying parameters of LP-HNM tracks. Furthermore, a post-processing codebook mapping, employed to enhance the interpolation of the spectral envelope of speech, results in improved output quality for longer length speech gaps. For different packet loss rates and patterns or distributions of missing speech gaps, the proposed interpolation methods are evaluated and compared with popular AR-based interpolation methods and the speech packet recovery method specified in the ITU G.711 standard, as a reference. The evaluation results show that the proposed methods substantially improve the restoration of formants and harmonic tracks and consistently results in significant performance gain and improved perceptual quality of speech.
C1 [Zavarehei, Esfandiar] CSR, DSP Audio Grp, Cambridge CB4 0WZ, England.
   [Vaseghi, Saeed] Brunel Univ, Sch Engn & Design, London UB8 3PH, England.
C3 Cambridge Silicon Radio (CSR); Brunel University
RP Zavarehei, E (corresponding author), CSR, DSP Audio Grp, Cambridge CB4 0WZ, England.
EM esfandiar.zavarehei@csr.com; saeed.vaseghi@brunel.ac.uk
CR Ahmadi S, 1998, IEEE T SPEECH AUDI P, V6, P495, DOI 10.1109/89.709675
   [Anonymous], P IEEE INF C NEW YOR
   [Anonymous], 1996, P800 ITU T
   Cohen I, 2005, IEEE T SPEECH AUDI P, V13, P870, DOI 10.1109/TSA.2005.851940
   *DVSI, 1991, INMARSAT M VOICE COD
   Elsabrouty M, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P361, DOI 10.1109/ISSPA.2003.1224715
   EPHRAIM Y, 1985, IEEE T ACOUST SPEECH, V33, P443, DOI 10.1109/TASSP.1985.1164550
   Esquef PAA, 2006, IEEE T AUDIO SPEECH, V14, P1391, DOI 10.1109/TSA.2005.858018
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Godsill S, 1998, SPRING INT SER ENG C, P133
   Godsill S. J., 1998, Digital Audio Restora- tion
   Gündüzhan E, 2001, IEEE T SPEECH AUDI P, V9, P778, DOI 10.1109/89.966081
   *ITU T, 1999, G711 ITU T
   JANSSEN AJEM, 1986, IEEE T ACOUST SPEECH, V34, P317, DOI 10.1109/TASSP.1986.1164824
   Kauppinen I., 2002, P PROC DAFX, P105
   KONDOZ AM, 1999, DIGITAL SPEECH CODIN
   Lindblom J, 2002, 2002 IEEE SPEECH CODING WORKSHOP PROCEEDINGS, P65, DOI 10.1109/SCW.2002.1215725
   Lindblom J, 2002, INT CONF ACOUST SPEE, P173
   MILNER BP, P ICSLP 2004, P1549
   MURTHI MN, P ICASSP 2006, V1, P21
   QIAN, P EUR 2003, P1433
   QUAST H, P INT C AC SPEECH SI, V1, P353
   Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023
   Rodbro CA, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P104
   Rodbro CA, 2006, IEEE T AUDIO SPEECH, V14, P1609, DOI 10.1109/TSA.2005.858561
   Stylianou Y, 2001, IEEE T SPEECH AUDI P, V9, P232, DOI 10.1109/89.905997
   SUNDBERG J, 1979, J PHONETICS, V7, P71, DOI 10.1016/S0095-4470(19)31040-X
   VALENZUELA R, 1989, P ICASSP, P1334
   VASEGHI S, ICASSP 2006, V3, P844
   VASEGHI SV, 1990, IEE PROC-I, V137, P38, DOI 10.1049/ip-i-2.1990.0007
   WANG J, 2001, P IEEE INT C AC SPEE, V2, P745
   WANG J, P IEEE WORKSH SPEECH, P126
   YE H, 2005, THESIS CAMBRIDGE U C
   ZAVAREHEI E, P ICSLP 2006
   ZAVAREHEI E, IEEE T AUDI IN PRESS
   Zavarehei E, 2006, SPEECH COMMUN, V48, P1545, DOI 10.1016/j.specom.2006.03.003
NR 36
TC 6
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 493
EP 502
DI 10.1109/TMM.2008.917345
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100017
DA 2024-07-18
ER

PT J
AU Fernández-Escribano, G
   Bialkowski, J
   Gámez, JA
   Kalva, H
   Cuenca, P
   Orozco-Barbosa, L
   Kaup, A
AF Fernandez-Escribano, Gerardo
   Bialkowski, Jens
   Gamez, Jose A.
   Kalva, Hari
   Cuenca, Pedro
   Orozco-Barbosa, Luis
   Kaup, Andre
TI Low-complexity heterogeneous video transcoding using data mining
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data mining; H.263; H.264; interprediction; MPEG-2; supervised
   classification; video transcoding
ID MODE DECISION; H.264
AB Recent developments have given birth to H.264/AVC: a video coding standard offering better bandwidth to video quality ratios than previous standards (such as H.263, MPEG-2, MPEG-4, etc.), due to its improved inter- and intraprediction modes at the expense of higher computation complexity. It is expected that H.264/AVC will take over the digital video market, replacing the use of previous standards in most digital video applications. This creates an important need for heterogeneous video transcoding technologies from older standards to H.264. In this paper, we focus our attention on the interframe prediction, the most computationally intensive task involved in the heterogeneous video transcoding process. This paper presents a novel macroblock (MB) mode decision algorithm for interframe prediction based on data mining techniques to be used as part of a very low complexity heterogeneous video transcoder. The proposed approach is based on the hypothesis that NIB coding mode decisions in H.264 video have a correlation with the distribution of the motion compensated residual in the decoded video. We use data mining tools to exploit the correlation and derive decision trees to classify the incoming decoded MBs into one of the several coding modes in H.264. The proposed approach reduces the H.264 NIB mode computation process into a decision tree lookup with very low complexity. For general validation purposes, we apply our algorithm to two of the most important heterogeneous video transcoders: MPEG-2 to H.264 and H.263 to H.264. Our results show that the our data-mining based transcoding algorithm is able to maintain a good video quality while considerably reducing the computational complexity by 72% on average when applied in MPEG-2 to H.264 transcoders, and by 62% on average when applied in H.263 to H.264 transcoders. Finally, we conduct a comparative study with some of the most prominent fast interprediction methods for H.264 presented in the literature. Our results show that the proposed data mining-based approach achieves the best results for video transcoding applications.
C1 [Fernandez-Escribano, Gerardo; Gamez, Jose A.; Cuenca, Pedro; Orozco-Barbosa, Luis] Univ Castilla La Mancha, Inst Invest Informat Albacete, Albacete 02071, Spain.
   [Bialkowski, Jens; Kaup, Andre] Univ Erlangen Nurnberg, D-91058 Erlangen, Germany.
   [Kalva, Hari] Florida Atlantic Univ, Dept Comp Sci & Engn, Boca Raton, FL 33431 USA.
C3 Universidad de Castilla-La Mancha; University of Erlangen Nuremberg;
   State University System of Florida; Florida Atlantic University
RP Fernández-Escribano, G (corresponding author), Univ Castilla La Mancha, Inst Invest Informat Albacete, Albacete 02071, Spain.
EM gerardo@dsi.uclm.es; bial@lnt.de; jgamez@dsi.uclm.es; hari@cse.fau.edu;
   pcuenca@dsi.uclm.es; lorozco@dsi.uclm.es; kaup@lnt.de
RI Orozco, Luis Barbosa/AAV-3788-2020; Fernández-Escribano,
   Gerardo/I-1167-2015; Cuenca, Pedro/P-7960-2019; Gamez, Jose/K-5098-2014
OI Fernández-Escribano, Gerardo/0000-0002-0037-2061; Cuenca,
   Pedro/0000-0002-2791-0165; Gamez, Jose/0000-0003-1188-1117;
   Orozco-Barbosa, Luis/0000-0003-1510-1608; Kalva,
   Hari/0000-0002-7165-5499; Kaup, Andre/0000-0002-0929-5074
CR Fayyad U, 1996, AI MAG, V17, P37
   Kim YH, 2004, ELECTRON LETT, V40, P1172, DOI 10.1049/el:20046155
   Lee JY, 2003, LECT NOTES COMPUT SC, V2899, P410
   Shang JS, 2000, COMPUT SCI ENG, V2, P10, DOI 10.1109/5992.814651
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P688, DOI 10.1109/TCSVT.2003.815168
NR 6
TC 25
Z9 30
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 286
EP 299
DI 10.1109/TMM.2007.911838
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700013
DA 2024-07-18
ER

PT J
AU Lin, HY
   Lu, YH
   Liu, BD
   Yang, JF
AF Lin, Heng-Yao
   Lu, Ying-Hong
   Liu, Bin-Da
   Yang, Jar-Ferr
TI A highly efficient VLSI architecture for H.264/AVC CAVLC decoder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE context-based adaptive variable length coding (CAVLC); H.264/AVC;
   variable length coding
ID SYSTEM
AB In this paper, an efficient algorithm is proposed to improve the decoding efficiency of the context-based adaptive variable length coding (CAVLC) procedure. Due to the data dependency among symbols in the decoding How, the CAVLC decoder requires large computation time, which dominates the overall decoder system performance. To expedite its decoding speed, the critical path in the CAVLC decoder is first analyzed and then reduced by forwarding the adaptive detection for succeeding symbols. With a shortened critical path, the CAVLC architecture is further divided into two segments, which can be easily implemented by a pipeline structure. Consequently, the overall performance is effectively improved. In the hardware implementation, a low power combined LUT and single output buffer have been adopted to reduce the area as well as power consumption without affecting the decoding performance. Experimental results show that the proposed architecture surpassing other recent designs can approximately reduce power consumption by 40% and achieve three times decoding speed in comparison to the original decoding procedure suggested in the H.264 standard. The maximum frequency can be larger than 210 MHz, which can easily support the real-time requirement for resolutions higher than the HD1080 format.
C1 [Lin, Heng-Yao; Lu, Ying-Hong; Liu, Bin-Da; Yang, Jar-Ferr] Natl Cheng Kung Univ, Dept Elect Engn, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Lin, HY (corresponding author), Natl Cheng Kung Univ, Dept Elect Engn, 1 Univ Rd, Tainan 70101, Taiwan.
EM lhy92@spic.ee.ncku.edu.tw; y_h_lu@novatek.com.tw;
   bdliu@mail.ncku.edu.tw; jfyang@ee.ncku.edu.tw
CR Alle M, 2006, IEEE INT CONF ASAP, P317, DOI 10.1109/ASAP.2006.36
   Chang HC, 2005, IEEE INT SYMP CIRC S, P6110
   HASHEMIAN R, 1994, IEEE T CONSUM ELECTR, V40, P345, DOI 10.1109/30.320814
   *ISO IEC, 1999, 144962 ISO IEC
   *ISO IEC, 1995, 138182ITUT ISO IEC
   *ISO IEC, 1993, 111722 ISO IEC
   Jeon B, 1998, SIGNAL PROCESS-IMAGE, V12, P253, DOI 10.1016/S0923-5965(97)00041-6
   KANG HY, 2004, P IEEE ISCAS 2004 MA, P23
   Kim YH, 2006, IEEE T CONSUM ELECTR, V52, P943, DOI 10.1109/TCE.2006.1706492
   Lakhani G, 2004, IEEE T CIRC SYST VID, V14, P522, DOI 10.1109/TCSVT.2004.825565
   Lei SM, 1991, IEEE T CIRC SYST VID, V1, P147, DOI 10.1109/76.109154
   Lin HY, 2006, IEEE INT SYMP CIRC S, P2689
   Lin Y, 2006, Proceedings of 2006 Chinese Control and Decision Conference, P601
   MA DS, 1993, IEEE T CONSUM ELECTR, V39, P448, DOI 10.1109/30.234619
   Moon YH, 2005, IEEE T CONSUM ELECTR, V51, P933, DOI 10.1109/TCE.2005.1510506
   Nikara J, 2004, IEEE T VLSI SYST, V12, P676, DOI 10.1109/tvlsi.2004.825840
   Shieh BJ, 2001, IEEE T CIRC SYST VID, V11, P210, DOI 10.1109/76.905986
   Tseng SY, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1073, DOI 10.1109/ICME.2006.262720
   Wang SH, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P51
   Wen YN, 2006, 2006 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P1240
   WU D, 2003, P IEEE INT C ASIC BE, V2, P692
   Yu GS, 2006, IEEE INT SYMP CIRC S, P5583
   1990, ITU T RECOMMEND H 26
   1997, ITU T RECOMMEND H 26
NR 24
TC 28
Z9 29
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 31
EP 42
DI 10.1109/TMM.2007.911299
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200004
DA 2024-07-18
ER

PT J
AU Chen, ZZ
   Ngan, KN
AF Chen, Zhenzhong
   Ngan, King Ngi
TI A rate and distortion analysis of multiscale binary shape coding based
   on statistical learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE rate-distortion; shape coding; statistical learning; support vector
   regression
ID SUPPORT VECTOR REGRESSION; MULTIPLE VIDEO OBJECTS; MPEG-4; OPTIMIZATION;
   PARAMETERS; FRAMEWORK; SCHEME; MODEL
AB In this paper, we propose a statistical learning-based approach to analyze the rate-distortion characteristics of MPEG-4 multiscale binary shape coding. We employ the polynomial kernel function and epsilon-Insensitive loss function for our support vector regression. To improve the accuracy of the estimation, rate and distortion related features are incorporated in the statistical learning framework. Our experimental results show that the proposed approach can achieve good performance, e.g., modelling the rate-distortion curves accurately.
C1 Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Chen, ZZ (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM zchen@ee.cuhk.edu.hk; knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014; 陈, 震中/C-6857-2014; Chen, Zhenzhong/B-3110-2011;
   Chen, Zhenzhong/C-2529-2015
OI Ngan, N/0000-0003-1946-3235; 
CR Alatan AA, 1998, IEEE T CIRC SYST VID, V8, P802, DOI 10.1109/76.735378
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], P INT C ART NEUR NET
   Bertsekas D. P., 1995, NONLINEAR PROGRAMMIN
   BOOT J.C.G., 1964, QUADRATIC PROGRAMMIN
   Boswell D., 2002, Introduction to Support Vector Machines
   Brady N, 1999, IEEE T CIRC SYST VID, V9, P1170, DOI 10.1109/76.809154
   Chen Z, 2004, IEE P-VIS IMAGE SIGN, V151, P250, DOI 10.1049/ip-vis:20040517
   Chen ZZ, 2005, IEEE T CIRC SYST VID, V15, P1170, DOI 10.1109/TCSVT.2005.852621
   Chen ZZ, 2004, IEEE T CIRC SYST VID, V14, P869, DOI 10.1109/TCSVT.2004.828331
   Cherkassky V, 2004, NEURAL NETWORKS, V17, P113, DOI 10.1016/S0893-6080(03)00169-2
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   Costa L., 2001, SHAPE ANAL CLASSIFIC
   Descombes X, 1999, IEEE T IMAGE PROCESS, V8, P954, DOI 10.1109/83.772239
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Erol B, 2000, IEEE T MULTIMEDIA, V2, P129, DOI 10.1109/6046.845016
   GROUIOS G, 1999, PHYS THER REV, V4, P29
   Gunn S. R., 1998, SUPPORT VECTOR MACHI
   Hang HM, 1997, IEEE T CIRC SYST VID, V7, P287
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P1221, DOI 10.1109/76.974677
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P928, DOI 10.1109/76.937431
   Hotter M., 1990, Signal Processing: Image Communication, V2, P409, DOI 10.1016/0923-5965(90)90027-F
   Huber P., 1981, Robust Statistics
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   *ISO IEC, 1999, 1449621999 ISO IEC
   Katsaggelos AK, 1998, P IEEE, V86, P1126, DOI 10.1109/5.687833
   KOENEN R, 2001, MPEG 4 OVERVIEW ISO
   Kung S.Y., 1993, Digital Neural Network
   Lee SH, 1999, IEEE T CIRC SYST VID, V9, P44, DOI 10.1109/76.744274
   Lin LJ, 1998, IEEE T CIRC SYST VID, V8, P446, DOI 10.1109/76.709411
   Meier FW, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P9, DOI 10.1109/ICIP.1997.638660
   Melnikov G, 2000, IEEE T CIRC SYST VID, V10, P744, DOI 10.1109/76.856451
   Müller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   NEUHOFF DL, 1985, IEEE T INFORM THEORY, V31, P53, DOI 10.1109/TIT.1985.1056998
   Nunes P, 2000, SIGNAL PROCESS-IMAGE, V15, P585, DOI 10.1016/S0923-5965(99)00041-7
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   Schuster G., 1997, RATE DISTORTION BASE
   Schuster GM, 1998, IEEE T IMAGE PROCESS, V7, P13, DOI 10.1109/83.650847
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Vapnik V., 1999, NATURE STAT LEARNING
   Vetro A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P802, DOI 10.1109/ICIP.1999.823007
   Vetro A, 2003, IEEE T IMAGE PROCESS, V12, P356, DOI 10.1109/TIP.2003.809016
   Vetro A, 1999, IEEE T CIRC SYST VID, V9, P186, DOI 10.1109/76.744285
   Wang HH, 2003, IEEE T IMAGE PROCESS, V12, P1181, DOI 10.1109/TIP.2003.816570
   Wang Y., 2002, VIDEO PROCESSING COM
   Yamaguchi N, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P504, DOI 10.1109/ICIP.1997.647960
   2001, ISOIECJTC1SC29WG11
NR 47
TC 1
Z9 1
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 987
EP 994
DI 10.1109/TMM.2007.898929
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800008
DA 2024-07-18
ER

PT J
AU Guo, JM
AF Guo, Jing-Ming
TI A new model-based digital halftoning and data hiding designed with LMS
   optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data hiding; dot-diffusion; dot-gain; error diffusion; halftone;
   least-squares; ordered dither; printer model; watermarking
ID ERROR; WATERMARKING
AB This work employs the well known least-mean-square (LMS) method to design an adaptive filter to produce high-quality halftone images. The filter can be regarded as a transformation medium between original gray level images and corresponding halftone images. Experimental results indicate that the proposed LMS-designed halftoning offers the extra benefit of edge enhancement. Since a halftone image is typically used in printing, a modified printer model, which can coordinate with the proposed LMS-designed halftoning, is proposed to eliminate the harm caused by the dot-gain effect in printing. Moreover, two data hiding applications, the Direct Embedding LMS-designed Halftone technique (DELDH) and the Information Sharing LMS-Designed Halftone technique (ISLDH), are proposed to demonstrate the performance of the proposed LMS-designed halftoning. The experimental results show that, both techniques can be used with the proposed modified printer model to achieve excellent image quality and decoded visual patterns.
C1 Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei 10617, Taiwan.
C3 National Taiwan University of Science & Technology
RP Guo, JM (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei 10617, Taiwan.
EM jmguo@seed.net.tw
CR ANASTASSIOU D, 1988, UNPUB IEEE S CIRC SY
   FLOYD R, 1975, SID INT S, P36
   Fu MS, 2001, SIGNAL PROCESS-IMAGE, V16, P909, DOI 10.1016/S0923-5965(00)00052-7
   Fu MS, 2000, PROC SPIE, V4067, P1671, DOI 10.1117/12.386631
   FU MS, 2000, P IEEE INT C AC SPEE, V4, P2318
   Fu WS, 2001, INT CONF ACOUST SPEE, P1965, DOI 10.1109/ICASSP.2001.941332
   Goldschneider JR, 1997, IEEE T IMAGE PROCESS, V6, P956, DOI 10.1109/83.597271
   GOLDSCHNEIDER JR, 1996, P IEEE INT C IM PROC, V1, P565
   Hel-Or HZ, 2001, J ELECTRON IMAGING, V10, P794, DOI 10.1117/1.1382612
   Jarvis JF, 1976, Comput Graph Image Process, V5, P13, DOI DOI 10.1016/S0146-664X(76)80003-2
   KNOW KT, Patent No. 5734752
   Knox KT, 1999, J ELECTRON IMAGING, V8, P422, DOI 10.1117/1.482710
   Lai CC, 1998, J IMAGING SCI TECHN, V42, P241
   Li P., 2004, IEEE Transactions on Image Processing, V13
   Lin YY, 1997, IEEE SIGNAL PROC LET, V4, P36, DOI 10.1109/97.554466
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Naor M, 1994, LECT NOTES COMPUTER, V950, P1, DOI [10.1007/BFb0053419, DOI 10.1007/BFB0053419]
   Pappas TN, 1997, IEEE T IMAGE PROCESS, V6, P1014, DOI 10.1109/83.597276
   PAPPAS TN, 1995, IEEE T IMAGE PROCESS, V4, P66, DOI 10.1109/83.350813
   Pappas TN, 1999, IEEE T IMAGE PROCESS, V8, P1102, DOI 10.1109/83.777090
   Pei SC, 2003, IEEE SIGNAL PROC LET, V10, P349, DOI 10.1109/LSP.2003.817856
   Pei SC, 2003, IEEE T CIRC SYST VID, V13, P867, DOI 10.1109/TCSVT.2003.815943
   ROETLING PG, 1979, J APPL PHOTOGR ENG, V5, P179
   Rosefielde S, 2005, EUR J COMP ECON, V2, P3
   Shiau JN, 1996, P SOC PHOTO-OPT INS, V2658, P222, DOI 10.1117/12.236968
   SONNENBERG H, 1982, APPL OPTICS, V21, P1745, DOI 10.1364/AO.21.001745
   STUCKI P, 1981, RZ1060 IBM RES LAB
   Ulichney R., 1987, DIGITAL HALFTONING
   Wang SG, 2000, PROC SPIE, V3971, P218, DOI 10.1117/12.384976
   WANG SG, Patent No. 5790703
NR 30
TC 19
Z9 19
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 687
EP 700
DI 10.1109/TMM.2007.895678
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200002
DA 2024-07-18
ER

PT J
AU Kuo, WC
AF Kuo, Wen-Chung
TI Comments on "A multikey secure multimedia proxy using asymmetric
   reversible parametric sequences: Theory, design, and implementation"
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE authentication; multimedia; RSA
AB To protect multimedia data in audio and video streaming applications from unauthorized access by intruders, Yeung et al. recently proposed a multikey multimedia proxy with enhanced security features based on asymmetric reversible parametric sequences (ARPS) and the RSA technique. The authors claimed that their scheme provided a number of security features, including data confidentiality against member collusion. However, the security of their scheme is not as robust as they claim. In this paper, it is shown that the YLY-scheme remains vulnerable to collusion attacks.
C1 Natl Formosa Univ, Dept Comp Sci & Informat Engn, Yunlin 632, Taiwan.
C3 National Formosa University
RP Kuo, WC (corresponding author), Natl Formosa Univ, Dept Comp Sci & Informat Engn, Yunlin 632, Taiwan.
EM simonkuo@nfu.edu.tw
CR [Anonymous], 2016, HDB APPL CRYPTOGRAPH
   Malik D., 2004, Discrete Mathematical Structures: Theory and Applications
   RIVEST RL, 1978, COMMUN ACM, V21, P120, DOI [10.1145/359340.359342, 10.1145/357980.358017]
   *RSA CRYPT STAND R, 1999, PKCS1 V2 1 RSA CRYPT
   Yeung SF, 2005, IEEE T MULTIMEDIA, V7, P330, DOI 10.1109/TMM.2005.843361
NR 5
TC 1
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 420
EP 421
DI 10.1109/TMM.2006.886386
PG 2
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900019
DA 2024-07-18
ER

PT J
AU Kusmierek, E
   Dong, YF
   Du, DHC
AF Kusmierek, E
   Dong, YF
   Du, DHC
TI Loopback: Exploiting collaborative caches for large-scale streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content distribution; peer-to-peer streaming; video streaming
ID VIDEO
AB In this paper, we propose a Loopback approach in a two-level streaming architecture to exploit collaborative client/proxy buffers for improving the quality and efficiency of large-scale streaming applications. At the upper level we use a content delivery network (CDN) to deliver video from a central server to proxy servers. At the lower level a proxy server delivers video with the help of collaborative client caches. In particular, a proxy server and its clients in a local domain cache different portions of a video and form delivery loops. In each loop, a single video stream originates at the proxy, passes through a number of clients, and finally is passed back to the proxy. As a result, with limited bandwidth and storage space contributed by collaborative clients, we are able to significantly reduce the required network bandwidth, I/O bandwidth, and cache space of a proxy. Furthermore, we develop a local repair scheme to address the client failure issue for enhancing service quality and eliminating most required repairing load at the central server. For popular videos, our local repair scheme is able to handle most of single-client failures without service disruption and retransmissions from the central server. Our analysis and simulations have shown the effectiveness of the proposed scheme.
C1 Poznan Supercomp & Networking Ctr, PL-61704 Poznan, Poland.
   Univ Hawaii, Dept Elect & Comp Engn, Honolulu, HI 96822 USA.
   Univ Minnesota, Dept Comp Sci, Minneapolis, MN 55455 USA.
C3 Polish Academy of Sciences; Poznan Supercomputing & Networking Center;
   University of Hawaii System; University of Minnesota System; University
   of Minnesota Twin Cities
RP Poznan Supercomp & Networking Ctr, PL-61704 Poznan, Poland.
EM kusmiere@man.poznan.pl; yinafei@hawaii.edu; du@cs.umn.edu
CR AGGARWAL C, 1996, P INT C MULT SYST 96
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   CHU YH, 2000, P ACM SIGMETRICS 200
   DONG Y, 2004, P 8 INT MULT SYST AP
   Guo M, 2004, IEEE INFOCOM SER, P1501
   GUO Y, 2003, P WWW 2003
   Heffeeda M., 2003, P ACM MULTIMEDIA, P45
   Hu AL, 2001, IEEE INFOCOM SER, P508, DOI 10.1109/INFCOM.2001.916754
   HUA K, 1998, P ACM MULT
   Jannotti J, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P197
   JUNG J, 2002, P INT WORLD WID WEB
   KUSMIERCK E, 2005, 0529 U MINN DEP COMP
   Ma WH, 2004, IEEE T MULTIMEDIA, V6, P599, DOI 10.1109/TMM.2004.830819
   Ma WH, 2002, IEEE T MULTIMEDIA, V4, P539, DOI 10.1109/TMM.2002.806536
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Sheu S, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P110, DOI 10.1109/MMCS.1997.609583
   Tran D.A., 2003, P IEEE INFOCOM
   XU D, 2003, SPIE ACM C MULT COMP
NR 18
TC 16
Z9 19
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 233
EP 242
DI 10.1109/TMM.2005.864277
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300005
DA 2024-07-18
ER

PT J
AU Shirani, S
AF Shirani, S
TI Content-based multiple description image coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID DESIGN
AB The multiple description coding method proposed in this paper provides the least amount of degradation, caused by loss of descriptors, for those areas of the image which are of greater interest. This is achieved by employing a nonlinear geometrical transform to add redundancy mainly to the area of interest followed by a partitioning of the transformed image into subimages which are coded and transmitted separately. Simulations show that this approach yields acceptable performance even when only one descriptor is received.
C1 McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
C3 McMaster University
RP McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
CR Basu A, 1998, IEEE T SYST MAN CY A, V28, P137, DOI 10.1109/3468.661143
   Chung DM, 1999, IEEE T CIRC SYST VID, V9, P895, DOI 10.1109/76.785727
   ELGAMAL AA, 1982, IEEE T INFORM THEORY, V28, P851, DOI 10.1109/TIT.1982.1056588
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   INGLE A, 1995, IEEE T SPEECH AUDI P, V3, P48, DOI 10.1109/89.365381
   VAISHAMPAYAN VA, 1993, IEEE T INFORM THEORY, V39, P821, DOI 10.1109/18.256491
   WALLACE RS, 1994, INT J COMPUT VISION, V13, P71, DOI 10.1007/BF01420796
   Wang Y, 2001, IEEE T IMAGE PROCESS, V10, P351, DOI 10.1109/83.908500
NR 9
TC 9
Z9 12
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 411
EP 419
DI 10.1109/TMM.2005.864349
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300021
DA 2024-07-18
ER

PT J
AU Ma, YF
   Hua, XS
   Lu, L
   Zhang, HJ
AF Ma, YF
   Hua, XS
   Lu, L
   Zhang, HJ
TI A generic framework of user attention model and its application in video
   summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 10th ACM International Conference on Multimedia
CY DEC 01-06, 2002
CL Juan les Pins, FRANCE
SP ACM
DE attention modeling; video content analysis; video summarization
ID VISUAL-ATTENTION; SELECTIVE ATTENTION
AB Due to the information redundancy of video, automatically extracting essential video content is one of key techniques for accessing and managing large video library. In this paper, we present a generic framework of a user attention model, which estimates the attentions viewers may pay to video contents. As human attention is an effective and efficient mechanism for information prioritizing and filtering, user attention model provides an effective approach to video indexing based on importance ranking. In particular, we define viewer attention through multiple sensory perceptions, i.e. visual and aural stimulus as well as partly semantic understanding. Also, a set of modeling methods for visual and aural attentions are proposed. As one of important applications of user attention model, a feasible solution of video summarization, without fully semantic understanding of video content as well as complex heuristic rules, is implemented to demonstrate the effectiveness, robustness, and generality of the user attention model. The promising results from the user study on video summarization indicate that the user attention model is an alternative way to video understanding.
C1 Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Microsoft Research Asia; Microsoft
RP Microsoft Res Asia, Beijing 100080, Peoples R China.
EM yfma@microsoft.com; xshua@mierosoft.com; llu@microsoft.com;
   hjzhang@microsoft.com
CR Ahmad, 1991, ADV NEURAL INFORM PR, V4, P420
   [Anonymous], 2018, WORKSHOP IIPHDW
   [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   Baluja S, 1997, ROBOT AUTON SYST, V22, P329, DOI 10.1016/S0921-8890(97)00046-8
   BOLLMANN R, 1997, INTEGRATION STATIC D, P483
   Broadbent DE, 2013, PERCEPTION COMMUNICA
   Campisi P, 1999, PROC SPIE, V3813, P861, DOI 10.1117/12.366844
   Christel M. G., 1998, CHI 98. Human Factors in Computing Systems. CHI 98 Conference Proceedings, P171, DOI 10.1145/274644.274670
   CRIK F, 1990, P COLD SPRING HARB S, V15, P953
   DEUTSCH JA, 1963, PSYCHOL REV, V70, P80, DOI 10.1037/h0039515
   Dufaux F, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P275, DOI 10.1109/ICIP.2000.899354
   Egeth HE, 1997, ANNU REV PSYCHOL, V48, P269, DOI 10.1146/annurev.psych.48.1.269
   Girgensohn A, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P756, DOI 10.1109/MMCS.1999.779294
   Gong YH, 2000, PROC CVPR IEEE, P174, DOI 10.1109/CVPR.2000.854772
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   Horvitz E, 2003, COMMUN ACM, V46, P52, DOI 10.1145/636772.636798
   HUA XS, 2004, P PAC RIM C MULT 200
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   James W., 1890, The Principles of Psychology, V1
   KIM C, 2000, P 8 ACM INT C MULT, P303
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   LAN DJ, 2003, P INT C MULT EXP BAL, V3, P469
   Li SZ, 2002, LECT NOTES COMPUT SC, V2353, P67
   Li Y, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P269
   LU L, 2001, P IEEE INT C MULT EX, P749
   Lu Lie., 2001, Proceedings of the ninth ACM international conference on Multimedia, P203
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Ma YF, 2002, IEEE IMAGE PROC, P129
   MILANESE R, 1995, OPT ENG, V34, P2428, DOI 10.1117/12.205668
   Nam J., 1999, PROC 7 ACM INT C MUL, P53
   Omoigui N., 1999, P SIGCHI C HUMAN FAC, P136
   Orriols X, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P335, DOI 10.1109/ICCV.2001.937645
   Salah AA, 2002, IEEE T PATTERN ANAL, V24, P420, DOI 10.1109/34.990146
   Smith MA, 1997, PROC CVPR IEEE, P775, DOI 10.1109/CVPR.1997.609414
   Stefanidis A, 2000, 11TH INTERNATIONAL WORKSHOP ON DATABASE AND EXPERT SYSTEMS APPLICATION, PROCEEDINGS, P906, DOI 10.1109/DEXA.2000.875134
   Sundaram Hari., 2002, COMUNICA O APRESENTA, DOI [DOI 10.1145/641007.641042, 10.1145/641007.641042]
   Syeda-Mahmood T., 2001, PROC ACM MULITMEDIA, V9, P119, DOI DOI 10.1145/500141.500161
   TREISMAN A, 1988, PSYCHOL REV, V95, P15, DOI 10.1037/0033-295X.95.1.15
   TREISMAN A, 1998, VISUAL ATTENTION
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Wolf W, 1996, INT CONF ACOUST SPEE, P1228, DOI 10.1109/ICASSP.1996.543588
   WOLFE JM, 1990, AI AND THE EYE, P79
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 44
TC 339
Z9 400
U1 0
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 907
EP 919
DI 10.1109/TMM.2005.854410
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900012
DA 2024-07-18
ER

PT J
AU AlRegib, G
   Altunbasak, Y
   Rossignac, J
AF AlRegib, G
   Altunbasak, Y
   Rossignac, J
TI An unequal error protection method for progressively transmitted 3-D
   models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE joint source and channel coding; multimedia networking; packet loss
   resilient transmission; unequal error protection (UEP); virtual reality
   over IP networks; 3-D graphics streaming
ID COMPRESSION
AB In this paper, we present a packet-loss resilient system for the transmission of progressively compressed three-dimensional (3-D) models. It is based on a joint source and channel coding approach that trades off geometry precision for increased error resiliency to optimize the decoded model quality on the client side. We derive a theoretical framework for the overall system by which the channel packet loss behavior and the channel bandwidth can be directly related to the decoded model quality at the receiver. First, the 3-D model is progressively compressed into a base mesh and a number of refinement layers. Then, we assign optimal forward error correction code rates to protect these layers according to their importance to the decoded model quality. Experimental results show that with the proposed unequal error protection approach, the decoded model quality degrades more gracefully (compared to either no error protection or equal error protection methods) as the packet-loss rate increases.
C1 Georgia Tech, Ctr Signal & Image Proc, Atlanta, GA 30332 USA.
   Georgia Inst Technol, Sch Elect & Comp Engn, Savannah, GA 31407 USA.
   Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
   Georgia Inst Technol, GVU Ctr, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology
RP Georgia Tech, Ctr Signal & Image Proc, Atlanta, GA 30332 USA.
EM gregib@ece.gatech.edu; yucel@ece.gatech.edu; jarek@cc.gatech.edu
OI AlRegib, Ghassan/0000-0001-6818-8001
CR Bajaj C L., 1998, Error resilient transmission of compressed VRML
   BELEGUNDU AD, 1990, OPTIMIZATION CONCEPT
   Bischoff S, 2002, COMPUT GRAPH-UK, V26, P665, DOI 10.1016/S0097-8493(02)00122-X
   Blahut R.E., 1983, Theory and practice of error control codes
   Chow MM, 1997, VISUALIZATION '97 - PROCEEDINGS, P347, DOI 10.1109/VISUAL.1997.663902
   DEERING M, 1995, P 22 ANN C COMP GRAP, P13
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Girod B, 1998, P SOC PHOTO-OPT INS, V3653, P833, DOI 10.1117/12.334735
   GUMHOLD S, 1998, P SIGGRAPH 98, P133
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   HOPPE H, 1998, MSRTR982
   Horn U, 1999, SIGNAL PROCESS-IMAGE, V15, P77, DOI 10.1016/S0923-5965(99)00025-9
   Onwubiko C., 2000, INTRO ENG DESIGN OPT
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   POPOVIC J, 1997, P SIGGRAPH 97, P217
   RONFARD R, 1996, P EUR 96 COMP GRAPH, V15, pC67
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   ROSSIGNAC J, 2000, LECT ACM SIGGRAPH 00
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Taubin G, 1998, P IEEE, V86, P1228, DOI 10.1109/5.687837
   TAUBIN G, 1998, P SIGGRAPH 98, P123
   TOUMA C, 1998, P GRAPH INT VANC BC
   WALSH AE, 2002, MPEG4 JUMP START
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wong W., 1996, Principles of color design
   YAN Z, 1999, P 1999 IEEE INT S CI, V4, P495
   Yan ZD, 2001, IEEE T CIRC SYST VID, V11, P860, DOI 10.1109/76.931112
NR 27
TC 23
Z9 30
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 766
EP 776
DI 10.1109/TMM.2005.850981
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000016
DA 2024-07-18
ER

PT J
AU Ho, WKH
   Cheuk, WK
   Lun, DPK
AF Ho, WKH
   Cheuk, WK
   Lun, DPK
TI Content-based scalable H.263 video coding for road traffic monitoring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based scalable video coding; H.263
AB For sending video data through very low bit-rate mobile channels, video codec with high compression rate is the prerequisite Although the H;263 video codec is recommended as one of the candidates due to its, simplicity and efficiency, it is generally believed that its compression efficiency can be further improved if the content-based scalable video, coding technique can be applied. in this paper, we propose a Modified H.263 encoder which supports real-time content-based scalable video coding. The proposed technique is applied to real-time video surveillance systems for road traffic monitoring. For the proposed approach, the moving objects, i.e. cars, are first extracted from the steady background. Their activities are then further classified as fast or slow by assessing the regularity of their motion. The information is then passed to a modified H.263 encoder to reduce the temporal and spatial redundancies in the video. As compared with the conventional H.263 encoder using for the same application, the proposed system has a 20% increase in compression rate with negligible visual distortion. The proposed system fully complies with the ITU H.263 standard hence the encoded bit stream is completely comprehensible to the conventional H.263 decoder.
C1 Hong Kong Polytech Univ, Ctr Multimedia Signal Proc, Dept Elect & Informat Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Hong Kong Polytech Univ, Ctr Multimedia Signal Proc, Dept Elect & Informat Engn, Hong Kong, Hong Kong, Peoples R China.
EM enpklun@polyu.edu.hk
RI Lun, Daniel Pak Kong/H-2120-2017
OI Lun, Daniel Pak Kong/0000-0003-3891-1363
CR Fukuhara T, 1997, IEEE T CIRC SYST VID, V7, P212, DOI 10.1109/76.554432
   Han SC, 1998, IEEE J SEL AREA COMM, V16, P56, DOI 10.1109/49.650920
   HO KH, 2000, P INT WORKSH MULT DA, P163
   HUANG CL, 1996, P INT C IM PROC, V1, P649
   JEONG YA, 1996, P IEEE AS PAC C, P133
   KIM JT, 1993, IEEE J SEL AREA COMM, V11, P59, DOI 10.1109/49.210544
   MCLAREN DL, 1991, IEE P COMMUNICATION, V138
   Netravali A.N., 1995, DIGITAL PICTURES REP, V2nd
   NGAN KN, 1989, IEEE T ACOUST SPEECH, V37, P1743, DOI 10.1109/29.46556
   NINOMIYA Y, 1988, P 2 INT WORKSH SIGN, P579
   PERKINS MG, 1989, PSYCHOPHYSICALLY JUS, P1815
   SALARI E, 1995, P 1995 INT C AC SPEE, V4, P2241
   SOH J, 1995, P IEEE INT C INTELLI, V1, P679
   STEWART BD, 1994, P 7 INT C ROAD TRAFF, P133
   Talluri R, 1997, IEEE T CIRC SYST VID, V7, P221, DOI 10.1109/76.554433
NR 15
TC 7
Z9 10
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 615
EP 623
DI 10.1109/TMM.2005.850959
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000003
DA 2024-07-18
ER

PT J
AU Snoek, CGM
   Worring, M
AF Snoek, CGM
   Worring, M
TI Multimedia event-based video indexing using time intervals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE context; multimodal integration; semantic event classification;
   statistical pattern recognition; synchronization; time interval
   relations; video indexing
AB We propose the time interval multimedia event (TIME) framework as a robust approach for classification of semantic events in multimodal video documents. The representation used in TIME extends the Allen temporal interval relations and allows for proper inclusion of context and synchronization of the heterogeneous information sources involved in multimodal video analysis. To demonstrate the viability of our approach, it was evaluated on the domains of soccer and news broadcasts. For automatic classification of semantic events, we compare three different machine learning techniques, i.c. C4.5 decision tree, maximum entropy, and support vector machine. The results show that semantic video indexing results significantly benefit from using the TIME framework.
C1 Univ Amsterdam, Inst Informat, Intelligent Syst Lab, NL-1098 SJ Amsterdam, Netherlands.
C3 University of Amsterdam
RP Univ Amsterdam, Inst Informat, Intelligent Syst Lab, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.
EM cgmsnoek@science.uva.nl
RI Worring, Marcel/JRW-7059-2023
OI Snoek, Cees/0000-0001-9092-1556
CR Aiello M., 2002, International Journal on Document Analysis and Recognition, V5, P1, DOI 10.1007/s10032-002-0080-x
   ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434
   [Anonymous], P 3 ACM INT C MULT S
   ASSFALG J, 2002, P IEEE INT C MULT EX
   BAAN J, 2001, P 10 TEXT RETR C GAI
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Berger AL, 1996, COMPUT LINGUIST, V22, P39
   Bertini M, 2002, PATTERN RECOGN, V35, P581, DOI 10.1016/S0031-3203(01)00061-9
   DARROCH JN, 1972, ANN MATH STAT, V43, P1470, DOI 10.1214/aoms/1177692379
   Eickeler S, 1999, INT CONF ACOUST SPEE, P2997, DOI 10.1109/ICASSP.1999.757471
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   HAN M, 2002, P ACM MULT JUAN LES
   HUANG J, 1999, P IEEE WORKSH MULT S
   Ide I, 1999, LECT NOTES COMPUT SC, V1554, P87
   Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Lau Raymond., 1993, Proc. ARPA Human Language Technologies Workshop, P81
   Leonardi R, 2002, IEEE MULTIMEDIA, V9, P44, DOI 10.1109/93.998057
   LIN WH, 2002, P ACM MULT JUAN LES
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Quinlan J. R., 1993, PROGRAMS MACHINE LEA
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek CGM, 2005, MULTIMED TOOLS APPL, V25, P5, DOI 10.1023/B:MTAP.0000046380.27575.a5
   Snoek CGM, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P481
   Vapnik VN, 2000, NATURE STAT LEARNING, DOI DOI 10.1007/978-1-4757-3264-1
   YOW D, 1995, P AS C COMP VIS SING
   Zhou WS, 2002, INFORM SYST, V27, P559, DOI 10.1016/S0306-4379(02)00018-2
NR 28
TC 59
Z9 72
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 638
EP 647
DI 10.1109/TMM.2005.850966
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000006
DA 2024-07-18
ER

PT J
AU Lu, Y
   Zhang, HJ
   Liu, WY
   Hu, CH
AF Lu, Y
   Zhang, HJ
   Liu, WY
   Hu, CH
TI Joint semantics and feature based image retrieval using relevance
   feedback
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE automatic image annotation; image retrieval; image semantics; relevance
   feedback
AB Relevance feedback is a powerful technique for image retrieval and has been an active research direction for the past few years. Various ad hoc parameter estimation techniques have been proposed for relevance feedback. In addition, methods that perform optimization on multilevel image content model have been formulated. However, these methods only perform relevance feedback on low-level image features and fail to address the images' semantic content. In this paper, we propose a relevance feedback framework to take advantage of the semantic contents of images in addition to low-level features. By forming a semantic network on top of the keyword association on the images, we are able to accurately deduce and utilize the images' semantic contents for retrieval purposes. We also propose a ranking measure that is suitable for our framework. The accuracy and effectiveness of our method is demonstrated with experimental results on real-world image collections.
C1 Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   Beijing Sigma Ctr, Microsoft Res China, Beijing 100080, Peoples R China.
   City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 Simon Fraser University; Microsoft; City University of Hong Kong
RP Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM yel@cs.sfu.ca; hjzhang@microsoft.com; csliuwy@cityu.edu.hk;
   chhu@microsoft.com
RI LIU, Wenyin/C-1345-2012
CR BUCKLEY C, SIGIR 95
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   FURNAS GW, 1987, COMMUN ACM, V30, P964, DOI 10.1145/32206.32212
   LEE C, 1998, INFORMATION EMBEDDIN
   LU Y, 2000, P ACM MULT, P31
   PAEK S, SIGIR 99
   Rui Y, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P815, DOI 10.1109/ICIP.1997.638621
   RUI Y, 1990, P ACM MULT ORL FL OC, V2, P67
   SALTON G, 1983, INTRO MODERN INFORMA
   SHAW WM, 1995, INFORM PROCESS MANAG, V31, P491, DOI 10.1016/0306-4573(95)00011-5
   VASCONCELOS N, 1998, DCC 98 SNOWB UT
   [No title captured]
NR 12
TC 42
Z9 59
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 339
EP 347
DI 10.1109/TMM.2003.813280
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500006
DA 2024-07-18
ER

PT J
AU Akinlar, C
   Mukherjee, S
AF Akinlar, C
   Mukherjee, S
TI A scalable bandwidth guaranteed distributed continuous media file system
   using network attached autonomous disks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE distributed file system; file system; Linux operating system; network
   and disk scheduling; network attached storage devices; network file
   system; quality of service
AB Repository for continous media data differs from that of the traditional text-based data both in storage space and streaming bandwidth requirements. The file systems used for continuous media streams need to support large volumes and high bandwidth. In this paper, we propose a novel scalable distributed continuous media file system built using autonomous disks. Autonomous disks are attached directly to the network and are able to perform lightweight processing. We discuss different ways to realize the autonomous disk, and describe a prototype implementation on Linux platform using PC-based hardware. We present the basic requirements of the continuous media file system and present the design methodology and a prototype Linux-based implementation of the distributed file system that supports the requirements. We present experimental results on the performance of the proposed file system prototyped using autonomous disks. We show that the performance of the file system scales linearly with the number of disks and the number of clients. The file system performs much superior to NFS running on the same hardware platform and can deliver higher raw disk bandwidth to the applications. We also present bandwidth and time sensitive read/write procedures for the file system and show that the file system can provide strict bandwidth guarantees for continuous media streams.
C1 Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   Panason Technol Inc, Princeton, NJ 08540 USA.
C3 University System of Maryland; University of Maryland College Park
RP Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM akinlar@cs.umd.edu; sarit@research.panasonic.com
RI Akinlar, Cuneyt/AAH-7483-2019; Akinlar, Cuneyt/U-5132-2019
OI AKINLAR, CUNEYT/0000-0002-0961-7790
CR ACHARYA A, 1998, INT C ARCH SUPP PROG
   AINE H, CENTRAVISION FILE SY
   Akinlar C, 2000, SIXTH IEEE REAL-TIME TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P237, DOI 10.1109/RTTAS.2000.852468
   ANDERSON DP, 1992, ACM T COMPUT SYST, V10, P311, DOI 10.1145/138873.138875
   Anderson TE, 1996, ACM T COMPUT SYST, V14, P41, DOI 10.1145/225535.225537
   Beck M., 1998, LINUX KERNEL INTERNA, V2nd
   BENNER AF, 1996, FIBER CHANNEL GIGABI
   *CARN MELL U, EXTR NASD
   Comer D.E., 1995, INTERNETWORKING TCP, V1
   *COMPAQ, VIRT INT ARCH SYST A
   GEMMELL DJ, 1995, COMPUTER, V28, P40, DOI 10.1109/2.384117
   GHANDEHARIZADEH S, 1995, P ACM SIGMETRICS PER, P37
   GIBSON G, 1997, P ACM INT C MEAS MOD
   GIBSON GA, 1997, CMUCS97118
   KEETON K, 1998, ACM SIGMOD REC
   MOLNAR I, LINUX RAID DRIVERS
   MUKHERJEE S, 1999, PINTLTR21199
   Ozden B, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P580, DOI 10.1109/MMCS.1996.535026
   RAMAMRITHAM K, 1994, P IEEE, V82, P55, DOI 10.1109/5.259426
   RIEDEL E, 1996, P 5 NASA GODD SPAC F
   RIELDEL E, 1997, CMUCS97198
   ROMPOGIANNAKIS Y, 1998, ACM MULTIMEDIA 1998, P297
   Sandberg Russel., 1985, Proceedings of the Summer USENIX conference, P119
   *SEAG TECHN, STOR NETW OBJ OR DEV
   SHENOY PJ, 1998, P ACM SPIE MULT COMP, P124
   SOLTIS SR, 1998, P 6 NASA GODD SSPAC
   Thekkath C. A., 1997, Operating Systems Review, V31, P224, DOI 10.1145/269005.266694
   Triantafillou P, 1998, PARALLEL COMPUT, V24, P21, DOI 10.1016/S0167-8191(97)00115-4
   Triantafillou P, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P500, DOI 10.1109/MMCS.1999.779252
NR 29
TC 0
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 71
EP 96
DI 10.1109/TMM.2003.808820
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200007
DA 2024-07-18
ER

PT J
AU Chen, J
   Yang, M
   Gong, WP
   Yu, Y
AF Chen, Jun
   Yang, Meng
   Gong, Wenping
   Yu, Yang
TI Multi-Neighborhood Guided Kendall Rank Correlation Coefficient for
   Feature Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature matching; Kendall rank correlation coefficient; sorting plans;
   outlier ratio; rank list
ID IMAGE REGISTRATION; SAMPLE CONSENSUS; ALGORITHM; SLAM; SELECTION;
   LOCALITY; MODEL
AB Seeking feature correspondences among two or more images is an important problem in computer vision and image processing. The putative matches constructed by the similarity of feature descriptors are often contaminated by many false matches. Typically, the local neighborhood points of a true match point have a rank order, which will be maintained in the corresponding image, and we call it rank consistency. In this paper, we design a number of sorting plans to obtain the neighborhood rank lists by taking full advantage of the local neighborhood geometry structure. In order to measure the differences between rank lists, we adopt the statistically famous Kendall rank correlation coefficient and generalize its definition for matching problem. We design a neighborhood common element guidance strategy and a multi-neighborhood strategy to improve the universality and robustness of our method. Our method has linear complexity and it has superiority over state-of-the-art methods on several challenging data sets. It also performs well in image registration and loop-closure detection tasks. The source code of our method is publicly available at https://github.com/MnYangs/mGKRCC.
C1 [Chen, Jun; Yang, Meng] China Univ Geosci, Sch Automat, Wuhan 430074, Peoples R China.
   [Chen, Jun; Yang, Meng] Minist Educ, Engn Res Ctr Intelligent Technol Geoexplorat, Hubei Key Lab Adv Control & Intelligent Automat Co, Wuhan, Peoples R China.
   [Gong, Wenping] China Univ Geosci, Fac Engn, Wuhan 430074, Peoples R China.
   [Yu, Yang] Chinese Acad Sci, Shanghai Inst Tech Phys, Key Lab Infrared Syst Detect & Imaging Technol, Shanghai 200083, Peoples R China.
C3 China University of Geosciences; China University of Geosciences;
   Chinese Academy of Sciences; Shanghai Institute of Technical Physics,
   CAS
RP Chen, J (corresponding author), China Univ Geosci, Sch Automat, Wuhan 430074, Peoples R China.
EM chenjun71983@163.com; 20171002458@cug.edu.cn; wenpinggong@cug.edu.cn;
   yuyang@mail.sitp.ac.cn
RI yu, yang/HIZ-9682-2022
OI Chen, Jun/0000-0001-9005-6849
FU National Natural Science Foundation of China
FX No Statement Available
CR Barath D, 2019, PROC CVPR IEEE, P10189, DOI 10.1109/CVPR.2019.01044
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Benesty J., 2009, NOISE REDUCTION SPEE, P1, DOI [10.1007/978-3-642-00296-0_5, DOI 10.1007/978-3-642-00296-05]
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blanco JL, 2009, AUTON ROBOT, V27, P327, DOI 10.1007/s10514-009-9138-7
   Bruzzone L, 2013, P IEEE, V101, P609, DOI 10.1109/JPROC.2012.2197169
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Cech J, 2010, IEEE T PATTERN ANAL, V32, P1568, DOI 10.1109/TPAMI.2009.176
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Nguyen DD, 2019, IEEE T INTELL TRANSP, V20, P4103, DOI 10.1109/TITS.2018.2881556
   Fagin R, 2003, SIAM J DISCRETE MATH, V17, P134, DOI 10.1137/S0895480102412856
   Fan AX, 2022, IEEE T PATTERN ANAL, V44, P8212, DOI 10.1109/TPAMI.2021.3109784
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fritsch J, 2013, IEEE INT C INTELL TR, P1693, DOI 10.1109/ITSC.2013.6728473
   Fritzke B., 1995, Advances in Neural Information Processing Systems 7, P625
   Gehrig Mathias, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3192, DOI 10.1109/ICRA.2017.7989362
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   Hill DLG, 2001, PHYS MED BIOL, V46, pR1, DOI 10.1088/0031-9155/46/3/201
   Ho KL, 2006, ROBOT AUTON SYST, V54, P740, DOI 10.1016/j.robot.2006.04.016
   Jiang JJ, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107665
   Jiang XY, 2022, ISPRS J PHOTOGRAMM, V190, P181, DOI 10.1016/j.isprsjprs.2022.06.009
   Jiang XY, 2021, INFORM FUSION, V73, P22, DOI 10.1016/j.inffus.2021.02.012
   Jiang XY, 2019, IEEE T GEOSCI REMOTE, V57, P6462, DOI 10.1109/TGRS.2019.2906183
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226
   Li JY, 2020, IEEE T IMAGE PROCESS, V29, P3296, DOI 10.1109/TIP.2019.2959244
   Li ZZ, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108489
   Lin WY, 2018, IEEE T PATTERN ANAL, V40, P34, DOI 10.1109/TPAMI.2017.2652468
   Loiola EM, 2007, EUR J OPER RES, V176, P657, DOI 10.1016/j.ejor.2005.09.032
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Ma JY, 2022, INT J COMPUT VISION, V130, P2249, DOI 10.1007/s11263-022-01644-2
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2022, IEEE T INTELL TRANSP, V23, P7896, DOI [10.1109/PESGM46819.2021.9638223, 10.1109/TITS.2021.3074520]
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Ma JY, 2019, PATTERN RECOGN, V92, P231, DOI 10.1016/j.patcog.2019.04.001
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Ma JY, 2018, IEEE T GEOSCI REMOTE, V56, P4435, DOI 10.1109/TGRS.2018.2820040
   Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954
   Ma JY, 2015, IEEE T SIGNAL PROCES, V63, P1115, DOI 10.1109/TSP.2014.2388434
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Myers J. L., 2010, RES DESIGN STAT ANAL
   Nair P, 2009, IEEE T MULTIMEDIA, V11, P611, DOI 10.1109/TMM.2009.2017629
   Nelsen R.B., 2001, Encyclopaedia of mathematics, V3, P226
   Pereira FI, 2018, IEEE T INTELL TRANSP, V19, P3584, DOI 10.1109/TITS.2018.2853579
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sun K, 2020, IEEE T MULTIMEDIA, V22, P2246, DOI 10.1109/TMM.2019.2957984
   Talker L, 2019, IEEE T PATTERN ANAL, V41, P2846, DOI 10.1109/TPAMI.2018.2869560
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Tsintotas KA, 2021, ROBOT AUTON SYST, V141, DOI 10.1016/j.robot.2021.103782
   Tsintotas KA, 2018, IEEE INT CONF ROBOT, P5979, DOI 10.1109/ICRA.2018.8461146
   Tuia D, 2016, ISPRS J PHOTOGRAMM, V120, P1, DOI 10.1016/j.isprsjprs.2016.07.004
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang G, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107986
   Wu Y, 2015, IEEE GEOSCI REMOTE S, V12, P43, DOI 10.1109/LGRS.2014.2325970
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Ye XY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3128292
   Ye YX, 2019, IEEE T GEOSCI REMOTE, V57, P9059, DOI 10.1109/TGRS.2019.2924684
   Ye YX, 2017, IEEE T GEOSCI REMOTE, V55, P2941, DOI 10.1109/TGRS.2017.2656380
   Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282
   Zhang FB, 2021, IEEE T MULTIMEDIA, V23, P1410, DOI 10.1109/TMM.2020.2997193
   Zhang JH, 2019, IEEE I CONF COMP VIS, P5844, DOI 10.1109/ICCV.2019.00594
   Zhang Y, 2004, PHOTOGRAMM ENG REM S, V70, P657
NR 64
TC 12
Z9 12
U1 12
U2 63
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7113
EP 7127
DI 10.1109/TMM.2022.3217410
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300012
DA 2024-07-18
ER

PT J
AU Chen, XW
   Fan, GL
AF Chen, Xiaowei
   Fan, Guoliang
TI Indoor Camera Pose Estimation From Room Layouts and Image Outer Corners
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image outer corners (IOCs); PnL (perspective-n-line) problem; camera
   pose estimation; NSGA-II
ID GENETIC ALGORITHM; POINTS
AB To support indoor scene understanding, room layouts have been recently introduced that define a few typical space configurations according to junctions and boundary lines. In this paper, we study camera pose estimation from eight common room layouts with at least two boundary lines that is cast as a PnL (Perspective-n-Line) problem. Specifically, the intersecting points between image borders and room layout boundaries, named image outer corners (IOCs), are introduced to create additional auxiliary lines for PnL optimization. Therefore, a new PnL-IOC algorithm is proposed which has two implementations according to the room layout types. The first one considers six layouts with more than two boundary lines where 3D correspondence estimation of IOCs creates sufficient line correspondences for camera pose estimation. The second one is an extended version to handle two challenging layouts with only two coplanar boundaries where correspondence estimation of IOCs is ill-posed due to insufficient conditions. Thus the powerful NSGA-II algorithm is embedded in PnL-IOC to estimate the correspondences of IOCs. In the last step, the camera pose is jointly optimized with 3D correspondence refinement of IOCs in the iterative Gauss-Newton algorithm. Experiment results on both simulated and real images show the advantages of the proposed PnL-IOC method on the accuracy and robustness of camera pose estimation from eight different room layouts over the existing PnL methods.
C1 [Chen, Xiaowei; Fan, Guoliang] Oklahoma State Univ, Sch Elect & Comp Engn, Stillwater, OK 74078 USA.
C3 Oklahoma State University System; Oklahoma State University - Stillwater
RP Fan, GL (corresponding author), Oklahoma State Univ, Sch Elect & Comp Engn, Stillwater, OK 74078 USA.
EM xiaowei.chen@okstate.edu; guoliang.fan@okstate.edu
OI Chen, Xiaowei/0000-0002-7608-3119
FU U.S. National Institutes of Health (NIH) [R15 AG061833]; Oklahoma Center
   for the Advancement of Science and Technology (OCAST) Health Research
   [HR18-069]
FX This work was supported in part by the U.S. National Institutes of
   Health (NIH) under Grant R15 AG061833 and in part by the Oklahoma Center
   for the Advancement of Science and Technology (OCAST) Health Research
   under Grant HR18-069.
CR Abdel-Aziz YI, 2015, PHOTOGRAMM ENG REM S, V81, P103, DOI 10.14358/PERS.81.2.103
   Ansar A, 2003, IEEE T PATTERN ANAL, V25, P578, DOI 10.1109/TPAMI.2003.1195992
   Bettadapura V, 2015, IEEE WINT CONF APPL, P626, DOI 10.1109/WACV.2015.89
   Blickle T, 2000, EVOL COMPUT, V1, P181, DOI DOI 10.13140/RG.2.2.23541.42726
   CAGLIOTI V, 1993, PATTERN RECOGN, V26, P1603, DOI 10.1016/0031-3203(93)90016-P
   Cai XJ, 2019, IEEE SENS J, V19, P10003, DOI 10.1109/JSEN.2019.2927733
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chang Liu, 2010, Proceedings 2010 3rd International Conference on Intelligent Networks and Intelligent Systems (ICINIS 2010), P132, DOI 10.1109/ICINIS.2010.39
   Chen HS, 2022, PROC CVPR IEEE, P2771, DOI 10.1109/CVPR52688.2022.00280
   CHEN HH, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P374
   Chen J, 2009, P IEEE INT C MAN SER, P1, DOI [10.1109/ICMSS.2009.5302835, DOI 10.1109/ICMSS.2009.5302835]
   Chen X, 2021, P IEEE CVF INT C COM, P3456
   Chen XW, 2022, IEEE COMPUT SOC CONF, P1548, DOI 10.1109/CVPRW56347.2022.00161
   Choi WG, 2013, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2013.12
   Christy S, 1999, COMPUT VIS IMAGE UND, V73, P137, DOI 10.1006/cviu.1998.0717
   Coughlan JM, 2003, NEURAL COMPUT, V15, P1063, DOI 10.1162/089976603765202668
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Deb K., 1995, Complex Systems, V9, P115
   Del Pero L, 2013, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2013.27
   Del Pero L, 2012, PROC CVPR IEEE, P2719, DOI 10.1109/CVPR.2012.6247994
   Deng W, 2022, INFORM SCIENCES, V585, P441, DOI 10.1016/j.ins.2021.11.052
   DHOME M, 1989, IEEE T PATTERN ANAL, V11, P1265, DOI 10.1109/34.41365
   Dornaika F, 1999, REAL-TIME IMAGING, V5, P215, DOI 10.1006/rtim.1997.0117
   Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71
   Gander W, 2004, Least Squares Fit of Point Clouds
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   He ZN, 2014, IEEE T EVOLUT COMPUT, V18, P269, DOI 10.1109/TEVC.2013.2258025
   Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kodali Shyam P., 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P763, DOI 10.1109/ICETET.2008.139
   Kozeny V, 2015, EXPERT SYST APPL, V42, P2998, DOI 10.1016/j.eswa.2014.11.028
   KUMAR R, 1994, CVGIP-IMAG UNDERSTAN, V60, P313, DOI 10.1006/ciun.1994.1060
   Lee CY, 2017, IEEE I CONF COMP VIS, P4875, DOI 10.1109/ICCV.2017.521
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li LY, 2017, IEEE T CYBERNETICS, V47, P841, DOI 10.1109/TCYB.2016.2530407
   Li SQ, 2012, IEEE T PATTERN ANAL, V34, P1444, DOI 10.1109/TPAMI.2012.41
   Li ZG, 2019, IEEE I CONF COMP VIS, P7677, DOI 10.1109/ICCV.2019.00777
   Lilian Zhang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P217, DOI 10.1007/978-3-642-37431-9_17
   Lin H. J., 2019, P AS C PATT REC CHAM, P719
   Lin HJ, 2018, INT C PATT RECOG, P842, DOI 10.1109/ICPR.2018.8546278
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mirzaei F. M., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P5581, DOI 10.1109/ICRA.2011.5980272
   Mitra K, 2009, CHEM ENG SCI, V64, P5043, DOI 10.1016/j.ces.2009.08.012
   Mohan N, 2022, MULTIMED TOOLS APPL, V81, P1921, DOI 10.1007/s11042-021-11358-1
   Murata T, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON EVOLUTIONARY COMPUTATION, VOLS 1 AND 2, P289, DOI 10.1109/ICEC.1995.489161
   Orlando SA, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P305, DOI 10.5220/0007356503050312
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Pribyl B, 2015, P BRIT MACH VIS C, P1
   Qin Li-Juan, 2008, Acta Automatica Sinica, V34, P130
   Qin LJ, 2008, 2008 7TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION, VOLS 1-23, P4217, DOI 10.1109/WCICA.2008.4594518
   Rabbani M, 2022, J IND MANAG OPTIM, V18, P1035, DOI 10.3934/jimo.2021007
   Ragusa F, 2019, ACM J COMPUT CULT HE, V12, DOI 10.1145/3276772
   Ren YZ, 2017, LECT NOTES COMPUT SC, V10115, P36, DOI 10.1007/978-3-319-54193-8_3
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schwing AG, 2012, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2012.6248006
   Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377
   Silva M., 2012, WORKSH COL DEPTH CAM
   Sun Y, 2018, WIRELESS PERS COMMUN, V102, P1369, DOI 10.1007/s11277-017-5200-5
   Taira H, 2018, PROC CVPR IEEE, P7199, DOI 10.1109/CVPR.2018.00752
   Terzakis George, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P478, DOI 10.1007/978-3-030-58452-8_28
   Nguyen THC, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010072
   Vakhitov A, 2021, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR46437.2021.00463
   Wang HY, 2010, LECT NOTES COMPUT SC, V6312, P435, DOI 10.1007/978-3-642-15552-9_32
   Wang P, 2019, MACH VISION APPL, V30, P603, DOI 10.1007/s00138-019-01012-0
   Weidong Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P632, DOI 10.1007/978-3-030-58517-4_37
   WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354
   Xu C, 2017, IEEE T PATTERN ANAL, V39, P1209, DOI 10.1109/TPAMI.2016.2582162
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P3014, DOI 10.1109/TMM.2020.2967645
   Yang C, 2022, IEEE WINT CONF APPL, P235, DOI 10.1109/WACV51458.2022.00031
   Zhang WD, 2017, IEEE T MULTIMEDIA, V19, P935, DOI 10.1109/TMM.2016.2642780
   Zhang YD, 2017, PROC CVPR IEEE, P5057, DOI 10.1109/CVPR.2017.537
   Zhou LP, 2019, AAAI CONF ARTIF INTE, P9307
NR 74
TC 0
Z9 0
U1 8
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7992
EP 8005
DI 10.1109/TMM.2022.3233308
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400029
PM 38084118
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Gao, GW
   Yu, Y
   Lu, HM
   Yang, J
   Yue, D
AF Gao, Guangwei
   Yu, Yi
   Lu, Huimin
   Yang, Jian
   Yue, Dong
TI Context-Patch Representation Learning With Adaptive Neighbor Embedding
   for Robust Face Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Faces; Image reconstruction; Face recognition; Representation
   learning; Adaptation models; Testing; Adaptive neighbor embedding;
   contextual information; face super-resolution; representation learning
ID HALLUCINATION; REGRESSION
AB Representation learning steered robust face image super-resolution (FSR) methods have attracted extensive attention in the past few decades. Most previous methods were devoted to exploiting the local position patches in the training set for FSR. However, they usually overlooked the sufficient usage of the contextual information around the testing patches, which are useful for stable representation learning. In this article, we attempt to utilize the context-patch around the testing patch and propose a method named context-patch representation learning with adaptive neighbor embedding (CRL-ANE) for FSR. On one hand, we simultaneously use the testing position patch and its adjacent ones for stable representation weight learning. This contextual information can compensate for recovering missing details in the target patch. On the other hand, for each input patch set, due to its inherent facial structural properties, we design an adaptive neighbor embedding strategy to elaborately and adaptively choose primary candidates for more accurate reconstruction. These two improvements enable the proposed method to achieve better SR performance than some of the other methods. Qualitative and quantitative experiments on some benchmarks have validated the superiority of the proposed method over some state-of-the-art methods.
C1 [Gao, Guangwei] Nanjing Univ Posts & Telecommun, Inst Adv Technol, Nanjing 210023, Peoples R China.
   [Gao, Guangwei; Yu, Yi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
   [Lu, Huimin] Kyushu Inst Technol, Dept Mech & Control Engn, Kitakyushu 8048550, Japan.
   [Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Peoples R China.
   [Yue, Dong] Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210023, Peoples R China.
   [Yue, Dong] Nanjing Univ Posts & Telecommun, Coll Artificial Intelligence, Nanjing 210023, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Research Organization
   of Information & Systems (ROIS); National Institute of Informatics (NII)
   - Japan; Kyushu Institute of Technology; Nanjing University of Science &
   Technology; Nanjing University of Posts & Telecommunications; Nanjing
   University of Posts & Telecommunications
RP Yu, Y (corresponding author), Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
EM csggao@gmail.com; yiyu@nii.ac.jp; dr.huimin.lu@ieee.org;
   csjyang@njust.edu.cn; medongy@vip.163.com
RI WANG, SHIHAO/KHC-8263-2024; li, xiaomin/KCX-9845-2024
OI Lu, Huimin/0000-0001-9794-3221
FU National Key Research and Development Program of China [2018AAA0100102,
   2018AAA0100100]; National Natural Science Foundation of China [61972212,
   61772568, 62076139]; Natural Science Foundation of Jiangsu Province
   [BK20190089]; Six Talent Peaks Project in Jiangsu Province [RJFW-011]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grants 2018AAA0100102 and
   2018AAA0100100, in part by the National Natural Science Foundation of
   China under Grants 61972212, 61772568, and 62076139, in part by the
   Natural Science Foundation of Jiangsu Province under Grant BK20190089,
   and in part by Six Talent Peaks Project in Jiangsu Province under Grant
   RJFW-011. The guest editor coordinating the review of this manuscript
   and approving it for publication was Professor David Crandall.
CR Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen L, 2020, IEEE T IMAGE PROCESS, V29, P9002, DOI 10.1109/TIP.2020.3023580
   Chen L, 2019, IEEE T IMAGE PROCESS, V28, P5897, DOI 10.1109/TIP.2019.2920510
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Gao G., 2022, arXiv
   Gao GW, 2022, AAAI CONF ARTIF INTE, P661
   Gao GW, 2022, IEEE T CIRC SYST VID, V32, P2550, DOI 10.1109/TCSVT.2020.3042178
   Gao GW, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107539
   Gao GW, 2017, PATTERN RECOGN, V66, P129, DOI 10.1016/j.patcog.2016.12.021
   Gunawardana A, 2005, J MACH LEARN RES, V6, P2049
   Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920
   Hsu CC, 2019, IEEE T IMAGE PROCESS, V28, P6225, DOI 10.1109/TIP.2019.2924554
   Huang H, 2010, PATTERN RECOGN, V43, P2532, DOI 10.1016/j.patcog.2010.02.007
   Jia K, 2008, IEEE T IMAGE PROCESS, V17, P873, DOI 10.1109/TIP.2008.922421
   Jiang JJ, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3485132
   Jiang JJ, 2020, IEEE T CYBERNETICS, V50, P324, DOI 10.1109/TCYB.2018.2868891
   Jiang JJ, 2019, INFORM SCIENCES, V481, P174, DOI 10.1016/j.ins.2018.12.064
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Jing XY, 2017, IEEE T IMAGE PROCESS, V26, P1363, DOI 10.1109/TIP.2017.2651364
   Jung CK, 2011, IEEE SIGNAL PROC LET, V18, P367, DOI 10.1109/LSP.2011.2140370
   Li JS, 2019, IEEE T CIRC SYST VID, V29, P104, DOI 10.1109/TCSVT.2017.2778227
   Li JC, 2024, Arxiv, DOI arXiv:2109.14335
   Li MY, 2021, IEEE T MULTIMEDIA, V23, P468, DOI 10.1109/TMM.2020.2984092
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Liu LC, 2018, IEEE T CYBERNETICS, V48, P1474, DOI 10.1109/TCYB.2017.2703134
   Liu LC, 2018, IEEE T CYBERNETICS, V48, P1189, DOI 10.1109/TCYB.2017.2682853
   Liu ZS, 2021, IEEE T IMAGE PROCESS, V30, P4157, DOI 10.1109/TIP.2021.3069554
   Ma C, 2020, PROC CVPR IEEE, P5568, DOI 10.1109/CVPR42600.2020.00561
   Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019
   Rajput SS, 2018, INFORM SCIENCES, V463, P227, DOI 10.1016/j.ins.2018.06.050
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Shi JG, 2019, IEEE T MULTIMEDIA, V21, P2223, DOI 10.1109/TMM.2019.2898752
   Shi YK, 2020, IEEE T PATTERN ANAL, V42, P2809, DOI 10.1109/TPAMI.2019.2915301
   Song YB, 2019, INT J COMPUT VISION, V127, P785, DOI 10.1007/s11263-019-01148-6
   Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005
   Wang L, 2020, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR42600.2020.00383
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2014, IEEE T CIRC SYST VID, V24, P802, DOI 10.1109/TCSVT.2013.2290574
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7353, DOI 10.1109/CVPR42600.2020.00738
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Yu X, 2020, IEEE T PATTERN ANAL, V42, P2926, DOI 10.1109/TPAMI.2019.2916881
   Yu X, 2017, PROC CVPR IEEE, P5367, DOI 10.1109/CVPR.2017.570
   Zeng X, 2018, IEEE T CYBERNETICS, V48, P716, DOI 10.1109/TCYB.2017.2655027
   Zhang HM, 2020, IEEE T IMAGE PROCESS, V29, P3132, DOI 10.1109/TIP.2019.2957925
   Zhang KP, 2018, LECT NOTES COMPUT SC, V11215, P196, DOI 10.1007/978-3-030-01252-6_12
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
NR 49
TC 3
Z9 3
U1 5
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1879
EP 1889
DI 10.1109/TMM.2022.3192769
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100023
DA 2024-07-18
ER

PT J
AU Hu, HZ
   Pu, JF
   Zhou, WG
   Li, HQ
AF Hu, Hezhen
   Pu, Junfu
   Zhou, Wengang
   Li, Houqiang
TI Collaborative Multilingual Continuous Sign Language Recognition: A
   Unified Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Assistive technologies; Gesture recognition; Visualization; Videos;
   Hidden Markov models; Feature extraction; Speech recognition; Continuous
   Sign Language Recognition; Multilingual
ID HAND GESTURE RECOGNITION; VIDEO; MODEL
AB Current continuous sign language recognition systems generally target on a single language. When it comes to the multilingual problem, existing solutions often build separate models based on the same network and then train them with their corresponding sign language corpora. Observing that different sign languages share some low-level visual patterns, we argue that it is beneficial to optimize the recognition model in a collaborative way. With this motivation, we propose the first unified framework for multilingual continuous sign language recognition. Our framework consists of a shared visual encoder for visual information encoding, multiple language-dependent sequential modules for long-range temporal dependency learning aimed at different languages, and a universal sequential module to learn the commonality of all languages. An additional language embedding is introduced to distinguish different languages within the shared temporal encoders. Further, we present a max-probability decoding method to obtain the alignment between sign videos and sign words for visual encoder refinement. We evaluate our approach on three continuous sign language recognition benchmarks, i.e., RWTH-PHOENIX-Weather, CSL and GSL-SD. The experimental results reveal that our method outperforms the individually trained recognition models. Our method also demonstrates better performance compared with state-of-the-art algorithms.
C1 [Hu, Hezhen; Pu, Junfu; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Anhui, Peoples R China.
EM alexhu@mail.ustc.edu.cn; pjh@mail.ustc.edu.cn; zhwg@ustc.edu.cn;
   lihq@ustc.edu.cn
FU National Natural Science Foundation of China [U20A20183, 62021001]; GPU
   cluster built by MCC Laboratory of Information Science and Technology
   Institution, USTC
FX This work was supported in part by the National Natural Science
   Foundation of China under Contracts U20A20183 and 62021001, and in part
   by GPU cluster built by MCC Laboratory of Information Science and
   Technology Institution, USTC.
CR Adaloglou N, 2022, IEEE T MULTIMEDIA, V24, P1750, DOI 10.1109/TMM.2021.3070438
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Buehler P, 2009, PROC CVPR IEEE, P2953, DOI 10.1109/CVPRW.2009.5206523
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Y., 2022, ADV NEUR IN
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Evangelidis GD, 2015, LECT NOTES COMPUT SC, V8925, P595, DOI 10.1007/978-3-319-16178-5_42
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   Guo D, 2020, IEEE T IMAGE PROCESS, V29, P1575, DOI 10.1109/TIP.2019.2941267
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu H., 2021, ACM TOMM, V17, P1
   Hu H., 2021, CVPR, P16428
   Hu HZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11067, DOI 10.1109/ICCV48922.2021.01090
   Hu HZ, 2021, AAAI CONF ARTIF INTE, V35, P1558
   Hu HZ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3422360
   Hu LY, 2022, LECT NOTES COMPUT SC, V13695, P511, DOI 10.1007/978-3-031-19833-5_30
   Huang DA, 2016, LECT NOTES COMPUT SC, V9908, P137, DOI 10.1007/978-3-319-46493-0_9
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Huang WB, 2019, IEEE T IMAGE PROCESS, V28, P1773, DOI 10.1109/TIP.2018.2877936
   Jin T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5065, DOI 10.1145/3474085.3475456
   Ka Leong Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P697, DOI 10.1007/978-3-030-58586-0_41
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2016, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR.2016.412
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Koller Oscar., 2016, P BMVC, P1, DOI 10.5244/C.30.136
   Li D., 2020, NeurIPS, V33, P12034
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu X, 2021, PROC CVPR IEEE, P10626, DOI 10.1109/CVPR46437.2021.01049
   Liu X, 2020, IEEE T IMAGE PROCESS, V29, P4583, DOI 10.1109/TIP.2020.2974061
   Liu ZZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P145, DOI 10.1145/3240508.3240530
   Min YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11522, DOI 10.1109/ICCV48922.2021.01134
   Min YC, 2020, PROC CVPR IEEE, P5760, DOI 10.1109/CVPR42600.2020.00580
   Papastratis I, 2020, IEEE ACCESS, V8, P91170, DOI 10.1109/ACCESS.2020.2993650
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pfister T, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.20
   Pu JF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1497, DOI 10.1145/3394171.3413931
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Qiu ZF, 2019, PROC CVPR IEEE, P12048, DOI 10.1109/CVPR.2019.01233
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi BW, 2021, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR46437.2021.00415
   Shi XX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P818, DOI 10.1145/3343031.3351060
   Simonyan K, 2014, ADV NEUR IN, V27
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tao F., 2020, IEEE Trans. Multimedia, V23
   Tornay S, 2020, INT CONF ACOUST SPEE, P6309, DOI [10.1109/icassp40776.2020.9054631, 10.1109/ICASSP40776.2020.9054631]
   Varol G, 2021, PROC CVPR IEEE, P16852, DOI 10.1109/CVPR46437.2021.01658
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang HJ, 2019, IEEE T MULTIMEDIA, V21, P2806, DOI 10.1109/TMM.2019.2915032
   Wang LC, 2014, IEEE T MULTIMEDIA, V16, P751, DOI 10.1109/TMM.2014.2298382
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Xie P, 2022, IEEE T MULTIMEDIA, V24, P3908, DOI 10.1109/TMM.2021.3109665
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yang H, 2020, IEEE T IMAGE PROCESS, V29, P5783, DOI 10.1109/TIP.2020.2984904
   Yang SJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P21, DOI 10.1145/3343031.3350859
   Yang ZY, 2019, Arxiv, DOI arXiv:1908.01341
   Yin F, 2016, LECT NOTES COMPUT SC, V9911, P434, DOI 10.1007/978-3-319-46478-7_27
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhe Niu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P172, DOI 10.1007/978-3-030-58517-4_11
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
   Zhou H, 2019, IEEE INT CON MULTI, P1282, DOI 10.1109/ICME.2019.00223
   Zhu YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P802, DOI 10.1145/3343031.3350932
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 80
TC 4
Z9 4
U1 6
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7559
EP 7570
DI 10.1109/TMM.2022.3223260
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000059
DA 2024-07-18
ER

PT J
AU Huang, TX
   Zou, H
   Cui, JH
   Zhang, JN
   Yang, XM
   Li, L
   Liu, Y
AF Huang, Tianxin
   Zou, Hao
   Cui, Jinhao
   Zhang, Jiangning
   Yang, Xuemeng
   Li, Lin
   Liu, Yong
TI Adaptive Recurrent Forward Network for Dense Point Cloud Completion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D point clouds; recurrent structure; highly efficient completion
AB Point cloud completion is an interesting and challenging task in 3D vision, which aims to recover complete shapes from sparse and incomplete point clouds. Existing completion networks often require a vast number of parameters and substantial computational costs to achieve a high performance level, which may limit their practical application. In this work, we propose a novel Adaptive efficient Recurrent Forward Network (ARFNet), which is composed of three parts: Recurrent Feature Extraction (RFE), Forward Dense Completion (FDC) and Raw Shape Protection (RSP). In an RFE, multiple short global features are extracted from incomplete point clouds, while a dense quantity of completed results are generated in a coarse-to-fine pipeline in the FDC. Finally, we propose the Adamerge module to preserve the details from the original models by merging the generated results with the original incomplete point clouds in the RSP. In addition, we introduce the Sampling Chamfer Distance to better capture the shapes of the models and the balanced expansion constraint to restrict the expansion distances from coarse to fine. According to the experiments on ShapeNet and KITTI, our network can achieve state-of-the-art completion performances on dense point clouds with fewer parameters, smaller model sizes, lower memory costs and a faster convergence.
C1 [Huang, Tianxin; Zou, Hao; Cui, Jinhao; Zhang, Jiangning; Yang, Xuemeng; Li, Lin; Liu, Yong] Zhejiang Univ, Inst Cyber Syst & Control, Hangzhou 310058, Zhejiang, Peoples R China.
   [Liu, Yong] Zhejiang Univ, Huzhou Inst, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Liu, Y (corresponding author), Zhejiang Univ, Inst Cyber Syst & Control, Hangzhou 310058, Zhejiang, Peoples R China.
EM 21725129@zju.edu.cn; zouhao@zju.edu.cn; 21932011@zju.edu.cn;
   186368@zju.edu.cn; xuemengyang@zju.edu.cn; 22032043@zju.edu.cn;
   yongliu@iipc.zju.edu.cn
RI Huang, Tianxin/JQI-8017-2023
OI Huang, Tianxin/0000-0003-3579-371X; jiangning, zhang/0000-0001-8891-6766
FU Key Research and Development Project of Zhejiang Province [2021C01035]
FX Thisworkwas supported by the Key Research and Development Project of
   Zhejiang Province under Grant 2021C01035.
CR Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700
   Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P3022, DOI 10.1109/TMM.2021.3068606
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Giancola S, 2019, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2019.00145
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Han ZZ, 2019, IEEE I CONF COMP VIS, P10441, DOI 10.1109/ICCV.2019.01054
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Huang JJ, 2022, IEEE T MULTIMEDIA, V24, P188, DOI 10.1109/TMM.2020.3047762
   Huang TX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12488, DOI 10.1109/ICCV48922.2021.01228
   Huang TX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P890, DOI 10.1145/3343031.3351061
   Kingma D. P., 2014, arXiv
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Li DP, 2017, IEEE T VIS COMPUT GR, V23, P1809, DOI 10.1109/TVCG.2016.2553102
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Peng Songyou, 2020, Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16, P523
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Reddy ND, 2018, PROC CVPR IEEE, P1906, DOI 10.1109/CVPR.2018.00204
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheng XH, 2022, IEEE T MULTIMEDIA, V24, P2617, DOI 10.1109/TMM.2021.3086711
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie Haozhe, 2020, P EUR C COMP VIS
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang W., 2020, P EUR C COMP VIS, P512, DOI 10.1007/978-3-030-58595-2_31
   Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768
NR 54
TC 2
Z9 2
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5903
EP 5915
DI 10.1109/TMM.2022.3200851
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500019
DA 2024-07-18
ER

PT J
AU Kulkarni, A
   Patil, PW
   Murala, S
   Gupta, S
AF Kulkarni, Ashutosh
   Patil, Prashant W.
   Murala, Subrahmanyam
   Gupta, Sunil
TI Unified Multi-Weather Visibility Restoration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lightweight; degradation removal; multi-weather restoration;
   surveillance applications
ID SINGLE; FRAMEWORK; RAIN; ATTENTION; NETWORK; MODEL; HAZE
AB Automated surveillance is widely opted for applications such as traffic monitoring, vehicle identification, etc. But, various weather degradation factors such as rain and snow streaks, along with atmospheric veil severely affect the perceptual quality of an image, eventually affecting the performance of these applications. There exist weather specific (rain, haze, snow, etc.) methods focusing on respective restoration task. As image restoration is a preprocessing step for high level surveillance applications, it is practically inapplicable to have different architectures for different weather restoration. In this paper, we propose a lightweight unified network, having 1.1 M parameters (1/40th and 1/6th of the existing rain with veil removal, and snow with veil removal methods respectively) for removal of rain and snow along with the veiling effect present in the images. In this network, we propose two parallel streams to handle the degradations and restoration: First, degradation removal stream (DRS) focuses mainly on removing randomly repeating degradations i.e., rain and snow streaks, through the proposed adaptive multi-scale feature sharing block (AMFSB) and stage-wise subtractive block (SSB). Second, feature corrector stream (FCS) mainly focuses on refining the partial outputs of the first stream, reducing the veiling effect and acts supplementary to the first stream. Finally, we leverage contrastive regularization for better convergence of the proposed network. Substantial experiments on synthetic as well as real-world images, along with extensive ablation studies, demonstrate that the proposed method performs competitively with the existing methods for multi-weather image restoration. The code is available at: https://github.com/AshutoshKulkarni4998/UVRNet.
C1 [Kulkarni, Ashutosh; Murala, Subrahmanyam] Indian Inst Technol Ropar, Dept Elect Engn, Comp Vis & Pattern Recognit Lab, Rupnagar 140001, Punjab, India.
   [Patil, Prashant W.] Deakin Univ, Appl Artificial Intelligence Inst A2I2, Geelong, Vic 3125, Australia.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Ropar; Deakin University
RP Murala, S (corresponding author), Indian Inst Technol Ropar, Dept Elect Engn, Comp Vis & Pattern Recognit Lab, Rupnagar 140001, Punjab, India.
EM ashutosh.20eez0008@iitrpr.ac.in; patilprashant1208@gmail.com;
   subbumurala@iitrpr.ac.in; sunil.gupta@deakin.edu.au
RI Murala, Subrahmanyam/D-1397-2017
OI Patil, Prashant/0000-0003-2604-6501; gupta, sunil/0000-0002-4669-9940
FU Department of Science and Technology-Science and Engineering Research
   Board (DST-SERB), India [CRG/2022/006876]
FX This work was supported by the Department of Science and
   Technology-Science and Engineering Research Board (DST-SERB), India,
   under Grant CRG/2022/006876. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Dr. Yong
   Luo.
CR Ancuti CO, 2011, LECT NOTES COMPUT SC, V6493, P501
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen WT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4176, DOI 10.1109/ICCV48922.2021.00416
   Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia TY, 2022, IEEE T IND INFORM, V18, P1511, DOI 10.1109/TII.2021.3059020
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Kulkarni A, 2022, IEEE SIGNAL PROC LET, V29, P229, DOI 10.1109/LSP.2021.3134171
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Li MH, 2019, Arxiv, DOI arXiv:1909.06148
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li RT, 2020, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR42600.2020.00324
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liu R. W., 2022, IEEE Trans. Ind. Informat., early access, DOI [10.1109/TIL2022.3170594, DOI 10.1109/TIL2022.3170594]
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu YF, 2018, IEEE T IMAGE PROCESS, V27, P3064, DOI 10.1109/TIP.2018.2806202
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Mustafa A, 2022, IEEE WINT CONF APPL, P21, DOI 10.1109/WACV51458.2022.00010
   Patil PW, 2021, IEEE T IMAGE PROCESS, V30, P7889, DOI 10.1109/TIP.2021.3108405
   Patil PW, 2019, IEEE T INTELL TRANSP, V20, P4066, DOI 10.1109/TITS.2018.2880096
   Rakotonirina NC, 2020, INT CONF ACOUST SPEE, P3637, DOI 10.1109/ICASSP40776.2020.9054071
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Salazar-Colores S, 2019, IEEE T IMAGE PROCESS, V28, P2357, DOI 10.1109/TIP.2018.2885490
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Sen Deng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14548, DOI 10.1109/CVPR42600.2020.01457
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Shin J, 2022, IEEE T MULTIMEDIA, V24, P245, DOI 10.1109/TMM.2021.3050053
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Wang YL, 2017, IEEE T IMAGE PROCESS, V26, P3936, DOI 10.1109/TIP.2017.2708502
   Wei-Ting Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P754, DOI 10.1007/978-3-030-58589-1_45
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Xianhui Zheng, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P258, DOI 10.1007/978-3-642-42051-1_33
   Xu J., 2012, CANADIAN CTR SCI ED, V5, P49, DOI DOI 10.5539/CIS.V5N3P49
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Yu FS, 2016, Arxiv, DOI [arXiv:1511.07122, DOI 10.48550/ARXIV.1511.07122]
   Yu SJ, 2014, LECT NOTES COMPUT SC, V8866, P439, DOI 10.1007/978-3-319-12436-0_49
   Zamir S. W., 2022, PROC CVPR IEEE, P5728
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H., 2018, PROC INT C LEAR REP
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
   Zhu Q., 2014, IEEE INT SYMP CIRC, P1
NR 66
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7686
EP 7698
DI 10.1109/TMM.2022.3225712
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400006
DA 2024-07-18
ER

PT J
AU Li, J
   Li, WY
   Xu, ZC
   Wang, YH
   Liu, QG
AF Li, Jin
   Li, Wanyun
   Xu, Zichen
   Wang, Yuhao
   Liu, Qiegen
TI Wavelet Transform-Assisted Adaptive Generative Modeling for Colorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic colorization; wavelet transform; unsupervised learning;
   generative model; multi-scale
AB Unsupervised deep learning has recently demonstrated the promise of producing high-quality samples. While it has tremendous potential to promote the image colorization task, the performance is limited owing to the high-dimension of data manifold and model capability. This study presents a novel scheme that exploits the score-based generative model in wavelet domain to address the issues. By taking advantage of the multi-scale and multi-channel representation via wavelet transform, the proposed model learns the richer priors from stacked coarse and detailed wavelet coefficient components jointly and effectively. This strategy also reduces the dimension of the original manifold and alleviates the curse of dimensionality, which is beneficial for estimation and sampling. Moreover, dual consistency terms in the wavelet domain, namely data-consistency and structure-consistency are devised to leverage colorization task better. Specifically, in the training phase, a set of multi-channel tensors consisting of wavelet coefficients is used as the input to train the network with denoising score matching. In the inference phase, samples are iteratively generated via annealed Langevin dynamics with data and structure consistencies. Experiments demonstrated remarkable improvements of the proposed method on both generation and colorization quality, particularly in colorization robustness and diversity.
C1 [Li, Jin; Li, Wanyun; Xu, Zichen; Wang, Yuhao; Liu, Qiegen] Nanchang Univ, Informat Engn Sch, Nanchang 330031, Peoples R China.
C3 Nanchang University
RP Wang, YH; Liu, QG (corresponding author), Nanchang Univ, Informat Engn Sch, Nanchang 330031, Peoples R China.
EM lijin@email.ncu.edu.cn; liwanyun@email.ncu.edu.cn; xuz@ncu.edu.cn;
   wangyuhao@ncu.edu.cn; liuqiegen@hotmail.com
RI tong, li/KDO-7821-2024; Shi, Yaolin/JXN-8322-2024; Wang,
   Zixi/KEI-0077-2024; li, wanyun/HZH-5867-2023
OI Xu, Zichen/0000-0001-9293-8028
FU National Natural Science Foundation of China [61871206]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61871206.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Acharya M., 2020, P INT S DEV CIRC SYS, P1
   Akansu A.N., 2001, Multiresolution signal decomposition: transforms, subbands, and wavelets, V2nd
   Anwar S, 2022, Arxiv, DOI [arXiv:2008.10774, DOI 10.48550/ARXIV.2008.10774]
   Baig MH, 2017, COMPUT VIS IMAGE UND, V164, P111, DOI 10.1016/j.cviu.2017.01.010
   Bakry D., 2014, Analysis and geometry of Markov diffusion operators
   Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bian YX, 2021, APL PHOTONICS, V6, DOI 10.1063/5.0039206
   Block A., 2020, arXiv
   Brooks S, 2011, CH CRC HANDB MOD STA, P1, DOI 10.1201/b10905
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Cao Y, 2017, LECT NOTES ARTIF INT, V10534, P151, DOI 10.1007/978-3-319-71249-9_10
   Charpiat G, 2008, LECT NOTES COMPUT SC, V5304, P126, DOI 10.1007/978-3-540-88690-7_10
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Chowdhury M. M. H., 2012, Int J Comput Sci Issues, V9, P327
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Deshpande A, 2015, IEEE I CONF COMP VIS, P567, DOI 10.1109/ICCV.2015.72
   Fang FM, 2020, IEEE T VIS COMPUT GR, V26, P2931, DOI 10.1109/TVCG.2019.2908363
   Fatima A, 2021, MULTIMED TOOLS APPL, V80, P3775, DOI 10.1007/s11042-020-09861-y
   Fefferman C, 2016, J AM MATH SOC, V29, P983, DOI 10.1090/jams/852
   Feurer M, 2019, SPRING SER CHALLENGE, P3, DOI 10.1007/978-3-030-05318-5_1
   Ghazali K. H., 2007, P 5 STUD C RES DEV, P1
   Ghosh S, 2019, NATL CONF COMMUN, DOI 10.1109/ncc.2019.8732250
   Ghosh S, 2018, IEEE GLOB CONF SIG, P26, DOI 10.1109/GlobalSIP.2018.8646671
   Ghosh S, 2016, IEEE IMAGE PROC, P1823, DOI 10.1109/ICIP.2016.7532673
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Guo TT, 2017, IEEE COMPUT SOC CONF, P1100, DOI 10.1109/CVPRW.2017.148
   Hyvärinen A, 2005, J MACH LEARN RES, V6, P695
   Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jayaram V, 2020, PR MACH LEARN RES, V119
   Kaselimi M, 2019, INT CONF ACOUST SPEE, P2747, DOI 10.1109/ICASSP.2019.8683110
   Kim Y, 2015, IEEE IMAGE PROC, P1404, DOI 10.1109/ICIP.2015.7351031
   Kingma D. P., 2014, arXiv
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6073, DOI 10.1109/TNNLS.2018.2817538
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6323, DOI 10.1109/TNNLS.2018.2829867
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin J, 2000, J SOUND VIB, V234, P135, DOI 10.1006/jsvi.2000.2864
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu QG, 2020, MAGN RESON MED, V83, P322, DOI 10.1002/mrm.27921
   Luan Q., 2007, Proceedings of the 18th Eurographics conference on Rendering Techniques, P309
   Narayanan H., 2010, ADV NEURAL INFORM PR, V2, P1786
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Quan C, 2021, IEEE T MED IMAGING, V40, P3265, DOI 10.1109/TMI.2021.3081677
   Rifai S., 2011, Advances in Neural Information Processing Systems (NIPS), P2294
   Robert C.P., 2010, Monte Carlo Statistical Methods, V2, DOI 10.1007/978-1-4757-4145-2
   Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sharma A, 2016, 2016 INTERNATIONAL CONFERENCE ON EMERGING TRENDS IN ELECTRICAL ELECTRONICS & SUSTAINABLE ENERGY SYSTEMS (ICETEESES), P374, DOI 10.1109/ICETEESES.2016.7581412
   Shin DH, 2005, IEEE T CONSUM ELECTR, V51, P218, DOI 10.1109/TCE.2005.1405723
   Song Y, 2019, ADV NEUR IN, V32
   Stankovic RS, 2003, COMPUT ELECTR ENG, V29, P25, DOI 10.1016/S0045-7906(01)00011-8
   Suárez PL, 2017, IEEE COMPUT SOC CONF, P212, DOI 10.1109/CVPRW.2017.32
   Sutherland D., 2018, PMLR, P652
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142
   Vitoria P, 2020, IEEE WINT CONF APPL, P2434, DOI [10.1109/WACV45572.2020.9093389, 10.1109/wacv45572.2020.9093389]
   Vonesch C, 2007, IEEE T SIGNAL PROCES, V55, P4415, DOI 10.1109/TSP.2007.896255
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Yi-Chin Huang, 2005, 13th Annual ACM International Conference on Multimedia, P351, DOI 10.1145/1101149.1101223
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Zhang D., 2019, Fundamentals of Image Data Mining, Texts in Computer, P35, DOI [DOI 10.1007/978-3-030-17989-2_3, 10.1007/978-3-030-17989-23]
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao JJ, 2020, INT J COMPUT VISION, V128, P818, DOI 10.1007/s11263-019-01271-4
   Zhou JJ, 2020, IEEE SIGNAL PROC LET, V27, P2054, DOI 10.1109/LSP.2020.3037690
   Zhou YF, 2021, Arxiv, DOI arXiv:2105.04538
   Zhu CQ, 1998, INT J REMOTE SENS, V19, P3197, DOI 10.1080/014311698214262
NR 72
TC 2
Z9 2
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4547
EP 4562
DI 10.1109/TMM.2022.3177933
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Xiong, ZW
   Li, Y
   Tian, XM
   Zha, ZJ
AF Liu, Yajing
   Xiong, Zhiwei
   Li, Ya
   Tian, Xinmei
   Zha, Zheng-Jun
TI Domain Generalization Via Encoding and Resampling in a Unified Latent
   Space
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain generalization; variational encoding; adversarial examples
AB Domain generalization aims to generalize a network trained on multiple domains to unknown yet related domains. Operating under the assumption that invariant information generalizes well to unknown domains, previous work has aimed to minimize the discrepancies amongst distributions across given domains. However, without prior regularization of feature distributions, the network in practice overfits the invariant information in the given domains. Moreover, if there are insufficient samples in given domains, then domain generalizability is limited, as diverse domain variations are not captured. To address these two drawbacks, we propose to explicitly map features in known and unknown domains onto latent space in a fixed Gaussian mixture distribution by variational coding. As a result, features in different classes follow Gaussian distributions with different mean values. The predefined latent space narrows discrepancies between known and unknown domains and effectively separates samples into different classes. Moreover, we propose to perturb sample features with gradients from the distribution regularized loss. This perturbation generates samples beyond but near the latent space of prior distributions, which has a profound impact on domain variations. Experiments and visualizations demonstrate the effectiveness of our proposed method.
C1 [Liu, Yajing; Xiong, Zhiwei; Tian, Xinmei; Zha, Zheng-Jun] Univ Sci & Technol China, Hefei 230027, Peoples R China.
   [Li, Ya] iLYTEK Res, Hefei 230088, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xiong, ZW (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM lyj123@mail.ustc.edu.cn; zwxiong@ustc.edu.cn; yali8@iflytek.com;
   xinmei@ustc.edu.cn; zhazj@ustc.edu.cn
RI Yang, Yifan/JTV-1487-2023; Jiang, Yu/JEZ-9814-2023; jing,
   wang/KCZ-2144-2024; LIU, HUI/JPX-8014-2023; Zha,
   Zheng-Jun/AAF-8667-2020; cheng, chen/JHS-9462-2023; Liu,
   Jinyu/JYQ-6274-2024; huang, libo/JMB-4345-2023; SUN,
   YANLING/JTT-9082-2023; Wang, Chao/JHT-6081-2023; Wang,
   Minghao/JMD-0670-2023; WANG, YANG/JFA-8821-2023; wang,
   wenjuan/JGD-0428-2023; Yang, Tian/JFB-1008-2023; zhao,
   lin/JPK-8436-2023; wu, xiaokang/JUJ-4602-2023; FENG, X/JPL-4188-2023
OI wang, wenjuan/0000-0002-4220-8817; 
FU National Key R&D Program of China [2017YFA0700800]; National Natural
   Science Foundation of China [U19B2038]; University Synergy Innovation
   Program of Anhui Province [GXXT-2019-025]; Key Scientific Technological
   Innovation Research Project by Ministry of Education
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFA0700800, in part by the National Natural Science
   Foundation of China under Grant U19B2038, in part by the University
   Synergy Innovation Program of Anhui Province under Grant GXXT-2019-025,
   and in part by the Key Scientific Technological Innovation Research
   Project by Ministry of Education.
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 1997, Information theory and statistics
   Arjovsky M, 2020, Arxiv, DOI arXiv:1907.02893
   Baldi P., 2012, P ICML WORKSH UNS TR, P37
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Choi MJ, 2010, PROC CVPR IEEE, P129, DOI 10.1109/CVPR.2010.5540221
   Diederik Kingma P, 2014, PROC INT C LEARN REP
   Doersch C, 2021, Arxiv, DOI arXiv:1606.05908
   Everingham M., 2009, The PASCAL Visual Object Classes Challenge 2009 (VOC) Results
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ghifary M, 2017, IEEE T PATTERN ANAL, V39, P1414, DOI 10.1109/TPAMI.2016.2599532
   Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   Griffin G., 2007, CALTECH 256 OBJECT C
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ilse M, 2020, PR MACH LEARN RES, V121, P322
   Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965
   Klys J, 2018, ADV NEUR IN, V31
   Koh P.W., 2021, INT C MACHINE LEARNI, P5637
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Liu YJ, 2019, PROC CVPR IEEE, P7186, DOI 10.1109/CVPR.2019.00736
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long MS, 2017, PR MACH LEARN RES, V70
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Makhzani A, 2016, Arxiv, DOI [arXiv:1511.05644, DOI 10.48550/ARXIV.1511.05644]
   Muandet Krikamol, 2013, INT C MACH LEARN, P10
   Okamoto H, 2019, PROC INT C LEARN REP
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Qian SS, 2018, IEEE T MULTIMEDIA, V20, P2086, DOI 10.1109/TMM.2017.2785227
   Raina R., 2007, SELF TAUGHT LEARNING
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sagawa S, 2020, Arxiv, DOI arXiv:1911.08731
   Shankar S., 2018, ARXIV180410745
   Sohn K., 2015, Neural Information Processing Systems, P3483
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Szegedy C., 2014, P ICLR
   Volpi R, 2018, ADV NEUR IN, V31
   Xu Z, 2014, LECT NOTES COMPUT SC, V8691, P628, DOI 10.1007/978-3-319-10578-9_41
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Zheng K, 2021, PROC AAAI C ARTIF IN, P3538
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
NR 49
TC 8
Z9 8
U1 4
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 126
EP 139
DI 10.1109/TMM.2021.3121564
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400009
DA 2024-07-18
ER

PT J
AU Ma, JY
   Wang, Y
   Fan, AX
   Xiao, GB
   Chen, RQ
AF Ma, Jiayi
   Wang, Yang
   Fan, Aoxiang
   Xiao, Guobao
   Chen, Riqing
TI Correspondence Attention Transformer: A Context-Sensitive Network for
   Two-View Correspondence Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Feature extraction; Transformers; Memory management;
   Information exchange; Geometry; Cameras; Feature matching; deep
   learning; context-sensitive; self-attention
ID IMAGE; REGISTRATION
AB Seeking reliable correspondences then recovering camera poses from a set of putative correspondences extracted from two images of the same scene is a fundamental problem in computer vision. Recent advances have demonstrated that this problem can be effectively solved by using a deep architecture based on the multi-layer perceptron, where the context normalization is designed to make the network permutation-equivariant and embed global information in the sparse point data. However, the context normalization simply normalizes the feature maps according to their distribution and treats each correspondence equally, leading to difficulties in adequately capturing scene geometry encoded by the inliers, especially in case of severe outliers. To address this issue, this paper designs a context-sensitive network based on the self-attention mechanism, termed as correspondence attention transformer (CAT), to enhance the consistent geometry information of inliers and simultaneously suppress outliers during embedding global information. In particular, we design an attentionstyle structure to aggregate features from all correspondences, i.e., a spatial attention namely CAT-S, which provides each correspondence with information exchange from others in the putative set. To capture the contextual information in a more comprehensive and robust way, we also introduce a multi-head mechanism in our structure to exploit the geometrical context from different aspects. Moreover, considering the high memory request in spatial attention, we propose a covariance normalized channel attention CAT-C in our framework, which can largely reduce the memory consumption and parameter scale, but it asks for eigenvalue decomposition in each attention block thus resulting in more runtime. Anyway, these two attention mechanisms can realize information exchange from the spatial or channel aspect, which both contribute to constructing the geometrical context between inliers and encourage the network to pay more attention to the feature subset about potential inliers. Extensive experiments have been conducted over both indoor and outdoor datasets on the tasks of camera pose estimation, outlier removal, and image registration, which demonstrate the superiority of our method that realizes a large performance improvement compared with the current state-of-the-art approaches.
C1 [Ma, Jiayi; Wang, Yang; Fan, Aoxiang] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
   [Xiao, Guobao] Minjiang Univ, Coll Comp & Control Engn, Fujian Prov Key Lab Informat Proc & Intelligent C, Fuzhou 350108, Peoples R China.
   [Chen, Riqing] Fujian Agr & Forestry Univ, Coll Comp & Informat Sci, Fuzhou 350002, Peoples R China.
C3 Wuhan University; Minjiang University; Fujian Agriculture & Forestry
   University
RP Chen, RQ (corresponding author), Fujian Agr & Forestry Univ, Coll Comp & Informat Sci, Fuzhou 350002, Peoples R China.
EM jyma2010@gmail.com; wangyangwhu@gmail.com; fanaoxiang@whu.edu.cn;
   gbx@mju.edu.cn; riqing.chen@fafu.edu.cn
RI Sun, Zhichao/KIC-2765-2024; Ma, Jiayi/Y-2470-2019
OI Ma, Jiayi/0000-0003-3264-3265; Ma, Jinlong/0000-0002-4288-6327; Zhang,
   Yongqiang/0000-0001-7748-7548; Xiao, Guobao/0000-0003-2928-8100; Ma,
   Jinlong/0000-0003-0229-0788
FU National Natural Science Foundation of China [62072223, 61972093]; Key
   Research and Development Program of Hubei Province [2020BAB113]; Natural
   Science Fund of Hubei Province [2019CFA037]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072223 and 61972093, in part by the
   Key Research and Development Program of Hubei Province under Grant
   2020BAB113, and in part by the Natural Science Fund of Hubei Province
   under Grant 2019CFA037.
CR Barath D, 2019, PROC CVPR IEEE, P10189, DOI 10.1109/CVPR.2019.01044
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Chen YP, 2018, ADV NEUR IN, V31
   Chum O, 2005, PROC CVPR IEEE, P772
   Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221
   Cour T., 2007, Advances in Neural Information Processing Systems, V19, P313
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   González-Díaz I, 2017, IEEE T MULTIMEDIA, V19, P544, DOI 10.1109/TMM.2016.2616298
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339
   Jiang B, 2020, IEEE T MULTIMEDIA, V22, P2074, DOI 10.1109/TMM.2019.2951466
   Jiang XY, 2021, INFORM FUSION, V73, P22, DOI 10.1016/j.inffus.2021.02.012
   Jiang XY, 2021, IEEE T GEOSCI REMOTE, V59, P1577, DOI 10.1109/TGRS.2020.3001089
   Jiang XY, 2019, IEEE T GEOSCI REMOTE, V57, P6462, DOI 10.1109/TGRS.2019.2906183
   Jiang XY, 2020, IEEE T IMAGE PROCESS, V29, P736, DOI 10.1109/TIP.2019.2934572
   Kingma D. P., 2014, arXiv
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228
   Li XR, 2010, INT J COMPUT VISION, V89, P1, DOI 10.1007/s11263-010-0318-x
   LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Ma JY, 2019, PATTERN RECOGN, V92, P231, DOI 10.1016/j.patcog.2019.04.001
   Ma JY, 2019, IEEE T IMAGE PROCESS, V28, P4045, DOI 10.1109/TIP.2019.2906490
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JY, 2016, IEEE T IMAGE PROCESS, V25, P53, DOI 10.1109/TIP.2015.2467217
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Mukherjee D, 2015, MACH VISION APPL, V26, P443, DOI 10.1007/s00138-015-0679-9
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Ono Y., 2018, ADV NEURAL INFORM PR, V31, P6234
   Pachauri D., 2013, Advances in Neural Information Processing Systems, V26, P1860
   Plötz T, 2018, ADV NEUR IN, V31
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Ranftl R, 2018, LECT NOTES COMPUT SC, V11205, P292, DOI 10.1007/978-3-030-01246-5_18
   Rousseeuw P. J., 2005, Robust Regression and Outlier Detection
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Schönberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Sun K, 2020, IEEE T MULTIMEDIA, V22, P2246, DOI 10.1109/TMM.2019.2957984
   Sun W., 2020, P IEEE CVF C COMP VI, P11286
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Ulyanov Dmitry, 2016, arXiv
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165
   Wahba G., 1990, SPLINE MODELS OBSERV
   Wang QL, 2021, IEEE T PATTERN ANAL, V43, P2582, DOI 10.1109/TPAMI.2020.2974833
   Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458
   Yan JC, 2018, IEEE T CYBERNETICS, V48, P765, DOI 10.1109/TCYB.2017.2655538
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zhang JH, 2019, IEEE I CONF COMP VIS, P5844, DOI 10.1109/ICCV.2019.00594
   Zhao C, 2019, PROC CVPR IEEE, P215, DOI 10.1109/CVPR.2019.00030
NR 67
TC 31
Z9 31
U1 11
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3509
EP 3524
DI 10.1109/TMM.2022.3162115
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500002
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Qian, TW
   Chen, JJ
   Chen, SX
   Wu, B
   Jiang, YG
AF Qian, Tianwen
   Chen, Jingjing
   Chen, Shaoxiang
   Wu, Bo
   Jiang, Yu-Gang
TI Scene Graph Refinement Network for Visual Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Cognition; Transformers; Feature
   extraction; Semantics; Noise measurement; Visual Question Answering;
   Scene Graph; Transformers; Cross-modal Learning
ID LANGUAGE; VISION
AB Visual Question Answering aims to answer the free-form natural language question based on the visual clues in a given image. It is a difficult problem as it requires understanding the fine-grained structured information of both language and image for compositional reasoning. To establish the compositional reasoning, recent works attempt to introduce the scene graph in VQA. However, as the generated scene graphs are usually quite noisy, it greatly limits the performance of question answering. Therefore, this paper proposes to refine the scene graphs for improving the effectiveness. Specifically, we present a novel Scene Graph Refinement network (SGR), which introduces a transformer-based refinement network to enhance the object and relation features for better classification. Moreover, as the question provides valuable clues for distinguishing whether the < subject, predicate, object > triplets are helpful or not, the SGR network exploits the semantic information presented in the questions to select the most relevant relations for question answering. Extensive experiments are conducted on the GQA benchmark demonstrate the effectiveness of our method.
C1 [Qian, Tianwen; Chen, Jingjing; Chen, Shaoxiang; Jiang, Yu-Gang] Fudan Univ, Shanghai 200437, Peoples R China.
   [Wu, Bo] MIT IBM Watson AI Lab, Cambridge, MA 02141 USA.
C3 Fudan University
RP Chen, JJ (corresponding author), Fudan Univ, Shanghai 200437, Peoples R China.
EM twqian19@fudan.edu.cn; chenjingjing@fudan.edu.cn; sxchen13@fudan.edu.cn;
   bobbywu.cs@gmail.com; ygj@fudan.edu.cn
RI WANG, YILUN/KFB-0627-2024; chen, huan/KEC-2019-2024
OI Qian, Tianwen/0000-0002-3881-4857
FU NSFC [62072116]; Shanghai Science and Technology Program Project
   [21JC1400600]; Shanghai Education Development Foundation and Shanghai
   Municipal Education Commission
FX This work was supported in part by NSFC Project under Grant 62072116 and
   in part by Shanghai Science and Technology Program Project under Grant
   21JC1400600. The work of Y.-G. Jiang was sponsored in part by "Shuguang
   Program" supported by Shanghai Education Development Foundation and
   Shanghai Municipal Education Commission.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   *BEMY, BE MY EYES
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Chen JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1020, DOI 10.1145/3240508.3240627
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen S., 2020, P IEEE CVF C COMP VI, P10638
   Chen WH, 2021, IEEE WINT CONF APPL, P655, DOI 10.1109/WACV48630.2021.00070
   Chhabra A, 2020, INT CONF SOFT COMP, P121, DOI [10.1109/iscmi51676.2020.9311591, 10.1109/ISCMI51676.2020.9311591]
   DAMODARAN V, 2021, UNDERSTANDING ROLE S
   DEVLIN J, 2019, NAACL, P4272
   DONG X, 2022, STACKED HYBRIDATTENT
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5257, DOI 10.1145/3474085.3475643
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4
   Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Hudson D., 2019, P ADV NEUR INF PROC, P5903
   Hudson Drew A, 2019, P IEEE CVF C COMP VI, P6700, DOI DOI 10.1109/CVPR.2019.00686
   Hudson Drew A., 2018, Compositional attention networks for machine reasoning"
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kim Eun-Sol, 2020, P IEEE CVF C COMP VI, P14581
   Kim JH, 2018, ADV NEUR IN, V31
   KONER R, 2021, GRAPHHOPPER MULTIHOP
   Krishna R, 2018, PROC CVPR IEEE, P6867, DOI 10.1109/CVPR.2018.00718
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li Gen, 2020, AAAI, P11336
   LI LH, 2019, VISUALBERT A SIMPLE
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   LI X, 2021, PATTERN RECOGNIT, V124
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   LIU Z, 2021, IEEE T KNOWL DATA EN, P1
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2016, ADV NEUR IN, V29
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qi MS, 2019, PROC CVPR IEEE, P3952, DOI 10.1109/CVPR.2019.00408
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857
   Song Xue, 2021, IEEE T MULTIMEDIA
   SU W, 2019, VLBERT PRETRAINING O
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Velickovic Petar, 2017, ARXIV171010903, DOI DOI 10.48550/ARXIV.1710.10903
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang Z, 2021, IEEE T MULTIMEDIA, V23, P4027, DOI 10.1109/TMM.2020.3037461
   WU B, 2021, P 35TH C NEURAL INF
   Wu CF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P519, DOI 10.1145/3240508.3240513
   Wu Y., 2016, GOOGLES NEURAL MACHI
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yang CF, 2016, IEEE IC COMP COM NET
   Yang X., 2021, P IEEE INT C COMPUTE, P2197
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yi KX, 2018, ADV NEUR IN, V31
   Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P3208
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yutian Guo, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P9, DOI 10.1145/3372278.3390709
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555
   Zhang JJ, 2022, MULTIMEDIA SYST, V28, P45, DOI 10.1007/s00530-021-00802-9
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhou YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1245, DOI 10.1145/3394171.3413998
   Zhuang Y, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3283
NR 73
TC 13
Z9 13
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3950
EP 3961
DI 10.1109/TMM.2022.3169065
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500031
DA 2024-07-18
ER

PT J
AU Qiao, T
   Wu, JS
   Zheng, N
   Xu, M
   Luo, XY
AF Qiao, Tong
   Wu, Jiasheng
   Zheng, Ning
   Xu, Ming
   Luo, Xiangyang
TI FGDNet: Fine-Grained Detection Network Towards Face Anti-Spoofing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Face recognition; Faces; Transformers; Detectors;
   Convolution; Convolutional neural networks; Data augmentation; face
   anti-spoofing; self-attention; transformer style network
ID ATTACK
AB With the development of facial recognition technology, face anti-spoofing as the most important security module of face recognition system becomes more and more important. As a matter of fact, face anti-spoofing is still a challenging task, especially facing various attacks simultaneously. Moreover, most of current detectors mainly focus on binary classification while usually fail to complete the task of fine-grained multiple classification, referring to as replay, print, partial mask, and full mask attacks. To fill the gap, in this context, it is proposed to design the fine-grained detection network for classifying various face spoofing attack modes. First, we propose to establish a Transformer style network structure for feature extraction, where the convolution mapping operation is adopted instead of traditional linear mapping. Specifically, we adopt the self-attention module for extracting long distance feature, and convolution mapping is used to maintain the model's ability to extract local features. Finally, the simple yet effective linear classifier is introduced for fine-grained classification. Moreover, with the help of the VGG based style-transfer network, the well-designed scheme of data augmentation module is proposed for solving the problem of insufficient training samples. In the large-scale experiments, compared with the baseline detectors, our proposed fine-grained classifier with low computation cost performs its superiority for multiple classification.
C1 [Qiao, Tong; Wu, Jiasheng; Zheng, Ning; Xu, Ming] Hangzhou Dianzi Univ, Hangzhou 310005, Peoples R China.
   [Qiao, Tong] Henan Key Lab Cyberspace Situat Awareness, Zhengzhou 450001, Peoples R China.
   [Luo, Xiangyang] Zhengzhou Sci & Technol Inst, State Key Lab Math Engn & Adv Comp, Zhengzhou 450001, Peoples R China.
C3 Hangzhou Dianzi University; PLA Information Engineering University
RP Luo, XY (corresponding author), Zhengzhou Sci & Technol Inst, State Key Lab Math Engn & Adv Comp, Zhengzhou 450001, Peoples R China.
EM tong.qiao@hdu.edu.cn; 202270037@hdu.edu.cn; nzheng@hdu.edu.cn;
   mxu@hdu.edu.cn; xiangyangluo@126.com
OI Xu, Ming/0000-0001-9332-5258; Qiao, Tong/0000-0003-4912-2132
FU Open Foundation of Henan Key Laboratory of Cyberspace Situation
   Awareness [HNTS2022016]; Fundamental Research Funds for the Provincial
   Universities of Zhejiang [GK219909299001-007]; Open Projects Program of
   National Laboratory of Pattern Recognition; National Key R&D Program of
   China [2022YFB3102900]; National Natural Science Foundation of China
   [U1804263, 62172435]; Zhongyuan Science and Technology Innovation
   Leading Talent Project of China [214200510019]
FX This work was supported by in part by the Open Foundation of Henan Key
   Laboratory of Cyberspace Situation Awareness under Grant HNTS2022016, in
   part by the Fundamental Research Funds for the Provincial Universities
   of Zhejiang under Grant GK219909299001-007, in part by the Open Projects
   Program of National Laboratory of Pattern Recognition, in part by the
   National Key R&D Program of China under Grant 2022YFB3102900, in part by
   the National Natural Science Foundation of China under Grants U1804263
   and 62172435, and in part by the Zhongyuan Science and Technology
   Innovation Leading Talent Project of China under Grant 214200510019. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. Xiaochun Cao.
CR Alhejazi MM, 2022, INF SECUR J, V31, P125, DOI 10.1080/19393555.2020.1869356
   Bao W, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND SIGNAL PROCESSING, P233
   Boulkenafet Z, 2017, IEEE SIGNAL PROC LET, V24, P141, DOI 10.1109/LSP.2016.2630740
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Boulkenafet Z, 2017, IEEE INT CONF AUTOMA, P612, DOI 10.1109/FG.2017.77
   Chen ZH, 2021, AAAI CONF ARTIF INTE, V35, P1132
   Chingovska I., 2012, P BIOSIG P INT C BIO, P1
   Deb D, 2021, IEEE T INF FOREN SEC, V16, P1143, DOI 10.1109/TIFS.2020.3029879
   Ding F, 2021, IEEE T MULTIMEDIA, V24, P3429, DOI 10.1109/TMM.2021.3098422
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du YT, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/6652727
   George A, 2021, PROC CVPR IEEE, P7878, DOI 10.1109/CVPR46437.2021.00779
   George A, 2020, IEEE T INF FOREN SEC, V15, P42, DOI 10.1109/TIFS.2019.2916652
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kollreider K, 2007, IEEE T INF FOREN SEC, V2, P548, DOI 10.1109/TIFS.2007.902037
   Komkov S, 2021, INT C PATT RECOG, P819, DOI 10.1109/ICPR48806.2021.9412236
   Komulainen J, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS)
   Li JZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3891, DOI 10.1145/3474085.3475367
   Li L, 2016, INT CONF IMAG PROC
   Li M, 2019, Arxiv, DOI arXiv:1904.11617
   Li Z, 2023, IEEE T MULTIMEDIA, V25, P62, DOI 10.1109/TMM.2021.3121140
   Liu AJ, 2022, IEEE T INF FOREN SEC, V17, P2497, DOI 10.1109/TIFS.2022.3188149
   Liu AJ, 2021, IEEE INT CONF COMP V, P814, DOI 10.1109/ICCVW54120.2021.00096
   Liu AJ, 2021, IEEE WINT CONF APPL, P1178, DOI 10.1109/WACV48630.2021.00122
   Liu AJ, 2021, IEEE T INF FOREN SEC, V16, P2759, DOI 10.1109/TIFS.2021.3065495
   Liu SQ, 2016, IEEE COMPUT SOC CONF, P1551, DOI 10.1109/CVPRW.2016.193
   Liu Y., 2020, EUR C COMP VIS, P406, DOI DOI 10.1007/978-3-030-58523
   Liu YJ, 2019, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR.2019.00481
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Maatta J, 2011, INT JOINT C BIOM IJC, P1, DOI DOI 10.1109/IJCB.2011.6117510
   Nosaka R, 2011, LECT NOTES COMPUT SC, V7088, P82, DOI 10.1007/978-3-642-25346-1_8
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Patel K, 2016, IEEE T INF FOREN SEC, V11, P2268, DOI 10.1109/TIFS.2016.2578288
   Pereira TD, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shao R, 2020, AAAI CONF ARTIF INTE, V34, P11974
   Shen M, 2021, IEEE T INF FOREN SEC, V16, P4063, DOI 10.1109/TIFS.2021.3102492
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tu XG, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3402446
   Wang JJ, 2021, AAAI CONF ARTIF INTE, V35, P2746
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Yin BJ, 2021, Arxiv, DOI arXiv:2105.03162
   Yu ZT, 2023, Arxiv, DOI arXiv:2202.08192
   Yu ZT, 2020, PROC CVPR IEEE, P5294, DOI 10.1109/CVPR42600.2020.00534
   Yuen P. C., 2019, US Patent, Patent No. [10,380,444, 10380444]
   Zhang L, 2023, IEEE T MULTIMEDIA, V25, P4785, DOI 10.1109/TMM.2022.3182509
   Zhao XC, 2018, IEEE T MULTIMEDIA, V20, P552, DOI 10.1109/TMM.2017.2750415
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zitong Yu, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P285, DOI 10.1109/TBIOM.2021.3065526
NR 52
TC 1
Z9 1
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7350
EP 7363
DI 10.1109/TMM.2022.3221532
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000047
DA 2024-07-18
ER

PT J
AU Sepehri, Y
   Pad, P
   Kundig, C
   Frossard, P
   Dunbar, LA
AF Sepehri, Yamin
   Pad, Pedram
   Kundig, Clement
   Frossard, Pascal
   Dunbar, L. Andrea
TI Privacy-Preserving Image Acquisition for Neural Vision Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optical sensors; Optical filters; Optical imaging; Optical computing;
   Task analysis; Optical refraction; Optical fiber networks;
   Privacy-preserving vision; neural network; trainable optical
   convolution; deep learning
AB Preserving privacy is a growing concern in our society where cameras are ubiquitous. In this work, we propose a trainable image acquisition method that removes the sensitive information in the optical domain before it reaches the image sensor. The method benefits from a trainable optical convolution kernel, which transmits the desired information whilst filtering out the sensitive information, making it irretrievable against different privacy attacks in the digital domain. This is in contrast with the current digital privacy-preserving methods that are all vulnerable to direct access attacks. Also, in contrast with most of the previous optical privacy-preserving methods that cannot be trained, our method is data-driven and optimized for the specific application at hand. Moreover, there is no additional computation or power burden on the acquisition system since it works passively in the optical domain and can be even used in conjunction with other privacy-preserving techniques in the digital domain. We demonstrate our new, generic method in several scenarios such as smile or open-mouth detection as the desired attribute while the gender or wearing make-up is filtered out as the sensitive content. Through several experiments, we show that this method is able to reduce around 65% of sensitive content while causing a negligible reduction in the desired information. Moreover, we tested our method by deep reconstruction attack and confirmed the ineffectiveness of this attack to reconstruct the original sensitive content. This new method has different use cases such as feedback systems for smart TV content or outdoor advertising.
C1 [Sepehri, Yamin; Pad, Pedram; Kundig, Clement; Dunbar, L. Andrea] CSEM, CH-2002 Neuchatel, Switzerland.
   [Sepehri, Yamin; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
C3 Swiss Center for Electronics & Microtechnology (CSEM); Swiss Federal
   Institutes of Technology Domain; Ecole Polytechnique Federale de
   Lausanne
RP Sepehri, Y (corresponding author), CSEM, CH-2002 Neuchatel, Switzerland.
EM yamin.sepehri@csem.ch; pedram.pad@csem.ch; clement.kuendig@csem.ch;
   pascal.frossard@epfl.ch; andrea.dunbar@csem.ch
OI Sepehri, Yamin/0000-0003-1372-0368; Frossard, Pascal/0000-0002-4010-714X
CR Ambs P, 2007, AIP CONF PROC, V949, P226, DOI 10.1063/1.2812301
   Arai H., 2012, NTT Tech. Rev., V10, P1
   Chang J, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30619-y
   Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hinojosa C, 2022, Arxiv, DOI arXiv:2206.03891
   Hinojosa C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2553, DOI 10.1109/ICCV48922.2021.00257
   Huang C., 2018, arXiv
   Huang C, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19120656
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Lin X, 2018, SCIENCE, V361, P1004, DOI 10.1126/science.aat8084
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mireshghallah F, 2020, Arxiv, DOI arXiv:2004.12254
   Nakashima S, 2010, PROCD SOC BEHV, V2, P213, DOI 10.1016/j.sbspro.2010.01.038
   Orabona E, 2013, MICROMACHINES-BASEL, V4, P206, DOI 10.3390/mi4020206
   Osia SA, 2020, IEEE T KNOWL DATA EN, V32, P54, DOI 10.1109/TKDE.2018.2878698
   Pad Pedram, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12282, DOI 10.1109/CVPR42600.2020.01230
   Liew SP, 2020, Arxiv, DOI arXiv:2010.14023
   Perrone D, 2016, IEEE T PATTERN ANAL, V38, P1041, DOI 10.1109/TPAMI.2015.2477819
   Pittaluga F, 2017, IEEE T PATTERN ANAL, V39, P2215, DOI 10.1109/TPAMI.2016.2637354
   Saleh B.E. A., 1991, RAY OPTICS, P1, DOI DOI 10.1002/0471213748.CH1
   Salem A, 2020, PROCEEDINGS OF THE 29TH USENIX SECURITY SYMPOSIUM, P1291
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi WX, 2022, LIGHT-SCI APPL, V11, DOI 10.1038/s41377-022-00809-5
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh M, 2010, ADV MATER, V22, P673, DOI 10.1002/adma.200901141
   Song MK, 2020, IEEE J SEL AREA COMM, V38, P2430, DOI 10.1109/JSAC.2020.3000372
   Vizitiu A, 2020, COMPUT MATH METHOD M, V2020, DOI 10.1155/2020/3910250
   Wang Z, 2021, INT J HUM RESOUR MAN, V32, P2264, DOI [10.1080/09585192.2019.1579254, 10.1109/CVPRW.2019.00007]
   Whitehill J, 2012, PROC CVPR IEEE, P2488, DOI 10.1109/CVPR.2012.6247964
   Wilkinson NJ, 2019, INT J ADV MANUF TECH, V105, P4599, DOI 10.1007/s00170-019-03438-2
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
NR 34
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6232
EP 6244
DI 10.1109/TMM.2022.3207018
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500042
DA 2024-07-18
ER

PT J
AU Shao, Z
   Han, JG
   Debattista, K
   Pang, YW
AF Shao, Zhuang
   Han, Jungong
   Debattista, Kurt
   Pang, Yanwei
TI Textual Context-Aware Dense Captioning With Diverse Words
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dense Captioning; Enhanced Transformer Dense Captioner; Textual Context
   Module; Dynamic Vocabulary Frequency Histogram
ID NETWORKS
AB Dense captioning generates more detailed spoken descriptions for complex visual scenes. Despite several promising leads, existing methods still have two broad limitations: 1) The vast majority of prior arts only consider visual contextual clues during captioning but ignore potentially important textual context; 2) current imbalanced learning mechanisms limit the diversity of vocabulary learned from the dictionary, thus giving rise to low language-learning efficiency. To alleviate these gaps, in this paper, we propose an end-to-end enhanced dense captioning architecture, namely Enhanced Transformer Dense Captioner (ETDC), which obtains textual context from surrounding regions and dynamically diversifies the vocabulary bank during captioning. Concretely, we first propose the Textual Context Module (TCM), which is integrated into each self-attention layer of the Transformer decoder, to capture the surrounding textual context. Moreover, we take full advantage of the class information of object context and propose a Dynamic Vocabulary Frequency Histogram (DVFH) re-sampling strategy during training to balance words with different frequencies. The proposed method is tested on the standard dense captioning datasets and surpasses the state-of-the-art methods in terms of mean Average Precision (mAP).
C1 [Shao, Zhuang; Debattista, Kurt] Univ Warwick, Warwick Mfg Grp, Coventry CV4 7AL, England.
   [Han, Jungong] Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, England.
   [Pang, Yanwei] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Pang, Yanwei] Shanghai Artificial Intelligence Lab, Shanghai 200032, Peoples R China.
C3 University of Warwick; University of Sheffield; Tianjin University
RP Han, JG (corresponding author), Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, England.
EM zhuang.shao@warwick.ac.uk; jungonghan77@gmail.com;
   k.debattista@warwick.ac.uk; pyw@tju.edu.cn
RI Shao, Zhuang/GYU-2414-2022
FU National Key Research and Development Program of China
FX No Statement Available
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Cao JL, 2022, PROC CVPR IEEE, P9448, DOI 10.1109/CVPR52688.2022.00924
   Cao JL, 2023, IEEE T NEUR NET LEAR, V34, P2425, DOI 10.1109/TNNLS.2021.3106641
   Cao JL, 2020, IEEE T CIRC SYST VID, V30, P3372, DOI 10.1109/TCSVT.2019.2950526
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Gao MQ, 2023, ARTIF INTELL REV, V56, P457, DOI 10.1007/s10462-022-10176-7
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lavie A., 2007, P 2 WORKSH STAT MACH, P228
   Lei Ba J., 2016, arXiv
   Li XY, 2019, AAAI CONF ARTIF INTE, P8650
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AA, 2021, IEEE T MULTIMEDIA, V23, P4515, DOI 10.1109/TMM.2020.3043084
   Liu AA, 2019, MULTIMED TOOLS APPL, V78, P677, DOI 10.1007/s11042-017-5532-x
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu Y, 2023, NEUROCOMPUTING, V517, P213, DOI 10.1016/j.neucom.2022.09.048
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Mahajan S., 2020, P ADV NEUR INF PROC, V33, P3613
   Miyaguchi K., 2019, P 22 INT C ART INT S, P3440
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruder S, 2017, Arxiv, DOI arXiv:1609.04747
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shao Zhuang, 2022, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2022.3152990
   Sharma Himanshu, 2020, 2020 International Conference on Power Electronics & IoT Applications in Renewable Energy and its Control (PARC), P325, DOI 10.1109/PARC49193.2020.236619
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J N., 2020, EUROPEAN C COMPUTER, P370
   Wang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6827, DOI 10.1109/ICCV48922.2021.00677
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yang LJ, 2017, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR.2017.214
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yin GJ, 2019, PROC CVPR IEEE, P6234, DOI 10.1109/CVPR.2019.00640
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
NR 45
TC 16
Z9 16
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8753
EP 8766
DI 10.1109/TMM.2023.3241517
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000015
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Shen, XJ
   Cai, YA
   Abhadiomhen, SE
   Liu, ZF
   Zhan, YZ
   Fan, JP
AF Shen, Xiang-Jun
   Cai, Yanan
   Abhadiomhen, Stanley Ebhohimhen
   Liu, Zhifeng
   Zhan, Yong-Zhao
   Fan, Jianping
TI Deep Robust Low Rank Correlation With Unifying Clustering Structure for
   Cross Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Adaptation models; Data models; Task analysis; Noise
   measurement; Feature extraction; Analytical models; Cross domain
   adaptation; low rank; correlation analysis; unifying clustering
   structure; deep network
AB Cross domain adaptation aims to improve the performance of the target domain model by making full use of information rich source domain samples. However, as information becomes richer, the noise also increases. In order to improve the reliability of cross domain adaptation, we propose a novel method based on deep robust low rank correlation. Borrowed from the traditional idea of Canonical Correlation Analysis (CCA), we developed a robust correlation model to maximize the correlation between source and target domains. Also, the low-rank characteristics of cross domain data can effectively reduce the negative influence of noisy data. Furthermore, in order that the cross-domain data can share a unifying clustering structure, we introduced a common Laplacian affinity structure. Then the learned features can be smoothed and aligned to the unifying structure. In this way, we obtain a deep robust low rank correlation model with the help of the unifying clustering structure, which can effectively reduce the influence of noise and improve the performance of cross domain adaptation. Experimental results on three datasets including Office-31, ImageCLEF-DA and Office-Home show that our model significantly outperforms state-of-the-art cross domain adaptation methods.
C1 [Shen, Xiang-Jun; Cai, Yanan; Abhadiomhen, Stanley Ebhohimhen; Liu, Zhifeng; Zhan, Yong-Zhao] JiangSu Univ, Sch Comp Sci & Commun Engn, Jiangsu 212013, Peoples R China.
   [Abhadiomhen, Stanley Ebhohimhen] Univ Nigeria, Dept Comp Sci, Nsukka 410001, Nigeria.
   [Fan, Jianping] Lenovo Res, AI Lab, Charlotte, NC USA.
C3 Jiangsu University; University of Nigeria
RP Liu, ZF (corresponding author), JiangSu Univ, Sch Comp Sci & Commun Engn, Jiangsu 212013, Peoples R China.
EM xjshen@ujs.edu.cn; lovlic@163.com; stanley.abhadiomhen@unn.edu.ng;
   liuzf@ujs.edu.cn; yzzhan@ujs.edu.cn; jfan1@lenovo.com
RI Abhadiomhen, Stanley Ebhohimhen/AAH-5788-2021
OI Abhadiomhen, Stanley Ebhohimhen/0000-0002-9509-1915; Fan,
   Jianping/0000-0003-2290-1785; Cai, Yanan/0000-0002-0856-4404; shen,
   xiangjun/0000-0002-3359-8972
CR Andrew G., 2013, ICML, P1247
   Blaschko M. B., 2008, PROC IEEE C COMPUT V, P1
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai D, 2015, IEEE T CYBERNETICS, V45, P1669, DOI 10.1109/TCYB.2014.2358564
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chapelle O., 2005, Proceedings of the tenth international workshop on artificial intelligence and statistics, P57
   Chaudhuri K., 2009, P 26 ANN INT C MACH, P129
   Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072
   Chen Jianhui, 2011, P 17 ACM SIGKDD INT, P42
   Dai ZH, 2017, 31 ANN C NEURAL INFO, V30
   Gao QX, 2020, AAAI CONF ARTIF INTE, V34, P3938
   Garbin Stephan J., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P220, DOI 10.1007/978-3-030-58604-1_14
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Hong CQ, 2016, INT CONF DAT MIN WOR, P446, DOI [10.1109/ICDMW.2016.0070, 10.1109/ICDMW.2016.26]
   Hoyer L, 2022, LECT NOTES COMPUT SC, V13690, P372, DOI 10.1007/978-3-031-20056-4_22
   Hui Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8722, DOI 10.1109/CVPR42600.2020.00875
   Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924
   Ji P, 2017, ADV NEUR IN, V30
   Jin C, 2015, AAAI CONF ARTIF INTE, P151
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kingma D. P., 2014, arXiv
   Kumar A, 2018, ADV NEUR IN, V31
   Li GR, 2021, PROC CVPR IEEE, P9752, DOI 10.1109/CVPR46437.2021.00963
   Li JC, 2021, PROC CVPR IEEE, P2505, DOI 10.1109/CVPR46437.2021.00253
   Li J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2145
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Liu G., 2010, P INT C MACH LEARN, P663
   Makari F, 2015, KNOWL INF SYST, V42, P493, DOI 10.1007/s10115-013-0718-7
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Njima W, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9122414
   Prabhu V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8485, DOI 10.1109/ICCV48922.2021.00839
   Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2017, PR MACH LEARN RES, V70
   Shi Q, 2015, IEEE T GEOSCI REMOTE, V53, P5677, DOI 10.1109/TGRS.2015.2427791
   Sun QS, 2005, PATTERN RECOGN, V38, P2437, DOI 10.1016/j.patcog.2004.12.013
   Tang H, 2022, IEEE T PATTERN ANAL, V44, P6517, DOI 10.1109/TPAMI.2021.3087830
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang H, 2019, KNOWL-BASED SYST, V163, P1009, DOI 10.1016/j.knosys.2018.10.022
   Wang JY, 2022, IEEE T NEUR NET LEAR, V33, P6844, DOI 10.1109/TNNLS.2021.3083695
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Xu Y, 2016, IEEE T IMAGE PROCESS, V25, P850, DOI 10.1109/TIP.2015.2510498
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Zhou ZH, 2005, IEEE T KNOWL DATA EN, V17, P1529, DOI 10.1109/TKDE.2005.186
NR 47
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8334
EP 8345
DI 10.1109/TMM.2023.3235526
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000054
DA 2024-07-18
ER

PT J
AU Song, JR
   Mao, XH
   Yang, FZ
AF Song, Jiarun
   Mao, Xionghui
   Yang, Fuzheng
TI The Impact of Black Edge Artifact on QoE of the FOV-Based Cloud VR
   Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud computing; Games; Quality of experience; Rendering (computer
   graphics); Servers; Delays; Streaming media; Black edge; cloud VR;
   latency; user experience; virtual reality
ID VIDEO GAMES; QUALITY
AB Cloud virtual reality (Cloud VR) services usually introduce high latency in rendering and streaming, resulting in a mismatch between the visual and vestibular systems, causing user sickness and dizziness during the service. To solve this problem, asynchronous rendering technology is usually used to provide smooth viewing. The asynchronous solution, on the other hand, will introduce another "black edge" (BE) artifact in the service, which frequently appears at the viewport's boundary with a black area when users turn their heads. This unwanted BE artifact also has an impact on the user's quality of experience (QoE). In this paper, we investigated the impact of the BE artifact on the user's QoE of the field of view (FOV) Cloud VR gaming services. The appearance of the BE artifact during the playing period was regarded as a series of BE events and the impact of BE artifact on the users' QoE was evaluated by accumulating the influence of the BE events during the whole playing period. More specifically, the user's QoE affected by a single BE event was first evaluated by combining the area ratio and duration of the BE artifact. Then, the QoE affected by multiple BE events was analyzed, where the cumulative influence of the previous BE events on the user's current QoE was evaluated. Finally, a unified event-based evaluation model was proposed to predict the user's time-varying QoE at any point in time. Experimental results showed that the proposed model performed exceptionally well in predicting the impact of BE artifact on the user's QoE.
C1 [Song, Jiarun; Mao, Xionghui] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Yang, Fuzheng] Xidian Univ, State Key Lab ISN, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University
RP Song, JR (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM jrsong@xidian.edu.cn; xhmao_98@stu.xidian.edu.cn;
   fzhyang@mail.xidian.edu.cn
OI Song, Jiarun/0000-0001-6718-4201
FU National Natural Science Foundation of China
FX No Statement Available
CR Alshahrani A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10165515
   ALVR, 2020, ABOUT US
   Amin Rahul, 2013, Human-Computer Interaction. Users and Contexts of Use. 15th International Conference, HCI International 2013. Proceedings: LNCS 8006, P97, DOI 10.1007/978-3-642-39265-8_11
   [Anonymous], 2018, Subjective Evaluation Methods for Gaming Quality, ITU-T Recommen dation P.809
   [Anonymous], 2020, Recommendation ITU-TP.919
   Asynchronous TimeWarp (ATW), 2016, ABOUT US
   Beeler D., 2016, Asynchronous spacewarp
   Berger V.W., 2014, Wiley StatsRef: Statistics Reference Online, DOI [10.1002/9781118445112.stat06558, DOI 10.1002/9781118445112.STAT06558]
   Chen H, 2021, IEEE T MULTIMEDIA, V23, P584, DOI 10.1109/TMM.2020.2985538
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Choi SW, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7090171
   Claypool M, 2014, ANN WORK NETW
   Dai JM, 2020, IEEE T CIRC SYST VID, V30, P3843, DOI 10.1109/TCSVT.2019.2946755
   Denes G, 2019, IEEE T VIS COMPUT GR, V25, P2072, DOI 10.1109/TVCG.2019.2898741
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   Ge XH, 2017, IEEE T MULTIMEDIA, V19, P2345, DOI 10.1109/TMM.2017.2733461
   Hosny YSS, 2020, 2020 IEEE GRAPHICS AND MULTIMEDIA (GAME), P13, DOI 10.1109/GAME50158.2020.9315059
   Huang C. Y., 2014, P WORKSH MOB VID DEL, P1
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Huawei Technologies Co. Ltd., 2018, Cloud VR network solution white paper
   Huawei Technologies Co. Ltd., 2019, Cloud VR user experience and evaluation white paper
   Huawei Technologies Co. Ltd., 2018, Cloud VR black edge and network latency relationship white paper
   ITU-T, 2008, ITU T P910 SUBJECTIV
   Kramer DL, 2001, AM ZOOL, V41, P137, DOI 10.1668/0003-1569(2001)041[0137:TBEOIL]2.0.CO;2
   Liu YW, 2021, IEEE T VEH TECHNOL, V70, P2728, DOI 10.1109/TVT.2021.3057684
   Liu YW, 2019, IEEE T MULTIMEDIA, V21, P1302, DOI 10.1109/TMM.2018.2876044
   Mingyi Yang, 2019, 2019 IEEE 5th International Conference on Computer and Communications (ICCC), P1226, DOI 10.1109/ICCC47050.2019.9064326
   Networking CV, 2017, Cisco Global Cloud Index: Forecast and Methodology, 2017-2022
   Nyamtiga BW, 2022, IEEE ACCESS, V10, P95892, DOI 10.1109/ACCESS.2022.3205120
   Schmidt S, 2017, INT WORK QUAL MULTIM
   SEFERIDIS V, 1992, ELECTRON LETT, V28, P2013, DOI 10.1049/el:19921290
   Shi S, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P222, DOI 10.1145/3304109.3306217
   Song J., 2021, P INT C MULT EXP, P1
   Nguyen TC, 2019, IEEE ACCESS, V7, P3031, DOI 10.1109/ACCESS.2018.2888700
   Unity Technologies, 2022, Unity
   Venturelli Luca, 2014, 25th IET Irish Signals & Systems Conference 2014 and 2014 China-Ireland International Conference on Information and Communications Technologies (ISSC 2014/CIICT 2014). Proceedings, P334
   Vlahovic S., 2019, INT WORK QUAL MULTIM, P1, DOI DOI 10.1109/qomex.2019.8743193
   Vlahovic S, 2019, 2019 15TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (CONTEL), DOI 10.1109/contel.2019.8848531
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wibawa Ramadhan Cakra, 2019, 2019 12th International Conference on Information & Communication Technology and System (ICTS). Proceedings, P215, DOI 10.1109/ICTS.2019.8850992
   Wu D, 2014, IEEE T CIRC SYST VID, V24, P1405, DOI 10.1109/TCSVT.2014.2302543
   Xiong H., 2020, Cloud VR: Technology and Application
   Xu M, 2019, IEEE T CIRC SYST VID, V29, P3516, DOI [10.1109/TCSVT.2018.2886277, 10.1080/17445302.2018.1558727]
   Xue Z, 2015, IEEE T CIRC SYST VID, V25, P2013, DOI 10.1109/TCSVT.2014.2364419
   Yen-Chun Li, 2020, QoEVMA'20: Proceedings of the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications, P37, DOI 10.1145/3423328.3423497
   Zhang JQ, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P753, DOI [10.1109/VRW50115.2020.00227, 10.1109/VRW50115.2020.00-48]
   Zhao JB, 2017, P IEEE VIRT REAL ANN, P313, DOI 10.1109/VR.2017.7892302
   Zou WJ, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455970
   Zou WJ, 2018, IET IMAGE PROCESS, V12, P374, DOI 10.1049/iet-ipr.2017.0826
NR 49
TC 6
Z9 7
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8020
EP 8035
DI 10.1109/TMM.2022.3232229
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000059
DA 2024-07-18
ER

PT J
AU Wang, R
   Wu, ZX
   Weng, ZJ
   Chen, JJ
   Qi, GJ
   Jiang, YG
AF Wang, Rui
   Wu, Zuxuan
   Weng, Zejia
   Chen, Jingjing
   Qi, Guo-Jun
   Jiang, Yu-Gang
TI Cross-Domain Contrastive Learning for Unsupervised Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive learning; unsupervised domain adaptation; source data-free
AB Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a fully-labeled source domain to a different unlabeled target domain. Most existing UDA methods learn domain-invariant feature representations by minimizing feature distances across domains. In this work, we build upon contrastive self-supervised learning to align features so as to reduce the domain discrepancy between training and testing sets. Exploring the same set of categories shared by both domains, we introduce a simple yet effective framework CDCL, for domain alignment. In particular, given an anchor image from one domain, we minimize its distances to cross-domain samples from the same class relative to those from different categories. Since target labels are unavailable, we use a clustering-based approach with carefully initialized centers to produce pseudo labels. In addition, we demonstrate that CDCL is a general framework and can be adapted to the data-free setting, where the source data are unavailable during training, with minimal modification. We conduct experiments on two widely used domain adaptation benchmarks, i.e., Office-31 and VisDA-2017, for image classification tasks, and demonstrate that CDCL achieves state-of-the-art performance on both datasets.
C1 [Wang, Rui; Wu, Zuxuan; Weng, Zejia; Chen, Jingjing; Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
   [Wang, Rui; Wu, Zuxuan; Weng, Zejia; Chen, Jingjing; Jiang, Yu-Gang] Shanghai Collaborat Innovat Ctr Intelligent Visua, Shanghai, Peoples R China.
   [Qi, Guo-Jun] Futurewei Technol, Seattle Cloud Lab, Bellevue, WA 98004 USA.
C3 Fudan University; Huawei Technologies
RP Wu, ZX (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.; Wu, ZX (corresponding author), Shanghai Collaborat Innovat Ctr Intelligent Visua, Shanghai, Peoples R China.
EM ruiwang16@fudan.edu.cn; zxwu@fudan.edu.cn; zjweng20@fudan.edu.cn;
   chenjingjing@fudan.edu.cn; guojunq@gmail.com; ygj@fudan.edu.cn
RI Qi, Guo-Jun/AAH-8294-2019; chen, huan/KEC-2019-2024; WANG,
   YILUN/KFB-0627-2024
OI Weng, Zejia/0000-0001-9706-6484
FU NSFC [62102092]
FX This work was supported by NSFC under Grant 62102092
CR Ben-David S., 2007, ADV NEURAL INFORM PR, V19
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen XY, 2019, PR MACH LEARN RES, V97
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ge Y., 2020, P NIPS, V33, P11309
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu LQ, 2020, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR42600.2020.00410
   Jin X, 2021, IEEE T MULTIMEDIA, V24, P3636, DOI 10.1109/TMM.2021.3104379
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Khosla P., 2020, ADV NEURAL INF PROCE, V33, P18661
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li R., 2020, P IEEE CVF C COMP VI, P9641, DOI DOI 10.1109/CVPR42600.2020.00966
   Liang J., 2020, INT C MACH LEARN, P6028, DOI DOI 10.48550/ARXIV.2002.08546
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Lu Z., 2020, P IEEECVF C COMPUTER, P9111
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Park C, 2020, Arxiv, DOI arXiv:2006.10297
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K., 2018, PROC 6 INT C LEARN R
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Shuyang Dai, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12625), P268, DOI 10.1007/978-3-030-69538-5_17
   Sohn K, 2016, ADV NEUR IN, V29
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Thota M, 2021, IEEE COMPUT SOC CONF, P2209, DOI 10.1109/CVPRW53098.2021.00250
   Toldo M, 2021, IEEE WINT CONF APPL, P1357, DOI 10.1109/WACV48630.2021.00140
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Zellinger W., 2017, PROC 5 INT C LEARN R
   Zhang YC, 2019, PR MACH LEARN RES, V97
   Zheng YH, 2019, IEEE T MULTIMEDIA, V21, P2292, DOI 10.1109/TMM.2019.2900166
NR 47
TC 48
Z9 48
U1 33
U2 81
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1665
EP 1673
DI 10.1109/TMM.2022.3146744
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100005
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wang, WL
   Zhou, WG
   Bao, JM
   Li, HQ
AF Wang, Weilun
   Zhou, Wengang
   Bao, Jianmin
   Li, Houqiang
TI Coherent Image Animation Using Spatial-Temporal Correspondence
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image animation; self-supervised learning; temporal coherence;
   spatial-temporal correspondence
ID VIDEO GENERATION
AB Recent studies have achieved remarkable success using deep generative models for the image animation of an arbitrary object.However, previous methods synthesize animated results in a frame-by-frame manner, which is prone to producing flickering and temporally inconsistent results. In this paper, we propose a novel self-supervised framework leveraging temporal information for image animation. Our framework processes a video clip directly instead of processing each frame independently. To achieve coherence in the animated video, we design a spatial-temporal correspondence network (STCN) to maintain the consistency of the keypoints. Specifically, the STCN takes full advantage of temporal information to propagate the keypoints between adjacent frames, and it can be trained with consistent keypoints during the forward and backward process. Furthermore, we apply a 3D-CNN-based generator and discriminator in our framework to ensure coherence in the final output video. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our method.
C1 [Wang, Weilun; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol GIPAS, Hefei 230027, Peoples R China.
   [Bao, Jianmin] Microsoft Res, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol GIPAS, Hefei 230027, Peoples R China.
EM wwlustc@mail.ustc.edu.cn; zhwg@ustc.edu.cn; jianbao@microsoft.com;
   lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
OI Wang, Weilun/0000-0002-0037-4147
CR Ardino P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14729, DOI 10.1109/ICCV48922.2021.01448
   Biyani N., 2021, ARXIV
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chaoyue Wang, 2020, IEEE Transactions on Artificial Intelligence, V1, P34, DOI 10.1109/TAI.2020.3031581
   Cui RP, 2020, IEEE T MULTIMEDIA, V22, P2551, DOI 10.1109/TMM.2019.2960700
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Goodfellow I., 2016, ADV NEURAL INFORM PR, P64
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jaderberg M, 2015, ADV NEUR IN, V28
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Keshari A, 2021, BRIT MACH VIS C
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Moing Guillaume, 2021, NEURIPS
   Li X., 2022, P IEEE CVF WINT C AP, P3103
   Li YT, 2018, AAAI CONF ARTIF INTE, P7065
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Ludwiczuk B., 2016, Tech. Rep. CMU-CS-16-118
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Ohnishi K, 2018, AAAI CONF ARTIF INTE, P2387
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Salimans T, 2016, ADV NEUR IN, V29
   Sheng L, 2020, INT J COMPUT VISION, V128, P2552, DOI 10.1007/s11263-020-01334-x
   Siarohin A, 2019, ADV NEUR IN, V32
   Siarohin A, 2019, PROC CVPR IEEE, P2372, DOI 10.1109/CVPR.2019.00248
   Soomro K., 2012, CoRR, V2
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P774, DOI 10.1145/3240508.3240704
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Vondrick C, 2017, 017 IEEE INT C COMPU, P2849
   Wang T., 2018, ARXIV
   Wang W, 2020, IEEE T MULTIMEDIA, V22, P2808, DOI 10.1109/TMM.2019.2963621
   Wang YH, 2020, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR42600.2020.00531
   Wei DX, 2021, IEEE T MULTIMEDIA, V23, P2457, DOI 10.1109/TMM.2020.3011290
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Wu L, 2020, IEEE T IMAGE PROCESS, V29, P1233, DOI 10.1109/TIP.2019.2940684
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37
   Yang CH, 2018, PROCEEDINGS OF 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS), P201, DOI 10.1109/CCOMS.2018.8463302
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 50
TC 1
Z9 1
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3397
EP 3408
DI 10.1109/TMM.2022.3160297
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200035
DA 2024-07-18
ER

PT J
AU Wang, XH
   Zhu, LC
   Zheng, ZD
   Xu, ML
   Yang, Y
AF Wang, Xiaohan
   Zhu, Linchao
   Zheng, Zhedong
   Xu, Mingliang
   Yang, Yi
TI Align and Tell: Boosting Text-Video Retrieval With Local Alignment and
   Fine-Grained Supervision
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Visualization; Computational modeling; Training; Feature
   extraction; Task analysis; Transformers; Text-video retrieval;
   Multimodal Understanding; Video captioning
ID FRAMEWORK
AB Text-video retrieval is one of the basic tasks for multimodal research and has been widely harnessed in many real-world systems. Most existing approaches directly compare the global representation between videos and text descriptions and utilize the global contrastive loss to train the model. These designs overlook the local alignment and the word-level supervision signal. In this paper, we propose a new framework, called Align and Tell, for text-video retrieval. Compared to the previous work, our framework contains additional modules, i.e., two transformer decoders for local alignment and one captioning head to enhance the representation learning. First, we introduce a set of learnable queries to interact with both textual representations and video representations and project them to a fixed number of local features. After that, local contrastive learning is performed to complement the global comparison. Moreover, we design a video captioning head to provide additional supervision signals during training. This word-level supervision can enhance the visual presentation and alleviate the cross-modal gap. The captioning head can be removed during inference and does not introduce extra computational costs. Extensive empirical results demonstrate that our Align and Tell model can achieve state-of-the-art performance on four text-video retrieval datasets, including MSR-VTT, MSVD, LSMDC, and ActivityNet-Captions.
C1 [Wang, Xiaohan; Zhu, Linchao; Yang, Yi] Zhejiang Univ, Sch Comp Sci, Hangzhou, Peoples R China.
   [Zheng, Zhedong] Natl Univ Singapore, Sch Comp, Sea NExT Joint Lab, Singapore, Singapore.
   [Xu, Mingliang] Zhengzhou Univ, Sch Comp Sci, Zhengzhou, Peoples R China.
C3 Zhejiang University; National University of Singapore; Zhengzhou
   University
RP Zhu, LC (corresponding author), Zhejiang Univ, Sch Comp Sci, Hangzhou, Peoples R China.
EM xiaohan.wang@zju.edu.cn; linchao.zhu@uts.edu.au; zdzheng@nus.edu.sg;
   iexumingliang@zzu.edu.cn; yangyics@zju.edu.cn
RI Wang, Xiaohan/JKI-4414-2023; Zheng, Zhedong/R-5314-2019
OI Wang, Xiaohan/0000-0001-5273-4223; Zheng, Zhedong/0000-0002-2434-9050
FU National Key Ramp;D Program of China
FX No Statement Available
CR Amrani E, 2021, AAAI CONF ARTIF INTE, V35, P6644
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bogolin SV, 2022, PROC CVPR IEEE, P5184, DOI 10.1109/CVPR52688.2022.00513
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Croitoru I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11563, DOI 10.1109/ICCV48922.2021.01138
   Desai K, 2021, PROC CVPR IEEE, P11157, DOI 10.1109/CVPR46437.2021.01101
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Dzabraev M, 2021, IEEE COMPUT SOC CONF, P3349, DOI 10.1109/CVPRW53098.2021.00374
   Faghri D. J., 2018, P BRIT MACH VIS C
   Fan HH, 2020, AAAI CONF ARTIF INTE, V34, P10754
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Kingma D, 2014, C LEARNING REPRESENT, P12
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Liu Y., 2019, P BRIT MACH VIS C, P279
   Luo HS, 2021, Arxiv, DOI arXiv:2104.08860
   Manaris B., 1998, Advances in Computers, VVolume 47, P1, DOI [10.1016/S0065-2458, DOI 10.1016/S0065-2458]
   Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990
   Miech A, 2020, Arxiv, DOI arXiv:1804.02516
   Miech A, 2021, PROC CVPR IEEE, P9821, DOI 10.1109/CVPR46437.2021.00970
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Paszke A, 2019, ADV NEUR IN, V32
   Patrick Mandela, 2021, P INT C LEARN REPR
   Radford A, 2021, PR MACH LEARN RES, V139
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Shen L, 2020, FRONT COMPUT SCI-CHI, V14, DOI 10.1007/s11704-019-8229-7
   Simonyan K, 2014, ADV NEUR IN, V27
   Wang L., 2020, IEEETrans. Pattern Anal. Mach. Intell., early access, DOI [10.1109/TPAMI.2020.3015894.[29]L., DOI 10.1109/TPAMI.2020.3015894.[29]L]
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang XS, 2018, IEEE T MULTIMEDIA, V20, P2360, DOI 10.1109/TMM.2018.2807588
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhao S, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P970, DOI 10.1145/3477495.3531950
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhou RW, 2020, IEEE T NEUR NET LEAR, V31, P1592, DOI 10.1109/TNNLS.2019.2920905
   Zhu L., 2020, P IEEE CVF C COMP VI, DOI 10.1109/CVPR42600.2020.00877
   Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503
NR 50
TC 25
Z9 25
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6079
EP 6089
DI 10.1109/TMM.2022.3204444
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300007
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Yin, X
   Min, D
   Huo, YC
   Yoon, SE
AF Yin, Xu
   Min, Dongbo
   Huo, Yuchi
   Yoon, Sung-Eui
TI Contour-Aware Equipotential Learning for Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Category-level contour learning; semantic boundary refinement;
   supervised semantic segmentation
ID NETWORK
AB With increasing demands for high-quality semantic segmentation in the industry, hard-distinguishing semantic boundaries have posed a significant threat to existing solutions. Inspired by real-life experience, i.e., combining varied observations contributes to higher visual recognition confidence, we present the equipotential learning (EPL) method. This novel module transfers the predicted/ground-truth semantic labels to a self-defined potential domain to learn and infer decision boundaries along customized directions. The conversion to the potential domain is implemented via a lightweight differentiable anisotropic convolution without incurring any parameter overhead. Besides, the designed two loss functions, the point loss and the equipotential line loss implement anisotropic field regression and category-level contour learning, respectively, enhancing prediction consistencies in the inter/intra-class boundary areas. More importantly, EPL is agnostic to network architectures, and thus it can be plugged into most existing segmentation models. This paper is the first attempt to address the boundary segmentation problem with field regression and contour learning. Meaningful performance improvements on Pascal Voc 2012 and Cityscapes demonstrate that the proposed EPL module can benefit the off-the-shelf fully convolutional network models when recognizing semantic boundary areas. Besides, intensive comparisons and analysis show the favorable merits of EPL for distinguishing semantically-similar and irregular-shaped categories.
C1 [Yin, Xu] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon 34141, South Korea.
   [Min, Dongbo] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul 03760, South Korea.
   [Huo, Yuchi] Zhejiang Lab, Hangzhou 310058, Peoples R China.
   [Huo, Yuchi] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Yoon, Sung-Eui] Korea Adv Inst Sci & Technol, Fac Sch Comp, Daejeon 34141, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Ewha Womans
   University; Zhejiang Laboratory; Zhejiang University; Korea Advanced
   Institute of Science & Technology (KAIST)
RP Huo, YC (corresponding author), Zhejiang Lab, Hangzhou 310058, Peoples R China.; Huo, YC (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.; Yoon, SE (corresponding author), Korea Adv Inst Sci & Technol, Fac Sch Comp, Daejeon 34141, South Korea.
EM yinofsgvr@kaist.ac.kr; dbmin@ewha.ac.kr; huo.yuchi.sc@gmail.com;
   sungeui@gmail.com
RI YIN, XU/JZE-3622-2024
OI Huo, Yuchi/0000-0003-3296-7999; YIN, XU/0000-0003-1967-7030; Min,
   Dongbo/0000-0003-4825-5240
FU National Research Foundation of Korea through the Korea government, MSIT
   [2019R1A2C3002833, 2021R1A4A1032582]; Zhejiang Lab [121005-PI2101]
FX This work was supported in part by the National Research Foundation of
   Korea through the Korea government, MSIT under Grants 2019R1A2C3002833,
   2021R1A4A1032582, and in part by Zhejiang Lab under Grant 121005-PI2101.
CR Acuna D, 2019, PROC CVPR IEEE, P11067, DOI 10.1109/CVPR.2019.01133
   Audebert N, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102809
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Borse S, 2021, PROC CVPR IEEE, P5897, DOI 10.1109/CVPR46437.2021.00584
   Bowen Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12472, DOI 10.1109/CVPR42600.2020.01249
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng BW, 2021, PROC CVPR IEEE, P15329, DOI 10.1109/CVPR46437.2021.01508
   Cohen N, 2017, Arxiv, DOI arXiv:1605.06743
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Everingham M., 2012, PATTERN ANAL STAT MO, V2007, P1, DOI DOI 10.1007/S11263-014-0733-5
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gu ZX, 2021, IEEE T MULTIMEDIA, V23, P3738, DOI 10.1109/TMM.2020.3035231
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Kervadec H, 2019, PR MACH LEARN RES, V102, P285
   Kirillov A., 2020, P IEEECVF C COMPUTER, P9799, DOI DOI 10.48550/ARXIV.1912.08193
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Marin D, 2019, IEEE I CONF COMP VIS, P2131, DOI 10.1109/ICCV.2019.00222
   Naylor P, 2019, IEEE T MED IMAGING, V38, P448, DOI 10.1109/TMI.2018.2865709
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Rau R, 1997, IEEE T SIGNAL PROCES, V45, P468, DOI 10.1109/78.554310
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Song Q, 2022, AAAI CONF ARTIF INTE, P2280
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Wang C, 2022, AAAI CONF ARTIF INTE, P2397
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P435, DOI 10.1007/978-3-030-58520-4_26
   Xue Y, 2020, AAAI CONF ARTIF INTE, V34, P12565
   Ye SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6423, DOI 10.1109/ICCV48922.2021.00638
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Yu ZD, 2018, LECT NOTES COMPUT SC, V11207, P400, DOI 10.1007/978-3-030-01219-9_24
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P489, DOI 10.1007/978-3-030-58610-2_29
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
NR 43
TC 2
Z9 2
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6146
EP 6156
DI 10.1109/TMM.2022.3205441
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500036
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Liang, C
   Jiang, LX
AF Zhang, Yue
   Liang, Chao
   Jiang, Longxiang
TI Confidence-Aware Active Feedback for Interactive Instance Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; Interactive instance search
ID OBJECT RETRIEVAL; CONSISTENCY; KNOWLEDGE; RANKING
AB Online relevance feedback (RF) is widely utilized in instance search (INS) tasks to further refine imperfect ranking results, but it often has low interaction efficiency. The active learning (AL) technique addresses this problem by selecting valuable feedback candidates. However, mainstream AL methods require an initial labeled set for a cold start and are often computationally complex to solve. Therefore, they cannot fully satisfy the requirements for online RF in interactive INS tasks. To address this issue, we propose a confidence-aware active feedback method (CAAF) that is specifically designed for online RF in interactive INS tasks. Inspired by the explicit difficulty modeling scheme in self-paced learning, CAAF utilizes a pairwise manifold ranking loss to evaluate the ranking confidence of each unlabeled sample. The ranking confidence improves not only the interaction efficiency by indicating valuable feedback candidates but also the ranking quality by modulating the diffusion weights in manifold ranking. In addition, we design two acceleration strategies, an approximate optimization scheme and a top-$K$ search scheme, to reduce the computational complexity of CAAF. Extensive experiments on both image INS tasks and video INS tasks searching for buildings, landscapes, persons, and human behaviors demonstrate the effectiveness of the proposed method. Notably, in the real-world, large-scale video INS task of NIST TRECVID 2021, CAAF uses 25% fewer feedback samples to achieve a performance that is nearly equivalent to the champion solution. Moreover, with the same number of feedback samples, CAAF's mAP is 51.9%, significantly surpassing the champion solution by 5.9%. Code is available at https://github.com/nercms-mmap/caaf.
C1 [Zhang, Yue; Liang, Chao; Jiang, Longxiang] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Zhang, Yue; Liang, Chao; Jiang, Longxiang] Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan 430072, Peoples R China.
C3 Wuhan University; Wuhan University
RP Liang, C (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.; Liang, C (corresponding author), Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan 430072, Peoples R China.
EM moozy924@whu.edu.cn; cliang@whu.edu.cn; jianglx@whu.edu.cn
OI Zhang, Yue/0000-0003-4185-112X
FU National Natural Science Foundation of China [U1903214, 61862015,
   61876135, 62071338]; Fundamental Research Founds for the Central
   Universities [2042022kf0001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1903214, 61862015, 61876135, and
   62071338, and in part by the Fundamental Research Founds for the Central
   Universities under Grant 2042022kf0001
CR [Anonymous], 2009, Technical report
   Bezdek J. C., 2003, Neural, Parallel & Scientific Computations, V11, P351
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Caramalau R, 2021, PROC CVPR IEEE, P9578, DOI 10.1109/CVPR46437.2021.00946
   Chen ZX, 2018, IEEE T MULTIMEDIA, V20, P2126, DOI 10.1109/TMM.2017.2785253
   Das Bhattacharjee S, 2018, IEEE T MULTIMEDIA, V20, P2761, DOI 10.1109/TMM.2018.2814338
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doherty W. J., 2020, Laws of UX: Using Psychology to Design Better Products & Services
   Ebert S, 2012, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2012.6248108
   Han CC, 2018, LECT NOTES COMPUT SC, V11256, P3, DOI 10.1007/978-3-030-03398-9_1
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jiang L, 2014, ADV NEUR IN, V27
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YY, 2022, IEEE T MULTIMEDIA, V24, P415, DOI 10.1109/TMM.2021.3052354
   Lin L, 2018, IEEE T PATTERN ANAL, V40, P7, DOI 10.1109/TPAMI.2017.2652459
   Lin TC, 2015, IEEE T IMAGE PROCESS, V24, P1330, DOI 10.1109/TIP.2015.2403236
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P5573, DOI 10.1109/TIP.2021.3086590
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P3168, DOI 10.1109/TIP.2019.2957930
   Liu ZM, 2019, IEEE I CONF COMP VIS, P6121, DOI 10.1109/ICCV.2019.00622
   Ma H., 2021, IEEE Access, V9
   Meng JJ, 2016, IEEE T MULTIMEDIA, V18, P116, DOI 10.1109/TMM.2015.2500734
   Niu Y., 2021, PROC TRECVID, P1
   Pang SM, 2019, IEEE T MULTIMEDIA, V21, P760, DOI 10.1109/TMM.2018.2866230
   Pawan Kumar M., 2010, NIPS
   Peng Y., 2020, PROC TRECVID, P1
   Philbin J., 2007, PROC IEEE C COMPUT V, P1
   Rossetto L, 2021, IEEE T MULTIMEDIA, V23, P243, DOI 10.1109/TMM.2020.2980944
   Rottmann M, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P158, DOI 10.1109/ICMLA.2018.00031
   Sener Ozan, 2018, INT C LEARN REPR ICL
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sinha S, 2019, IEEE I CONF COMP VIS, P5971, DOI 10.1109/ICCV.2019.00607
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tang YP, 2019, AAAI CONF ARTIF INTE, P5117
   Wei J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3835, DOI 10.1145/3474085.3475451
   Wu XP, 2021, IEEE T IMAGE PROCESS, V30, P6512, DOI 10.1109/TIP.2021.3094744
   Xiaofei He, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P119
   Yang J., 2022, Video instance search for composite semantics
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1640, DOI 10.1109/TMM.2020.3001527
   Yuan JS, 2011, IEEE T PATTERN ANAL, V33, P1728, DOI 10.1109/TPAMI.2011.38
   Zhang D., 2020, Adv. Neural Info. Process. Syst., V33, P12236
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P475, DOI 10.1109/TPAMI.2018.2881114
   Zhao MB, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106505
   Zhao WQ, 2021, IEEE T IMAGE PROCESS, V30, P7995, DOI 10.1109/TIP.2021.3112011
   Zhao X, 2017, MULTIMED TOOLS APPL, V76, P12133, DOI 10.1007/s11042-016-4142-3
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   Zobel J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P307, DOI 10.1145/290941.291014
NR 49
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7173
EP 7184
DI 10.1109/TMM.2022.3217965
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000033
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, L
   Wang, XQ
   Li, P
   Yang, X
   Zhang, Q
   Wang, WM
   Schönlieb, CB
   Chen, CLP
AF Zhu, Lei
   Wang, Xiaoqiang
   Li, Ping
   Yang, Xin
   Zhang, Qing
   Wang, Weiming
   Schonlieb, Carola-Bibiane
   Chen, C. L. Philip
TI S <SUP>3</SUP> Net: Self-Supervised Self-Ensembling Network for
   Semi-Supervised RGB-D Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency detection; Feature extraction; Convolutional neural networks;
   Task analysis; Detectors; Object detection; Training; RGB-D salient
   object detection; self-supervised learning; semi-supervised learning;
   and cross-model and cross-level feature aggregation
ID SEGMENTATION; FUSION
AB RGB-D salient object detection aims to detect visually distinctive objects or regions from a pair of the RGB image and the depth image. State-of-the-art RGB-D saliency detectors are mainly based on convolutional neural networks but almost suffer from an intrinsic limitation relying on the labeled data, thus degrading detection accuracy in complex cases. In this work, we present a self-supervised self-ensembling network (S-3 Net) for semi-supervised RGB-D salient object detection by leveraging the unlabeled data and exploring a self-supervised learning mechanism. To be specific, we first build a self-guided convolutional neural network (SG-CNN) as a baseline model by developing a series of three-layer cross-model feature fusion (TCF) modules to leverage complementary information among depth and RGB modalities and formulating an auxiliary task that predicts a self-supervised image rotation angle. After that, to further explore the knowledge from unlabeled data, we assign SG-CNN to a student network and a teacher network, and encourage the saliency predictions and self-supervised rotation predictions from these two networks to be consistent on the unlabeled data. Experimental results on seven widely-used benchmark datasets demonstrate that our network quantitatively and qualitatively outperforms the state-of-the-art methods.
C1 [Zhu, Lei] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Zhu, Lei] Univ Cambridge, Dept Appl Math & Theoret Phys DAMTP, Cambridge CB3 0WA, England.
   [Wang, Xiaoqiang] Zhejiang Univ, Coll Comp Sci & Technol, Shatin, Hangzhou 310058, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong 00852, Peoples R China.
   [Yang, Xin] Dalian Univ Technol, Dept Comp Sci, Dalian 116024, Peoples R China.
   [Zhang, Qing] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.
   [Wang, Weiming] Hong Kong Metropolitan Univ, Sch Sci & Technol, Ho Man Tin, Hong Kong 00852, Peoples R China.
   [Schonlieb, Carola-Bibiane] Univ Cambridge, Dept Appl Math & Theoret Phys DAMTP, Cambridge CB3 0WA, England.
   [Chen, C. L. Philip] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Chen, C. L. Philip] Dalian Maritime Univ, Nav Coll, Dalian 116026, Peoples R China.
   [Chen, C. L. Philip] Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.
C3 Hong Kong University of Science & Technology; University of Cambridge;
   Zhejiang University; Hong Kong Polytechnic University; Dalian University
   of Technology; Sun Yat Sen University; University of Cambridge; South
   China University of Technology; Dalian Maritime University; University
   of Macau
RP Zhang, Q (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.
EM zhulei9009@gmail.com; xq.wang@zju.edu.cn; p.li@polyu.edu.hk;
   xinyang@dlut.edu.cn; zhangqing.whu.cs@gmail.com; wmwang@ouhk.edu.hk;
   cbs31@cam.ac.uk; philip.chen@ieee.org
RI Zhang, Ge/KGL-7634-2024; QIU, LI/JPK-7397-2023; Li,
   Xiaoli/JVZ-4089-2024; Wang, Peiyun/JVE-1196-2024; zhang,
   xiang/JJD-7003-2023; Yao, Chen/JVD-6226-2023; Jiang, Yu/JEZ-9814-2023;
   Li, Ping/AAO-2019-2020; liu, xy/JEP-3175-2023; Li, Jiawei/JOJ-9277-2023;
   Chen, Chao/JHS-6563-2023; yang, kun/JGM-4169-2023; yang,
   xu/JMP-5558-2023; wangwangwang, yuanyaunyuan/HHN-6432-2022; wang,
   hang/JND-8481-2023; liu, jianyang/JXL-6273-2024; zhao,
   lin/JJF-0406-2023; LIU, JIALIN/JXN-8034-2024; ZHANG,
   XUCHEN/KBB-7989-2024; Wang, Xiaoqiang/IQS-3727-2023; yang,
   zhou/KBB-6972-2024; Wang, Yuchen/JPW-9345-2023; li,
   yansong/JXL-5023-2024; LI, Wenhui/JCD-9947-2023
OI Li, Ping/0000-0002-1503-0240; Zhu, Lei/0000-0003-3871-663X; Wang,
   Xiaoqiang/0000-0001-6096-8011; Wang, Weiming/0000-0002-9068-0227;
   Schonlieb, Carola-Bibiane/0000-0003-0099-6306
FU National Natural Science Foundation of China under [61902275, 61802453,
   61802072]; Hong Kong Polytechnic University under [P0030419, P0030929,
   P0035358]; National Key Researchand Development Program of China under
   [2019YFB1703600]; Hong Kong Metropolitan University under Research
   [2020/1.12]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61902275, 61802453,and 61802072, in
   part by Hong Kong Polytechnic University under Grants P0030419,
   P0030929, and P0035358, in part by the National Key Researchand
   Development Program of China under Grant 2019YFB1703600, and in part by
   Hong Kong Metropolitan University under Research Grant 2020/1.12.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen S., 2020, ECCV, P520, DOI DOI 10.1007/978-3-030-58598-3_31
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cheng Y, 2014, IEEE INT CON MULTI
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Deng J., 2009, J ALLOY COMPD, P248, DOI DOI 10.1016/j.jallcom.2006.10.076
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Ding XY, 2019, IEEE T MULTIMEDIA, V21, P124, DOI 10.1109/TMM.2018.2851389
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Feng ZY, 2019, PROC CVPR IEEE, P10356, DOI 10.1109/CVPR.2019.01061
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Gidaris S., 2018, PROC INT C LEARN REP
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Guo JF, 2016, IEEE INT CON MULTI
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang PS, 2018, INT CONF DIGIT SIG
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Laine Samuli, 2016, PROC INT C LEARN REP
   Lang CY, 2016, IEEE T NEUR NET LEAR, V27, P1190, DOI 10.1109/TNNLS.2015.2513393
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liang FF, 2018, NEUROCOMPUTING, V275, P2227, DOI 10.1016/j.neucom.2017.10.052
   Liu D, 2021, IEEE T MULTIMEDIA, V23, P967, DOI 10.1109/TMM.2020.2991523
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Tarvainen Antti, 2017, ADV NEURAL INFORM PR, P2, DOI DOI 10.1137/0330046
   Tsai CC, 2020, IEEE T MULTIMEDIA, V22, P1016, DOI 10.1109/TMM.2019.2936803
   Wang AZ, 2017, IEEE SIGNAL PROC LET, V24, P663, DOI 10.1109/LSP.2017.2688136
   Wang LS, 2021, IEEE T CIRC SYST VID, V31, P728, DOI 10.1109/TCSVT.2020.2988768
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Yu LQ, 2019, LECT NOTES COMPUT SC, V11765, P605, DOI 10.1007/978-3-030-32245-8_67
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang D., 2020, ANN C NEURAL INFORMA
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P1755, DOI 10.1109/TPAMI.2019.2900649
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao HL, 2009, VISUAL COMPUT, V25, P973, DOI 10.1007/s00371-008-0308-y
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao MY, 2016, PATTERN RECOGN, V51, P281, DOI 10.1016/j.patcog.2015.09.008
   Zhou Y, 2019, IEEE T CYBERNETICS, V49, P1173, DOI 10.1109/TCYB.2018.2793278
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu L, 2020, IEEE T CIRC SYST VID, V30, P3358, DOI 10.1109/TCSVT.2019.2941017
   Zhu L, 2020, IEEE T VIS COMPUT GR, V26, P2471, DOI 10.1109/TVCG.2018.2889055
NR 79
TC 3
Z9 3
U1 5
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 676
EP 689
DI 10.1109/TMM.2021.3129730
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900001
DA 2024-07-18
ER

PT J
AU Zhu, PF
   Yao, XJ
   Wang, Y
   Cao, M
   Hui, BY
   Zhao, S
   Hu, QH
AF Zhu, Pengfei
   Yao, Xinjie
   Wang, Yu
   Cao, Meng
   Hui, Binyuan
   Zhao, Shuai
   Hu, Qinghua
TI Latent Heterogeneous Graph Network for Incomplete Multi-View Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graph learning; heterogeneous graph network; incomplete multi-view
   learning
ID SCALE; CLASSIFICATION; RECOGNITION; SCENE
AB Multi-view learning has progressed rapidly in recent years. Although many previous studies assume that each instance appears in all views, it is common in real-world applications for instances to be missing from some views, resulting in incomplete multi-view data. To tackle this problem, we propose a novel Latent Heterogeneous Graph Network (LHGN) for incomplete multi-view learning, which aims to use multiple incomplete views as fully as possible in a flexible manner. By learning a unified latent representation, a trade-off between consistency and complementarity among different views is implicitly realized. To explore the complex relationship between samples and latent representations, a neighborhood constraint and a view-existence constraint are proposed, for the first time, to construct a heterogeneous graph. Finally, to avoid any inconsistencies between training and test phase, a transductive learning technique is applied based on graph learning for classification tasks. Extensive experimental results on real-world datasets demonstrate the effectiveness of our model over existing state-of-the-art approaches. Our code is available at: https://github.com/yxjdarren/LHGN_TMM_2022.
C1 [Zhu, Pengfei; Yao, Xinjie; Wang, Yu; Cao, Meng; Hui, Binyuan; Zhao, Shuai; Hu, Qinghua] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Zhao, Shuai] Tianjin Co Ltd, Automot Data China, Tianjin 300300, Peoples R China.
C3 Tianjin University
RP Wang, Y (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM zhupengfei@tju.edu.cn; yaoxinjie@tju.edu.cn; wangyu_@tju.edu.cn;
   caomeng@tju.edu.cn; huibinyuan@tju.edu.cn; zhaoshuai@catarc.ac.cn;
   huqinghua@tju.edu.cn
RI Hu, Qinghua/B-8857-2008
OI Hu, Qinghua/0000-0001-7765-8095; Wang, Yu/0000-0002-4788-8655
FU Key-Area Research and Development Program of Guangdong Province
   [2019B010153002]; National Natural Science Foundation of China
   [62106174, 61732011, 61876127]; National Key Research and Development
   Program of China [2019YFB2101901]; Natural Science Foundation of Tianjin
   [17JCZDJC30800]; China Postdoctoral Science Foundation [2021TQ0242,
   2021M690118]; Key-Area Research and Development Program of Guangdong
   Province [2019B010153002]; National Natural Science Foundation of China
   [62106174, 61732011, 61876127]; National Key Research and Development
   Program of China [2019YFB2101901]; Natural Science Foundation of Tianjin
   [17JCZDJC30800]; China Postdoctoral Science Foundation [2021TQ0242,
   2021M690118]
FX This work was supported in part by the Key-Area Research and Development
   Program of Guangdong Province under Grant 2019B010153002,in part by the
   National Natural Science Foundation of China under Grants 62106174,
   61732011, and 61876127, in part by the National Key Research and
   Development Program of China under Grant 2019YFB2101901, in part by the
   Natural Science Foundation of Tianjin under Grant 17JCZDJC30800, and in
   part by China Postdoctoral Science Foundation under Grants 2021TQ0242
   and 2021M690118.
CR Andrew G., 2013, ICML, P1247
   Appice A, 2016, IEEE T SERV COMPUT, V9, P832, DOI 10.1109/TSC.2015.2430327
   Bach Francis R, 2004, P 21 INT C MACH LEAR, P6, DOI 10.1145/1015330.1015424
   Chang SY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P119, DOI 10.1145/2783258.2783296
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Davis J. V., 2007, ICML, P209
   Ding ZM, 2016, AAAI CONF ARTIF INTE, P1181
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Kipf T. N., 2017, 8 INT C LEARN REPR, P1
   Krizhevsky A., 2012, Advances in Neural Information Processing Systems, V25, P1106
   Kuanar SK, 2015, IEEE T MULTIMEDIA, V17, P1166, DOI 10.1109/TMM.2015.2443558
   Kumar A., 2011, P 28 INT C MACHINE L, P393
   Kumar P., 2011, Adv. Neural Inf. Process. Syst., P1413, DOI DOI 10.5555/2986459.2986617
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Le Quoc V., 2014, P INT C MACH LEARN I
   Li JL, 2021, IEEE INTERNET THINGS, V8, P11219, DOI 10.1109/JIOT.2021.3051905
   Li RH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2916
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Lin Y., 2021, PROC IEEECVF C COMPU, p11 174
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1303, DOI 10.1109/TPAMI.2019.2895608
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Tran L, 2017, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2017.528
   Marlin B. M., 2011, P 22 INT JOINT C ART, P2686
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Shao WX, 2013, IEEE DATA MINING, P1181, DOI 10.1109/ICDM.2013.117
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Sun JW, 2015, PR MACH LEARN RES, V37, P757
   Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velickovic Petar, 2018, INT C LEARN REPR
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3677
   Wang QQ, 2018, IEEE DATA MINING, P1290, DOI 10.1109/ICDM.2018.00174
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang X, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2022, DOI 10.1145/3308558.3313562
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wen J, 2019, AAAI CONF ARTIF INTE, P5393
   Xu C, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2490539
   Yan XQ, 2021, NEUROCOMPUTING, V448, P106, DOI 10.1016/j.neucom.2021.03.090
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270, DOI DOI 10.1145/1869790.1869829
   Yin QY, 2017, PATTERN RECOGN, V67, P313, DOI 10.1016/j.patcog.2017.01.035
   Yuan Lei, 2012, KDD, P1149
   Zhang CQ, 2022, IEEE T PATTERN ANAL, V44, P2402, DOI 10.1109/TPAMI.2020.3037734
   Zhang CQ, 2019, ADV NEUR IN, V32
   Zhang CQ, 2017, IEEE T IMAGE PROCESS, V26, P648, DOI 10.1109/TIP.2016.2627806
   Zhang CX, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P793, DOI 10.1145/3292500.3330961
   Zhang YZ, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P399, DOI 10.1145/3178876.3186106
   Zhao H., 2016, IJCAI, P2392
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   Zhao L, 2018, NEUROCOMPUTING, V275, P1053, DOI 10.1016/j.neucom.2017.07.016
NR 66
TC 8
Z9 8
U1 4
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3033
EP 3045
DI 10.1109/TMM.2022.3154592
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, YQ
   Li, XY
   Zheng, M
   Yang, JH
   Wang, ZH
   Guo, XQ
   Chai, ZF
   Yuan, YC
   Jiang, SQ
AF Zhu, Yongqing
   Li, Xiangyang
   Zheng, Mao
   Yang, Jiahao
   Wang, Zihan
   Guo, Xiaoqian
   Chai, Zifeng
   Yuan, Yuchen
   Jiang, Shuqiang
TI Focus and Align: Learning Tube Tokens for Video-Language Pre-Training
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Electron tubes; Semantics; Visualization; Feature extraction; Task
   analysis; Transformers; Detectors; Local alignment mechanism; semantic
   centers; tube tokens; video-language pre-training
AB Video-language pre-training (VLP) has attracted increasing attention for cross-modality understanding tasks. To enhance visual representations, recent works attempt to adopt transformer-based architectures as video encoders. These works usually focus on the visual representations of the sampled frames. Compared with frame representations, frame patches incorporate more fine-grained spatio-temporal information, which could lead to a better understanding of video contents. However, how to exploit the spatio-temporal information within frame patches for VLP has been less investigated. In this work, we propose a method to learn tube tokens to model the key spatio-temporal information from frame patches. To this end, multiple semantic centers are introduced to focus on the underlying patterns of frame patches. Based on each semantic center, the spatio-temporal information within frame patches is integrated into a unique tube token. Complementary to frame representations, tube tokens provide detailed clues of video contents. Furthermore, to better align the generated tube tokens and the contents of descriptions, a local alignment mechanism is introduced. The experiments based on a variety of downstream tasks demonstrate the effectiveness of the proposed method.
C1 [Zhu, Yongqing; Li, Xiangyang; Yang, Jiahao; Wang, Zihan; Guo, Xiaoqian; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zhu, Yongqing; Li, Xiangyang; Yang, Jiahao; Wang, Zihan; Guo, Xiaoqian; Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Zheng, Mao; Chai, Zifeng; Yuan, Yuchen] Tencent, Dept Machine Learning Platform, Beijing 100193, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Tencent
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM yongqing.zhu@vipl.ict.ac.cn; xiangyang.li@vipl.ict.ac.cn;
   moonzheng@tencent.com; jiahao.yang@vipl.ict.ac.cn;
   zihan.wang@vipl.ict.ac.cn; xiaoqian.guo@vipl.ict.ac.cn;
   zifengchai@tencent.com; yuchenyuan@tencent.com; sqjiang@ict.ac.cn
RI Li, Xiangyang/AAF-1260-2019
OI Li, Xiangyang/0000-0002-3944-4704
FU National Natural Science Foundation of China
FX No Statement Available
CR Amrani E, 2021, AAAI CONF ARTIF INTE, V35, P6644
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bai Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3556, DOI 10.1145/3474085.3475519
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Bengio S, 2015, ADV NEUR IN, V28
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1523, DOI 10.1109/ICCV48922.2021.00157
   Chen SZ, 2019, IEEE T MULTIMEDIA, V21, P2407, DOI 10.1109/TMM.2019.2896515
   Cheng X., 2021, arXiv
   Cheng ZY, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1069, DOI 10.1145/2911451.2914765
   Croitoru I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11563, DOI 10.1109/ICCV48922.2021.01138
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Dzabraev M, 2021, IEEE COMPUT SOC CONF, P3349, DOI 10.1109/CVPRW53098.2021.00374
   Fang H., 2021, arXiv
   Fu T.-J., 2021, arXiv
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao LL, 2022, IEEE T IMAGE PROCESS, V31, P202, DOI 10.1109/TIP.2021.3120867
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Huang ZC, 2020, Arxiv, DOI arXiv:2004.00849
   Kingma D. P., 2014, arXiv
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Li Dongxu, 2022, P IEEE CVF C COMP VI, P4953
   Li JH, 2021, ADV NEUR IN, V34
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Lin K, 2021, AAAI CONF ARTIF INTE, V35, P2047
   Liu S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11895, DOI 10.1109/ICCV48922.2021.01170
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Loshchilov I, 2017, P 5 INT C LEARN REPR
   Loukas A., 2020, IBER CONF INF SYST, DOI DOI 10.23919/cisti49556.2020.9141108
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Luo HS, 2020, Arxiv, DOI arXiv:2002.06353
   Pasunuru R, 2017, ARXIV170802300, P979, DOI [10.18653/v1/D17-1103, DOI 10.18653/V1/D17-1103]
   Patrick Mandela, 2020, P INT C LEARN REPR
   Perez-Martin J, 2021, IEEE WINT CONF APPL, P3038, DOI 10.1109/WACV48630.2021.00308
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rohrbach A, 2015, LECT NOTES COMPUT SC, V9358, P209, DOI 10.1007/978-3-319-24947-6_17
   Seo PH, 2021, PROC CVPR IEEE, P16872, DOI 10.1109/CVPR46437.2021.01660
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Thao Minh Le, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9969, DOI 10.1109/CVPR42600.2020.00999
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang AJP, 2022, PROC CVPR IEEE, P3303, DOI 10.1109/CVPR52688.2022.00331
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P2661, DOI 10.1109/TMM.2022.3149716
   Wang Z., 2022, PROC INT C LEARN REP
   Xia Zhuofan, 2022, P IEEECVF C COMPUTER, P4794, DOI DOI 10.48550/ARXIV.2201.00520
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu H, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P6787
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yang A, 2022, PROC CVPR IEEE, P16421, DOI 10.1109/CVPR52688.2022.01595
   Yang A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1666, DOI 10.1109/ICCV48922.2021.00171
   Yang B, 2021, AAAI CONF ARTIF INTE, V35, P3119
   Yang JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11542, DOI 10.1109/ICCV48922.2021.01136
   Yuan L., 2021, arXiv
   Zellers R, 2021, Advances in Neural Information Processing Systems, V34
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhao B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1177
   Zheng Y, 2022, IEEE T CIRC SYST VID, V32, P31, DOI 10.1109/TCSVT.2021.3058626
   Zhu L., 2020, P IEEE CVF C COMP VI, DOI 10.1109/CVPR42600.2020.00877
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 69
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8036
EP 8050
DI 10.1109/TMM.2022.3231108
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000019
DA 2024-07-18
ER

PT J
AU Fan, HN
   Hu, HM
   Liu, SL
   Lu, WQ
   Pu, SL
AF Fan, Haonan
   Hu, Hai-Miao
   Liu, Shuailing
   Lu, Weiqing
   Pu, Shiliang
TI Correlation Graph Convolutional Network for Pedestrian Attribute
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Task analysis; Lighting; Image resolution; Hair;
   Surveillance; Manifolds; Pedestrian Attribute Recognition; Correlation
   Graph Convolutional Network; Comprehensive relationship Framework; Inter
   Relation; Spatial Relation; Hierarchical Relation
ID IMAGE CLASSIFICATION; ATTENTION
AB The pedestrian attribute recognition aims at generating the structured description of pedestrian, which plays an important role in surveillance. However, it is difficult to achieve accurate recognition results due to diverse illumination, partial body occlusion and limited resolutions. Therefore, this paper proposes a comprehensive relationship framework for comprehensively describing and utilizing relations among attributes, describing different type of relations in the same dimension, and implementing complex transfers of relations in a GCN manner. This framework is named Correlation Graph Convolutional Network (CGCN). Based on the proposed framework, the feature vectors are built to associate attributes with image features and generate different relation matrices through self-attention among different feature vectors, describing different attribute relations. Then, we conduct multi-layer transfer of attribute relations by means of graph convolution, realizing complex utilization of attribute relations. In addition, the relations among attributes are fully exploited and two types of relations, namely the explicit and implicit relations, are proposed to be integrate into the proposed comprehensive relationship framework. The experimental results on RAP and PETA demonstrate that the recognition performance of the proposed CGCN can obviously outperform the state-of-the-arts, and moreover, the CGCN can achieve a better synergy with different relations.
C1 [Fan, Haonan; Hu, Hai-Miao; Liu, Shuailing; Lu, Weiqing] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Hu, Hai-Miao] Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310052, Peoples R China.
   [Pu, Shiliang] Hikvision, Hikvision Res Inst, Hangzhou 310000, Peoples R China.
C3 Beihang University; Beihang University
RP Hu, HM (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.; Hu, HM (corresponding author), Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310052, Peoples R China.
EM 809685324@qq.com; 1900704168@qq.com; 574168985@qq.com;
   frank0139@163.com; pushiliang@hikvision.com
RI Zhang, Yanyan/JFA-9161-2023
FU National Key Research and Development Program [2020AAA0130200]; National
   Natural Science Foundation of China [61772058]; Fundamental Research
   Funds for the Central Universities
FX This work was supported in part by the National Key Research and
   Development Program under Grant 2020AAA0130200, in part by the National
   Natural Science Foundation of China under Grant 61772058, and in part by
   the Fundamental Research Funds for the Central Universities. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Concetto Spampinato.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   [Anonymous], 2018, ARXIV180809102
   Chen YQ, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P114, DOI 10.5220/0006622901140122
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Chu WT, 2018, IEEE T MULTIMEDIA, V20, P2491, DOI 10.1109/TMM.2018.2801718
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jaha ES, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Ji Z, 2020, IEEE T CIRC SYST VID, V30, P1709, DOI 10.1109/TCSVT.2019.2904996
   Ji Z, 2019, PATTERN RECOGN LETT, V120, P89, DOI 10.1016/j.patrec.2019.01.010
   Ji Z, 2017, IEEE IMAGE PROC, P151, DOI 10.1109/ICIP.2017.8296261
   Kipf TN, 2017, INT C LEARN REPR
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Layne R, 2014, ADV COMPUT VIS PATT, P93, DOI 10.1007/978-1-4471-6296-4_5
   Li D, 2016, ARXIV161105603
   Li DW, 2019, IEEE T IMAGE PROCESS, V28, P1575, DOI 10.1109/TIP.2018.2878349
   Li DW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P111, DOI 10.1109/ACPR.2015.7486476
   Li JS, 2018, IEEE T IMAGE PROCESS, V27, P4651, DOI 10.1109/TIP.2018.2839521
   Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41
   Liu F, 2017, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2017.443
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Peng PX, 2016, LECT NOTES COMPUT SC, V9908, P336, DOI 10.1007/978-3-319-46493-0_21
   Prosser B., 2010, PERSON RE IDENTIFICA, V2, P1
   Sarafianos N, 2018, LECT NOTES COMPUT SC, V11215, P708, DOI 10.1007/978-3-030-01252-6_42
   Sarfraz M., 2017, BRIT MACH VIS C
   Simonyan K., 2014, 14091556 ARXIV
   Sudowe P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P329, DOI 10.1109/ICCVW.2015.51
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy C., 2015, PROC IEEE C COMPUT V, P1
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan ZC, 2019, IEEE T IMAGE PROCESS, V28, P6126, DOI 10.1109/TIP.2019.2919199
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang JY, 2017, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2017.65
   Yang S, 2019, IEEE T MULTIMEDIA, V21, P3194, DOI 10.1109/TMM.2019.2919469
   Yin GJ, 2019, PROC CVPR IEEE, P6234, DOI 10.1109/CVPR.2019.00640
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhao X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3177
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zhu JQ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P331, DOI 10.1109/ICCVW.2013.51
NR 47
TC 11
Z9 12
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 49
EP 60
DI 10.1109/TMM.2020.3045286
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300004
DA 2024-07-18
ER

PT J
AU Hu, PP
   Ho, ESL
   Munteanu, A
AF Hu, Pengpeng
   Ho, Edmond Shu-Lim
   Munteanu, Adrian
TI 3DBodyNet: Fast Reconstruction of 3D Animatable Human Body Shape From a
   Single Commodity Depth Camera
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Shape; Image reconstruction; Cameras; Solid
   modeling; Clothing; Deep learning; Human body shape; Body shape under
   clothing; depth camera; 3D Scanning; deep learning on point clouds
AB Knowledge about individual body shape has numerous applications in various domains such as healthcare, fashion and personalized entertainment. Most of the depth based whole body scanners need multiple cameras surrounding the user and requiring the user to keep a canonical pose strictly during capturing depth images. These scanning devices are expensive and need professional knowledge for operation. In order to make 3D scanning as easy-to-use and fast as possible, there is a great demand to simplify the process and to reduce the hardware requirements. In this paper, we propose a deep learning algorithm, dubbed 3DBodyNet, to rapidly reconstruct the 3D shape of human bodies using a single commodity depth camera. As easy-to-use as taking a photo using a mobile phone, our algorithm only needs two depth images of the front-facing and back-facing bodies. The proposed algorithm has strong operability since it is insensitive to the pose and the pose variations between the two depth images. It can also reconstruct an accurate body shape for users under tight/loose clothing. Another advantage of our method is the ability to generate an animatable human body model. Extensive experimental results show that the proposed method enables robust and easy-to-use animatable human body reconstruction, and outperforms the state-of-the-art methods with respect to running time and accuracy.
C1 [Hu, Pengpeng; Munteanu, Adrian] Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
   [Ho, Edmond Shu-Lim] Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
C3 Vrije Universiteit Brussel; Northumbria University
RP Hu, PP (corresponding author), Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium.
EM phu@etrovub.be; e.ho@northumbria.ac.uk; acmuntea@etrovub.be
RI Munteanu, Adrian/HKO-9955-2023; Ho, Edmond S. L./JDW-1835-2023; Hu,
   Pengpeng/ADG-7735-2022
OI Munteanu, Adrian/0000-0001-7290-0428; Ho, Edmond S.
   L./0000-0001-5862-106X; Hu, Pengpeng/0000-0002-2547-1517
FU Innoviris (project eTailor); Treedy
FX The authors would like to acknowledge the support of Innoviris (project
   eTailor), the close collaboration with Treedy's in the framework of this
   project.
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Cui Y., 2012, ACCV Workshops, P133
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fuster-Guilló A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20133690
   Groueix T, 2018, LECT NOTES COMPUT SC, V11206, P235, DOI 10.1007/978-3-030-01216-8_15
   Gschwandtner Michael, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P199, DOI 10.1007/978-3-642-24031-7_20
   Hasler N, 2009, COMPUT GRAPH-UK, V33, P211, DOI 10.1016/j.cag.2009.03.026
   Helten T, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P279, DOI 10.1109/3DV.2013.44
   Hu P., 2018, INT J CLOTHING SCI T
   Hu PP, 2021, IEEE T IND INFORM, V17, P3793, DOI 10.1109/TII.2020.3016591
   Hu PP, 2017, VISUAL COMPUT, V33, P961, DOI 10.1007/s00371-017-1388-3
   Jiang HY, 2019, IEEE I CONF COMP VIS, P5430, DOI 10.1109/ICCV.2019.00553
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P39
   King DB, 2015, ACS SYM SER, V1214, P1
   Kowalski M, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P318, DOI 10.1109/3DV.2015.43
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu ZB, 2017, IEEE T CYBERNETICS, V47, P695, DOI 10.1109/TCYB.2016.2524406
   Lo FPW, 2020, IEEE T IND INFORM, V16, P577, DOI 10.1109/TII.2019.2942831
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lu Y, 2021, IEEE T MULTIMEDIA, V23, P1136, DOI 10.1109/TMM.2020.2993948
   Lunscher N, 2018, IEEE COMPUT SOC CONF, P1208, DOI 10.1109/CVPRW.2018.00157
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Qi CR, 2017, ADV NEUR IN, V30
   Seitz S.M., 2006, IEEE COMP SOC C COMP, P519
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu FZ, 2019, PROC CVPR IEEE, P959, DOI 10.1109/CVPR.2019.00105
   Xu ZY, 2021, IEEE T MULTIMEDIA, V23, P1542, DOI 10.1109/TMM.2020.3001540
   Yang JL, 2016, LECT NOTES COMPUT SC, V9908, P439, DOI 10.1007/978-3-319-46493-0_27
   Young Min Kim, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1542, DOI 10.1109/ICCVW.2009.5457430
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zeng W, 2020, PROC CVPR IEEE, P7052, DOI 10.1109/CVPR42600.2020.00708
   Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zhou GL, 2021, IEEE T MULTIMEDIA, V23, P1630, DOI 10.1109/TMM.2020.3001533
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
NR 47
TC 18
Z9 18
U1 3
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2139
EP 2149
DI 10.1109/TMM.2021.3076340
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200028
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Lin, WS
   Ghinea, G
AF Lin, Weisi
   Ghinea, Gheorghita
TI Progress and Opportunities in Modelling Just-Noticeable Difference (JND)
   for Multimedia
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Solid modeling; Computational modeling; Data models;
   Psychology; Brain modeling; Multimedia systems; JND model; multimedia;
   human visual system; human perception; brain functioning; machine
   learning
ID VIDEO QUALITY; VISUAL-ATTENTION; DISTORTION; CONTRAST; PERSONALITY;
   PERCEPTION; MASKING; DISCRIMINATION; DATASET; PROFILE
AB Just-Noticeable Difference (JND) is the minimal amount of signal change that the human being is able to perceive. The human has five major sensing organs, namely, eyes, ears, nose, skin and tongue, and therefore JND exists for the corresponding five signal modalities and their derivatives. JND can play an important role in many multimedia applications and services, because these imperfect human perceptual characteristics may be turned into advantages for relevant system design, development and optimization. This paper starts off by giving a general description for JND concepts and the related statistical processes. Then, existing computational models for visual JND, which represent the majority of the related research so far, are to be reviewed systematically, with both handcrafted modeling and machine learning approaches. Furthermore, research attempts will be surveyed for JNDs for audio, smell, haptics and gustatory signals, as well as cross-modality/media efforts. Finally, possible future directions and opportunities are analysed and discussed.
C1 [Lin, Weisi] Nanyang Technol Univ, Dept Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Ghinea, Gheorghita] Brunel Univ London, Dept Comp Sci, Uxbridge UB8 3PH, Middx, England.
C3 Nanyang Technological University; Brunel University
RP Lin, WS (corresponding author), Nanyang Technol Univ, Dept Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM wslin@ntu.edu.sg; george.ghinea@brunel.ac.uk
RI Ghinea, Gheorghita/AAG-6770-2020; Lin, Wei/D-3353-2012; Lin,
   Weisi/A-3696-2011
OI Ghinea, Gheorghita/0000-0003-2578-5580; Lin, Weisi/0000-0001-9866-1947
CR Adelstein B., 2003, P 5 INT C MULTIMODAL, P73, DOI [DOI 10.1145/958432.958448, 10.1145/958432.958448]
   Ademoye OA, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957753
   AHUMADA AJ, 1992, P SOC PHOTO-OPT INS, V1666, P365
   Akan AK, 2020, IEEE IMAGE PROC, P2171, DOI 10.1109/ICIP40778.2020.9191090
   Allin S, 2002, 10TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P299, DOI 10.1109/HAPTIC.2002.998972
   Amelio M, 2016, TRENDS FOOD SCI TECH, V47, P64, DOI 10.1016/j.tifs.2015.11.001
   [Anonymous], WIKIPEDIA
   [Anonymous], 2015, PROC 7 INT WORKSHOP
   [Anonymous], 2002, ITU R RECOMMENDATION
   [Anonymous], 2007, PROC 23 SPRING C COM
   [Anonymous], 1996, BRAIN THEORY
   [Anonymous], 2020, 72 ANN TECHNOLOGY EN
   [Anonymous], 1988, Eye, brain, and vision
   [Anonymous], 2015, 1291702 ISOIEC JTC 1
   Arvaniti M., 2012, SEEING PERCEIVING, V25, P83
   Bachmann T., 2014, Visual masking: Studying perception, attention, and consciousness
   Bae SH, 2017, IEEE T CIRC SYST VID, V27, P1196, DOI 10.1109/TCSVT.2016.2539862
   Bae SH, 2014, IEEE T IMAGE PROCESS, V23, P3227, DOI 10.1109/TIP.2014.2327808
   Bauman B, 2017, FUTURE INTERNET, V9, DOI 10.3390/fi9030040
   Bejan A, 2019, EUR REV, V27, P187, DOI 10.1017/S1062798718000741
   Benali-Khoudja M, 2003, MHS2003: PROCEEDINGS OF 2003 INTERNATIONAL SYMPOSIUM ON MICROMECHATRONICS AND HUMAN SCIENCE, P153
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   Bovik A., 2020, PROC WORKSHOP AESTHE, DOI [10.1145/3423268.3423585, DOI 10.1145/3423268.3423585]
   Bradley AP, 1999, IEEE T IMAGE PROCESS, V8, P717, DOI 10.1109/83.760338
   Breen SP, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-43944-7
   Breslin PAS, 2013, CURR BIOL, V23, pR409, DOI 10.1016/j.cub.2013.04.010
   Brooks J, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376806
   BROWN B, 1989, OPTOMETRY VISION SCI, V66, P467, DOI 10.1097/00006324-198907000-00010
   Bushdid C, 2014, SCIENCE, V343, P1370, DOI 10.1126/science.1249168
   Camacho S, 2015, J FOOD SCI, V80, pS1583, DOI 10.1111/1750-3841.12922
   Carlson N.R., 2010, PSYCHOL SCI BEHAV
   Carrasco M, 2011, VISION RES, V51, P1484, DOI 10.1016/j.visres.2011.04.012
   Chalmers A., 2007, INT J VIRTUAL REALIT, V6, P1
   Chastrette M, 2002, OLFACTION, TASTE, AND COGNITION, P100, DOI 10.1017/CBO9780511546389.012
   Chen PF, 2021, IEEE T IMAGE PROCESS, V30, P3279, DOI 10.1109/TIP.2021.3060255
   Chen SY, 2006, INT J HUM-COMPUT ST, V64, P1200, DOI 10.1016/j.ijhcs.2006.08.010
   Chen WL, 2020, IEEE T CIRC SYST VID, V30, P334, DOI 10.1109/TCSVT.2019.2890878
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Cheng B, 2008, ELECTRON LETT, V44, P1098, DOI 10.1049/el:20081199
   Chiu YJ, 1999, IEEE T CIRC SYST VID, V9, P438, DOI 10.1109/76.754773
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Chou CH, 2010, IEEE T IMAGE PROCESS, V19, P2966, DOI 10.1109/TIP.2010.2052261
   Ciubotaru B, 2014, IEEE T BROADCAST, V60, P50, DOI 10.1109/TBC.2013.2290238
   Claypool M., 1998, END TO END QUALITY M
   Cohen MA, 2016, TRENDS COGN SCI, V20, P324, DOI 10.1016/j.tics.2016.03.006
   Comsa IS, 2020, IEEE MULTIMEDIA, V27, P27, DOI 10.1109/MMUL.2019.2954405
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Covaci A, 2020, IEEE T MULTIMEDIA, V22, P1249, DOI 10.1109/TMM.2019.2941274
   Covaci A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2378, DOI 10.1145/3343031.3350954
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Daly S, 2001, VISION MODELS AND APPLICATIONS TO IMAGE AND VIDEO PROCESSING, P179
   Demattè ML, 2009, CHEM SENSES, V34, P103, DOI 10.1093/chemse/bjn055
   DENEELING JND, 1994, MUSCLE NERVE, V17, P454, DOI 10.1002/mus.880170414
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Dhurandhar A., 2018, PROC 32 INT C NEURAL, P592
   Dong L, 2015, SIGNAL PROCESS-IMAGE, V33, P1, DOI 10.1016/j.image.2015.02.001
   Doukakis E, 2019, IEEE T VIS COMPUT GR, V25, P1865, DOI 10.1109/TVCG.2019.2898823
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   Eysenck M., 2005, COGNITIVE PSYCHOL, V5th
   Fan C, SIAT PJND PICTURE LE
   Fan CL, 2019, J VIS COMMUN IMAGE R, V62, P140, DOI 10.1016/j.jvcir.2019.04.016
   Fechner GT., 1966, ELEMENT PSYCHOPHYSIC, DOI DOI 10.1037/11304-026
   Ferster D, 2000, ANNU REV NEUROSCI, V23, P441, DOI 10.1146/annurev.neuro.23.1.441
   Feyzabadi S, 2013, IEEE T HAPTICS, V6, P309, DOI 10.1109/TOH.2013.4
   Friedman L., 2010, EXPERTSWEIGH IPHONE
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Fujisaki W., 2009, EXP BRAIN RES, V25, P482
   Fujisaki W, 2009, EXP BRAIN RES, V198, P245, DOI 10.1007/s00221-009-1870-x
   Gao L, 2016, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-016-0091-z
   Gao Y, 2013, J VIS COMMUN IMAGE R, V24, P700, DOI 10.1016/j.jvcir.2012.04.004
   Gaudrain E, 2018, EAR HEARING, V39, P226, DOI 10.1097/AUD.0000000000000480
   Genecov AM, 2014, IEEE HAPTICS SYM, P333, DOI 10.1109/HAPTICS.2014.6775477
   Ghinea G, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2617994
   Ghinea G, 2011, MULTIMED TOOLS APPL, V55, P601, DOI 10.1007/s11042-010-0581-4
   GILMORE MM, 1989, PERCEPT PSYCHOPHYS, V46, P555, DOI 10.3758/BF03208152
   Goettker A, 2018, P NATL ACAD SCI USA, V115, P2240, DOI 10.1073/pnas.1704799115
   GOLDBERG LR, 1990, J PERS SOC PSYCHOL, V59, P1216, DOI 10.1037/0022-3514.59.6.1216
   Green BG, 2004, CHEM SENSES, V29, P617, DOI 10.1093/chemse/bjh065
   GREEN BG, 1977, PERCEPT PSYCHOPHYS, V22, P331, DOI 10.3758/BF03199698
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Guan Y, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P394, DOI 10.1145/3341302.3342063
   Gulliver SR, 2010, ONLINE INFORM REV, V34, P39, DOI 10.1108/14684521011024119
   Guntuku S., 2014, PROC ASIAN C COMPUT, P3
   Guntuku SC, 2015, INT CONF AFFECT, P236, DOI 10.1109/ACII.2015.7344577
   Hadizadeh H, 2019, IEEE T IMAGE PROCESS, V28, P2242, DOI 10.1109/TIP.2018.2883893
   Hadizadeh H, 2017, IEEE SIGNAL PROC LET, V24, P1218, DOI 10.1109/LSP.2017.2717946
   Hansung Kim, 2020, Real VR - Immersive Digital Reality: How to Import the Real World into Head-Mounted Immersive Displays. Lecture Notes in Computer Science (LNCS 11900), P293, DOI 10.1007/978-3-030-41816-8_13
   Harrar V, 2008, EXP BRAIN RES, V186, P517, DOI 10.1007/s00221-007-1253-0
   Hidalgo J, 2011, RHINOLOGY, V49, P513, DOI 10.4193/Rhino11.013
   Höntsch I, 2002, IEEE T IMAGE PROCESS, V11, P213, DOI 10.1109/83.988955
   Hoshino S., 2011, P IEEE INT WORKSH TE, P1, DOI [10.1109/CQR.2011.5996082, DOI 10.1109/CQR.2011.5996082]
   Hu SD, 2016, INT CONF ACOUST SPEE, P1070, DOI 10.1109/ICASSP.2016.7471840
   Huang J, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P721, DOI [10.1109/VRW50115.2020.00-64, 10.1109/VRW50115.2020.00211]
   Hussain N., 2018, IEEE INT C MULTIMEDI, P1
   Hwang J., 2008, CROSS LAYER END TO E
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jacquot L, 2010, RHINOLOGY, V48, P281, DOI 10.4193/Rhino09.200
   Jakhetiya V, 2018, NEUROCOMPUTING, V275, P366, DOI 10.1016/j.neucom.2017.08.031
   JAYANT N, 1993, P IEEE, V81, P1385, DOI 10.1109/5.241504
   Jia YT, 2006, IEEE T CIRC SYST VID, V16, P820, DOI 10.1109/TCSVT.2006.877397
   Jiang QP, 2022, Arxiv, DOI arXiv:2108.05058
   Jin J, 2022, Arxiv, DOI arXiv:2102.08168
   Jin L., 2016, Electronic Imaging, V2016, P1
   Jovanovic B, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00057
   Kani-Zabihi E, 2021, MULTIMED TOOLS APPL, V80, P2377, DOI 10.1007/s11042-020-09757-x
   KELLY DH, 1979, J OPT SOC AM, V69, P1266, DOI 10.1364/JOSA.69.001266
   Ki S, 2018, IEEE T IMAGE PROCESS, V27, P3178, DOI 10.1109/TIP.2018.2818439
   Kim H, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P120, DOI 10.1109/VR.2019.8798247
   Kitayama S, 2003, PSYCHOL SCI, V14, P201, DOI 10.1111/1467-9280.02432
   Kolb B., 1996, FUNDAMENTALS HUMAN N, V4th
   Kurzweil R., 2005, SINGULARITY IS NEAR, P652
   Lechien JR, 2020, EUR ARCH OTO-RHINO-L, V277, P2251, DOI 10.1007/s00405-020-05965-1
   Lee B., 2004, J KOREAN I IND ENG, V30, P27
   LEGGE GE, 1980, J OPT SOC AM, V70, P1458, DOI 10.1364/JOSA.70.001458
   Li J., 2002, PROC 10 ACM MULTIMED, P592
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Lin H., 2020, QUALITY USER EXPERIE, V5, P1, DOI DOI 10.1007/S41233-020-00034-1
   Lin HA, 2020, PARASITOLOGY, V147, P1577, DOI 10.1017/S0031182020001183
   Lin JY, 2015, PROC SPIE, V9599, DOI 10.1117/12.2188389
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Lin WS, 2005, IEEE T CIRC SYST VID, V15, P900, DOI 10.1109/TCSVT.2005.848345
   Lin WS, 2006, SIGN PROC COMMUN SER, V28, P281
   Liu AM, 2010, IEEE T CIRC SYST VID, V20, P1648, DOI 10.1109/TCSVT.2010.2087432
   Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743
   Liu TJ, 2013, APSIPA TRANS SIGNAL, V2, DOI 10.1017/ATSIP.2013.5
   Liu TJ, 2013, IEEE T IMAGE PROCESS, V22, P1793, DOI 10.1109/TIP.2012.2236343
   Liu XH, 2018, LECT NOTES COMPUT SC, V11164, P458, DOI 10.1007/978-3-030-00776-8_42
   Long Marshall., 2014, ARCHITECTURAL ACOUST, VSecond, P81
   Lu ZK, 2005, IEEE T IMAGE PROCESS, V14, P1928, DOI 10.1109/TIP.2005.854478
   Lyon DM, 1923, J PHARMACOL EXP THER, V21, P229
   Maggioni E, 2020, ACM T COMPUT-HUM INT, V27, DOI 10.1145/3402449
   Maiero J, 2019, IEEE T HAPTICS, V12, P483, DOI 10.1109/TOH.2019.2911519
   MALONEY RK, 1987, J OPT SOC AM A, V4, P2336, DOI 10.1364/JOSAA.4.002336
   Manocha P, 2020, Arxiv, DOI arXiv:2001.04460
   MCBURNEY DH, 1967, PERCEPT PSYCHOPHYS, V2, P175, DOI 10.3758/BF03213046
   MCKEE SP, 1984, VISION RES, V24, P25, DOI 10.1016/0042-6989(84)90140-8
   MCL-JCI, JND DAT
   MCL-JCV, JND DAT
   McShefferty D, 2015, TRENDS HEAR, V19, DOI 10.1177/2331216515572316
   Mesfin G, 2020, MULTIMED TOOLS APPL, V79, P7987, DOI 10.1007/s11042-019-08473-5
   Mesfin G, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303080
   MICHON JA, 1966, PERCEPT PSYCHOPHYS, V1, P329
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Moein ST, 2020, INT FORUM ALLERGY RH, V10, P944, DOI 10.1002/alr.22587
   Murray N, 2017, INT WORK QUAL MULTIM
   Mushtaq MS, 2014, IEEE ICC, P2289, DOI 10.1109/ICC.2014.6883664
   Nakajima M, 2020, IEEE/SICE I S SYS IN, P1238, DOI [10.1109/sii46433.2020.9025959, 10.1109/SII46433.2020.9025959]
   Nakano K, 2019, INT SYM MIX AUGMENT, P212, DOI 10.1109/ISMAR.2019.000-1
   Nakatani M, 2018, LECT NOTES ELECTR EN, V432, P437, DOI 10.1007/978-981-10-4157-0_73
   Narciso D, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3380903
   Narumi T, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P93
   Narumi T, 2011, P IEEE VIRT REAL ANN, P127, DOI 10.1109/VR.2011.5759450
   Netravali A.N., 1988, DIGITAL PICTURES REP
   Niijima A, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382849
   Niimura Y, 2003, P NATL ACAD SCI USA, V100, P12235, DOI 10.1073/pnas.1635157100
   Nilsson DE, 2013, VISUAL NEUROSCI, V30, P5, DOI 10.1017/S0952523813000035
   NILSSON DE, 1994, P ROY SOC B-BIOL SCI, V256, P53, DOI 10.1098/rspb.1994.0048
   Nisbett RE, 2005, TRENDS COGN SCI, V9, P467, DOI 10.1016/j.tics.2005.08.004
   Nothdurft HC, 2000, VISION RES, V40, P1183, DOI 10.1016/S0042-6989(00)00031-6
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oleszkiewicz A, 2020, BEHAV NEUROSCI, V134, P394, DOI 10.1037/bne0000378
   Olson HarryFerdinand., 1967, Music, Physics and Engineering
   Osorio D, 2008, VISION RES, V48, P2042, DOI 10.1016/j.visres.2008.06.018
   Painter T, 2000, P IEEE, V88, P451, DOI 10.1109/5.842996
   Parise C, 2008, NEUROSCI LETT, V442, P257, DOI 10.1016/j.neulet.2008.07.010
   Parise CV, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0005664
   Pause BM, 1998, J RES PERS, V32, P510, DOI 10.1006/jrpe.1998.2228
   Peiris RL, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5452, DOI 10.1145/3025453.3025824
   Pelli DG, 2013, VISION RES, V90, P10, DOI 10.1016/j.visres.2013.04.015
   Pickering GJ, 2016, CHEMOSENS PERCEPT, V9, P37, DOI 10.1007/s12078-016-9203-5
   Preechayasomboon P, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376512
   Raheel A, 2021, INFORM FUSION, V65, P37, DOI 10.1016/j.inffus.2020.08.007
   Rainer B, 2012, INT WORK QUAL MULTIM, P278, DOI 10.1109/QoMEX.2012.6263842
   Ramalho J., 2013, P 11 EUROPEAN C INTE, P107, DOI DOI 10.1145/2465958.2465969
   Ranasinghe N, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1731, DOI 10.1145/3025453.3025723
   Rizzi A, 2017, TEXT INST BOOK SER, P23, DOI 10.1016/B978-0-08-101270-3.00002-3
   ROBINSON DA, 1965, J PHYSIOL-LONDON, V180, P569, DOI 10.1113/jphysiol.1965.sp007718
   Saleme EB, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3319853
   Saleme EB, 2019, IEEE MULTIMEDIA, V26, P66, DOI 10.1109/MMUL.2018.2873565
   Schmitt M, 2018, IEEE T MULTIMEDIA, V20, P1781, DOI 10.1109/TMM.2017.2777466
   Scott MJ, 2016, IEEE T MULTIMEDIA, V18, P1796, DOI 10.1109/TMM.2016.2574623
   Shen XL, 2021, IEEE T IMAGE PROCESS, V30, P26, DOI 10.1109/TIP.2020.3029428
   Shepherd GM, 2004, PLOS BIOL, V2, P572, DOI 10.1371/journal.pbio.0020146
   Sherrick C.E., 1986, HDB PERCEPTION HUMAN
   Silva JM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457451
   Spence C, 2011, ATTEN PERCEPT PSYCHO, V73, P971, DOI 10.3758/s13414-010-0073-7
   Spence C, 2017, INT J HUM-COMPUT ST, V107, P62, DOI 10.1016/j.ijhcs.2017.06.003
   STEVENS SS, 1957, PSYCHOL REV, V64, P153, DOI 10.1037/h0046162
   STILLMAN JA, 1993, J ACOUST SOC AM, V93, P425, DOI 10.1121/1.405622
   SWITKES E, 1988, J OPT SOC AM A, V5, P1149, DOI 10.1364/JOSAA.5.001149
   Szczesniak AS, 2002, FOOD QUAL PREFER, V13, P215, DOI 10.1016/S0950-3293(01)00039-8
   Szigeti T., 2013, End-to-End QoS Network Design: Quality of Service for Rich-Media Cloud Networks
   Thaddeus-Johns Josie, 2017, GUARDIAN
   Thompson B, 2007, J VISION, V7, DOI 10.1167/7.10.12
   Tong HHY, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P428, DOI 10.1109/ICIP.1998.999032
   van der Helm PA, 2010, ATTEN PERCEPT PSYCHO, V72, P1854, DOI 10.3758/APP.72.7.1854
   VANNES FL, 1967, J OPT SOC AM, V57, P401, DOI 10.1364/JOSA.57.000401
   Vatakis A, 2008, EXP BRAIN RES, V185, P521, DOI 10.1007/s00221-007-1168-9
   Vatakis A, 2006, BRAIN RES, V1111, P134, DOI 10.1016/j.brainres.2006.05.078
   Vater C, 2020, VISION RES, V171, P46, DOI 10.1016/j.visres.2020.04.006
   VideoSet, JND DAT
   VVC, JND DAT
   Waldin N, 2017, COMPUT GRAPH FORUM, V36, P467, DOI 10.1111/cgf.13141
   Waltl M, 2013, SIGNAL PROCESS-IMAGE, V28, P136, DOI 10.1016/j.image.2012.10.009
   Waltl M, 2012, INT WORK QUAL MULTIM, P115, DOI 10.1109/QoMEX.2012.6263841
   Wan WB, 2020, PATTERN RECOGN LETT, V130, P157, DOI 10.1016/j.patrec.2018.08.009
   Wandell B. A, 1995, Foundations of vision
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009
   Wang HK, 2020, IEEE SIGNAL PROC LET, V27, P181, DOI 10.1109/LSP.2019.2957647
   Wang QJ, 2020, J EXP PSYCHOL HUMAN, V46, P1118, DOI 10.1037/xhp0000820
   Wang SQ, 2016, IEEE T IMAGE PROCESS, V25, P3838, DOI 10.1109/TIP.2016.2573597
   Watson A.B., 1993, SID INT S, V24, P946
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Wilson G, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2555
   Wu HR, 2013, P IEEE, V101, P2025, DOI 10.1109/JPROC.2013.2262911
   Wu JJ, 2019, FRONT COMPUT SCI-CHI, V13, P4, DOI 10.1007/s11704-016-6213-z
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Wu Y., 2020, PROC IEEE INT C MULT, P1
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
   Yaguchi A, 2020, PERCEPTION, V49, P405, DOI 10.1177/0301006620907827
   Yanagida Y., 2019, P 2019 IEEE INT S OL, P1, DOI DOI 10.1109/ISOEN.2019.8823180
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Yang XK, 2005, IEEE T CIRC SYST VID, V15, P742, DOI 10.1109/TCSVT.2005.848313
   Yost WilliamA., 1994, FUNDAMENTALS HEARING, V3rd
   Yuan D, 2019, IEEE ACCESS, V7, P29014, DOI 10.1109/ACCESS.2019.2901342
   Zeng ZP, 2019, IEEE ACCESS, V7, P132111, DOI 10.1109/ACCESS.2019.2939569
   Zerkus M., 1993, Virtual Real. Syst, V1, P88
   Zhai GT, 2008, SIGNAL PROCESS-IMAGE, V23, P417, DOI 10.1016/j.image.2008.04.007
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang F, 2011, IEEE T IMAGE PROCESS, V20, P3207, DOI 10.1109/TIP.2011.2146263
   Zhang L., 2013, Selective visual attention: Computational models and applications
   Zhang XH, 2005, SIGNAL PROCESS, V85, P795, DOI 10.1016/j.sigpro.2004.12.002
   Zhang XH, 2008, J VIS COMMUN IMAGE R, V19, P30, DOI 10.1016/j.jvcir.2007.06.001
   Zhang XF, 2022, IEEE T CIRC SYST VID, V32, P2746, DOI 10.1109/TCSVT.2021.3096528
   Zhang XF, 2020, IEEE T IMAGE PROCESS, V29, P3777, DOI 10.1109/TIP.2020.2965994
   Zhang XF, 2019, IEEE T IMAGE PROCESS, V28, P1163, DOI 10.1109/TIP.2018.2874283
   Zhang XF, 2017, IEEE SIGNAL PROC LET, V24, P96, DOI 10.1109/LSP.2016.2641456
   Zhang Y., 2020, PROC IEEE INT C MULT, P1
   Zhang Y, 2022, IEEE T CIRC SYST VID, V32, P1197, DOI 10.1109/TCSVT.2021.3076224
   Zhu Y, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183512
   Zhu Y, 2015, COMPUT HUM BEHAV, V49, P412, DOI 10.1016/j.chb.2015.02.054
   ZWISLOCKI J, 1956, J ACOUST SOC AM, V28, P860, DOI 10.1121/1.1908495
NR 245
TC 12
Z9 13
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3706
EP 3721
DI 10.1109/TMM.2021.3106503
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400004
DA 2024-07-18
ER

PT J
AU Liu, YH
   Xie, JK
   Qiao, Y
   Tang, Y
   Yang, X
AF Liu, Yuhao
   Xie, Jiake
   Qiao, Yu
   Tang, Yong
   Yang, Xin
TI Prior-Induced Information Alignment for Image Matting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image matting; Gaussian distribution; information alignment
AB Image matting is an ill-posed problem that aims to estimate the opacity of foreground pixels in an image. However, most existing deep learning-based methods still suffer from the coarse-grained details. In general, these algorithms are incapable of felicitously distinguishing the degree of exploration between deterministic domains (e.g. certain FG and RC pixels) and undetermined domains (e.g. uncertain in-between pixels), or inevitably lose information in the continuous sampling process, leading to a sub-optimal result. In this paper, we propose a novel network named Prior-Induced Information Alignment Matting Network (PIIAMatting), which can efficiently model the distinction of pixel-wise response maps and the correlation of layer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation mechanism (DGM) and an Information Alignment strategy (IA). Specifically, the DGM can dynamically acquire a pixel-wise domain response map learned from the prior distribution. The response map can present the relationship between the opacity variation and the convergence process during training. On the other hand, the IA comprises an Information Match Module (IMM) and an Information Aggregation Module (IAM), jointly scheduled to match and aggregate the adjacent layer-wise features adaptively. Resides, we also develop a Multi-Scale Refinement (MSR) module to integrate multi-scale receptive field information at the refinement stage to recover the fluctuating appearance details. Extensive quantitative and qualitative evaluations demonstrate that the proposed PIIAMatting performs favourably against state-of-the-art image matting methods on the Alphamatting.com , Composition-1 K and Distinctions-646 dataset.
C1 [Liu, Yuhao; Qiao, Yu; Yang, Xin] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
   [Xie, Jiake; Tang, Yong] Winroad Holdings Ltd, Hangzhou 310000, Peoples R China.
C3 Dalian University of Technology
RP Yang, X (corresponding author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.; Tang, Y (corresponding author), Winroad Holdings Ltd, Hangzhou 310000, Peoples R China.
EM yuhaoLiu7456@gmail.com; kelisiyaer@gmail.com; coachqiao2018@gmail.com;
   ty@road.win; xinyang@dlut.edu.cn
OI Liu, Yuhao/0000-0003-0550-4788; Qiao, Yu/0000-0001-7205-2924; Tang,
   Yong/0000-0002-3446-1919; , Xin/0000-0002-8046-722X; xie, jia
   ke/0000-0002-2009-9891
FU National Natural Science Foundation of China [91748104, 61972067];
   Innovation Technology Funding of Dalian [2018J11CY010, 2020JJ26GX036]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 91748104 and 61972067, in part by the
   Innovation Technology Funding of Dalian under Projects 2018J11CY010 and
   2020JJ26GX036, and in part by PicUP.Ai project of the Winroad Holdings
   Ltd.
CR Aksoy Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201275
   Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   [Anonymous], 2019, P IEEECVF INT C COMP
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen GY, 2019, INT J COMPUT VISION, V127, P1527, DOI 10.1007/s11263-019-01202-3
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610
   Cho D, 2019, IEEE T IMAGE PROCESS, V28, P1054, DOI 10.1109/TIP.2018.2872925
   Cho D, 2017, IEEE T PATTERN ANAL, V39, P1504, DOI 10.1109/TPAMI.2016.2606397
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Ehrlich M, 2019, IEEE I CONF COMP VIS, P3483, DOI 10.1109/ICCV.2019.00358
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng XX, 2016, LECT NOTES COMPUT SC, V9906, P204, DOI 10.1007/978-3-319-46475-6_13
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Goodfellow I., 2014, PROC NEURAL INF PRO, P2680
   Grady L, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P423
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He SQ, 2018, IEEE T MULTIMEDIA, V20, P496, DOI 10.1109/TMM.2017.2740026
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Karacan L, 2015, IEEE I CONF COMP VIS, P424, DOI 10.1109/ICCV.2015.56
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kingma DP., 2014, ADAM METHOD STOCHAST
   Lee P., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2193, DOI 10.1109/CVPR.2011.5995665
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li HL, 2009, IEEE T MULTIMEDIA, V11, P77, DOI 10.1109/TMM.2008.2008922
   Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu PY, 2020, PROC CVPR IEEE, P2105, DOI 10.1109/CVPR42600.2020.00218
   Lu H., IEEE T PATTERN ANAL
   Lutz S., 2018, BMVC
   Paszke A, 2019, ADV NEUR IN, V32
   Qiao Y, 2020, P IEEE CVF C COMP VI, P13676
   Qiao Y, 2020, COMPUT GRAPH FORUM, V39, P565, DOI 10.1111/cgf.14168
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sengupta S, 2020, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR42600.2020.00236
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Wang J, 2005, IEEE I CONF COMP VIS, P936
   Wang J, 2007, PROC CVPR IEEE, P281
   Wang Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P999
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Yang X., 2020, ACM T MULTIM COMPUT, V16, P1
   Yang X, 2018, ADV NEUR IN, V31
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
   Zheng YJ, 2009, IEEE I CONF COMP VIS, P889, DOI 10.1109/ICCV.2009.5459326
   Zhu HY, 2016, IEEE T MULTIMEDIA, V18, P1516, DOI 10.1109/TMM.2016.2571629
NR 56
TC 6
Z9 6
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2727
EP 2738
DI 10.1109/TMM.2021.3087007
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Su, HN
   Yu, L
   Jung, C
AF Su, Haonan
   Yu, Long
   Jung, Cheolkon
TI Joint Contrast Enhancement and Noise Reduction of Low Light Images Via
   JND Transform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Noise reduction; Transforms; Image color analysis;
   Colored noise; Adaptation models; Lighting; Contrast enhancement;
   luminance adaptation; noise reduction; JND transform; visual masking;
   HVS response model; Weber's law
ID HISTOGRAM EQUALIZATION; SPARSE; ALGORITHM; SIGNAL; MODEL
AB Low light images suffer from a low dynamic range and severe noise due to low signal-to-noise ratio (SNR). In this paper, we propose joint contrast enhancement and noise reduction of low light images via just-noticeable-difference (JND) transform. We adopt the JND transform to achieve both contrast enhancement and noise reduction based on human visual perception. First, we generate a JND map based on an the human visual system (HVS) response model from foreground and background luminance, called JND transform. Second, for base image, we perform perceptual contrast enhancement based on luminance adaptation to effectively allocate a dynamic range to each gray level while preventing under enhancement (tone distortion) and over-enhancement. Third, we refine the JND map using Weber's law, luminance adaptation and visual masking. Weber's law enhances the JND map based on the luminance variation after contrast enhancement. Luminance adaptation suppresses noise for smooth regions, while visual masking enforces detail enhancement for textural regions. Fourth, we perform inverse JND transform to generate the enhanced luma channel from the JND map and base image. Finally, we conduct chroma denoising by transferring texture information of the enhanced luma channel to the chroma channels with guided filtering. Experimental results show that the proposed method achieves both contrast enhancement and noise reduction for low light images as well as outperforms state-of-the-art methods in terms of quantitative measurements.
C1 [Su, Haonan; Yu, Long; Jung, Cheolkon] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University
RP Jung, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM haonansu@stu.xidian.edu.cn; lyu@stu.xidian.edu.cn; zhengzk@xidian.cdu.cn
OI Su, Haonan/0000-0002-5481-1082
FU National Natural Science Foundation of China [61872280]; International
   S&T Cooperation Program of China [2014DFG12780]; China Postdoctoral
   Science Foundation [2019M663929XB]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61872280, in part by the International
   S&T Cooperation Program of China under Grant 2014DFG12780, and in part
   by the China Postdoctoral Science Foundation 2019M663929XB.
CR Agaian SS, 2001, IEEE T IMAGE PROCESS, V10, P367, DOI 10.1109/83.908502
   [Anonymous], 1993, P 4 EUR WORKSH REND
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cepeda-Negrete J, 2018, IEEE ACCESS, V6, P14935, DOI 10.1109/ACCESS.2017.2763898
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Dabov K, 2007, IEEE IMAGE PROC, P313, DOI 10.1109/icip.2007.4378954
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399
   Fu HY, 2012, INT C PATT RECOG, P3656
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gu K, 2014, IEEE IMAGE PROC, P506, DOI 10.1109/ICIP.2014.7025101
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hitaj B, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P603, DOI 10.1145/3133956.3134012
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Huang TH, 2013, IEEE T IMAGE PROCESS, V22, P4587, DOI 10.1109/TIP.2013.2272517
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   JAYANT N, 1992, IEEE J SEL AREA COMM, V10, P796, DOI 10.1109/49.138986
   Jiang XS, 2013, IEEE IMAGE PROC, P553, DOI 10.1109/ICIP.2013.6738114
   Dinh KQ, 2016, IEEE SIGNAL PROC LET, V23, P1071, DOI 10.1109/LSP.2016.2580711
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liang YX, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P3132, DOI 10.1145/3292500.3330646
   Ling ZG, 2015, IET IMAGE PROCESS, V9, P1012, DOI 10.1049/iet-ipr.2014.0580
   Liu XM, 2015, INT CONF ACOUST SPEE, P2274, DOI 10.1109/ICASSP.2015.7178376
   Loza A, 2013, DIGIT SIGNAL PROCESS, V23, P1856, DOI 10.1016/j.dsp.2013.06.002
   Luisier F, 2008, IEEE T IMAGE PROCESS, V17, P482, DOI 10.1109/TIP.2008.919370
   Maciej Z., 2017, P INT C LEARN REPR W
   Malm H, 2007, IEEE I CONF COMP VIS, P1395
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Panetta K, 2013, IEEE T CONSUM ELECTR, V59, P643, DOI 10.1109/TCE.2013.6626251
   Paris S, 2008, FOUND TRENDS COMPUT, V4, P1, DOI 10.1561/0600000020
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   Pei SC, 2017, IEEE T MULTIMEDIA, V19, P1956, DOI 10.1109/TMM.2017.2688924
   Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082
   Pizurica A, 2006, IEEE T IMAGE PROCESS, V15, P654, DOI 10.1109/TIP.2005.863698
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Shih KT, 2016, IEEE T MULTIMEDIA, V18, P300, DOI 10.1109/TMM.2015.2503918
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu JJ, 2016, INT CONF ACOUST SPEE, P1581, DOI 10.1109/ICASSP.2016.7471943
   Wu XL, 2011, IEEE T IMAGE PROCESS, V20, P1262, DOI 10.1109/TIP.2010.2092438
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Yang GZ, 1996, IMAGE VISION COMPUT, V14, P135, DOI 10.1016/0262-8856(95)01047-5
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Yu L, 2020, INT CONF ACOUST SPEE, P2658, DOI [10.1109/icassp40776.2020.9053027, 10.1109/ICASSP40776.2020.9053027]
   Yu L, 2018, IEEE ACCESS, V6, P36132, DOI 10.1109/ACCESS.2018.2848671
   Zhang H., 2011, 2011 4 INT C IM SIGN, P704
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang XH, 2005, SIGNAL PROCESS, V85, P795, DOI 10.1016/j.sigpro.2004.12.002
   Zhang XD, 2012, INT C PATT RECOG, P2034
NR 61
TC 8
Z9 8
U1 4
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 17
EP 32
DI 10.1109/TMM.2020.3043106
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300002
DA 2024-07-18
ER

PT J
AU Tsai, FS
   Chang, WW
   Lee, CC
AF Tsai, Fu-Sheng
   Chang, Wei-Wen
   Lee, Chi-Chun
TI A Social Condition-Enhanced Network for Recognizing Power Distance Using
   Expressive Prosody and Intrinsic Brain Connectivity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cultural differences; Global communication; Speech recognition; Loss
   measurement; Reliability; Quality of experience; Organizations; Culture
   dimensions; fMRI; prosody; center-loss embedding; power distance index
ID ANTERIOR CINGULATE GYRUS; EMOTION RECOGNITION; DECISION-MAKING; CULTURE;
   FEATURES; IDENTIFICATION; PERSONALITY; COGNITION; BEHAVIOR
AB Culture is the social norm that often dictates a person's thoughts, decision-making, and social behaviors during interaction at an individual level. In this study, we present a computational framework that automatically assesses an individual culture attribute of power distance (PDI), i.e., the measure to describe one's acceptance of social status, power and authority in organizations through multimodal modeling of a participant's expressive prosodic structures and brain connectivity using a social condition-enhanced network. In specific, we propose a joint learning approach of center-loss embedding network architecture that learns to "centerize" the embedding space given a particular social interaction condition to enhance the PDI discriminability of the representation. Our proposed method achieves 88.5% and 73.1% in binary classification task of recognizing low versus high power distance on prosodic and fMRI modality separately. After performing multimodal fusion, it improves to 96.2% of 2-class recognition rate (7.7% relative improvement). Further analyses reveal that average and standard deviation of speech energy are significantly correlated with power distance index; the right middle cingulate cortex (MCC) of brain region achieves the best recognition accuracy demonstrating its role in processing a person's belief about power distance.
C1 [Tsai, Fu-Sheng; Lee, Chi-Chun] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
   [Tsai, Fu-Sheng; Chang, Wei-Wen] MOST Joint Res Ctr AI Technol, Taipei, Taiwan.
   [Tsai, Fu-Sheng; Chang, Wei-Wen] All Vista Healthcare, Taipei, Taiwan.
   [Lee, Chi-Chun] Natl Taiwan Normal Univ, Dept Int Human Resource Dev, Taipei 106, Taiwan.
C3 National Tsing Hua University; National Taiwan Normal University
RP Lee, CC (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
EM fstsai@gapp.nthu.edu.tw; changw@ntnu.edu.tw; cclee@ee.nthu.edu.tw
OI /0000-0002-8786-0366; Lee, Chi-Chun/0000-0003-0186-4321
CR Adolphs R, 2001, CURR OPIN NEUROBIOL, V11, P231, DOI 10.1016/S0959-4388(00)00202-6
   [Anonymous], 2006, Praat: Doing phonetics by computer
   [Anonymous], 2009, ROLE PROSODY AFFECTI
   Apps MAJ, 2016, NEURON, V90, P692, DOI 10.1016/j.neuron.2016.04.018
   Apps MAJ, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00251
   Auberge V., 2002, SPEECH PROSODY 2002
   Bandura A, 2002, APPL PSYCHOL-INT REV, V51, P269, DOI 10.1111/1464-0597.00092
   Barbulescu A, 2017, SPEECH COMMUN, V95, P78, DOI 10.1016/j.specom.2017.07.003
   Bassett DS, 2015, NAT NEUROSCI, V18, P744, DOI 10.1038/nn.3993
   Batrinca L, 2016, IEEE T MULTIMEDIA, V18, P659, DOI 10.1109/TMM.2016.2522763
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   BOCHNER S, 1994, J CROSS CULT PSYCHOL, V25, P233, DOI 10.1177/0022022194252005
   Brockner J, 2001, J EXP SOC PSYCHOL, V37, P300, DOI 10.1006/jesp.2000.1451
   Bulut M., 2004, P 6 INT C MULT INT, P205
   Bzdok D, 2016, NEUROSCI BIOBEHAV R, V68, P319, DOI 10.1016/j.neubiorev.2016.02.024
   Cai J, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P443, DOI 10.1109/MIPR.2019.00089
   Cai J, 2018, IEEE INT CONF AUTOMA, P302, DOI 10.1109/FG.2018.00051
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Chiao JY, 2009, NEUROPSYCHOLOGIA, V47, P354, DOI 10.1016/j.neuropsychologia.2008.09.023
   Crane E, 2007, LECT NOTES COMPUT SC, V4738, P95
   Critchley HD, 2000, BRAIN, V123, P2203, DOI 10.1093/brain/123.11.2203
   Dai DY, 2019, INT CONF ACOUST SPEE, P7405, DOI [10.1109/icassp.2019.8683765, 10.1109/ICASSP.2019.8683765]
   de Mesquita CPB, 2016, ECOGRAPHY, V39, P970, DOI 10.1111/ecog.01797
   Dehak N, 2007, IEEE T AUDIO SPEECH, V15, P2095, DOI 10.1109/TASL.2007.902758
   Eisenberger NI, 2004, TRENDS COGN SCI, V8, P294, DOI 10.1016/j.tics.2004.05.010
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Glenn AL, 2009, MOL PSYCHIATR, V14, P5, DOI 10.1038/mp.2008.104
   Grawunder Sven, 2014, P INT C SPEECH PROS, P159, DOI DOI 10.21437/SPEECHPROSODY.2014-20
   Gunes H., 2008, AFFECT COMPUT INTECH
   Guntuku SC, 2015, INT CONF AFFECT, P236, DOI 10.1109/ACII.2015.7344577
   Hadland KA, 2003, NEUROPSYCHOLOGIA, V41, P919, DOI 10.1016/S0028-3932(02)00325-1
   Harms MB, 2010, NEUROPSYCHOL REV, V20, P290, DOI 10.1007/s11065-010-9138-6
   HOFSTEDE G, 1986, INT J INTERCULT REL, V10, P301, DOI 10.1016/0147-1767(86)90015-5
   Hofstede G., 1984, Asia Pacific Journal of Management, V1, P81, DOI [10.1007/BF01733682, DOI 10.1007/BF01733682]
   Hofstede G., 2010, CULTURES ORG SOFTWAR
   Hofstede G., 1991, Cultures and organizations, DOI DOI 10.1016/S0005-7967(02)00184-5
   House R. J., 2004, CULTURE LEADERSHIP O
   HURLEY DS, 1992, APPL LINGUIST, V13, P258
   Jaimes A, 2006, IEEE MULTIMEDIA, V13, P12, DOI 10.1109/MMUL.2006.8
   Jian MN, 2018, IEEE GLOB COMM CONF, DOI 10.1145/3301506.3301511
   Karg M, 2013, IEEE T AFFECT COMPUT, V4, P341, DOI 10.1109/T-AFFC.2013.29
   Keesing Felix., 1958, Cultural Anthropology: The Science of Custom
   Kirkman BL, 2009, ACAD MANAGE J, V52, P744, DOI 10.5465/AMJ.2009.43669971
   Koski JE, 2015, SOC NEUROSCI-UK, V10, P527, DOI 10.1080/17470919.2015.1013223
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Liew SL, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0016901
   Lin CY, 2005, INT CONF ACOUST SPEE, P601
   McColl D, 2016, J INTELL ROBOT SYST, V82, P101, DOI 10.1007/s10846-015-0259-2
   McGraw LA, 2010, TRENDS NEUROSCI, V33, P103, DOI 10.1016/j.tins.2009.11.006
   Mixdorff H., 2015, P 16 ANN C INT SPEEC
   Mumford JA, 2012, SOC COGN AFFECT NEUR, V7, P738, DOI 10.1093/scan/nss059
   Park G, 2015, J PERS SOC PSYCHOL, V108, P934, DOI 10.1037/pspp0000020
   Picard R.W., 2000, Affective Computing
   Picard RW, 2001, IEEE T PATTERN ANAL, V23, P1175, DOI 10.1109/34.954607
   Rossion B, 2003, BRAIN, V126, P2381, DOI 10.1093/brain/awg241
   Rudebeck PH, 2006, SCIENCE, V313, P1310, DOI 10.1126/science.1128197
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Scholtes VA, 2011, INJURY, V42, P236, DOI 10.1016/j.injury.2010.11.042
   Schwartz S. H., 1994, Individualism and collectivism: Theory, method, and applications, P85
   Scott MJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P481, DOI 10.1145/2733373.2806254
   Scott MJ, 2016, IEEE T MULTIMEDIA, V18, P1796, DOI 10.1109/TMM.2016.2574623
   Shine JM, 2016, P NATL ACAD SCI USA, V113, P9888, DOI 10.1073/pnas.1604898113
   Smith PB, 1996, J CROSS CULT PSYCHOL, V27, P231, DOI 10.1177/0022022196272006
   Stevanovic D, 2015, EPIDEMIOL PSYCH SCI, V24, P323, DOI 10.1017/S2045796014000201
   Szucs D, 2020, NEUROIMAGE, V221, DOI 10.1016/j.neuroimage.2020.117164
   Taras V, 2012, J WORLD BUS, V47, P329, DOI 10.1016/j.jwb.2011.05.001
   Taras V, 2010, J APPL PSYCHOL, V95, P405, DOI 10.1037/a0018938
   TREVINO LK, 1986, ACAD MANAGE REV, V11, P601, DOI 10.2307/258313
   Tsai FS, 2018, INTERSPEECH, P436
   Tylor E. B., 1871, J MURRAY, V2
   Tzourio-Mazoyer N, 2002, NEUROIMAGE, V15, P273, DOI 10.1006/nimg.2001.0978
   Uskul AK, 2016, EMOTION, V16, P11, DOI 10.1037/emo0000110
   Van Bezooijen R, 1999, J LANG SOC PSYCHOL, V18, P31, DOI 10.1177/0261927X99018001003
   Varini P, 2017, IEEE T MULTIMEDIA, V19, P2832, DOI 10.1109/TMM.2017.2705915
   Weick M, 2011, CAN J EXP PSYCHOL, V65, P208, DOI 10.1037/a0024258
   West J., 2004, MANAGE INT REV, V44, P239
   Xu, 2017, DESTECH T COMPUT SCI
   Yang SM, 2017, 2017 32ND YOUTH ACADEMIC ANNUAL CONFERENCE OF CHINESE ASSOCIATION OF AUTOMATION (YAC), P785, DOI 10.1109/YAC.2017.7967516
   You Hongyuan., 2017, 2017 International Workshop on Pattern Recognition in Neuroimaging (PRNI), P1
   Zhang RY, 2017, PROC INT CONF DOC, P25, DOI 10.1109/ICDAR.2017.324
NR 80
TC 0
Z9 0
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2046
EP 2057
DI 10.1109/TMM.2021.3075091
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200021
DA 2024-07-18
ER

PT J
AU Wei, YW
   Wang, X
   He, XN
   Nie, LQ
   Rui, Y
   Chua, TS
AF Wei, Yinwei
   Wang, Xiang
   He, Xiangnan
   Nie, Liqiang
   Rui, Yong
   Chua, Tat-Seng
TI Hierarchical User Intent Graph Network for Multimedia Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Convolution; Semantics;
   Collaboration; Recommender systems; Convolutional codes; Graph
   convolution network; multimedia recommendation; user intention modeling;
   hierarchical graph structure
AB Understanding user preference on item context is the key to acquire a high-quality multimedia recommendation. Typically, the pre-existing features of items are derived from pre-trained models (e.g. visual features of micro-videos extracted from some neural networks), and then introduced into the recommendation framework (e.g. collaborative filtering) to capture user preference. However, we argue that such a paradigm is insufficient to output satisfactory user representations, which hardly profile personal interests well. The key reason is that present works largely leave user intents untouched, then failing to encode such informative representation of users. In this work, we aim to learn multi-level user intents from the co-interacted patterns of items, so as to obtain high-quality representations of users and items and further enhance the recommendation performance. Towards this end, we develop a novel framework, Hierarchical User Intent Graph Network, which exhibits user intents in a hierarchical graph structure, from the fine-grained to coarse-grained intents. In particular, we get the multi-level user intents by recursively performing two operations: 1) intra-level aggregation, which distills the signal pertinent to user intents from co-interacted item graphs; and 2) inter-level aggregation, which constitutes the supernode in higher levels to model coarser-grained user intents via gathering the nodes' representations in the lower ones. Then, we refine the user and item representations as a distribution over the discovered intents, instead of simple pre-existing features. To demonstrate the effectiveness of our model, we conducted extensive experiments on three public datasets. Our model achieves significant improvements over the state-of-the-art methods, including MMGCN and DisenGCN. Furthermore, by visualizing the item representations, we provide the semantics of user intents.
C1 [Wei, Yinwei; Wang, Xiang; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore, Singapore.
   [Nie, Liqiang] Shandong Univ, Coll Comp Sci, Technol, Qingdao, Shandong, Peoples R China.
   [He, Xiangnan] Univ Sci Technol China, Hefei, Peoples R China.
   [Rui, Yong] Lenovo Grp, Beijing, Peoples R China.
C3 National University of Singapore; Shandong University; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS; Legend
   Holdings; Lenovo
RP Nie, LQ (corresponding author), Shandong Univ, Coll Comp Sci, Technol, Qingdao, Shandong, Peoples R China.
EM weiyinwei@hotmail.com; xiangwang@u.nus.edu; xiangnanhe@gmail.com;
   nieliqiang@gmail.com; yongrui@lenovo.com; chuats@comp.nus.edu.sg
RI Wei, Yinwei/JHX-9398-2023; He, Xiangnan/G-3986-2011
OI Wei, Yinwei/0000-0003-1791-3159; He, Xiangnan/0000-0003-2838-861X
FU Sea-NExT Joint Laboratory
FX This work was supported by the Sea-NExT Joint Laboratory.
CR Arora S., 2016, SIMPLE TOUGH TO BEAT
   Barkan O, 2019, RECSYS 2019: 13TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P228, DOI 10.1145/3298689.3347038
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Devlin J., 2018, BERT PRE TRAINING DE
   Doulamis ND, 2016, IEEE T CYBERNETICS, V46, P2810, DOI 10.1109/TCYB.2015.2489841
   Du YL, 2019, IEEE T MULTIMEDIA, V21, P555, DOI 10.1109/TMM.2018.2887018
   Fan WQ, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P417, DOI 10.1145/3308558.3313488
   Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kingma D.P., 2014, ARXIV14126980
   Kipf TN, 2017, INT C LEARN REPR
   Lei CY, 2016, PROC CVPR IEEE, P2545, DOI 10.1109/CVPR.2016.279
   Liu JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P665, DOI 10.1145/3343031.3350991
   Liu S, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3020, DOI 10.1145/3308558.3313513
   Ma JX, 2019, PR MACH LEARN RES, V97
   Ma JX, 2019, ADV NEUR IN, V32
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.5555/1795114.1795167
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989
   Wei YW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3541, DOI 10.1145/3394171.3413556
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wu JX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P827, DOI 10.1145/3343031.3350938
   Wu L, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P235, DOI 10.1145/3331184.3331214
   Xin X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P125, DOI 10.1145/3331184.3331188
   Xu K, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8350934
   Yang YJ, 2018, IEEE T MULTIMEDIA, V20, P1888, DOI 10.1109/TMM.2017.2779043
   Ye FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P55, DOI 10.1145/3394171.3413941
   Ying R, 2018, ADV NEUR IN, V31
   Yu WJ, 2019, PROC CVPR IEEE, P2932, DOI 10.1109/CVPR.2019.00305
   Zhang J, 2017, IEEE T MULTIMEDIA, V19, P2439, DOI 10.1109/TMM.2017.2701641
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
NR 40
TC 26
Z9 26
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2701
EP 2712
DI 10.1109/TMM.2021.3088307
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000001
DA 2024-07-18
ER

PT J
AU Zhang, HW
   Qian, SS
   Fang, Q
   Xu, CS
AF Zhang, Huaiwen
   Qian, Shengsheng
   Fang, Quan
   Xu, Changsheng
TI Multi-Modal Meta Multi-Task Learning for Social Media Rumor Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Social networking (online); Feature extraction; Learning
   systems; Semantics; Media; Blogs; Meta learning; multi-modal; multi-task
   learning; rumor detection; social media
AB With the rapid development of social media platforms and the increasing scale of the social media data, the rumor detection task has become vitally important since the authenticity of posts cannot be guaranteed. To date, Many approaches have been proposed to facilitate the rumor detection process by utilizing the multi-task learning mechanism, which aims to improve the performance of rumor detection task by leveraging the useful information in the stance detection task. However, most of the existing approaches suffer from three limitations: (1) only focus on the textual content and ignore the multi-modal information which is key component contained in social media data; (2) ignore the difference of feature space between the stance detection task and rumor detection task, resulting in the unsatisfactory usage of stance information; (3) largely neglect the semantic information hidden in the fine-grained stance labels. Therefore, in this paper, we design a Multi-modal Meta Multi-Task Learning (MM-MTL) framework for social media rumor detection. To make use of multiple modalities, we design a multi-modal post embedding layer which considers both textual and visual content. To overcome the feature-sharing problem of the stance detection task and rumor detection task, we propose a meta knowledge-sharing scheme to share some higher meta network-layers and capture the meta knowledge behind the multi-modal post. To better utilize the semantic information hidden in the fine-grained stance labels, we employ the attention mechanism to estimate the weight of each reply. Extensive experiments on two Twitter benchmark datasets demonstrate that our proposed method achieves state-of-the-art performance.
C1 [Zhang, Huaiwen; Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zhang, Huaiwen; Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM huaiwen.zhang@nlpr.ia.ac.cn; shengsheng.qian@nlpr.ia.ac.cn;
   qfang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665; Zhang, Huaiwen/0000-0002-3183-9218
FU National Key Research, and Development Program of China
   [2017YFB1002804]; National Natural Science Foundation of China
   [62036012, 61721004, 61720106006, 61802405, 62072456, 61832002,
   61936005, U1705262]; Key Research Program of Frontier Sciences, CAS
   [QYZDJSSWJSC039]; Open Research Projects of Zhejiang Laboratory
   [2021KE0AB05]; K.C. Wong Education Foundation; CCF-Tencent Open Fund
FX This work was supported in part by National Key Research, and
   Development Program of China under Grant 2017YFB1002804, in part by the
   National Natural Science Foundation of China under Grants 62036012,
   61721004, 61720106006, 61802405, 62072456, 61832002, 61936005, and
   U1705262, in part by the Key Research Program of Frontier Sciences, CAS,
   under Grant QYZDJSSWJSC039, in part by the Open Research Projects of
   Zhejiang Laboratory 2021KE0AB05, in part by the K.C.Wong Education
   Foundation, and in part by CCF-Tencent Open Fund. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Guo-Jun Qi.
CR Allcott H, 2017, J ECON PERSPECT, V31, P211, DOI 10.1257/jep.31.2.211
   Ando RK, 2005, J MACH LEARN RES, V6, P1817
   [Anonymous], 2008, PROC INT C MACHINE L
   [Anonymous], 2017, Bayesian Hypernetworks
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Brazdil P. B., 2009, Metalearning: Applications to Data Mining
   Brock A., 2017, SMASH ONE SHOT MODEL
   Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
   Castillo C., 2011, P 20 INT C WORLD WID, P675, DOI DOI 10.1145/1963405.1963500
   Chen JK, 2018, AAAI CONF ARTIF INTE, P5070
   Chen T, 2018, LECT NOTES ARTIF INT, V11154, P40, DOI 10.1007/978-3-030-04503-6_4
   Chen YC., 2017, P 11 INT WORKSH SEM, P465, DOI DOI 10.18653/V1/S17-2081
   Derczynski L., 2017, P 11 INT WORKSH SEM, P69, DOI DOI 10.18653/V1/S17-2006
   Devlin J., 2018, BERT PRE TRAINING DE
   Donovan P, 2007, DIOGENES, V54, P59, DOI 10.1177/0392192107073434
   Dungs Sebastian., 2018, P 27 INT C COMPUTATI, P3360
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Finn C, 2018, ADV NEUR IN, V31
   Finn C, 2017, PR MACH LEARN RES, V70
   Guacho GB, 2018, 2018 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM), P322, DOI 10.1109/ASONAM.2018.8508241
   Ha D., 2017, ICLR
   Huang Q., 2018, J COMPUT SOCIAL SCI, V08
   Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Kochkina Elena, 2018, ARXIV180603713, P3402
   Kwon S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0168344
   Kwon S, 2013, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2013.61
   Liu X., 2015, P 24 ACM INT C INF K, P1867, DOI [10.1145/2806416.2806651, DOI 10.1145/2806416.2806651]
   Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]
   Ma J., 2015, P 24 ACM INT C INF K, P1751, DOI DOI 10.1145/2806416.2806607
   Ma J., 2016, P 25 INT JOINT C ART, P3818
   Ma J, 2018, COMPANION PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2018 (WWW 2018), P585, DOI 10.1145/3184558.3188729
   Pan Z, 2018, HYPERST NET HYPERNET
   Parikh SB, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P104, DOI [10.1109/BigMM.2019.00025, 10.1109/BigMM.2019.00-37]
   Popat K, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P22
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Sanh V., 2019, DISTILBERT DISTILLED
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2371
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun C, 2019, LECT NOTES ARTIF INT, V11856, P194, DOI 10.1007/978-3-030-32381-3_16
   Thorne J, 2017, P EMNLP WORKSH NAT L, P80, DOI DOI 10.18653/V1/W17-4214
   Wang JD, 2012, INFORM RETRIEVAL, V15, P278, DOI 10.1007/s10791-012-9193-0
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Wolf T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P38
   Wu LW, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4644
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
   Yu F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3901
   Zhang HW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1942, DOI 10.1145/3343031.3350850
   Zhang Q, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2333, DOI 10.1145/3308558.3313718
   Zubiaga A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150989
NR 54
TC 27
Z9 29
U1 9
U2 57
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1449
EP 1459
DI 10.1109/TMM.2021.3065498
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200017
DA 2024-07-18
ER

PT J
AU Zhao, AT
   Dong, JY
   Li, JB
   Qi, L
   Zhou, HY
AF Zhao, Aite
   Dong, Junyu
   Li, Jianbo
   Qi, Lin
   Zhou, Huiyu
TI Associated Spatio-Temporal Capsule Network for Gait Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Gait recognition; Data mining; Legged locomotion;
   Heuristic algorithms; Data models; Biological system modeling;
   Associated capsules; capsule network; gait recognition; multi-sensor;
   spatio-temporal
ID PARKINSONS-DISEASE; VARIABILITY
AB It is a challenging task to identify a person based on her/his gait patterns. State-of-the-art approaches rely on the analysis of temporal or spatial characteristics of gait, and gait recognition is usually performed on single modality data (such as images, skeleton joint coordinates, or force signals). Evidence has shown that using multi-modality data is more conducive to gait research. Therefore, we here establish an automated learning system, with an associated spatio-temporal capsule network (ASTCapsNet) trained on multi-sensor datasets, to analyze multimodal information for gait recognition. Specifically, we first design a low-level feature extractor and a high-level feature extractor for spatio-temporal feature extraction of gait with a novel recurrent memory unit and a relationship layer. Subsequently, a Bayesian model is employed for the decision-making of class labels. Extensive experiments on several public datasets (normal and abnormal gait) validate the effectiveness of the proposed ASTCapsNet, compared against several state-of-the-art methods.
C1 [Zhao, Aite; Li, Jianbo] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
   [Dong, Junyu; Qi, Lin] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
   [Zhou, Huiyu] Univ Leicester, Sch Informat, Leicester LE1 7RH, Leics, England.
C3 Qingdao University; Ocean University of China; University of Leicester
RP Li, JB (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
EM zhaoaite@gmail.com; dongjunyu@ouc.edu.cn; lijianbo@qdu.edu.cn;
   qilin@ouc.edu.cn; hz143@leicester.ac.uk
RI Zhou, Huiyu/O-2692-2014
OI Zhou, Huiyu/0000-0003-1634-9840; Zhao, Aite/0000-0003-3494-175X; Dong,
   Junyu/0000-0001-7012-2087; Li, Jianbo/0000-0001-8756-7007
FU National Key Research and Development Plan Key Special Projects
   [2018YFB2100303]; Shandong Province Colleges and Universities Youth
   Innovation Technology Plan Innovation Team Project [2020KJN011];
   Shandong Provincial Natural Science Foundation [ZR2020MF060]; Program
   for Innovative Postdoctoral Talents in Shandong Province [40618030001];
   National Natural Science Foundation of China [61802216]; Postdoctoral
   Science Foundation of China [2018M642613]
FX This work was supported in part by National Key Research and Development
   Plan Key Special Projects under Grant 2018YFB2100303, in part by
   Shandong Province Colleges and Universities Youth Innovation Technology
   Plan Innovation Team Project under Grant 2020KJN011, in part by Shandong
   Provincial Natural Science Foundation under Grant ZR2020MF060, in part
   by Program for Innovative Postdoctoral Talents in Shandong Province
   under Grant 40618030001, in part by National Natural Science Foundation
   of China under Grant 61802216, and in part by Postdoctoral Science
   Foundation of China under Grant 2018M642613.
CR Alharthi AS, 2019, PROC IEEE INT SYMP, P1401, DOI 10.1109/ISIE.2019.8781511
   Alotaibi M, 2017, COMPUT VIS IMAGE UND, V164, P103, DOI 10.1016/j.cviu.2017.10.004
   Andersson VO, 2015, AAAI CONF ARTIF INTE, P425
   [Anonymous], 2021, PHYS DAT
   [Anonymous], 1997, NEURAL COMPUT
   Bregler C, 1997, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.1997.609382
   Cao K, 2019, IEEE T PATTERN ANAL, V41, P788, DOI 10.1109/TPAMI.2018.2818162
   Chattopadhyay P, 2014, IEEE T INF FOREN SEC, V9, P1843, DOI 10.1109/TIFS.2014.2352114
   Choi S, 2019, IEEE T INF FOREN SEC, V14, P2577, DOI 10.1109/TIFS.2019.2901823
   Chung J., 2014, NIPS 2014 WORKSH DEE
   database C.-A, CAS A DAT
   Feng Y, 2016, INT C PATT RECOG, P325, DOI 10.1109/ICPR.2016.7899654
   Frenkel-Toledo S, 2005, MOVEMENT DISORD, V20, P1109, DOI 10.1002/mds.20507
   Gianaria E., 2014, HUMAN CLASSIFICATION
   Gianaria E, 2019, MULTIMED TOOLS APPL, V78, P13925, DOI 10.1007/s11042-018-6865-9
   Hausdorff JM, 2007, EUR J NEUROSCI, V26, P2369, DOI 10.1111/j.1460-9568.2007.05810.x
   Hausdorff JM, 1998, MOVEMENT DISORD, V13, P428, DOI 10.1002/mds.870130310
   He YW, 2019, IEEE T INF FOREN SEC, V14, P102, DOI 10.1109/TIFS.2018.2844819
   Ho V., 2019, P IEEE RIVF INT C CO, P1
   Hong S, 2017, INT J FUZZY LOG INTE, V17, P51, DOI 10.5391/IJFIS.2017.17.2.51
   Hossain Emdad, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8227, P721, DOI 10.1007/978-3-642-42042-9_89
   Hu H., 2017, Proceedings_of_the_34th_International_Conference_on_Machine_Learning-Volume, P1568
   Jane YN, 2016, J BIOMED INFORM, V60, P169, DOI 10.1016/j.jbi.2016.01.014
   Jia W, 2017, IEEE T IMAGE PROCESS, V26, P4483, DOI 10.1109/TIP.2017.2705424
   Kumar P, 2019, IEEE T FUZZY SYST, V27, P956, DOI 10.1109/TFUZZ.2018.2870590
   Li J, 2017, 2017 IEEE SMARTWORLD, UBIQUITOUS INTELLIGENCE & COMPUTING, ADVANCED & TRUSTED COMPUTED, SCALABLE COMPUTING & COMMUNICATIONS, CLOUD & BIG DATA COMPUTING, INTERNET OF PEOPLE AND SMART CITY INNOVATION (SMARTWORLD/SCALCOM/UIC/ATC/CBDCOM/IOP/SCI)
   Li SQ, 2019, IEEE T MULTIMEDIA, V21, P2361, DOI 10.1109/TMM.2019.2900134
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu TL, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115672
   Park S, 2017, IEEE ENG MED BIO, P1409, DOI 10.1109/EMBC.2017.8037097
   Qi GJ, 2016, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2016.249
   Sabour S, 2017, ADV NEUR IN, V30
   SDU, 2021, SDU GAIT DAT
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shu XB, 2018, IEEE T CIRC SYST VID, V28, P454, DOI 10.1109/TCSVT.2016.2607345
   Sun JN, 2020, IEEE T MULTIMEDIA, V22, P2833, DOI 10.1109/TMM.2020.2966863
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Wang W, 2016, IEEE IMAGE PROC, P3151, DOI 10.1109/ICIP.2016.7532940
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xue ZJ, 2010, PATTERN RECOGN, V43, P2904, DOI 10.1016/j.patcog.2010.03.011
   Ye MX, 2020, IEEE T MULTIMEDIA, V22, P1113, DOI 10.1109/TMM.2019.2942479
   Yogev G, 2005, EUR J NEUROSCI, V22, P1248, DOI 10.1111/j.1460-9568.2005.04298.x
   Yun Shi, 2015, Applied Mechanics and Materials, V701-702, P274, DOI 10.4028/www.scientific.net/AMM.701-702.274
   Zeng W, 2015, INFORM SCIENCES, V317, P246, DOI 10.1016/j.ins.2015.04.047
   Zhang LH, 2018, ADV NEUR IN, V31
   Zhao AT, 2018, KNOWL-BASED SYST, V145, P91, DOI 10.1016/j.knosys.2018.01.004
   Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
NR 48
TC 22
Z9 22
U1 3
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 846
EP 860
DI 10.1109/TMM.2021.3060280
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100026
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tan, XB
   Xu, L
   Ni, JW
   Li, SM
   Jiang, XF
   Zheng, Q
AF Tan, Xiaobin
   Xu, Lei
   Ni, Jiawei
   Li, Simin
   Jiang, Xiaofeng
   Zheng, Quan
TI Game Theory Based Dynamic Adaptive Video Streaming for Multi-Client Over
   NDN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Quality of experience; Bit rate; Games; Bandwidth;
   Adaptation models; Bayes methods; Dynamic adaptive streaming; game
   theory; multi-client; named data networking
ID RATE ADAPTATION
AB The performance of Dynamic Adaptive Streaming (DAS) in multi-client scenarios can be improved by taking advantage of the aggregation capability of Named Data Networking (NDN). In this paper, we propose a client-side game theory based (GB) ABR algorithm for NDN that can achieve proactive aggregation of requests among clients as much as possible without requiring coordinating with other clients or scheduling by a central controller. We model the interaction between a DAS client and network as an incomplete information non-cooperative game. Then, this game is transformed into a complete but imperfect information game by Harsanyi transformation, and each client can issue an appropriate bitrate request by solving the Bayesian Nash Equilibrium (BNE) problem respectively. By designing the payoff function pair elaborately, the equilibrium point of the game can correspond to the situation that multiple clients issuing the same video bitrate request, that is, requests aggregation, which will reduce the repeated traffic and also achieve fairness. Compared with the existing solutions, through simulation and real-world experiments in multi-client video distribution scenarios, the GB algorithm outperforms the comparison algorithms in terms of overall Quality of Experience (QoE), fairness, and network bandwidth utilization, etc.
C1 [Tan, Xiaobin; Xu, Lei; Li, Simin; Jiang, Xiaofeng; Zheng, Quan] Univ Sci & Technol China, Lab Future Networks, Hefei 230027, Peoples R China.
   [Ni, Jiawei] Jiangsu Automat Res Inst, Lianyungang 222006, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zheng, Q (corresponding author), Univ Sci & Technol China, Lab Future Networks, Hefei 230027, Peoples R China.
EM xbtan@ustc.edu.cn; xl10092@mail.ustc.edu.cn; nijiawei@jari.cn;
   lisimin@mail.ustc.edu.cn; jxf@ustc.edu.cn; qzheng@ustc.edu.cn
RI Zheng, Quan/HZH-4993-2023; Li, Simin/Q-1923-2015
OI Tan, Xiaobin/0000-0001-7489-2839
FU National Key R&D Program of China [2020YFA0711400]; National Science
   Foundation of China [61673360]; Key R&D Program of Anhui Province
   [202004a05020078]; CETC Joint Advanced Research Foundation
   [6141B08080101]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2020YFA0711400, in part by the National Science Foundation
   of China under Grant 61673360, in part by the Key R&D Program of Anhui
   Province in 2020 under Grant 202004a05020078, and in part by the CETC
   Joint Advanced Research Foundation under Grant 6141B08080101. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Chonggang Wang.
CR Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   Alt B, 2019, IEEE INFOCOM SER, P1000, DOI [10.1109/INFOCOM.2019.8737418, 10.1109/infocom.2019.8737418]
   [Anonymous], 2015, P IEEE INT C MULT EX
   [Anonymous], 1984, ACM Transaction on Computer Systems
   [Anonymous], 1980, Lecture Notes in Computer Science
   Awiphan Suphakit, 2019, 2019 IEEE 8th Global Conference on Consumer Electronics (GCCE), P181, DOI 10.1109/GCCE46687.2019.9015382
   Awiphan S, 2018, PROCEEDINGS OF 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS), P291, DOI 10.1109/CCOMS.2018.8463312
   Bentaleb A, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P13, DOI 10.1145/3204949.3204961
   Cisco V., 2018, CISC VIS NETW IND GL
   Elgabli A, 2018, IEEE ACM T NETWORK, V26, P1633, DOI 10.1109/TNET.2018.2844123
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Hassan M, 2019, AIP CONF PROC, V2184, DOI 10.1063/1.5136374
   Hu H, 2002, INT J GAME THEORY, V30, P517, DOI 10.1007/s001820200095
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Irondi I, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3269494
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li RD, 2020, IEEE T BIG DATA, V6, P233, DOI 10.1109/TBDATA.2018.2878584
   Li Yaguang, 2018, INT C LEARN REPR
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu W, 2020, IEEE WCNC, DOI 10.1109/wcnc45663.2020.9120755
   Long YS, 2016, IEEE INT CONF CON AU, P1, DOI [10.1007/s00170-016-9151-x, 10.1109/VLSI-TSA.2016.7480514, 10.1109/ICCA.2016.7505243]
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mastorakis S., 2015, Technical Report NDN-0028
   Monks J., 2019, PROC IEEE INT S BROA, P1
   Mueller C., 2013, Proceedings of the IEEE International Conference on Multimedia and Expo (ICME) 2013, P1
   NDN Team, NDN FORW DAEM
   Rainer B, 2016, IEEE J SEL AREA COMM, V34, P2130, DOI 10.1109/JSAC.2016.2577365
   Samain J, 2017, IEEE T MULTIMEDIA, V19, P2166, DOI 10.1109/TMM.2017.2733340
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Thijssen J, 2014, NONCOOPERATIVE GAME
   Villa Bjorn J., 2012, Information and Communication Technologies. Proceedings 18th EUNICE/IFIP WG 6.2, 6.6. International Conference, EUNICE 2012, P183, DOI 10.1007/978-3-642-32808-4_17
   Xu ZM, 2019, IEEE T CIRC SYST VID, V29, P1781, DOI 10.1109/TCSVT.2018.2849015
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Yin XQ, 2017, P AMER CONTR CONF, P4236, DOI 10.23919/ACC.2017.7963606
   Yuan H, 2018, IEEE T MOBILE COMPUT, V17, P2334, DOI 10.1109/TMC.2018.2800749
   Yuan H, 2018, IEEE T MULTIMEDIA, V20, P183, DOI 10.1109/TMM.2017.2724850
   Zhang LX, 2014, ACM SIGCOMM COMP COM, V44, P66, DOI 10.1145/2656877.2656887
   Zou LH, 2018, IEEE T BROADCAST, V64, P26, DOI 10.1109/TBC.2017.2722221
NR 39
TC 4
Z9 4
U1 4
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 29
PY 2021
VL 24
BP 3491
EP 3505
DI 10.1109/TMM.2021.3100768
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NJ
UT WOS:000824706800001
DA 2024-07-18
ER

PT J
AU Zhang, CX
   Zhou, ZK
   Chen, Z
   Hu, WM
   Li, M
   Jiang, SF
AF Zhang, Congxuan
   Zhou, Zhongkai
   Chen, Zhen
   Hu, Weiming
   Li, Ming
   Jiang, Shaofeng
TI Self-Attention-Based Multiscale Feature Learning Optical Flow With
   Occlusion Feature Map Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optical flow; Estimation; Image motion analysis; Computer vision;
   Optical losses; Computational modeling; Robustness; Learning optical
   flow; self-attention; multiscale feature; large displacements;
   occlusions
ID QUALITY ASSESSMENT
AB Even though optical flow approaches based on convolutional neural networks have achieved remarkable performance with respect to both accuracy and efficiency, large displacements and motion occlusions remain challenges for most existing learning-based models. To address the abovementioned issues, we propose in this paper a self-attention-based multiscale feature learning optical flow computation method with occlusion feature map prediction. First, we exploit a self-attention mechanism-based multiscale feature learning module to compensate for large displacement optical flows, and the presented module is able to capture long-range dependencies from the input frames. Second, we design a simple but effective self-learning module to acquire an occlusion feature map, in which the predicted occlusion map is utilized to correct the optical flow estimation in occluded areas. Third, we explore a hybrid loss function that integrates the photometric and smoothness losses into the classical endpoint error (EPE)-based loss to ensure the accuracy and robustness of the presented network. Finally, we compare the proposed method with some state-of-the-art approaches using the MPI-Sintel and KITTI test databases. The experimental results demonstrate that the proposed method achieved competitive performance with respect to both accuracy and robustness, and it produced the better results compared to other methods under large displacements and motion occlusions.
C1 [Zhang, Congxuan; Jiang, Shaofeng] Nanchang Hangkong Univ, Key Lab Nondestruct Testing, Minist Educ, Nanchang 330063, Jiangxi, Peoples R China.
   [Zhang, Congxuan; Hu, Weiming] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Zhou, Zhongkai] Hohai Univ, Coll Internet Things Engn, Changzhou 213022, Jiangsu, Peoples R China.
   [Chen, Zhen] Nanchang Hangkong Univ, Key Lab Nondestruct Testing, Minist Educ, Nanchang 330063, Jiangxi, Peoples R China.
   [Li, Ming] Nanchang Hangkong Univ, Key Lab Jiangxi Prov Image Proc & Pattern Recogni, Nanchang 330063, Jiangxi, Peoples R China.
C3 Nanchang Hangkong University; Chinese Academy of Sciences; Institute of
   Automation, CAS; Hohai University; Nanchang Hangkong University;
   Nanchang Hangkong University
RP Chen, Z (corresponding author), Nanchang Hangkong Univ, Key Lab Nondestruct Testing, Minist Educ, Nanchang 330063, Jiangxi, Peoples R China.
EM zcxdsg@163.com; jsczzzk123@163.com; dr_chenzhen@163.com;
   wmhu@nlpr.ia.ac.cn; liming@nchu.edu.cn; jiangshaofeng@nchu.edu.cn
OI Zhang, Congxuan/0000-0003-1356-1205
FU National Key Research and Development Program of China [2020YFC2003800];
   National Natural Science Foundation of China [61866026, 61772255,
   61866025]; Advantage Subject Team Project of Jiangxi Province
   [20165BCB19007]; Outstanding Young Talents Program of Jiangxi Province
   [20192BCB23011]; National Natural Science Foundation of Jiangxi Province
   [20202ACB214007]; Aeronautical Science Foundation of China
   [2018ZC56008]; China Postdoctoral Science Foundation [2019 M650894]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFC2003800, in part by the
   National Natural Science Foundation of China under Grants 61866026,
   61772255, and 61866025, in part by the Advantage Subject Team Project of
   Jiangxi Province under Grant 20165BCB19007, in part by the Outstanding
   Young Talents Program of Jiangxi Province under Grant 20192BCB23011, in
   part by the National Natural Science Foundation of Jiangxi Province
   under Grant 20202ACB214007, in part by the Aeronautical Science
   Foundation of China under Grant 2018ZC56008, and in part by the China
   Postdoctoral Science Foundation (2019 M650894).
CR Ali S, 2016, COMPUT VIS IMAGE UND, V145, P95, DOI 10.1016/j.cviu.2015.12.003
   Bailer C, 2017, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2017.290
   Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457
   Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964
   Bao LC, 2014, PROC CVPR IEEE, P3534, DOI 10.1109/CVPR.2014.452
   Bao WB, 2019, IEEE T IMAGE PROCESS, V28, P4233, DOI 10.1109/TIP.2019.2903656
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Chen ZY, 2013, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2013.316
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Gadot D, 2016, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2016.459
   Gageik N, 2015, IEEE ACCESS, V3, P599, DOI 10.1109/ACCESS.2015.2432455
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Hur J, 2019, PROC CVPR IEEE, P5747, DOI 10.1109/CVPR.2019.00590
   Hur J, 2017, IEEE I CONF COMP VIS, P312, DOI 10.1109/ICCV.2017.42
   Ilg E, 2018, LECT NOTES COMPUT SC, V11216, P626, DOI 10.1007/978-3-030-01258-8_38
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Ince S, 2008, IEEE T IMAGE PROCESS, V17, P1443, DOI 10.1109/TIP.2008.925381
   Janai J, 2018, LECT NOTES COMPUT SC, V11220, P713, DOI 10.1007/978-3-030-01270-0_42
   Ke RM, 2019, IEEE T INTELL TRANSP, V20, P54, DOI 10.1109/TITS.2018.2797697
   Kennedy R, 2015, LECT NOTES COMPUT SC, V8932, P364, DOI 10.1007/978-3-319-14612-6_27
   Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579
   Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470
   Manasa K, 2016, IEEE T IMAGE PROCESS, V25, P2480, DOI 10.1109/TIP.2016.2548247
   McGuire K, 2017, IEEE ROBOT AUTOM LET, V2, P1070, DOI 10.1109/LRA.2017.2658940
   Meister S, 2018, AAAI CONF ARTIF INTE, P7251
   Monzón N, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2526903
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Neoral M, 2019, LECT NOTES COMPUT SC, V11364, P159, DOI 10.1007/978-3-030-20870-7_10
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Ren Z, 2017, AAAI CONF ARTIF INTE, P1495
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sevilla-Lara L, 2016, PROC CVPR IEEE, P3889, DOI 10.1109/CVPR.2016.422
   Sun DQ, 2008, LECT NOTES COMPUT SC, V5304, P83
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Tretiak O., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P16
   Tu ZG, 2016, PATTERN RECOGN, V50, P223, DOI 10.1016/j.patcog.2015.09.002
   Wan YL, 2014, IEEE T MULTIMEDIA, V16, P637, DOI 10.1109/TMM.2014.2299515
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513
   Wannenwetsch AS, 2017, IEEE I CONF COMP VIS, P1182, DOI 10.1109/ICCV.2017.133
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Werlberger M, 2009, PROC BR MACH VIS C
   Wu GL, 2017, IEEE T MULTIMEDIA, V19, P1730, DOI 10.1109/TMM.2017.2691538
   Wulff J, 2017, PROC CVPR IEEE, P6911, DOI 10.1109/CVPR.2017.731
   Xiao JJ, 2006, LECT NOTES COMPUT SC, V3951, P211
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xu J, 2017, PROC CVPR IEEE, P5807, DOI 10.1109/CVPR.2017.615
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Yeh HH, 2013, IEEE T MULTIMEDIA, V15, P1944, DOI 10.1109/TMM.2013.2280250
   Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1
   Zhang CX, 2017, IEEE T IMAGE PROCESS, V26, P4055, DOI 10.1109/TIP.2017.2712279
   Zhang SH, 2011, IEEE T MULTIMEDIA, V13, P1286, DOI 10.1109/TMM.2011.2165052
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 67
TC 10
Z9 10
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 13
PY 2021
VL 24
BP 3340
EP 3354
DI 10.1109/TMM.2021.3096083
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NQ
UT WOS:000824707500003
DA 2024-07-18
ER

PT J
AU Blanes, I
   Hernández-Cabronero, M
   Serra-Sagristà, J
   Marcellin, MW
AF Blanes, Ian
   Hernandez-Cabronero, Miguel
   Serra-Sagrista, Joan
   Marcellin, Michael W.
TI Redundancy and Optimization of tANS Entropy Encoders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Redundancy; Automata; Entropy; Encoding; Decoding; Probability; Image
   coding; Tabled Asymmetrical Numeral Systems; Entropy encoder redundancy;
   Optimization
AB Nowadays entropy encoders are part of almost all data compression methods, with the Asymmetrical Numeral Systems (ANS) family of entropy encoders having recently risen in popularity. Entropy encoders based on the tabled variant of ANS are known to provide varying performances depending on their internal design. In this paper, we present a method that calculates encoder redundancies in almost linear time, which translates in practice to thousand-fold speedups in redundancy calculations for small automatons, and allows redundancy calculations for automatons with tens of millions of states that would be otherwise prohibitive. We also address the problem of improving tabled ANS encoder designs, by employing the aforementioned redundancy calculation method in conjunction with a stochastic hill climbing strategy. The proposed approach consistently outperforms state-of-the-art methods in tabled ANS encoder design. For automatons of twice the alphabet size, experimental results show redundancy reductions around 10% over the default initialization method and over 30% for random initialization.
C1 [Blanes, Ian; Hernandez-Cabronero, Miguel; Serra-Sagrista, Joan] Univ Autnoma Barcelona, Informat & Commun Engn Dept, Barcelona 08193, Spain.
   [Marcellin, Michael W.] Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
C3 Autonomous University of Barcelona; University of Arizona
RP Blanes, I (corresponding author), Univ Autnoma Barcelona, Informat & Commun Engn Dept, Barcelona 08193, Spain.
EM ian.blanes@uab.cat; miguel.hernandez@uab.cat; joan.serra@uab.cat;
   marcellin@ece.arizona.edu
OI Hernandez-Cabronero, Miguel/0000-0001-9301-4337; Marcellin,
   Michael/0000-0001-9606-134X
FU Centre National d'Etudes Spatiales; Spanish Ministry of Economy, and
   Competitiveness (MINECO); European Regional Development Fund (FEDER);
   European Union [RTI2018-095287-B-I00]; Catalan Government [BP2018-00008,
   2017SGR-463]
FX This work was supported in part by Centre National d'Etudes Spatiales,
   in part by the Spanish Ministry of Economy, and Competitiveness
   (MINECO), in part by the European Regional Development Fund (FEDER), in
   part by the European Union under Grant RTI2018-095287-B-I00, and in part
   by the Catalan Government under grants Beatriu de Pinos BP2018-00008,
   and 2017SGR-463.
CR Alakuijala J, 2016, 7932 RFC
   Alakuijala J, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3231935
   [Anonymous], 2016, Smaller and faster data compression with Zstandard
   [Anonymous], 2003, COMPUTING PAGERANK U
   Bainville E, 2017, LZFSE
   Chen QB, 2018, IEEE T MULTIMEDIA, V20, P1113, DOI 10.1109/TMM.2017.2762004
   Cheng ZX, 2020, IEEE T MULTIMEDIA, V22, P860, DOI 10.1109/TMM.2019.2938345
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   Collet Y., 2013, Finite State Entropy
   Dubé D, 2019, IEEE INT SYMP INFO, P1682, DOI [10.1109/ISIT.2019.8849430, 10.1109/isit.2019.8849430]
   Duda J., 2013, ARXIV13112540
   Duda J, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P65, DOI 10.1109/PCS.2015.7170048
   Duda Jarek, 2009, ARXIV09020271
   Garren ST, 2000, BERNOULLI, V6, P215, DOI 10.2307/3318575
   Giesen Fabian, 2014, ARXIV14023392
   Gormish M. J., 1999, US Patent, Patent No. [5,912,636, 5912636]
   Kalluri M, 2019, IEEE T MULTIMEDIA, V21, P39, DOI 10.1109/TMM.2018.2847228
   Konstantinov Fedor, 2019, Distributed Computer and Communication Networks. 22nd International Conference, DCCN 2019. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 11965), P125, DOI 10.1007/978-3-030-36614-8_10
   Krivokuca M, 2020, IEEE T IMAGE PROCESS, V29, P2217, DOI 10.1109/TIP.2019.2957853
   Langville AN, 2006, SIAM J MATRIX ANAL A, V27, P968, DOI 10.1137/040619028
   Long R, 2017, IEEE DATA COMPR CONF, P330, DOI 10.1109/DCC.2017.76
   Mackin A, 2019, IEEE T MULTIMEDIA, V21, P1499, DOI 10.1109/TMM.2018.2880603
   Marwood D, 2018, IEEE IMAGE PROC, P405, DOI 10.1109/ICIP.2018.8451393
   Moffat A, 2020, ACM T INFORM SYST, V38, DOI 10.1145/3397175
   Moffat A, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P405, DOI 10.1145/3159652.3159663
   Moffat A, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P677, DOI 10.1145/3132847.3132888
   Najmabadi SM, 2019, J SIGNAL PROCESS SYS, V91, P805, DOI 10.1007/s11265-018-1421-4
   Najmabadi SM, 2017, CONF DESIGN ARCHIT
   Najmabadi SM, 2015, INT SYMP IMAGE SIG, P256, DOI 10.1109/ISPA.2015.7306068
   OHara Michael J, 2010, SIAM DAT MIN C COL O, P713
   Peter P, 2019, IEEE IMAGE PROC, P3557, DOI [10.1109/ICIP.2019.8803760, 10.1109/icip.2019.8803760]
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Townsend J., 2019, INT C LEARNING REPRE
   Townsend J, 2020, INT C LEARN REPR MAY, P1
   Weissenberger A, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337888
   Yokoo H, 2018, PROCEEDINGS OF 2018 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA2018), P638, DOI 10.23919/ISITA.2018.8664207
   Yokoo H, 2016, PROCEEDINGS OF 2016 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA 2016), P631
   Yokoo H, 2016, IEEE INT SYMP INFO, P11, DOI 10.1109/ISIT.2016.7541051
   Yuan X, 2020, IEEE T MULTIMEDIA, V22, P2889, DOI 10.1109/TMM.2020.2967646
NR 39
TC 1
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4341
EP 4350
DI 10.1109/TMM.2020.3040547
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900033
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Chen, CF
   Qian, SS
   Fang, Q
   Xu, CS
AF Chen, Chaofan
   Qian, Shengsheng
   Fang, Quan
   Xu, Changsheng
TI HAPGN: Hierarchical Attentive Pooling Graph Network for Point Cloud
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Task analysis; Layout;
   Logic gates; Machine learning; Two dimensional displays; Point cloud
   segmentation; hierarchical graph pooling; gated graph attention network
AB Among different 3D data representations, point cloud stands out for its efficiency and flexibility. Hence, many researchers have been involved in the point cloud analysis recently. Existing approaches for point cloud segmentation task typically suffer from two limitations: 1) They usually treat different neighbor points as equals which cannot characterize the correlation between the center point and its neighborhoods well. Moreover, different parts may have different local structures for a point cloud, but they just learn a single representation space which is not sufficient and stable. 2) They often capture hierarchical information by heuristic sampling approaches which cannot reveal the spatial relationships of points well to learn global features. To overcome these limitations, we propose a novel hierarchical attentive pooling graph network (HAPGN) which utilizes the gated graph attention network (GGAN) and hierarchical graph pooling module (HiGPool) as building blocks for point cloud segmentation. Specifically, GGAN can highlight not only the importance of different neighbor points but also the importance of different representation spaces to enhance the local feature extraction. HiGPool is a novel pooling module that can capture the spatial layouts of points to learn the hierarchical features adequately. Experimental results on the ShapeNet part dataset and S3DIS dataset show that HAPGN can achieve superior performance over the state-of-the-art segmentation approaches. Furthermore, we also combine our proposed HiGPool with some recent approaches for point cloud classification and achieve better results on the ModelNet40 dataset.
C1 [Chen, Chaofan] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM chencfbupt@gmail.com; shengsheng.qian@nlpria.ac.cn; qfang@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
OI Chen, Chaofan/0000-0001-7970-7698; xu, chang sheng/0000-0001-8343-9665
FU National Natural Science Foundation of China [61721004, 61532009,
   61720106006, 61572503, 61802405, 61872424, 61702509, 61832002, 61936005,
   U1705262]; Key Research Program of Frontier Sciences, CAS
   [QYZDJ-SSW-JSC039]; K. C. Wong Education Foundation
FX This work was supported in part by the National Natural Science
   Foundation of China underGrants, 61721004, 61532009, 61720106006,
   61572503, 61802405, 61872424, 61702509, 61832002, 61936005, and
   U1705262, in part by the Key Research Program of Frontier Sciences, CAS,
   under Grant QYZDJ-SSW-JSC039, and in part by the K. C. Wong Education
   Foundation. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Federica Battisti.
CR Nguyen A, 2013, PROCEEDINGS OF THE 2013 6TH IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P225, DOI 10.1109/RAM.2013.6758588
   [Anonymous], 2014, CORR
   [Anonymous], 2016, NEURIPS 3D DEEP LEAR
   [Anonymous], 2018, ARXIV
   [Anonymous], 2017, 31 INT CONFNEURAL IN
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Behnke S., 2019, ARXIV191205905
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bruna Joan, 2014, ICLR
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Defferrard M., 2016, P 30 INT C NEURAL IN, V29, P3844
   Engelmann F, 2017, IEEE INT CONF COMP V, P716, DOI 10.1109/ICCVW.2017.90
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Hao P, P 28 INT JOINT C ART P 28 INT JOINT C ART, P3238
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1157, DOI 10.1145/3343031.3350966
   Hu J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P456, DOI 10.1145/3240508.3240626
   Jagannathan A, 2007, IEEE T PATTERN ANAL, V29, P2195, DOI 10.1109/TPAMI.2007.1125
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kearnes S, 2016, J COMPUT AID MOL DES, V30, P595, DOI 10.1007/s10822-016-9938-8
   Kingma D. P., 2014, arXiv
   Kipf T. N., 2017, 8 INT C LEARN REPR, P1
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Kong X, 2019, IEEE INT C INT ROBOT, P3467, DOI [10.1109/IROS40897.2019.8968296, 10.1109/iros40897.2019.8968296]
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Levie R, 2019, IEEE T SIGNAL PROCES, V67, P97, DOI 10.1109/TSP.2018.2879624
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li Yingzhen, 2018, INT C MACH LEARN
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu R, 2009, COMPUT GRAPH FORUM, V28, P397, DOI 10.1111/j.1467-8659.2009.01379.x
   Liu X H, 2019, P 27 ACM INT C MULT, P989, DOI DOI 10.1145/3343031.3350960
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Maturana D, 2015, IEEE INT CONF ROBOT, P3471, DOI 10.1109/ICRA.2015.7139679
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qiu JZ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2110, DOI 10.1145/3219819.3220077
   RABBANI T., 2006, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, V36, P248
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Songgaojun D., P 25 ACM SIGKDD INT P 25 ACM SIGKDD INT, P1007
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Velickovic P, 2017, ARXIV
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang X., 2019, NEURIPS, P4573
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu M, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1457, DOI 10.1145/3366423.3380219
   Wu M, 2019, IEEE DATA MINING, P648, DOI 10.1109/ICDM.2019.00075
   Xiao D, 2011, COMPUT GRAPH-UK, V35, P685, DOI 10.1016/j.cag.2011.03.020
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Ying Z., 2018, Advances in Neural Information Processing Systems, P4805
NR 62
TC 26
Z9 27
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2335
EP 2346
DI 10.1109/TMM.2020.3009499
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800014
DA 2024-07-18
ER

PT J
AU Jing, PG
   Shang, YC
   Nie, LQ
   Su, YT
   Liu, J
   Wang, M
AF Jing, Peiguang
   Shang, Yuechen
   Nie, Liqiang
   Su, Yuting
   Liu, Jing
   Wang, Meng
TI Learning Low-Rank Sparse Representations With Robust Relationship
   Inference for Image Memorability Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image memorability prediction; low-rank; sparse; relationship structure
ID REGRESSION; CLASSIFICATION; MEMORY
AB Image memorability prediction aims to estimate the degree to which an image will be remembered by observers. Generally, the core problem in image memorability prediction is how to obtain effective representations to characterize the visual content of an image. In contrast to existing methods, which focus more on exploring the factors that make images memorable, in this paper, we first propose a general framework for learning joint low-rank and sparse principal feature representations, called the LSPFR framework, to obtain the lowest-rank intrinsic representation for image memorability prediction. By considering the joint optimization of the nuclear and l(1)-norms, the global low-rank structure and the local patterns embedded in data can be exploited to make the learned features more robust and informative. To improve our framework based on the exploitation of sample relationship structure information, we present an extended version of LSPFR, named E-LSPFR, in which the underlying relationship structure matrix is inferred through a negative log-likelihood term with a sparsity constraint. The results of experiments conducted on four publicly available datasets confirm the superior performance of our proposed approaches.
C1 [Jing, Peiguang; Shang, Yuechen; Su, Yuting; Liu, Jing] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250000, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Sch Comp & Informat Sci, Hefei 230009, Peoples R China.
C3 Tianjin University; Shandong University; Hefei University of Technology
RP Su, YT (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM pgjing@tju.edu.cn; shangyuechen0904@gmail.com; nicliqiang@gmail.com;
   ytsu@tju.edu.cn; jliu_tju@tju.edu.cn; eric.mengwang@gmail.com
RI Jing, LIU/JCP-2850-2023; Wang, Meng/ITR-8699-2023
OI Jing, LIU/0000-0001-5172-4605; Jing, Peiguang/0000-0003-2648-7358
FU National Natural Science Foundation of China [61802277, 61701341];
   Peiyang Elite Scholar Program of Tianjin University [2020XRG-0104]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61802277 and 61701341 and in part by
   the Peiyang Elite Scholar Program of Tianjin University under Grant
   2020XRG-0104.
CR Akagunduz E., IEEE T PATTERN ANAL
   Bainbridge WA, 2013, J EXP PSYCHOL GEN, V142, P1323, DOI 10.1037/a0033872
   Baveye Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P491, DOI 10.1145/2964284.2967269
   Borkin MA, 2013, IEEE T VIS COMPUT GR, V19, P2306, DOI 10.1109/TVCG.2013.234
   Brewin CR, 2010, PSYCHOL REV, V117, P210, DOI 10.1037/a0018113
   Bylinskii Z, 2015, VISION RES, V116, P165, DOI 10.1016/j.visres.2015.03.005
   Cahill L, 1995, CONSCIOUS COGN, V4, P410, DOI 10.1006/ccog.1995.1048
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Celikkale B, 2013, IEEE COMPUT SOC CONF, P976, DOI 10.1109/CVPRW.2013.142
   Chen CF, 2012, PROC CVPR IEEE, P2618, DOI 10.1109/CVPR.2012.6247981
   Cohendet R., 2016, P INT C QUAL MULT EX P INT C QUAL MULT EX, P1
   Cohendet R, 2019, IEEE I CONF COMP VIS, P2531, DOI 10.1109/ICCV.2019.00262
   Cohendet R, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P178, DOI 10.1145/3206025.3206056
   Constantin M. G., 2019, P MEDIAEVAL WORKSH P MEDIAEVAL WORKSH
   Constantin MG, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3301299
   Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]
   Ding ZM, 2016, AAAI CONF ARTIF INTE, P1181
   Ding Z, 2014, AAAI CONF ARTIF INTE, P1192
   Do, 2018, P MEDIAEVAL WORKSH
   Dubey R, 2015, IEEE I CONF COMP VIS, P1089, DOI 10.1109/ICCV.2015.130
   Egilmez H.E., 2016, ARXIV161105181
   Fajtl J, 2018, PROC CVPR IEEE, P6363, DOI 10.1109/CVPR.2018.00666
   Fang X, 2018, IEEE T NEURAL NETW L, V29
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Hou CP, 2013, IEEE T IMAGE PROCESS, V22, P340, DOI 10.1109/TIP.2012.2214044
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Isola P, 2014, IEEE T PATTERN ANAL, V36, P1469, DOI 10.1109/TPAMI.2013.200
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Isola Phillip, 2011, Advances in Neural Information Processing Systems (NIPS), V24, P2429, DOI DOI 10.1167/12.9.1082
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jing PG, 2018, IEEE T KNOWL DATA EN, V30, P1519, DOI 10.1109/TKDE.2017.2785784
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Khosla A., 2012, P ADV NEUR INF PROC, P296
   Khosla A, 2015, IEEE I CONF COMP VIS, P2390, DOI 10.1109/ICCV.2015.275
   Li JJ, 2017, IEEE T CYBERNETICS, V47, P3516, DOI 10.1109/TCYB.2016.2565898
   Li X, 2016, IEEE T MULTIMEDIA, V18, P474, DOI 10.1109/TMM.2016.2518478
   Lin, 2010, ARXIV10095055
   Liu AN, 2018, SIGNAL PROCESS, V152, P206, DOI 10.1016/j.sigpro.2018.06.001
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Luo MN, 2018, IEEE T NEUR NET LEAR, V29, P944, DOI 10.1109/TNNLS.2017.2650978
   MARQUARDT DW, 1975, AM STAT, V29, P3, DOI 10.2307/2683673
   McGaugh JL, 2000, SCIENCE, V287, P248, DOI 10.1126/science.287.5451.248
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Peng HW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1147, DOI 10.1145/2733373.2806303
   Quillan M. R, 1966, AFCRL66189 BOLT BER AFCRL66189 BOLT BER
   Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3
   Saleh B, 2013, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2013.107
   Shekhar S, 2017, IEEE INT CONF COMP V, P2730, DOI 10.1109/ICCVW.2017.321
   Siarohin A, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311781
   Simonyan K., 2014, 14091556 ARXIV
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Soloveychik I, 2017, IEEE T SIGNAL PROCES, V65, P2379, DOI 10.1109/TSP.2017.2652358
   Squalli-Houssaini H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2371, DOI 10.1109/ICASSP.2018.8462292
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Wang HY, 2017, IEEE T MULTIMEDIA, V19, P969, DOI 10.1109/TMM.2016.2638624
   Wen J, 2018, NEURAL NETWORKS, V102, P36, DOI 10.1016/j.neunet.2018.02.002
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yuan M, 2010, J MACH LEARN RES, V11, P2261
   Zhang YMZ, 2013, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2013.93
   Zhang Z, 2016, IEEE T IMAGE PROCESS, V25, P2429, DOI 10.1109/TIP.2016.2547180
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou P, 2016, IEEE T NEUR NET LEAR, V27, P1080, DOI 10.1109/TNNLS.2015.2436951
   Zhu XF, 2019, IEEE T KNOWL DATA EN, V31, P1532, DOI 10.1109/TKDE.2018.2858782
NR 67
TC 3
Z9 3
U1 8
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2259
EP 2272
DI 10.1109/TMM.2020.3009485
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800008
DA 2024-07-18
ER

PT J
AU Lee, S
   Jo, SY
   An, GH
   Kang, SJ
AF Lee, Siyeong
   Jo, So Yeon
   An, Gwon Hwan
   Kang, Suk-Ju
TI Learning to Generate Multi-Exposure Stacks With Cycle Consistency for
   High Dynamic Range Imaging
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic range; Neural networks; Image restoration; Distortion; Training;
   Light sources; Brightness; High dynamic range imaging; inverse-tone
   mapping; image restoration; deep learning
ID EXPANSION
AB Inverse tone mapping aims at recovering the lost scene radiances from a single exposure image. With the successful use of deep learning in numerous applications, many inverse tone mapping methods use convolution neural networks in a supervised manner. As these approaches are trained with many pre-fixed high dynamic range (HDR) images, they fail to flexibly expand the dynamic ranges of images. To overcome this limitation, we consider a multiple exposure image synthesis approach for HDR imaging. In particular, we propose a pair of neural networks that represent mappings between images that have exposure levels one unit apart (stop-up/down network). Therefore, it is possible to construct two positive-feedback systems to generate images with greater or lesser exposure. Compared to previous works using the conditional generative adversarial learning framework, the stop-up/down network employs HDR friendly network structures and several techniques to stabilize the training processes. Experiments on HDR datasets demonstrate the advantages of the proposed method compared to conventional methods. Consequently, we apply our approach to restore the full dynamic range of scenes agilely with only two networks and generate photorealistic images in complex lighting situations.
C1 [Lee, Siyeong] NAVER LABS, Seongnam Si 13638, Gyeonggi Do, South Korea.
   [An, Gwon Hwan] LG Display, Seoul 07796, South Korea.
   [Jo, So Yeon; Kang, Suk-Ju] Sogang Univ, Dept Elect Engn, Seoul 07796, South Korea.
C3 Naver; LG Display; Sogang University
RP Kang, SJ (corresponding author), Sogang Univ, Dept Elect Engn, Seoul 07796, South Korea.
EM siyeong.lee@naverlabs.com; soyeonjo@sogang.ac.kr; joviahn@gmail.com;
   sjkang@sogang.ac.kr
OI Lee, Siyeong/0000-0002-5165-1335
FU MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information
   Technology Research Center) support program [IITP-2020-2018-0-01421];
   Ministry of the Interior and Safety of Korean government
   [19PQWO-B153369-01]; National Research Foundation of Korea (NRF) - Korea
   government (MSIT) [2020M3H4A1A02084899]
FX This work was supported in part by the MSIT (Ministry of Science and
   ICT), Korea, under the ITRC (Information Technology Research Center)
   support program (IITP-2020-2018-0-01421) supervised by the IITP
   (Institute of Information & communications Technology Planning &
   Evaluation), under Grant 19PQWO-B153369-01 from Smart road lighting
   platform development and empirical study on test-bed Program funded by
   Ministry of the Interior and Safety of Korean government, and in part
   the National Research Foundation of Korea (NRF) grant funded by the
   Korea government (MSIT) (No. 2020M3H4A1A02084899). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Han Hu.
CR [Anonymous], 2017, Advanced high dynamic range imaging
   Arrighetti W, 2017, J IMAGING, V3, DOI 10.3390/jimaging3040040
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Canas A. J., 2004, P 1 INT C CONC MAPP, P197
   Debevec P.E., 2008, Recovering High Dynamic Range Radiance Maps from Photographs, P31
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Froehlich J, 2014, PROC SPIE, V9023, DOI 10.1117/12.2040003
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Heusel M., 2017, ADV NEURAL INFORM PR, P6626
   Huang D.-S., 2008, ASP THEOR METH ISS 4
   Ioffe S., 2015, 32 INT C MACHINE LEA
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim Min H., 2008, Proceedings of the Tenth IASTED International Conference on Computer Graphics and Imaging, P152
   King DB, 2015, ACS SYM SER, V1214, P1
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Lee S, 2018, IEEE ACCESS, V6, P49913, DOI 10.1109/ACCESS.2018.2868246
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Maas A.L., 2013, P 30 INT C MACH LEAR, V30, P3
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mao X., 2018, IEEE T PATTERN ANAL, P1
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Masia B, 2017, MULTIMED TOOLS APPL, V76, P631, DOI 10.1007/s11042-015-3036-0
   Mathieu M., 2015, PROC INT C LEARN REP
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Ponomarenko N., TID2008 DATABASE EVA, P14
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Shin S., 2018, 2018 IEEE INT C CONS, V2018, P1
   Spinoulas L, 2015, IEEE COMPUT SOC CONF
   Sun N, 2010, IEEE IMAGE PROC, P2973, DOI 10.1109/ICIP.2010.5653371
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Turkowski K, 1990, GRAPHICS GEMS, P147, DOI DOI 10.1016/B978-0-08-050753-8.50042-5
   Xu B., ARXIV150500853CSSTAT
   Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193
   You S., 2018, ARXIV181207134CS
   Zhang JS, 2017, IEEE I CONF COMP VIS, P4529, DOI 10.1109/ICCV.2017.484
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 43
TC 11
Z9 11
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2561
EP 2574
DI 10.1109/TMM.2020.3013378
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600002
DA 2024-07-18
ER

PT J
AU Li, NJ
   Chang, FL
   Liu, CS
AF Li, Nanjun
   Chang, Faliang
   Liu, Chunsheng
TI Spatial-Temporal Cascade Autoencoder for Video Anomaly Detection in
   Crowded Scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Anomaly detection; Trajectory; Three-dimensional
   displays; Hidden Markov models; Image reconstruction; Two dimensional
   displays; Anomaly detection; video surveillance; spatial-temporal
   cascade autoencoder; 3D gradient; optical flow; two-stream framework
ID ABNORMAL EVENT DETECTION; NEURAL-NETWORKS; LOCALIZATION; REPRESENTATION;
   CLASSIFICATION; RECOGNITION; HISTOGRAMS
AB Time-efficient anomaly detection and localization in video surveillance still remains challenging due to the complexity of "anomaly". In this paper, we propose a cuboid-patch-based method characterized by a cascade of classifiers called a spatial-temporal cascade autoencoder (ST-CaAE), which makes full use of both spatial and temporal cues from video data. The ST-CaAE has two main stages, defined by two proposed neural networks: a spatial-temporal adversarial autoencoder (ST-AAE) and a spatial-temporal convolutional autoencoder (ST-CAE). First, the ST-AAE is used to preliminarily identify anomalous video cuboids and exclude normal cuboids. The key idea underlying ST-AAE is to obtain a Gaussian model to fit the distribution of the regular data. Then in the second stage, the ST-CAE classifies the specific abnormal patches in each anomalous cuboid with reconstruction error based strategy that takes advantage of the CAE and skip connection. A two-stream framework is utilized to fuse the appearance and motion cues to achieve more complete detection results, taking the gradient and optical flow cuboids as inputs for each stream. The proposed ST-CaAE is evaluated using three public datasets. The experimental results verify that our framework outperforms other state-of-the-art works.
C1 [Li, Nanjun; Chang, Faliang; Liu, Chunsheng] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
C3 Shandong University
RP Chang, FL (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM sdulnj@gmail.com; flchang@sdu.edu.cn; liuchunsheng@sdu.edu.cn
RI Liu, Chunsheng/L-1636-2017
OI Liu, Chunsheng/0000-0001-5516-2486
FU National Key Research and Development Program of China [2018YFB1305300];
   National Natural Science Foundation of China [61673244, 61703240]; Key
   Research and Development Program of Shandong Province of China
   [2019JZZY010130, 2018CXGC0907]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB1305300, in part by the
   National Natural Science Foundation of China under Grants 61673244 and
   61703240, and in part by Key Research and Development Program of
   Shandong Province of China under Grants 2019JZZY010130 and 2018CXGC0907.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   [Anonymous], 2016, ANOMALY DETECTION VI
   [Anonymous], 2015, EMPIRICAL EVALUATION
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, P BRIT MACH VIS C BM
   [Anonymous], 2013, MULTI DIGIT NUMBER R
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Chen TY, 2018, MULTIMED TOOLS APPL, V77, P14137, DOI 10.1007/s11042-017-5020-3
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Dimokranitou Asimenia., 2017, Adversarial autoencoders for anomalous event detection in images
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feng YC, 2021, IEEE T CYBERNETICS, V51, P1849, DOI 10.1109/TCYB.2019.2909480
   Feng YC, 2017, NEUROCOMPUTING, V219, P548, DOI 10.1016/j.neucom.2016.09.063
   Gandhi T, 2007, IEEE T INTELL TRANSP, V8, P413, DOI 10.1109/TITS.2007.903444
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008
   Jiang F, 2009, IEEE T IMAGE PROCESS, V18, P907, DOI 10.1109/TIP.2008.2012070
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Kim J, 2009, PROC CVPR IEEE, P2913
   Kingma D. P., 2014, arXiv
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Li NN, 2015, NEUROCOMPUTING, V155, P309, DOI 10.1016/j.neucom.2014.12.064
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Makhzani A, 2016, P INT C LEARN REPR
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Piciarelli C, 2006, PATTERN RECOGN LETT, V27, P1835, DOI 10.1016/j.patrec.2006.02.004
   Ravanbakhsh M, 2019, IEEE WINT CONF APPL, P1896, DOI 10.1109/WACV.2019.00206
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K, 2014, ADV NEUR IN, V27
   Sodemann AA, 2012, IEEE T SYST MAN CY C, V42, P1257, DOI 10.1109/TSMCC.2012.2215319
   Spampinato C., 2019, INT J COMPUT VISION, P1
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tung F, 2011, IMAGE VISION COMPUT, V29, P230, DOI 10.1016/j.imavis.2010.11.003
   Ullah H, 2018, NEUROCOMPUTING, V290, P74, DOI 10.1016/j.neucom.2018.02.045
   Ullah M, 2018, J IMAGING, V4, DOI 10.3390/jimaging4090107
   Wang T, 2019, IEEE T INF FOREN SEC, V14, P1390, DOI 10.1109/TIFS.2018.2878538
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Xu M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9163337
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Zhang S, 2016, IEEE T IMAGE PROCESS, V25, P5469, DOI 10.1109/TIP.2016.2601493
   Zhang W, 2019, IEEE T CIRC SYST VID, V29, P2442, DOI 10.1109/TCSVT.2018.2865749
   Zhang Y, 2017, GEOFLUIDS, DOI 10.1155/2017/6345810
   Zhang Y, 2016, PATTERN RECOGN, V51, P443, DOI 10.1016/j.patcog.2015.09.005
   Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1933, DOI 10.1145/3123266.3123451
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
   Zhou Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1087
   Zhu XB, 2014, PATTERN RECOGN, V47, P1791, DOI 10.1016/j.patcog.2013.11.018
NR 64
TC 91
Z9 98
U1 5
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 203
EP 215
DI 10.1109/TMM.2020.2984093
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600016
DA 2024-07-18
ER

PT J
AU Liang, HZ
   Wen, GH
   Hu, Y
   Luo, MN
   Yang, P
   Xu, YX
AF Liang, Haozan
   Wen, Guihua
   Hu, Yang
   Luo, Mingnan
   Yang, Pei
   Xu, Yingxue
TI MVANet: Multi-Task Guided Multi-View Attention Network for Chinese Food
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Semantics; Feature extraction; Image recognition; Deep
   learning; Shape; Fuses; Food recognition; convolutional neural network;
   multi-task learning; multi-view attention
ID MODEL
AB Food recognition plays a much critical role in various health-care applications. However, it poses many challenges to current approaches due to the diverse appearances of food dishes and the non-uniform composition of ingredients for the foods in the same category. Current methods primarily focus on the appearance of foods without considering their semantic information, easily finding the wrong attention areas of food images. Second, these methods lack the dynamic weighting of multiple semantic features in the modeling process. Thus this paper proposes a novel Multi-View Attention Network within the multi-task learning framework that incorporates multiple semantic features into the food recognition task from both ingredient recognition and recipe modeling. It also utilizes the multi-view attention mechanism to automatically adjust the weights of different semantic features and enables different tasks to interact with each other so as to obtain a more comprehensive feature representation. The experiments conducted on both ChineseFoodNet and VIREO Food-172 benchmark databases validate the proposed method with the obvious improvement of the performance and the lower parameter size.
C1 [Liang, Haozan; Wen, Guihua; Hu, Yang; Luo, Mingnan; Yang, Pei; Xu, Yingxue] South China Univ Technol Panyu, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.
RP Wen, GH (corresponding author), South China Univ Technol Panyu, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.
EM cslianghaozan@mail.scut.edu.cn; crghwen@scut.edu.cn;
   superhy199148@hotmail.com; csluomingnan@mail.scut.edu.cn;
   yangpei@scut.edu.cn; csxuyingxue@mail.scut.edu.cn
OI Xu, Yingxue/0000-0002-9657-3107; Hu, Yang/0000-0002-4856-5014
FU China National Science Foundation [61273363, 61976092]; Guangdong
   Province Key Area RD Plan Project [2020B1111120001]; Natural Science
   Foundation of Guangdong [2018A030313356]; Guangzhou Science and
   Technology Planning Project [201604020179, 201803010088]
FX Manuscript received November 13, 2019; revised April 9, 2020 and July
   29, 2020; accepted September 21, 2020. Date of publication October 6,
   2020; date of current version October 19, 2021. This work was supported
   in part by China National Science Foundation under Grants 61273363 and
   61976092, in part by Guangdong Province Key Area R&D Plan
   Project(2020B1111120001), in part by the Natural Science Foundation of
   Guangdong under Grant 2018A030313356, and in part by the Guangzhou
   Science and Technology Planning Project under Grants 201604020179 and
   201803010088. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Fatih Porikli.
   (Corresponding author: Guihua Wen.)
CR Aguilar E, 2019, J VIS COMMUN IMAGE R, V60, P360, DOI 10.1016/j.jvcir.2019.03.011
   Aguilar E, 2018, IEEE T MULTIMEDIA, V20, P3266, DOI 10.1109/TMM.2018.2831627
   [Anonymous], 2015, P IEEE INT C MULT EX
   Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663
   Cao JJ, 2018, PROC CVPR IEEE, P4290, DOI 10.1109/CVPR.2018.00451
   Caruana R., 1993, ICML, P41, DOI 10.1016/b978-1-55860-307-3.50012-5
   Chen JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1020, DOI 10.1145/3240508.3240627
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T., 2015, P NEUR INF PROC SYST
   Chen X., 2017, arXiv preprint arXiv:1705.02743
   Christodoulidis S, 2015, LECT NOTES COMPUT SC, V9281, P458, DOI 10.1007/978-3-319-23222-5_56
   Deng Y, 2019, AAAI CONF ARTIF INTE, P6318
   Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226
   Duong L, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P845
   Ege T, 2018, P JOINT WORKSH MULT, P53, DOI [DOI 10.1145/3230519.3230594, 10.1145/3230519.3230594]
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151
   Han C., 2018, P 2018 ACM MULT C, P510, DOI DOI 10.1145/3240508.3240611
   Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550
   Hassannejad H, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P41, DOI 10.1145/2986035.2986042
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Z., 2018, COMPUTER VISION PATT
   Jiang SQ, 2020, IEEE T IMAGE PROCESS, V29, P265, DOI 10.1109/TIP.2019.2929447
   Kagaya H, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1085, DOI 10.1145/2647868.2654970
   Kawano Y, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P589, DOI 10.1145/2638728.2641339
   Kawano Y, 2013, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW.2013.5
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Liu C, 2016, LECT NOTES COMPUT SC, V9677, P37, DOI 10.1007/978-3-319-39601-9_4
   Lu Y., 2018, P JOINT WORKSH MULT, P46, DOI [10.1145/3230519.3230593, DOI 10.1145/3230519.3230593, 10.48550/arXiv.1806.10343]
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
   Min WQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1331, DOI 10.1145/3343031.3350948
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Ratner A, 2019, AAAI CONF ARTIF INTE, P4763
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Rei M, 2019, AAAI CONF ARTIF INTE, P6916
   Rodríguez P, 2018, LECT NOTES COMPUT SC, V11212, P357, DOI 10.1007/978-3-030-01237-3_22
   Ruder S, 2017, arXiv preprint arXiv, DOI DOI 10.48550/ARXIV.1706.05098
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sener Ozan, 2018, NEURIPS, V2, P4
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sutskever Ilya, 2014, P 27 INT C NEURAL IN, P3104
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   Varior R. R., 2019, ABS190106026 CORR
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu CF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P519, DOI 10.1145/3240508.3240513
   Xia XJ, 2020, IEEE T MULTIMEDIA, V22, P569, DOI 10.1109/TMM.2019.2933330
   Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077
   Xu YL, 2018, IEEE T NEUR NET LEAR, V29, P5408, DOI 10.1109/TNNLS.2018.2802469
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P365, DOI 10.1109/TPAMI.2018.2797062
   Zhang XJ, 2016, J COMPUT SCI TECH-CH, V31, P489, DOI 10.1007/s11390-016-1642-6
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao XY, 2018, LECT NOTES COMPUT SC, V11205, P415, DOI 10.1007/978-3-030-01246-5_25
   Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 74
TC 16
Z9 17
U1 5
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3551
EP 3561
DI 10.1109/TMM.2020.3028478
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100010
DA 2024-07-18
ER

PT J
AU Lin, X
   Ma, LZ
   Sheng, B
   Wang, ZJ
   Chen, WS
AF Lin, Xiao
   Ma, Lizhuang
   Sheng, Bin
   Wang, Zhi-Jie
   Chen, Wansheng
TI Utilizing Two-Phase Processing With FBLS for Single Image Deraining
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Single image rain removal; computer vision; machine learning; image
   processing
AB Rain removal from a single image is a challenging problem and has attracted much attention in recent years. In this paper, we revisit the single image deraining problem, and present a novel solution. The central idea of our solution is to merge the merits of two-phase processing methods and the Fuzzy Broad Learning System (FBLS). Specifically, our solution first uses the dehazing algorithm to preprocess the input rainy image and separates it into the detail layer and the base layer. After that, it puts the Y-channel image of the detail layer into the FBLS to obtain the derained Y channel image, which is then combined with the Cb and Cr channel images to obtain the derained detail layer. Later, it fuses the derained detail layer and the base layer to get a preliminary derained image. Finally, it superimposes the details extracted from the dehazed image with some transparency on the preliminary result, obtaining the final result. Experimental results based on both real and synthetic rainy images demonstrate that our proposed solution can outperform several state-of-the-art algorithms, while it consumes much less running time and training time, compared against the competitors.
C1 [Lin, Xiao] Shanghai Normal Univ, Dept Comp Sci, Shanghai 200234, Peoples R China.
   [Lin, Xiao; Ma, Lizhuang; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Ma, Lizhuang] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200241, Peoples R China.
   [Wang, Zhi-Jie] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
   [Wang, Zhi-Jie] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.
   [Chen, Wansheng] Shanghai Normal Univ, Dept Comp Sci & Engn, Shanghai 200234, Peoples R China.
C3 Shanghai Normal University; Shanghai Jiao Tong University; East China
   Normal University; Chongqing University; Sun Yat Sen University;
   Shanghai Normal University
RP Wang, ZJ (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
EM Lin6008@126.com; ma-lz@cs.sjtu.edu.cn; shengbin@sjtu.edu.cn;
   cszjwang@yahoo.com; 1246523423@qq.com
RI Yu, Chongxiu/KDM-7354-2024; Li, Chun/KBC-9591-2024; Sun,
   Peng/KDO-4243-2024
OI Yu, Chongxiu/0000-0002-8221-6221; Wang, Zhi-Jie/0000-0002-6865-7899
FU NSFC [U1811264, 61972425, 61972157, 61775139, 61872242]; National Key
   R&D Program of China [2018YFB1004400]; Shanghai Science and Technology
   Innovation Action Plan Project [16111107502, 17511107203]; Research
   Grants Council of Hong Kong [28200215]; Key R&D Program of Guangdong
   Province [2018B010107005, 2019B010120001]; National High-tech R&D
   Program of China (863 Program) [2015AA015904]; Key Program for
   International S&T Cooperation Project of China [2016YFE0129500]; Science
   and Technology Commission of Shanghai Municipality [16DZ0501100,
   17411952600]
FX This work was supported in part by the NSFC under Grants U1811264,
   61972425, 61972157, 61775139, and 61872242, in part by the National Key
   R&D Program of China under Grant. 2018YFB1004400, in part by the
   Shanghai Science and Technology Innovation Action Plan Project under
   Grants 16111107502 and 17511107203, in part by the Research Grants
   Council of Hong Kong (No. 28200215), in part by the Key R&D Program of
   Guangdong Province under Grants 2018B010107005 and 2019B010120001, in
   part by the National High-tech R&D Program of China (863 Program) under
   Grant 2015AA015904, in part by the Key Program for International S&T
   Cooperation Project of China under Grant 2016YFE0129500, and in part by
   the Science and Technology Commission of Shanghai Municipality under
   Grants 16DZ0501100 and 17411952600. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Sebastian Knorr.
CR [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2807593
   Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2
   Bossu J, 2011, INT J COMPUT VISION, V93, P348, DOI 10.1007/s11263-011-0421-7
   Chen BH, 2017, IEEE T IND ELECTRON, V64, P6573, DOI 10.1109/TIE.2017.2682036
   Chen CLP, 2018, IEEE T NEUR NET LEAR, V29, P10, DOI 10.1109/TNNLS.2017.2716952
   Chen DY, 2014, IEEE T CIRC SYST VID, V24, P1430, DOI 10.1109/TCSVT.2014.2308627
   Chen J, 2018, PROC CVPR IEEE, P6286, DOI 10.1109/CVPR.2018.00658
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Ding XH, 2016, MULTIMED TOOLS APPL, V75, P2697, DOI 10.1007/s11042-015-2657-7
   Fan ZW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1751, DOI 10.1145/3240508.3240694
   Feng S, 2020, IEEE T CYBERNETICS, V50, P414, DOI 10.1109/TCYB.2018.2857815
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Fu YH, 2011, INT CONF ACOUST SPEE, P1453
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Gui Y, 2020, IEEE T CIRC SYST VID, V30, P4781, DOI 10.1109/TCSVT.2019.2961267
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Jiang TX, 2019, IEEE T IMAGE PROCESS, V28, P2089, DOI 10.1109/TIP.2018.2880512
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189
   Kukolj D, 2004, IEEE T SYST MAN CY B, V34, P272, DOI 10.1109/TSMCB.2003.811119
   Le YQ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4137
   Li S., 2018, ARXIVABS180402688
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Lin X, 2019, INFORM SCIENCES, V485, P521, DOI 10.1016/j.ins.2019.02.002
   Ling ZG, 2018, IEEE T MULTIMEDIA, V20, P1699, DOI 10.1109/TMM.2017.2778565
   Liu W, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1807
   Liu W, 2018, LECT NOTES COMPUT SC, V10827, P67, DOI 10.1007/978-3-319-91452-7_5
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Nie MD, 2019, INFORM SCIENCES, V476, P95, DOI 10.1016/j.ins.2018.10.010
   Ouali B., 2008, ELECTRON LETT, V44, P800
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Quan Z, 2019, IEEE INT C BIOINFORM, P717, DOI 10.1109/BIBM47256.2019.8983267
   Quan Z, 2019, IEEE-ACM T AUDIO SPE, V27, P853, DOI 10.1109/TASLP.2019.2899494
   Shen L, 2018, INT C PATT RECOG, P2821, DOI 10.1109/ICPR.2018.8545729
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Wang C, 2017, MULTIMED TOOLS APPL, V76, P2019, DOI 10.1007/s11042-015-3195-z
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Wang YL, 2017, IEEE T IMAGE PROCESS, V26, P3936, DOI 10.1109/TIP.2017.2708502
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZJ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1867, DOI 10.1109/ICASSP.2018.8461285
   Wang ZJ, 2016, GEOINFORMATICA, V20, P19, DOI 10.1007/s10707-015-0230-1
   Wang ZJ, 2015, IEEE T KNOWL DATA EN, V27, P866, DOI 10.1109/TKDE.2014.2345402
   Wang ZT, 2018, AER ADV ENG RES, V152, P1
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Wu HN, 2018, IEEE T FUZZY SYST, V26, P3379, DOI 10.1109/TFUZZ.2018.2826475
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Xu J., 2012, CANADIAN CTR SCI ED, V5, P49, DOI DOI 10.5539/CIS.V5N3P49
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yu LY, 2019, IEEE T MULTIMEDIA, V21, P1621, DOI 10.1109/TMM.2018.2887027
   Zhang H., 2017, ARXIVABS170105957
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang H, 2018, IEEE T IMAGE PROCESS, V27, P2121, DOI 10.1109/TIP.2017.2786469
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang X.-P., 2016, PROC IEEE INT C MULT, P1
   Zhao TY, 2018, IEEE T IMAGE PROCESS, V27, P4740, DOI 10.1109/TIP.2018.2845118
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 63
TC 17
Z9 18
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 664
EP 676
DI 10.1109/TMM.2020.2987703
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200011
DA 2024-07-18
ER

PT J
AU Lin, YH
   Chen, HH
AF Lin, Yi-Hsun
   Chen, Homer H.
TI Tag Propagation and Cost-Sensitive Learning for Music Auto-Tagging
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Training data; Propagation losses; Multimedia systems;
   Tagging; Music; Social networking (online); Music auto-tagging; music
   information retrieval; tag propagation; cost-sensitive learning
ID MODEL
AB The performance of music auto-tagging depends on the quality of training data. In practice, the links between songs and tags in the manually labeled training data can be incorrect (false positive) or missing (false negative). In this paper, we propose a cost-sensitive tag propagation learning method to improve auto-tagging. Specifically, we exploit music context to determine similar songs and propagate tags between them. Both propagated tags and original tags are used to optimize the auto-tagging models, and cost-sensitivity is incorporated into the loss function to enhance the robustness by adjusting the weight of relevant (positive) links with respect to irrelevant (negative) links. The proposed method is tested on three auto-tagging models: 2D-CNN, CRNN, and SampleCNN. The Million Song Dataset is used for training, and four music contexts, artist, playlist, tag, and listener, are used for song similarity measurement. The experimental results show 1) The proposed method can successfully improve the performance of the three auto-tagging models, 2) The cost-sensitive loss function helps reduce the impact of missing tags, and 3) The artist music context is more powerful for tag propagation than the other three music contexts.
C1 [Lin, Yi-Hsun] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Dept Elect Engn, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University; National Taiwan
   University
RP Chen, HH (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Grad Inst Commun Engn, Taipei 10617, Taiwan.; Chen, HH (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
EM jeffrey82221@gmail.com; homer@ntu.edu.tw
FU Ministry of Science and Technology of Taiwan [106-2221-E-002-041-MY3,
   108-2218-E-002-054]
FX This work was supported by the Ministry of Science and Technology of
   Taiwan under Grants 106-2221-E-002-041-MY3 and 108-2218-E-002-054. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Pro. YeWang.
CR [Anonymous], Learning from labeled and unlabeled data with label propagation
   Baluja S, 2008, WORLD WID WEB C, P895
   Begwani N., 2018, ARXIV181112776
   Bertin-Mahieux T, 2008, J NEW MUSIC RES, V37, P115, DOI 10.1080/09298210802479250
   Bertoni A, 2011, LECT NOTES ARTIF INT, V6911, P219, DOI 10.1007/978-3-642-23780-5_24
   Chen CM, 2013, 2013 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCES ON WEB INTELLIGENCE (WI) AND INTELLIGENT AGENT TECHNOLOGIES (IAT), VOL 1, P65, DOI 10.1109/WI-IAT.2013.10
   Choi K, 2017, INT CONF ACOUST SPEE, P2392, DOI 10.1109/ICASSP.2017.7952585
   Choi Keunwoo, 2016, ARXIV160600298, DOI 10.5281/zenodo.1416254
   Chung CH, 2017, P 18 INT SOC MUS INF, P478
   Fan W, 1999, MACHINE LEARNING, PROCEEDINGS, P97
   Feki Ghada, 2013, 2013 11th International Workshop on Content-Based Multimedia Indexing (CBMI), P149, DOI 10.1109/CBMI.2013.6576573
   Frénay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Jin C, 2018, J VIS COMMUN IMAGE R, V55, P720, DOI 10.1016/j.jvcir.2018.08.009
   Kanishcheva O., 2015, P BCI ACM, P1
   Kim JH, 2009, P INT SOC MUS INF RE, V9, P375
   Knees P, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542206
   Lamere P, 2008, J NEW MUSIC RES, V37, P101, DOI 10.1080/09298210802479284
   Law E, 2010, LECT NOTES ARTIF INT, V6322, P211, DOI 10.1007/978-3-642-15883-4_14
   Lee J, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8010150
   Leevy JL, 2018, J BIG DATA-GER, V5, DOI 10.1186/s40537-018-0151-6
   Li Y., 2019, PLOS ONE, V14
   Lin YH, 2018, EUR SIGNAL PR CONF, P2270, DOI 10.23919/EUSIPCO.2018.8553318
   Liu D, 2011, IEEE T MULTIMEDIA, V13, P702, DOI 10.1109/TMM.2011.2134078
   Lo HY, 2011, IEEE T MULTIMEDIA, V13, P518, DOI 10.1109/TMM.2011.2129498
   Makki S., 2018, P 1 INT C BIG DATA C, P42
   McFee B, 2012, P 21 INT C WORLD WID, P909, DOI [DOI 10.1145/2187980, 10.1145/2187980.2188222]
   Moore J.L., 2012, P ISMIR, P349
   Patel T, 2017, 2017 INTERNATIONAL CONFERENCE ON INNOVATIVE MECHANISMS FOR INDUSTRY APPLICATIONS (ICIMIA), P219, DOI 10.1109/ICIMIA.2017.7975606
   Salminen J, 2019, J BUS RES, V101, P203, DOI 10.1016/j.jbusres.2019.04.018
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Tsukuda K., 2017, P 18 INT SOC MUS INF, P544
   Wan J., 2018, IEEE Trans Inf. Forensics Secur., V14, P1729
   Wei T, 2020, IEEE T NEUR NET LEAR, V31, P2315, DOI 10.1109/TNNLS.2019.2935143
   Wu GQ, 2018, NEURAL NETWORKS, V108, P411, DOI 10.1016/j.neunet.2018.09.003
   Yang Y.-H., 2012, Proceedings of the 21st international conference companion on World Wide Web, P869, DOI DOI 10.1145/2187980.2188217
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
NR 36
TC 6
Z9 6
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1605
EP 1616
DI 10.1109/TMM.2020.3001521
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300010
DA 2024-07-18
ER

PT J
AU Mei, SH
   Ma, MY
   Wan, S
   Hou, JH
   Wang, ZY
   Feng, DD
AF Mei, Shaohui
   Ma, Mingyang
   Wan, Shuai
   Hou, Junhui
   Wang, Zhiyong
   Feng, David Dagan
TI Patch Based Video Summarization With Block Sparse Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video summarization (VS); keyframe extraction; orthogonal matching
   pursuit (OMP); sparse representation; block sparse representation
AB In recent years, sparse representation has been successfully utilized for video summarization (VS). However, most of the sparse representation based VS methods characterize each video frame with global features. As a result, some important local details could be neglected by global features, which may compromise the performance of summarization. In this paper, we propose to partition each video frame into a number of patches and characterize each patch with global features. Instead of concatenating the features of each patch and utilizing conventional sparse representation, we formulate the VS problem with such video frame representation as block sparse representation by considering each video frame as a block containing a number of patches. By taking the reconstruction constraint into account, we devise a simultaneous version of block-based OMP (Orthogonal Matching Pursuit) algorithm, namely SBOMP, to solve the proposed model. The proposed model is further extended to a neighborhood based model which considers temporally adjacent frames as a super block. This is one of the first sparse representation based VS methods taking both spatial and temporal contexts into account with blocks. Experimental results on two widely used VS datasets have demonstrated that our proposed methods present clear superiority over existing sparse representation based VS methods and are highly comparable to some deep learning ones requiring supervision information for extra model training.
C1 [Mei, Shaohui; Ma, Mingyang; Wan, Shuai] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
   [Hou, Junhui] City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Hou, Junhui] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 51800, Peoples R China.
   [Wang, Zhiyong; Feng, David Dagan] Univ Sydney, Sch Comp Sci, Sydney, NSW 2006, Australia.
C3 Northwestern Polytechnical University; City University of Hong Kong;
   Shenzhen Research Institute, City University of Hong Kong; City
   University of Hong Kong; University of Sydney
RP Mei, SH (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
EM meish@nwpu.edu.cn; mamingyang@mail.nwpu.edu.cn; swan@nwpu.edu.cn;
   jh.hou@cityu.edu.hk; zhiyong.wang@sydney.edu.au;
   dagan.feng@sydney.edu.au
RI Wang, Zhiyong/AAJ-3419-2021; Mei, Shao-Hui/AAB-9154-2022; Ma,
   Mingyang/AAA-8074-2022; Wan, Shuai/AAA-8777-2022; Ma,
   Mingyang/JXM-3330-2024
OI Wan, Shuai/0000-0001-8617-149X; Wang, Zhiyong/0000-0002-8043-0312; Hou,
   Junhui/0000-0003-3431-2021; Feng, Dagan/0000-0002-3381-214X; Ma,
   Mingyang/0000-0002-2944-628X; MEI, Shaohui/0000-0002-8018-596X
FU Innovation Foundation for Doctor Dissertation of Northwestern
   Polytechnical University [CX201914]; National Natural Science Foundation
   of China [61671383, 61871342]; Fundamental Research Funds for the
   Central Universities [3102018AX001]; Hong Kong Research Grants Council
   [9042820]
FX This work was supported in part by the Innovation Foundation for Doctor
   Dissertation of Northwestern Polytechnical University under Grant
   CX201914, in part by the National Natural Science Foundation of China
   under Grants 61671383 and 61871342, in part by the Fundamental Research
   Funds for the Central Universities under Grant 3102018AX001, and in part
   by Hong Kong Research Grants Council under Grant 9042820.
CR [Anonymous], 2011, OPEN VIDEO PROJECT
   [Anonymous], 2010, Image Analysis for Multimedia Interactive Services (WIAMIS), 2010 11th International Workshop on, DOI DOI 10.1109/WIC0M.2010.5601233
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Cong Y, 2017, IEEE T IMAGE PROCESS, V26, P185, DOI 10.1109/TIP.2016.2619260
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Donahue J, 2014, PR MACH LEARN RES, V32
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eldar YC, 2010, IEEE T SIGNAL PROCES, V58, P3042, DOI 10.1109/TSP.2010.2044837
   Eldar YC, 2009, 2009 16TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, VOLS 1 AND 2, P1
   Eldar YC, 2009, IEEE T INFORM THEORY, V55, P5302, DOI 10.1109/TIT.2009.2030471
   Elhamifar E, 2017, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2017.197
   Elhamifar E, 2012, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2012.6247852
   Elhamifar E, 2012, IEEE T SIGNAL PROCES, V60, P4094, DOI 10.1109/TSP.2012.2196694
   Etezadifar P, 2017, MULTIMED TOOLS APPL, V76, P7947, DOI 10.1007/s11042-016-3433-z
   Fathian M, 2011, LECT NOTES ARTIF INT, V7004, P12, DOI 10.1007/978-3-642-23896-3_2
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Fu TJ, 2019, IEEE WINT CONF APPL, P1579, DOI 10.1109/WACV.2019.00173
   Furini M, 2007, P 6 ACM INT C IM VID, P635
   Furini M, 2010, MULTIMED TOOLS APPL, V46, P47, DOI 10.1007/s11042-009-0307-7
   Guan GL, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2632267
   Guan GL, 2013, IEEE T CIRC SYST VID, V23, P729, DOI 10.1109/TCSVT.2012.2214871
   Guan GL, 2012, IEEE INT CONF MULTI, P570, DOI 10.1109/ICMEW.2012.105
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Hannane R, 2016, INT J MULTIMED INF R, V5, P89, DOI 10.1007/s13735-016-0095-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang BX, 2015, SIGNAL PROCESS, V106, P231, DOI 10.1016/j.sigpro.2014.07.023
   Huang C, 2020, IEEE T CIRC SYST VID, V30, P577, DOI 10.1109/TCSVT.2019.2890899
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Ji Z, 2020, IEEE T CIRC SYST VID, V30, P1709, DOI 10.1109/TCSVT.2019.2904996
   Ji Z, 2019, INFORM SCIENCES, V478, P152, DOI 10.1016/j.ins.2018.09.050
   Jung Y, 2019, AAAI CONF ARTIF INTE, P8537
   Kumar M., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2437, DOI 10.1109/ICIP.2011.6116136
   Li JT, 2017, NEUROCOMPUTING, V266, P66, DOI 10.1016/j.neucom.2017.04.065
   Liu YL, 2014, IEEE IJCNN, P3909, DOI 10.1109/IJCNN.2014.6889581
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Ma M., 2019, VIDEO SUMMARIZATION, DOI [10.1016/j.neucom.2019.07.108, DOI 10.1016/J.NEUCOM.2019.07.108]
   Ma MY, 2019, IEEE ACCESS, V7, P11763, DOI 10.1109/ACCESS.2019.2891834
   Ma MY, 2017, IEEE IMAGE PROC, P2911, DOI 10.1109/ICIP.2017.8296815
   Mahasseni B, 2017, PROC CVPR IEEE, P2077, DOI 10.1109/CVPR.2017.224
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Mei SH, 2014, IEEE INT CON MULTI
   Mei SH, 2015, PATTERN RECOGN, V48, P522, DOI 10.1016/j.patcog.2014.08.002
   Mishali M, 2010, IEEE J-STSP, V4, P375, DOI 10.1109/JSTSP.2010.2042414
   Panda R, 2017, PROC CVPR IEEE, P4274, DOI 10.1109/CVPR.2017.455
   Plummer BA, 2017, PROC CVPR IEEE, P1052, DOI 10.1109/CVPR.2017.118
   Rochan M, 2018, LECT NOTES COMPUT SC, V11216, P358, DOI 10.1007/978-3-030-01258-8_22
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Simakov D., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587842
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   SONG YL, 2015, PROC CVPR IEEE, P5179, DOI [DOI 10.1109/CVPR.2015.7299154, 10.1109/CVPR.2015.7299154]
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wang S, 2016, ARTIF INTELL MED, V66, P1, DOI 10.1016/j.artmed.2015.08.006
   Wei HW, 2018, AAAI CONF ARTIF INTE, P216
   Yu KM, 2014, NEUROCOMPUTING, V129, P136, DOI 10.1016/j.neucom.2013.09.046
   Zhang JP, 2018, IEEE J BIOMED HEALTH, V22, P1521, DOI 10.1109/JBHI.2017.2775662
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322
   Zhu XQ, 2004, MULTIMEDIA SYST, V10, P98, DOI 10.1007/s00530-004-0142-7
NR 63
TC 21
Z9 21
U1 2
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 732
EP 747
DI 10.1109/TMM.2020.2987683
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200015
DA 2024-07-18
ER

PT J
AU Park, SH
   Kang, JW
AF Park, Sang-hyo
   Kang, Je-Won
TI Fast Multi-Type Tree Partitioning for Versatile Video Coding Using a
   Lightweight Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Complexity theory; Image coding; Transforms; Tools; Standards;
   Quantization (signal); Block partitioning; Deep learning; Encoding
   complexity; Image compression; Intra prediction; Multi-type tree; Neural
   network; Video compression; VVC
ID CU SIZE DECISION; DEPTH DECISION; MODE; SELECTION
AB In this paper, we propose a fast decision scheme using a lightweight neural network (LNN) to avoid redundant block partitioning in versatile video coding (VVC). A more versatile block structure, named the multi-type tree (MTT) structure, which includes binary trees (BTs) and ternary trees (TTs), is adopted by VCC, in addition to the traditional quadtree structure. The MTT improved the coding efficiency compared with previous video coding standards. However, the new tree structures, mainly TT, significantly increased the complexity of the VVC encoder. Although widespread application of VVC has been inhibited, this problem has not yet been investigated thoroughly in the literature. In this study, we first determine the statistical characteristics of coded parameters that exhibit correlation with the TT and develop two useful types of features-explicit VVC features (EVFs) and derived VVC features (DVFs)-to facilitate the intra coding of VVC. These features can be obtained efficiently during the intra prediction before the determination of the best block partitioning during rate-distortion optimization in VVC encoding. Our LNN model decides whether to terminate the nested TT block structures subsequent to a quadtree based on the features. The experimental results confirm that the proposed method substantially decreases the encoding complexity of VVC with a slight coding loss under the All Intra configuration. Our code, models, and dataset are available at https://github.com/foriamweak/MTTPartitioning_LNN.
C1 [Park, Sang-hyo] Kyungpook Natl Univ, Sch Comp Sci & Engn, Daegu 41566, South Korea.
   [Kang, Je-Won] Ewha Womans Univ, Dept Elect & Elect Engn, Seoul, South Korea.
   [Kang, Je-Won] Ewha Womans Univ, Smart Factory Multidisciplinary Program, Seoul, South Korea.
C3 Kyungpook National University; Ewha Womans University; Ewha Womans
   University
RP Kang, JW (corresponding author), Ewha Womans Univ, Dept Elect & Elect Engn, Seoul, South Korea.; Kang, JW (corresponding author), Ewha Womans Univ, Smart Factory Multidisciplinary Program, Seoul, South Korea.
EM s.park@knu.ac.kr; jewonk@ewha.ac.kr
OI Park, Sang-hyo/0000-0002-7282-7686; Kang, Jewon/0000-0002-1637-9479
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [NRF-2019R1C1C1010249]; BK21 FOUR project (AI-driven Convergence
   Software Education Research Program) - Ministry of Education, School of
   Computer Science and Engineering, Kyungpook National University, Korea
   [4199990214394]; Basic Science Research Program through the NRF -
   Ministry of Education [2020R1I1A3072227]
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) grant funded by the Korea government (MSIT) under Grant
   NRF-2019R1C1C1010249, in part by the BK21 FOUR project (AI-driven
   Convergence Software Education Research Program) funded by the Ministry
   of Education, School of Computer Science and Engineering, Kyungpook
   National University, Korea under Grant 4199990214394, and in part by the
   Basic Science Research Program through the NRF funded by the Ministry of
   Education under Grant 2020R1I1A3072227.
CR Bossen, 2019, JVETN1010
   Bossen F., 2019, JVETO0003 ITUT ISOIE
   Bross B., 2019, JVET M
   Chien W.-J, 2019, JVETO0013
   Cho S, 2013, IEEE T CIRC SYST VID, V23, P1555, DOI 10.1109/TCSVT.2013.2249017
   Duanmu F, 2016, IEEE J EM SEL TOP C, V6, P517, DOI 10.1109/JETCAS.2016.2597698
   Fan YB, 2020, IEEE ACCESS, V8, P107900, DOI 10.1109/ACCESS.2020.3000565
   Gu JW, 2018, IEEE SIGNAL PROC LET, V25, P159, DOI 10.1109/LSP.2017.2766766
   Gweon R. H., 2011, JCTVCF045 ITUT ISOIE
   High Efficiency Video Coding, 2013, Standard Rec. ITU-T H.265, Version 1, ISO/IEC Standard 23008-2
   Jin ZP, 2018, IEEE ACCESS, V6, P54660, DOI 10.1109/ACCESS.2018.2872492
   Kim K, 2019, IEEE T CIRC SYST VID, V29, P1462, DOI 10.1109/TCSVT.2018.2839113
   Lin P. H., 2017, JVETF0063 ITUT ISOIE
   Liu ZY, 2016, IEEE T IMAGE PROCESS, V25, P5088, DOI 10.1109/TIP.2016.2601264
   Min B, 2015, IEEE T CIRC SYST VID, V25, P892, DOI 10.1109/TCSVT.2014.2363739
   Park SH, 2019, IEEE ACCESS, V7, P172597, DOI 10.1109/ACCESS.2019.2956196
   Park SH, 2015, 2015 IEEE 5TH INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - BERLIN (ICCE-BERLIN), P113, DOI 10.1109/ICCE-Berlin.2015.7391208
   Peng ZJ, 2019, SIGNAL PROCESS-IMAGE, V78, P171, DOI 10.1016/j.image.2019.06.014
   Ryu S, 2018, IEEE T IMAGE PROCESS, V27, P5525, DOI 10.1109/TIP.2018.2857404
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Shen LQ, 2013, IEEE T CONSUM ELECTR, V59, P207, DOI 10.1109/TCE.2013.6490261
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P269, DOI 10.1109/TMM.2018.2856078
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xu M, 2018, IEEE T IMAGE PROCESS, V27, P5044, DOI 10.1109/TIP.2018.2847035
   Yang H, 2020, IEEE T CIRC SYST VID, V30, P1668, DOI 10.1109/TCSVT.2019.2904198
   Zhang QW, 2020, IEEE ACCESS, V8, P117539, DOI 10.1109/ACCESS.2020.3004580
   Zhang T, 2017, IEEE T CIRC SYST VID, V27, P1714, DOI 10.1109/TCSVT.2016.2556518
   Zhang YM, 2016, IEEE ACM T NETWORK, V24, P1632, DOI 10.1109/TNET.2015.2425146
   Zhang Y, 2018, IEEE T CIRC SYST VID, V28, P3208, DOI 10.1109/TCSVT.2017.2747659
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
NR 32
TC 39
Z9 39
U1 3
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4388
EP 4399
DI 10.1109/TMM.2020.3042062
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900037
DA 2024-07-18
ER

PT J
AU Shi, YX
   Wei, Z
   Ling, HF
   Wang, ZY
   Zhu, PF
   Shen, JL
   Li, P
AF Shi, Yuxuan
   Wei, Zhen
   Ling, Hefei
   Wang, Ziyang
   Zhu, Pengfei
   Shen, Jialie
   Li, Ping
TI Adaptive and Robust Partition Learning for Person Retrieval With Policy
   Gradient
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Adaptation models; Robustness; Pose estimation;
   Adaptive systems; Training; Computational modeling; Adaptive partition
   network; person re-identification (re-id); person retrieval;
   reinforcement learning
ID NETWORK
AB Person retrieval aims at effectively matching the pedestrian images over an extensive database given a specified identity. As extracting effective features is crucial in a high-performance retrieval system, recent significant progress was achieved by part-based models that have constructed robust local representations on top of vertically striped part features. However, this kind of models use predefined partitioning strategies, making the number and size of each partition identical even when input images vary a lot. This unchangeable setting usually leads to less flexibility and robustness in capturing visual variance. The primary reason for such a negative effect is that a fixed partitioning strategy is unable to deal with (a) the significant variance from pose, illumination and viewpoint which is common in a pedestrian image dataset, and (b) also the inference error and misalignment of human bodies introduced by the prepositive pedestrian detection module or human pose estimation module. In this paper, we tackle this problem via introducing the novel Adaptive Partition Network (APN). The APN utilizes deep reinforcement learning and applies an agent to generate optimal partitioning strategies dynamically for different input images. The agent inside the APN is optimized with the policy gradient algorithm and maximizes the reward of choosing the best partition setting. By leveraging the supervision cues from the objective partitioning strategies that are generated on a set of held-out training images, the agent is trained jointly with other parts of APN, which ensures the APN's robustness and generalization ability. Extensive experimental results on multiple datasets, including CUHK03, DukeMTMC and Market-1501, demonstrate the superiority of APN over the state-of-the-art models.
C1 [Shi, Yuxuan; Ling, Hefei; Wang, Ziyang; Li, Ping] Huazhong Univ Sci & Technol, Dept Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Wei, Zhen] Ecole Polytech Fed Lausanne, Sch Comp & Commun Sci, CH-1015 Lausanne, VD, Switzerland.
   [Zhu, Pengfei] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Shen, Jialie] Queens Univ Belfast, Sch EEECS, Belfast BT7 1NN, Antrim, North Ireland.
C3 Huazhong University of Science & Technology; Swiss Federal Institutes of
   Technology Domain; Ecole Polytechnique Federale de Lausanne; Tianjin
   University; Queens University Belfast
RP Ling, HF (corresponding author), Huazhong Univ Sci & Technol, Dept Comp Sci & Technol, Wuhan 430074, Peoples R China.
EM shiyx@hust.edu.cn; zhen.wei@hotmail.com; lhefei@hust.edu.cn;
   wanazyll@hust.edu.cn; zhupengfei@tju.edu.cn; jialie@gmail.com;
   lpshome@hust.edu.cn
OI Shi, Yuxuan/0000-0001-7858-5369; Ling, Hefei/0000-0001-6797-7412
FU Natural Science Foundation of China [61972169, U1536203]; National Key
   Research and Development Program of China [2016QY01W0200]; Major
   Scientific and Technological Project of Hubei Province [2018AAA068,
   2019AAA051]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61972169 and U1536203, in part by the National Key
   Research and Development Program of China under Grant 2016QY01W0200, and
   in part by the Major Scientific and Technological Project of Hubei
   Province (2018AAA068 and 2019AAA051). The associate editor coordinating
   the reviewof this manuscript and approving it for publication was Prof.
   Engin Erzin.
CR [Anonymous], 2018, IEEE Trans. Multimedia
   [Anonymous], 2017, ARXIV
   [Anonymous], 2016, arXiv preprint arXiv:1611.05244
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Barbosa Igor Barros, 2018, COMPUT VIS IMAGE UND
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fox N. A., 2007, IEEE TMM
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Hsieh J.-W., 2008, IEEE TMM
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jose C, 2016, LECT NOTES COMPUT SC, V9909, P875, DOI 10.1007/978-3-319-46454-1_53
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Karanam S., 2016, ARXIV PREPRINT ARXIV
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Ling HF, 2019, NEUROCOMPUTING, V347, P109, DOI 10.1016/j.neucom.2019.01.027
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186
   Shi Y., 2020, IEEE T MULTIMEDIA
   Shi YX, 2020, NEUROCOMPUTING, V402, P124, DOI 10.1016/j.neucom.2020.03.057
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Sunderrajan S., 2016, IEEE TMM
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Ustinova E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang H., 2018, IEEE CVPR
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wei Z, 2019, PROC CVPR IEEE, P7108, DOI 10.1109/CVPR.2019.00728
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Yao H., 2017, ARXIV170700798
   Zhang H., 1995, ACM COMPUT SURVEYS
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Z., 2017, IEEE Trans. on Circuits and Systems for Video Technology
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 62
TC 18
Z9 18
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3264
EP 3277
DI 10.1109/TMM.2020.3023272
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000025
DA 2024-07-18
ER

PT J
AU Wen, J
   Yan, K
   Zhang, Z
   Xu, Y
   Wang, JQ
   Fei, LK
   Zhang, B
AF Wen, Jie
   Yan, Ke
   Zhang, Zheng
   Xu, Yong
   Wang, Junqian
   Fei, Lunke
   Zhang, Bob
TI Adaptive Graph Completion Based Incomplete Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Electronic mail; Clustering methods; Machine learning; Visualization;
   Task analysis; Optimization; Incomplete multi-view clustering; common
   representation; graph completion; similarity graph
AB In real-world applications, it is often that the collected multi-view data are incomplete, i.e., some views of samples are absent. Existing clustering methods for incomplete multi-view data all focus on obtaining a common representation or graph from the available views but neglect the hidden information of missing views and information imbalance of different views. To solve these problems, a novel method, called adaptive graph completion based incomplete multi-view clustering (AGC_IMC), is proposed in this paper. Specifically, AGC_IMC develops a joint framework for graph completion and consensus representation learning, which mainly contains three components, i.e., within-view preservation, between-view inferring, and consensus representation learning. To reduce the negative influence of information imbalance, AGC_IMC introduces some adaptive weights to balance the importance of different views during the consensus representation learning. Importantly, AGC_IMC has the potential to recover the similarity graphs of all views with the optimal cluster structure, which encourages it to obtain a more discriminative consensus representation. Experimental results on five well-known datasets show that AGC_IMC significantly outperforms the state-of-the-art methods.
C1 [Wen, Jie; Yan, Ke; Zhang, Zheng; Xu, Yong; Wang, Junqian] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Zhang, Zheng; Xu, Yong] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Fei, Lunke] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Zhang, Bob] Univ Macau, Dept Comp & Informat Sci, PAMI Res Grp, Taipa, Macao, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Guangdong
   University of Technology; University of Macau
RP Xu, Y (corresponding author), Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.; Xu, Y (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM jiewen_pr@126.com; yanke401@163.com; darrenzz219@gmail.com;
   yongxu@ymail.com; wangjunqian@stu.hit.edu.cn; flksxm@126.com;
   bobzhang@um.edu.mo
RI Yan, Kefen/GZK-4905-2022; Zhang, Zhang/JAX-2097-2023; Wen,
   Jie/AAH-8083-2020; Zhang, Bob/HIR-3656-2022; Zhang, Bob/ABD-5926-2021;
   Wen, Jie/G-7235-2015
OI Zhang, Bob/0000-0001-6512-0474; Zhang, Bob/0000-0003-2497-9519; Zhang,
   Zheng/0000-0003-1470-6998; Fei, Lunke/0000-0001-6072-7875; Wen,
   Jie/0000-0001-9554-2379; yan, ke/0000-0002-5326-4267
FU Shenzhen Fundamental Research Fund [JCYJ20190806142416685]; Guangdong
   Basic and Applied Basic Research Foundation [2019A1515110582,
   2019A1515110475]; National Postdoctoral Program for Innovative Talent
   [BX20190100]; Establishment of Key Laboratory of Shenzhen Science and
   Technology Innovation Committee [ZDSYS20190902093015527]; Natural
   Science Foundation of Guangdong Province [2019A1515011811]; University
   of Macau [MYRG2019-00006-FST]
FX This work was supported in part by Shenzhen Fundamental Research Fund
   under Grant JCYJ20190806142416685, in part by Guangdong Basic and
   Applied Basic Research Foundation under Grants 2019A1515110582 and
   2019A1515110475, in part by the National Postdoctoral Program for
   Innovative Talent under Grant BX20190100, in part by the Establishment
   of Key Laboratory of Shenzhen Science and Technology Innovation
   Committee under Grant ZDSYS20190902093015527, in part by the Natural
   Science Foundation of Guangdong Province under Grant 2019A1515011811,
   and in part by the University of Macau (File no. MYRG2019-00006-FST).
CR Bhadra S, 2017, MACH LEARN, V106, P713, DOI 10.1007/s10994-016-5618-0
   Chao Guoqing, 2017, ARXIV171206246
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Greene D., 2006, P 23 INT C MACH LEAR, V148, P377
   Guo J, 2018, AAAI CONF ARTIF INTE, P298
   HAGEN L, 1992, IEEE T COMPUT AID D, V11, P1074, DOI 10.1109/43.159993
   Hu ML, 2019, AAAI CONF ARTIF INTE, P3838
   Hu ML, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2262
   Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuanar SK, 2015, IEEE T MULTIMEDIA, V17, P1166, DOI 10.1109/TMM.2015.2443558
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, PICT COD SYMP, P164, DOI 10.1109/PCS.2018.8456278
   Kumar P., 2011, Adv. Neural Inf. Process. Syst., P1413, DOI DOI 10.5555/2986459.2986617
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li JX, 2021, IEEE T CYBERNETICS, V51, P534, DOI 10.1109/TCYB.2019.2915789
   Li JX, 2019, INFORM FUSION, V45, P215, DOI 10.1016/j.inffus.2018.02.005
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Liu XW, 2021, IEEE T PATTERN ANAL, V43, P2634, DOI 10.1109/TPAMI.2020.2974828
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1303, DOI 10.1109/TPAMI.2019.2895608
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1191, DOI 10.1109/TPAMI.2019.2892416
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Luo GN, 2018, IEEE T BIO-MED ENG, V65, P1924, DOI 10.1109/TBME.2017.2762762
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Rai N, 2016, INT C PATT RECOG, P2192, DOI 10.1109/ICPR.2016.7899961
   Ren PZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2644
   Shao WX, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1012, DOI 10.1109/BigData.2016.7840701
   Shao WX, 2015, LECT NOTES ARTIF INT, V9284, P318, DOI 10.1007/978-3-319-23528-8_20
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   Simonyan K., 2014, 14091556 ARXIV
   Tang C, 2019, AAAI CONF ARTIF INTE, P5101
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Trivedi A., 2010, NIPS WORKSH, P1
   Wang H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3677
   Wang QQ, 2018, IEEE DATA MINING, P1290, DOI 10.1109/ICDM.2018.00174
   Wen J, 2021, IEEE T CYBERNETICS, V51, P101, DOI 10.1109/TCYB.2020.2987164
   Wen J, 2019, AAAI CONF ARTIF INTE, P5393
   Wen J, 2020, IEEE T CIRC SYST VID, V30, P75, DOI 10.1109/TCSVT.2018.2889727
   Wen J, 2020, IEEE T CYBERNETICS, V50, P1418, DOI 10.1109/TCYB.2018.2884715
   Wen J, 2018, NEURAL NETWORKS, V108, P83, DOI 10.1016/j.neunet.2018.08.007
   Wen J, 2018, PATTERN RECOGN, V81, P326, DOI 10.1016/j.patcog.2018.04.004
   Wright TG, 2001, SIAM J SCI COMPUT, V23, P591, DOI 10.1137/S1064827500373012
   Wu J, 2018, LECT NOTES ARTIF INT, V11012, P98, DOI 10.1007/978-3-319-97304-3_8
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xu C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3933
   Yan K, 2021, IEEE ACM T COMPUT BI, V18, P2682, DOI 10.1109/TCBB.2020.2991268
   Yan K, 2019, BIOINFORMATICS, V35, P2982, DOI 10.1093/bioinformatics/btz040
   Yang Y, 2018, BIG DATA MIN ANAL, V1, P83, DOI 10.26599/BDMA.2018.9020003
   Yi SY, 2019, IEEE T MULTIMEDIA, V21, P1399, DOI 10.1109/TMM.2018.2877888
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang Changqing, 2019, P ADV NEUR INF PROC, P557
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zhao H., 2016, IJCAI, P2392
NR 61
TC 104
Z9 106
U1 10
U2 50
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2493
EP 2504
DI 10.1109/TMM.2020.3013408
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800026
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Yang, XH
   Li, F
   Liu, HT
AF Yang, Xiaohan
   Li, Fan
   Liu, Hantao
TI TTL-IQA: Transitive Transfer Learning Based No-Reference Image Quality
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Distortion; Image quality; Databases; Image recognition;
   Feature extraction; Deep learning; Transitive transfer learning; image
   quality assessment; auxiliary domain; distortion translation; semantic
   feature transfer; generative adversarial network
ID STATISTICS; FRAMEWORK; NETWORK
AB Image quality assessment (IQA) based on deep learning faces the overfitting problem due to limited training samples available in existing IQA databases. Transfer learning is a plausible solution to the problem, in which the shared features derived from the large-scale Imagenet source domain could be transferred from the original recognition task to the intended IQA task. However, the Imagenet source domain and the IQA target domain as well as their corresponding tasks are not directly related. In this paper, we propose a new transitive transfer learning method for no-reference image quality assessment (TTL-IQA). First, the architecture of the multi-domain transitive transfer learning for IQA is developed to transfer the Imagenet source domain to the auxiliary domain, and then to the IQA target domain. Second, the auxiliary domain and the auxiliary task are constructed by a new generative adversarial network based on distortion translation (DT-GAN). Furthermore, a TTL network of the semantic features transfer (SFTnet) is proposed to optimize the shared features for the TTL-IQA. Experiments are conducted to evaluate the performance of the proposed method on various IQA databases, including the LIVE, TID2013, CSIQ, LIVE multiply distorted and LIVE challenge. The results show that the proposed method significantly outperforms the state-of-the-art methods. In addition, our proposed method demonstrates a strong generalization ability.
C1 [Yang, Xiaohan; Li, Fan] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF243AA, Wales.
C3 Xi'an Jiaotong University; Cardiff University
RP Li, F (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
EM yangxiaohan@stu.xjtu.edu.cn; lifan@mail.xjtu.edu.cn;
   LiuH35@cardiff.ac.uk
FU National Science Foundation of China [62071369]
FX This work was supported in part by the National Science Foundation of
   China (62071369).
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Bare B, 2017, IEEE INT CON MULTI, P1356, DOI 10.1109/ICME.2017.8019508
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gong B., 2013, NIPS, P1286
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou WL, 2015, IEEE MULTIMEDIA, V22, P46, DOI 10.1109/MMUL.2014.55
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Isola P., 2017, P IEEE C COMP VIS PA, P1125
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2019, IEEE T IMAGE PROCESS, V28, P1866, DOI 10.1109/TIP.2018.2881828
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Kaiser L., 2017, ARXIV170603059
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li F, 2018, IEEE T MULTIMEDIA, V20, P1154, DOI 10.1109/TMM.2017.2764329
   Li YM, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P685, DOI 10.1109/ICDSP.2016.7868646
   Lin H., 2018, CORR
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Lucie L., 2019, P ICIP, P22
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Makhzani Alireza, 2016, ICLR
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moghadam A. Ebrahimi, 2015, Majlesi J. Electr. Eng., V9, P55, DOI [10.48550/arXiv.1406.7799, DOI 10.48550/ARXIV.1406.7799]
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simonyan K., 2014, 14091556 ARXIV
   Tan B, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1155, DOI 10.1145/2783258.2783295
   Wang SQ, 2016, IEEE J EM SEL TOP C, V6, P532, DOI 10.1109/JETCAS.2016.2598756
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang XH, 2019, IEEE ACCESS, V7, P123788, DOI 10.1109/ACCESS.2019.2938900
   Yang XH, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20110885
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yi HY, 2018, INT C PATT RECOG, P1349, DOI 10.1109/ICPR.2018.8546299
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang J, 2020, IEEE T EVOLUT COMPUT, V24, P424, DOI 10.1109/TEVC.2019.2926107
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang W, 2018, IEEE T CIRC SYST VID, V28, P1462, DOI 10.1109/TCSVT.2017.2650910
   Zhang W, 2017, NEUROCOMPUTING, V247, P183, DOI 10.1016/j.neucom.2017.03.054
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
NR 64
TC 19
Z9 19
U1 6
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4326
EP 4340
DI 10.1109/TMM.2020.3040529
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900032
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Yao, XX
   Wu, Q
   Zhang, P
   Bao, FX
AF Yao, Xunxiang
   Wu, Qiang
   Zhang, Peng
   Bao, Fangxun
TI Weighted Adaptive Image Super-Resolution Scheme Based on Local Fractal
   Feature and Image Roughness
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fractals; Image resolution; Image edge detection; Interpolation; Image
   segmentation; Rough surfaces; Surface roughness; Rational fractal model;
   local fractal analysis; ARFM algorithm; local adaptive threshold;
   sub-block selection
ID SURFACE-ROUGHNESS; INTERPOLATION; REGRESSION; ALGORITHM; NETWORK
AB Image super-resolution aims to reconstruct a high-resolution image from the known low-resolution version. During this process, it should keep the degree of image roughness non-decreasing, which reflects various texture features and appearance. However, this point is not well addressed in the current work. This work argues that reducing roughness during image super-resolution is the key reason causing various problems such as artificial texture and/or edge blur. In this work, keeping the image roughness non-decreasing during super-resolution is being well investigated for the first time to our best knowledge. Image super-resolution is cast as an optimization problem to keep image roughness non-decreasing. In order to tackle this problem, the image super-resolution is approached based on the theory of fractal, where adaptive fractal interpolation function is proposed. In this way, the rational fractal interpolation model is adaptive to every local region. Thus, the roughness of every image region can be best maintained while super-resolution is carried out through fractal interpolation. In this work, the image roughness is reflected by the fractal dimension, which is a key element affecting the construction of fractal interpolation model. That is, the image roughness is measurable using fractal dimension. Mathematically, the overall image super-resolution process can be converted into a fractal interpolation optimization problem where the local fractal dimension is maintained. Although adaptive super-resolution on image segments may best maintain image roughness using the proposed method, it still generates unnecessary block artifacts. To tackle this problem, this work proposes a fine-grained pixel-wise fractal function. Our extensive experimental results demonstrate that the proposed method achieves encouraging performance with the state-of-the-art super-resolution algorithms.
C1 [Yao, Xunxiang; Wu, Qiang; Zhang, Peng] Univ Technol, Global Big Data & Technol Ctr, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.
   [Bao, Fangxun] Shandong Univ, Sch Math, Jinan 250100, Shandong, Peoples R China.
C3 University of Technology Sydney; Shandong University
RP Yao, XX (corresponding author), Univ Technol, Global Big Data & Technol Ctr, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.
EM xunxiang.yao@student.uts.edu.au; qiang.wu@uts.edu.au;
   peng.zhang-2@student.uts.edu.au; fxbao@sdu.edu.cn
OI Wu, Qiang/0000-0001-5641-2483; Zhang, Peng/0000-0001-6794-7352
FU National Natural Science Foundation of China [61672018]; China
   Scholarship Council
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672018 and in part by China
   Scholarship Council. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Chang-Su Kim.
CR Al-Saidi NMG, 2018, COMPUT APPL MATH, V37, P996, DOI 10.1007/s40314-016-0378-9
   [Anonymous], 2014, ACCV WORKSH IM REST
   Asuni N, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P58
   Rodrigues FAA, 2016, IEEE GEOSCI REMOTE S, V13, P132, DOI 10.1109/LGRS.2015.2496340
   Bao FX, 2018, SCI CHINA INFORM SCI, V61, DOI 10.1007/s11432-017-9258-5
   BARNSLEY MF, 1986, CONSTR APPROX, V2, P303, DOI 10.1007/BF01893434
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bouboulis P, 2007, B GREEK MATH SOC, V54, P179
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen HG, 2017, IEEE T MULTIMEDIA, V19, P1702, DOI 10.1109/TMM.2017.2688920
   Cheng J, 2013, PROCEEDINGS OF THE 2013 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE FOR ENGINEERING SOLUTIONS (CIES), P1, DOI 10.1109/CIES.2013.6611721
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Eberhart RC, 2001, IEEE C EVOL COMPUTAT, P81, DOI 10.1109/CEC.2001.934374
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Franzen Rich, Kodak lossless true color image suite
   Gadde SGK, 2016, INVEST OPHTH VIS SCI, V57, P246, DOI 10.1167/iovs.15-18287
   Giachetti A, 2011, IEEE T IMAGE PROCESS, V20, P2760, DOI 10.1109/TIP.2011.2136352
   Giusto DD, 2005, IEEE T CONSUM ELECTR, V51, P103, DOI 10.1109/TCE.2005.1405706
   Gribbon KT, 2004, INT SYM ELECT DES TE, P126
   Han ZJ, 2001, IEEE T IND ELECTRON, V48, P920, DOI 10.1109/41.954556
   Han ZJ, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P222, DOI 10.1109/ICIP.1998.727171
   He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Kamguem R, 2013, INT J PRECIS ENG MAN, V14, P183, DOI 10.1007/s12541-013-0026-x
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lee KC, 2005, PRECIS ENG, V29, P95, DOI 10.1016/j.precisioneng.2004.05.002
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Mandelbrot B.B., 1983, The fractal geometry of nature, v, VVolume 1, P173
   Matsumoto S, 2012, OPTIM LETT, V6, P1265, DOI 10.1007/s11590-011-0371-6
   Millán H, 2007, GEODERMA, V138, P185, DOI 10.1016/j.geoderma.2006.11.019
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Park SJ, 2018, LECT NOTES COMPUT SC, V11220, P455, DOI 10.1007/978-3-030-01270-0_27
   Peleg S, 1984, IEEE Trans Pattern Anal Mach Intell, V6, P518, DOI 10.1109/TPAMI.1984.4767557
   PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P661, DOI 10.1109/TPAMI.1984.4767591
   Pickup LC, 2004, ADV NEUR IN, V16, P1587
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   Tai YW, 2010, PROC CVPR IEEE, P2400, DOI 10.1109/CVPR.2010.5539933
   Tian YP, 2016, IEEE IMAGE PROC, P2827, DOI 10.1109/ICIP.2016.7532875
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tu GF, 2005, INT C COMMUN CIRCUIT, P701
   Wang Q, 2005, IEEE I CONF COMP VIS, P709
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Wee YC, 2010, IEEE T CONSUM ELECTR, V56, P1537, DOI 10.1109/TCE.2010.5606294
   Wei XY, 2016, IEEE T IMAGE PROCESS, V25, P3723, DOI 10.1109/TIP.2016.2563178
   Xie HP, 1997, FRACTALS, V5, P625, DOI 10.1142/S0218348X97000504
   Xu HT, 2013, IEEE T CIRC SYST VID, V23, P1740, DOI 10.1109/TCSVT.2013.2248305
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yao XX, 2019, IMAGE VISION COMPUT, V82, P39, DOI 10.1016/j.imavis.2019.02.002
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang CM, 2013, IEEE IMAGE PROC, P1046, DOI 10.1109/ICIP.2013.6738216
   Zhang HC, 2013, IEEE T CYBERNETICS, V43, P1035, DOI 10.1109/TSMCB.2012.2222375
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang YF, 2020, IEEE T MULTIMEDIA, V22, P1407, DOI 10.1109/TMM.2019.2943750
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 64
TC 11
Z9 14
U1 3
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1426
EP 1441
DI 10.1109/TMM.2020.2997126
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200019
DA 2024-07-18
ER

PT J
AU Zhang, P
   Xu, JS
   Wu, Q
   Huang, Y
   Ben, XY
AF Zhang, Peng
   Xu, Jingsong
   Wu, Qiang
   Huang, Yan
   Ben, Xianye
TI Learning Spatial-Temporal Representations Over Walking Tracklet for
   Long-Term Person Re-Identification in the Wild
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Tracking; Three-dimensional displays; Image color analysis;
   Cameras; Streaming media; Trajectory; Long-term person
   re-identification; space-time patterns; 3D skeleton normalization;
   dataset collection
ID NETWORK; ATTENTION
AB Long-term person re-identification (re-ID) aims to build identity correspondence of the Target Subject of Interest (TSI) exposed under surveillance cameras over a long time interval. Compared to the conventional short-term re-ID studied by most existing works, it suffers an additional problem: significant dressing change observed with time lapsing. Unfortunately, this variation in long-term person re-ID case contradicts the assumption of prior short-term re-ID approaches, and thus causes significant difficulties if conventional short-term re-ID methods are applied. To address the problem, this paper proposes to learn hybrid feature representation via a two-stream network named SpTSkM, including a spatial-temporal stream and a skeleton motion stream. The former performs directly on image sequences, which tends to learn identity-related spatial-temporal patterns such as body geometric structure and body movement. The latter operates on normalized 3D skeletons by adapting graph convolutional network, which tends to learn pure motion patterns from skeleton sequences. Both streams extract fine-grained level time-gap stable information that is robust to appearance changes in long-term re-ID and meanwhile maintains sufficient discriminability to differentiate different people. The final matching metric is obtained by mixing information of the two streams in a score-level fusion strategy. In addition, we collect a Cloth-Varying vIDeo re-ID (CVID-reID) dataset particularly for long-term re-ID. It contains video tracklets of celebrities posted on the Internet. These videos are snapshots under extremely different scenarios that include highly dynamic background, diverse camera views and abundant cloth variations on each TSI. These factors cause CVID-reID more complicated and closer to practice. Our experiments demonstrate the difficulty of long-term person re-ID and also validate the effectiveness of the proposed SpTSkM, showing the best performance.
C1 [Zhang, Peng; Xu, Jingsong; Wu, Qiang; Huang, Yan] Univ Technol Sydney, Sch Elect & Data Engn, Global Big Data & Technol Ctr, Sydney, NSW 2007, Australia.
   [Ben, Xianye] Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Peoples R China.
C3 University of Technology Sydney; Shandong University
RP Ben, XY (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Peoples R China.
EM peng.zhang-2@student.uts.edu.au; jingsong.xu@uts.edu.au;
   qiang.wu@uts.edu.au; yan.huang-3@student.uts.edu.au; benxianye@gmail.com
RI Huang, Yan/N-3447-2018
OI Huang, Yan/0000-0002-1363-5318; Ben, Xianye/0000-0001-8083-3501; Wu,
   Qiang/0000-0001-5641-2483; Zhang, Peng/0000-0001-6794-7352
FU National Key R&D Program of China [2017YFC0803401]; Natural Science
   Foundation of China [61971468, 61571275]; Open Projects Program of
   National Laboratory of Pattern Recognition [202000022]; Shandong
   Provincial Key Research and Development Program (Major Scientific and
   Technological Innovation Project) [2019JZZY010119]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFC0803401, in part by the Natural Science Foundation of
   China under Grants 61971468 and 61571275, in part by the Open Projects
   Program of National Laboratory of Pattern Recognition under Grant
   202000022, and in part by the Shandong Provincial Key Research and
   Development Program (Major Scientific and Technological Innovation
   Project) under Grant 2019JZZY010119. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   YingLi Tian. (Corresponding author: Xianye Ben.)
CR [Anonymous], 2016, P BR MACH VIS C
   [Anonymous], 2018, REVISITING TEMPORAL
   Barbosa IB, 2012, LECT NOTES COMPUT SC, V7583, P433, DOI 10.1007/978-3-642-33863-2_43
   Ben XY, 2019, PATTERN RECOGN, V90, P87, DOI 10.1016/j.patcog.2019.01.017
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Gray Douglas, 2007, P IEEE INT WORKSH PE, V3, P1
   Haque A, 2016, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2016.138
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Leibe B., 2017, ARXIV170307737CS
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Wei, 2012, AS C COMP VIS ACCV 2, P31
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Munaro M, 2014, IEEE INT CONF ROBOT, P4512, DOI 10.1109/ICRA.2014.6907518
   Rao S., 2018, VIDEO BASED PERSON R
   Sekachev B., 2019, POWERFUL EFFICIENT C
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065
   Tan ZC, 2019, IEEE T IMAGE PROCESS, V28, P6126, DOI 10.1109/TIP.2019.2919199
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang P, 2018, IEEE WINT CONF APPL, P494, DOI 10.1109/WACV.2018.00060
   Zhang W, 2019, IEEE T CIRC SYST VID, V29, P2442, DOI 10.1109/TCSVT.2018.2865749
   Zhang W, 2019, IEEE T NEUR NET LEAR, V30, P3847, DOI 10.1109/TNNLS.2019.2899588
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P2042, DOI 10.1109/TIP.2017.2672440
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
NR 52
TC 14
Z9 19
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3562
EP 3576
DI 10.1109/TMM.2020.3028461
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100011
DA 2024-07-18
ER

PT J
AU Mo, DM
   Lai, ZH
   Wang, XZ
   Wong, WK
AF Mo, Dongmei
   Lai, Zhihui
   Wang, Xizhao
   Wong, Waikeung
TI Jointly Sparse Locality Regression for Image Feature Extraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Face recognition; Manifolds; Minimization; Linear
   discriminant analysis; Convergence; Redundancy; Regression; face
   recognition; feature extraction; local structure; joint sparsity
ID FEATURE-SELECTION METHOD; FACE-RECOGNITION; CLASSIFICATION
AB This paper proposes a novel method called Jointly Sparse Locality Regression (JSLR) for feature extraction and selection. JSLR utilizes joint L2,1-norm minimization on regularization term, and also introduces the locality to characterize the local geometric structure of the data. There are three main contributions in JSLR for face recognition. Firstly, it eliminates the drawback in ridge regression andLinear Discriminant Analysis (LDA) that when the number of the classes is too small, not enough projections can be obtained for feature extraction. Secondly, by using the local geometric structure as the regularization term, JSLR is able to preserve local information and find an embedding subspace which can detect the most essential data manifold structure. Moreover, since the L2,1 -norm based loss function is robust to outliers in data points, JSLR provides the joint sparsity for robust feature selection. The theoretical connections of the proposed method and the previous regression methods are explored and the convergence of the proposed algorithm is also proved. Experimental evaluation on several well-known data sets shows the merits of the proposed method on feature selection and classification.
C1 [Mo, Dongmei; Wong, Waikeung] Hong Kong Polytech Univ, Inst Text & Clothing, Hong Kong, Peoples R China.
   [Lai, Zhihui; Wang, Xizhao] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Lai, Zhihui] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518060, Peoples R China.
   [Wang, Xizhao] Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
C3 Hong Kong Polytechnic University; Shenzhen University; Shenzhen
   Institute of Artificial Intelligence & Robotics for Society; Shenzhen
   University
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Inst Text & Clothing, Hong Kong, Peoples R China.
EM dongmei.mo@connect.polyu.hk; lai_zhi_hui@163.com; xizhaowang@ieee.org;
   calvin.wong@polyu.edu.hk
RI Lai, Zhihui/R-1000-2019
OI Lai, Zhihui/0000-0002-4388-3080; Wong, Wai Keung/0000-0002-5214-7114
FU HongKong PolytechnicUniversity [RHR1]; General Research Fund of the
   Research Grants Council of Hong Kong [15202217]
FX This work was supported in part by The Hong Kong Polytechnic University
   (Project Code: RHR1) and in part by General Research Fund of the
   Research Grants Council of Hong Kong (Project Code: 15202217).
CR [Anonymous], 2007, 0749 UMASS
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Cai D., 2007, AAAI, P528
   Cai D, 2007, IEEE DATA MINING, P73, DOI 10.1109/ICDM.2007.89
   Chen JH, 2015, IEEE T NEUR NET LEAR, V26, P2291, DOI 10.1109/TNNLS.2014.2377477
   Chow TWS, 2008, IEEE T SYST MAN CY B, V38, P499, DOI 10.1109/TSMCB.2007.914707
   Clemmensen L, 2011, TECHNOMETRICS, V53, P406, DOI 10.1198/TECH.2011.08118
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Dagher I., 2010, Proceedings of the 2010 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications (CIMSA 2010), P97, DOI 10.1109/CIMSA.2010.5611752
   Dai G, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P61, DOI 10.1109/ICME.2004.1394125
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Eskandari M, 2015, COMPUT VIS IMAGE UND, V137, P63, DOI 10.1016/j.cviu.2015.02.011
   Feng J, 2016, PATTERN RECOGN, V51, P295, DOI 10.1016/j.patcog.2015.08.018
   Han JQ, 2015, PATTERN RECOGN, V48, P3927, DOI 10.1016/j.patcog.2015.06.003
   Hastie T., 2003, The Elements of Statistical Learning: Data Mining, Inference, and Prediction
   He JR, 2014, IEEE IJCNN, P2263, DOI 10.1109/IJCNN.2014.6889396
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2004, ADV NEUR IN, V16, P153
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642
   Huang P, 2016, NEUROCOMPUTING, V190, P50, DOI 10.1016/j.neucom.2016.01.001
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148
   Lashkia GV, 2004, IEEE T SYST MAN CY B, V34, P888, DOI 10.1109/TSMCB.2003.817106
   Li Y, 2015, IEEE T NEUR NET LEAR, V26, P1388, DOI 10.1109/TNNLS.2014.2341627
   Li ZC, 2015, IEEE T IMAGE PROCESS, V24, P5343, DOI 10.1109/TIP.2015.2479560
   Li ZM, 2017, IEEE T NEUR NET LEAR, V28, P278, DOI 10.1109/TNNLS.2015.2508025
   Liao SL, 2018, IEEE T IMAGE PROCESS, V27, P5668, DOI 10.1109/TIP.2018.2859589
   Liu XW, 2014, IEEE T NEUR NET LEAR, V25, P1083, DOI 10.1109/TNNLS.2013.2287275
   Liu Y, 2015, IEEE T CYBERNETICS, V45, P1209, DOI 10.1109/TCYB.2014.2347372
   Lu HP, 2006, INT C PATT RECOG, P776
   Luo L, 2017, IEEE T NEUR NET LEAR, V28, P2168, DOI 10.1109/TNNLS.2016.2573644
   Luo L, 2015, PATTERN RECOGN, V48, P3811, DOI 10.1016/j.patcog.2015.06.012
   Majumdar A, 2012, INT CONF ACOUST SPEE, P3421, DOI 10.1109/ICASSP.2012.6288651
   Majumdar A, 2009, INT CONF ACOUST SPEE, P861, DOI 10.1109/ICASSP.2009.4959720
   Majumdar A, 2010, IEEE T SYST MAN CY B, V40, P1359, DOI 10.1109/TSMCB.2009.2038493
   Majumdar A, 2009, CAN J ELECT COMPUT E, V34, P136, DOI 10.1109/CJECE.2009.5599420
   Martinez A., 1998, AR FACE DATABASE
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Nie FP, 2019, IEEE T IMAGE PROCESS, V28, P2428, DOI 10.1109/TIP.2018.2886761
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Ning X, 2018, IEEE T IMAGE PROCESS, V27, P2575, DOI 10.1109/TIP.2018.2806229
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Qian JJ, 2015, PATTERN RECOGN, V48, P3145, DOI 10.1016/j.patcog.2015.04.017
   Qian JJ, 2013, IEEE T IMAGE PROCESS, V22, P3591, DOI 10.1109/TIP.2013.2264676
   Qiao Z., 2009, IAENG INT J APPL MAT, V39, P1
   Raghavendra R, 2011, PATTERN RECOGN, V44, P1076, DOI 10.1016/j.patcog.2010.11.008
   Shi CJA, 2015, IEEE T MULTIMEDIA, V17, P16, DOI 10.1109/TMM.2014.2375792
   Shikkenawis G, 2016, NEUROCOMPUTING, V173, P196, DOI 10.1016/j.neucom.2015.01.100
   Sim T., 2001, Technical report CMU-RI-TR-OI-02
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   van den Berg E, 2010, IEEE T INFORM THEORY, V56, P2516, DOI 10.1109/TIT.2010.2043876
   Wang Q, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2929
   Wang W, 2018, IEEE T IMAGE PROCESS, V27, P2664, DOI 10.1109/TIP.2018.2810515
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xiang SM, 2012, IEEE T NEUR NET LEAR, V23, P1738, DOI 10.1109/TNNLS.2012.2212721
   Yan H, 2015, PATTERN RECOGN, V48, P1827, DOI 10.1016/j.patcog.2014.10.021
   Yang J, 2017, IEEE T PATTERN ANAL, V39, P156, DOI 10.1109/TPAMI.2016.2535218
   Yang JB, 2012, IEEE T SYST MAN CY B, V42, P1550, DOI 10.1109/TSMCB.2012.2195000
   Yang M, 2013, IEEE T IMAGE PROCESS, V22, P1753, DOI 10.1109/TIP.2012.2235849
   Yang WK, 2016, NEUROCOMPUTING, V175, P198, DOI 10.1016/j.neucom.2015.10.049
   Yang Y, 2015, IEEE T CYBERNETICS, V45, P1069, DOI 10.1109/TCYB.2014.2344015
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yang Y, 2011, P AM MATH SOC, V139, P3171, DOI 10.1090/S0002-9939-2011-10735-4
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng ZQ, 2016, NEUROCOMPUTING, V173, P102, DOI 10.1016/j.neucom.2015.05.119
   Zeng ZL, 2015, PATTERN RECOGN, V48, P2656, DOI 10.1016/j.patcog.2015.02.025
   Zhang FL, 2015, IEEE T NEUR NET LEAR, V26, P2247, DOI 10.1109/TNNLS.2014.2376530
   Zhang HM, 2017, INFORM SCIENCES, V394, P1, DOI 10.1016/j.ins.2017.02.020
   Zheng WM, 2006, INT C PATT RECOG, P784
   Zheng ZL, 2015, INFORM SCIENCES, V303, P1, DOI 10.1016/j.ins.2015.01.004
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 75
TC 5
Z9 6
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2873
EP 2888
DI 10.1109/TMM.2019.2961508
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900009
DA 2024-07-18
ER

PT J
AU Wang, BS
   Zhao, Y
   Chen, CLP
AF Wang, Bingshu
   Zhao, Yong
   Philip Chen, C. L.
TI Moving Cast Shadows Segmentation Using Illumination Invariant Feature
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Estimation; Color; Video sequences; Image color analysis;
   Feature extraction; Distribution functions; Moving shadows segmentation;
   bidirectional reflectance distribution function; illumination invariant;
   shadow direction features
AB This paper presents an effective framework for removing moving cast shadows. Taking the reflection property of object surface for shadow regions under static and fixed scenes, an approximation estimation strategy of bidirectional reflectance distribution function as illumination invariant feature is proposed. It is valid for different types of shadow scenes. In this paper, we propose a new multiple ratios-based technique to justify shadow type for each frame: intensity ratio, area ratio and edge ratio of shadow regions are introduced. According to shadow types, several specified strategies are designed. For weak shadows, multiple features fusion strategy is employed, including color constancy, texture consistency and illumination invariant. For strong shadows, illumination invariant is utilized to detect the umbra and color constancy is utilized to detect the penumbra. Moreover, a suite of shadow direction features is firstly proposed to identify penumbra. The proposed approach is verified in fourteen video sequences varying from weak to strong shadows. The experimental results demonstrate the effectiveness and robustness of the proposed method for both indoor and outdoor scenes compared with some state-of-the-art approaches.
C1 [Wang, Bingshu] Univ Macau, Dept Comp & Informat Sci, Fac Sci & Technol, Macau 999078, Peoples R China.
   [Zhao, Yong] Peking Univ, Shenzhen Grad Sch, Key Lab Integrated Microsyst, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Philip Chen, C. L.] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China.
   [Philip Chen, C. L.] Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.
C3 University of Macau; Peking University; South China University of
   Technology; University of Macau
RP Chen, CLP (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China.
EM yb77408@umac.mo; zhaoyong@pkusz.edu.cn; philip.chen@ieee.org
RI wang, bingshu/GRR-5161-2022; Chen, Ci/ISU-4741-2023
OI Chen, Ci/0000-0003-0813-5543; Wang, Bingshu/0000-0002-2603-8328
FU National Natural Science Foundation of China Key Program [U1813203,
   U1801261]; National Natural Science Foundation ofChina [61751202,
   61751205, 61572540]; Key Program for International S&T Cooperation
   Projects of China [2016YFE0121200]; Macau Science and Technology
   Development Fund (FDCT) [079/2017/A2, 024/2015/AMJ, 0119/2018/A3];
   National Sci-Tech Support Plan [2015BAK01B04]; Shenzhen Technology
   Program [JCYJ20160506172651253]
FX This work was supported in part by the National Natural Science
   Foundation of China Key Program under Grants U1813203 and U1801261; in
   part by the National Natural Science Foundation ofChina underGrant
   61751202, 61751205, and 61572540; in part by the Key Program for
   International S&T Cooperation Projects of China underGrant
   2016YFE0121200; in part by the Macau Science and Technology Development
   Fund (FDCT) under Grants 079/2017/A2, 024/2015/AMJ, and 0119/2018/A3; in
   part by National Sci-Tech Support Plan 2015BAK01B04; and in part by
   Shenzhen Technology Program under Grant JCYJ20160506172651253.
CR Al-Najdawi N, 2012, PATTERN RECOGN LETT, V33, P752, DOI 10.1016/j.patrec.2011.12.013
   Amato A, 2011, IEEE T IMAGE PROCESS, V20, P2954, DOI 10.1109/TIP.2011.2132728
   Asaidi H, 2014, J VISUAL LANG COMPUT, V25, P333, DOI 10.1016/j.jvlc.2014.02.001
   Benedek C, 2008, IEEE T IMAGE PROCESS, V17, P608, DOI 10.1109/TIP.2008.916989
   Bouwmans T, 2014, Background modeling and foreground detection for video surveillance
   Chen CLP, 2018, IEEE T NEUR NET LEAR, V29, P10, DOI 10.1109/TNNLS.2017.2716952
   Feng S, 2020, IEEE T CYBERNETICS, V50, P414, DOI 10.1109/TCYB.2018.2857815
   Gomes V, 2017, PATTERN RECOGN, V63, P30, DOI 10.1016/j.patcog.2016.09.008
   Hsieh JW, 2003, IMAGE VISION COMPUT, V21, P505, DOI 10.1016/S0262-8856(03)00030-1
   Huang JB, 2009, INT CONF ACOUST SPEE, P769, DOI 10.1109/ICASSP.2009.4959697
   Huerta I, 2015, IMAGE VISION COMPUT, V41, P42, DOI 10.1016/j.imavis.2015.06.003
   Jiang K, 2013, IET COMPUT VIS, V7, P115, DOI 10.1049/iet-cvi.2012.0106
   Jung CR, 2009, IEEE T MULTIMEDIA, V11, P571, DOI 10.1109/TMM.2009.2012924
   Khan SH, 2016, IEEE T PATTERN ANAL, V38, P431, DOI 10.1109/TPAMI.2015.2462355
   Khare M., 2020, IET COMPUT VIS, V50, P414
   KUBELKA P, 1948, J OPT SOC AM, V38, P448, DOI 10.1364/JOSA.38.000448
   Lee H, 2016, IEEE T MULTIMEDIA, V18, P2093, DOI 10.1109/TMM.2016.2595262
   Lee J, 2015, LECT N MECH ENG, P299, DOI 10.1007/978-3-319-15536-4_25
   Leone A, 2007, PATTERN RECOGN, V40, P1222, DOI 10.1016/j.patcog.2006.09.017
   Lin Y., 2018, Advances in Computer and Computational Sciences, P487
   Liu LC, 2017, IEEE T CYBERNETICS, V47, P600, DOI 10.1109/TCYB.2016.2521428
   Liu LC, 2015, INFORM SCIENCES, V315, P1, DOI 10.1016/j.ins.2015.03.067
   Liu Z, 2012, IEEE T CIRC SYST VID, V22, P56, DOI 10.1109/TCSVT.2011.2158335
   Martel-Brisson N, 2005, PROC CVPR IEEE, P643
   Peng JT, 2015, IEEE T GEOSCI REMOTE, V53, P4810, DOI 10.1109/TGRS.2015.2410991
   Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520
   Prati A., 2003, TECH REP
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Russell M., 2016, Computational Visual Media, V2, P195, DOI DOI 10.1007/S41095-016-0058-0
   Russell M, 2019, IEEE T CIRC SYST VID, V29, P2652, DOI 10.1109/TCSVT.2017.2763181
   Salvador E, 2004, COMPUT VIS IMAGE UND, V95, P238, DOI 10.1016/j.cviu.2004.03.008
   Sanin A, 2012, PATTERN RECOGN, V45, P1684, DOI 10.1016/j.patcog.2011.10.001
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Stauder J, 1999, IEEE T MULTIMEDIA, V1, P65, DOI 10.1109/6046.748172
   Su YZ, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.5.053015
   Subudhi BN, 2016, INFORM SCIENCES, V331, P15, DOI 10.1016/j.ins.2015.10.031
   Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483
   Wang B., 2016, Asian Conference on Computer Vision, P521
   Wang BS, 2018, OPT ENG, V57, DOI 10.1117/1.OE.57.9.093102
   Wang BS, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1628, DOI 10.1109/ICASSP.2018.8461695
   Wang J, 2014, J VIS COMMUN IMAGE R, V25, P978, DOI 10.1016/j.jvcir.2014.02.015
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wu Q, 2012, IEEE INT CONF ROBOT, P2177, DOI 10.1109/ICRA.2012.6224561
   Yang KF, 2015, PROC CVPR IEEE, P2254, DOI 10.1109/CVPR.2015.7298838
   Zhang PY, 2016, PATTERN RECOGN, V52, P249, DOI 10.1016/j.patcog.2015.09.024
   Zhang W, 2007, IEEE T MULTIMEDIA, V9, P1202, DOI 10.1109/TMM.2007.902842
   Zhang X, 2017, IEEE T MULTIMEDIA, V19, P2425, DOI 10.1109/TMM.2017.2701645
   Zhong DL, 2010, 2010 11TH INTERNATIONAL CONFERENCE ON ELECTRONIC PACKAGING TECHNOLOGY & HIGH DENSITY PACKAGING (ICEPT-HDP), P1377, DOI 10.1109/ICEPT.2010.5582819
   Zhou YC, 2015, IEEE J-STARS, V8, P2351, DOI 10.1109/JSTARS.2014.2359965
   Zhu ZQ, 2015, PATTERN RECOGN, V48, P2592, DOI 10.1016/j.patcog.2015.01.001
   Zickler T, 2008, INT J COMPUT VISION, V79, P13, DOI [10.1007/s11263-007-0087-3, 10.1007/s11I263-007-0087-3]
NR 51
TC 13
Z9 14
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2221
EP 2233
DI 10.1109/TMM.2019.2954752
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200003
DA 2024-07-18
ER

PT J
AU Yang, FX
   Zhong, Z
   Luo, ZM
   Lian, S
   Li, SZ
AF Yang, Fengxiang
   Zhong, Zhun
   Luo, Zhiming
   Lian, Sheng
   Li, Shaozi
TI Leveraging Virtual and Real Person for Unsupervised Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Cameras; Data mining; Training data; Feature extraction;
   Annotations; Person re-identification; generative adversarial network;
   collaborative filtering
ID RETRIEVAL
AB Person re-identification (re-ID) is a challenging instance retrieval problem, especially when identity annotations are not available for training. Although modern deep re-ID approaches have achieved great improvement, it is still difficult to optimize the deep re-ID model and learn discriminative person representation without annotations in training data. To address this challenge, this study considers the problem of unsupervised person re-ID and introduces a novel approach to solve this problem by leveraging virtual and real data. Our approach includes two components: virtual person generation and training of the deep re-ID model. For virtual person generation, we learn a person generation model and a camera style transfer model using unlabeled real data to generate virtual persons with different poses and camera styles. The virtual data is formed as labeled training data, enabling subsequent training deep re-ID model in supervision. For training of the deep re-ID model, we divide it into three steps: 1) pre-training a coarse re-ID model by using virtual data; 2) collaborative filtering based positive pair mining from the real data; and 3) fine-tuning of the coarse re-ID model by leveraging the mined positive pairs and virtual data. The final re-ID model is achieved by iterating between step 2 and step 3 until convergence. Extensive experiments demonstrate the effectiveness of our method. Experimental results on two large-scale datasets, Market-1501 and DukeMTMC-reID, show the advantages of our method over state-of-the-art approaches in unsupervised person re-ID. Our code is now available online.(1)
C1 [Yang, Fengxiang; Zhong, Zhun; Lian, Sheng; Li, Shaozi] Xiamen Univ, Dept Artificial Intelligence, Xiamen 361005, Peoples R China.
   [Luo, Zhiming] Xiamen Univ, Postdoctoral Mobile Stn Informat & Commun Engn, Xiamen 361005, Peoples R China.
C3 Xiamen University; Xiamen University
RP Li, SZ (corresponding author), Xiamen Univ, Dept Artificial Intelligence, Xiamen 361005, Peoples R China.; Luo, ZM (corresponding author), Xiamen Univ, Postdoctoral Mobile Stn Informat & Commun Engn, Xiamen 361005, Peoples R China.
EM yangfx@stu.xmu.edu.cn; zhunzhong007@gmail.com; zhiming.luo@xmu.edu.cn;
   lancerlian@stu.xmu.edu.cn; szlig@xmu.edu.cn
RI Lian, Sheng/AAX-9397-2021; Li, SZ/G-3959-2010
OI Lian, Sheng/0000-0003-2967-3041; Yang, Fengxiang/0000-0003-3780-251X;
   Luo, Zhiming/0000-0002-3411-9582
FU National Natural Science Foundation of China [61876159, 61806172,
   61572409, U1705286, 61571188]; China Postdoctoral Science Foundation
   [2019M652257]; National Key Research and Development Program of China
   [2018YFC0831402]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61876159, 61806172, 61572409, U1705286,
   and 61571188, in part by the China Postdoctoral Science Foundation under
   Grant 2019M652257, and in part by the National Key Research and
   Development Program of China under Grant 2018YFC0831402. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Professor Mohammed Daoudi.
CR [Anonymous], 2017, 2017 INT C SMART, DOI DOI 10.1109/ICSGEA.2017.74
   Breese J., 1998, P 14 C UNC ART INT, P43
   Chen KW, 2011, IEEE T MULTIMEDIA, V13, P625, DOI 10.1109/TMM.2011.2131639
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A., 2017, ARXIV170307737
   Kingma D. P., 2013, ARXIV13126114
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu ZM, 2017, IEEE I CONF COMP VIS, P2448, DOI 10.1109/ICCV.2017.266
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Yang Y, 2017, AAAI CONF ARTIF INTE, P4306
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zhao B., 2017, arXiv:1704.04886
   Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L., 2016, ARXIV161002984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 39
TC 25
Z9 26
U1 3
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2444
EP 2453
DI 10.1109/TMM.2019.2957928
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xue, F
   Hong, RC
   He, XN
   Wang, JW
   Qian, SS
   Xu, CS
AF Xue, Feng
   Hong, Richang
   He, Xiangnan
   Wang, Jianwei
   Qian, Shengsheng
   Xu, Changsheng
TI Knowledge-Based Topic Model for Multi-Modal Social Event Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Analytical models; Knowledge based systems; Social networking (online);
   Data mining; Data models; Internet; Knowledge engineering; Knowledge
   embedding; multi-modal; topic coherence; event classification
AB With the accumulation of data on the Internet and progress in representation learning techniques, knowledge priors learned from a large-scale knowledge base has been increasingly used in probabilistic topic models. However, it is challenging to learn interpretable topics and a discriminative event representation based on multi-modal information. To address these issues, we propose a knowledge priors- and max-margin-based topic model for multi-modal social event analysis, called the KGE-MMSLDA, in which feature representation and knowledge priors are jointly learned. Our model has three main advantages over current methods: (1) It integrates additional knowledge from external knowledge base into a unified topic model in which the max-margin classifier, and multi-modal information are exploited to increase the number of event descriptions obtained. (2) We mined knowledge priors from over 74,000 web documents. Multi-modal data with these knowledge priors are then incorporated into the topic model to increase the number of coherent topics learned. (3) A large-scale multi-modal dataset (containing 10 events, where each event contained approximately 7,000 Flickr pages) was collected and has been released publicly for event topic mining and classification research. In comparative experiments, the proposed method outperformed state-of-the-art models on topic coherence, and obtained a classification accuracy of 85.1%.
C1 [Xue, Feng; Hong, Richang] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230601, Peoples R China.
   [Xue, Feng; Hong, Richang] Hefei Univ Thchnol, Sch Comp Sci & Informat Engn, Hefei 230601, Peoples R China.
   [He, Xiangnan] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230031, Peoples R China.
   [Wang, Jianwei] Minglue Technol Grp, Beijing 100083, Peoples R China.
   [Qian, Shengsheng; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Hefei University of Technology; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Hong, RC (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230601, Peoples R China.
EM feng.xue@hfut.edu.cn; hongrc.hfut@gmail.com; xiangnanhe@gmail.com;
   jwwang2015@mail.hfut.edu.cn; shengsheng.qian@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665; Xue, Feng/0000-0003-4962-9734
FU National Key Research and Development Program of China [2017YFB0803301];
   National Natural Science Foundation of China [61772170]
FX This work was supported in part by the National Key Research and
   Development Program of China (No. 2017YFB0803301) and in part by the
   National Natural Science Foundation of China (No. 61772170).
CR Allan J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P37, DOI 10.1145/290941.290954
   Andrzejewski D., 2011, Proceedings of the 22nd international joint conference on artificial intelligence
   Andrzejewski D., 2009, P NAACL HLT 2009 WOR, P43, DOI DOI 10.3115/1621829.1621835
   [Anonymous], 2010, P 24 ANN C NEUR INF, DOI DOI 10.1109/ICPR.2014.65
   [Anonymous], 2011, Advances in neural information processing systems
   [Anonymous], 2008, P ADV NEURAL INFORM
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Bao Y, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P239, DOI 10.1145/2505515.2505556
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Batmanghelich K., 2016, P ACL
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Bordes A, 2011, 25 AAAI C ART INT
   Bordes A., 2013, P 26 INT C NEURAL IN, P2787
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Das R, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P795
   Diakopoulos N., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P115, DOI 10.1109/VAST.2010.5652922
   Doshi-Velez F, 2015, AAAI CONF ARTIF INTE, P2575
   Gao Y, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1701222
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Guo S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P84
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hoffman M., 2010, ADV NEURAL INFORM PR, V23, P856
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Kalamaras I, 2014, IEEE T MULTIMEDIA, V16, P1460, DOI 10.1109/TMM.2014.2316473
   Krestel R., 2009, Proceedings of the 3rd ACM Conference on Recommender Systems, P61, DOI [DOI 10.1145/1639714.1639726, 10.1145/1639714.1639726]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumaran G., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P297, DOI 10.1145/1008992.1009044
   Lau J. H., 2014, P 14 C EUR CHAPT ASS, P530, DOI [DOI 10.3115/V1/E14-1056, 10.3115/v1/E14-1056]
   Lin DH, 2013, IEEE I CONF COMP VIS, P841, DOI 10.1109/ICCV.2013.109
   Lin PP, 2014, IEEE CONF COMPUT, P111, DOI 10.1109/INFCOMW.2014.6849180
   Makkonen J, 2004, INFORM RETRIEVAL, V7, P347, DOI 10.1023/B:INRT.0000011210.12953.86
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Mimno D, 2011, EMNLP, P262, DOI DOI 10.5555/2145432.2145462
   Newman D., 2010, AUTOMATIC EVALUATION
   Nguyen D. Q., 2015, Transactions of the Association for Computational Linguistics, V3, P299, DOI DOI 10.1162/TACL_A_00140
   Niu ZX, 2011, PROC CVPR IEEE, P1769, DOI 10.1109/CVPR.2011.5995426
   Patel D., 2008, P 2008 ACM SIGMOD IN, P393, DOI DOI 10.1145/1376616.1376658
   Prabhudesai KS, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2451, DOI 10.1109/ICASSP.2018.8462003
   PUTTHIVIDHYA D, 2010, PROC CVPR IEEE, P3408, DOI DOI 10.1109/CVPR.2010.5540000
   Qian S., 2014, MULTIMODAL SUPERVISE, P152
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Ramage D., 2009, Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, EMNLP'09, P248, DOI 10.3115/1699510.1699543
   Rodrigues F, 2017, IEEE T PATTERN ANAL, V39, P2409, DOI 10.1109/TPAMI.2017.2648786
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang Y., 2011, LECT NOTES COMPUTER, V1674, P39, DOI DOI 10.1007/978-3-642-23620-4_8
   Wang Z, 2014, AAAI CONF ARTIF INTE, P1112
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xie RB, 2016, AAAI CONF ARTIF INTE, P2659
   Yang S, 2015, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2015.7298769
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yang Y., 2002, P 8 ACM SIGKDD INT C, P688, DOI DOI 10.1145/775047.775150
   Yao L, 2017, AAAI CONF ARTIF INTE, P3119
   Yao L, 2015, LECT NOTES ARTIF INT, V9078, P586, DOI 10.1007/978-3-319-18032-8_46
   Zhang T, 2014, ACM T MULTIMEDIA COM, V10, P1
   Zhu J, 2014, J MACH LEARN RES, V15, P1073
   Zhuang Y., 2012, P 20 ACM INT C MULT, P957
NR 59
TC 18
Z9 20
U1 3
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2098
EP 2110
DI 10.1109/TMM.2019.2951194
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500015
DA 2024-07-18
ER

PT J
AU Zeng, HT
   Song, XH
   Chen, GW
   Jiang, SQ
AF Zeng, Haitao
   Song, Xinhang
   Chen, Gongwei
   Jiang, Shuqiang
TI Learning Scene Attribute for Scene Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Semantics; Context modeling; Image
   recognition; Computer vision; Aggregates; Scene recognition; scene
   attribute
ID FEATURES
AB Scene recognition has been a challenging task in the field of computer vision and multimedia for a long time. The current scene recognition works often extract object features and scene features through CNN, and combine these two types of features to obtain complementary and discriminative scene representations. However, when the scene categories are visually similar, the object features might lack of discriminations. Therefore, it may be debatable to consider only object features. In contrast to the existing works, in this paper, we discuss the discrimination of scene attributes in local regions and utilize scene attributes as the complementary features of object and scene features. We extract these visual features from two individual CNN branches, one extracting the global features of the image while the other extracting the features of local regions. Through contextual modeling framework, we aggregate these features and generate more discriminative scene representations, which achieve better performance than the feature aggregation of object and scene. Moreover, we achieve the new state-of-the-art performance on both standard scene recognition benchmarks by aggregating more complementary visual features: MIT67 (88.06%) and SUN397 (74.12%).
C1 [Zeng, Haitao] China Univ Min & Technol, Beijing 100083, Peoples R China.
   [Zeng, Haitao; Song, Xinhang; Chen, Gongwei; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 China University of Mining & Technology; Chinese Academy of Sciences;
   Institute of Computing Technology, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM haitao.zeng@vipl.ict.ac.cn; xinhang.song@vipl.ict.ac.cn;
   gongwei.chen@vipl.ict.ac.cn; sqjiang@ict.ac.cn
RI Zeng, Haitao/ISU-2331-2023
OI Zeng, Haitao/0000-0003-1538-461X; song, xinhang/0000-0002-0895-1076;
   Chen, Gongwei/0000-0002-0634-6075
FU National Natural Science Foundation of China [61532018, 61902378];
   Beijing Natural Science Foundation [L182054, Z190020]; National Program
   for Special Support of Eminent Professionals; National Program for
   Support of Top-notch Young Professionals; National Postdoctoral Program
   for Innovative Talents [BX201700255]; China Postdoctoral Science
   Foundation [2018M631583]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61532018 and 61902378, in part by the
   Beijing Natural Science Foundation under Grants L182054 and Z190020, in
   part by National Program for Special Support of Eminent Professionals
   and National Program for Support of Top-notch Young Professionals, in
   part by the National Postdoctoral Program for Innovative Talents under
   Grant BX201700255, and in part by the China Postdoctoral Science
   Foundation under Grant 2018M631583.
CR [Anonymous], PROC CVPR IEEE
   [Anonymous], P AS C COMP VIS
   [Anonymous], 2015, P INT C LEARN REPR
   Cheng XJ, 2018, PATTERN RECOGN, V74, P474, DOI 10.1016/j.patcog.2017.09.025
   Dixit M, 2015, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2015.7298916
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Ferrari V., 2007, P 20 INT C NEUR INF, P433
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Guo S, 2017, IEEE T IMAGE PROCESS, V26, P808, DOI 10.1109/TIP.2016.2629443
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Huang GL, 2017, IEEE ICC
   Jiang SQ, 2019, IEEE T MULTIMEDIA, V21, P1609, DOI 10.1109/TMM.2018.2876830
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwitt Roland, 2012, Computer Vision - ECCV 2012. Proceedings of the 12th European Conference on Computer Vision, P359, DOI 10.1007/978-3-642-33765-9_26
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li L, 2010, AAAI CONF ARTIF INTE, P1377
   Li X, 2014, LECT NOTES COMPUT SC, V8695, P234, DOI 10.1007/978-3-319-10584-0_16
   Li YS, 2017, IEEE I CONF COMP VIS, P5757, DOI 10.1109/ICCV.2017.613
   Liu XC, 2016, IEEE INT CON MULTI
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Oliva A, 2002, LECT NOTES COMPUT SC, V2525, P263
   Patterson G, 2016, LECT NOTES COMPUT SC, V9910, P85, DOI 10.1007/978-3-319-46466-4_6
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Rasiwasia N, 2013, IEEE T PATTERN ANAL, V35, P2665, DOI 10.1109/TPAMI.2013.69
   Rasiwasia N, 2012, IEEE T PATTERN ANAL, V34, P902, DOI 10.1109/TPAMI.2011.175
   Russakovsky O., 2010, LNCS, P1, DOI DOI 10.1007/978-3-642-35749-7_1
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simonyan K., 2014, 14091556 ARXIV
   Song XH, 2017, IEEE T IMAGE PROCESS, V26, P2721, DOI 10.1109/TIP.2017.2686017
   Song XH, 2016, PATTERN RECOGN, V59, P98, DOI 10.1016/j.patcog.2016.01.019
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Wang LM, 2017, IEEE T IMAGE PROCESS, V26, P2055, DOI 10.1109/TIP.2017.2675339
   Wang XY, 2013, IEEE I CONF COMP VIS, P2120, DOI 10.1109/ICCV.2013.264
   Wang Z, 2017, IEEE T IMAGE PROCESS, V26, P2028, DOI 10.1109/TIP.2017.2666739
   Wu RB, 2015, IEEE I CONF COMP VIS, P1287, DOI 10.1109/ICCV.2015.152
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Zhang D., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P266, DOI 10.1145/1076034.1076081
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3241, DOI 10.1109/TIP.2014.2328894
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao ZY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1760, DOI 10.1145/3240508.3240698
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou MM, 2014, INTERNATIONAL CONFERENCE ON EDUCATION AND SOCIAL SCIENCES (INTCESS14), VOLS I AND II, P487
   Zhu JQ, 2017, IMAGE VISION COMPUT, V58, P224, DOI 10.1016/j.imavis.2016.07.004
NR 51
TC 9
Z9 10
U1 6
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1519
EP 1530
DI 10.1109/TMM.2019.2944241
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100012
DA 2024-07-18
ER

PT J
AU Xu, N
   Zhang, HW
   Liu, AA
   Nie, WZ
   Su, YT
   Nie, J
   Zhang, YD
AF Xu, Ning
   Zhang, Hanwang
   Liu, An-An
   Nie, Weizhi
   Su, Yuting
   Nie, Jie
   Zhang, Yongdong
TI Multi-Level Policy and Reward-Based Deep Reinforcement Learning
   Framework for Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Measurement; Task analysis; Reinforcement learning;
   Optimization; Adaptation models; Semantics; Multi-level policy;
   multi-level reward; reinforcement learning; image captioning
AB Image captioning is one of the most challenging tasks in AI because it requires an understanding of both complex visuals and natural language. Because image captioning is essentially a sequential prediction task, recent advances in image captioning have used reinforcement learning (RL) to better explore the dynamics of word-by-word generation. However, the existing RL-based image captioning methods rely primarily on a single policy network and reward function-an approach that is not well matched to the multi-level (word and sentence) and multi-modal (vision and language) nature of the task. To solve this problem, we propose a novel multi-level policy and reward RL framework for image captioning that can be easily integrated with RNN-based captioning models, language metrics, or visual-semantic functions for optimization. Specifically, the proposed framework includes two modules: 1) a multi-level policy network that jointly updates the word- and sentence-level policies for word generation; and 2) a multi-level reward function that collaboratively leverages both a vision-language reward and a language-language reward to guide the policy. Furthermore, we propose a guidance term to bridge the policy and the reward for RL optimization. The extensive experiments on the MSCOCO and Flickr30k datasets and the analyses show that the proposed framework achieves competitive performances on a variety of evaluation metrics. In addition, we conduct ablation studies on multiple variants of the proposed framework and explore several representative image captioning models and metrics for the word-level policy network and the language-language reward function to evaluate the generalization ability of the proposed framework.
C1 [Xu, Ning; Liu, An-An; Nie, Weizhi; Su, Yuting] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Zhang, Hanwang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Nie, Jie] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Tianjin University; Nanyang Technological University; Ocean University
   of China; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Nie, J (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
EM ningxu@tju.edu.cn; hanwangzhang@ntu.edu.sg; anan0422@gmail.com;
   weizhinie@tju.edu.cn; ytsu@tju.edu.cn; niejie@ouc.edu.cn;
   zhyd73@ustc.edu.cn
RI Nie, Jie/ABG-9228-2021; Nie, Weizhi/ABF-5316-2021
OI Nie, Jie/0000-0003-4952-7666; nie, weizhi/0000-0002-0578-8138; Zhang,
   Hanwang/0000-0001-7374-8739
FU National Natural Science Foundation of China [61772359, 61525206,
   61872267]; National Key Research and Development Program of China
   [2017YFC0820600]; Grant of 2019 Tianjin New Generation Artificial
   Intelligence Major Program; Grant of 2018 Tianjin New Generation
   Artificial Intelligence Major Program [18ZXZNGX00150]; Open Project
   Program of the State Key Lab of CAD & CG, Zhejiang University [A1907];
   Grant of Elite Scholar Program of Tianjin University [2019XRX-0035]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61772359, 61525206, and 61872267, in
   part by the National Key Research and Development Program of China
   2017YFC0820600, in part by the Grant of 2019 Tianjin New Generation
   Artificial Intelligence Major Program, in part by the Grant of 2018
   Tianjin New Generation Artificial Intelligence Major Program
   18ZXZNGX00150, in part by the Open Project Program of the State Key Lab
   of CAD & CG, Zhejiang University underGrant A1907, and in part by the
   Grant of Elite Scholar Program of Tianjin University 2019XRX-0035. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Mohammed Daoudi.
CR Aker A, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P1250
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2014, CORR
   [Anonymous], 2014, Transactions of the Association for Computational Linguistics
   [Anonymous], P C NEUR INF PROC SY
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 2013, P 2013 C EMP METH NA
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Chen FH, 2018, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2018.00146
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen WZ, 2017, IEEE IJCNN, P1403, DOI 10.1109/IJCNN.2017.7966017
   Chen XP, 2018, PROC CVPR IEEE, P7995, DOI 10.1109/CVPR.2018.00834
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Dai SC, 2016, CHIN CONT DECIS CONF, P20, DOI 10.1109/CCDC.2016.7530948
   Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Kong XY, 2017, PROC CVPR IEEE, P7072, DOI 10.1109/CVPR.2017.748
   Krull A, 2017, PROC CVPR IEEE, P2566, DOI 10.1109/CVPR.2017.275
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova Polina, 2012, Association for Computational Linguistics
   Lin CY, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P150
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AA, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P821
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao Junhua, 2015, P INT C LEARN REPR
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Moxley E, 2010, IEEE T MULTIMEDIA, V12, P184, DOI 10.1109/TMM.2010.2041101
   Müller H, 2017, IEEE T MULTIMEDIA, V19, P2093, DOI 10.1109/TMM.2017.2729400
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Paszke A., 2017, NIPS 2017 WORKSH AUT
   Ren Z, 2017, PROC CVPR IEEE, P1151, DOI 10.1109/CVPR.2017.128
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutskever I, 2014, ADV NEUR IN, V27
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Wu SM, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P1180, DOI 10.1145/2998181.2998364
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
   Zhu YK, 2017, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2017.7953175
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 60
TC 73
Z9 74
U1 3
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1372
EP 1383
DI 10.1109/TMM.2019.2941820
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200021
DA 2024-07-18
ER

PT J
AU Zhang, YB
   Jia, K
   Wang, ZX
AF Zhang, Yabin
   Jia, Kui
   Wang, Zhixin
TI Part-Aware Fine-Grained Object Categorization Using Weakly Supervised
   Part Detection Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Detectors; Task analysis; Streaming media; Feature
   extraction; Benchmark testing; Supervised learning; Fine-grained object
   categorization; part proposal; weakly supervised learning
AB Fine-grained object categorization aims for distinguishing objects of subordinate categories that belong to the same entry-level object category. It is a rapidly developing subfield in multimedia content analysis. The task is challenging due to the facts that (1) training images with ground-truth labels are difficult to obtain, and (2) variations among different subordinate categories are subtle. It is well established that characterizing features of different subordinate categories are located on local parts of object instances. However, manually annotating object parts requires expertise, which is also difficult to generalize to new fine-grained categorization tasks. In this work, we propose a Weakly Supervised Part Detection Network (PartNet) that is able to detect discriminative local parts for the use of fine-grained categorization. A vanilla PartNet builds on top of a base subnetwork two parallel streams of upper network layers, which respectively compute scores of classification probabilities (over subordinate categories) and detection probabilities (over a specified number of discriminative part detectors) for local regions of interest (RoIs). The image-level prediction is obtained by aggregating element-wise products of these region-level probabilities, and meanwhile diverse part detectors can be learned in an end-to-end fashion under the image-level supervision. To generate a diverse set of RoIs as inputs of PartNet, we propose a simple Discretized Part Proposals module (DPP) that directly targets for proposing candidates of discriminative local parts, with no bridging via object-level proposals. Experiments on benchmark datasets of CUB-200-2011, Oxford Flower 102 and Oxford-IIIT Pet show the efficacy of our proposed method for both discriminative part detection and fine-grained categorization. In particular, we achieve the new state-of-the-art performance on CUB-200-2011 and Oxford-IIIT Pet datasets when ground-truth part annotations are not available.
C1 [Zhang, Yabin; Jia, Kui; Wang, Zhixin] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
C3 South China University of Technology
RP Jia, K (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
EM zhang.yabin@mail.scut.edu.cn; kuijia@scut.edu.cn;
   wang.zhixin@mail.scut.edu.cn
RI wang, zhixin/GZH-2500-2022; Zhang, Yabin/AAF-7624-2021
OI Wang, Zhixin/0000-0002-0326-2503; Zhang, Yabin/0000-0002-2179-3973
FU National Natural Science Foundation of China [61771201]; Program for
   Guangdong Introducing Innovative and Entrepreneurial Teams
   [2017ZT07X183]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61771201 and in part by the Program for
   Guangdong Introducing Innovative and Entrepreneurial Teams under Grant
   2017ZT07X183.
CR [Anonymous], 2016, ARXIV161000824
   [Anonymous], 2016, ARXIV160306765
   [Anonymous], 2010, CNSTR2010001 CALTECH
   [Anonymous], 2015, PROC 3 INT C LEARNIN
   [Anonymous], 2015, ARXIV151107063
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donggeun Yoo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P71, DOI 10.1109/CVPRW.2015.7301274
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo YM, 2018, IEEE T MULTIMEDIA, V20, P1525, DOI 10.1109/TMM.2017.2766842
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2016, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2016.8
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jia K, 2017, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2017.425
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22
   Khosla A., 2011, P CVPR WORKSHOP FINE, V2, P1
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Li ZC, 2017, IEEE INT CONF COMP V, P1199, DOI 10.1109/ICCVW.2017.145
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rippel O., 2016, P 4 INT C LEARN REPR
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wah Catherine, 2011, Technical report
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Yu F, 2015, P INT C LEARN REPR
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2019, IEEE T CIRC SYST VID, V29, P418, DOI 10.1109/TCSVT.2018.2797923
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127
   Zhu L, 2015, IEEE T MULTIMEDIA, V17, P981, DOI 10.1109/TMM.2015.2431496
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 54
TC 16
Z9 18
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1345
EP 1357
DI 10.1109/TMM.2019.2939747
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xia, XJ
   Togneri, R
   Sohel, F
   Zhao, YJ
   Huang, DF
AF Xia, Xianjun
   Togneri, Roberto
   Sohel, Ferdous
   Zhao, Yuanjun
   Huang, Defeng
TI Multi-Task Learning for Acoustic Event Detection Using Event and Frame
   Position Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acoustics; Task analysis; Neural networks; Event detection; Training;
   Indexes; Hidden Markov models; Acoustic event detection; multi-label
   classification; joint learning; multi-task
ID CLASSIFICATION; SCENES
AB Acoustic event detection deals with the acoustic signals to determine the sound type and to estimate the audio event boundaries. Multi-label classification based approaches are commonly used to detect the frame wise event types with a median filter applied to determine the happening acoustic events. However, the multi-label classifiers are trained only on the acoustic event types ignoring the frame position within the audio events. To deal with this, this paper proposes to construct a joint learning based multi-task system. The first task performs the acoustic event type detection and the second task is to predict the frame position information. By sharing representations between the two tasks, we can enable the acoustic models to generalize better than the original classifier by averaging respective noise patterns to be implicitly regularized. Experimental results on the monophonic UPC-TALP and the polyphonic TUT Sound Event datasets demonstrate the superior performance of the joint learning method by achieving lower error rate and higher F-score compared to the baseline AED system.
C1 [Xia, Xianjun; Togneri, Roberto; Zhao, Yuanjun; Huang, Defeng] Univ Western Australia, Dept Elect Elect & Comp Engn, Perth, WA 6009, Australia.
   [Sohel, Ferdous] Murdoch Univ, Coll Sci Hlth Engn & Educ, Perth, WA 6150, Australia.
C3 University of Western Australia; Murdoch University
RP Xia, XJ (corresponding author), Univ Western Australia, Dept Elect Elect & Comp Engn, Perth, WA 6009, Australia.
EM xxjpjj@mail.ustc.edu.cn; robeno.togneri@uwa.edu.au;
   F.Sohel@murdoch.edu.au; yuanjun.zhao@research.uwa.edu.au;
   david.huang@uwa.edu.au
RI Sohel, Ferdous/C-2428-2013; Huang, David/JTS-8033-2023; Zhao,
   Yuanjun/IAP-9801-2023; Togneri, Roberto/C-2466-2013; Huang,
   David/H-5081-2014
OI Sohel, Ferdous/0000-0003-1557-4907; Zhao, Yuanjun/0000-0002-6153-3543;
   Huang, David/0000-0002-1431-8859; Togneri, Roberto/0000-0002-3778-4633
FU Research Training Program (RTP) from The University ofWestern Australia
FX This work was supported by the Research Training Program (RTP) from The
   University ofWestern Australia. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. M.
   Shamim Hossain.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   Adavanne S., 2016, TECH REP
   Adavanne S., 2017, TECH REP
   [Anonymous], FUT INF COMM C
   [Anonymous], TECH REP
   [Anonymous], 2013, P 21 EUR SIGN PROC C
   [Anonymous], 2017, Detection and Classification of Acoustic Scenes and Events (DCASE)
   [Anonymous], 2017, P DET CLASS AC SCEN
   [Anonymous], P IEEE AASP CHALL DE
   [Anonymous], 2017, DCASE2017 CHALLENGE
   [Anonymous], 506909 UPC
   Cakir E, 2015, IEEE IJCNN
   Çakir E, 2017, IEEE-ACM T AUDIO SPE, V25, P1291, DOI 10.1109/TASLP.2017.2690575
   Chou S.Y., 2017, RECALL, V14, P55
   He JC, 2017, PATTERN RECOGN, V66, P44, DOI 10.1016/j.patcog.2016.11.029
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Phan H, 2015, IEEE-ACM T AUDIO SPE, V23, P20, DOI 10.1109/TASLP.2014.2367814
   Komatsu T, 2016, INT CONF ACOUST SPEE, P2259, DOI 10.1109/ICASSP.2016.7472079
   Lin XD, 2016, IEEE T MULTIMEDIA, V18, P1480, DOI 10.1109/TMM.2016.2571999
   Mesaros A, 2018, IEEE-ACM T AUDIO SPE, V26, P379, DOI 10.1109/TASLP.2017.2778423
   Mesaros A, 2016, EUR SIGNAL PR CONF, P1128, DOI 10.1109/EUSIPCO.2016.7760424
   Morfi V, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8081397
   Niessen ME, 2013, IEEE WORK APPL SIG
   Parascandolo G, 2016, INT CONF ACOUST SPEE, P6440, DOI 10.1109/ICASSP.2016.7472917
   Phan H., 2017, P DCASE 2017 WORKSH
   Ruder S, 2017, ARXIV170605098, DOI DOI 10.48550/ARXIV.1706.05098
   Sarafianos N, 2018, PATTERN RECOGN, V80, P94, DOI 10.1016/j.patcog.2018.02.028
   Smith JBL, 2014, IEEE T MULTIMEDIA, V16, P1219, DOI 10.1109/TMM.2014.2310706
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Temko A., 2006, Multimodal Technologies for Perception of Humans. First International Evaluation Workshop on Classification of Events, Activities and Relationships, CLEAR 2006. Revised Selected Papers (Lecture Notes in Computer Science Vol.4122), P311
   Wang B, 2016, PATTERN RECOGN, V52, P75, DOI 10.1016/j.patcog.2015.10.006
   Xia XJ, 2019, IEEE T MULTIMEDIA, V21, P1359, DOI 10.1109/TMM.2018.2879750
   Xia XJ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P306, DOI 10.1109/ICASSP.2018.8461845
   Xia XJ, 2017, INTERSPEECH, P474, DOI 10.21437/Interspeech.2017-746
   Xia XJ, 2018, PATTERN RECOGN, V81, P1, DOI 10.1016/j.patcog.2018.03.025
   Xia XJ, 2017, IEEE INT CON MULTI, P163, DOI 10.1109/ICME.2017.8019452
   Xia XJ, 2017, IEEE INT CON MULTI, P157, DOI 10.1109/ICME.2017.8019418
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zheng Y, 2017, PATTERN RECOGN, V67, P97, DOI 10.1016/j.patcog.2017.01.029
   Zhuang XD, 2009, INT CONF ACOUST SPEE, P69, DOI 10.1109/ICASSP.2009.4959522
NR 40
TC 14
Z9 14
U1 4
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 569
EP 578
DI 10.1109/TMM.2019.2933330
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700001
DA 2024-07-18
ER

PT J
AU Kim, BK
   Kim, G
   Lee, SY
AF Kim, Bo-Kyeong
   Kim, Geonmin
   Lee, Soo-Young
TI Style-Controlled Synthesis of Clothing Segments for Fashion Image
   Manipulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Image segmentation; Image color analysis; Geometry; Neural
   networks; Visualization; Image synthesis; Clothing design; convolutional
   neural network; image synthesis; style transfer; virtual try-on
ID TRY-ON
AB We propose an approach for digitally altering peoples outfits in images. Given images of a person and a desired clothing style, our method generates a new clothing item image. The new item displays the color and pattern of the desired style while geometrically mimicking the persons original item. Through superimposition, the altered image is made to look as if the person is wearing the new item. Unlike recent works with full-image synthesis, our work relies on segment synthesis, yielding benefits in virtual try-on. For the synthesis process, we assume two underlying factors characterizing clothing segments: geometry and style. These two factors are disentangled via preprocessing and combined using a neural network. We explore several networks and introduce important aspects of the architecture and learning process. Our experimental results are three-fold: 1) on images from fashion-parsing datasets, we demonstrate the generation of high-quality clothing segments with fine-level style control; 2) on a virtual try-on benchmark, our method shows superiority over prior synthesis methods; and 3) in transferring clothing styles, we visualize the differences between our method and neural style transfer.
C1 [Kim, Bo-Kyeong; Kim, Geonmin; Lee, Soo-Young] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Lee, SY (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 305701, South Korea.
EM bokyeong1015@gmail.com; gmkim90@kaist.ac.kr; sylee@kaist.ac.kr
FU Institute for Information and Communications Technology Promotion (IITP)
   - Korean Government (MSIT) [2016-0-00562 (R0124-16-0002)]
FX This work was supported by an Institute for Information and
   Communications Technology Promotion (IITP) grant funded by the Korean
   Government (MSIT) under Grant 2016-0-00562 (R0124-16-0002). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Xilin Chen.
CR [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2018, ICLR
   [Anonymous], 2013, P INT C MACH LEARN W
   [Anonymous], 2014, AUTOENCODING VARIATI
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen Q, 2015, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR.2015.7299169
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Fouhey DF, 2015, IEEE I CONF COMP VIS, P1053, DOI 10.1109/ICCV.2015.126
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Hauswiesner S, 2013, IEEE T VIS COMPUT GR, V19, P1552, DOI 10.1109/TVCG.2013.67
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Jiang SH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3721
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Kulkarni TD, 2015, ADV NEUR IN, V28
   Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li Y., 2017, Universal style transfer via feature transforms, P386
   Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Reed S, 2016, PR MACH LEARN RES, V48
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723
   Sekhavat YA, 2017, IEEE T MULTIMEDIA, V19, P1041, DOI 10.1109/TMM.2016.2639380
   Sekine M., 2014, Int. Conf. on 3D Body Scanning Technologies, P406
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhao B, 2017, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2017.652
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 50
TC 29
Z9 29
U1 5
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 298
EP 310
DI 10.1109/TMM.2019.2929000
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300002
DA 2024-07-18
ER

EF