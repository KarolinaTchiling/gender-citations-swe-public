FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Dai, HH
   Gao, SS
   Huang, H
   Mao, DQ
   Zhang, CH
   Zhou, YF
AF Dai, Honghao
   Gao, Shanshan
   Huang, Hong
   Mao, Deqian
   Zhang, Chenhao
   Zhou, Yuanfeng
TI An Adaptive Sample Assignment Network for Tiny Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tiny object detection; sample assignment strategy; focus enhancement;
   adaptive cropping method
AB Tiny objects often have a small proportion of pixels in the image, leading to significant differences in the number of positive and negative samples and the lack of feature information. Accurately determining the position and category of tiny objects remains a huge challenge for object detection research. Therefore, we design an Adaptive Sample Assignment Strategy(ASAS) and tiny object focusing enhancement module to solve the above two problems. Specifically, starting from the study of positive and negative sample selection and balance strategies for tiny objects, we construct a lightweight Object Existence Probability Determination Network (OEPD/Net) to focus on the areas where tiny objects exist, and achieve adaptive assignment and balance of samples. A top/down, layer by layer focusing enhancement module is designed to effectively enhance the propagation ability of high/level semantic information for tiny objects. The above two solutions have excellent generalization and migration capabilities and can be applied to any stage and two-stage object detection network, effectively enhancing TOD performance. Finally, this article provides a performance analysis of detection performance the detection network based on the OEPD/Net output results, and demonstrates the effectiveness of the proposed OEPD-Net and focusing enhancement module through extensive experiments on a public dataset.
C1 [Dai, Honghao; Huang, Hong; Mao, Deqian] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Peoples R China.
   [Gao, Shanshan] Shandong Univ Finance & Econ, Shandong China US Digital Media Int Cooperat Res C, Sch Comp Sci & Technol, Shandong Prov Key Lab Digital Media Technol, Jinan 250014, Peoples R China.
   [Zhang, Chenhao] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
   [Zhou, Yuanfeng] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University of
   Finance & Economics; Beijing Institute of Technology; Shandong
   University
RP Gao, SS (corresponding author), Shandong Univ Finance & Econ, Shandong China US Digital Media Int Cooperat Res C, Sch Comp Sci & Technol, Shandong Prov Key Lab Digital Media Technol, Jinan 250014, Peoples R China.
EM dhhtang@163.com; gsszxy@aliyun.com; hh981222@163.com; mamba_mdq@163.com;
   zachzhang07@163.com; yfzhou@sdu.edu.cn
RI Zhou, Yuanfeng/AAT-4670-2020; zhang, chenhao/KMY-0335-2024
OI Mao, Deqian/0000-0002-8711-2296; Zhang, Chenhao/0000-0002-4932-0705;
   Huang, Hong/0009-0000-5315-4300
FU National Key Research and Development Plan on Strategic International
   Scientific and Technological Innovation Cooperation
FX No Statement Available
CR Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen YK, 2021, Arxiv, DOI arXiv:2004.12432
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Ding J, 2022, IEEE T PATTERN ANAL, V44, P7778, DOI 10.1109/TPAMI.2021.3117983
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Gerónimo D, 2010, IEEE T PATTERN ANAL, V32, P1239, DOI 10.1109/TPAMI.2009.122
   Kisantal M, 2019, Arxiv, DOI arXiv:1902.07296
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Lee C, 2022, PROC CVPR IEEE, P14116, DOI 10.1109/CVPR52688.2022.01374
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu HK, 2020, IEEE T GEOSCI REMOTE, V58, P8689, DOI 10.1109/TGRS.2020.2989825
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sixt L, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00066
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Tychsen-Smith L, 2017, IEEE I CONF COMP VIS, P428, DOI 10.1109/ICCV.2017.54
   Wang JW, 2021, INT C PATT RECOG, P3791, DOI 10.1109/ICPR48806.2021.9413340
   Xie J, 2023, IEEE T MULTIMEDIA, V25, P2153, DOI 10.1109/TMM.2022.3143707
   Xu C, 2022, LECT NOTES COMPUT SC, V13669, P526, DOI 10.1007/978-3-031-20077-9_31
   Xu C, 2022, ISPRS J PHOTOGRAMM, V190, P79, DOI 10.1016/j.isprsjprs.2022.06.002
   Yu XH, 2020, IEEE WINT CONF APPL, P1246, DOI 10.1109/WACV45572.2020.9093394
   Zhang CH, 2022, IEEE T CIRC SYST VID, V32, P7772, DOI 10.1109/TCSVT.2022.3183641
   Zhang CH, 2020, COMPUT GRAPH FORUM, V39, P411, DOI 10.1111/cgf.14155
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhu CC, 2018, PROC CVPR IEEE, P5127, DOI 10.1109/CVPR.2018.00538
NR 35
TC 1
Z9 1
U1 13
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2918
EP 2931
DI 10.1109/TMM.2023.3305120
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400017
DA 2024-08-05
ER

PT J
AU Huang, J
   Gong, YS
   Zhang, L
   Zhang, J
   Nie, LQ
   Yin, YL
AF Huang, Jin
   Gong, Yongshun
   Zhang, Lu
   Zhang, Jian
   Nie, Liqiang
   Yin, Yilong
TI Modeling Multiple Aesthetic Views for Series Photo Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Transformers; Convolutional neural networks; Quality
   assessment; Visualization; Task analysis; Representation learning; Image
   aesthetic assessment; series photo selection; multi-view learning;
   visual attention; graph neural network
ID IMAGE
AB Numerous photos are taken in daily life, and sorting them is laborious and time consuming. The large number of similar images exacerbates the difficulty of album management, under this scenario, serial photo selection (SPS) emerges. As an important branch of image aesthetic quality assessment, it focuses on identifying the best image among a series of almost identical photos. Currently, most existing SPS methods focus only on extracting features from the original image, while neglecting the fact that multiple views of the image can provide much more detailed aesthetic information. In this article, we propose a Siamese network structure called SPSNet to enhance the representation learning of multi-view features by acquiring the depth, generic, and handcrafted features of images. In specific, we implement a parallel structure to extract deep and shallow features, fusing local and global representations at different resolutions interactively. The aggregation of multiple views of image via a self-attentive module with adaptive weights enables the model to discriminate the importance of each view. Moreover, we employ a graph neural network to construct the relationships among the multi-view features. Our proposed method, which is trained by a Siamese network, can effectively distinguish the nuances of similar images, and thus, select the best one from a series of almost identical photos. Extensive experiments conducted on the aesthetic dataset demonstrate that our method outperforms other state-of-the-art SPS methods, which achieves the 75.36% accuracy on the Phototriage dataset. Besides, our model is up to 3.04% better than the baseline methods in terms of the average accuracy.
C1 [Huang, Jin; Gong, Yongshun; Yin, Yilong] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Zhang, Lu] Univ Queensland, Brisbane, Qld 4072, Australia.
   [Zhang, Jian] Univ Technol Sydney, Multimedia & Data Analyt Lab, Ultimo, NSW 2007, Australia.
   [Nie, Liqiang] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen Campus, Harbin 518055, Peoples R China.
C3 Shandong University; University of Queensland; University of Technology
   Sydney; Harbin Institute of Technology
RP Gong, YS; Yin, YL (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM jinhuang.mla@gmail.com; ysgong@sdu.edu.cn; l.zhang3@uq.edu.au;
   Jian.Zhang@uts.edu.au; nieliqiang@gmail.com; ylyin@sdu.edu.cn
OI Zhang, Lu/0000-0002-2225-9772; Zhang, Jian/0000-0002-7240-3541; Gong,
   Yongshun/0000-0003-3948-4471
FU National Natural Science Foundation of China
FX No Statement Available
CR Abnar S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4190
   [Anonymous], 2011, ACM International Conference on Multimedia MM
   Belkin M, 2002, ADV NEUR IN, V14, P585
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chang HW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925908
   Chen Q., 2020, CVPR, P14102
   Chen YP, 2022, PROC CVPR IEEE, P5260, DOI 10.1109/CVPR52688.2022.00520
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830
   Dasgupta S, 2002, ADV NEUR IN, V14, P375
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P870, DOI 10.1145/3240508.3240531
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Dosovitskiy A., 2021, PROC ICLR
   Gong YS, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1120, DOI 10.1145/3474085.3481538
   Gong YS, 2023, IEEE T KNOWL DATA EN, V35, P686, DOI 10.1109/TKDE.2021.3072642
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Huang J, 2022, PROC IEEE INT C MULT, P1
   Huang J, 2020, INT CONF ACOUST SPEE, P2742, DOI [10.1109/ICASSP40776.2020.9053731, 10.1109/icassp40776.2020.9053731]
   Kao YY, 2016, SIGNAL PROCESS-IMAGE, V47, P500, DOI 10.1016/j.image.2016.05.004
   Ke Y., 2006, CVPR, P419, DOI DOI 10.1109/CVPR.2006.303
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Ko K, 2018, IEEE IMAGE PROC, P2491, DOI 10.1109/ICIP.2018.8451621
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Li RY, 2018, AAAI CONF ARTIF INTE, P3546
   Liu D, 2020, IEEE WINT CONF APPL, P3558, DOI [10.1109/WACV45572.2020.9093412, 10.1109/wacv45572.2020.9093412]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Malach E, 2017, ADV NEUR IN, V30
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mehta S., 2022, PROC INT C LEARN REP
   Mehta S., 2021, PROC INT C LEARN REP
   Murray A., 2017, CoRR
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   She DY, 2021, PROC CVPR IEEE, P8471, DOI 10.1109/CVPR46437.2021.00837
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Teng JY, 2022, IEEE T MULTIMEDIA, V24, P1141, DOI 10.1109/TMM.2021.3120545
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xiao T, 2021, ADV NEUR IN, V34
   Zeng H, 2020, IEEE T IMAGE PROCESS, V29, P1548, DOI 10.1109/TIP.2019.2941778
   Zhang J., 2018, IMAG COMPUTTECHN APP, P1
   Zhang L, 2022, IEEE T MULTIMEDIA, V24, P1415, DOI 10.1109/TMM.2021.3064408
   Zhang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4086, DOI 10.1109/ICCV48922.2021.00407
NR 63
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1983
EP 1995
DI 10.1109/TMM.2023.3290751
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800023
DA 2024-08-05
ER

PT J
AU Kong, XY
   Chen, YY
   He, ZY
AF Kong, Xiaoyu
   Chen, Yongyong
   He, Zhenyu
TI When Channel Correlation Meets Sparse Prior: Keeping Interpretability in
   Image Compressive Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Correlation; Transforms; Indexes; Thresholding
   (Imaging); Optimization; Iterative algorithms; Channel correlation;
   compressive sensing; deep prior; sparse prior
ID LOW-RANK; RECONSTRUCTION; RECOVERY; ALGORITHMS; NETWORK
AB Image compressive sensing (CS), recovering an unknown image by resorting to a small number of its measurements, has become an increasingly popular topic in multimedia technology and applications. For a better reconstruction, diverse priors, from the original sparse prior to the new deep prior, have been exploited. Despite the powerful learning capability and satisfactory reconstruction performance, the deep prior is known as a black box and loses clear interpretability. In this article, we first revisit image CS with different priors and observe that the method with hand-crafted sparse prior could still outperform state-of-art methods with deep prior or no prior when under the same settings, while the interpretability is well preserved. Then, towards a better performance of the sparse-prior-based method, we propose a Channel Adaptive Thresholding Network, namely CAT-Net. CAT-Net draws the support from channel correlation calculation to extend the single thresholding in the iterative soft thresholding algorithm (ISTA) into channel-wise thresholding. The channel adaptive thresholding conducts soft thresholding operation in each channel of the image features and can be adjusted adaptively to the inputs, which can reconstruct more precisely than a single static thresholding. The careful CAT operation can preserve patterns both in detail and holistically well. Experimental results demonstrate the proposed method outperforms the state-of-the-art image CS methods with both traditional and deep priors.
C1 [Kong, Xiaoyu; Chen, Yongyong; He, Zhenyu] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [He, Zhenyu] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory
RP Chen, YY; He, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM 21s151102@stu.hit.edu.cn; yongyongchen.cn@gmail.com; zhenyuhe@hit.edu.cn
RI kong, xiaoyu/KIK-7342-2024; Chen, yongyong/P-3801-2016
OI Kong, Xiaoyu/0009-0004-1386-9394; Chen, yongyong/0000-0003-1970-1993
FU National Natural Science Foundation of China
FX No Statement Available
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chen C, 2011, CONF REC ASILOMAR C, P1193, DOI 10.1109/ACSSC.2011.6190204
   Chen YY, 2020, IEEE T IMAGE PROCESS, V29, P1426, DOI 10.1109/TIP.2019.2941319
   Chen Z, 2021, IEEE T IMAGE PROCESS, V30, P7112, DOI 10.1109/TIP.2021.3088611
   Cheng ZQ, 2017, PROC CVPR IEEE, P4169, DOI 10.1109/CVPR.2017.444
   Cheng ZQ, 2017, IEEE T MULTIMEDIA, V19, P1170, DOI 10.1109/TMM.2016.2647386
   Cheng ZQ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1365, DOI 10.1145/2964284.2964326
   Cotter SF, 2005, IEEE T SIGNAL PROCES, V53, P2477, DOI 10.1109/TSP.2005.849172
   Cui WX, 2023, IEEE T MULTIMEDIA, V25, P816, DOI 10.1109/TMM.2021.3132489
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Duarte MF, 2013, IEEE T INFORM THEORY, V59, P4280, DOI 10.1109/TIT.2013.2252051
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ganie JA, 2021, INT J WAVELETS MULTI, V19, DOI 10.1142/S0219691321500028
   Gregor K., 2010, P 27 INT C INT C MAC, P399
   Haupt J, 2006, IEEE T INFORM THEORY, V52, P4036, DOI 10.1109/TIT.2006.880031
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hui C., 2022, P IEEE INT C MULT EX, P1
   Ing CK, 2011, STAT SINICA, V21, P1473, DOI 10.5705/ss.2010.081
   Jiang QR, 2020, IEEE T MULTIMEDIA, V22, P594, DOI 10.1109/TMM.2019.2931400
   Kim YK, 2010, IEEE IMAGE PROC, P3365, DOI 10.1109/ICIP.2010.5652744
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Lai ZY, 2016, MED IMAGE ANAL, V27, P93, DOI 10.1016/j.media.2015.05.012
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Li CQ, 2019, J INF SECUR APPL, V48, DOI 10.1016/j.jisa.2019.102361
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Liu J., 2019, INT C LEARN REPR
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Mallat S., 1999, A wavelet Tour of Signal Processing
   Meinhardt T, 2017, IEEE I CONF COMP VIS, P1799, DOI 10.1109/ICCV.2017.198
   Metzler CA, 2017, ADV NEUR IN, V30
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Michailovich O, 2011, IEEE T MED IMAGING, V30, P1100, DOI 10.1109/TMI.2011.2142189
   Moreau T., 2017, P INT C LEARN REPR
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Qu XB, 2014, MED IMAGE ANAL, V18, P843, DOI 10.1016/j.media.2013.09.007
   Qu XB, 2012, MAGN RESON IMAGING, V30, P964, DOI 10.1016/j.mri.2012.02.019
   Ravishankar S, 2017, IEEE T MED IMAGING, V36, P1116, DOI 10.1109/TMI.2017.2650960
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Song JC, 2023, IEEE T IMAGE PROCESS, V32, P2202, DOI 10.1109/TIP.2023.3263100
   Song JC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4249, DOI 10.1145/3474085.3475562
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Usman M, 2011, MAGN RESON MED, V66, P1163, DOI 10.1002/mrm.22883
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1
   Yang Y, 2020, IEEE T PATTERN ANAL, V42, P521, DOI 10.1109/TPAMI.2018.2883941
   You D., 2021, P IEEE INT C MULT EX, P1, DOI DOI 10.1109/ICME51207.2021.9428249
   You D, 2021, IEEE T IMAGE PROCESS, V30, P6066, DOI 10.1109/TIP.2021.3091834
   Yuan X, 2021, IEEE SIGNAL PROC MAG, V38, P65, DOI 10.1109/MSP.2020.3023869
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2014, SIGNAL PROCESS, V103, P114, DOI 10.1016/j.sigpro.2013.09.025
   Zhang J, 2013, IEEE INT SYMP CIRC S, P2836
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KY, 2023, IEEE T MULTIMEDIA, V25, P5676, DOI 10.1109/TMM.2022.3198323
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
   Zheng HY, 2021, PROC CVPR IEEE, P630, DOI 10.1109/CVPR46437.2021.00069
   Zheng S, 2021, IEEE T MULTIMEDIA, V23, P3577, DOI 10.1109/TMM.2020.3028479
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
NR 69
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2953
EP 2965
DI 10.1109/TMM.2023.3305828
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700001
DA 2024-08-05
ER

PT J
AU Li, FY
   Sheng, Y
   Zhang, XP
   Qin, C
AF Li, Fengyong
   Sheng, Yang
   Zhang, Xinpeng
   Qin, Chuan
TI iSCMIS:Spatial-Channel Attention Based Deep Invertible Network for
   Multi-Image Steganography
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Steganography; Distortion; Neural networks; Couplings; Security;
   Visualization; Additives; Attention mechanism; high-capacity; invertible
   network; multi-image steganography; visual quality
ID STEGANALYSIS
AB Multi-image steganography refers to a stegano- graphic method where a user tries to hide multiple confidential images within a single cover image, and all confidential images can be correspondingly recovered perfectly by the recipient. Multi-image steganography essentially belongs to a high-capacity image steganographic scheme, but such high hiding capacity may easily cause severe contour shadows or color distortion of steganographic images, resulting in a significant reduction in anti-steganalysis capability. To address the above problem, this article designs a deep invertible neural network by introducing spatial-channel joint attention mechanism, in which the confidential image hiding and recovery can be regarded as a pair of coupled invertible processes. Specifically, a series of simple invertible networks having the same structure are firstly used to construct a cascaded deep invertible neural network framework, in which multiple confidential images can be sequentially embedded into a single cover image through a series of flexible cascaded iterative operations. Subsequently, spatial-channel joint attention module is designed to re-construct invertible network model, which can guide the embedding of secret information into more secure image regions. Accordingly, this joint attention mechanism can effectively address the problem of visual quality and security degradation of steganographic images due to high embedding capacity. Extensive experiments demonstrate that our scheme can obtain superior performance over different large-scale image sets, and outperforms state-of-the art methods with higher visual quality and stronger anti-steganalysis capability.
C1 [Li, Fengyong; Sheng, Yang] Shanghai Univ Elect Power, Coll Comp Sci & Technol, Shanghai 201306, Peoples R China.
   [Li, Fengyong] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Qin, Chuan] Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
C3 Shanghai University of Electric Power; Guangxi Normal University; Fudan
   University; University of Shanghai for Science & Technology
RP Qin, C (corresponding author), Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
EM fyli@shiep.edu.cn; sy20001017xsf@163.com; zhangxinpeng@fudan.edu.cn;
   qin@usst.edu.cn
RI Qin, Chuan/C-1106-2017
OI Qin, Chuan/0000-0002-0370-4623; Li, Fengyong/0000-0002-3385-8164
FU National Natural Science Foundation of China
FX No Statement Available
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ardizzone L, 2019, Arxiv, DOI [arXiv:1907.02392, 10.48550/arXiv.1907.02392]
   Baluja S, 2017, ADV NEUR IN, V30
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Das A, 2021, Arxiv, DOI arXiv:2101.00350
   Denemark T, 2014, IEEE INT WORKS INFOR, P48, DOI 10.1109/WIFS.2014.7084302
   Dinh L, 2015, Arxiv, DOI [arXiv:1410.8516, 10.48550/arXiv.1410.8516]
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Guan ZY, 2023, IEEE T PATTERN ANAL, V45, P372, DOI 10.1109/TPAMI.2022.3141725
   Haibin Wu, 2020, Digital Forensics and Watermarking. 18th International Workshop, IWDW 2019. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12022), P3, DOI 10.1007/978-3-030-43575-2_1
   Hayes J., 2017, NIPS, P1954
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Jia J, 2023, IEEE T MULTIMEDIA, V25, P7364, DOI 10.1109/TMM.2022.3221894
   Jing JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4713, DOI 10.1109/ICCV48922.2021.00469
   Li B, 2014, IEEE T INF FOREN SEC, V9, P1264, DOI 10.1109/TIFS.2014.2326954
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Li L, 2020, IEEE T MULTIMEDIA, V22, P2526, DOI 10.1109/TMM.2019.2959909
   Li WJ, 2023, IEEE T MULTIMEDIA, V25, P8320, DOI 10.1109/TMM.2023.3234812
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Lu W, 2021, IEEE T DEPEND SECURE, V18, P1137, DOI 10.1109/TDSC.2019.2933621
   Mielikainen J, 2006, IEEE SIGNAL PROC LET, V13, P285, DOI 10.1109/LSP.2006.870357
   Mingqing Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P126, DOI 10.1007/978-3-030-58452-8_8
   Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Tan JX, 2022, IEEE T NETW SCI ENG, V9, P888, DOI 10.1109/TNSE.2021.3139671
   Tang WX, 2017, IEEE SIGNAL PROC LET, V24, P1547, DOI 10.1109/LSP.2017.2745572
   van der Ouderaa TFA, 2019, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2019.00485
   Wang Y, 2008, IEEE T INFORM THEORY, V54, P2706, DOI 10.1109/TIT.2008.921684
   Weng SW, 2023, IEEE T MULTIMEDIA, V25, P8738, DOI 10.1109/TMM.2023.3241541
   Weng XY, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P87, DOI 10.1145/3323873.3325011
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang JH, 2020, IEEE T INF FOREN SEC, V15, P839, DOI 10.1109/TIFS.2019.2922229
   Zhang R, 2020, IEEE T INF FOREN SEC, V15, P1138, DOI 10.1109/TIFS.2019.2936913
   Zhang SQ, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22207844
   Zhou WB, 2017, IEEE T INF FOREN SEC, V12, P2654, DOI 10.1109/TIFS.2017.2718480
NR 40
TC 3
Z9 3
U1 17
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3137
EP 3152
DI 10.1109/TMM.2023.3307970
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700015
DA 2024-08-05
ER

PT J
AU Liang, YZ
   Zhu, LC
   Wang, XH
   Yang, Y
AF Liang, Yuanzhi
   Zhu, Linchao
   Wang, Xiaohan
   Yang, Yi
TI IcoCap: Improving Video Captioning by Compounding Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-modal understanding; representation learning; video captioning
AB Video captioning is a more challenging task compared to image captioning, primarily due to differences in content density. Video data contains redundant visual content, making it difficult for captioners to generalize diverse content and avoid being misled by irrelevant elements. Moreover, redundant content is not well-trimmed to match the corresponding visual semantics in the ground truth, further increasing the difficulty of video captioning. Current research in video captioning predominantly focuses on captioner design, neglecting the impact of content density on captioner performance. Considering the differences between videos and images, there exists an another line to improve video captioning by leveraging concise and easily-learned image samples to further diversify video samples. This modification to content density compels the captioner to learn more effectively against redundancy and ambiguity. In this article, we propose a novel approach called Image-Compounded learning for video Captioners (IcoCap) to facilitate better learning of complex video semantics. IcoCap comprises two components: the Image-Video Compounding Strategy (ICS) and Visual-Semantic Guided Captioning (VGC). ICS compounds easily-learned image semantics into video semantics, further diversifying video content and prompting the network to generalize contents in a more diverse sample. Besides, learning with the sample compounded with image contents, the captioner is compelled to better extract valuable video cues in the presence of straightforward image semantics. This helps the captioner further focus on relevant information while filtering out extraneous content. Then, VGC guides the network in flexibly learning ground truth captions based on the compounded samples, helping to mitigate the mismatch between the ground truth and ambiguous semantics in video samples. Our experimental results demonstrate the effectiveness of IcoCap in improving the learning of video captioners. Applied to the widely-used MSVD, MSR-VTT, and VATEX datasets, our approach achieves competitive or superior results compared to state-of-the-art methods, illustrating its capacity to handle the redundant and ambiguous video data
C1 [Liang, Yuanzhi] Univ Technol Sydney, Australian Artificial Intelligence Inst, Sydney, NSW 2007, Australia.
   [Zhu, Linchao; Wang, Xiaohan; Yang, Yi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 University of Technology Sydney; Zhejiang University
RP Zhu, LC (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM yuanzhi.liang@student.uts.edu.au; zhulinchao@zju.edu.cn;
   xiaohan.wang@zju.edu.cn; yangyics@zju.edu.cn
FU Australian Research Council
FX No Statement Available
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Aafaq N, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355390
   Alwassel H., 2020, NEURIPS, V33, P9758
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen David, 2011, ACL
   Chen JW, 2019, AAAI CONF ARTIF INTE, P8167
   Chen SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1523, DOI 10.1109/ICCV48922.2021.00157
   Chen SZ, 2019, IEEE T MULTIMEDIA, V21, P2407, DOI 10.1109/TMM.2019.2896515
   Chen SH, 2023, Arxiv, DOI arXiv:2304.08345
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22
   Dempsey PW, 1996, SCIENCE, V271, P348, DOI 10.1126/science.271.5247.348
   Denkowski M., 2014, P WMT ACL, P376
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Han YH, 2020, IEEE T CIRC SYST VID, V30, P875, DOI 10.1109/TCSVT.2019.2897604
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hou JY, 2019, IEEE I CONF COMP VIS, P8917, DOI 10.1109/ICCV.2019.00901
   Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Li HD, 2019, IEEE INT CON MULTI, P1312, DOI 10.1109/ICME.2019.00228
   Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2208
   Li YW, 2023, PROC CVPR IEEE, P2604, DOI 10.1109/CVPR52729.2023.00256
   Li YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3799, DOI 10.1145/3474085.3478331
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin K, 2022, PROC CVPR IEEE, P17928, DOI 10.1109/CVPR52688.2022.01742
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2021, IEEE T PATTERN ANAL, V43, P3259, DOI 10.1109/TPAMI.2019.2940007
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Mittal S, 2021, J SYST ARCHITECT, V115, DOI 10.1016/j.sysarc.2021.102041
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Patrick M., 2021, PROC INT C LEARN REP
   Radford A, 2021, PR MACH LEARN RES, V139
   RUMELHART DE, 1985, COGNITIVE SCI, V9, P75, DOI 10.1207/s15516709cog0901_5
   Seo PH, 2022, PROC CVPR IEEE, P17938, DOI 10.1109/CVPR52688.2022.01743
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Tang MK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4858, DOI 10.1145/3474085.3479207
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang HY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1519, DOI 10.1145/3240508.3240677
   Wang XH, 2023, IEEE T MULTIMEDIA, V25, P6079, DOI 10.1109/TMM.2022.3204444
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Wang XL, 2021, PROC CVPR IEEE, P3023, DOI 10.1109/CVPR46437.2021.00304
   Wu A, 2022, IEEE T PATTERN ANAL, V44, P4178, DOI 10.1109/TPAMI.2021.3060446
   Wu X., 2022, IEEE Trans. Big Data, DOI [10.1109/TBDATA.2022.3186991, DOI 10.1109/TBDATA.2022.3186991]
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang Bang, 2022, P CHIN C PATT REC CO, P368, DOI DOI 10.1007/978--3-031-18907-4_29
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Ye HH, 2022, PROC CVPR IEEE, P17918, DOI 10.1109/CVPR52688.2022.01741
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Zhao WT, 2021, ADV NEUR IN, V34
   Zheng Q., 2020, 2020 CVPR, P13093, DOI 10.1109/CVPR42600.2020.01311
   Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503
   Zhu Linchao, 2020, P IEEECVF C COMPUTER, DOI 10.1109/CVPR42600.2020.00877
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 66
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4389
EP 4400
DI 10.1109/TMM.2023.3322329
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100026
DA 2024-08-05
ER

PT J
AU Lyu, Y
   Chen, PB
   Sun, JN
   Peng, B
   Wang, X
   Dong, J
AF Lyu, Yueming
   Chen, Peibin
   Sun, Jingna
   Peng, Bo
   Wang, Xu
   Dong, Jing
TI DRAN: Detailed Region-Adaptive Normalization for Conditional Image
   Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Conditional image synthesis; makeup transfer; semantic image synthesis;
   generative adversarial network
AB In recent years, conditional image synthesis has attracted growing attention due to its controllability in the image generation process. Although recent works have achieved realistic results, most of them have difficulty handling fine-grained styles with subtle details. To address this problem, a novel normalization module, named Detailed Region-Adaptive Normalization (DRAN), is proposed. It adaptively learns both fine-grained and coarse-grained style representations. Specifically, we first introduce a multi-level structure, Spatiality-aware Pyramid Pooling, to guide the model to learn coarse-to-fine features. Then, to adaptively fuse different levels of styles, we propose Dynamic Gating, making it possible to adaptively fuse different levels of styles according to different spatial regions. Finally, we collect a new makeup dataset (Makeup-Complex dataset) that contains a wide range of complex makeup styles with diverse poses and expressions. To evaluate the effectiveness and show the general use of our method, we conduct a set of experiments on makeup transfer and semantic image synthesis. Quantitative and qualitative experiments show that equipped with DRAN, simple baseline models are able to achieve promising improvements in complex style transfer and detailed texture synthesis.
C1 [Lyu, Yueming; Peng, Bo; Dong, Jing] Chinese Acad Sci, Inst Automat, Ctr Res Intelligent Percept & Comp, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Lyu, Yueming; Peng, Bo; Dong, Jing] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Chen, Peibin; Sun, Jingna; Wang, Xu] ByteDance Inc, Beijing 100191, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Dong, J (corresponding author), Chinese Acad Sci, Inst Automat, Ctr Res Intelligent Percept & Comp, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
EM yueming.lv@cripac.ia.ac.cn; chenpeibin@bytedance.com; 850248724@qq.com;
   bo.peng@nlpr.ia.ac.cn; wangxu.ailab@bytedance.com; jdong@nlpr.ia.ac.cn
OI peng, bo/0000-0002-9014-7369; Lyu, Yueming/0000-0003-4028-5250
FU National Key Research and Development Program of China
FX No Statement Available
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Brock A., 2019, PROC INT C LEARN REP
   Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012
   Chen HJ, 2019, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR.2019.01028
   Chen TY, 2022, IEEE T MULTIMEDIA, V24, P2975, DOI 10.1109/TMM.2021.3091859
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng H, 2021, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR46437.2021.00648
   Deng QY, 2023, IEEE T MULTIMEDIA, V25, P2799, DOI 10.1109/TMM.2022.3151507
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I., 2014, Adv. NeuralInf. Process. Syst., P1
   Gu Q, 2019, IEEE I CONF COMP VIS, P10480, DOI 10.1109/ICCV.2019.01058
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2018, INT C LEARNING REPRE
   Kingma D.P., 2014, Proc. of ICLR
   Kingma DP, 2018, ADV NEUR IN, V31
   Krizhevsky I., 2012, Adv. Neural Inf. Process.Syst., P1
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Ling J, 2021, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR46437.2021.00924
   Liu YH, 2023, IEEE T MULTIMEDIA, V25, P3343, DOI 10.1109/TMM.2022.3159115
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lv ZY, 2021, PROC CVPR IEEE, P10801, DOI 10.1109/CVPR46437.2021.01066
   Lyu YM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3601, DOI 10.1145/3474085.3475531
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Organisciak D, 2021, INT C PATT RECOG, P6011, DOI 10.1109/ICPR48806.2021.9412604
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Tan ZT, 2021, PROC CVPR IEEE, P7958, DOI 10.1109/CVPR46437.2021.00787
   Tan ZT, 2022, IEEE T PATTERN ANAL, V44, P4852, DOI 10.1109/TPAMI.2021.3076487
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Y., 2021, P IEEE CVF INT C COM, p13 749
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1
   Yu T, 2020, AAAI CONF ARTIF INTE, V34, P12733
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhong YY, 2021, IEEE T IMAGE PROCESS, V30, P2587, DOI 10.1109/TIP.2020.3048632
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2020, PROC CVPR IEEE, P5466, DOI 10.1109/CVPR42600.2020.00551
NR 57
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1969
EP 1982
DI 10.1109/TMM.2023.3290481
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800014
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, RB
   Ying, XH
   Xing, BW
AF Wang, Ruibin
   Ying, Xianghua
   Xing, Bowei
TI Exploiting Temporal Correlations for 3D Human Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D human pose estimation; Temporal correlation; Sequence modeling
AB Exploiting the rich temporal information in human pose sequences to facilitate 3D pose estimation has garnered particular attention. While various learning architectures have been designed for temporal exploiting, these architectures are usually trained via the 3D pose loss independently imposed on every single frame, without explicit temporal signals introduced for supervision. This inevitably increases the difficulty of temporal exploiting, since the network must reason about the meaningful temporal information based on the non-temporal single-frame supervision first. Only then, the network can utilize this information to guide sequence modeling. Recently, some work introduce temporal smoothness as an explicit supervision signal, which makes the network more straightforwardly reaches the temporal information from the supervision signal, thus improving the temporal exploiting. However, the temporal smoothness only roughly measures the short-term temporal properties between adjacent frame pairs. In this work, we propose to generalize the supervision of temporal smoothness to temporal correlations, letting the network precisely consider more comprehensive temporal properties in sequences. We contribute two novel correlation-based loss functions, which adopt different strategies to respectively regularize the encoder and decoder sides of the network for temporal exploiting. Besides, we design a pre-training scheme to ensure a general convergence of existing pose estimators under our correlation losses. Experiments on three benchmarks demonstrate that our method can be compatible with different networks, improving their temporal exploiting ability to output more accurate and robust pose estimations.
C1 [Wang, Ruibin; Ying, Xianghua; Xing, Bowei] Peking Univ, Sch Intelligence Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Ying, XH (corresponding author), Peking Univ, Sch Intelligence Sci & Technol, Beijing 100871, Peoples R China.
EM robin_wang@pku.edu.cn; xhying@pku.edu.cn; xingbowei@pku.edu.cn
FU National Key Research and Development Program of China
FX No Statement Available
CR Arnab A, 2019, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2019.00351
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Chen LJ, 2021, IEEE WINT CONF APPL, P1049, DOI 10.1109/WACV48630.2021.00109
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen X, 2023, IEEE Trans. Multimedia, early acces, DOI [10.1109/TMM.2023.3272736, DOI 10.1109/TMM.2023.3272736]
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Cho K., 2014, ARXIV14061078, V1406, P1078, DOI DOI 10.3115/V1/D14-1179
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Honari S, 2023, IEEE T PATTERN ANAL, V45, P6415, DOI 10.1109/TPAMI.2022.3215307
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Kenkun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P318, DOI 10.1007/978-3-030-58607-2_19
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Lee K, 2023, IEEE T PATTERN ANAL, V45, P1781, DOI 10.1109/TPAMI.2022.3164344
   Li TQ, 2023, INT J COMPUT VISION, V131, P1389, DOI 10.1007/s11263-023-01756-3
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Li WH, 2022, PROC CVPR IEEE, P13137, DOI 10.1109/CVPR52688.2022.01280
   Lin MD, 2017, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2017.588
   Liu H, 2023, IEEE T MULTIMEDIA, V25, P1390, DOI 10.1109/TMM.2022.3141888
   Liu JF, 2021, IEEE INT CONF ROBOT, P3374, DOI 10.1109/ICRA48506.2021.9561605
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Ma XX, 2021, PROC CVPR IEEE, P6234, DOI 10.1109/CVPR46437.2021.00617
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mu Jiteng, 2020, P IEEE CVF C COMP VI, P12386
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Shan WK, 2022, LECT NOTES COMPUT SC, V13665, P461, DOI 10.1007/978-3-031-20065-6_27
   Shere M, 2021, IEEE WINT CONF APPL, P81, DOI 10.1109/WACV48630.2021.00013
   Shuai H, 2023, IEEE T PATTERN ANAL, V45, P4122, DOI 10.1109/TPAMI.2022.3188716
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Tang ZH, 2023, IEEE T MULTIMEDIA, V25, P8712, DOI 10.1109/TMM.2023.3240455
   Tang ZH, 2023, PROC CVPR IEEE, P4790, DOI 10.1109/CVPR52729.2023.00464
   Tekin B., 2016, PROC BRIT MACH VIS C
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Vaswani A, 2017, ADV NEUR IN, V30
   Veges M., 2020, Neural Information Processing. 27th International Conference, ICONIP 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12532), P557, DOI 10.1007/978-3-030-63830-6_47
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wen YL, 2023, PROC CVPR IEEE, P21243, DOI 10.1109/CVPR52729.2023.02035
   Wu A., 2020, ADV NEURAL INFORM PR, Vvol 33, P6040, DOI [10.1101/2020.08.20.259705, DOI 10.1101/2020.08.20.259705]
   Xu TH, 2021, PROC CVPR IEEE, P16100, DOI 10.1109/CVPR46437.2021.01584
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang J, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108439
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Zhang JL, 2022, PROC CVPR IEEE, P13222, DOI 10.1109/CVPR52688.2022.01288
   Zhang XY, 2022, IEEE T MULTIMEDIA, V24, P166, DOI 10.1109/TMM.2020.3047552
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhao QT, 2023, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR52729.2023.00857
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zou SH, 2023, IEEE T CIRC SYST VID, V33, P4921, DOI 10.1109/TCSVT.2023.3244152
NR 52
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4527
EP 4539
DI 10.1109/TMM.2023.3323874
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100036
DA 2024-08-05
ER

PT J
AU Wu, JS
   Hao, FW
   Liang, WY
   Xu, J
AF Wu, Jiesheng
   Hao, Fangwei
   Liang, Weiyun
   Xu, Jing
TI Transformer Fusion and Pixel-Level Contrastive Learning for RGB-D
   Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-modality fusion; pixel-level contrastive learning; RGB-D salient
   object detection; transformer
ID NETWORK; REFINEMENT
AB Current RGB-D salient object detection (RGB-D SOD) methods mainly develop a generalizable model trained by binary cross-entropy (BCE) loss based on convolutional or Transformer backbones. However, they usually exploit convolutional modules to fuse multi-modality features, with little attention paid to capturing the long-range multi-modality interactions for feature fusion. Furthermore, BCE loss does not explicitly explore intra- and inter-pixel relationships in a joint embedding space. To address these issues, we propose a cross-modality interaction parallel-transformer (CIPT) module, which better captures the long-range multi-modality interactions, generating more comprehensive fusion features. Besides, we propose a pixel-level contrastive learning (PCL) method that improves inter-pixel discrimination and intra-pixel compactness, resulting in a well-structured embedding space and a better saliency detector. Specifically, we propose an asymmetric network (TPCL) for RGB-D SOD, which consists of a Swin V2 Transformer-based backbone and a designed lightweight backbone (LDNet). Moreover, an edge-guided module and a feature enhancement (FE) module are proposed to refine the learned fusion features. Extensive experiments demonstrate that our method achieves excellent performance against 15 state-of-the-art methods on seven public datasets. We expect our work to facilitate the exploration of applying Transformer and contrastive learning for RGB-D SOD tasks.
C1 [Wu, Jiesheng; Hao, Fangwei; Liang, Weiyun; Xu, Jing] Nankai Univ, Coll Artificial Intelligence, Tianjin 300071, Peoples R China.
C3 Nankai University
RP Xu, J (corresponding author), Nankai Univ, Coll Artificial Intelligence, Tianjin 300071, Peoples R China.
EM jasonwu@mail.nankai.edu.cn; haofangwei@mail.nankai.edu.cn;
   weiyunliang@mail.nankai.edu.cn; xujing@nankai.edu.cn
OI Wu, Jiesheng/0000-0002-6941-3300; Xu, Jing/0000-0001-8532-2241
FU National Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Islam MA, 2020, Arxiv, DOI arXiv:2001.08248
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2017, IEEE INT C INT ROBOT, P4911, DOI 10.1109/IROS.2017.8206370
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen Shaoxiang, 2020, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-58598-3_31
   Chen T., P INT C MACH LEARN, P1597
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cheng Y, 2014, IEEE INT CON MULTI
   Chu X, 2021, ARXIV, DOI DOI 10.48550/ARXIV.2102.10882
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2023, IEEE T MULTIMEDIA, V25, P6971, DOI 10.1109/TMM.2022.3216476
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dou ZY, 2022, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52688.2022.01763
   Enze Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P696, DOI 10.1007/978-3-030-58601-0_41
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan Y., P 16 EUR C COMP VIS
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Gunel B., 2020, Supervised contrastive learning for pre-trained language model fine-tuning
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huo FS, 2022, IEEE T CIRC SYST VID, V32, P3111, DOI 10.1109/TCSVT.2021.3102268
   Ji W., 2022, P INT C LEARN REPR
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Lee M., 2022, SPSN: Superpixel prototype sampling network for RGB-D salient object detection
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liu H., 2022, CMX: Cross-modal fusion for RGB-X semantic segmentation with transformers
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Mahadevan V, 2009, PROC CVPR IEEE, P1007, DOI 10.1109/CVPRW.2009.5206573
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Oord A.v.d., 2018, ARXIV
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Sun YJ, 2022, Arxiv, DOI arXiv:2207.00794
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu Zhengzheng, 2022, IEEE T MULTIMEDIA
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Dijk T, 2019, IEEE I CONF COMP VIS, P2183, DOI 10.1109/ICCV.2019.00227
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang FY, 2022, IEEE T IMAGE PROCESS, V31, P1285, DOI 10.1109/TIP.2022.3140606
   Wang G., 2018, P 13 C IM GRAPH TECH, P359
   Wang KX, 2020, IEEE INT CONF ROBOT, P4782, DOI [10.1109/icra40945.2020.9196847, 10.1109/ICRA40945.2020.9196847]
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XQ, 2022, IEEE T IMAGE PROCESS, V31, P1107, DOI 10.1109/TIP.2021.3139232
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu YH, 2023, IEEE T PATTERN ANAL, V45, P12760, DOI 10.1109/TPAMI.2022.3202765
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yang T, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5669, DOI 10.1145/3503161.3548037
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P5346, DOI 10.1109/TCSVT.2022.3144852
   Ye Mang, 2019, CVPR, P6210
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang M, 2023, IEEE T MULTIMEDIA, V25, P5142, DOI 10.1109/TMM.2022.3187856
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao XQ, 2022, AAAI CONF ARTIF INTE, P3463
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 106
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1011
EP 1026
DI 10.1109/TMM.2023.3275308
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700015
DA 2024-08-05
ER

PT J
AU Xie, JJ
   Zhang, S
   Xia, BH
   Xiao, Z
   Jiang, HB
   Zhou, SW
   Qin, Z
   Chen, HY
AF Xie, Jiajia
   Zhang, Sheng
   Xia, Beihao
   Xiao, Zhu
   Jiang, Hongbo
   Zhou, Siwang
   Qin, Zheng
   Chen, Hongyang
TI Pedestrian Trajectory Prediction Based on Social Interactions Learning
   With Random Weights
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pedestrians; Trajectory; Generative adversarial networks; Training; Long
   short term memory; Task analysis; Predictive models; Social
   interactions; GAN; graph with random weights
ID INTENT PREDICTION; FORCE MODEL
AB Pedestrian trajectory prediction is a critical technology in the evolution of self-driving cars toward complete artificial intelligence. Over recent years, focusing on the trajectories of pedestrians to model their social interactions has surged with great interest in more accurate trajectory predictions. However, existing methods for modeling pedestrian social interactions rely on pre-defined rules, struggling to capture non-explicit social interactions. In this work, we propose a novel framework named DTGAN, which extends the application of Generative Adversarial Networks (GANs) to graph sequence data, with the primary objective of automatically capturing implicit social interactions and achieving precise predictions of pedestrian trajectory. DTGAN innovatively incorporates random weights within each graph to eliminate the need for pre-defined interaction rules. We further enhance the performance of DTGAN by exploring diverse task loss functions during adversarial training, which yields improvements of 16.7% and 39.3% on metrics ADE and FDE, respectively. The effectiveness and accuracy of our framework are verified on two public datasets. The experimental results show that our proposed DTGAN achieves superior performance and is well able to understand pedestrians' intentions.
C1 [Xie, Jiajia; Xiao, Zhu; Jiang, Hongbo; Zhou, Siwang; Qin, Zheng] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Xie, Jiajia; Xiao, Zhu; Jiang, Hongbo; Zhou, Siwang; Qin, Zheng] Hunan Univ, Shenzhen Res Inst, Shenzhen 518055, Peoples R China.
   [Zhang, Sheng] Zhejiang Univ, Sch Comp Sci & Technol, Hangzhou 310000, Peoples R China.
   [Xia, Beihao] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Chen, Hongyang] Zhejiang Lab, Hangzhou 311121, Peoples R China.
C3 Hunan University; Hunan University; Zhejiang University; Huazhong
   University of Science & Technology; Zhejiang Laboratory
RP Xiao, Z; Jiang, HB (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.; Xia, BH (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM xiejiajia@hnu.edu.cn; ghz@zju.edu.cn; xbh_hust@hust.edu.cn;
   zhxiao@hnu.edu.cn; hongbojiang2004@gmail.com; swzhou@hnu.edu.cn;
   zqin@hnu.edu.cn; hongyang@zhejianglab.com
OI Xie, Jiajia/0000-0001-6133-1987; Xia, Beihao/0000-0001-6156-6946; Qin,
   Zheng/0000-0003-0877-3887
FU NSFC
FX No Statement Available
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Antonini G, 2006, TRANSPORT RES B-METH, V40, P667, DOI 10.1016/j.trb.2005.09.006
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Cunjun Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P507, DOI 10.1007/978-3-030-58610-2_30
   Feng PM, 2017, IEEE T MULTIMEDIA, V19, P725, DOI 10.1109/TMM.2016.2638206
   Gao X. Shi, 2022, CoRR, Vabs/2202.03954, P1, DOI [10.48550/arXiv.2202.03954[56]S., DOI 10.48550/ARXIV.2202.03954[56]S]
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Havyarimana V, 2020, IEEE T INTELL TRANSP, V21, P680, DOI 10.1109/TITS.2019.2891585
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Huang YF, 2019, IEEE I CONF COMP VIS, P6281, DOI 10.1109/ICCV.2019.00637
   Huang YR, 2020, IEEE T INTELL TRANSP, V21, P5036, DOI 10.1109/TITS.2019.2948188
   Hung JR, 2023, ACM T DES AUTOMAT EL, V28, DOI 10.1145/3524124
   Hung JR, 2021, ACM T RECONFIG TECHN, V14, DOI 10.1145/3470536
   Jiang HB, 2021, IEEE T NETW SCI ENG, V8, P53, DOI 10.1109/TNSE.2020.3025529
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kosaraju Vineet, 2019, Advances in Neural Information Processing Systems
   Leal-Taixé L, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130233
   Leal-Taixé L, 2014, PROC CVPR IEEE, P3542, DOI 10.1109/CVPR.2014.453
   Lei K, 2019, IEEE INFOCOM SER, P388, DOI [10.1109/INFOCOM.2019.8737631, 10.1109/infocom.2019.8737631]
   Li H, 2023, PROCEEDINGS OF THE 32ND USENIX SECURITY SYMPOSIUM, P1181
   Li H, 2020, IEEE SYST J, V14, P653, DOI 10.1109/JSYST.2019.2906120
   Li LH, 2022, PROC CVPR IEEE, P2221, DOI 10.1109/CVPR52688.2022.00227
   Li YK, 2018, IEEE T MULTIMEDIA, V20, P3289, DOI 10.1109/TMM.2018.2834873
   Li ZQ, 2023, IEEE T EM TOP COMP I, V7, P178, DOI 10.1109/TETCI.2022.3193373
   Liang RQ, 2021, AAAI CONF ARTIF INTE, V35, P2029
   Liu CX, 2022, WORLD WIDE WEB, V25, P2515, DOI 10.1007/s11280-021-00995-z
   Long WC, 2022, IEEE T VEH TECHNOL, V71, P4718, DOI 10.1109/TVT.2022.3151762
   Luber M, 2010, IEEE INT CONF ROBOT, P464, DOI 10.1109/ROBOT.2010.5509779
   Lucic M, 2018, ADV NEUR IN, V31
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Mirza M., 2014, ARXIV
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Mohamed A, 2022, LECT NOTES COMPUT SC, V13682, P463, DOI 10.1007/978-3-031-20047-2_27
   Monfort M, 2015, AAAI CONF ARTIF INTE, P3672
   Pang SM, 2022, IEEE T INTELL TRANSP, V23, P24609, DOI 10.1109/TITS.2022.3193442
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Pellegrini S, 2010, LECT NOTES COMPUT SC, V6311, P452, DOI 10.1007/978-3-642-15549-9_33
   Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144
   Saleh K, 2018, IEEE T INTELL VEHICL, V3, P414, DOI 10.1109/TIV.2018.2873901
   Tay MKC, 2008, SPRINGER TRAC ADV RO, V42, P381
   Vemula A, 2018, IEEE INT CONF ROBOT, P4601
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wong CH, 2022, LECT NOTES COMPUT SC, V13682, P682, DOI 10.1007/978-3-031-20047-2_39
   Xia BH, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108552
   Xiao JH, 2022, IEEE T INTELL TRANSP, V23, P7680, DOI 10.1109/TITS.2021.3071761
   Xiao Z, 2024, IEEE T MOBILE COMPUT, V23, P4918, DOI 10.1109/TMC.2023.3298643
   Xiao Z, 2023, IEEE J SEL AREA COMM, V41, P457, DOI 10.1109/JSAC.2022.3227027
   Xiao Z, 2022, IEEE T INTELL TRANSP, V23, P9680, DOI 10.1109/TITS.2021.3105550
   Xiao Z, 2023, IEEE T CYBERNETICS, V53, P2346, DOI 10.1109/TCYB.2021.3117705
   Xu YY, 2018, PROC CVPR IEEE, P5275, DOI 10.1109/CVPR.2018.00553
   Zhang KP, 2022, IEEE T INTELL TRANSP, V23, P22343, DOI 10.1109/TITS.2022.3164450
   Zheng J., 2022, CoRR, Vabs/2207.08630, P1, DOI [10.48550/arXiv.2207.086302, DOI 10.48550/ARXIV.2207.086302]
   Zhou SW, 2023, IEEE T MULTIMEDIA, V25, P2022, DOI 10.1109/TMM.2022.3142952
NR 55
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7503
EP 7515
DI 10.1109/TMM.2024.3368931
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000017
DA 2024-08-05
ER

PT J
AU Yuan, BW
   Sheng, YF
   Bao, BK
   Chen, YPP
   Xu, CS
AF Yuan, Bowen
   Sheng, Yefei
   Bao, Bing-Kun
   Chen, Yi-Ping Phoebe
   Xu, Changsheng
TI Semantic Distance Adversarial Learning for Text-to-Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Text-to-image synthesis; adversarial learning; cycle consistency
AB Text-to-Image (T2I) synthesis is a cross-modality task that requires a text description as input to generate a realistic and semantically consistent image. To guarantee semantic consistency, previous studies regenerate text descriptions from synthetic images and align them with the given descriptions. However, the existing redescription modules lack explicit modeling of their training objectives, which is crucial for reliable measurement of semantic distance between redescriptions and given text inputs. Consequently, the aligned text redescriptions suffer from training bias caused by the emergence of adversarial image samples, unseen semantics, and mistaken contents from low-quality synthesized images. To this end, we propose a SEMantic distance Adversarial learning (SEMA) framework for Text-to-Image synthesis which strengthens semantic consistency from two aspects: 1) We introduce adversarial learning between the image generator and the text redescription module to mutually promote or demote the quality of generated image or text instances. This learning model ensures accurate redescription of image contents, thus diminishing the generation of adversarial image samples. 2) We introduce two-fold semantic distance discrimination (SEM distance) to characterize semantic relevance between matching text or image pairs. The unseen semantics and mistaken contents will be penalized with a large SEM distance. The proposed discrimination method also simplifies the model training process with no need to optimize multiple discriminators. Experimental results on CUB Birds 200 and MS-COCO datasets show that the proposed model outperforms the state-of-the-art methods.
C1 [Yuan, Bowen; Sheng, Yefei; Bao, Bing-Kun] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
   [Bao, Bing-Kun; Xu, Changsheng] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Chen, Yi-Ping Phoebe] La Trobe Univ, Melbourne, Vic 3086, Australia.
   [Xu, Changsheng] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 101408, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Peng Cheng Laboratory;
   La Trobe University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of
   Automation, CAS
RP Bao, BK (corresponding author), Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
EM yuanbw0925@gmail.com; Ysheng990618@gmail.com; bingkunbao@njupt.edu.cn;
   phoebe.chen@latrobe.edu.au; csxu@nlpr.ia.ac.cn
RI Chen, Yi-Ping Phoebe/B-8844-2008
OI Chen, Yi-Ping Phoebe/0000-0002-4122-3767; Yuan,
   Bowen/0000-0002-8051-3070; Bao, Bingkun/0000-0001-5956-831X; Sheng,
   Yefei/0000-0002-2547-4646; xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Project
FX No Statement Available
CR Almahairi A, 2018, PR MACH LEARN RES, V80
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Brown T., 2020, P ADV NEUR INF PROC, P1877
   Chen HQ, 2021, INT J MACH LEARN CYB, V12, P3295, DOI 10.1007/s13042-020-01240-1
   Chen T., PROC INT C MACH LEAR, P1597
   Cheng F., 2020, CVPR, P10911
   Cheng J, 2022, IEEE T CIRC SYST VID, V32, P5187, DOI 10.1109/TCSVT.2021.3136857
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding GQ, 2022, PROC CVPR IEEE, P11184, DOI 10.1109/CVPR52688.2022.01091
   Ding M., 2021, Advances in Neural Information Processing Systems (NeurIPS-21), V34, P19822
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gu SY, 2022, PROC CVPR IEEE, P10686, DOI 10.1109/CVPR52688.2022.01043
   He R., 2015, inProc. Adv. Neural Inf.Process. Syst.
   Hensel M, 2017, ADV NEUR IN, V30
   Hinz T, 2022, IEEE T PATTERN ANAL, V44, P1552, DOI 10.1109/TPAMI.2020.3021209
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li W, 2022, Upainting: Unified text-to-image diffusion generation with cross-modal guidance
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Li X., 2019, Adv. Neural Inf. Process.Syst.
   Li Z, 2021, NEUROCOMPUTING, V464, P130, DOI 10.1016/j.neucom.2021.08.085
   Liao WT, 2022, PROC CVPR IEEE, P18166, DOI 10.1109/CVPR52688.2022.01765
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Pan B., PROC INT C MACH LEAR, P7476
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A., 2019, OpenAI blog, V1, P9
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical Text-Conditional Image Generation with CLIP Latents, V1, P3
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Ramesh P, 2022, 38 INT C MACHINE LEA
   Reed S. E., 2016, Proc. of Advances in Neural Information Processing Systems NIPS, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ruan SL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13940, DOI 10.1109/ICCV48922.2021.01370
   Saharia C., 2022, Photorealistic text-to-image diffusion models with deep language understanding
   Salimans T, 2016, ADV NEUR IN, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan HC, 2023, IEEE T NEUR NET LEAR, V34, P10309, DOI 10.1109/TNNLS.2022.3165573
   Tan HC, 2021, IEEE T IMAGE PROCESS, V30, P1275, DOI 10.1109/TIP.2020.3026728
   Tao M, 2022, PROC CVPR IEEE, P16494, DOI 10.1109/CVPR52688.2022.01602
   Tsue T., 2020, Cycle text-to-image GAN with BERT
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang M, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102761
   Wu CF, 2022, LECT NOTES COMPUT SC, V13676, P720, DOI 10.1007/978-3-031-19787-1_41
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang L, 2022, Diffusion Models: A Comprehensive Survey of Methods and Applications
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Ye H., 2021, 32 BRIT MACHINE VISI, P154
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang H, 2021, PROC CVPR IEEE, P833, DOI 10.1109/CVPR46437.2021.00089
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang Z., 2022, OptGAN: Optimizing and interpreting the latent space of the conditional text-to-image GANs
   Zhou YF, 2022, PROC CVPR IEEE, P17886, DOI 10.1109/CVPR52688.2022.01738
   Zhu B, 2020, PROC CVPR IEEE, P5518, DOI 10.1109/CVPR42600.2020.00556
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 61
TC 4
Z9 4
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1255
EP 1266
DI 10.1109/TMM.2023.3278992
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700032
DA 2024-08-05
ER

PT J
AU Guo, S
   Hu, JC
   Zhou, K
   Wang, JH
   Song, L
   Xie, R
   Zhang, WJ
AF Guo, Shuai
   Hu, Jingchuan
   Zhou, Kai
   Wang, Jionghao
   Song, Li
   Xie, Rong
   Zhang, Wenjun
TI Real-Time Free Viewpoint Video Synthesis System Based on DIBR and a
   Depth Estimation Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Real-time systems; Estimation; Three-dimensional displays;
   Rendering (computer graphics); Costs; Streaming media; Free viewpoint
   video (FVV); depth image-based rendering (DIBR); depth estimation;
   dataset
ID CONTENT CREATION; GENERATION; IMAGES
AB Depth image-based rendering (DIBR) view synthesis is the most widely employed method in real-time FVV research. Despite recent progress, most DIBR-based FVV synthesis approaches are not sufficiently simple and effective in filling holes and artifacts. Additionally, they use RGB-D cameras, which are difficult to widely adopt or take considerable time to estimate high-quality depth images. This article introduces a real-time FVV synthesis system based on DIBR and a depth estimation network. This system includes a 12-view synchronous camera system, a new multistage depth estimation network, a new GPU-accelerated DIBR algorithm, and a virtual view parameter generation method. This system provides the first real-time FVV solution for background-fixed fields based on DIBR and a depth estimation network. It can infer depth images for all camera views and synthesize any virtual view along the horizontal circular arc of the camera rig in real time. To our knowledge, we are the first to introduce background models and foreground masks and a refined multistage structure to address real-time high-quality depth estimation and DIBR FVV synthesis. We also build a high-quality multiview RGB-D synchronous dataset that has promising DIBR FVV synthesis performance to train and evaluate our system. The experimental results demonstrate the real-time and better performance of the proposed system.
C1 [Guo, Shuai; Hu, Jingchuan; Zhou, Kai; Wang, Jionghao; Xie, Rong; Zhang, Wenjun] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Song, Li] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Song, L (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM shuaiguo@sjtu.edu.cn; hujingchuan@sjtu.edu.cn; gyxxzk@gmail.com;
   shanemankiw@sjtu.edu.cn; song_li@sjtu.edu.cn; xierong@sjtu.edu.cn;
   zhangwenjun@sjtu.edu.cn
OI Song, Li/0000-0002-7124-5182; Guo, Shuai/0000-0001-9102-6545
FU National Key Ramp;D Project of China
FX No Statement Available
CR 4DReplay, 2022, About us
   Aanæs H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9
   Aich S, 2021, IEEE INT CONF ROBOT, P11746, DOI 10.1109/ICRA48506.2021.9560885
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Carballeira P, 2022, IEEE T MULTIMEDIA, V24, P2378, DOI 10.1109/TMM.2021.3079711
   Chen L, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115935
   Chen WY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1315
   Cho JH, 2017, IEEE SIGNAL PROC LET, V24, P329, DOI 10.1109/LSP.2017.2661319
   Do L, 2012, IEEE T CONSUM ELECTR, V58, P633, DOI 10.1109/TCE.2012.6227470
   Eigen D, 2014, ADV NEUR IN, V27
   Fang JM, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555383
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gao XS, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P275, DOI 10.1109/MIPR49039.2020.00064
   Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257
   Guo S, 2022, PROCEEDINGS OF THE 13TH ACM MULTIMEDIA SYSTEMS CONFERENCE, MMSYS 2022, P265, DOI 10.1145/3524273.3532897
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Hedman P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275084
   Horng YR, 2010, IEEE INT SYMP CIRC S, P2650, DOI 10.1109/ISCAS.2010.5537052
   Hu J., 2022, P IEEE INT C MULT EP, P1
   Huang BC, 2021, IEEE IMAGE PROC, P3163, DOI 10.1109/ICIP42928.2021.9506469
   Huang ZW, 2022, LECT NOTES COMPUT SC, V13674, P624, DOI 10.1007/978-3-031-19781-9_36
   Intel, 2022, True view
   Intel, 2022, Realsense
   Jantet V, 2011, 3D RES, V2, DOI 10.1007/3DRes.04(2011)4
   Kingma D. P., 2014, arXiv
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Knorr S., 2006, P IEEE 3 INT S 3D DA, P703
   Lai-Man Po, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2589, DOI 10.1109/ICIP.2011.6116194
   Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35
   Lamba M, 2021, PROC CVPR IEEE, P3486, DOI 10.1109/CVPR46437.2021.00349
   Lee CC, 2015, APSIPA TRANS SIGNAL, V4, DOI 10.1017/ATSIP.2015.18
   Lee J.H., 2019, arXiv
   Li QB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417785
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Li T., 2022, P IEEE CVF C COMP VI, P5521
   Li YZ, 2019, IEEE T CIRC SYST VID, V29, P1179, DOI 10.1109/TCSVT.2018.2825022
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Lin KE, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1729, DOI 10.1109/ICCV48922.2021.00177
   Lin SC, 2021, PROC CVPR IEEE, P8758, DOI 10.1109/CVPR46437.2021.00865
   Lin Zhi-Hao, 2022, P IEEE CVF C COMP VI, P15702
   Liu L., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Lombardi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323020
   Long XX, 2021, PROC CVPR IEEE, P8254, DOI 10.1109/CVPR46437.2021.00816
   Microsoft, 2022, Kinect azure
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Ming Y, 2021, NEUROCOMPUTING, V438, P14, DOI 10.1016/j.neucom.2020.12.089
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   O'Dwyer N, 2018, LECT NOTES COMPUT SC, V11318, P348, DOI 10.1007/978-3-030-04028-4_39
   Pagés R, 2018, J VIS COMMUN IMAGE R, V53, P192, DOI 10.1016/j.jvcir.2018.03.012
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Park K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480487
   Poznan University of Technology, 2015, ISO/IEC JTCI/SC29/WG11 MPEG2014/M32243
   Pumarola Albert, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P10313, DOI 10.1109/CVPR46437.2021.01018
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Rogge S, 2019, 2019 INTERNATIONAL CONFERENCE ON 3D IMMERSION (IC3D), DOI 10.1109/ic3d48390.2019.8975995
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Schöps T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272
   Sharma M., 2019, P IEEE INT C 3D IMM
   Slabaugh G.G., 1999, Computing Euler angles from a rotation matrix, P39
   Somraj N., 2020, Pose-Warping for View Synthesis
   Son J., 2020, P SIGGRAPH AS 2020 X, P1
   Song LC, 2023, IEEE T VIS COMPUT GR, V29, P2732, DOI 10.1109/TVCG.2023.3247082
   Stereolabs, 2022, Zed cameras
   Sun SH, 2018, LECT NOTES COMPUT SC, V11207, P162, DOI 10.1007/978-3-030-01219-9_10
   Tankovich V, 2021, PROC CVPR IEEE, P14357, DOI 10.1109/CVPR46437.2021.01413
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Wang FJ, 2021, PROC CVPR IEEE, P14189, DOI 10.1109/CVPR46437.2021.01397
   Wang P. S., 2011, Pattern Recognition, Machine Intelligence and Biometrics
   Wang Y., 2020, Virtual Reality Intell. Hardware, V2, P247
   Wolk D, 2019, IEEE INT CONF ROBOT, P6101, DOI [10.1109/ICRA.2019.8794182, 10.1109/icra.2019.8794182]
   Xian WQ, 2021, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR46437.2021.00930
   Xie Y., 2021, P IEEE INT C 3D IMM, P1
   Xu GW, 2022, PROC CVPR IEEE, P12971, DOI 10.1109/CVPR52688.2022.01264
   Yang JY, 2020, PROC CVPR IEEE, P4876, DOI 10.1109/CVPR42600.2020.00493
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yoon JS, 2020, PROC CVPR IEEE, P5335, DOI 10.1109/CVPR42600.2020.00538
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Yu BX, 2023, IEEE INT CONF ROBOT, P3116, DOI 10.1109/ICRA48891.2023.10161471
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zhang L, 2011, IEEE T BROADCAST, V57, P372, DOI 10.1109/TBC.2011.2122930
   Zhou K., 2022, P INT BROADC CONV
   Zhou K, 2022, IEEE I C VI COM I PR, DOI 10.1109/VCIP56404.2022.10008839
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
   Zinger S, 2010, J VIS COMMUN IMAGE R, V21, P533, DOI 10.1016/j.jvcir.2010.01.004
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 91
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6701
EP 6716
DI 10.1109/TMM.2024.3355639
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600015
DA 2024-08-05
ER

PT J
AU Köksal, A
   Ak, KE
   Sun, Y
   Rajan, D
   Lim, JH
AF Koksal, Ali
   Ak, Kenan E.
   Sun, Ying
   Rajan, Deepu
   Lim, Joo Hwee
TI Controllable Video Generation With Text-Based Instructions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Controllable video generation; video generation with textual
   instructions; motion generation; conditional generative models
AB Most of the existing studies on controllable video generation either transfer disentangled motion to an appearance without detailed control over motion or generate videos of simple actions such as the movement of arbitrary objects conditioned on a control signal from users. In this study, we introduce Controllable Video Generation with text-based Instructions (CVGI) framework that allows text-based control over action performed on a video. CVGI generates videos where hands interact with objects to perform the desired action by generating hand motions with detailed control through text-based instruction from users. By incorporating the motion estimation layer, we divide the task into two sub-tasks: (1) control signal estimation and (2) action generation. In control signal estimation, an encoder models actions as a set of simple motions by estimating low-level control signals for text-based instructions with given initial frames. In action generation, generative adversarial networks (GANs) generate realistic hand-based action videos as a combination of hand motions conditioned on the estimated low control level signal. Evaluations on several datasets (EPIC-Kitchens-55, BAIR robot pushing, and Atari Breakout) show the effectiveness of CVGI in generating realistic videos and in the control over actions.
C1 [Koksal, Ali; Ak, Kenan E.; Sun, Ying; Lim, Joo Hwee] Inst Infocomm Res, Dept Visual Intelligence, Singapore 138632, Singapore.
   [Koksal, Ali; Rajan, Deepu; Lim, Joo Hwee] Nanyang Technol Univ NTU, Sch Comp Sci & Engn SCSE, Jurong 639798, Singapore.
   [Rajan, Deepu; Lim, Joo Hwee] Ctr Frontier AI Res, ASTAR, Singapore 117602, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Nanyang Technological University; Agency
   for Science Technology & Research (A*STAR)
RP Köksal, A (corresponding author), Inst Infocomm Res, Dept Visual Intelligence, Singapore 138632, Singapore.; Köksal, A (corresponding author), Nanyang Technol Univ NTU, Sch Comp Sci & Engn SCSE, Jurong 639798, Singapore.
EM ali013@e.ntu.edu.sg; kenan_emir_ak@i2r.a-star.edu.sg;
   suny@i2r.a-star.edu.sg; asdrajan@ntu.edu.sg; joohwee@i2r.a-star.edu.sg
RI Koksal, Ali/KFF-5608-2024
OI Koksal, Ali/0000-0001-8966-592X; Sun, Ying/0000-0002-7224-6726; LIM, Joo
   Hwee/0000-0002-4103-3824
CR Ardino Pierfrancesco, 2021, P IEEECVF INT C COMP, P14749
   Babaeizadeh M., 2021, arXiv
   Babaeizadeh M, 2018, Arxiv, DOI arXiv:1710.11252
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Clark A, 2019, Arxiv, DOI arXiv:1907.06571
   Cui RP, 2020, IEEE T MULTIMEDIA, V22, P2551, DOI 10.1109/TMM.2019.2960700
   Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44
   Denton Emily, 2018, P MACHINE LEARNING R, V80
   Ebert Frederik, 2017, ARXIV171005268, P344, DOI DOI 10.48550/ARXIV.1710.05268
   Esser P., 2018, P EUR C COMP VIS WOR, P1
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Finn C., 2016, Adv. Neural Inf. Process.Syst., V29, P1
   Franceschi J.-Y., 2020, PMLR, P3233
   Gafni O, 2019, Arxiv, DOI arXiv:1904.08379
   Hao ZK, 2018, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2018.00819
   Hensel M, 2017, ADV NEUR IN, V30
   Ho JAT, 2022, Arxiv, DOI [arXiv:2204.03458, DOI 10.48550/ARXIV.2204.03458,ARXIV]
   Hong W., 2022, arXiv
   Hoppe T, 2022, Arxiv, DOI arXiv:2206.07696
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Kim SW, 2020, PROC CVPR IEEE, P1228, DOI 10.1109/CVPR42600.2020.00131
   Kingma D. P., 2014, arXiv
   Koksal Ali, 2020, P AS C COMP VIS
   Kwon YH, 2019, PROC CVPR IEEE, P1811, DOI 10.1109/CVPR.2019.00191
   Le Moing G., 2021, ADV NEURAL INFORM PR, V34, P14042
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Li B, 2021, J NEUROL, V268, P2042, DOI 10.1007/s00415-019-09596-3
   Li Y, 2018, LECT NOTES COMPUT SC, V11209, P639, DOI 10.1007/978-3-030-01228-1_38
   Lin JX, 2021, IEEE T PATTERN ANAL, V43, P1254, DOI 10.1109/TPAMI.2019.2950198
   Luc P, 2021, Arxiv, DOI arXiv:2003.04035
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Marwah T, 2017, IEEE I CONF COMP VIS, P1435, DOI 10.1109/ICCV.2017.159
   Menapace W, 2021, PROC CVPR IEEE, P10056, DOI 10.1109/CVPR46437.2021.00993
   Nash C, 2022, Arxiv, DOI arXiv:2203.09494
   Nawhal Megha, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P382, DOI 10.1007/978-3-030-58610-2_23
   Oh J., 2015, PROC ADV NEURAL I, P2863
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Rakhimov R, 2020, Arxiv, DOI arXiv:2006.10704
   Saito M, 2020, INT J COMPUT VISION, V128, P2586, DOI 10.1007/s11263-020-01333-y
   Salimans T, 2016, ADV NEUR IN, V29
   Seo Y, 2022, IEEE IMAGE PROC, P3943, DOI 10.1109/ICIP46576.2022.9897982
   Siarohin A, 2019, ADV NEUR IN, V32
   Siarohin A, 2019, PROC CVPR IEEE, P2372, DOI 10.1109/CVPR.2019.00248
   Singer U, 2022, Arxiv, DOI arXiv:2209.14792
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Unterthiner T, 2019, Arxiv, DOI [arXiv:1812.01717, 10.48550/arXiv.1812.01717]
   Villegas R., 2017, PMLR, P3560
   Villegas R., 2022, arXiv
   Voleti V, 2022, Arxiv, DOI arXiv:2205.09853
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Wang W, 2020, IEEE T MULTIMEDIA, V22, P2808, DOI 10.1109/TMM.2019.2963621
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weissenborn D, 2020, Arxiv, DOI arXiv:1906.02634
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Wu CF, 2022, LECT NOTES COMPUT SC, V13676, P720, DOI 10.1007/978-3-031-19787-1_41
   Lee AX, 2018, Arxiv, DOI arXiv:1804.01523
   Yan W., 2021, arXiv, DOI DOI 10.48550/ARXIV.2104.10157
   Yan YC, 2019, IEEE T MULTIMEDIA, V21, P1799, DOI 10.1109/TMM.2018.2885235
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu B, 2020, PROC CVPR IEEE, P5518, DOI 10.1109/CVPR42600.2020.00556
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 64
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 190
EP 201
DI 10.1109/TMM.2023.3262972
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500017
DA 2024-08-05
ER

PT J
AU Lin, KZ
   Wang, XH
   Zhu, LC
   Zhang, B
   Yang, Y
AF Lin, Kezhou
   Wang, Xiaohan
   Zhu, Linchao
   Zhang, Bang
   Yang, Yi
TI SKIM: Skeleton-Based Isolated Sign Language Recognition With Part Mixing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sign language; Face recognition; Biological system modeling; Manuals;
   Benchmark testing; Assistive technologies; Data augmentation; sign
   language recognition; skeleton
ID MODEL
AB In this article, we present skeleton-based isolated sign language recognition (IsoSLR) with part mixing - SKIM. An IsoSLR model that solely takes the skeleton representation of the human body as input. Previous skeleton-based works either perform worse when compared to RGB-based counterparts or require fusion with other modalities to obtain competitive results. With SKIM, a single skeleton-based model without complex pre-training can obtain similar or even higher accuracy than current state-of-the-art methods. This margin can be further increased by simple late fusion within the same modality. To achieve this, we first develop a novel data augmentation technique called part mixing. It swaps the corresponding keypoints within one region (e.g. hand) between two randomly selected samples and combines their labels linearly as the new label. As regions like hand and face are key articulators for sign language, direct swapping of such parts creates a believable pseudo sign that promotes the model to recognize the true pairs. Secondly, following current advances in skeleton-based action recognition, we devise a channel-wise graph neural network with multi-scale awareness and per-keypoint temporal re-weighting. With this design, the backbone is capable of leveraging both manual and non-manual features. The combination of hand mixing and the channel-wise multi-scale GCN backbone allows us to achieve state-of-the-art accuracy on both WLASL and NMFs-CSL benchmarks.
C1 [Lin, Kezhou; Zhu, Linchao; Yang, Yi] Zhejiang Univ, Hangzhou 310027, Peoples R China.
   [Wang, Xiaohan] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Zhang, Bang] Alibaba Grp, DAMO Acad, Hangzhou 311121, Peoples R China.
C3 Zhejiang University; Zhejiang University; Alibaba Group
RP Yang, Y (corresponding author), Zhejiang Univ, Hangzhou 310027, Peoples R China.
EM kezhoulin@zju.edu.cn; wxh1996111@gmail.com; zhulinchao7@gmail.com;
   bangzhang@gmail.com; yangyics@zju.edu.cn
RI Wang, Xiaohan/JKI-4414-2023
OI Wang, Xiaohan/0000-0001-5273-4223
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR Albanie Samuel, 2020, ECCV, P35, DOI DOI 10.1007/978-3-030-58621-8_3
   Avola D, 2020, IEEE T MULTIMEDIA, V22, P2481, DOI 10.1109/TMM.2019.2960588
   Bohácek M, 2022, IEEE WINT CONF APPL, P182, DOI 10.1109/WACVW54805.2022.00024
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Contributors M., 2020, Open-MMlab pose estimation toolbox and benchmark
   Cooper H, 2012, J MACH LEARN RES, V13, P2205
   Defferrard M, 2016, ADV NEUR IN, V29
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Duan HD, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P7351, DOI 10.1145/3503161.3548546
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Han XT, 2022, PR MACH LEARN RES
   Hosain A, 2021, IEEE WINT CONF APPL, P3428, DOI 10.1109/WACV48630.2021.00347
   Hu HZ, 2023, IEEE T PATTERN ANAL, V45, P11221, DOI 10.1109/TPAMI.2023.3269220
   Hu HZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11067, DOI 10.1109/ICCV48922.2021.01090
   Hu HZ, 2021, AAAI CONF ARTIF INTE, V35, P1558
   Hu HZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436754
   Huang J, 2019, IEEE T CIRC SYST VID, V29, P2822, DOI 10.1109/TCSVT.2018.2870740
   Jaejun Yoo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8372, DOI 10.1109/CVPR42600.2020.00840
   Jiang S, 2021, arXiv
   Jiang SY, 2021, IEEE COMPUT SOC CONF, P3408, DOI 10.1109/CVPRW53098.2021.00380
   Joze H. V., 2019, P BIRT MACH VIS C
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Li DX, 2020, PROC CVPR IEEE, P6204, DOI 10.1109/CVPR42600.2020.00624
   Li DX, 2020, IEEE WINT CONF APPL, P1448, DOI [10.1109/wacv45572.2020.9093512, 10.1109/WACV45572.2020.9093512]
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin KZ, 2023, PROCEEDINGS OF THE 61ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2023): LONG PAPERS, VOL 1, P12904
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Paszke A, 2019, ADV NEUR IN, V32
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Tunga A, 2021, IEEE WINT CONF APPL, P31, DOI 10.1109/WACVW52041.2021.00008
   Vaswani A, 2017, ADV NEUR IN, V30
   Verma V, 2019, PR MACH LEARN RES, V97
   Yasir F, 2015, 2015 IEEE 8TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND APPLICATIONS (IWCIA) PROCEEDINGS, P35, DOI 10.1109/IWCIA.2015.7449458
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARNING REPRE
   Zhou H, 2022, IEEE T MULTIMEDIA, V24, P768, DOI 10.1109/TMM.2021.3059098
NR 46
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4271
EP 4280
DI 10.1109/TMM.2023.3321502
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LB3A1
UT WOS:001184265900001
DA 2024-08-05
ER

PT J
AU Lin, SQ
   Yu, T
   Feng, RY
   Li, X
   Yu, XY
   Xiao, L
   Chen, ZB
AF Lin, Shiqi
   Yu, Tao
   Feng, Ruoyu
   Li, Xin
   Yu, Xiaoyuan
   Xiao, Lei
   Chen, Zhibo
TI Local Patch AutoAugment With Multi-Agent Collaboration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic augmentation; data augmentation; multi-agent reinforcement
   learning; reinforcement learning
AB Data augmentation (DA) plays a critical role in improving the generalization of deep learning models. Recent works on automatically searching for DA policies from data have achieved great success. However, existing automated DA methods generally perform the search at the image level, which limits the exploration of diversity in local regions. In this paper, we propose a more fine-grained automated DA approach, dubbed Patch AutoAugment, to divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. We formulate it as a multi-agent reinforcement learning (MARL) problem, where each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. We show the effectiveness of our method on multiple benchmark datasets of image classification, fine-grained image recognition and object detection (e.g., CIFAR-10, CIFAR-100, ImageNet, CUB-200-2011, Stanford Cars, FGVC-Aircraft and Pascal VOC 2007). Extensive experiments demonstrate that our method outperforms the state-of-the-art DA methods while requiring fewer computational resources.
C1 [Lin, Shiqi; Yu, Tao; Feng, Ruoyu; Li, Xin; Chen, Zhibo] Univ Sci & Technol China, Dept Elect Engineer & Informat Sci, Hefei 230026, Peoples R China.
   [Yu, Xiaoyuan; Xiao, Lei] Huawei Cloud Technol & Comp Co Ltd, Hangzhou 310051, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, ZB (corresponding author), Univ Sci & Technol China, Dept Elect Engineer & Informat Sci, Hefei 230026, Peoples R China.
EM linsq047@mail.ustc.edu.cn; yutao666@mail.ustc.edu.cn;
   ustcfry@mail.ustc.edu.cn; lixin666@mail.ustc.edu.cn;
   yuxiaoyuan@huawei.com; xiaolei34@huawei.com; chenzhibo@ustc.edu.cn
RI Li, Xin/ABQ-3970-2022; feng, ruoyu/KVB-6685-2024
OI Li, Xin/0000-0002-6352-6523
FU National Natural Science Foundation of China
FX No Statement Available
CR Arulkumaran K, 2017, IEEE SIGNAL PROC MAG, V34, P26, DOI 10.1109/MSP.2017.2743240
   Bansal T., 2017, P INT C LEARN REPR
   Boutilier C, 1996, THEORETICAL ASPECTS OF RATIONALITY AND KNOWLEDGE, P195
   Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919
   Chen B, 2023, IEEE T MULTIMEDIA, V25, P1345, DOI 10.1109/TMM.2022.3141616
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Chu TS, 2016, IEEE DECIS CONTR P, P7592, DOI 10.1109/CDC.2016.7799442
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Dabouei A, 2021, PROC CVPR IEEE, P13789, DOI 10.1109/CVPR46437.2021.01358
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Florensa C., 2018, arXiv
   Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974
   Furuta R, 2020, IEEE T MULTIMEDIA, V22, P1704, DOI 10.1109/TMM.2019.2960636
   Gong CY, 2021, PROC CVPR IEEE, P1055, DOI 10.1109/CVPR46437.2021.00111
   Gontijo-Lopes R, 2020, Arxiv, DOI arXiv:2002.08973
   Han D, 2017, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR.2017.668
   Han L, 2019, PR MACH LEARN RES, V97
   Hataya R., 2020, P COMP VIS ECCV 2020, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI [arXiv:1912.02781, DOI 10.48550/ARXIV:1912.02781]
   Ho D, 2019, PR MACH LEARN RES, V97
   Hong S, 2019, IEEE INT CONF COMP V, P127, DOI 10.1109/ICCVW.2019.00021
   Huang SL, 2021, AAAI CONF ARTIF INTE, V35, P1628
   Ke X, 2019, IEEE T MULTIMEDIA, V21, P2093, DOI 10.1109/TMM.2019.2895511
   Kim J., 2021, P INT C LEARN REPR
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li YG, 2020, Arxiv, DOI arXiv:2003.03780
   Liao Xuan, 2020, P IEEE CVF C COMP VI, P9394, DOI DOI 10.1109/CVPR42600.2020.00941
   Lim S., 2019, P INT C NEUR INF PRO, V32, P6665
   Lin C, 2019, IEEE I CONF COMP VIS, P6578, DOI 10.1109/ICCV.2019.00668
   Liu A, 2021, P IEEE CVF INT C COM, P12219
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma CF, 2021, IEEE T MED IMAGING, V40, P2563, DOI 10.1109/TMI.2020.3048477
   Ma J., 2008, Robot Soccer World Cup, P532
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Mnih V, 2016, PR MACH LEARN RES, V48
   Müller SG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P754, DOI 10.1109/ICCV48922.2021.00081
   Foerster JN, 2018, Arxiv, DOI arXiv:1709.04326
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1021, DOI 10.1109/TMM.2020.2991532
   Papoudakis G., 2020, P 35 C NEUR INF PROC
   Rashid T, 2018, PR MACH LEARN RES, V80
   Ren S., 2015, Advances in neural information processing systems, P91
   Riba E, 2020, IEEE WINT CONF APPL, P3663, DOI 10.1109/WACV45572.2020.9093363
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559
   Singla S, 2021, PROC CVPR IEEE, P12848, DOI 10.1109/CVPR46437.2021.01266
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395
   Tran T, 2017, ADV NEUR IN, V30
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Uddin A. S., 2020, P INT C LEARN REPR, P1
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Q, 2022, IEEE T MULTIMEDIA, V24, P1031, DOI 10.1109/TMM.2021.3104141
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yuan HJ, 2023, IEEE T MULTIMEDIA, V25, P203, DOI 10.1109/TMM.2021.3124083
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zagoruyko S., 2016, Wide residual networks, DOI DOI 10.5244/C.30.87
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang JS, 2022, IEEE T MULTIMEDIA, V24, P4538, DOI 10.1109/TMM.2021.3119994
   Zhang Xinyi, 2019, INT C LEARN REPR
   Zhang Z. Yang, 2021, HDB REINFORCEMENT LE, P321, DOI [DOI 10.1007/978-3-030-60990-012, DOI 10.1007/978-3-030-60990-0_12]
   Zhao YY, 2022, IEEE T MULTIMEDIA, V24, P2150, DOI 10.1109/TMM.2021.3076612
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2438546
NR 73
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 724
EP 736
DI 10.1109/TMM.2023.3270635
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000014
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, DZ
   Hu, W
   Li, X
AF Liu, Daizong
   Hu, Wei
   Li, Xin
TI Robust Geometry-Dependent Attack for 3D Point Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE geometry-dependent attack; point cloud processing; Disentanglement
AB Deep learning models for point clouds have shown to be vulnerable to adversarial attacks, which have received increasing attention in various safety-critical applications such as autonomous driving, robotics, and surveillance. Since existing 3D attack methods either modify the local points or perform global point-wise perturbations over the point cloud, they fail to capture the dependency between neighboring points for preserving the geometrical context and topological smoothness of the original 3D object. In this article, we propose a novel Geometry-Dependent Attack (GDA), which aims to generate more robust adversarial point clouds with lower perturbation costs by capturing and preserving the geometry-guided topology information. Specifically, we first analyze the geometric information of each benign point cloud following the graph signal processing and disentangle it into low-frequency (flat) and high-frequency (contour) components. Then, considering the varying characteristics of smoothness and sharpness after disentanglement, we design two collaborative patch-aware and point-aware attacks to perturb these two components separately to misclassify the 3D object. We test the proposed GDA attack using five popular point cloud networks (PointNet, PointNet++, DGCNN, PointTransformer, and PointMLP) on both ModelNet40 and ShapNetPart datasets. Experimental results show that our GDA attack achieves 100% success rates with the lowest perturbation cost. It also demonstrates the increased capability to defeat several existing defense models over other competing attacks.
C1 [Liu, Daizong; Hu, Wei] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
   [Li, Xin] Univ Albany, Dept Comp Sci, Albany, NY 12222 USA.
C3 Peking University; State University of New York (SUNY) System; State
   University of New York (SUNY) Albany
RP Hu, W (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
EM dzliu@stu.pku.edu.cn; forhuwei@pku.edu.cn; xli48@albany.edu
OI Li, Xin/0000-0003-2067-2763; liu, daizong/0000-0001-8179-4508
FU National Natural Science Foundation of China
FX No Statement Available
CR Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen SH, 2018, IEEE T SIGNAL PROCES, V66, P666, DOI 10.1109/TSP.2017.2771730
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Cortes C., 2012, ARXIV
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3507, DOI 10.1109/TIP.2017.2699922
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Duan YQ, 2019, PROC CVPR IEEE, P949, DOI 10.1109/CVPR.2019.00104
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fujihashi T, 2022, IEEE T MULTIMEDIA, V24, P2179, DOI 10.1109/TMM.2021.3077772
   Gao X, 2020, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR42600.2020.00719
   Hamdi Abdullah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P241, DOI 10.1007/978-3-030-58610-2_15
   Hang Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10353, DOI 10.1109/CVPR42600.2020.01037
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hu W, 2022, IEEE T MULTIMEDIA, V24, P3961, DOI 10.1109/TMM.2021.3111440
   Huang QD, 2022, PROC CVPR IEEE, P15314, DOI 10.1109/CVPR52688.2022.01490
   Hui L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5490, DOI 10.1109/ICCV48922.2021.00546
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Kurakin A., 2017, P INT C LEARN REPR
   Lee KB, 2020, Arxiv, DOI arXiv:2005.11626
   Li XK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16472, DOI 10.1109/ICCV48922.2021.01618
   Liu DZ, 2023, Arxiv, DOI arXiv:2207.13326
   Liu DZ, 2023, IEEE T PATTERN ANAL, V45, P4727, DOI 10.1109/TPAMI.2022.3193449
   Liu D, 2019, IEEE IMAGE PROC, P2279, DOI 10.1109/icip.2019.8803770
   Liu HB, 2021, PROC CVPR IEEE, P6182, DOI 10.1109/CVPR46437.2021.00612
   Liu YC, 2019, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2019.00534
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Ma CC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1819, DOI 10.1145/3394171.3413875
   Ma X., 2022, PROC INT C LEARN REP, P1
   Madry A, 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.06083
   Mao Zhuoqing Morley, 2022, arXiv
   Ortega A, 2018, P IEEE, V106, P808, DOI 10.1109/JPROC.2018.2820126
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Sandryhaila A, 2014, IEEE T SIGNAL PROCES, V62, P3042, DOI 10.1109/TSP.2014.2321121
   Shao YT, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Singh SP, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185097
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Szegedy C., 2014, PROC 2 INT C LEARN R
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tsai T, 2020, AAAI CONF ARTIF INTE, V34, P954
   Tu CC, 2019, AAAI CONF ARTIF INTE, P742
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen YX, 2022, IEEE T PATTERN ANAL, V44, P2984, DOI 10.1109/TPAMI.2020.3044712
   Weng TY, 2023, IEEE T MULTIMEDIA, V25, P6653, DOI 10.1109/TMM.2022.3212914
   Wicker M, 2019, PROC CVPR IEEE, P11759, DOI 10.1109/CVPR.2019.01204
   Wu Z., 2020, arXiv
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang C, 2019, PROC CVPR IEEE, P9128, DOI 10.1109/CVPR.2019.00935
   Xiang Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7577, DOI 10.1109/ICCV48922.2021.00750
   Xiaoyi Dong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11513, DOI 10.1109/CVPR42600.2020.01153
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   YANG B, 2019, P 33 INT C NEUR INF, P605
   Yang JC, 2021, Arxiv, DOI arXiv:1902.10899
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Zaheer M, 2017, ADV NEUR IN, V30
   Zhang Y, 2019, IEEE INT CONF BIG DA, P5654, DOI [10.1109/BigData47090.2019.9006307, 10.1109/bigdata47090.2019.9006307]
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao Y, 2020, PROC CVPR IEEE, P1198, DOI 10.1109/CVPR42600.2020.00128
   Zheng TH, 2019, IEEE I CONF COMP VIS, P1598, DOI 10.1109/ICCV.2019.00168
   Zhou H, 2019, IEEE I CONF COMP VIS, P1961, DOI 10.1109/ICCV.2019.00205
   Zhu H., 2022, IEEE T MULTIMEDIA EA, DOI [10.1109/IMM2022.3189778, DOI 10.1109/IMM2022.3189778]
NR 74
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2866
EP 2877
DI 10.1109/TMM.2023.3304896
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400012
DA 2024-08-05
ER

PT J
AU Ou, FZ
   Chen, XY
   Zhao, K
   Wang, SQ
   Wang, YG
   Kwong, S
AF Ou, Fu-Zhao
   Chen, Xingyu
   Zhao, Kai
   Wang, Shiqi
   Wang, Yuan-Gen
   Kwong, Sam
TI Refining Uncertain Features With Self-Distillation for Face Recognition
   and Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature refinement; face recognition; person re-identification;
   recognition optimization; quality self-distillation
ID IMAGE QUALITY ASSESSMENT; NETWORK; GAN
AB Deep recognition models aim to recognize targets with various quality levels in uncontrolled application circumstances, and typically low-quality images usually retard the recognition performance dramatically. As such, a straightforward solution is to restore low-quality input images as pre-processing during deployment. However, this scheme cannot guarantee that deep recognition features of the processed images are conducive to recognition accuracy. How deep recognition features of low-quality images can be refined during training to optimize recognition models has largely escaped research attention in the field of metric learning. In this paper, we propose a quality-aware feature refinement framework based on the dedicated quality priors obtained according to the recognition performance, and a novel quality self-distillation algorithm to learn recognition models. We further show that the proposed scheme can significantly boost the performance of the recognition model with two popular deep recognition tasks, including face recognition and person re-identification. Extensive experimental results provide sufficient evidence on the effectiveness and impressive generalization capability of the proposed framework. Moreover, our framework can be essentially integrated with existing state-of-the-art classification loss functions and network architectures, without extra computation costs during deployment.
C1 [Ou, Fu-Zhao; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Chen, Xingyu] TikTok Ecommerce, Shanghai 200233, Peoples R China.
   [Zhao, Kai] Univ Calif Los Angeles, Dept Radiol Sci, Los Angeles, CA 90095 USA.
   [Wang, Yuan-Gen] Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
   [Kwong, Sam] Lingnan Univ, Dept Comp & Decis Sci, Hong Kong 999077, Peoples R China.
C3 City University of Hong Kong; University of California System;
   University of California Los Angeles; Guangzhou University; Lingnan
   University
RP Wang, SQ (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.; Kwong, S (corresponding author), Lingnan Univ, Dept Comp & Decis Sci, Hong Kong 999077, Peoples R China.
EM fuzhao.ou@my.cityu.edu.hk; clinene0322@gmail.com; kz@kaizhao.net;
   shiqwang@cityu.edu.hk; wangyg@gzhu.edu.cn; samkwong@ln.edu.hk
RI ZHAO, KAI/ABR-6426-2022; Kwong, Sam/C-9319-2012
OI ZHAO, KAI/0000-0002-2496-0829; Kwong, Sam/0000-0001-7484-7261; Wang,
   Yuan-Gen/0000-0003-3010-4196; Ou, Fu-Zhao/0000-0003-1245-8345
FU Hong Kong Innovation and Technology Commission
FX No Statement Available
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   An X, 2021, IEEE INT CONF COMP V, P1445, DOI 10.1109/ICCVW54120.2021.00166
   [Anonymous], 2016, P 24 ACM INT C MULT, DOI 10.1145/2964284.2967209
   [Anonymous], 2016, ISO/IEC 29794-1
   Bai ZY, 2021, IEEE INT CONF AUTOMA
   Best-Rowden L, 2018, IEEE T INF FOREN SEC, V13, P3064, DOI 10.1109/TIFS.2018.2799585
   Cao QX, 2017, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2017.180
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chen E. J., 2021, IEEE 23 INT WORKSHOP, P1
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Choi JY, 2012, IEEE T IMAGE PROCESS, V21, P1366, DOI 10.1109/TIP.2011.2168413
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741
   Deng Y, 2020, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR42600.2020.00520
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Frogner C, 2015, ADV NEUR IN, V28
   Ou FZ, 2021, PROC CVPR IEEE, P7666, DOI 10.1109/CVPR46437.2021.00758
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P1505, DOI 10.1109/TMM.2023.3240880
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hernandez-Ortega J, 2021, Arxiv, DOI arXiv:2006.03298
   Hernandez-Ortega J, 2019, INT CONF BIOMETR
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Hou YN, 2019, IEEE I CONF COMP VIS, P1013, DOI 10.1109/ICCV.2019.00110
   Hu WP, 2020, IEEE T MULTIMEDIA, V22, P1234, DOI 10.1109/TMM.2019.2938685
   Huang G. B., 2008, WORKSHOP FACESREAL L, P1
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   Jiang K, 2020, IEEE T MULTIMEDIA, V22, P2734, DOI 10.1109/TMM.2019.2960586
   Karanam S, 2015, IEEE I CONF COMP VIS, P4516, DOI 10.1109/ICCV.2015.513
   Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527
   Kim M, 2022, PROC CVPR IEEE, P18729, DOI 10.1109/CVPR52688.2022.01819
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li S, 2021, PROC CVPR IEEE, P15624, DOI 10.1109/CVPR46437.2021.01537
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li X, 2015, IEEE I CONF COMP VIS, P3765, DOI 10.1109/ICCV.2015.429
   Li XM, 2018, LECT NOTES COMPUT SC, V11217, P278, DOI 10.1007/978-3-030-01261-8_17
   Liu F, 2022, LECT NOTES COMPUT SC, V13672, P701, DOI 10.1007/978-3-031-19775-8_41
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCV.2017.478, 10.1109/ICCVW.2017.361]
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Meng Q, 2021, PROC CVPR IEEE, P14220, DOI 10.1109/CVPR46437.2021.01400
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Olsen MA, 2016, IET BIOMETRICS, V5, P47, DOI 10.1049/iet-bmt.2014.0055
   Ou FZ, 2022, IEEE T MULTIMEDIA, V24, P4197, DOI 10.1109/TMM.2021.3114551
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Romero A., 2015, ICLR, P1
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schlett T, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3507901
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sengupta S, 2016, IEEE WINT CONF APPL
   Shi JG, 2019, IEEE T MULTIMEDIA, V21, P2223, DOI 10.1109/TMM.2019.2898752
   Shi YC, 2021, IEEE COMPUT SOC CONF, P2789, DOI 10.1109/CVPRW53098.2021.00314
   Shi YC, 2019, IEEE I CONF COMP VIS, P6901, DOI 10.1109/ICCV.2019.00700
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Song GL, 2018, AAAI CONF ARTIF INTE, P7347
   Sukmin Yun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13873, DOI 10.1109/CVPR42600.2020.01389
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan XY, 2007, LECT NOTES COMPUT SC, V4778, P168
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Terhorst P, 2020, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR42600.2020.00569
   Tianyue D., 2018, Tech. Rep. 1801, P74
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaserstein L. N., 1969, PROBL PEREDACHI INF, V5, P64
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45
   Wang XB, 2020, AAAI CONF ARTIF INTE, V34, P12241
   Wang XB, 2019, IEEE I CONF COMP VIS, P9357, DOI 10.1109/ICCV.2019.00945
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wang ZX, 2022, PROC CVPR IEEE, P17491, DOI 10.1109/CVPR52688.2022.01699
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Xiao JM, 2019, PATTERN RECOGN, V87, P332, DOI 10.1016/j.patcog.2018.10.028
   Xie W., 2020, PROC BRIT MACH VIS C, P1
   Yang CL, 2019, PROC CVPR IEEE, P2854, DOI 10.1109/CVPR.2019.00297
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yu L, 2019, PROC CVPR IEEE, P2902, DOI 10.1109/CVPR.2019.00302
   Yu TY, 2019, IEEE I CONF COMP VIS, P552, DOI 10.1109/ICCV.2019.00064
   Yuan L, 2020, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR42600.2020.00396
   Yuge Huang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P138, DOI 10.1007/978-3-030-58577-8_9
   Zeng H, 2018, IEEE IMAGE PROC, P609, DOI 10.1109/ICIP.2018.8451285
   Zhang LF, 2019, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2019.00381
   Zhang N, 2023, IEEE T MULTIMEDIA, V25, P3217, DOI 10.1109/TMM.2022.3157036
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao K, 2019, PROC CVPR IEEE, P1136, DOI 10.1109/CVPR.2019.00123
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng ZD, 2023, Arxiv, DOI arXiv:2204.13096
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou DW, 2021, IEEE T MULTIMEDIA, V24, P3469, DOI 10.1109/TMM.2021.3099297
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou X, 2023, PROC CVPR IEEE, P19691, DOI 10.1109/CVPR52729.2023.01886
NR 116
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6981
EP 6995
DI 10.1109/TMM.2024.3358697
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000058
DA 2024-08-05
ER

PT J
AU Shi, JY
   Zhong, JQ
   Cao, WM
AF Shi, Junyu
   Zhong, Jianqi
   Cao, Wenming
TI Multi-Semantics Aggregation Network Based on the Dynamic-Attention
   Mechanism for 3D Human Motion Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion prediction; graph convolution network; dynamic attention;
   skeleton-based data processing
AB Graph convolutional network-based methods have recently shown promising performance in skeleton-based data processing. However, these methods have two critical issues in skeleton-based motion prediction tasks: First, graph modeling of motion poses is based on the fixed graph according to the physical connection of human joints and ignores the exploration of deep implicit information based on human dynamic kinetics. Second, existing methods usually use motion information in a single semantic space to model the whole motion sequences, underestimating diverse semantic patterns for improving the modeling ability. To address the first issue, we propose the Attention-based Dynamic Graph Convolution method, which tries to capture implicit semantic information dynamically. To address the second issue, we propose the Kinematic-based Semantics Aggregation Block (KSAB), which combines various semantic features from four semantic perspectives to rich motion representation. Integrating the above two designs, we propose a novel Multi-Semantics Aggregation Network (MANet), resulting in more comprehensive feature extraction in dynamic implicit semantics learning to enhance motion prediction. Extensive experiments are conducted to validate the effectiveness of MANet, which outperforms state-of-the-art methods by 10.9%, 6.6%, and 19.6% in terms of MPJPE for motion prediction on Human3.6M, CMU Mocap, and 3DPW datasets, respectively.
C1 [Shi, Junyu; Zhong, Jianqi; Cao, Wenming] State Key Lab Radio Frequency Heterogeneous Integr, Shenzhen 518060, Guangdong, Peoples R China.
RP Cao, WM (corresponding author), State Key Lab Radio Frequency Heterogeneous Integr, Shenzhen 518060, Guangdong, Peoples R China.
EM 2020112101@email.szu.edu.cn; zhongjianqi2017@email.szu.edu.cn;
   wmcao@szu.edu.cn
OI Shi, Junyu/0009-0009-0978-9670; cao, wenming/0000-0002-8174-6167; zhong,
   jianqi/0000-0001-7939-1494
FU National Natural Science Foundation of China
FX No Statement Available
CR Aksan E, 2021, INT CONF 3D VISION, P565, DOI 10.1109/3DV53792.2021.00066
   Bouazizi A., 2022, INT JOINT C ART INT, P791
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Bütepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173
   Chen M., 2020, INT C MACHINE LEARNI, P1725, DOI DOI 10.5555/3524938.3525099
   Corona E, 2020, PROC CVPR IEEE, P6990, DOI 10.1109/CVPR42600.2020.00702
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Dang LW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11447, DOI 10.1109/ICCV48922.2021.01127
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Gao XH, 2023, IEEE T MULTIMEDIA, V25, P405, DOI 10.1109/TMM.2021.3127040
   Ghosh P, 2017, INT CONF 3D VISION, P458, DOI 10.1109/3DV.2017.00059
   Gui LY, 2018, IEEE INT C INT ROBOT, P562, DOI 10.1109/IROS.2018.8594452
   Guo X, 2019, AAAI CONF ARTIF INTE, P2580
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548
   Li MS, 2022, LECT NOTES COMPUT SC, V13666, P18, DOI 10.1007/978-3-031-20068-7_2
   Li MS, 2021, IEEE INT CONF COMP V, P854, DOI 10.1109/ICCVW54120.2021.00101
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li N, 2023, IEEE T MULTIMEDIA, V25, P3046, DOI 10.1109/TMM.2022.3154609
   Liu XL, 2020, Arxiv, DOI arXiv:2010.05133
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Ma TZ, 2022, PROC CVPR IEEE, P6427, DOI 10.1109/CVPR52688.2022.00633
   Mao W, 2021, INT J COMPUT VISION, V129, P2513, DOI 10.1007/s11263-021-01483-7
   Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Martinez-Gonzalez A, 2021, IEEE INT CONF COMP V, P2276, DOI 10.1109/ICCVW54120.2021.00257
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Sofianos T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11189, DOI 10.1109/ICCV48922.2021.01102
   Tu ZG, 2023, IEEE T MULTIMEDIA, V25, P1819, DOI 10.1109/TMM.2022.3168137
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Wiederer J, 2020, IEEE INT C INT ROBOT, P10676, DOI 10.1109/IROS45743.2020.9341214
   Xu BQ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3247103
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Xu Binqian, 2023, arXiv
   Yujun Cai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P226, DOI 10.1007/978-3-030-58571-6_14
NR 41
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5194
EP 5206
DI 10.1109/TMM.2023.3330075
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600039
DA 2024-08-05
ER

PT J
AU Wang, RS
   Shi, YX
   Ling, HF
   Li, ZY
   Zhao, CX
   Wei, BH
   Li, H
   Li, P
AF Wang, Runsheng
   Shi, Yuxuan
   Ling, Hefei
   Li, Zongyi
   Zhao, Chengxin
   Wei, Bohao
   Li, He
   Li, Ping
TI Gait Recognition With Multi-Level Skeleton-Guided Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gait recognition; Semantics; skeleton-aided gait recognition;
   model-based method; multi-modality
AB Existing methods combining skeleton and silhouette representations demonstrate explicit effectiveness for gait recognition. However, current related methods simply combine the video-level representations of model-based skeleton data and gait silhouettes for retrieval. Therefore, diverse skeleton information is not fully exploited in existing related works: Firstly, the position and movement of bones are not clear from individual silhouettes. This indicates that the frame-level interaction between features of skeletons and silhouettes is critical, which is ignored by previous methods. Secondly, diverse part-level skeleton-guided gait features are not fully captured in existing related approaches. To solve the above issues, we present a novel framework with multi-level skeleton-guided refinement, including frame-level, part-level, and video-level skeleton-guided refinement, for comprehensive skeleton-aided gait representation learning. First, two modules are proposed for frame-level skeleton-guided refinement. Specifically, Visual Skeleton Enhanced Backbone (VSEB) is proposed to visually highlight the global and part-level skeleton regions for the feature of each silhouette frame. Moreover, Cross-Visual-Model Frame-level Interaction (CVMFI) is proposed to further transfer the model-based skeleton information to features of the visual modalities. Secondly, part-level visual and model-based skeleton features are utilized to refine the final gait representation. Concretely, in VSEB, Part Skeleton Enhance Network (PSEN) is proposed to visually enhance the position and movement of part-level skeletons. In addition, Semantic Part Pooling (SPP) is proposed for capturing the model-based skeleton features of different semantic parts. Finally, as the video-level skeleton-guided refinement, multi-modal video-level features are combined to boost the final recognition performance. Extensive experimental results on prevailing datasets demonstrate that our approach outperforms most existing methods, including the skeleton-aided multi-modal methods. With the multi-level refinement guided by the skeleton modalities, the framework is expected to provide a deeper understanding of skeleton-aided gait recognition.
C1 [Wang, Runsheng; Shi, Yuxuan; Ling, Hefei; Li, Zongyi; Zhao, Chengxin; Wei, Bohao; Li, He; Li, Ping] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Shi, YX (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
EM wrsh@hust.edu.cn; shiyx@hust.edu.cn; lhefei@hust.edu.cn;
   zongyili@hust.edu.cn; chengxinzhao@hust.edu.cn; xavid@hust.edu.cn;
   he_li@hust.edu.cn; lpshome@hust.edu.cn
OI Shi, Yuxuan/0000-0001-7858-5369; Ling, Hefei/0000-0001-6797-7412
FU Natural Science Foundation of China
FX No Statement Available
CR Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Castro FM, 2020, NEURAL COMPUT APPL, V32, P14173, DOI 10.1007/s00521-020-04811-z
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Chen X, 2021, IEEE T IMAGE PROCESS, V30, P3041, DOI 10.1109/TIP.2021.3055936
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   He YW, 2019, IEEE T INF FOREN SEC, V14, P102, DOI 10.1109/TIFS.2018.2844819
   Hsu HM, 2022, IEEE IMAGE PROC, P2546, DOI 10.1109/ICIP46576.2022.9897409
   Huang XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12889, DOI 10.1109/ICCV48922.2021.01267
   Huang Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14900, DOI 10.1109/ICCV48922.2021.01465
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jin X, 2022, PROC CVPR IEEE, P14258, DOI 10.1109/CVPR52688.2022.01388
   Li GD, 2023, APPL INTELL, V53, P1535, DOI 10.1007/s10489-022-03543-y
   Li N, 2023, IEEE T MULTIMEDIA, V25, P3046, DOI 10.1109/TMM.2022.3154609
   Li SQ, 2019, IEEE T MULTIMEDIA, V21, P2361, DOI 10.1109/TMM.2019.2900134
   Liang JH, 2022, LECT NOTES COMPUT SC, V13665, P375, DOI 10.1007/978-3-031-20065-6_22
   Lin BB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3054, DOI 10.1145/3394171.3413861
   Lin BB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14628, DOI 10.1109/ICCV48922.2021.01438
   Peng Y., 2023, Multimedia Tools Appl., P1
   Rijun Liao, 2017, Biometric Recognition. 12th Chinese Conference, CCBR 2017. Proceedings: LNCS 10568, P474, DOI 10.1007/978-3-319-69923-3_51
   Shiraga K, 2016, INT CONF BIOMETR
   Song YF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1625, DOI 10.1145/3394171.3413802
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Takemura Noriko, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0039-6
   Teepe T, 2021, IEEE IMAGE PROC, P2314, DOI 10.1109/ICIP42928.2021.9506717
   Tong SB, 2018, IEEE ACCESS, V6, P57583, DOI 10.1109/ACCESS.2018.2874073
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang L., 2023, PROC IEEE INT C ACOU, P1
   Weizhi An, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P421, DOI 10.1109/TBIOM.2020.3008862
   Wolf T, 2016, IEEE IMAGE PROC, P4165, DOI 10.1109/ICIP.2016.7533144
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xiang Li, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12624), P3, DOI 10.1007/978-3-030-69535-4_1
   Xiang Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13306, DOI 10.1109/CVPR42600.2020.01332
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yao LX, 2022, IEEE T CIRC SYST VID, V32, P3615, DOI 10.1109/TCSVT.2021.3112564
   Yu SQ, 2006, INT C PATT RECOG, P441
   Yu SQ, 2017, IEEE COMPUT SOC CONF, P532, DOI 10.1109/CVPRW.2017.80
   Zhang ZY, 2019, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR.2019.00484
   Zhao LM, 2022, APPL INTELL, V52, P2023, DOI 10.1007/s10489-021-02484-2
   Zhu H., 2023, PROC IEEE INT JOINT, P1
   Zhu Z., 2021, ICCV, p14 789, DOI DOI 10.1109/ICCV48922.2021.01452
NR 41
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4515
EP 4526
DI 10.1109/TMM.2023.3323887
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100052
DA 2024-08-05
ER

PT J
AU Wang, YZ
   Lu, T
   Yao, Y
   Zhang, YD
   Xiong, ZX
AF Wang, Yuanzhi
   Lu, Tao
   Yao, Yuan
   Zhang, Yanduo
   Xiong, Zixiang
TI Learning to Hallucinate Face in the Dark
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face hallucination; image relighting; duplex fusing-embedding learning;
   feature fusion; feature embedding
ID HISTOGRAM EQUALIZATION; SUPERRESOLUTION; RETINEX; IMAGES
AB Face hallucination in low-light environments is an extremely challenging task due to the significant loss of facial structure and facial texture information. Although cascading image relighting and face hallucination tasks is a feasible strategy, simply cascading these two tasks does not achieve satisfactory results because they do not fit into each other naturally. In this article, we propose a novel duplex fusing-embedding learning approach to tackle this challenge in low-light environments. The core of the proposed approach is the duplexity of feature fusion and embedding between relighting and hallucination tasks. In the feature fusion phase, the shallow features from two tasks are bidirectionally fused and activated into a consistent feature space. In the feature embedding phase, the fused features from the previous iteration are fed back and bidirectionally embedded into the deep features of two tasks in the current iteration so that they can learn feature representations that consistently represent both tasks, thereby boosting the performance of relighting and hallucination to generate photorealistic HR face images. Experimental results show that the proposed approach allows current face hallucination methods to learn to hallucinate face in the dark.
C1 [Wang, Yuanzhi; Lu, Tao; Yao, Yuan] Wuhan Inst Technol, Hubei Key Lab Intelligent Robot, Wuhan 430205, Peoples R China.
   [Wang, Yuanzhi] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, PCA Lab, Key Lab Intelligent Percept & Syst High Dimens Inf, Nanjing 210094, Peoples R China.
   [Zhang, Yanduo] Hubei Univ Arts & Sci, Comp Sch, Xiangyang 441053, Peoples R China.
   [Xiong, Zixiang] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.
C3 Wuhan Institute of Technology; Nanjing University of Science &
   Technology; Hubei University of Arts & Science; Texas A&M University
   System; Texas A&M University College Station
RP Lu, T (corresponding author), Wuhan Inst Technol, Hubei Key Lab Intelligent Robot, Wuhan 430205, Peoples R China.
EM w906522992@gmail.com; lutxyl@gmail.com; yaoyuan.usc@gmail.com;
   zhangyanduo@hotmail.com; zx@ece.tamu.edu
OI Wang, Yuanzhi/0000-0003-2594-2574; Lu, Tao/0000-0001-8117-2012; Xiong,
   Zixiang/0000-0002-4714-3311
FU National Natural Science Foundation of China
FX No Statement Available
CR Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Bao QQ, 2023, IEEE T MULTIMEDIA, V25, P8554, DOI 10.1109/TMM.2023.3238522
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Cheng F., 2021, PROC INT C MULTIMEDI, P1
   Coltuc D, 2006, IEEE T IMAGE PROCESS, V15, P1143, DOI 10.1109/TIP.2005.864170
   Das SD, 2021, IEEE IMAGE PROC, P2788, DOI 10.1109/ICIP42928.2021.9506473
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Ding X, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102816
   Guo KH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3495258
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Huang H, 2010, PATTERN RECOGN, V43, P2532, DOI 10.1016/j.patcog.2010.02.007
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Jiang K, 2022, AAAI CONF ARTIF INTE, P1078
   Jiang K, 2022, IEEE T NEUR NET LEAR, V33, P378, DOI 10.1109/TNNLS.2020.3027849
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kalarot R, 2020, IEEE WINT CONF APPL, P359, DOI [10.1109/WACV45572.2020.9093399, 10.1109/wacv45572.2020.9093399]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kruthiventi SSS, 2017, IEEE IMAGE PROC, P4207, DOI 10.1109/ICIP.2017.8297075
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Li-Wen Wang, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P550, DOI 10.1007/978-3-030-67070-2_33
   Liang JX, 2022, IEEE T MULTIMEDIA, V24, P1609, DOI 10.1109/TMM.2021.3068840
   Lin SD, 2023, IEEE T MULTIMEDIA, V25, P9506, DOI 10.1109/TMM.2023.3254141
   Liu C, 2001, PROC CVPR IEEE, P192
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Lu T, 2024, IEEE T NEUR NET LEAR, V35, P3938, DOI 10.1109/TNNLS.2022.3201448
   Lu T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5501, DOI 10.1145/3474085.3475682
   Ma C, 2020, PROC CVPR IEEE, P5568, DOI 10.1109/CVPR42600.2020.00561
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Rao N, 2021, IEEE SIGNAL PROC LET, V28, P1250, DOI 10.1109/LSP.2021.3079848
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shen L, 2017, Arxiv, DOI arXiv:1711.02488
   Wang Chenyang, 2023, IEEE Transactions on Biometrics, Behavior, and Identity Science, P435, DOI 10.1109/TBIOM.2023.3264223
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Wang YJ, 2023, INT J COMPUT VISION, V131, P1002, DOI 10.1007/s11263-022-01730-5
   Wang YZ, 2023, IEEE T CIRC SYST VID, V33, P2533, DOI 10.1109/TCSVT.2022.3224940
   Wang YZ, 2021, IEEE COMPUT SOC CONF, P252, DOI 10.1109/CVPRW53098.2021.00034
   Wang YZ, 2021, NEUROCOMPUTING, V462, P282, DOI 10.1016/j.neucom.2021.07.096
   Wang YZ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20205852
   Wang YL, 2019, ADV NEUR IN, V32
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C., 2018, PROC BRIT MACH VIS C, P1
   Xiaobin Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P763, DOI 10.1007/978-3-030-58548-8_44
   Yang WH, 2020, IEEE T IMAGE PROCESS, V29, P5737, DOI 10.1109/TIP.2020.2981922
   Yang X, 2018, LECT NOTES COMPUT SC, V11165, P441, DOI 10.1007/978-3-030-00767-6_41
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou EJ, 2015, AAAI CONF ARTIF INTE, P3871
NR 57
TC 9
Z9 9
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2314
EP 2326
DI 10.1109/TMM.2023.3294808
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100018
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Wen, JY
   Qin, FW
   Du, J
   Fang, ME
   Wei, XH
   Chen, CLP
   Li, P
AF Wen, Jinyu
   Qin, Feiwei
   Du, Jiao
   Fang, Meie
   Wei, Xinhua
   Chen, C. L. Philip
   Li, Ping
TI MsgFusion: Medical Semantic Guided Two-Branch Network for Multimodal
   Brain Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Brain image; feature extraction; image fusion; two-branch network;
   medical semantic information
ID LOW-RANK; INFORMATION; TRANSFORM; FREQUENCY
AB Multimodal image fusion plays an essential role in medical image analysis and application, where computed tomography (CT), magnetic resonance (MR), single-photon emission computed tomography (SPECT), and positron emission tomography (PET) are commonly-used modalities, especially for brain disease diagnoses. Most existing fusion methods do not consider the characteristics of medical images, and they adopt similar strategies and assessment standards to natural image fusion. While distinctive medical semantic information (MS-Info) is hidden in different modalities, the ultimate clinical assessment of the fusion results is ignored. Our MsgFusion first builds a relationship between the key MS-Info of the MR/CT/PET/SPECT images and image features to guide the CNN feature extractions using two branches and the design of the image fusion framework. For MR images, we combine the spatial domain feature and frequency domain feature (SF) to develop one branch. For PET/SPECT/CT images, we integrate the gray color space feature and adapt the HSV color space feature (GV) to develop another branch. A classification-based hierarchical fusion strategy is also proposed to reconstruct the fusion images to persist and enhance the salient MS-Info reflecting anatomical structure and functional metabolism. Fusion experiments are carried out on many pairs of MR-PET/SPECT and MR-CT images. According to seven classical objective quality assessments and one new subjective clinical quality assessment from 30 clinical doctors, the fusion results of the proposed MsgFusion are superior to those of the existing representative methods.
C1 [Wen, Jinyu; Du, Jiao; Fang, Meie] Guangzhou Univ, Metaverse Inst, Sch Comp Sci & Cyber Engn, Guangzhou 511400, Peoples R China.
   [Qin, Feiwei] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Wei, Xinhua] South China Univ Technol, Affiliated Hosp 2, Dept Radiol, Guangzhou 510641, Peoples R China.
   [Chen, C. L. Philip] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Chen, C. L. Philip] Pazhou Lab, Guangzhou 510335, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Sch Design, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Res Inst Sports Sci & Technol, Kowloon, Hong Kong, Peoples R China.
C3 Guangzhou University; Hangzhou Dianzi University; South China University
   of Technology; South China University of Technology; Pazhou Lab; Hong
   Kong Polytechnic University; Hong Kong Polytechnic University
RP Fang, ME (corresponding author), Guangzhou Univ, Metaverse Inst, Sch Comp Sci & Cyber Engn, Guangzhou 511400, Peoples R China.
EM wjy1361120721@163.com; qinfeiwei@hdu.edu.cn; dujiao19880429@126.com;
   fme@gzhu.edu.cn; eyxinhuawei@163.com; philip.chen@ieee.org;
   p.li@polyu.edu.hk
RI Chen, C. L. Philip/O-2657-2016; Fang, Meie/IYS-4458-2023; Qin,
   Feiwei/ABE-8478-2020
OI Fang, Meie/0000-0003-4292-8889; Qin, Feiwei/0000-0001-5036-9365; Wen,
   Jinyu/0000-0001-9312-8289; Li, Ping/0000-0002-1503-0240
FU National Natural Science Foundation of China
FX No Statement Available
CR Abiko K., 2019, P IEEE VIS COMM IM P, P1
   Algarni AD, 2020, WIRELESS PERS COMMUN, V111, P1033, DOI 10.1007/s11277-019-06899-6
   Atta-Fosu T., 2018, Research in Shape Analysis, P111
   Das M, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3165263
   Diwakar M, 2021, BIOMED SIGNAL PROCES, V68, DOI 10.1016/j.bspc.2021.102788
   Du J, 2020, COMPUT METH PROG BIO, V196, DOI 10.1016/j.cmpb.2020.105603
   Fang L, 2012, IEEE T MULTIMEDIA, V14, P1359, DOI 10.1109/TMM.2012.2191269
   Gnutti A, 2021, IEEE IMAGE PROC, P1594, DOI 10.1109/ICIP42928.2021.9506636
   Guo J, 2018, LECT NOTES COMPUT SC, V11208, P282, DOI 10.1007/978-3-030-01225-0_17
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hermessi H, 2018, NEURAL COMPUT APPL, V30, P2029, DOI 10.1007/s00521-018-3441-1
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jin X, 2018, SIGNAL PROCESS, V153, P379, DOI 10.1016/j.sigpro.2018.08.002
   Kumar N, 2019, LECT NOTES COMPUT SC, V11846, P121, DOI 10.1007/978-3-030-33226-6_14
   Kuo CC, 2020, QUANT IMAG MED SURG, V10, P907, DOI 10.21037/qims.2020.03.19
   Li HF, 2018, PATTERN RECOGN, V79, P130, DOI 10.1016/j.patcog.2018.02.005
   Li H, 2022, Arxiv, DOI arXiv:1804.09325
   Li H, 2022, Arxiv, DOI arXiv:1804.08992
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li P, 2018, CHIN CONTR CONF, P9584, DOI 10.23919/ChiCC.2018.8483674
   Liang XC, 2019, IEEE SENS J, V19, P7107, DOI 10.1109/JSEN.2019.2913281
   Liu Jixin, 2018, Journal of Computer Applications, V38, P3355, DOI 10.11772/j.issn.1001-9081.2018040806
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Naidu V. P. S, 2011, P INT C IM INF PROC, P1
   Panigrahy C, 2020, IEEE SIGNAL PROC LET, V27, P690, DOI 10.1109/LSP.2020.2989054
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Pezzotti E., 2017, Ph.D. dissertation
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Rao YJ, 1997, MEAS SCI TECHNOL, V8, P355, DOI 10.1088/0957-0233/8/4/002
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Sengupta A, 2020, IEEE ACCESS, V8, P88385, DOI 10.1109/ACCESS.2020.2993607
   Shen YQ, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101908
   Parvathy VS, 2020, INT J IMAG SYST TECH, V30, P847, DOI 10.1002/ima.22436
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Wang B, 2021, IEEE T MULTIMEDIA, V23, P3137, DOI 10.1109/TMM.2020.3020695
   Wang LM, 2015, Arxiv, DOI [arXiv:1508.01667, DOI 10.48550/ARXIV.1508.01667]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei T, 2022, MED IMAGE ANAL, V82, DOI 10.1016/j.media.2022.102618
   Xia WY, 2019, IEEE ACCESS, V7, P125976, DOI 10.1109/ACCESS.2019.2939229
   Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484
   Xu K, 2020, PROC CVPR IEEE, P1737, DOI 10.1109/CVPR42600.2020.00181
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yan X, 2018, Arxiv, DOI arXiv:1806.07272
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhang Y, 2017, INFORM FUSION, V35, P81, DOI 10.1016/j.inffus.2016.09.006
   Zhao F, 2021, IEEE T MULTIMEDIA, V23, P2745, DOI 10.1109/TMM.2020.3016123
   Zheng YF, 2007, INFORM FUSION, V8, P177, DOI 10.1016/j.inffus.2005.04.003
   Zhou T, 2019, LECT NOTES COMPUT SC, V11767, P629, DOI 10.1007/978-3-030-32251-9_69
   Zhou T, 2019, LECT NOTES COMPUT SC, V11767, P186, DOI 10.1007/978-3-030-32251-9_21
NR 52
TC 4
Z9 4
U1 17
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 944
EP 957
DI 10.1109/TMM.2023.3273924
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700002
DA 2024-08-05
ER

PT J
AU Yang, MY
   Huo, JY
   Zhou, XL
   Qiao, WH
   Wan, S
   Wang, H
   Yang, FZ
AF Yang, Mingyi
   Huo, Junyan
   Zhou, Xile
   Qiao, Wenhan
   Wan, Shuai
   Wang, Hao
   Yang, Fuzheng
TI Joint Rate-Distortion Optimization for Video Coding and Learning-Based
   In-Loop Filtering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Filtering; Filtering theory; Video coding; Optimization;
   Distortion; Rate-distortion; Rate-distortion optimization (RDO); video
   coding; learning-based in-loop filter
AB Learning-based in-loop filters (ILFs) have recently been widely deployed in the video codec to remove compression artifacts and to obtain better-quality reconstructed videos. However, in the existing codec, the impact of the learning-based ILF is not considered in the Rate-Distortion optimization (RDO) process. With the learning-based ILF, the set of coding parameters selected by the conventional RDO process may no longer be the best one, and the best overall Rate-Distortion (R-D) performance can not be guaranteed. In this article, we propose a joint RDO (JRDO) for Video Coding and learning-based in-loop filtering, which incorporates the effect of the learning-based ILF on the reconstructed video into the RDO process, aiming to achieve the best overall R-D performance of the reconstructed video after in-loop filtering. Furthermore, to realize the proposed JRDO in a standardized video codec, we propose practical strategies to efficiently estimate the effect of learning-based ILF during the RDO process, i.e., efficiently estimate the distortion of the reconstructed block after in-loop filtering during the RDO process. Extensive experiments demonstrate that the proposed joint RDO is standard-compliant and can improve the R-D performance without increasing the decoding time. Besides, the superiority of joint RDO is achieved in various ILFs, indicating the generality of the proposed work.
C1 [Yang, Mingyi; Huo, Junyan; Zhou, Xile; Qiao, Wenhan; Yang, Fuzheng] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
   [Wan, Shuai] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China.
   [Wan, Shuai] RMIT Univ, Sch Engn, Melbourne, Vic 3001, Australia.
   [Wang, Hao] CXMT, Hefei 230000, Peoples R China.
C3 Xidian University; Northwestern Polytechnical University; Royal
   Melbourne Institute of Technology (RMIT)
RP Huo, JY (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
EM myyang_96@stu.xidian.edu.cn; jyhuo@mail.xidian.edu.cn;
   zhouxile@stu.xidian.edu.cn; 20011210334@stu.xidian.edu.cn;
   swan@nwpu.edu.cn; hao3.wang@cxmt.com; fzhyang@mail.xidian.edu.cn
OI wan, shuai/0000-0001-8617-149X; Huo, Junyan/0000-0003-3793-9153
FU National Natural Science Foundation of China
FX No Statement Available
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Akossou AYJ., 2013, Int. J. Math. Comput, V20, P84
   Bharanitharan K, 2008, IEEE T MULTIMEDIA, V10, P1250, DOI 10.1109/TMM.2008.2004904
   Bjontegaard G., 2001, Calculation of Average PSNR Differences between RDcurves
   Bossen F, 2010, 3 JCT VC MEET GUANGZ
   Bross B, 2019, Document JVET-N1001
   Chen GY, 2019, IEEE IMAGE PROC, P1725, DOI 10.1109/icip.2019.8803127
   Chien WJ, 2021, IEEE T CIRC SYST VID, V31, P3848, DOI 10.1109/TCSVT.2021.3101212
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Ding DD, 2020, IEEE T CIRC SYST VID, V30, P1871, DOI 10.1109/TCSVT.2019.2935508
   Ding DD, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954565
   Fischer K, 2020, IEEE INT WORKSH MULT, DOI 10.1109/MMSP48831.2020.9287136
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Huang YW, 2021, IEEE T CIRC SYST VID, V31, P3818, DOI 10.1109/TCSVT.2021.3088134
   Huang ZM, 2021, IEEE COMPUT SOC CONF, P1866, DOI 10.1109/CVPRW53098.2021.00207
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jia S., 2017, P IEEE VIS COMM IM P, P1
   Kang J, 2017, IEEE IMAGE PROC, P26, DOI 10.1109/ICIP.2017.8296236
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lee B, 2016, IEEE T MULTIMEDIA, V18, P1257, DOI 10.1109/TMM.2016.2557075
   Li F, 2018, IEEE IMAGE PROC, P3918, DOI 10.1109/ICIP.2018.8451322
   Li TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921877
   Luo WJ, 2016, ADV NEUR IN, V29
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Park WS, 2016, 2016 IEEE 12TH IMAGE, VIDEO, AND MULTIDIMENSIONAL SIGNAL PROCESSING WORKSHOP (IVMSP)
   Pfaff J, 2021, IEEE T CIRC SYST VID, V31, P3834, DOI 10.1109/TCSVT.2021.3072430
   Schwarz H, 2021, IEEE T CIRC SYST VID, V31, P3891, DOI 10.1109/TCSVT.2021.3072202
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun HM, 2017, IEEE T MULTIMEDIA, V19, P2375, DOI 10.1109/TMM.2017.2700629
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Wang MH, 2020, IEEE T MULTIMEDIA, V22, P1395, DOI 10.1109/TMM.2019.2947351
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xia JY, 2020, IEEE IMAGE PROC, P1291, DOI 10.1109/ICIP40778.2020.9190743
   Yeo CH, 2013, IEEE T CIRC SYST VID, V23, P1170, DOI 10.1109/TCSVT.2013.2240918
   Zhang SF, 2020, IEEE T CIRC SYST VID, V30, P1888, DOI 10.1109/TCSVT.2019.2938192
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3827, DOI 10.1109/TIP.2018.2815841
   Zhao X, 2021, IEEE T CIRC SYST VID, V31, P3878, DOI 10.1109/TCSVT.2021.3087706
NR 40
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2851
EP 2865
DI 10.1109/TMM.2023.3304895
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400027
DA 2024-08-05
ER

PT J
AU Yang, X
   Tian, MH
   Li, MJ
   Wei, ZY
   Yuan, L
   Wang, NN
   Gao, XB
AF Yang, Xi
   Tian, Menghui
   Li, Meijie
   Wei, Ziyu
   Yuan, Liu
   Wang, Nannan
   Gao, Xinbo
TI SSRR: Structural Semantic Representation Reconstruction for
   Visible-Infrared Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Structural semantic representation reconstruction; fine-grained
   features; reconstructed features; semantic information; local
   consistency loss
ID TRANSFORMER
AB Visible-infrared Person Re-identification (VI-ReID) aims to retrieve the images of pedestrian with the same identity from different modalities and cameras given a pedestrian image. To reduce modality discrepancy, existing methods often perform hard partitioning to mine more detail. However, these methods employ only uniform partitioning, without considering pedestrian structure, and lose a lot of pedestrian semantic information. To this end, this paper proposes a structural semantic representation reconstruction (SSRR) method to capture pedestrian semantic information by focusing on pedestrian structure. Specifically, based on the fine-grained features obtained by hard partitioning, we carry out structural reconstruction to obtain the reconstructed features containing semantic information. By adopting the direct link reconstruction structure, the reciprocal learning of fine-grained features and semantic features is ensured. Semantic features are reconstructed based on fine-grained features, and semantic information is beneficial to fine-grained features to better capture pedestrian-related details. In addition, local consistency loss is introduced to ensure the consistency of fine-grained features in the same component location, further enhancing the discriminant of the learned reconstructed representation. Extensive experiments confirm the superiority of our method on two public datasets SYSU-MM01 and RegDB.
C1 [Yang, Xi; Li, Meijie; Wang, Nannan] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Tian, Menghui] Xidian Univ, Hangzhou Inst Technol, Hangzhou 311231, Peoples R China.
   [Wei, Ziyu] Fourth Mil Med Univ, Dept Biomed Engn, Xian 710032, Peoples R China.
   [Wei, Ziyu] Shaanxi Key Lab Bioelectromagnet Detect & Intellig, Xian 710032, Peoples R China.
   [Yuan, Liu] China Acad Elect & Informat Technol CETC, Beijing 100041, Peoples R China.
   [Gao, Xinbo] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; Xidian University; Air Force Military Medical
   University; Xidian University; Chongqing University of Posts &
   Telecommunications
RP Gao, XB (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM yangx@xidian.edu.cn; tianmenghui@stu.xidian.edu.cn;
   limj@stu.xidian.edu.cn; ziyuwei@fmmu.edu.cn; yuanliu@cetc.com.cn;
   nnwang@xidian.edu.cn; xbgao@mail.xidian.edu.cn
OI Wei, Ziyu/0000-0002-3094-3857; Wang, Nannan/0000-0002-4695-6134
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Feng YJ, 2023, IEEE T MULTIMEDIA, V25, P7647, DOI 10.1109/TMM.2022.3224663
   Fu CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11803, DOI 10.1109/ICCV48922.2021.01161
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   Hao Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P57, DOI 10.1145/3343031.3351006
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu WP, 2022, IEEE T CIRC SYST VID, V32, P5095, DOI 10.1109/TCSVT.2022.3147813
   Huang NC, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109145
   Huang ZP, 2022, AAAI CONF ARTIF INTE, P1034
   Jiang KZ, 2022, LECT NOTES COMPUT SC, V13674, P480, DOI 10.1007/978-3-031-19781-9_28
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Liang TF, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3965, DOI 10.1145/3503161.3547975
   Ling YG, 2023, AAAI CONF ARTIF INTE, P1631
   Ling YG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P889, DOI 10.1145/3394171.3413821
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Liu JL, 2022, PROC CVPR IEEE, P19344, DOI 10.1109/CVPR52688.2022.01876
   Liu JA, 2023, COMPUT VIS IMAGE UND, V232, DOI 10.1016/j.cviu.2023.103708
   Liu JN, 2022, IEEE T CIRC SYST VID, V32, P7226, DOI 10.1109/TCSVT.2022.3168999
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Pu N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2149, DOI 10.1145/3394171.3413673
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Si TZ, 2023, NEUROCOMPUTING, V523, P170, DOI 10.1016/j.neucom.2022.12.042
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tian XD, 2021, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR46437.2021.00157
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang PY, 2021, IEEE T MULTIMEDIA, V23, P1474, DOI 10.1109/TMM.2020.2999180
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Wei X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1028, DOI 10.1145/3394171.3413933
   Wei ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P225, DOI 10.1109/ICCV48922.2021.00029
   Wei ZY, 2022, IEEE T NEUR NET LEAR, V33, P4676, DOI 10.1109/TNNLS.2021.3059713
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu J. Jiang, 2022, ACM Trans. Multimedia Comput., Commun. Appl., V18, P1
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang JJ, 2022, IEEE T INTELL TRANSP, V23, P16799, DOI 10.1109/TITS.2021.3102266
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yuan X, 2023, IEEE T INF FOREN SEC, V18, P4681, DOI 10.1109/TIFS.2023.3297791
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zhang DM, 2022, IEEE T CIRC SYST VID, V32, P5361, DOI 10.1109/TCSVT.2022.3144775
   Zhang J. Wang, 2023, Pattern Recognit., V139
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhang W.-J., 2023, Inf. Fusion, V100
   Zhang YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P788, DOI 10.1145/3474085.3475250
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zhao ZW, 2021, AAAI CONF ARTIF INTE, V35, P3520
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
NR 63
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6273
EP 6284
DI 10.1109/TMM.2023.3347855
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600012
DA 2024-08-05
ER

PT J
AU You, JK
   Zhou, YC
AF You, Jinkun
   Zhou, Yicong
TI Two-Stage Watermark Removal Framework for Spread Spectrum Watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; secret key estimation; spread spectrum watermarking;
   watermark removal; watermarking security
ID NATURAL WATERMARKING; SECURITY
AB Spread spectrum (SS) watermarking has gained significant attention as it prevents attackers from reading, tampering with, or removing watermarks. Secret key estimation can help with the first two unauthorized operations but cannot remove watermarks. Moreover, existing deep-learning watermark removal methods do not consider the characteristics of SS watermarking, thus leading to unsatisfactory results. In this paper, we design a secret key estimation method that treats secret key estimation as a binary classification problem and updates the estimated key via backpropagation and parameter optimization algorithms. We develop a watermark removal network using quaternion convolutional neural networks (QCNNs) to learn watermark features while capturing the relationship between channels to improve image quality. Based on our estimation method and QCNN-based network, we propose a two-stage watermark removal framework that utilizes information of the secret key to train the network. A loss function is introduced to directly prevent watermark extraction, thereby improving removal performance. Extensive experiments demonstrate the superiority of our methods over the state-of-the-art methods.
C1 [You, Jinkun; Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 University of Macau
RP Zhou, YC (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM youjinkun09@gmail.com; yicongzhou@um.edu.mo
RI Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384
FU Science and Technology Development Fund, Macau SAR
FX No Statement Available
CR Barni M, 2005, IEEE T MULTIMEDIA, V7, P23, DOI 10.1109/TMM.2004.840594
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas P., 2006, PROC 8 WORKSHOP MULT, P80
   Bas P., 2007, Break our Watermarking System, V2nd
   Bas P, 2007, LECT NOTES COMPUT SC, V4437, P1
   Bas P, 2013, IEEE T INF FOREN SEC, V8, P1306, DOI 10.1109/TIFS.2013.2267960
   Cayre F, 2005, IEEE T SIGNAL PROCES, V53, P3976, DOI 10.1109/TSP.2005.855418
   Chang J, 2022, IEEE T CIRC SYST VID, V32, P5055, DOI 10.1109/TCSVT.2022.3146517
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   DIFFIE W, 1976, IEEE T INFORM THEORY, V22, P644, DOI 10.1109/TIT.1976.1055638
   Geng LF, 2020, J REAL-TIME IMAGE PR, V17, P631, DOI 10.1007/s11554-020-00941-8
   Hatoum MW, 2021, SIGNAL PROCESS-IMAGE, V90, DOI 10.1016/j.image.2020.116019
   Hua G, 2020, IEEE SIGNAL PROC LET, V27, P770, DOI 10.1109/LSP.2020.2986154
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Kalker T, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P201, DOI 10.1109/MMSP.2001.962734
   Kerckhoffs A., 1883, J. des Sci. militaires, P5
   Li Q, 2022, IEEE T CIRC SYST VID, V32, P5695, DOI 10.1109/TCSVT.2021.3138795
   Loshchilov I., 2018, INT C LEARN REPR
   Malvar HS, 2003, IEEE T SIGNAL PROCES, V51, P898, DOI 10.1109/TSP.2003.809385
   Ntalianis KS, 2002, 2002 INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, DIGEST OF TECHNICAL PAPERS, P188, DOI 10.1109/ICCE.2002.1013985
   Parcollet T., 2019, PROC INT C LEARN REP, P1
   Pérez-Freire L, 2007, PROC SPIE, V6505, DOI 10.1117/12.704176
   Pérez-Freire L, 2008, IEEE T INF FOREN SEC, V3, P593, DOI 10.1109/TIFS.2008.2002938
   Qin C, 2024, IEEE T MULTIMEDIA, V26, P2164, DOI 10.1109/TMM.2023.3293272
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Su ZP, 2018, IEEE T MULTIMEDIA, V20, P2631, DOI 10.1109/TMM.2018.2812599
   Sun SL, 2020, IEEE T CYBERNETICS, V50, P3668, DOI 10.1109/TCYB.2019.2950779
   Wang CP, 2022, IEEE T CIRC SYST VID, V32, P7460, DOI 10.1109/TCSVT.2022.3188524
   Wang YG, 2021, IEEE T CIRC SYST VID, V31, P76, DOI 10.1109/TCSVT.2020.2971590
   Wang YG, 2018, IEEE T CYBERNETICS, V48, P2307, DOI 10.1109/TCYB.2017.2735989
   Wang YG, 2018, IEEE T IMAGE PROCESS, V27, P2063, DOI 10.1109/TIP.2018.2795745
   Xiang Y, 2015, IEEE-ACM T AUDIO SPE, V23, P2228, DOI 10.1109/TASLP.2015.2476755
   Xiao RY, 2021, IEEE T DEPEND SECURE, V18, P1793, DOI 10.1109/TDSC.2019.2938953
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   You JK, 2023, IEEE T MULTIMEDIA, V25, P2459, DOI 10.1109/TMM.2022.3147379
   You JK, 2022, IEEE T CIRC SYST VID, V32, P483, DOI 10.1109/TCSVT.2021.3065199
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 38
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7687
EP 7699
DI 10.1109/TMM.2024.3370380
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000072
DA 2024-08-05
ER

PT J
AU Cheng, JL
   Shi, DZ
   Li, CY
   Li, Y
   Ni, H
   Jin, LW
   Zhang, X
AF Cheng, Jiale
   Shi, Dongzi
   Li, Chenyang
   Li, Yu
   Ni, Hao
   Jin, Lianwen
   Zhang, Xin
TI Skeleton-Based Gesture Recognition With Learnable Paths and Signature
   Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Skeleton; Gesture recognition; Joints; Deep
   learning; Trajectory; Neural networks; graph convolutional network; path
   signature features
AB For the skeleton-based gesture recognition, graph convolutional networks (GCNs) have achieved remarkable performance since the human skeleton is a natural graph. However, the biological structure might not be the crucial one for motion analysis. Also, spatial differential information like joint distance and angle between bones may be overlooked during the graph convolution. In this article, we focus on obtaining meaningful joint groups and extracting their discriminative features by the path signature (PS) theory. Firstly, to characterize the constraints and dependencies of various joints, we propose three types of paths, i.e., spatial, temporal, and learnable path. Especially, a learnable path generation mechanism can group joints together that are not directly connected or far away, according to their kinematic characteristic. Secondly, to obtain informative and compact features, a deep integration of PS with few parameters are introduced. All the computational process is packed into two modules, i.e., spatial-temporal path signature module (ST-PSM) and learnable path signature module (L-PSM) for the convenience of utilization. They are plug-and-play modules available for any neural network like CNNs and GCNs to enhance the feature extraction ability. Extensive experiments have conducted on three mainstream datasets (ChaLearn 2013, ChaLearn 2016, and AUTSL). We achieved the state-of-the-art results with simpler framework and much smaller model size. By inserting our two modules into the several GCN-based networks, we can observe clear improvements demonstrating the great effectiveness of our proposed method.
C1 [Cheng, Jiale; Shi, Dongzi; Li, Chenyang; Li, Yu; Jin, Lianwen; Zhang, Xin] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Guangdong, Peoples R China.
   [Ni, Hao] UCL, Dept Math, London WC1E 6BT, England.
   [Zhang, Xin] Pazhou Lab, Guangzhou 510330, Peoples R China.
C3 South China University of Technology; University of London; University
   College London; Pazhou Lab
RP Zhang, X (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Guangdong, Peoples R China.
EM jialecheng100@gmail.com; eedongzishi@mail.scut.edu.cn;
   lichenyang.scut@foxmail.com; lyu.scut@qq.com; h.ni@ucl.ac.uk;
   eelwjin@scut.edu.cn; eexinzhang@scut.edu.cn
OI Cheng, Jiale/0000-0002-0032-7455; Jin, Lianwen/0000-0002-5456-0957; Ni,
   Hao/0000-0001-5485-4376
FU National Key Ramp;D Program of China; EPSRC [EP/S026347/1] Funding
   Source: UKRI
FX No Statement Available
CR Boedihardjo H, 2016, ADV MATH, V293, P720, DOI 10.1016/j.aim.2016.02.011
   Bonnier P, 2019, ADV NEUR IN, V32
   Chen K.-T, 1958, T AM MATH SOC, V89, P395, DOI DOI 10.2307/1993193
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng JL, 2022, IEEE T MED IMAGING, V41, P1665, DOI 10.1109/TMI.2022.3147690
   Cheng K., 2020, P EUR C COMP VIS
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Chevyrev I., 2016, arXiv, DOI [DOI 10.48550/ARXIV.1603.03788, 10.48550/arxiv.1603.03788]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Escalera S, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P365
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   Graham B, 2013, Arxiv, DOI arXiv:1308.0371
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hambly B, 2010, ANN MATH, V171, P109, DOI 10.4007/annals.2010.171.109
   Jiang SY, 2021, IEEE COMPUT SOC CONF, P3408, DOI 10.1109/CVPRW53098.2021.00380
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kidger P., 2021, P INT C LEARN REPR
   Kim J, 2022, INT C PATT RECOG, P3355, DOI 10.1109/ICPR56361.2022.9956156
   Korban Matthew, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P761, DOI 10.1007/978-3-030-58565-5_45
   Lai SX, 2020, IEEE T INF FOREN SEC, V15, P3553, DOI 10.1109/TIFS.2020.2991880
   Levin D, 2016, Arxiv, DOI arXiv:1309.0260
   Li B, 2019, AAAI CONF ARTIF INTE, P8561
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li CY, 2019, AAAI CONF ARTIF INTE, P8585
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Liao L., 2019, P IEEE VIS COMM IM P, P1
   Liao S., 2021, P 32 BRIT MACH VIS C, P173
   Lin C, 2018, IEEE INT CONF AUTOMA, P52, DOI 10.1109/FG.2018.00018
   Liu H, 2017, Arxiv, DOI [arXiv:1705.08106, DOI 10.48550/ARXIV.1705.08106]
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Lyons T., 2014, P ACM INT C BIG DAT, P1
   M. Contributors, 2020, OpenMMLab pose estimation toolbox and benchmark
   Mou C, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2159, DOI 10.1145/3394171.3413685
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Narayana P, 2018, PROC CVPR IEEE, P5235, DOI 10.1109/CVPR.2018.00549
   Nt H, 2019, Arxiv, DOI arXiv:1905.09550
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Qiu HL, 2022, Arxiv, DOI arXiv:2201.02849
   Reizenstein J, 2018, Arxiv, DOI arXiv:1802.08252
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Simonyan K, 2014, ADV NEUR IN, V27
   Sincan OM, 2020, IEEE ACCESS, V8, P181340, DOI 10.1109/ACCESS.2020.3028072
   Su B, 2018, IEEE T PATTERN ANAL, V40, P77, DOI 10.1109/TPAMI.2017.2665545
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan J, 2016, IEEE COMPUT SOC CONF, P761, DOI 10.1109/CVPRW.2016.100
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang HS, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P574, DOI 10.1109/ACPR.2015.7486568
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Wu WB, 2017, IEEE INT CONF COMP V, P623, DOI 10.1109/ICCVW.2017.79
   Xie ZC, 2018, IEEE T PATTERN ANAL, V40, P1903, DOI 10.1109/TPAMI.2017.2732978
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang W., 2022, Stochastic Analysis, Filtering, and Stochastic Optimization: A Commemorative Volume to Honor Mark HA Davis's Contributions, P431, DOI 10.1007/978-3-030-98519-6_18
   Zhang HB, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19051005
   Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24
   Zhang Xin, 2020, Med Image Comput Comput Assist Interv, V12267, P134, DOI 10.1007/978-3-030-59728-3_14
   Zhao Y, 2018, PROC CVPR IEEE, P6566, DOI 10.1109/CVPR.2018.00687
   Zheng W, 2019, Arxiv, DOI [arXiv:1805.02556, DOI 10.48550/ARXIV.1805.02556]
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 69
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3951
EP 3961
DI 10.1109/TMM.2023.3318242
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300022
OA Green Published
DA 2024-08-05
ER

PT J
AU Ding, KY
   Zhong, RJ
   Wang, ZH
   Yu, Y
   Fang, YM
AF Ding, Keyan
   Zhong, Rijin
   Wang, Zhihua
   Yu, Yang
   Fang, Yuming
TI Adaptive Structure and Texture Similarity Metric for Image Quality
   Assessment and Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Full-reference image quality assessment; perceptual optimization;
   structure similarity; texture similarity
ID DEVIATION; DEEP
AB Objective Image Quality Assessment (IQA) aims to design computational models that can automatically predict the perceived quality of images. The state-of-the-art full-reference IQA metric - Deep Image Structure and Texture Similarity (DISTS), neglects the fact that natural images often consist of local structure and texture, and requires supervised training on the annotated dataset. In this article, we introduce multiple adaptive strategies to improve DISTS, resulting in an opinion-unaware IQA metric, named A-DISTS. Specifically, A-DISTS first uses the dispersion index as a statistical feature to adaptively localize structure and texture regions at different scales. Second, it adaptively assigns the spatial weights between local structure and texture similarity measurements according to the estimated structure or texture probability maps. Finally, it calculates the entropy of image representation to adaptively weigh the importance of each feature map. As a result, A-DISTS is adapted to local image content and does not require any training. The experimental results demonstrated that the proposed metric correlates well with human rating in the standard and algorithm-dependent IQA databases, and exhibits competitive performance in the optimization tasks of single image super-resolution, motion deblurring, and multi-distortion removal.
C1 [Ding, Keyan] Zhejiang Univ, ZJU Hangzhou Global Sci & Technol Innovat Ctr, Hangzhou 310027, Peoples R China.
   [Zhong, Rijin; Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330013, Peoples R China.
   [Wang, Zhihua] Shenzhen MSU BIT Univ, Guangdong Lab Machine Percept & Intelligent Comp, Shenzhen 518115, Peoples R China.
   [Yu, Yang] Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R China.
C3 Zhejiang University; Jiangxi University of Finance & Economics; Shenzhen
   MSU-BIT University; National University of Defense Technology - China
RP Yu, Y (corresponding author), Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R China.
EM dingkeyan@zju.edu.cn; zhongrijin2000@163.com;
   zhihua.wang@my.cityu.edu.hk; yuyangnudt@hotmail.com;
   fa0001ng@e.ntu.edu.sg
OI Zhihua, WANG/0000-0002-4398-536X
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2002, Standard BT-500-11
   [Anonymous], 1966, The statistical analysis of series of events
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Cao Y, 2022, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR52688.2022.00576
   Chen Binjie, 2023, arXiv
   Comer ML, 1999, J ELECTRON IMAGING, V8, P279, DOI 10.1117/1.482677
   Cox D. R., 1966, Math. Gazette, V51, P266
   DASARATHY BV, 1991, PATTERN RECOGN LETT, V12, P497, DOI 10.1016/0167-8655(91)80014-2
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding KY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2483, DOI 10.1145/3474085.3475419
   Ding KY, 2021, INT J COMPUT VISION, V129, P1258, DOI 10.1007/s11263-020-01419-7
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Dosovitskiy A, 2016, 30 C NEURAL INFORM P, V29
   Duanmu ZF, 2021, ANNU REV VIS SCI, V7, P437, DOI 10.1146/annurev-vision-100419-120301
   Girod Bernd, 1993, P207
   Gu Jinjin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P633, DOI 10.1007/978-3-030-58621-8_37
   Hammou D, 2021, IEEE COMPUT SOC CONF, P541, DOI 10.1109/CVPRW53098.2021.00066
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff O. J., 2016, PROC 4 INT C LEARN R, P1
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Kligvasser T., 2021, Advances in Neural Information Processing Systems, P3939
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Laparra V., 2016, ELECT IMAGING, V2016, P1, DOI DOI 10.2352/ISSN.2470-1173.2016.16.HVEI-103
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liao XR, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P970, DOI 10.1145/3503161.3548193
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2
   Liu JZ, 2023, IEEE T MULTIMEDIA, V25, P5358, DOI 10.1109/TMM.2022.3190700
   Liu YM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508391
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Ma KD, 2020, IEEE T PATTERN ANAL, V42, P851, DOI 10.1109/TPAMI.2018.2889948
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Ni ZK, 2018, IEEE T IMAGE PROCESS, V27, P4516, DOI 10.1109/TIP.2018.2839890
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Shannon C. E., 2001, Bell Sys. Technical J, V5, P3, DOI [DOI 10.1002/J.1538-7305.1948.TB01338.X, 10.1145/584091.584093]
   Sheikh H. R., 2006, Image and video quality assessment research at LIVE
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sim K, 2021, IEEE T MULTIMEDIA, V23, P4037, DOI 10.1109/TMM.2020.3037482
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C., 2013, PROC 2 INT C LEARN R
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Video Quality Experts Group, 2000, VQEG M OTT CAN MARCH
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z., 2016, Electron. Imag., V2016, P1
   Xian WZ, 2024, IEEE T MULTIMEDIA, V26, P2219, DOI 10.1109/TMM.2023.3293730
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WX, 2020, IEEE IMAGE PROC, P111, DOI [10.1109/icip40778.2020.9191278, 10.1109/ICIP40778.2020.9191278]
   Zhang XD, 2013, IEEE SIGNAL PROC LET, V20, P319, DOI 10.1109/LSP.2013.2244081
   Zhou F, 2019, IEEE T IMAGE PROCESS, V28, P3528, DOI 10.1109/TIP.2019.2898638
   Zhu HW, 2023, Arxiv, DOI arXiv:2211.04927
   Zhu WH, 2019, IEEE T MULTIMEDIA, V21, P2334, DOI 10.1109/TMM.2019.2902484
   Zhu YC, 2023, IEEE T MULTIMEDIA, V25, P7607, DOI 10.1109/TMM.2022.3224319
NR 67
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5398
EP 5409
DI 10.1109/TMM.2023.3333208
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600043
DA 2024-08-05
ER

PT J
AU He, ZT
   Shao, F
   Chen, G
   Chai, XL
   Ho, YS
AF He, Zhentao
   Shao, Feng
   Chen, Gang
   Chai, Xiongli
   Ho, Yo-Sung
TI SCFANet: Semantics and Context Feature Aggregation Network for 360°
   Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360(degrees) omnidirectional images; feature aggregation; salient object
   detection; transformer
ID IMAGE; MODEL
AB How to solve the problem of geometric distortion is the key for salient object detection (SOD) in 360(degrees) omnidirectional images. Most of the current methods integrate global and local visual cues through the fusion of the 360(degrees) equirectangular images and corresponding 360(degrees )cube-map images. The fusion in a single level cannot effectively utilize the information between the 360(degrees) equirectangular images and corresponding 360(degrees) cube-map images. In this work, we innovatively propose a semantics and context feature aggregation network (SCFANet) by fully exploring the interactivity between the two projection data. Specifically, we use Vision Transformer (ViT) to capture global visual cues for 360(degrees) equirectangular images and Convolutional Neural Network (CNN) to capture local visual cues for 360(degrees) cube-map images. To achieve effective fusion of the two projection data, we design a semantic guidance module (SGM), in which semantic features are used to guide the information fusion of the 360(degrees) equirectangular images and corresponding 360(degrees )cube-map images at each level. Then, a context fusion module (CFM) containing one local input and two context inputs is designed to integrate multi-scale features, where the local input extracts its own multi-scale information, and the context inputs complements their fine details and location information. Finally, we use feature aggregation and refinement module (FARM) to aggregate semantics and context feature and adopt a deep supervision strategy for training. Extensive experiments on two public 360(degrees) datasets show that our SCFANet exhibits competitive performance compared to other state-of-the-art (SOTA) 360(degrees) salient object detection models.
C1 [He, Zhentao; Shao, Feng; Chen, Gang; Chai, Xiongli] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM hezhentao1999@163.com; shaofeng@nbu.edu.cn; cg17855310233@126.com;
   1801082022@nbu.cn; hoyo@gist.ac.kr
FU Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chao FY, 2018, IEEE INT CONF MULTI
   Chen CLZ, 2023, IEEE T CIRC SYST VID, V33, P457, DOI 10.1109/TCSVT.2022.3203421
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P3995, DOI 10.1109/TIP.2021.3068644
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen DW, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P92, DOI [10.1109/VR46266.2020.1581216087067, 10.1109/VR46266.2020.00-77]
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6308, DOI 10.1109/TCSVT.2022.3166914
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cheng Y, 2014, IEEE INT CON MULTI
   Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112
   Cong RM, 2024, IEEE T NEUR NET LEAR, V35, P9495, DOI 10.1109/TNNLS.2022.3233883
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Feng MY, 2020, IEEE T IMAGE PROCESS, V29, P4696, DOI 10.1109/TIP.2020.2975919
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   He HY, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01312-y
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang LM, 2022, IEEE T CIRC SYST VID, V32, P1366, DOI 10.1109/TCSVT.2021.3069812
   Huang MK, 2023, IEEE T CIRC SYST VID, V33, P6191, DOI 10.1109/TCSVT.2023.3253685
   Huang MK, 2020, IEEE SIGNAL PROC LET, V27, P1819, DOI 10.1109/LSP.2020.3028192
   Huo FS, 2022, IEEE T CIRC SYST VID, V32, P3111, DOI 10.1109/TCSVT.2021.3102268
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Kingma D. P., 2014, arXiv
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li J, 2020, IEEE J-STSP, V14, P38, DOI 10.1109/JSTSP.2019.2957982
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Ma GX, 2020, IEEE T VIS COMPUT GR, V26, P3535, DOI 10.1109/TVCG.2020.3023636
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Martin D., 2020, P IEEE C COMP VIS PA, P1
   Maugey T, 2017, IEEE INT WORKSH MULT
   Mauthner T, 2015, PROC CVPR IEEE, P2494, DOI 10.1109/CVPR.2015.7298864
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Qin XB, 2021, Arxiv, DOI arXiv:2101.04704
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qiu Y, 2022, Arxiv, DOI arXiv:2108.07851
   Tavakoli HR, 2020, Arxiv, DOI arXiv:1905.10693
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ren QH, 2021, IEEE T MULTIMEDIA, V23, P1442, DOI 10.1109/TMM.2020.2997178
   Rosin PL, 2009, PATTERN RECOGN, V42, P2363, DOI 10.1016/j.patcog.2009.04.021
   Shang-Hua Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P702, DOI 10.1007/978-3-030-58539-6_42
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Suzuki T, 2018, IEEE SYS MAN CYBERN, P2079, DOI 10.1109/SMC.2018.00358
   Tsiami A, 2020, PROC CVPR IEEE, P4765, DOI 10.1109/CVPR42600.2020.00482
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang GT, 2021, PROC CVPR IEEE, P15114, DOI 10.1109/CVPR46437.2021.01487
   Wang J, 2022, IEEE T CIRC SYST VID, V32, P2949, DOI 10.1109/TCSVT.2021.3099120
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xie CX, 2022, PROC CVPR IEEE, P11707, DOI 10.1109/CVPR52688.2022.01142
   Xu YY, 2022, IEEE T PATTERN ANAL, V44, P7235, DOI 10.1109/TPAMI.2021.3100259
   Yao ZJ, 2022, IEEE T MULTIMEDIA, V24, P4236, DOI 10.1109/TMM.2021.3115344
   Yu F., 2016, ICLR, P1
   Yu SY, 2021, AAAI CONF ARTIF INTE, V35, P3234
   Zhang J., 2020, P IEEE CVF C COMP VI, P8582, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang K, 2019, IEEE T CIRC SYST VID, V29, P3544, DOI 10.1109/TCSVT.2018.2883305
   Zhang YQ, 2020, IEEE J-STSP, V14, P27, DOI 10.1109/JSTSP.2019.2955824
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu G, 2023, IEEE T NEUR NET LEAR, V34, P6615, DOI 10.1109/TNNLS.2021.3127959
NR 83
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2276
EP 2288
DI 10.1109/TMM.2023.3293994
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100024
DA 2024-08-05
ER

PT J
AU Hu, YF
   Gao, JY
   Dong, JF
   Fan, B
   Liu, HM
AF Hu, Yufan
   Gao, Junyu
   Dong, Jianfeng
   Fan, Bin
   Liu, Hongmin
TI Exploring Rich Semantics for Open-Set Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Prototypes; Knowledge graphs; Visualization; Task analysis;
   Uncertainty; Training; Open-set action recognition; video action
   recognition; semantic relation modeling
AB Open-set action recognition (OSAR) aims to learn a recognition framework capable of both classifying known classes and identifying unknown actions in open-set scenarios. Existing OSAR methods typically reside in a data-driven paradigm, which ignore the rich semantics in both known and unknown categories. In fact, we humans have the capability of leveraging the captured semantic information, i.e., knowledge and experience, to incisively distinguish samples from known and unknown classes. Motivated by this observation, in this paper, we propose a Unified Semantic Exploration (USE) framework for recognizing actions in open-set scenarios. Specifically, we explore the explicit knowledge semantics by simulating the unknown classes with knowledge-guided virtual classes based on an external knowledge graph, which enables the model to simulate open-set perception during model training. Besides, we propose to learn the implicit data semantics by transferring the knowledge structure of action categories to the visual prototype space for semantic structure preservation. Extensive experiments on several action recognition benchmarks validate the effectiveness of our proposed method.
C1 [Hu, Yufan; Fan, Bin; Liu, Hongmin] Univ Sci & Technol Beijing, Sch Intelligence & Technol, Beijing 100083, Peoples R China.
   [Gao, Junyu] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Dong, Jianfeng] Zhejiang Gongshang Univ, Coll Comp & Informat Engn, Hangzhou 310018, Peoples R China.
C3 University of Science & Technology Beijing; Chinese Academy of Sciences;
   Institute of Automation, CAS; Zhejiang Gongshang University
RP Liu, HM (corresponding author), Univ Sci & Technol Beijing, Sch Intelligence & Technol, Beijing 100083, Peoples R China.
EM huyufanqaixuan@gmail.com; junyu.gao@nlpr.ia.ac.cn; dongjf24@gmail.com;
   bin.fan@ieee.org; hmliu_82@163.com
OI Dong, Jianfeng/0000-0001-5244-3274; fan, bin/0000-0002-1155-467X; Gao,
   Junyu/0000-0002-8105-5497
FU National Natural Science Foundation of China
FX No Statement Available
CR Bao WT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13329, DOI 10.1109/ICCV48922.2021.01310
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cen J, 2023, PROC CVPR IEEE, P15295, DOI 10.1109/CVPR52729.2023.01468
   Cen J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15313, DOI 10.1109/ICCV48922.2021.01505
   Chen GY, 2022, IEEE T PATTERN ANAL, V44, P8065, DOI 10.1109/TPAMI.2021.3106743
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Deng SL, 2023, IEEE T MULTIMEDIA, V25, P5763, DOI 10.1109/TMM.2022.3198880
   Ditria L., 2020, PROC ASIAN C COMPUT, P474
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feng YB, 2023, IEEE T MULTIMEDIA, V25, P9464, DOI 10.1109/TMM.2023.3252275
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gao J., 2021, P IEEE CVF INT C COM, P1523
   Gao JY, 2021, IEEE T PATTERN ANAL, V43, P3476, DOI 10.1109/TPAMI.2020.2985708
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Gao R, 2023, IEEE T MULTIMEDIA, V25, P1649, DOI 10.1109/TMM.2022.3145666
   Ge Z., 2017, PROC BRIT MACH VIS C
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   Guangyao Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P507, DOI 10.1007/978-3-030-58580-8_30
   Guo YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P103, DOI 10.1109/ICCV48922.2021.00017
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XX, 2021, INT C PATT RECOG, P4626, DOI 10.1109/ICPR48806.2021.9412989
   Hendrycks D., 2018, PROC INT C LEARN REP
   Hendrycks D., 2016, ABS160608415 ARXIV
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Jung CY, 2022, PROC CVPR IEEE, P18239, DOI 10.1109/CVPR52688.2022.01772
   Krishnan R, 2018, Arxiv, DOI arXiv:1811.03305
   Krishnan R, 2020, AAAI CONF ARTIF INTE, V34, P4477
   Krizhevsky Alex, 2010, Convolutional deep belief networks on cifar-10
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lee KH, 2018, ECO-EFFIC IND SCI, V33, P1, DOI 10.1007/978-3-319-70899-7_1
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Lu J, 2022, AAAI CONF ARTIF INTE, P1872
   Mandal D, 2019, PROC CVPR IEEE, P9977, DOI 10.1109/CVPR.2019.01022
   Monfort M, 2022, IEEE T PATTERN ANAL, V44, P9434, DOI 10.1109/TPAMI.2021.3126682
   Mundt M, 2019, IEEE INT CONF COMP V, P753, DOI 10.1109/ICCVW.2019.00098
   Neal L, 2018, LECT NOTES COMPUT SC, V11210, P620, DOI 10.1007/978-3-030-01231-1_38
   Oza P, 2019, PROC CVPR IEEE, P2302, DOI 10.1109/CVPR.2019.00241
   Lonij VPA, 2017, Arxiv, DOI arXiv:1708.08310
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Perera Pramuditha, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11811, DOI 10.1109/CVPR42600.2020.01183
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Schlachter P, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902738
   Shu Y, 2018, IEEE INT CON MULTI
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Subedar M, 2019, IEEE I CONF COMP VIS, P6310, DOI 10.1109/ICCV.2019.00640
   Sun X., 2020, P IEEE CVF C COMP VI, P13480
   Vaswani A, 2017, ADV NEUR IN, V30
   Vaze S., 2022, P INT C LEARN REPR, DOI 10.48550/arXiv.2110.06207
   Wang Y., 2021, ICCV, P9302
   Wu YR, 2023, ACM T EMBED COMPUT S, V22, DOI 10.1145/3587038
   Xu J. Yang, 2021, P IEEE CVF INT C COM, P9332
   Xu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9312, DOI 10.1109/ICCV48922.2021.00920
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang HM, 2022, IEEE T PATTERN ANAL, V44, P2358, DOI 10.1109/TPAMI.2020.3045079
   Yang KX, 2023, IEEE INT CON MULTI, P762, DOI 10.1109/ICME55011.2023.00136
   Yin C., 2021, PROC IEEECVF INT C C, P2177
   Yoshihashi R, 2019, PROC CVPR IEEE, P4011, DOI 10.1109/CVPR.2019.00414
   You RC, 2020, AAAI CONF ARTIF INTE, V34, P12709
   Yue ZQ, 2021, PROC CVPR IEEE, P15399, DOI 10.1109/CVPR46437.2021.01515
   Zhang H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6954, DOI 10.1109/ICCV48922.2021.00689
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhao C, 2023, PROC CVPR IEEE, P22982, DOI 10.1109/CVPR52729.2023.02201
   Zhong Z, 2021, PROC CVPR IEEE, P9457, DOI 10.1109/CVPR46437.2021.00934
NR 68
TC 12
Z9 12
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5410
EP 5421
DI 10.1109/TMM.2023.3333206
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600012
DA 2024-08-05
ER

PT J
AU Ke, JC
   Wang, J
   Chen, JC
   Jhuo, IH
   Lin, CW
   Lin, YY
AF Ke, Jingcheng
   Wang, Jia
   Chen, Jun-Cheng
   Jhuo, I-Hong
   Lin, Chia-Wen
   Lin, Yen-Yu
TI CLIPREC: Graph-Based Domain Adaptive Network for Zero-Shot Referring
   Expression Comprehension
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Referring expression comprehension; domain adaptive network; zero-shot
   learning; CLIP
ID LANGUAGE
AB Referring expression comprehension (REC) is a cross-modal matching task that aims to localize the target object in an image specified by a text description. Most existing approaches for this task focus on identifying only objects whose categories are covered by training data. This restricts their generalization to unseen categories and practical usage. To address this issue, we propose a domain adaptive network called CLIPREC for zero-shot REC, which integrates the Contrastive Language-Image Pretraining (CLIP) model for graph-based REC. The proposed CLIPREC is composed of a graph collaborative attention module with two directed graphs: one for objects in an image and the other for their corresponding categorical labels. To carry out zero-shot REC, we leverage the strong common image-text feature space from the CLIP model to correlate the two graphs. Furthermore, a multilayer perceptron is introduced to enable feature alignment so that the CLIP model is adapted to the expression representation from the language parser, resulting in effective reasoning from expressions involving both seen and unseen object categories. Extensive experimental and ablation results on several widely-adopted benchmarks show that the proposed approach performs favorably against state-of-the-art approaches for zero-shot REC.
C1 [Ke, Jingcheng; Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 300044, Taiwan.
   [Ke, Jingcheng; Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 300044, Taiwan.
   [Wang, Jia] Natl Yang Ming Chiao Tung Univ, Dept Elect Engn, Hsinchu 300093, Taiwan.
   [Chen, Jun-Cheng] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115201, Taiwan.
   [Jhuo, I-Hong] Microsoft, Seattle, WA 98052 USA.
   [Lin, Yen-Yu] Natl Yang Ming Chiao Tung Univ, Dept Comp Sci, Hsinchu 300093, Taiwan.
C3 National Tsing Hua University; National Tsing Hua University; National
   Yang Ming Chiao Tung University; Academia Sinica - Taiwan; Microsoft;
   National Yang Ming Chiao Tung University
RP Chen, JC (corresponding author), Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115201, Taiwan.
EM freedom6927@gmail.com; vicky.ee08@nycu.edu.tw;
   pullpull@citi.sinica.edu.tw; ihjhuo@gmail.com; cwlin@ee.nthu.edu.tw;
   lin@cs.nycu.edu.tw
RI Lin, Chia-Wen/ABH-6075-2020
OI Lin, Yen-Yu/0000-0002-7183-6070
FU National Science and Technology Council
FX No Statement Available
CR Bird S., 2004, NLTK: the natural language toolkit, P214, DOI DOI 10.3115/1225403.1225421
   Chen K, 2017, IEEE I CONF COMP VIS, P824, DOI 10.1109/ICCV.2017.95
   Chen L, 2021, AAAI CONF ARTIF INTE, V35, P1036
   Chen Xinpeng, 2018, Real-time referring expression comprehension by single-stage grounding network
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Feng CJ, 2022, LECT NOTES COMPUT SC, V13669, P701, DOI 10.1007/978-3-031-20077-9_41
   Frome A., 2013, Advances in neural information processing systems, V26
   Gao C, 2021, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR46437.2021.00308
   Gu X., 2022, PROC INT C LEARN REP
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Jia C, 2021, PR MACH LEARN RES, V139
   Jin Y, 2022, IEEE T MULTIMEDIA, V24, P1896, DOI 10.1109/TMM.2021.3073624
   Jing CC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4041, DOI 10.1145/3394171.3413902
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Kazi A, 2023, IEEE T PATTERN ANAL, V45, P1606, DOI 10.1109/TPAMI.2022.3170249
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li A, 2017, IEEE I CONF COMP VIS, P4193, DOI 10.1109/ICCV.2017.449
   Li JN, 2022, PR MACH LEARN RES
   Li Manling, 2022, P IEEE CVF C COMP VI, P16420
   Li ZH, 2019, AAAI CONF ARTIF INTE, P8690
   Liang C., 2021, Clawcranenet: Leveraging object-level relation for text-based video segmentation
   Liang C, 2023, IEEE T PATTERN ANAL, V45, P10055, DOI 10.1109/TPAMI.2023.3262578
   Liang C, 2022, PROC CVPR IEEE, P15544, DOI 10.1109/CVPR52688.2022.01512
   Liu YF, 2020, AAAI CONF ARTIF INTE, V34, P11645
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Ordonez V., 2011, NeurIPS, V24
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Pont-Tuset J, 2015, IEEE I CONF COMP VIS, P1546, DOI 10.1109/ICCV.2015.181
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Radford A, 2021, PR MACH LEARN RES, V139
   Recht B., 2018, ARXIV180600451
   Sadhu A, 2019, IEEE I CONF COMP VIS, P4693, DOI 10.1109/ICCV.2019.00479
   Socher R, 2013, P 2013 C EMP METH NA, P935
   Subramanian S, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P5198
   Wang J, 2019, IEEE I CONF COMP VIS, P4662, DOI 10.1109/ICCV.2019.00476
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Yang S., 2020, P IEEE CVF C COMP VI, P9952
   Yang SB, 2021, IEEE T PATTERN ANAL, V43, P2765, DOI 10.1109/TPAMI.2020.2973983
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang X, 2022, IEEE T MULTIMEDIA, V24, P313, DOI 10.1109/TMM.2021.3050058
NR 47
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2480
EP 2492
DI 10.1109/TMM.2023.3297312
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100008
DA 2024-08-05
ER

PT J
AU Li, HC
   Zheng, AH
   Sun, LP
   Luo, YL
AF Li, Hongchao
   Zheng, Aihua
   Sun, Liping
   Luo, Yonglong
TI Camera Topology Graph Guided Vehicle Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Topology; Convolutional neural networks; Network topology;
   Training; Representation learning; Aggregates; Vehicle
   re-identification; closed circuit television; camera topology graph;
   graph convolutional network
ID NETWORK
AB Vehicle re-identification (Re-ID) aims to retrieve vehicles across non-overlapping cameras. Most studies consider representation learning from single appearance information of the vehicle images. Some works adopt the spatio-temporal information to remove unreasonable vehicles to refine the results in the testing phase. However, they ignore the potential topological relations among cameras under the Closed Circuit Television (CCTV) camera systems in the training phase, which usually leads to suboptimal results due to the high intra-identity variations. To handle this problem, we propose a novel vehicle re-identification framework, which explicitly models the camera topological relations of all input images to aggregate neighbor images and thus acquires camera-independent representations. Specifically, we first construct a Camera Topology Graph (CTG) to elucidate the topological relations among cameras. It takes different cameras as nodes and constructs edges from four levels of the camera system, position, orientation, and individual. Then, we introduce a Camera Topology-based Graph Convolutional Network (CT-GCN), which suppresses irrelevant neighbor images and learns different camera representation functions. Finally, we propose a topological cross-entropy loss to obtain the more discriminative vehicle representations. The whole network is trained in an end-to-end manner. Extensive experiments on three benchmark datasets demonstrate the effectiveness of the proposed method against state-of-the-art vehicle Re-ID methods.
C1 [Li, Hongchao; Sun, Liping; Luo, Yonglong] Anhui Normal Univ, Sch Comp & Informat, Anhui Prov Key Lab Network & Informat Secur, Wuhu 241003, Peoples R China.
   [Zheng, Aihua] Anhui Univ, Sch Artificial Intelligence, Informat Mat & Intelligent Sensing Lab Anhui Prov, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
C3 Anhui Normal University; Anhui University
RP Luo, YL (corresponding author), Anhui Normal Univ, Sch Comp & Informat, Anhui Prov Key Lab Network & Informat Secur, Wuhu 241003, Peoples R China.
EM lhc950304@foxmail.com; ahzheng214@foxmail.com; slp620@163.com;
   ylluo@ustc.edu.cn
OI Li, Hongchao/0000-0001-6305-7141; Luo, Yonglong/0000-0003-4987-0376
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Chen J, 2018, Arxiv, DOI arXiv:1801.10247
   Chen Z, 2021, AAAI CONF ARTIF INTE, V35, P1113, DOI 10.1145/3474085.3475574
   Cheng D, 2022, IEEE T IMAGE PROCESS, V31, P3334, DOI 10.1109/TIP.2022.3169693
   Chu RH, 2019, IEEE I CONF COMP VIS, P8281, DOI 10.1109/ICCV.2019.00837
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan X, 2019, J VIS COMMUN IMAGE R, V60, P51, DOI 10.1016/j.jvcir.2019.01.010
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hamilton WL, 2017, ADV NEUR IN, V30
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   Hu WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P602, DOI 10.1145/3474085.3475219
   Isobe T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8506, DOI 10.1109/ICCV48922.2021.00841
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11165
   Khorramshahi Pirazh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P369, DOI 10.1007/978-3-030-58568-6_22
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Kingma D. P., 2014, arXiv
   Li HC, 2022, IEEE T IMAGE PROCESS, V31, P5949, DOI 10.1109/TIP.2022.3202370
   Li HC, 2022, IEEE T INTELL TRANSP, V23, P19557, DOI 10.1109/TITS.2022.3166463
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XB, 2020, IEEE T IMAGE PROCESS, V29, P2638, DOI 10.1109/TIP.2019.2950796
   Liu XB, 2018, IEEE INT CON MULTI
   Liu XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P907, DOI 10.1145/3394171.3413578
   Liu XC, 2016, IEEE INT CON MULTI
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Lv K., 2019, P CVPR WORKSH LONG B, P399
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Raj S, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108287
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen F, 2022, IEEE T INTELL TRANSP, V23, P8793, DOI 10.1109/TITS.2021.3086142
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210
   Sochor J, 2016, PROC CVPR IEEE, P3006, DOI 10.1109/CVPR.2016.328
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49
   Welling M., 2017, P 5 INT C LEARN REPR
   Yan K, 2017, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2017.68
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Zhang Z, 2021, PROC CVPR IEEE, P12131, DOI 10.1109/CVPR46437.2021.01196
   Zhao JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P205, DOI 10.1109/ICCV48922.2021.00027
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou SR, 2019, J VIS COMMUN IMAGE R, V59, P393, DOI 10.1016/j.jvcir.2019.01.029
   Zhou Y., 2017, 1 AS AUSTR C PREC PA, V1, P1
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 55
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1565
EP 1577
DI 10.1109/TMM.2023.3283054
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700053
DA 2024-08-05
ER

PT J
AU Li, XK
   Yi, RM
   Huang, YP
AF Li, Xiaokun
   Yi, Rumeng
   Huang, Yaping
TI Mutual Filter Teaching for Open-Set Semi-Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Task analysis; Semisupervised learning; Prototypes; Feature
   extraction; Training data; Semantics; Semi-supervised learning;
   open-set; mutual filter teaching; class prototypes; Mahalanobis distance
AB Open-set semi-supervised learning (OSSL) provides a practical solution by filtering out-of-distribution (OOD) samples from unlabeled data to guarantee the reliance on large unlabeled data in semi-supervised setting. However, existing OSSL methods mainly focus on identifying in-distribution (ID) samples and discarding OOD samples, while ignoring to make full use of samples that could not be exactly identified as ID or OOD samples. Those samples are more likely to be hard samples, which should be carefully explored to boost the performance in OSSL task. Hence, in this paper, we propose a novel framework, named Mutual Filter Teaching (MFT), where two networks are trained simultaneously to divide the unlabeled data into three parts: ID samples, OOD samples and hard samples. The samples are regarded as ID or OOD samples only if two networks give consistent decisions according to Mahalanobis distance between the unlabeled samples and their closest class prototypes. For those samples with inconsistent decisions, we treat them as hard samples and design an efficient mutual teaching scheme where the samples detected by only one network as positive samples are fed to its peer network for training. Furthermore, we propose to employ the prediction variance of two networks to dynamically rectify the learning from hard samples. Experiments on multiple benchmark datasets demonstrate that our approach achieves the state-of-the-art performance.
C1 [Li, Xiaokun; Huang, Yaping] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Yi, Rumeng] CSSC Syst Engn Res Inst, CSSC, Beijing 100094, Peoples R China.
C3 Beijing Jiaotong University
RP Huang, YP (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM 22110102@bjtu.edu.cn; yirumeng1203@gmail.com; yphuang@bjtu.edu.cn
OI Li, XiaoKun/0000-0002-1118-1025
FU National Natural Science Foundation of China
FX No Statement Available
CR Aila T., 2017, P INT C LEARN REPR
   Berthelot D, 2019, ADV NEUR IN, V32
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Cao K., 2022, PROC INT C LEARN REP, P1
   Chen YB, 2020, AAAI CONF ARTIF INTE, V34, P3569
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Guo LZ, 2020, PR MACH LEARN RES, V119
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D., 2017, ICLR
   Hendrycks D, 2019, ADV NEUR IN, V32
   Huang JK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8290, DOI 10.1109/ICCV48922.2021.00820
   Huang Z, 2023, IEEE T MULTIMEDIA, V25, P1844, DOI 10.1109/TMM.2022.3179895
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Le Y., 2015, CS 231N, V7, P3
   Lee D.H., 2013, WORKSH CHALL REPR LE, P07
   Lee K, 2018, ADV NEUR IN, V31
   Li Y, 2021, IEEE T MULTIMEDIA, V23, P1354, DOI 10.1109/TMM.2020.2997185
   Liang S., 2017, PROC INT C LEARN REP, P1
   Lin SH, 2022, IEEE T MULTIMEDIA, V24, P728, DOI 10.1109/TMM.2021.3058546
   Luo HX, 2021, Arxiv, DOI arXiv:2101.08237
   MCLACHLAN GJ, 1975, J AM STAT ASSOC, V70, P365
   Qing Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P438, DOI 10.1007/978-3-030-58610-2_26
   Saito Kuniaki, 2021, Adv. Neural Inf. Process. Syst., P25956
   Sajjadi M, 2016, ADV NEUR IN, V29
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P86
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Tack J., 2020, Advances in neural information processing systems, P11839
   Xie Q., 2020, Neural Information Processing Systems (NeurIPS), V33, P6256
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Yu Q, 2019, IEEE I CONF COMP VIS, P9517, DOI 10.1109/ICCV.2019.00961
   Zagoruyko S., 2016, Wide residual networks, DOI DOI 10.5244/C.30.87
   Zhang H., 2017, PROC INT C LEARN REP, P1
   Zhang YH, 2023, IEEE T MULTIMEDIA, V25, P1749, DOI 10.1109/TMM.2022.3158069
NR 34
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7700
EP 7708
DI 10.1109/TMM.2024.3370670
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000052
DA 2024-08-05
ER

PT J
AU Lin, RH
   Hu, HF
AF Lin, Ronghao
   Hu, Haifeng
TI Dynamically Shifting Multimodal Representations via Hybrid-Modal
   Attention for Multimodal Sentiment Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Acoustics; Visualization; Feature extraction; Task
   analysis; Logic gates; Sentiment analysis; Multi-stage fusion framework;
   intra- and inter-modality dynamics; multimodal representations shifting;
   hybrid-modal attention
ID PREDICTION; LANGUAGE; SPEECH; FUSION
AB In the field of multimodal machine learning, multimodal sentiment analysis task has been an active area of research. The predominant approaches focus on learning efficient multimodal representations containing intra- and inter-modality information. However, the heterogeneous nature of different modalities brings great challenges to multimodal representation learning. In this article, we propose a multi-stage fusion framework to dynamically fine-tune multimodal representations via a hybrid-modal attention mechanism. Previous methods mostly only fine-tune the textual representation due to the success of large corpus pre-trained models and neglect the inconsistency problem of different modality spaces. Thus, we design a module called the Multimodal Shifting Gate (MSG) to fine-tune the three modalities by modeling inter-modality dynamics and shifting representations. We also adopt a module named Masked Bimodal Adjustment (MBA) on the textual modality to improve the inconsistency of parameter spaces and reduce the modality gap. In addition, we utilize syntactic-level and semantic-level textual features output from different layers of the Transformer model to sufficiently capture the intra-modality dynamics. Moreover, we construct a Shifting HuberLoss to robustly introduce the variation of the shifting value into the training process. Extensive experiments on the public datasets, including CMU-MOSI and CMU-MOSEI, demonstrate the efficacy of our approach.
C1 [Lin, Ronghao; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
EM linrh7@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn
RI Lin, Ronghao/HGD-5967-2022
OI Lin, Ronghao/0000-0003-4530-4529; Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Chung J, 2014, PREPRINT
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Gers FA, 1999, IEE CONF PUBL, P850, DOI [10.1049/cp:19991218, 10.1162/089976600300015015]
   Gkoumas D, 2021, INFORM FUSION, V66, P184, DOI 10.1016/j.inffus.2020.09.005
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu Y, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2225
   Guo XB, 2023, IEEE T MULTIMEDIA, V25, P2085, DOI 10.1109/TMM.2022.3142448
   Pham H, 2018, FIRST GRAND CHALLENGE AND WORKSHOP ON HUMAN MULTIMODAL LANGUAGE (CHALLENGE-HML), P53
   Han X, 2021, AI OPEN, V2, P225, DOI 10.1016/j.aiopen.2021.08.002
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   iMotions, 2017, FACIAL EXPRESSION AN
   Jawahar G, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3651
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Kim J. H., 2017, PROC INT C LEARN REP, P1
   Kolen J., 2001, A field guide to dynamical recurrent neural networks, DOI 10.1109/9780470544037.ch14
   Li QC, 2021, INFORM FUSION, V65, P58, DOI 10.1016/j.inffus.2020.08.006
   Liang PP, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P150
   Licai Sun, 2021, MuSe '21: Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge, P15, DOI 10.1145/3475957.3484456
   Licai Sun, 2020, MuSe'20: Proceedings of the 1st International Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop, P27, DOI 10.1145/3423327.3423672
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Loshchilov I., 2018, INT C LEARN REPR
   Mai SJ, 2022, IEEE T MULTIMEDIA, V24, P2488, DOI 10.1109/TMM.2021.3082398
   OLSON DR, 1977, HARVARD EDUC REV, V47, P257, DOI 10.17763/haer.47.3.8840364413869005
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2023, IEEE T AFFECT COMPUT, V14, P108, DOI 10.1109/TAFFC.2020.3038167
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.0055, 10.1109/ICDM.2016.178]
   Poria Soujanya, 2015, P C EMP METH NAT LAN, DOI [10.18653/v1/d15-1303, 10.18653/v1/D15-1303]
   Truong QT, 2019, AAAI CONF ARTIF INTE, P305
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Stappen Lukas, 2021, MuSe '21: Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge, P5, DOI 10.1145/3475957.3484450
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tsai Y.-H. H., 2019, PROC INT C LEARN REP
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HH, 2017, IEEE INT CON MULTI, P949, DOI 10.1109/ICME.2017.8019301
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wang ZJ, 2021, ACL-IJCNLP 2021: THE JOINT CONFERENCE OF THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING: PROCEEDINGS OF THE SYSTEM DEMONSTRATIONS, P132
   Wei Han, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P6, DOI 10.1145/3462244.3479919
   Wu Y, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P4730
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Yang Zhilin, 2019, NeurIPS, V32, DOI DOI 10.5555/3454287.3454804
   Yoon S, 2018, IEEE W SP LANG TECH, P112, DOI 10.1109/SLT.2018.8639583
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu L, 2016, ANN ALLERTON CONF, P540, DOI 10.1109/ALLERTON.2016.7852278
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yuan J, 2008, J ACOUST SOC AM, V124, P2078, DOI 10.1121/1.2968700
   Zadeh A., 2018, PROC GRAND CHALLENGE
   Zadeh A., 2018, PROC GRAND CHALLENGE
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
NR 69
TC 0
Z9 0
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2740
EP 2755
DI 10.1109/TMM.2023.3303711
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400013
DA 2024-08-05
ER

PT J
AU Liu, YX
   Ge, HW
   Wang, Z
   Hou, YQ
   Zhao, MD
AF Liu, Yuxuan
   Ge, Hongwei
   Wang, Zhen
   Hou, Yaqing
   Zhao, Mingde
TI Discriminative Identity-Feature Exploring and Differential Aware
   Learning for Unsupervised Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Identity features; differential aware learning; unsupervised person
   re-identification; contrastive learning
ID DOMAIN ADAPTATION; UNCERTAINTY
AB Unsupervised person re-identification (Re-ID) aims to learn discriminative representations for person retrieval from unlabeled data. Currently, state-of-the-art techniques accomplish this task by using instance contrastive learning, which contrasts the similarities of the instances in different views. However, existing contrastive methods only focus on the positive effects of inter-instance relationships, while neglecting the negative effects of intra-instance redundancy information. This redundancy information can generate invalid or spurious intra-class relationships during the instance contrasting process, which enlarges the intra-class gaps and increases the noisy pseudo-labels. To address this issue, we propose a discriminative identity-feature exploring and differential aware learning (DiDAL) framework to learn more discriminative intra-identity representations. Specifically, the DiDAL extracts intra-instance salient features by synthetic complementary attention, and further explores the discriminative identity features by modeling the relationship among these salient features based on graph neural networks. This strategy aims to reduce the intra-instance redundancy information. Moreover, DiDAL explores hard instances by leveraging the extracted intra-instance salient features, and matches an anchor with multiple hard positive instances to enhance the robustness of the model to noisy pseudo-labels. Extensive experiment results on two widely used person re-identification datasets and a vehicle re-identification dataset demonstrate the superiority of the proposed method compared with existing state-of-the-art methods.
C1 [Liu, Yuxuan; Ge, Hongwei; Hou, Yaqing] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian, Peoples R China.
   [Ge, Hongwei] Washington Univ St Louis, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
   [Wang, Zhen] Beihang Univ, Sch Math Sci, Beijing 100091, Peoples R China.
   [Zhao, Mingde] McGill Univ, Sch Comp Sci, Montreal H3A 0E9, PQ, Canada.
   [Zhao, Mingde] Mila Quebec AI Inst, Montreal H3A 0E9, PQ, Canada.
C3 Dalian University of Technology; Washington University (WUSTL); Beihang
   University; McGill University
RP Ge, HW (corresponding author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian, Peoples R China.
EM lyx8880lzc@mail.dlut.edu.cn; hwge@dlut.edu.cn; wangzhen@dlut.edu.cn;
   houyq@dlut.edu.cn; mingde.zhao@mail.mcgill.ca
RI Wang, Zhen/I-1995-2013; Liu, Yuxuan/IVH-1356-2023
OI Ge, Hongwei/0000-0002-8937-1515; Wang, Zhen/0000-0001-7487-3187; Liu,
   Yuxuan/0000-0003-1168-6645; Zhao, Mingde/0000-0002-6687-8153
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai Y, 2021, IEEE T IMAGE PROCESS, V30, P6715, DOI 10.1109/TIP.2021.3094140
   Bai ZC, 2021, PROC CVPR IEEE, P12909, DOI 10.1109/CVPR46437.2021.01272
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Dai YX, 2021, PROC CVPR IEEE, P16140, DOI 10.1109/CVPR46437.2021.01588
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Ge Y., 2020, ICLR
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Han J, 2022, AAAI CONF ARTIF INTE, P790
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang Y, 2021, INT J COMPUT VISION, V129, P2244, DOI 10.1007/s11263-021-01474-8
   Ji HXY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3641, DOI 10.1109/ICCV48922.2021.00364
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kalantidis Y., 2020, ADV NEURAL INFORM PR, V33, P21798
   Kingma D.P., 2014, Proc. of ICLR
   Li MK, 2022, IEEE T IMAGE PROCESS, V31, P3606, DOI 10.1109/TIP.2022.3173163
   Li YY, 2022, IEEE T MULTIMEDIA, V24, P415, DOI 10.1109/TMM.2021.3052354
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Li ZY, 2022, AAAI CONF ARTIF INTE, P1527
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu TY, 2022, IEEE T IMAGE PROCESS, V31, P4240, DOI 10.1109/TIP.2022.3181811
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Ren XA, 2023, IEEE T MULTIMEDIA, V25, P4387, DOI 10.1109/TMM.2022.3174768
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Tan HC, 2023, IEEE T NEUR NET LEAR, V34, P8210, DOI 10.1109/TNNLS.2022.3144163
   Tao YS, 2023, IEEE T MULTIMEDIA, V25, P4586, DOI 10.1109/TMM.2022.3178599
   Verma A, 2023, IEEE T MULTIMEDIA, V25, P364, DOI 10.1109/TMM.2021.3126404
   Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xin Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P735, DOI 10.1007/978-3-030-58571-6_43
   Xuan SY, 2024, IEEE T PATTERN ANAL, V46, P1711, DOI 10.1109/TPAMI.2022.3163451
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang FX, 2020, IEEE T MULTIMEDIA, V22, P2444, DOI 10.1109/TMM.2019.2957928
   Yang L, 2019, PROC CVPR IEEE, P2293, DOI 10.1109/CVPR.2019.00240
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yao HT, 2022, Arxiv, DOI arXiv:2112.04662
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang MY, 2021, AAAI CONF ARTIF INTE, V35, P3360
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
NR 63
TC 0
Z9 0
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 623
EP 636
DI 10.1109/TMM.2023.3268369
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000006
DA 2024-08-05
ER

PT J
AU Mai, SJ
   Sun, Y
   Xiong, AL
   Zeng, Y
   Hu, HF
AF Mai, Sijie
   Sun, Ya
   Xiong, Aolin
   Zeng, Ying
   Hu, Haifeng
TI Multimodal Boosting: Addressing Noisy Modalities and Identifying
   Modality Contribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Task analysis; Boosting; Representation learning;
   Emotion recognition; Tensors; Sentiment analysis; Multimodal sentiment
   analysis; multimodal representation learning; noisy modalities;
   multimodal emotion recognition
ID SENTIMENT; FUSION
AB In multimodal representation learning, different modalities do not contribute equally. Especially when learning with noisy modalities that convey non-discriminative information, the prediction based on multimodal representation is often biased and even ignores the knowledge from informative modalities. In this paper, we aim to address the noisy modality problem and balance the contributions of multiple modalities dynamically in a parallel format. Specifically, we construct multiple base learners and formulate our framework as a boosting-like algorithm, where different base learners focus on different aspects of multimodal learning. To identify the contributions of individual base learners, we develop a contribution learning network that dynamically determines the contribution and noise level of each base learner. In contrast to the commonly considered attention mechanism, we define the transformation of predictive loss as the supervision signal to train the contribution learning network, which enables more accurate learning of modality importance. We derive the final prediction by incorporating the predictions of base learners based on their contributions. Notably, different from late fusion, we devise a multimodal base learner to explore the cross-modal interactions. To update the network, we design the 'complementary update mechanism', where for each base learner, we assign higher weights to those samples that are incorrectly predicted by other base learners. In this way, we can leverage the available information to correctly predict each sample to the utmost extent and enable different base learners to learn different aspects of multimodal information. Extensive experiments demonstrate that the proposed method achieves superior performance on multimodal sentiment analysis and emotion recognition.
C1 [Mai, Sijie; Sun, Ya; Xiong, Aolin; Zeng, Ying; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China.
EM maisj@mail2.sysu.edu.cn; suny278@mail2.sysu.edu.cn;
   xiongaolin@mail2.sysu.edu.cn; zengy268@mail2.sysu.edu.cn;
   huhaif@mail.sysu.edu.cn
OI Aolin, Xiong/0009-0005-2301-7897; Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China
FX No Statement Available
CR Akhtar MS, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P370
   Anastasopoulos A, 2019, Arxiv, DOI arXiv:1903.02930
   Angelou M, 2019, PATTERN RECOGN, V95, P296, DOI 10.1016/j.patcog.2019.06.013
   Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Behmanesh M, 2024, IEEE T NEUR NET LEAR, V35, P6991, DOI 10.1109/TNNLS.2022.3213589
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cambria E, 2013, PROCEEDINGS OF THE 2013 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE FOR HUMAN-LIKE INTELLIGENCE (CIHLI), P108, DOI 10.1109/CIHLI.2013.6613272
   Chauhan DS, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P2583, DOI 10.1145/3477495.3531900
   Chauhan DS, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4351
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Cho K., 2014, ARXIV14061078, V1406, P1078, DOI DOI 10.3115/V1/D14-1179
   Dai WL, 2021, Arxiv, DOI arXiv:2104.11560
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dumpala S. H., 2018, P 32 C NEUR INF PROC, P1
   Gkoumas D, 2021, INFORM FUSION, V66, P184, DOI 10.1016/j.inffus.2020.09.005
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hirano Yuki, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P141, DOI 10.1145/3462244.3479927
   Hou M, 2019, ADV NEUR IN, V32
   Kampman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P606
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim J.-H., 2017, P INT C LEARN REPR, P1
   Kingma D. P., 2014, arXiv
   Lee C, 2021, PR MACH LEARN RES, V130
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li QC, 2021, INFORM FUSION, V65, P58, DOI 10.1016/j.inffus.2020.08.006
   Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063
   Liang P. P., 2021, P 35 C NEUR INF PROC, P1
   Liang PP, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P150
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Louizos Christos, 2018, INT C LEARN REPR
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2016, ADV NEUR IN, V29
   Mai Sijie, 2023, IEEE Transactions on Multimedia, P4121, DOI 10.1109/TMM.2022.3171679
   Mai SJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542927
   Mai SJ, 2023, IEEE T AFFECT COMPUT, V14, P2276, DOI 10.1109/TAFFC.2022.3172360
   Mai SJ, 2022, IEEE T AFFECT COMPUT, V13, P320, DOI 10.1109/TAFFC.2020.3000510
   Mai SJ, 2022, IEEE T MULTIMEDIA, V24, P2488, DOI 10.1109/TMM.2021.3082398
   Mai SJ, 2021, IEEE-ACM T AUDIO SPE, V29, P1424, DOI 10.1109/TASLP.2021.3068598
   Mai SJ, 2020, AAAI CONF ARTIF INTE, V34, P164
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Mai SJ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P481
   Meng L, 2014, IEEE T KNOWL DATA EN, V26, P2293, DOI 10.1109/TKDE.2013.47
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Mittal T, 2020, AAAI CONF ARTIF INTE, V34, P1359
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Peng W, 2021, IEEE INTELL SYST, V36, P82, DOI 10.1109/MIS.2021.3057757
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pérez-Rúa JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Qureshi SA, 2019, IEEE INTELL SYST, V34, P45, DOI 10.1109/MIS.2019.2925204
   Rahate A, 2022, INFORM FUSION, V81, P203, DOI 10.1016/j.inffus.2021.12.003
   Rahman T, 2019, IEEE I CONF COMP VIS, P8907, DOI 10.1109/ICCV.2019.00900
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Rozgic V, 2012, ASIAPAC SIGN INFO PR
   Sankaran S, 2021, Arxiv, DOI arXiv:2104.03435
   Schapire RE, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1401
   Sebe N., 2005, of Pattern Recognition and Computer Vision, P387
   Shankar S, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P1167
   Stappen Lukas, 2021, MuSe '21: Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge, P75, DOI 10.1145/3475957.3484451
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Sun Y, 2023, IEEE T AFFECT COMPUT, V14, P2209, DOI 10.1109/TAFFC.2022.3178231
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tsai YHH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1823, DOI 10.18653/v1/2020.emnlp-main.143
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wei Han, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P6, DOI 10.1145/3462244.3479919
   Wu W, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6269, DOI 10.1109/ICASSP39728.2021.9414880
   Xue XJ, 2023, IEEE T KNOWL DATA EN, V35, P5105, DOI 10.1109/TKDE.2022.3155290
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Yang ZL, 2019, ADV NEUR IN, V32
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yu WM, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3718
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yuan ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4400, DOI 10.1145/3474085.3475585
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zeng Y, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P1262
   Zhang C, 2008, IEEE T MULTIMEDIA, V10, P1541, DOI 10.1109/TMM.2008.2007344
   Zhang K, 2022, IEEE T CIRC SYST VID, V32, P1034, DOI 10.1109/TCSVT.2021.3072412
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 91
TC 1
Z9 1
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3018
EP 3033
DI 10.1109/TMM.2023.3306489
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700006
DA 2024-08-05
ER

PT J
AU Mitra, S
   Jogani, S
   Soundararajan, R
AF Mitra, Shankhanil
   Jogani, Saiyam
   Soundararajan, Rajiv
TI Semi-Supervised Learning of Perceptual Video Quality by Generating
   Consistent Pairwise Pseudo-Ranks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality assessment; Video recording; Feature extraction; Solid modeling;
   Semisupervised learning; Predictive models; Distortion; No reference
   video quality assessment; quality feature learning; semi-supervised
   learning; pairwise ranks
ID SIMILARITY
AB Designing learning-based no-reference (NR) video quality assessment (VQA) algorithms for camera-captured videos is cumbersome due to the large number of human annotations of quality. In this work, we propose a semi-supervised learning (SSL) framework exploiting many unlabelled and very limited numbers of authentically distorted labelled videos. Our main contributions are twofold. Leveraging the benefits of consistency regularization and pseudo-labelling, our SSL model generates pairwise pseudo-ranks for the unlabelled videos using a student-teacher model on strong-weak augmented videos. We design the strong-weak augmentations to be quality invariant to use the unlabelled videos effectively in SSL. The generated pseudo-ranks are used along with the limited labels to train our SSL model. Our primary focus in SSL for NR VQA is to learn mapping from video feature representations to quality scores. We compare various feature extraction methods and show that our SSL framework can lead to improved performance on these features. We present a spatial and temporal feature extraction method based on predicting spatial and temporal entropic differences. We show that these features help achieve robust performance when trained with limited data, providing a better baseline to apply SSL. Extensive experiments on three popular VQA datasets demonstrate that the proposed semi-supervised VQA method improves on the performance of existing methods in terms of correlation with human opinion by approximately $15 \! - \! 20 \%$
C1 [Mitra, Shankhanil; Soundararajan, Rajiv] Indian Inst Sci, Dept Elect Commun Engn, Bengaluru 560012, India.
   [Jogani, Saiyam] Birla Inst Technol & Sci, Dept Comp Sci & Informat Syst, Pilani 403426, India.
C3 Indian Institute of Science (IISC) - Bangalore; Birla Institute of
   Technology & Science Pilani (BITS Pilani)
RP Mitra, S (corresponding author), Indian Inst Sci, Dept Elect Commun Engn, Bengaluru 560012, India.
EM shankhanilm@iisc.ac.in; f20190097@goa.bits-pilani.ac.in;
   rajivs@iisc.ac.in
FU Department of Science and Technology, Government of India
FX No Statement Available
CR Ahn S, 2018, IEEE IMAGE PROC, P619, DOI 10.1109/ICIP.2018.8451450
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Bovik R., 2017, On the robust perfor-manceoftheST-RREDvideoqualitypredictor
   Burges C.J.C., 2005, P 22 INT C MACH LEAR, V119, P89, DOI DOI 10.1145/1102351.1102363
   Caviedes JE, 2003, PROC SPIE, V5150, P621, DOI 10.1117/12.510112
   Chen P., 2021, P IEEECVF INT C COMP, P5178
   Chen PF, 2022, IEEE T IMAGE PROCESS, V31, P458, DOI 10.1109/TIP.2021.3130536
   Chen PF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P834, DOI 10.1145/3394171.3413717
   De Simone F, 2010, INT CONF ACOUST SPEE, P2430, DOI 10.1109/ICASSP.2010.5496296
   Dendi SVR, 2020, IEEE T IMAGE PROCESS, V29, P5612, DOI 10.1109/TIP.2020.2984879
   Farias MCQ, 2005, IEEE IMAGE PROC, P3593
   Ghadiyaram D, 2018, IEEE T CIRC SYST VID, V28, P2061, DOI 10.1109/TCSVT.2017.2707479
   Götz-Hahn F, 2021, IEEE ACCESS, V9, P72139, DOI 10.1109/ACCESS.2021.3077642
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Pham H, 2021, PROC CVPR IEEE, P11552, DOI 10.1109/CVPR46437.2021.01139
   Hosu V, 2017, INT WORK QUAL MULTIM
   Kancharla P, 2022, IEEE T IMAGE PROCESS, V31, P263, DOI 10.1109/TIP.2021.3130541
   Kingma D. P., 2014, arXiv
   Korhonen J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3311, DOI 10.1145/3394171.3413845
   Korhonen J, 2019, IEEE T IMAGE PROCESS, V28, P5923, DOI 10.1109/TIP.2019.2923051
   Lee D.H., 2013, WORKSH CHALL REPR LE, P07
   Li BW, 2022, IEEE T CIRC SYST VID, V32, P5944, DOI 10.1109/TCSVT.2022.3164467
   Li DQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2351, DOI 10.1145/3343031.3351028
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752
   Li YM, 2016, IEEE T CIRC SYST VID, V26, P1044, DOI 10.1109/TCSVT.2015.2430711
   Liao L, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3547849
   Liu WT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P546, DOI 10.1145/3240508.3240643
   Liu YX, 2022, IEEE T CIRC SYST VID, V32, P3500, DOI 10.1109/TCSVT.2021.3114509
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Manasa K, 2016, IEEE IMAGE PROC, P2400, DOI 10.1109/ICIP.2016.7532789
   Mitra S, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1914, DOI 10.1145/3503161.3548064
   Mitra S, 2021, IEEE SIGNAL PROC LET, V28, P170, DOI 10.1109/LSP.2021.3049682
   Mittal A, 2016, IEEE T IMAGE PROCESS, V25, P289, DOI 10.1109/TIP.2015.2502725
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Rimac-Drlje S, 2010, MULTIMED TOOLS APPL, V49, P425, DOI 10.1007/s11042-009-0442-1
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Sajjadi M, 2016, ADV NEUR IN, V29
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shen WH, 2022, IEEE T BROADCAST, V68, P651, DOI 10.1109/TBC.2022.3164332
   Sinno Z, 2019, IEEE T IMAGE PROCESS, V28, P612, DOI 10.1109/TIP.2018.2869673
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tu Z, 2021, IEEE OPEN J SIGNAL P, V2, P425, DOI 10.1109/OJSP.2021.3090333
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P4449, DOI 10.1109/TIP.2021.3072221
   Vu P. V., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2505, DOI 10.1109/ICIP.2011.6116171
   Vu PV, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013016
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z., 2021, arXiv
   Wu HN, 2022, LECT NOTES COMPUT SC, V13666, P538, DOI 10.1007/978-3-031-20068-7_31
   Xie Q., 2020, Advances in Neural Information Processing Systems, V33, P6256
   Xu JT, 2014, IEEE IMAGE PROC, P491, DOI 10.1109/ICIP.2014.7025098
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang FZ, 2005, IEEE SIGNAL PROC LET, V12, P685, DOI 10.1109/LSP.2005.855553
   Yang XL, 2023, IEEE T KNOWL DATA EN, V35, P8934, DOI 10.1109/TKDE.2022.3220219
   Ying ZQ, 2021, PROC CVPR IEEE, P14014, DOI 10.1109/CVPR46437.2021.01380
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   You JY, 2019, IEEE IMAGE PROC, P2349, DOI [10.1109/ICIP.2019.8803395, 10.1109/icip.2019.8803395]
   Yue GH, 2023, IEEE T MULTIMEDIA, V25, P6499, DOI 10.1109/TMM.2022.3209889
   Zhang Y, 2019, IEEE T CIRC SYST VID, V29, P2244, DOI 10.1109/TCSVT.2018.2868063
   Zheng Q, 2022, IEEE SIGNAL PROC LET, V29, P2228, DOI 10.1109/LSP.2022.3215311
NR 66
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6215
EP 6227
DI 10.1109/TMM.2023.3347090
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600023
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Peng, YT
   He, LH
   Hu, D
   Liu, YH
   Yang, LZ
   Shang, SH
AF Peng, Yitao
   He, Lianghua
   Hu, Die
   Liu, Yihang
   Yang, Longzhen
   Shang, Shaohua
TI Hierarchical Dynamic Masks for Visual Explanation of Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Neural networks; Decision making; Visualization; Reliability; Predictive
   models; Location awareness; Biological neural networks; Model
   interpretability; neural networks; image classification; model-agnostic
AB Despite the remarkable accomplishments of deep neural networks in computer vision tasks, the inherent opacity of their operations remains a pressing concern. Attribution methods generating visual explanatory maps representing the importance of image pixels for model classification are popular for explaining neural network decisions. However, the small and diverse decision regions in fine-grained or medical images limit the precision and comprehensiveness of the existing attribution methods when explaining decisions made for such a data type. This paper introduces a novel attribution method called hierarchical dynamic masks (HDM) to overcome these concerns to generate saliency maps with high recognition reliability and localization capability. Specifically, we suggest dynamic masks (DM), which enable multiple small-sized benchmark mask vectors to learn the image's critical information roughly through an optimization method. The benchmark mask vectors guide the learning of the large-sized combination mask vectors so that their overlay mask accurately learns detailed pixel importance information. Additionally, we construct the HDM by hierarchically concatenating DM modules. These DM modules search and combine the regions of interest in the remaining neural network classification decisions within the masked image in a learning-based way. Since HDM forces DM to perform importance analysis in different areas, it makes the fused saliency map more comprehensive. The experiments reveal that the proposed method outperforms existing approaches significantly regarding recognition credibility and positioning ability when qualitatively and quantitatively tested on CUB-200-2011 and iChallenge-PM datasets.
C1 [Peng, Yitao; He, Lianghua; Liu, Yihang; Yang, Longzhen; Shang, Shaohua] Tongji Univ, Sch Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Hu, Die] Fudan Univ, Sch Informat Sci & Technol, Shanghai 200433, Peoples R China.
C3 Tongji University; Fudan University
RP Hu, D (corresponding author), Fudan Univ, Sch Informat Sci & Technol, Shanghai 200433, Peoples R China.
EM pyt@tongji.edu.cn; helianghua@tongji.edu.cn; hudie@fudan.edu.cn;
   2111131@tongji.edu.cn; yanglongzhen@tongji.edu.cn;
   shaohuashang@tongji.edu.cn
OI Yang, Longzhen/0000-0002-5791-145X; Peng, Yitao/0000-0002-2680-5822; hu,
   die/0000-0001-8081-8512
FU National Key Ramp;D Program of China
FX No Statement Available
CR Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen YL, 2020, APPL SOFT COMPUT, V93, DOI 10.1016/j.asoc.2020.106335
   Chen ZM, 2021, IEEE T MULTIMEDIA, V23, P1827, DOI 10.1109/TMM.2020.3003779
   Cheng D, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109270
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Desai S, 2020, IEEE WINT CONF APPL, P972, DOI [10.1109/wacv45572.2020.9093360, 10.1109/WACV45572.2020.9093360]
   Fang CW, 2023, IEEE T MED IMAGING, V42, P1720, DOI 10.1109/TMI.2023.3237183
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Fu H., 2019, IEEE Dataport, DOI [10.21227/55pk-8203, DOI 10.21227/55PK-8203]
   Fu RG, 2020, Arxiv, DOI [arXiv:2008.02312, DOI 10.48550/ARXIV.2008.02312]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang PT, 2021, IEEE T IMAGE PROCESS, V30, P5875, DOI 10.1109/TIP.2021.3089943
   Li H, 2023, PROC CVPR IEEE, P15485, DOI 10.1109/CVPR52729.2023.01486
   Li XH, 2023, ARTIF INTELL-AMST, V314, DOI 10.1016/j.artint.2022.103823
   Liu HB, 2022, IEEE T MULTIMEDIA, V24, P2902, DOI 10.1109/TMM.2021.3090274
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lundberg SM, 2017, ADV NEUR IN, V30
   Muhammad MB, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206626
   Patrício C, 2023, Arxiv, DOI arXiv:2205.04766
   Pennisi M, 2021, ARTIF INTELL MED, V118, DOI 10.1016/j.artmed.2021.102114
   Petsiuk V, 2018, Arxiv, DOI [arXiv:1806.07421, 10.48550/arXiv.1806.07421]
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shrikumar A, 2017, PR MACH LEARN RES, V70
   Shu ZY, 2022, IEEE T MULTIMEDIA, V24, P1637, DOI 10.1109/TMM.2021.3070977
   Simonyan K, 2014, Arxiv, DOI [arXiv:1312.6034, DOI 10.48550/ARXIV.1312.6034]
   Singh G, 2021, IEEE ACCESS, V9, P41482, DOI 10.1109/ACCESS.2021.3064838
   Song YP, 2022, COMPUT AIDED DESIGN, V146, DOI 10.1016/j.cad.2022.103196
   Srinivas S., 2019, NEURIPS, P4126
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Tang S, 2017, IEEE T MULTIMEDIA, V19, P2105, DOI 10.1109/TMM.2017.2729786
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981
   Yin CX, 2022, IEEE T MULTIMEDIA, V24, P4183, DOI 10.1109/TMM.2021.3114541
   Yuan H, 2022, IEEE T PATTERN ANAL, V44, P2019, DOI 10.1109/TPAMI.2020.3028783
   Zhang DW, 2024, IEEE T NEUR NET LEAR, V35, P5395, DOI 10.1109/TNNLS.2022.3204337
   Zhang LB, 2022, IEEE T MULTIMEDIA, V24, P4409, DOI 10.1109/TMM.2021.3117064
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhao BX, 2021, IEEE T MULTIMEDIA, V23, P1722, DOI 10.1109/TMM.2020.3002614
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 42
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5311
EP 5325
DI 10.1109/TMM.2023.3331572
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600042
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Song, G
   Huang, K
   Su, HW
   Song, FY
   Yang, M
AF Song, Ge
   Huang, Kai
   Su, Hanwen
   Song, Fengyi
   Yang, Ming
TI Deep Ranking Distribution Preserving Hashing for Robust Multi-Label
   Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codes; Semantics; Training; Correlation; Task analysis; Robustness;
   Hamming distances; Cross-modal retrieval; deep hashing;
   out-of-distribution; multi-label
ID NETWORK
AB Deep supervised hashing techniques have exhibited remarkable efficiency in cross-modal retrieval tasks, because they enable the transformation of data from different modalities into compact binary codes that preserve semantic similarity structures. Nonetheless, existing methods often rely on pairwise or triplet relationships within known (or in-distribution) semantics during training, failing to capture the comprehensive ranking information inherent in web data that encompasses diverse concepts. In addition, these methods are vulnerable to out-of-distribution (OOD) semantic data when applied in realistic scenarios, resulting in suboptimal performance. In this paper, we propose ranking distribution preserving hashing (RDPH) to address these problems. We present a novel ranking loss, a differentiable surrogate that maximizes the NDCG metric for cross-modal retrieval. This loss incorporates two target ranking distributions derived from the ideal NDCG scores of samples and the cosine similarity of features. These distributions encourage RDPH to generate hash codes that approximate the desired inter-modal and intra-modal ranking distributions. To enhance the robustness of the hash codes against OOD data, RDPH leverages the CLIP paradigm to acquire OOD-resilient intermediate representations. Besides, we utilize the outlier exposure strategy to enhance the discriminative ability of OOD for hash codes under supervision by constructing auxiliary pseudo-OOD data from known data in feature space. Experiments on three datasets demonstrate that the proposed method achieves state-of-the-art performance on regular retrieval tasks and good results on simulated real-world retrieval tasks.
C1 [Song, Ge; Huang, Kai; Su, Hanwen; Song, Fengyi; Yang, Ming] Nanjing Normal Univ, Sch Comp & Elect Informat, Sch Artificial Intelligence, Nanjing 210023, Peoples R China.
C3 Nanjing Normal University
RP Yang, M (corresponding author), Nanjing Normal Univ, Sch Comp & Elect Informat, Sch Artificial Intelligence, Nanjing 210023, Peoples R China.
EM g.song@njnu.edu.cn; 212202029@njnu.edu.cn; 222212059@njnu.edu.cn;
   f.song@njnu.edu.cn; myang@njnu.edu.cn
OI Song, Fengyi/0000-0001-6587-3038
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai C, 2024, IEEE T NEUR NET LEAR, V35, P4756, DOI 10.1109/TNNLS.2022.3174970
   Bhunia AK, 2022, PROC CVPR IEEE, P989, DOI 10.1109/CVPR52688.2022.00107
   Chen ZD, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1694, DOI 10.1145/3343031.3350862
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Cong Bai, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P525, DOI 10.1145/3372278.3390711
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Fang A, 2022, PR MACH LEARN RES
   Hendrycks D, 2018, P INT C LEARN REPR
   Hu HT, 2020, PROC CVPR IEEE, P3120, DOI 10.1109/CVPR42600.2020.00319
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Hu P, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1721, DOI 10.1145/3343031.3351078
   Huiskes M.J., 2008, P ACM INT C MULT INF, P39, DOI 10.1145/1460096
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Joseph KJ, 2021, PROC CVPR IEEE, P5826, DOI 10.1109/CVPR46437.2021.00577
   Le DH, 2021, ADV NEUR IN, V34
   Li C, 2019, AAAI CONF ARTIF INTE, P176
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li JN, 2022, PR MACH LEARN RES
   Li L, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3712, DOI 10.1145/3503161.3548431
   Li LH, 2022, PROC CVPR IEEE, P10955, DOI 10.1109/CVPR52688.2022.01069
   Li TY, 2022, AAAI CONF ARTIF INTE, P10275
   Liang MY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3548391
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439
   Liu XQ, 2023, IEEE T MULTIMEDIA, V25, P9530, DOI 10.1109/TMM.2023.3254199
   Liu XW, 2019, AAAI CONF ARTIF INTE, P4400
   Ma HY, 2022, PROC CVPR IEEE, P18030, DOI 10.1109/CVPR52688.2022.01752
   Ming Yifei, 2022, P MACHINE LEARNING R
   Mu N, 2022, LECT NOTES COMPUT SC, V13686, P529, DOI 10.1007/978-3-031-19809-0_30
   Qiu Z., 2022, INT C MACH LEARN, P18122
   Radford A, 2021, PR MACH LEARN RES, V139
   Salvador A, 2021, PROC CVPR IEEE, P15470, DOI 10.1109/CVPR46437.2021.01522
   Shi Y., 2021, P BRIT MACH VIS C
   Shi YF, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4767
   Singh A, 2022, PROC CVPR IEEE, P15617, DOI 10.1109/CVPR52688.2022.01519
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tu Junfeng, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P453, DOI 10.1145/3503161.3548187
   Tu RC, 2023, IEEE T MULTIMEDIA, V25, P8946, DOI 10.1109/TMM.2023.3243608
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P1217, DOI 10.1109/TMM.2022.3140656
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Yang EK, 2022, PROC CVPR IEEE, P7541, DOI 10.1109/CVPR52688.2022.00740
   Yang JW, 2022, PROC CVPR IEEE, P19141, DOI 10.1109/CVPR52688.2022.01857
   Yang JK, 2024, Arxiv, DOI [arXiv:2110.11334, DOI 10.48550/ARXIV.2110.11334, 10.48550/arXiv.2110.11334]
   Yang KC, 2023, IEEE I CONF COMP VIS, P2910, DOI 10.1109/ICCV51070.2023.00273
   Yang Y., 2021, IJCAI, P3300
   Yang Y, 2023, IEEE T KNOWL DATA EN, V35, P2736, DOI 10.1109/TKDE.2021.3109131
   Yang Y, 2021, IEEE T KNOWL DATA EN, V33, P682, DOI 10.1109/TKDE.2019.2932742
   Yang Y, 2021, IEEE T KNOWL DATA EN, V33, P696, DOI 10.1109/TKDE.2019.2932666
   Yu E, 2022, NEUROCOMPUTING, V486, P215, DOI 10.1016/j.neucom.2021.11.035
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Yu J, 2021, AAAI CONF ARTIF INTE, V35, P4626
   Zhang HY, 2018, Arxiv, DOI arXiv:1710.09412
   Zhang Z, 2023, IEEE T KNOWL DATA EN, V35, P5091, DOI 10.1109/TKDE.2022.3144352
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu L, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P631, DOI 10.1145/3512527.3531417
NR 59
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7027
EP 7042
DI 10.1109/TMM.2024.3358995
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RE1G6
UT WOS:001225895800001
DA 2024-08-05
ER

PT J
AU Sun, Y
   Ren, ZW
   Hu, P
   Peng, DZ
   Wang, X
AF Sun, Yuan
   Ren, Zhenwen
   Hu, Peng
   Peng, Dezhong
   Wang, Xu
TI Hierarchical Consensus Hashing for Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Consensus learning; cross-modal retrieval; hierarchical hashing;
   learning to hash
ID ROBUST
AB Cross-modal hashing (CMH) has gained much attention due to its effectiveness and efficiency in facilitating efficient retrieval between different modalities. Whereas, most existing methods unconsciously ignore the hierarchical structural information of the data, and often learn a single-layer hash function to directly transform cross-modal data into common low-dimensional hash codes in one step. This sudden drop of dimension and the huge semantic gap can cause the discriminative information loss. To this end, we adopt a coarse-to-fine progressive mechanism and propose a novel <bold>Hierarchical Consensus Cross-Modal Hashing (HCCH)</bold>. Specifically, to mitigate the loss of important discriminative information, we propose a coarse-to-fine hierarchical hashing scheme that utilizes a two-layer hash function to refine the beneficial discriminative information gradually. And then, the $\ell _{2,1}$-norm is imposed on the layer-wise hash function to alleviate the effects of redundant and corrupted features. Finally, we present consensus learning to effectively encode data into a consensus space in such a progressive way, thereby reducing the semantic gap progressively. Through extensive contrast experiments with some advanced CMH methods, the effectiveness and efficiency of our HCCH method are demonstrated on four benchmark datasets.
C1 [Sun, Yuan; Hu, Peng; Peng, Dezhong; Wang, Xu] Sichuan Univ, Coll Comp Sci, Chengdu 610044, Peoples R China.
   [Ren, Zhenwen] Southwest Univ Sci & Technol, Dept Natl Def Sci & Technol, Mianyang 621010, Peoples R China.
   [Peng, Dezhong] Sichuan Zhiqian Technol Co Ltd, Chengdu 610041, Peoples R China.
C3 Sichuan University; Southwest University of Science & Technology - China
RP Wang, X (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610044, Peoples R China.
EM sunyuan_work@163.com; rzw@njust.edu.cn; penghu.ml@gmail.com;
   pengdz@scu.edu.cn; wangxu.scu@gmail.com
OI Sun, Yuan/0000-0002-9376-7248; REN, ZHEN WEN/0000-0003-3791-9750; Hu,
   Peng/0000-0003-3868-3997; Wang, Xu/0000-0002-4821-3334
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen Y, 2022, IEEE T KNOWL DATA EN, V34, P1177, DOI 10.1109/TKDE.2020.2995195
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Hu P, 2021, IEEE T CYBERNETICS, V51, P4982, DOI 10.1109/TCYB.2020.3027614
   Hu P, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1721, DOI 10.1145/3343031.3351078
   Jiang QY, 2019, IEEE T IMAGE PROCESS, V28, P3490, DOI 10.1109/TIP.2019.2897944
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jin S, 2021, IEEE T IMAGE PROCESS, V30, P6130, DOI 10.1109/TIP.2021.3091895
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li CX, 2019, IEEE T MULTIMEDIA, V21, P2863, DOI 10.1109/TMM.2019.2912714
   Li HX, 2023, IEEE T KNOWL DATA EN, V35, P1185, DOI 10.1109/TKDE.2021.3102119
   Li L, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3712, DOI 10.1145/3503161.3548431
   Li X, 2022, IEEE T CIRC SYST VID, V32, P933, DOI 10.1109/TCSVT.2021.3070129
   Lin QB, 2021, IEEE T MULTIMEDIA, V23, P550, DOI 10.1109/TMM.2020.2984081
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Liu JY, 2021, AAAI CONF ARTIF INTE, V35, P8671
   Liu X, 2023, IEEE T MULTIMEDIA, V25, P3811, DOI 10.1109/TMM.2022.3166668
   Liu X, 2022, IEEE T NEUR NET LEAR, V33, P6306, DOI 10.1109/TNNLS.2021.3076684
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Liu XB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1662, DOI 10.1145/3240508.3240683
   Meng M, 2021, IEEE T IMAGE PROCESS, V30, P986, DOI 10.1109/TIP.2020.3038365
   Nie XS, 2021, IEEE T CIRC SYST VID, V31, P401, DOI 10.1109/TCSVT.2020.2974877
   Qin JY, 2022, IEEE T IMAGE PROCESS, V31, P5343, DOI 10.1109/TIP.2022.3195059
   Ren ZW, 2020, IEEE T IMAGE PROCESS, V29, P2094, DOI 10.1109/TIP.2019.2938859
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Shi Y, 2022, IEEE T IMAGE PROCESS, V31, P2755, DOI 10.1109/TIP.2022.3158092
   Teng SH, 2023, IEEE T COMPUT SOC SY, V10, P577, DOI 10.1109/TCSS.2022.3195704
   Wang D, 2018, IEEE T CIRC SYST VID, V28, P2703, DOI 10.1109/TCSVT.2017.2723302
   Wang D, 2019, IEEE T PATTERN ANAL, V41, P2466, DOI 10.1109/TPAMI.2018.2861000
   Wang S, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2022.102886
   Wang X, 2022, IEEE T CYBERNETICS, V52, P1588, DOI 10.1109/TCYB.2020.2984489
   Wang YX, 2022, IEEE T CIRC SYST VID, V32, P8822, DOI 10.1109/TCSVT.2022.3195874
   Wang YX, 2022, IEEE T CYBERNETICS, V52, P10064, DOI 10.1109/TCYB.2021.3059886
   Wang YX, 2021, IEEE T KNOWL DATA EN, V33, P3507, DOI 10.1109/TKDE.2020.2974825
   Yu J, 2021, AAAI CONF ARTIF INTE, V35, P4626
   Zhan JW, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102753
   Zhan YW, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108262
   Zhan YW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3386, DOI 10.1145/3394171.3413962
   Zhang CQ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3054
   Zhang C, 2023, IEEE T KNOWL DATA EN, V35, P6475, DOI 10.1109/TKDE.2022.3172216
   Zhang DL, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108343
   Zhang DL, 2022, IEEE T CYBERNETICS, V52, P5947, DOI 10.1109/TCYB.2020.3032017
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zhen LL, 2022, IEEE T NEUR NET LEAR, V33, P798, DOI 10.1109/TNNLS.2020.3029181
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zhou T, 2023, IEEE T PATTERN ANAL, V45, P197, DOI 10.1109/TPAMI.2022.3147841
   Zhuo YX, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P158, DOI 10.1145/3512527.3531381
NR 46
TC 21
Z9 21
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 824
EP 836
DI 10.1109/TMM.2023.3272169
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700010
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Tang, T
   Yin, ZY
   Li, J
   Wang, HG
   Wu, DP
   Wang, RY
AF Tang, Tong
   Yin, Zhiyang
   Li, Jie
   Wang, Honggang
   Wu, Dapeng
   Wang, Ruyan
TI End-to-End Distortion Modeling for Error-Resilient Screen Content Video
   Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Encoding; Streaming media; Decoding; Image coding; Video
   coding; Estimation; Screen content video; error resilient coding;
   distortion modeling; error concealment
ID OPTIMIZATION; ALLOCATION; QUALITY; HEVC
AB To improve the compression performance of screen content coding, extension coding standards (HEVC-SCC, VVC-SCC) have been developed. However, considering the compression ratio alone may lead to packet losses in bitstreams which may cause plenty of images decoded incorrectly, degrading the video quality at the receiver side. Thus, it urgently needs to study source-channel jointly coding scheme of screen content video. The most significant challenge lies in the complex spatial-temporal characteristics of screen content video, which complicate the creation of an accurate end-to-end distortion model. In this article, we delve into the traits of screen content video and construct an end-to-end distortion model. Building upon this, we introduce an error resilient coding scheme specifically for screen content video. More specifically, we first consider the characteristic of non-stationary temporal domain variation and classify the screen content images into three types of frames using a fast block-searching method. We then propose an adaptive error concealment method, taking into account the spatial-temporal prediction characteristics. Following this, we derive a pixel-level end-to-end distortion model and incorporate it into the rate distortion optimization process. Our experimental results reveal that, compared to state-of-the-art methods, our proposed method significantly enhances both objective and subjective quality across a variety of channel conditions.
C1 [Tang, Tong; Yin, Zhiyang; Li, Jie; Wu, Dapeng; Wang, Ruyan] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Tang, Tong; Yin, Zhiyang; Li, Jie; Wu, Dapeng; Wang, Ruyan] Adv Network & Intelligent Interconnect Technol Key, Chongqing 400065, Peoples R China.
   [Tang, Tong; Yin, Zhiyang; Li, Jie; Wu, Dapeng; Wang, Ruyan] Chongqing Key Lab Ubiquitous Sensing & Networking, Chongqing 400065, Peoples R China.
   [Wang, Honggang] Univ Massachusetts Dartmouth, Elect & Comp Engn Dept, N Dartmouth, MA 02747 USA.
C3 Chongqing University of Posts & Telecommunications; University of
   Massachusetts System; University Massachusetts Dartmouth
RP Wu, DP (corresponding author), Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
EM tangtong@cqupt.edu.cn; yzy2022@gmail.com; lijie0093@gmail.com;
   hwang1@umassed.edu; wudp@cqupt.edu.cn; wangry@cqupt.edu.cn
RI Wu, Dapeng/IWE-0674-2023; TANG, TONG/KRO-7674-2024
OI Wu, Dapeng/0000-0003-2105-9418; , Tong/0000-0003-0616-2003
FU National Natural Science Foundation of China
FX No Statement Available
CR Akbari A, 2020, IEEE T CIRC SYST VID, V30, P2559, DOI 10.1109/TCSVT.2019.2927912
   Alariaoet N. Z., 2019, PROC IEEE 11 INT C H, P1
   [Anonymous], 2023, Video coding utilities (VCU)
   [Anonymous], 2023, HM reference software 16.7
   Benjak M, 2021, IEEE IMAGE PROC, P2114, DOI 10.1109/ICIP42928.2021.9506399
   Bjntegaard G., 2001, Doc. VCEG-M33
   Carreira JFM, 2019, IEEE T BROADCAST, V65, P282, DOI 10.1109/TBC.2018.2865644
   Cheng S, 2020, IEEE T IMAGE PROCESS, V29, P8636, DOI 10.1109/TIP.2020.3018256
   Chung B, 2020, IEEE T CIRC SYST VID, V30, P1535, DOI 10.1109/TCSVT.2019.2909564
   Fan QQ, 2020, IEEE T COMPUT, V69, P239, DOI 10.1109/TC.2019.2946795
   Gao P, 2019, IEEE T CIRC SYST VID, V29, P3326, DOI 10.1109/TCSVT.2018.2877903
   Gao W, 2018, PICT COD SYMP, P194, DOI 10.1109/PCS.2018.8456257
   Genser N, 2020, IEEE IMAGE PROC, P988, DOI 10.1109/ICIP40778.2020.9191038
   Guo Y., 2015, PROC IEEE VIS COMMUN, P1
   Hegazy RD, 2020, CONF REC ASILOMAR C, P693, DOI 10.1109/IEEECONF51394.2020.9443375
   Kazemi M, 2020, IEEE T MULTIMEDIA, V22, P2193, DOI 10.1109/TMM.2019.2957991
   Kazemi M, 2018, IEEE T MULTIMEDIA, V20, P781, DOI 10.1109/TMM.2017.2758578
   Khalfa Sara, 2022, 2022 19th International Multi-Conference on Systems, Signals & Devices (SSD), P464, DOI 10.1109/SSD54932.2022.9955746
   Kulupana G, 2021, IEEE T CONSUM ELECTR, V67, P107, DOI 10.1109/TCE.2021.3069464
   Kulupana G, 2019, IEEE T CIRC SYST VID, V29, P3367, DOI 10.1109/TCSVT.2018.2879956
   Kulupana G, 2018, IEEE ICCE
   Kwasinski A., 2023, PROC JOINT SOURCE CH, P111
   Li JH, 2018, IEEE T CIRC SYST VID, V28, P1369, DOI 10.1109/TCSVT.2017.2657758
   Li TS, 2021, IEEE T BROADCAST, V67, P159, DOI 10.1109/TBC.2020.3028340
   Li ZD, 2022, IEEE INTERNET THINGS, V9, P16941, DOI 10.1109/JIOT.2022.3143506
   Liu Z, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10144923
   Man Xu, 2022, 2022 7th International Conference on Signal and Image Processing (ICSIP), P707, DOI 10.1109/ICSIP55141.2022.9886385
   Tang T, 2022, IEEE T IMAGE PROCESS, V31, P2463, DOI 10.1109/TIP.2022.3152003
   Vajha M., 2021, PROC IEEE INF THEORY, P1
   Wang TY, 2021, IEEE T IMAGE PROCESS, V30, P1245, DOI 10.1109/TIP.2020.3043124
   Wang TY, 2018, IEEE ACCESS, V6, P71279, DOI 10.1109/ACCESS.2018.2879867
   Wang W, 2008, IEEE T MULTIMEDIA, V10, P1169, DOI 10.1109/TMM.2008.2001354
   Wu DP, 2021, IEEE T MULTIMEDIA, V23, P2208, DOI 10.1109/TMM.2021.3066050
   Xi Huang, 2021, 2021 8th International Conference on Information, Cybernetics, and Computational Social Systems (ICCSS), P420, DOI 10.1109/ICCSS53909.2021.9722004
   Xu JJ, 2018, IEEE IMAGE PROC, P3294, DOI 10.1109/ICIP.2018.8451175
   Xu XZ, 2022, IEEE T CIRC SYST VID, V32, P839, DOI 10.1109/TCSVT.2021.3064210
   Yao JC, 2019, IEEE T BROADCAST, V65, P546, DOI 10.1109/TBC.2018.2878360
   Yu H., 2014, Committee Input Doc. JCTVC-S1015
   Zamani S, 2016, 2016 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP)
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P1806, DOI 10.1109/TCSVT.2018.2851252
   Zhang Y, 2007, IEEE T MULTIMEDIA, V9, P445, DOI 10.1109/TMM.2006.887989
   Zhou ML, 2023, IEEE T IMAGE PROCESS, V32, P219, DOI 10.1109/TIP.2022.3224876
NR 42
TC 5
Z9 5
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4458
EP 4468
DI 10.1109/TMM.2023.3323895
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100011
DA 2024-08-05
ER

PT J
AU Wang, LF
   Zhan, YB
   Liu, W
   Yu, BS
   Tao, DP
AF Wang, Linfei
   Zhan, Yibing
   Liu, Wei
   Yu, Baosheng
   Tao, Dapeng
TI Bounding Box Vectorization for Oriented Object Detection With Tanimoto
   Coefficient Regression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Detectors; Measurement; Three-dimensional displays;
   Task analysis; Shape; Convergence; Bounding box regression; implicit
   vector representation; Oriented object detection
AB Current oriented object detection methods mainly utilize a vanilla coordinate-angle representation for bounding box regression, which usually suffers from inconsistency between the bounding box regression losses and prediction errors induced with respect to different rotation angles, aspect ratios, and scales. Therefore, although the existing oriented object detectors have achieved very good performances under coarse evaluation metrics such as AP(50), their performance significantly degrades when using stricter evaluation metric such as AP(75). To address the abovementioned issues, we propose a new regression method with bounding box vectorization that implicitly represents the shape and orientation of an object with a set of orthogonal vectors. By doing this, the proposed method delicately avoids the inconsistency issues encountered in oriented bounding box regression. During training, we introduce the Tanimoto coefficient to evaluate the similarity of the bounding box vector in a shape- and orientation-aware manner, and we refer to the proposed box-to-vector loss as the B2V loss. In addition to 2D object detection, the proposed method can be easily generalized to 3D scenarios involving orientation estimation, such as autonomous driving. We evaluate the proposed method through extensive experiments conducted on four popular oriented object detection datasets, including both 2D and 3D datasets, where the proposed method significantly outperforms the recently developed state-of-the-art methods when using a more accurate evaluation metric.
C1 [Wang, Linfei; Tao, Dapeng] Yunnan Univ, Sch Informat Sci & Engn, FIST LAB, Kunming 650091, Peoples R China.
   [Wang, Linfei] Yunnan United Vis Innovat Technol Co Ltd, Kunming 650504, Peoples R China.
   [Zhan, Yibing] JD Explore Acad, Beijing 100000, Peoples R China.
   [Liu, Wei] Tencent AI Lab, Shenzhen 518000, Peoples R China.
   [Yu, Baosheng] Univ Sydney, Camperdown, NSW 2006, Australia.
   [Tao, Dapeng] Yunnan Key Lab Media Convergence, Kunming 650032, Yunnan, Peoples R China.
C3 Yunnan University; Tencent; University of Sydney
RP Tao, DP (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, FIST LAB, Kunming 650091, Peoples R China.
EM linfei.w@outlook.com; zybjy@mail.ustc.edu.cn; wl2223@columbia.edu;
   baosheng.yu@sydney.edu.au; dapeng.tao@gmail.com
RI ; Tao, Dapeng/E-8649-2013
OI Liu, Wei/0000-0002-3865-8145; Tao, Dapeng/0000-0003-0783-5273; Wang,
   Linfei/0000-0003-1917-9031
FU Yunnan Provincial Major Science and Technology Special Plan
FX No Statement Available
CR Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen TI, 2023, IEEE T MULTIMEDIA, V25, P291, DOI 10.1109/TMM.2021.3125195
   Chen Z., 2020, P 16 EUR C COMP VIS, P195
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Ding J, 2019, PROC CVPR IEEE, P2844, DOI 10.1109/CVPR.2019.00296
   Ding L., 2021, P INT C LEARN REPR, P1
   Ding L, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P2797
   Du B, 2022, INT J COMPUT VISION, V130, P1961, DOI 10.1007/s11263-022-01616-6
   Flower DR, 1998, J CHEM INF COMP SCI, V38, P379, DOI 10.1021/ci970437z
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Han J., 2022, IEEE Trans. Geosci. Remote Sens., V60
   Han JM, 2021, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR46437.2021.00281
   Hou LP, 2022, IEEE T IMAGE PROCESS, V31, P1545, DOI 10.1109/TIP.2022.3143690
   Huang ZC, 2022, IEEE T IMAGE PROCESS, V31, P1895, DOI 10.1109/TIP.2022.3148874
   Jiang YY, 2017, Arxiv, DOI [arXiv:1706.09579, DOI 10.48550/ARXIV.1706.09579]
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li WT, 2022, PROC CVPR IEEE, P1819, DOI 10.1109/CVPR52688.2022.00187
   Liu ZK, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P324, DOI 10.5220/0006120603240331
   Pan Xingjia, 2020, P IEEE CVF C COMP VI, P11207
   Qian W, 2021, AAAI CONF ARTIF INTE, V35, P2458
   Ru LX, 2022, PROC CVPR IEEE, P16825, DOI 10.1109/CVPR52688.2022.01634
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang FF, 2023, IEEE T IMAGE PROCESS, V32, P1, DOI 10.1109/TIP.2022.3201467
   Wang GA, 2022, LECT NOTES COMPUT SC, V13685, P110, DOI 10.1007/978-3-031-19806-9_7
   Wang W, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1730, DOI 10.1145/3474085.3475317
   Whittle M, 2004, J CHEM INF COMP SCI, V44, P1840, DOI 10.1021/ci049867x
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Xie XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3500, DOI 10.1109/ICCV48922.2021.00350
   Xu YC, 2021, IEEE T PATTERN ANAL, V43, P1452, DOI [10.1109/TPAMI.2020.2974745, 10.1109/TGRS.2020.3026387]
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yang X., 2023, P C LEARN REPR, P1
   Yang X, 2023, IEEE T PATTERN ANAL, V45, P2384, DOI 10.1109/TPAMI.2022.3166956
   Yang X, 2023, IEEE T PATTERN ANAL, V45, P4335, DOI 10.1109/TPAMI.2022.3197152
   Yang X, 2021, PROC CVPR IEEE, P15814, DOI 10.1109/CVPR46437.2021.01556
   Yang X, 2021, PR MACH LEARN RES, V139
   Yang X, 2021, AAAI CONF ARTIF INTE, V35, P3163
   Yu BS, 2022, IEEE T PATTERN ANAL, V44, P8276, DOI 10.1109/TPAMI.2021.3103980
   Yu BS, 2019, IEEE T IMAGE PROCESS, V28, P2490, DOI 10.1109/TIP.2018.2886790
   Yu J, 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Yu Y, 2023, PROC CVPR IEEE, P13354, DOI 10.1109/CVPR52729.2023.01283
   Yu Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P464, DOI 10.1007/978-3-030-58565-5_28
   Zhan YB, 2018, IEEE T MULTIMEDIA, V20, P1796, DOI 10.1109/TMM.2017.2780770
   Zhang GJ, 2019, IEEE T GEOSCI REMOTE, V57, P10015, DOI 10.1109/TGRS.2019.2930982
   Zhang MJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1730, DOI 10.1145/3503161.3547817
   Zhang TW, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13183690
   Zhong Q., 2022, P FIND EMP METH NAT, P4064
   Zhou DF, 2019, INT CONF 3D VISION, P85, DOI 10.1109/3DV.2019.00019
   Zhou Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P7331, DOI 10.1145/3503161.3548541
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
NR 51
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5181
EP 5193
DI 10.1109/TMM.2023.3330103
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600024
DA 2024-08-05
ER

PT J
AU Wang, YH
   Ye, B
   Cai, ZC
AF Wang, Yuanhui
   Ye, Ben
   Cai, Zhanchuan
TI Dynamic Template Updating Using Spatial-Temporal Information in Siamese
   Trackers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object tracking; siamese tracker; dynamic template updating;
   spatial-temporal information; tracking confidence network
ID TRACKING; NETWORKS
AB Siamese trackers usually use the target in the first frame as a fixed template, but the static template cannot adapt to target changes. The existing updater is challenging to deal with target deformation and update noise, and there is an excellent risk of updating with an inaccurate updater. In our research, a dynamic template updating strategy based on spatial-temporal information is proposed to improve the tracking accuracy of the Siamese tracker. Furthermore, Tracking Confidence Network (TCNet) is proposed to judge whether to update, which ensures that high-quality target features are used to update and reduce the noise caused by adding unreliable targets. In experiments, the proposed method is embedded into two baseline trackers: SiamRPN and SiamFC++, and tested on five popular benchmarks. The experimental results show that the proposed method can improve the performance of the Siamese trackers while maintaining real-time speed.
C1 [Wang, Yuanhui; Ye, Ben; Cai, Zhanchuan] Macau Univ Sci & Technol, Sch Comp Sci & Engn, Macau 999078, Peoples R China.
   [Wang, Yuanhui] Zhuhai Coll Sci & Technol, Sch Comp Sci, Zhuhai 519041, Guangdong, Peoples R China.
C3 Macau University of Science & Technology
RP Ye, B (corresponding author), Macau Univ Sci & Technol, Sch Comp Sci & Engn, Macau 999078, Peoples R China.
EM wangyh400@163.com; zhyeben@126.com; zccai@must.edu.mo
OI Ye, Ben/0000-0002-3781-7597
FU Science and Technology Development Fund of Macau
FX No Statement Available
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cao ZA, 2022, PROC CVPR IEEE, P14778, DOI 10.1109/CVPR52688.2022.01438
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633
   Dong XP, 2023, IEEE T PATTERN ANAL, V45, P8049, DOI 10.1109/TPAMI.2022.3230064
   Fan BJ, 2022, IEEE T MULTIMEDIA, V24, P2766, DOI 10.1109/TMM.2021.3087347
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han G, 2023, IEEE T MULTIMEDIA, V25, P430, DOI 10.1109/TMM.2021.3127357
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Javed S, 2023, IEEE T PATTERN ANAL, V45, P6552, DOI [10.1109/IECON49645.2022.9969084, 10.1109/TPAMI.2022.3212594]
   Jung I., 2018, P ECCV, P83
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H., 2016, Modeling and propagating CNNs in a tree structure for visual tracking
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Sun X., 2021, UPDATABLE SIAMESE TR
   Tang F, 2022, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR52688.2022.00854
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xi M, 2022, IEEE T MULTIMEDIA, V24, P2791, DOI 10.1109/TMM.2021.3087340
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yang B, 2019, ADV NEUR IN, V32
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 42
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2006
EP 2015
DI 10.1109/TMM.2023.3291140
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800030
DA 2024-08-05
ER

PT J
AU Wen, WY
   Yuan, ZY
   Qi, SR
   Zhang, YS
   Fang, YM
AF Wen, Wenying
   Yuan, Ziye
   Qi, Shuren
   Zhang, Yushu
   Fang, Yuming
TI PPM-SEM: A Privacy-Preserving Mechanism for Sharing Electronic Patient
   Records and Medical Images in Telemedicine
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Privacy protection; medical image; CycleGAN; dual watermark; reversible
   data hiding
ID WATERMARKING; ROBUST
AB Despite the various privacy protection methods that are available through medical services platforms, it is still challenging for patients to achieve a desirable level of privacy protection during image sharing. Therefore, this article proposes a privacy protection mechanism, called PPM-SEM, for the secure sharing of electronic patient records (EPRs) and medical images in telemedicine; it includes two stages: Privacy preparation and privacy protection and reconstruction. In the first stage, a dual watermark (i.e., an image watermark) is generated by combining the patient's EPRs with an image, which can be utilized to ensure the security of patient identity data (i.e., a text watermark). In the second stage, a modal transformation network is constructed by training the dual watermark together as an additional channel. This network is called watermark-CycleGAN (W-CycleGAN), which can address the privacy and security issues concerning medical images and provide a double protection mechanism for EPRs. Experimental results demonstrate that only the recovery network with the correct key can restore high-quality medical images. In addition, the patient's EPRs can be fully extracted; i.e., 100$\%$ accuracy can be maintained. It is noted that the nonpaired recovery network can also recover visually meaningful medical images, thereby realizing privacy protection for patients in telemedicine scenarios.
C1 [Wen, Wenying; Yuan, Ziye; Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330013, Peoples R China.
   [Qi, Shuren; Zhang, Yushu] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
C3 Jiangxi University of Finance & Economics; Nanjing University of
   Aeronautics & Astronautics
RP Zhang, YS (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM wenyingwen@sina.cn; yzy_stu@sina.com; shurenqi@nuaa.edu.cn;
   yushu@nuaa.edu.cn; leo.fangyuming@foxmail.com
OI Wen, wenying/0000-0002-3098-4640; zhang, yushu/0000-0001-8183-8435
FU Natural Science Foundation of China
FX No Statement Available
CR Anand A, 2023, IEEE T DEPEND SECURE, V20, P859, DOI 10.1109/TDSC.2022.3144657
   Anand A, 2023, IEEE T COMPUT SOC SY, V10, P2033, DOI 10.1109/TCSS.2022.3140862
   Anand A, 2022, IEEE T COMPUT SOC SY, V9, P1265, DOI 10.1109/TCSS.2021.3125025
   Anand A, 2020, COMPUT COMMUN, V152, P72, DOI 10.1016/j.comcom.2020.01.038
   Baid U, 2021, Arxiv, DOI [arXiv:2107.02314, DOI 10.48550/ARXIV.2107.02314]
   Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Baluja S, 2017, ADV NEUR IN, V30
   Bilic P, 2023, MED IMAGE ANAL, V84, DOI 10.1016/j.media.2022.102680
   Cheng Q, 2009, IEEE T CIRC SYST VID, V19, P978, DOI 10.1109/TCSVT.2009.2020255
   Chowdhury MEH, 2020, IEEE ACCESS, V8, P132665, DOI 10.1109/ACCESS.2020.3010287
   Das J., 2021, P IEEE CVF C COMP VI
   Dong P, 2005, IEEE T IMAGE PROCESS, V14, P2140, DOI 10.1109/TIP.2005.857263
   Dong Y, 2023, IEEE T MULTIMEDIA, V25, P2698, DOI 10.1109/TMM.2022.3150180
   Gong X, 2023, IEEE T MED IMAGING, V42, P2057, DOI 10.1109/TMI.2022.3213244
   Guan ZY, 2023, IEEE T PATTERN ANAL, V45, P372, DOI 10.1109/TPAMI.2022.3141725
   Haddad S, 2020, IEEE T INF FOREN SEC, V15, P2556, DOI 10.1109/TIFS.2020.2972159
   He WG, 2021, IEEE T MULTIMEDIA, V23, P52, DOI 10.1109/TMM.2020.2982042
   Hu RW, 2022, IEEE T PATTERN ANAL, V44, P10196, DOI 10.1109/TPAMI.2021.3131250
   Hu RW, 2021, IEEE T IMAGE PROCESS, V30, P318, DOI 10.1109/TIP.2020.3036727
   Ke Y, 2020, IEEE T CIRC SYST VID, V30, P2353, DOI 10.1109/TCSVT.2019.2963393
   Kim BN, 2021, IEEE T MED IMAGING, V40, P1737, DOI 10.1109/TMI.2021.3065727
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Ma ZH, 2021, IEEE T CIRC SYST VID, V31, P4826, DOI 10.1109/TCSVT.2021.3055255
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Peng F, 2023, IEEE T MULTIMEDIA, V25, P892, DOI 10.1109/TMM.2021.3134159
   Qi Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13853, DOI 10.1109/CVPR42600.2020.01387
   Rahman T, 2021, COMPUT BIOL MED, V132, DOI 10.1016/j.compbiomed.2021.104319
   Tamimi AA, 2013, INT J ADV COMPUT SC, V4, P18
   Thakur S, 2020, LECT NOTES ELECTR EN, V587, P897, DOI 10.1007/978-981-32-9775-3_80
   Wei JH, 2021, IEEE T EMERG TOP COM, V9, P1109, DOI 10.1109/TETC.2020.2970094
   Wen WY, 2023, IEEE T COMPUT SOC SY, V10, P3169, DOI 10.1109/TCSS.2022.3218883
   Weng XY, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P87, DOI 10.1145/3323873.3325011
   Wu HZ, 2021, IEEE T CIRC SYST VID, V31, P2591, DOI 10.1109/TCSVT.2020.3030671
   Wu YD, 2023, IEEE T DEPEND SECURE, V20, P1744, DOI 10.1109/TDSC.2022.3162623
   Xia ZH, 2016, MULTIMED TOOLS APPL, V75, P1947, DOI 10.1007/s11042-014-2381-8
   Yan F, 2022, IEEE T IND INFORM, V18, P8885, DOI 10.1109/TII.2022.3159863
   Yin ZX, 2022, IEEE T DEPEND SECURE, V19, P992, DOI 10.1109/TDSC.2020.3019490
   Zhang LP, 2017, IEEE J BIOMED HEALTH, V21, P465, DOI 10.1109/JBHI.2016.2517146
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 40
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5795
EP 5806
DI 10.1109/TMM.2023.3339588
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100020
DA 2024-08-05
ER

PT J
AU Wu, H
   Fang, LC
   Yu, Q
   Yang, CZ
AF Wu, Hao
   Fang, Lincong
   Yu, Qian
   Yang, Chengzhuan
TI Learning Robust Point Representation for 3D Non-Rigid Shape Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Local point histogram; high-level point feature; shape descriptor; 3D
   non-rigid shape retrieval
ID CLASSIFICATION; DESCRIPTORS; RECOGNITION
AB Content-based 3D object retrieval is a challenging problem in computer vision and graphics, especially for non-rigid 3D shapes. This article proposes a multiview-based robust point representation approach for 3D non-rigid shape retrieval. First, we propose an efficient local descriptor called the local point histogram, which is robust to non-rigid changes in shape. Second, we encode local point histogram features into high-level point features (HPF) using Fisher vectors. Finally, we present an efficient feature fusion method that can further enhance the performance of 3D non-rigid shape retrieval. We extensively tested our approach on two benchmark 3D non-rigid shape datasets, including the SHREC2015 non-rigid shape and SHREC2015 canonical forms. Our method achieves 98.33% and 90.55% retrieval accuracy on the SHREC2015 non-rigid shape and SHREC2015 canonical forms datasets, surpassing previous state-of-the-art methods by nearly 2% and 7%, respectively. In addition, we further tested our method on the well-known 3D rigid shape dataset ModelNet, and the experimental results demonstrate that our method is also effective for 3D rigid shape retrieval. We also combine the proposed HPF shape features with deep convolutional features for the 3D rigid shape retrieval task, achieving a retrieval performance comparable to the prior state-of-the-art methods, which indicates a strong complementarity between HPF shape features and deep convolutional features.
C1 [Wu, Hao] Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
   [Fang, Lincong] Zhejiang Univ Finance & Econ, Sch Informat, Hangzhou 310018, Peoples R China.
   [Yu, Qian] Fudan Univ, Sch Comp Sci, Shanghai 200438, Peoples R China.
   [Yu, Qian] Jiangsu Univ Technol, Sch Comp Engn, Changzhou 213001, Peoples R China.
   [Yang, Chengzhuan] Zhejiang Normal Univ, Sch Comp Sci & Technol, Jinhua 321004, Peoples R China.
C3 Beijing Normal University; Zhejiang University of Finance & Economics;
   Fudan University; Jiangsu University of Technology; Zhejiang Normal
   University
RP Yang, CZ (corresponding author), Zhejiang Normal Univ, Sch Comp Sci & Technol, Jinhua 321004, Peoples R China.
EM wuhao@bnu.edu.cn; lincongfang@zufe.edu.cn; yuqian@jsut.edu.cn;
   czyang@zjnu.edu.cn
OI Yu, Qian/0000-0002-6224-5607; Fang, Lincong/0000-0002-9847-4436; Yang,
   Chengzhuan/0000-0002-6675-9074
FU National Nature Science Foundation of China
FX No Statement Available
CR Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bai SJ, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104756
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Barra V, 2013, PATTERN RECOGN, V46, P2985, DOI 10.1016/j.patcog.2013.03.019
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Ghodrati H, 2016, INT J MULTIMED INF R, V5, P151, DOI 10.1007/s13735-016-0103-x
   Han ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3707, DOI 10.1109/TIP.2017.2704426
   Hegde S. B., 2019, PROC BRIT MACH VIS C, P1
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Kuang ZZ, 2019, IEEE T MULTIMEDIA, V21, P3164, DOI 10.1109/TMM.2019.2918729
   Li CY, 2014, MULTIMEDIA SYST, V20, P253, DOI 10.1007/s00530-013-0318-0
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Lian Z., 2015, 8 EUR WORKSH 3D OBJ, P107
   Lian ZH, 2013, MACH VISION APPL, V24, P1685, DOI 10.1007/s00138-013-0501-5
   Lian ZH, 2013, INT J COMPUT VISION, V102, P221, DOI 10.1007/s11263-012-0548-1
   Lian ZH, 2010, IEEE IMAGE PROC, P3181, DOI 10.1109/ICIP.2010.5654226
   Limberger FA, 2018, COMPUT VIS IMAGE UND, V172, P1, DOI 10.1016/j.cviu.2018.04.002
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P8809, DOI 10.1109/TCSVT.2022.3191761
   Mahmoudi M, 2009, GRAPH MODELS, V71, P22, DOI 10.1016/j.gmod.2008.10.002
   Min P., 2003, PROC WEB3D S, P7
   Mohamed HH, 2021, 36TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING, SAC 2021, P1070, DOI 10.1145/3412841.3441984
   Mohamed HH, 2018, APPL INTELL, V48, P2873, DOI 10.1007/s10489-017-1114-x
   Mu Jiteng, 2021, P IEEECVF INT C COMP, P13001
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1962, DOI 10.1109/TMM.2020.3006371
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Ovsjanikov Maks, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P320, DOI 10.1109/ICCVW.2009.5457682
   Pan XA, 2011, PATTERN RECOGN LETT, V32, P787, DOI 10.1016/j.patrec.2011.01.003
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Pickup D, 2016, INT J COMPUT VISION, V120, P169, DOI 10.1007/s11263-016-0903-8
   Pickup D., 2015, EUROGRAPHICS WORKSHO, P1
   Pickup D., 2016, Comput. Vis. Media, V2, P231
   Pickup D, 2015, PATTERN RECOGN, V48, P2500, DOI 10.1016/j.patcog.2015.02.021
   Qingqing Zhang, 2019, 2019 10th International Conference on Information Technology in Medicine and Education (ITME). Proceedings, P607, DOI 10.1109/ITME.2019.00141
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rustamov R.M., 2007, P 5 EUR S GEOM PROC, V257, P225, DOI DOI 10.2312/SGP/SGP07/225-233
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Sfikas K., 2017, PROC EUROGRAPHICS WO, P1
   Sfikas K, 2012, VISUAL COMPUT, V28, P943, DOI 10.1007/s00371-012-0714-z
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Shi D., 2020, P INT C MACH LEARN, P8828
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Vranic D. V., 2003, PROC INT C IMAGE PRO, pIII
   Wang C, 2019, NEUROCOMPUTING, V323, P139, DOI 10.1016/j.neucom.2018.09.075
   Wang LQ, 2023, MACH INTELL RES, V20, P872, DOI 10.1007/s11633-023-1430-z
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Yang CZ, 2020, IEEE ACCESS, V8, P157587, DOI 10.1109/ACCESS.2020.3019800
   Yu Q, 2020, NEURAL PROCESS LETT, V52, P581, DOI 10.1007/s11063-020-10268-x
   Yu R., 2018, PROC EUR C COMPUT VI, P1
   Zhou K., 2021, PROC INT C LEARN REP
   Zhu YR, 2019, PROCEEDINGS OF THE 12TH INTERNATIONAL SYMPOSIUM ON VISUAL INFORMATION COMMUNICATION AND INTERACTION, VINCI 2019, DOI 10.1145/3356422.3356436
NR 62
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4430
EP 4444
DI 10.1109/TMM.2023.3323154
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100043
DA 2024-08-05
ER

PT J
AU Xu, RT
   Wang, CW
   Xu, SB
   Meng, WL
   Zhang, XP
AF Xu, Rongtao
   Wang, Changwei
   Xu, Shibiao
   Meng, Weiliang
   Zhang, Xiaopeng
TI Wave-Like Class Activation Map With Representation Fusion for
   Weakly-Supervised Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Class activation map; representation fusion; wave function; weakly
   supervised semantic segmentation
AB The Class Activation Map (CAM) is widely used to generate pseudo-labels for Weakly Supervised Semantic Segmentation (WSSS), while it does not adequately consider the modeling of foreground-independent information, resulting in prone to false positive pixels. In this paper, we propose a Wave-like Class Activation Map (WaveCAM) from the perspective of representation fusion and dynamic aggregation representation to alleviate the above problem. Specifically, our WaveCAM includes the foreground-aware representation modeling that enhances perception of foreground information, and the foreground-independent representation modeling that enhances perception of foreground-independent information, and a representation-adaptive fusion module that fuses the two representations. Both representations are expressed as wave functions with amplitude and phase to dynamically aggregate representations and extract semantic information after initialization, and they are fused through the adaptive fusion module to obtain an output containing rich semantic information. Extensive experiments on PASCAL VOC 2012 dataset and MS COCO 2014 dataset validate that our WaveCAM can easily embed multi-stage WSSS and end-to-end WSSS, achieving the state-of-the-art performance.
C1 [Xu, Rongtao; Wang, Changwei; Meng, Weiliang; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Xu, Rongtao; Wang, Changwei; Meng, Weiliang; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
   [Xu, Shibiao] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Beijing University of Posts & Telecommunications
RP Meng, WL (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.; Meng, WL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
EM xurongtao2019@ia.ac.cn; wangchangwei2019@ia.ac.cn;
   shibiaoxu@bupt.edu.cn; weiliang.meng@ia.ac.cn; xiaopeng.zhang@ia.ac.cn
OI wang, changwei/0000-0001-8259-7717; meng, wei liang/0000-0002-3221-4981;
   Xu, Rongtao/0000-0003-4619-9679
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Akiva P, 2021, Arxiv, DOI arXiv:2106.10309
   Al-Huda Z, 2021, INT J PATTERN RECOGN, V35, DOI 10.1142/S0218001421540264
   Araslanov N, 2020, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR42600.2020.00431
   Chen HJ, 2021, SEMINAR LEARNING CLI, P6900, DOI [10.1109/ICCV48922.2021.00684, DOI 10.1109/ICCV48922.2021.00684]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Chen ZZ, 2022, PROC CVPR IEEE, P959, DOI 10.1109/CVPR52688.2022.00104
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong J., 2021, IEEE Trans. Pattern Anal. Mach. Intell, DOI [10.1109/TPAM1.2021.3128560, DOI 10.1109/TPAM1.2021.3128560]
   Dong JH, 2020, PROC CVPR IEEE, P4022, DOI 10.1109/CVPR42600.2020.00408
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan JS, 2020, PROC CVPR IEEE, P4282, DOI 10.1109/CVPR42600.2020.00434
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   Hou QB, 2018, ADV NEUR IN, V31
   Jacobs K, 2006, CONTEMP PHYS, V47, P279, DOI 10.1080/00107510601101934
   Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216
   Kim B, 2021, AAAI CONF ARTIF INTE, V35, P1754
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   Krahenbuhl P., 2011, NeurIPS, V24
   Lee J., 2021, P IEEE CVF C COMP VI, P4071
   Lee J, 2021, PROC CVPR IEEE, P2643
   Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541
   Lee Jungbeom, 2021, Advances in Neural Information Processing Systems, V34
   Lee S, 2021, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR46437.2021.00545
   Li JL, 2023, IEEE T MULTIMEDIA, V25, P1686, DOI 10.1109/TMM.2022.3152388
   Li KP, 2019, IEEE I CONF COMP VIS, P5197, DOI 10.1109/ICCV.2019.00530
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu PD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2995, DOI 10.1145/3474085.3475217
   Loshchilov I., 2018, INT C LEARN REPR
   Lovasz L., 1993, Combinatorics, Paul erdos is eighty, V1, P1
   Meng M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3365, DOI 10.1109/ICCV48922.2021.00337
   Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203
   Ru LX, 2022, PROC CVPR IEEE, P16825, DOI 10.1109/CVPR52688.2022.01634
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142
   Su R., 2021, P IEEE CVF INT C COM, P7004
   Tang YH, 2022, PROC CVPR IEEE, P10925, DOI 10.1109/CVPR52688.2022.01066
   Vaswani A, 2017, ADV NEUR IN, V30
   Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315
   Wang C., 2022, PROC IEEE INT C MULT, P1
   Wang CW, 2022, AAAI CONF ARTIF INTE, P2388
   Wang CW, 2023, IEEE T MULTIMEDIA, V25, P3989, DOI 10.1109/TMM.2022.3169331
   Wang CW, 2022, LECT NOTES COMPUT SC, V13432, P528, DOI 10.1007/978-3-031-16434-7_51
   Wang CW, 2021, I S BIOMED IMAGING, P1319, DOI 10.1109/ISBI48211.2021.9433813
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Wu T, 2021, PROC CVPR IEEE, P16760, DOI 10.1109/CVPR46437.2021.01649
   Xie EZ, 2021, ADV NEUR IN, V34
   Xie JH, 2022, PROC CVPR IEEE, P979, DOI 10.1109/CVPR52688.2022.00106
   Xu Leiyang, 2022, IECON 2022 - 48th Annual Conference of the IEEE Industrial Electronics Society, P1, DOI 10.1109/IECON49645.2022.9968781
   Xu R., 2023, PROC AAAI C ARTIF IN, P1
   Xu RT, 2023, IEEE T IMAGE PROCESS, V32, P1052, DOI 10.1109/TIP.2023.3238648
   Xu RT, 2022, INT CONF ACOUST SPEE, P2505, DOI 10.1109/ICASSP43922.2022.9747786
   Xu RT, 2022, ENG APPL ARTIF INTEL, V110, DOI 10.1016/j.engappai.2022.104739
   Xu RT, 2021, LECT NOTES COMPUT SC, V12901, P503, DOI 10.1007/978-3-030-87193-2_48
   Yao Q, 2020, IEEE ACCESS, V8, P14413, DOI 10.1109/ACCESS.2020.2966647
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zhang BF, 2020, AAAI CONF ARTIF INTE, V34, P12765
   Zhang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7222, DOI 10.1109/ICCV48922.2021.00715
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhao M, 2021, I S BIOMED IMAGING, P118, DOI 10.1109/ISBI48211.2021.9433919
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
NR 72
TC 5
Z9 5
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 581
EP 592
DI 10.1109/TMM.2023.3267891
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000016
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Yu, C
   Zhang, DH
   Wu, Z
   Xie, CY
   Lu, Z
   Hu, Y
   Chen, Y
AF Yu, Cong
   Zhang, Dongheng
   Wu, Zhi
   Xie, Chunyang
   Lu, Zhi
   Hu, Yang
   Chen, Yan
TI MobiRFPose: Portable RF-Based 3D Human Pose Camera
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE RF signals; Three-dimensional displays; Antenna arrays; Pose estimation;
   Radio frequency; Cameras; Computational modeling; Human pose estimation;
   lightweight model; wireless sensing
AB Existing RF-based human pose estimation methods usually require intensive computations and cannot meet the real-time processing and portability requirements for mobile devices. To tackle the limitation, in this article, we introduce a lightweight RF-based pose estimation model, i.e., MobiRFPose, to construct the portable RF-based pose camera. Different from traditional optical-based cameras, the RF-based camera does not capture visual information, which means the privacy-preserving characteristic. Specifically, we only utilize a horizontal antenna array to transceive RF signals, then estimate the human locations on the RF signal heatmap and crop the human location regions, and finally estimate the fine-grained human poses based on the cropped small RF signal heatmaps. To evaluate the performance, we compare MobiRFPose with state-of-the-art methods. Experimental results demonstrate that MobiRFPose can achieve accurate 3D human pose estimation with fewer parameters and computations. We also test the trained MobiRFPose model using mobile computing devices, where the model structures and parameters only take up 268 KB and 3226 KB of disk space, and MobiRFPose can achieve 66 FPS processing speed. The pose estimation error is 11.05 cm in the case of a single person and 11.29 cm in the case of multiple people. All experimental results indicate that our proposed method can construct a portable RF camera to estimate human poses accurately.
C1 [Yu, Cong] China Acad Engn Phys, Inst Elect Engn, Mianyang 621900, Peoples R China.
   [Zhang, Dongheng; Wu, Zhi; Lu, Zhi; Chen, Yan] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
   [Xie, Chunyang] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Hu, Yang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
C3 Chinese Academy of Engineering Physics; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; University of
   Electronic Science & Technology of China; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Chen, Y (corresponding author), Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
EM congyu@std.uestc.edu.cn; dongheng@ustc.edu.cn; wzwyyx@mail.ustc.edu.cn;
   chunyangxie@std.uestc.edu.cn; zhilu@ustc.edu.cn; eeyhu@ustc.edu.cn;
   eecyan@ustc.edu.cn
OI Zhang, Dongheng/0000-0001-6309-6626; Lu, Zhi/0000-0001-6941-981X
FU National Natural Science Foundation of China
FX No Statement Available
CR Bajpai R, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3073720
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen J., 2022, IEEE Trans. Mobile Comput., early acce, Oc., V14, DOI [10.1109/TMC20223214721, DOI 10.1109/TMC20223214721]
   Chen Y, 2021, IEEE INTERNET THINGS, V8, P2762, DOI 10.1109/JIOT.2020.3022071
   Chen Y, 2020, IEEE T MOBILE COMPUT, V19, P2891, DOI 10.1109/TMC.2019.2934106
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Geng JQ, 2022, Arxiv, DOI arXiv:2301.00250
   Ghazalian R, 2021, IEEE T MULTIMEDIA, V23, P823, DOI 10.1109/TMM.2020.2990077
   Han Y, 2016, IEEE INTERNET THINGS, V3, P1036, DOI 10.1109/JIOT.2016.2548659
   He Y, 2023, IEEE INTERNET THINGS, V10, P1775, DOI 10.1109/JIOT.2022.3210686
   He Y, 2020, IEEE INTERNET THINGS, V7, P8296, DOI 10.1109/JIOT.2020.2989426
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hsu CY, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300778
   Jiang WJ, 2020, MOBICOM '20: PROCEEDINGS OF THE 26TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2020), P295, DOI 10.1145/3372224.3380900
   Kotaru M, 2015, ACM SIGCOMM COMP COM, V45, P269, DOI 10.1145/2829988.2787487
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Li TH, 2022, IEEE WINT CONF APPL, P1091, DOI 10.1109/WACV51458.2022.00116
   Li YD, 2023, IEEE T MOBILE COMPUT, V22, P7355, DOI 10.1109/TMC.2022.3207570
   Liu H, 2023, IEEE T MULTIMEDIA, V25, P1390, DOI 10.1109/TMM.2022.3141888
   Liu YH, 2022, IEEE T MULTIMEDIA, V24, P3060, DOI 10.1109/TMM.2021.3092579
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Majeed K, 2016, IEEE T MOBILE COMPUT, V15, P2794, DOI 10.1109/TMC.2015.2510631
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Ni JY, 2023, Arxiv, DOI arXiv:2307.03638
   Niu K, 2022, IEEE T MOBILE COMPUT, V21, P4156, DOI 10.1109/TMC.2021.3063135
   Qian K, 2018, ACM T EMBED COMPUT S, V17, DOI 10.1145/3157677
   Qian K, 2017, MOBIHOC'17: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, DOI 10.1145/3084041.3084067
   Qiu CR, 2023, IEEE T MULTIMEDIA, V25, P2613, DOI 10.1109/TMM.2022.3149129
   Ren Y., 2022, Proc. ACM Interact., Mobile, Wearable Ubiquitous Technol., V6, P1
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sengupta A, 2020, IEEE SENS J, V20, P10032, DOI 10.1109/JSEN.2020.2991741
   Shichao Yue, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3214289
   Song R, 2022, PROCEEDINGS OF THE 2022 THE 28TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, ACM MOBICOM 2022, P282, DOI 10.1145/3495243.3560529
   Wang F, 2019, IEEE I CONF COMP VIS, P5451, DOI 10.1109/ICCV.2019.00555
   Wang L, 2021, IEEE T MOBILE COMPUT, V20, P1730, DOI 10.1109/TMC.2019.2961885
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu Z, 2023, IEEE T MULTIMEDIA, V25, P4730, DOI 10.1109/TMM.2022.3181455
   Xie C., 2023, IEEE Trans. Multimedia, DOI [10.1109/TMM.20233268376, DOI 10.1109/TMM.20233268376]
   Xu QY, 2017, IEEE INTERNET THINGS, V4, P723, DOI 10.1109/JIOT.2017.2663318
   Xu XH, 2022, IEEE SENS J, V22, P24264, DOI 10.1109/JSEN.2022.3220419
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Yu C, 2023, IEEE T MULTIMEDIA, V25, P2926, DOI 10.1109/TMM.2022.3153136
   Zeng YZ, 2016, 2016 15TH ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN)
   Zhang DH, 2021, IEEE INTERNET THINGS, V8, P3904, DOI 10.1109/JIOT.2020.3025820
   Zhang DH, 2020, IEEE SYST J, V14, P661, DOI 10.1109/JSYST.2019.2904714
   Zhang DH, 2019, IEEE INTERNET THINGS, V6, P3899, DOI 10.1109/JIOT.2019.2893330
   Zhang DH, 2018, IEEE T VEH TECHNOL, V67, P7101, DOI 10.1109/TVT.2018.2827408
   Zhang F, 2018, IEEE INTERNET THINGS, V5, P2163, DOI 10.1109/JIOT.2018.2826227
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao MM, 2018, PROC CVPR IEEE, P7356, DOI 10.1109/CVPR.2018.00768
   Zhao MM, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P267, DOI 10.1145/3230543.3230579
   Zhao Mingmin, 2017, P MACHINE LEARNING R, V70
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zhou YJ, 2023, IEEE INTERNET THINGS, V10, P14128, DOI 10.1109/JIOT.2023.3262940
   Zou SH, 2023, IEEE T MULTIMEDIA, V25, P3560, DOI 10.1109/TMM.2022.3162469
NR 57
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3715
EP 3727
DI 10.1109/TMM.2023.3314979
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200022
DA 2024-08-05
ER

PT J
AU Yuan, J
   Hou, F
   Yang, Y
   Zhang, Y
   Shi, ZC
   Geng, X
   Fan, JP
   He, ZQ
   Rui, Y
AF Yuan, Jin
   Hou, Feng
   Yang, Ying
   Zhang, Yang
   Shi, Zhongchao
   Geng, Xin
   Fan, Jianping
   He, Zhiqiang
   Rui, Yong
TI Domain-Aware Graph Network for Bridging Multi-Source Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Feature extraction; Graph neural networks; Adaptation
   models; Self-supervised learning; Multitasking; Image color analysis;
   Multi-source domain adaptation; self-supervised learning; graph neural
   network; real-world applications
ID CORRELATION ALIGNMENT; KERNEL
AB Domain adaptation (DA) addresses the challenge of distribution discrepancy between the training and test data, while multi-source domain adaptation (MSDA) is particularly appealing for realistic scenarios. With the emergence of extensive unlabeled datasets, self-supervised learning has gained significant popularity in deep learning. It is noteworthy that multi-source domain adaptation and self-supervised learning share a common objective: leveraging unlabeled data to acquire more informative representations. However, conventional self-supervised learning encounters two main limitations. Firstly, the traditional pretext task falls to transfer fine-grained knowledge to downstream task with general representation learning. Secondly, the scheme of the same feature extractor with distinct prediction heads makes the cross-task knowledge exchange and information sharing ineffective. In order to tackle these challenges, we introduce a novel approach called Domain-Aware Graph Network (DAGNet). DAGNet utilizes a graph neural network as a bridge to facilitate efficient cross-task knowledge exchange. By employing a mask token strategy, we enhance the robustness of representations by selectively masking certain domain or self-supervised information. In terms of datasets, the uneven and style-based domain shifts in current datasets make it challenging to measure the model's domain adaptation performance in real-world applications. To address this issue, we introduce a benchmark dataset DomainVerse with continuous spatio-temporal domain shifts encountered in the real world. Our extensive experiments demonstrate that DAGNet achieves state-of-the-art performance not only on mainstream multi-source domain adaptation datasets but also on different settings within DomainVerse.
C1 [Yuan, Jin; Geng, Xin; Rui, Yong] Southeast Univ, Sch Comp Sci & Engn, Minist Educ, Nanjing 211189, Peoples R China.
   [Yuan, Jin; Geng, Xin; Rui, Yong] Southeast Univ, Key Lab Comp Network & Informat Integrat, Minist Educ, Nanjing 211189, Peoples R China.
   [Hou, Feng; He, Zhiqiang] Chinese Acad Sci, Inst Comp Technol, Beijing 100000, Peoples R China.
   [Hou, Feng; He, Zhiqiang] Univ Chinese Acad Sci, Beijing 100000, Peoples R China.
   [Yang, Ying; Zhang, Yang; Shi, Zhongchao; Fan, Jianping; Rui, Yong] Lenovo Res, AI Lab, Beijing 100000, Peoples R China.
C3 Southeast University - China; Southeast University - China; Chinese
   Academy of Sciences; Institute of Computing Technology, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Legend Holdings; Lenovo
RP Geng, X; Rui, Y (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Minist Educ, Nanjing 211189, Peoples R China.; Geng, X; Rui, Y (corresponding author), Southeast Univ, Key Lab Comp Network & Informat Integrat, Minist Educ, Nanjing 211189, Peoples R China.
EM yuanjin@seu.edu.cn; houfeng19@mails.ucas.ac.cn; yangying23@lenovo.com;
   zhangyang20@lenovo.com; shizc2@lenovo.com; xgeng@seu.edu.cn;
   jfan1@lenovo.com; hezq@lenovo.com; yongrui@lenovo.com
OI Hou, Feng/0000-0002-2587-720X; Fan, Jianping/0000-0003-2290-1785
FU AI Lab
FX No Statement Available
CR Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100
   Beery S, 2021, Arxiv, DOI arXiv:2105.03494
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Bruna W., 2014, INT C LEARN REPRE SE
   Cai ZY, 2023, PATTERN RECOGN LETT, V174, P124, DOI 10.1016/j.patrec.2023.09.005
   Crammer K, 2008, J MACH LEARN RES, V9, P1757
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P9943, DOI 10.1109/ICCV.2019.01004
   Devlin J, 2018, ARXIV
   Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Feng ZY, 2019, PROC CVPR IEEE, P10356, DOI 10.1109/CVPR.2019.01061
   French M., 2018, INT C LEARN REPRESEN, V6
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao JX, 2023, IEEE T MULTIMEDIA, V25, P4764, DOI 10.1109/TMM.2022.3181789
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Gidaris S, 2018, Arxiv, DOI arXiv:1803.07728
   Gidaris S, 2019, IEEE I CONF COMP VIS, P8058, DOI 10.1109/ICCV.2019.00815
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gretton A., 2006, Advances in neural information processing systems, V19, P513
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Trinh TH, 2019, Arxiv, DOI arXiv:1906.02940
   Hang Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P727, DOI 10.1007/978-3-030-58598-3_43
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2019, ADV NEUR IN, V32
   Hu YF, 2023, IEEE T MULTIMEDIA, V25, P2061, DOI 10.1109/TMM.2022.3142413
   Huang CQ, 2023, IEEE T MULTIMEDIA, V25, P4426, DOI 10.1109/TMM.2022.3175611
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Jiang X, 2020, PR MACH LEARN RES, V119
   Jin JQ, 2023, PROC CVPR IEEE, P11600, DOI 10.1109/CVPR52729.2023.01116
   Jong-Chyi Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P645, DOI 10.1007/978-3-030-58571-6_38
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Lanchantin J, 2021, PROC CVPR IEEE, P16473, DOI 10.1109/CVPR46437.2021.01621
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Lee H, 2020, PR MACH LEARN RES, V119
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Liang K., 2023, arXiv
   Liu XW, 2023, IEEE T PATTERN ANAL, V45, P8566, DOI 10.1109/TPAMI.2022.3233635
   Liu XW, 2021, PR MACH LEARN RES, V139
   Long MS, 2016, ADV NEUR IN, V29
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long Mingsheng., 2018, NeurIPS, P1647, DOI DOI 10.48550/ARXIV.1705.10667
   Luo ZP, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108955
   Luyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P608, DOI 10.1007/978-3-030-58568-6_36
   Mancini M, 2018, PROC CVPR IEEE, P3771, DOI 10.1109/CVPR.2018.00397
   Mansour M., 2008, C EMPIRICAL METHODS, P4694
   Netzer Y, 2011, P NIPS WORKSH DEEP L, P4
   Nguyen Tuan, 2021, Uncertainty in Artificial Intelligence, P225
   Nguyen VA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9332, DOI 10.1109/ICCV48922.2021.00922
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Quinonero-Candela M., 2008, Dataset Shift in Machine Learning
   Ren CX, 2022, IEEE T IMAGE PROCESS, V31, P2122, DOI 10.1109/TIP.2022.3152052
   Ren ZZ, 2018, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.2018.00086
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Schrod S, 2023, Arxiv, DOI arXiv:2306.00607
   Sun BC, 2017, ADV COMPUT VIS PATT, P153, DOI 10.1007/978-3-319-58347-1_8
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun ZR, 2022, IEEE T MULTIMEDIA, V24, P1093, DOI 10.1109/TMM.2021.3116430
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1147
   Tang C, 2023, IEEE T KNOWL DATA EN, V35, P6449, DOI 10.1109/TKDE.2022.3172687
   Tao ZL, 2023, IEEE T MULTIMEDIA, V25, P5107, DOI 10.1109/TMM.2022.3187556
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Venkat N., 2020, ADV NEURAL INFORM PR, V33, P4647, DOI DOI 10.48550/ARXIV.2103.11169
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Verma A, 2023, IEEE T MULTIMEDIA, V25, P364, DOI 10.1109/TMM.2021.3126404
   Wan XH, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3676, DOI 10.1145/3503161.3547864
   Wang MH, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2349, DOI 10.1145/3394486.3403284
   Wang QF, 2023, IEEE T MULTIMEDIA, V25, P1074, DOI 10.1109/TMM.2021.3138298
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wang SS, 2023, NEUROCOMPUTING, V523, P213, DOI 10.1016/j.neucom.2022.12.048
   Wang YM, 2023, IEEE T MULTIMEDIA, V25, P1008, DOI 10.1109/TMM.2021.3136098
   Wang Y, 2023, IEEE T MULTIMEDIA, V25, P90, DOI 10.1109/TMM.2021.3121559
   Wen J., 2020, PMLR, P10214
   Wilson G, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3400066
   Xu MH, 2024, IEEE T PATTERN ANAL, V46, P1727, DOI 10.1109/TPAMI.2022.3172372
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   You Yuning, 2020, Proc Mach Learn Res, V119, P10871
   Yu QQ, 2023, IEEE T MULTIMEDIA, V25, P1452, DOI 10.1109/TMM.2023.3234372
   Yuan J, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3578518
   Yuan J, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3907, DOI 10.1145/3503161.3548121
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhang XX, 2023, PROC CVPR IEEE, P16036, DOI 10.1109/CVPR52729.2023.01539
   Zhao H, 2019, INT C MACHINE LEARNI, P7523
   Zhao H., 2018, Advances in neural information processing systems, P8559
   Zhao SC, 2020, AAAI CONF ARTIF INTE, V34, P12975
   Zhao WT, 2024, IEEE T MULTIMEDIA, V26, P2659, DOI 10.1109/TMM.2023.3301279
   Zhou KY, 2021, IEEE T IMAGE PROCESS, V30, P8008, DOI 10.1109/TIP.2021.3112012
   Zhu YC, 2019, AAAI CONF ARTIF INTE, P5989
NR 98
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7210
EP 7224
DI 10.1109/TMM.2024.3361729
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000018
DA 2024-08-05
ER

PT J
AU Zhang, Y
   Zhang, L
   Zhao, X
   Fu, HY
   Yu, DQ
AF Zhang, Yan
   Zhang, Lu
   Zhao, Xin
   Fu, Hongyong
   Yu, Dequan
TI Automatic Point Cloud Registration for 3D Virtual-to-Real Registration
   Using Macro and Micro Structures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D; cross-source; macro/micro; point cloud; registration;
   virtual-to-real
ID OBJECTS
AB Virtual-to-real registration is a crucial aspect of 3D registration, which presents a more challenging multimodal 3D registration problem due to the different data structures between virtual and real models. In this paper, we utilize point cloud registration algorithm to align virtual and real models, transforming the multimodal 3D registration problem into a cross-source point cloud registration problem. We propose a method for extracting macro and micro structures to represent the shared features of virtual and real models, combined with a multi-constraint registration algorithm, to achieve high-accuracy virtual-to-real registration tasks. This method can register unseen 3D objects using virtual prior knowledge, and allow partial point cloud registration without the need for a 360-degree scan of the model. Our approach can effectively resist interference from typical cross-source point cloud registration problems such as varying densities, missing data, and distribution changes. Furthermore, by processing only 0.2% of the original number of point clouds through downsampling, we can effectively diminish the effects of noise and outlier, as well as significantly decrease time consuming. Experimental results show that our algorithm outperforms other advanced point cloud registration algorithms in cross-source point cloud registration for virtual-to-real registration.
C1 [Zhang, Yan; Zhao, Xin; Fu, Hongyong] Univ Chinese Acad Sci, Technol & Engn Ctr Space Utilizat, Beijing 100094, Peoples R China.
   [Zhang, Lu; Yu, Dequan] Chinese Acad Sci, Key Lab Space Utilizat Technol & Engn, Ctr Space Utilizat, Beijing 100094, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Technology & Engineering Center for Space Utilization, CAS; Chinese
   Academy of Sciences; Technology & Engineering Center for Space
   Utilization, CAS
RP Zhang, L (corresponding author), Chinese Acad Sci, Key Lab Space Utilizat Technol & Engn, Ctr Space Utilizat, Beijing 100094, Peoples R China.
EM zhangyan21@csu.ac.cn; zhanglu@csu.ac.cn; zhaoxin19@csu.ac.cn;
   fuhongyong@csu.ac.cn; ydqbuaa@163.com
OI Zhang, Yan/0009-0002-0196-6184
FU Key Laboratory of Space Utilization
FX No Statement Available
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Arvanitis G, 2022, IEEE T MULTIMEDIA, V24, P2230, DOI 10.1109/TMM.2021.3089838
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Biber P, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2743, DOI 10.1109/iros.2003.1249285
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Gavrilov M, 2004, ALGORITHMICA, V38, P59, DOI 10.1007/s00453-003-1043-4
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Guan T, 2009, IEEE T MULTIMEDIA, V11, P1393, DOI 10.1109/TMM.2009.2032684
   Hexner J, 2016, INT J COMPUT VISION, V118, P95, DOI 10.1007/s11263-015-0873-2
   Hodan T, 2018, LECT NOTES COMPUT SC, V11214, P19, DOI 10.1007/978-3-030-01249-6_2
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Huang XS, 2021, Arxiv, DOI [arXiv:2103.02690, DOI 10.48550/ARXIV.2103.02690]
   Huang XS, 2018, IEEE T CIRC SYST VID, V28, P2965, DOI 10.1109/TCSVT.2017.2730232
   Huang XS, 2017, IEEE T IMAGE PROCESS, V26, P3261, DOI 10.1109/TIP.2017.2695888
   HUTTENLOCHER DP, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P263
   Kim H, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P119, DOI 10.1109/3DV.2013.24
   Koide K, 2021, IEEE INT CONF ROBOT, P11054, DOI 10.1109/ICRA48506.2021.9560835
   Le HM, 2019, PROC CVPR IEEE, P124, DOI 10.1109/CVPR.2019.00021
   Lee SJ, 2014, J MECH SCI TECHNOL, V28, P3171, DOI 10.1007/s12206-014-0726-x
   Mellado N, 2016, IEEE T VIS COMPUT GR, V22, P2160, DOI 10.1109/TVCG.2015.2505287
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Peng FR, 2014, IEEE IMAGE PROC, P2026, DOI 10.1109/ICIP.2014.7025406
   Pribanic T, 2019, PATTERN RECOGN, V88, P532, DOI 10.1016/j.patcog.2018.12.008
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Wang LJ, 2019, Arxiv, DOI arXiv:1904.01428
   Yan L, 2023, IEEE T PATTERN ANAL, V45, P7986, DOI 10.1109/TPAMI.2022.3226498
   Yang H., 2019, arXiv, DOI 10.48550/arXiv.1903.08588
   Yang ZP, 2019, PROC CVPR IEEE, P4526, DOI 10.1109/CVPR.2019.00466
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhao H, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107272
   Zhong L., 2020, P 13 INT SCI PROF S, P286
   Zhong LS, 2018, IEEE T CIRC SYST VID, V28, P2302, DOI 10.1109/TCSVT.2017.2731519
NR 43
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6566
EP 6581
DI 10.1109/TMM.2024.3354570
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600038
DA 2024-08-05
ER

PT J
AU Zhou, J
   Xu, C
   Ge, YT
   Cheng, L
AF Zhou, Jun
   Xu, Chi
   Ge, Yuting
   Cheng, Li
TI Realistic Depth Image Synthesis for 3D Hand Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth noise modeling; 3D hand pose estimation; realistic depth synthesis
AB The training of depth image-based hand pose estimation model typically relies on real-life datasets which are expected to be 1) largescale and cover a diverse range of hand poses and hand shapes, and 2) always come with high-precision annotations. However, existing datasets in reality are rather limited in the above regards due to multitude practical constraints, with time and cost being the major concerns. This observation motivates us to propose an alternative approach, where hand pose model is primarily trained with synthesized hand depth images that closely mimicking the characteristic noise patterns of a specific depth camera make under consideration. It is achieved by firstly mapping a Gaussian distributed variable to certain specific non-i.i.d. (independent and identically distributed) depth noise pattern, and then transforming a "vanilla" noise-free synthetic depth image to a realistic-looking image. Extensive empirical experiments demonstrate that our approach is capable of generating camera-specific realistic-looking hand depth images with precise annotations; comparing to entirely relying on annotated real images, a hand pose model with better performance is obtained by using only a small fraction (10%) of annotated real images as well as our synthesized images.
C1 [Zhou, Jun; Xu, Chi; Ge, Yuting] China Univ Geosci, Sch Automat, Wuhan 430074, Peoples R China.
   [Zhou, Jun; Xu, Chi; Ge, Yuting] Hubei Key Lab Adv Control & Intelligent Automat Co, Wuhan 430074, Peoples R China.
   [Zhou, Jun; Xu, Chi; Ge, Yuting] Minist Educ, Engn Res Ctr Intelligent Technol Geoexplorat, Wuhan 430074, Peoples R China.
   [Cheng, Li] Univ Alberta, Dept Elect & Comp Engn, Vis & Learning Lab, Edmonton, AB T6G 2R3, Canada.
C3 China University of Geosciences; University of Alberta
RP Xu, C (corresponding author), China Univ Geosci, Sch Automat, Wuhan 430074, Peoples R China.
EM jchow@cug.edu.cn; xuchi@cug.edu.cn; gyting@cug.edu.cn;
   lcheng5@ualberta.ca
OI Ge, Yuting/0000-0002-7545-6438
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn MS, 2019, INT CONF UBIQ ROBOT, P707, DOI [10.1109/URAI.2019.8768489, 10.1109/urai.2019.8768489]
   [Anonymous], 2013, Consumer Depth Cameras for Computer Vision
   Armagan Anil, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P85, DOI 10.1007/978-3-030-58592-1_6
   Boisvert J., 2018, P EUR C COMP VIS WOR, P729
   Chen XH, 2020, NEUROCOMPUTING, V395, P138, DOI 10.1016/j.neucom.2018.06.097
   Chengde Wan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5147, DOI 10.1109/CVPR.2018.00540
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Fankhauser P, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P388, DOI 10.1109/ICAR.2015.7251485
   Ge LH, 2018, LECT NOTES COMPUT SC, V11217, P489, DOI 10.1007/978-3-030-01261-8_29
   Ge LH, 2018, PROC CVPR IEEE, P8417, DOI 10.1109/CVPR.2018.00878
   Gu X, 2020, IEEE T IMAGE PROCESS, V29, P6343, DOI 10.1109/TIP.2020.2988574
   Handa A, 2016, PROC CVPR IEEE, P4077, DOI 10.1109/CVPR.2016.442
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang WT, 2020, AAAI CONF ARTIF INTE, V34, P11061
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, INT C LEARNING REPRE
   Khamis S, 2015, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2015.7298869
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Landau MJ, 2016, IEEE T CYBERNETICS, V46, P3018, DOI 10.1109/TCYB.2015.2494877
   Lei JJ, 2017, IEEE T IMAGE PROCESS, V26, P1732, DOI 10.1109/TIP.2017.2656463
   Li R, 2019, PATTERN RECOGN, V93, P251, DOI 10.1016/j.patcog.2019.04.026
   Li SL, 2019, PROC CVPR IEEE, P11919, DOI 10.1109/CVPR.2019.01220
   Malik J, 2022, IEEE T PATTERN ANAL, V44, P8962, DOI 10.1109/TPAMI.2021.3122874
   Malik J, 2020, PROC CVPR IEEE, P7111, DOI 10.1109/CVPR42600.2020.00714
   Mallick T, 2014, IEEE SENS J, V14, P1731, DOI 10.1109/JSEN.2014.2309987
   Moon G, 2018, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2018.00533
   Mori S, 2023, IEEE T VIS COMPUT GR, V29, P3989, DOI 10.1109/TVCG.2022.3176958
   Oberweger M, 2020, IEEE T PATTERN ANAL, V42, P1898, DOI 10.1109/TPAMI.2019.2907951
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Planche B, 2017, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2017.00011
   Rad M, 2018, PROC CVPR IEEE, P4663, DOI 10.1109/CVPR.2018.00490
   Ren PF, 2022, IEEE T IMAGE PROCESS, V31, P5052, DOI 10.1109/TIP.2022.3192708
   Ren PF, 2023, IEEE T CYBERNETICS, V53, P315, DOI 10.1109/TCYB.2021.3083637
   Shen YF, 2022, IEEE ROBOT AUTOM LET, V7, P4845, DOI 10.1109/LRA.2022.3148788
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Sridhar S, 2016, LECT NOTES COMPUT SC, V9906, P294, DOI 10.1007/978-3-319-46475-6_19
   Tang DH, 2017, IEEE T PATTERN ANAL, V39, P1374, DOI 10.1109/TPAMI.2016.2599170
   Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Tzionas D, 2016, INT J COMPUT VISION, V118, P172, DOI 10.1007/s11263-016-0895-4
   Tzionas D, 2014, LECT NOTES COMPUT SC, V8753, P277, DOI 10.1007/978-3-319-11752-2_22
   Xiong F, 2019, IEEE I CONF COMP VIS, P793, DOI 10.1109/ICCV.2019.00088
   Xu C, 2017, INT J COMPUT VISION, V123, P454, DOI 10.1007/s11263-017-0998-6
   Xu C, 2013, IEEE I CONF COMP VIS, P3456, DOI 10.1109/ICCV.2013.429
   Yu Yu, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P615, DOI 10.1007/978-3-642-37447-0_47
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Z., arXiv
   Zhou YX, 2020, PROC CVPR IEEE, P5345, DOI 10.1109/CVPR42600.2020.00539
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 53
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5246
EP 5256
DI 10.1109/TMM.2023.3330522
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800003
DA 2024-08-05
ER

PT J
AU Feng, MZ
   Su, JB
AF Feng, Mingzheng
   Su, Jianbo
TI Learning Multi-Layer Attention Aggregation Siamese Network for Robust
   RGBT Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE RGBT tracking; hierarchical attention network; contribution-aware
   aggregation network
ID FUSION TRACKING; T TRACKING
AB Recent years have witnessed the popularity of integrating Siamese network into RGBT tracking for fast-tracking. However, these trackers mostly utilize the feature information of the last output layer and ignore the benefits of multi-layer information. In addition, they often adopt feature-level fusion for different modalities but fail to explore the strength of decision-level fusion, which may easily decrease their flexibility and independence. In this article, a novel multi-layer attention aggregation Siamese network on the decision level is proposed for robust RGBT tracking. To be specific, a hierarchical channel attention Siamese network is built to recalibrate the extracted multi-layer features from RGB and thermal infrared images. This can focus on more discriminative features to learn robust feature representation. Then, a depth-wise correlation operation is performed to produce RGB and thermal response maps, respectively. To better exploit and utilize the complementary RGB and thermal information, a contribution-aware aggregation network is designed to adaptively aggregate them. Lastly, a classification and regression network is adopted to complete the bounding box prediction. Extensive experiments on four large-scale RGBT benchmarks demonstrate outstanding tracking ability over other state-of-the-art trackers.
C1 [Feng, Mingzheng; Su, Jianbo] Shanghai Jiao Tong Univ, Dept Automat, Key Lab Syst Control & Informat Proc, Minist Educ, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Su, JB (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Key Lab Syst Control & Informat Proc, Minist Educ, Shanghai 200240, Peoples R China.
EM fmz@sjtu.edu.cn; jbsu@sjtu.edu.cn
FU National Natural Science Foundation of China [91748120]; Shanghai
   Cross-disciplinary Research Fund [JYJC202214]
FX This work was supported in part by the key Project of the National
   Natural Science Foundation of China under Grant 91748120, and in part by
   Shanghai Cross-disciplinary Research Fund under Grant JYJC202214.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Chenglong Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P222, DOI 10.1007/978-3-030-58542-6_14
   Cvejic N, 2007, PROC CVPR IEEE, P3307
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2018, PROC CVPR IEEE, P518, DOI 10.1109/CVPR.2018.00061
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Feng MZ, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102881
   Gao Y, 2019, IEEE INT CONF COMP V, P91, DOI 10.1109/ICCVW.2019.00017
   Guo C, 2022, VISUAL COMPUT, V38, P2555, DOI 10.1007/s00371-021-02131-4
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P2162, DOI 10.1109/TMM.2020.3008035
   Jung I., 2018, P ECCV, P83
   Kim HU, 2015, IEEE I CONF COMP VIS, P3011, DOI 10.1109/ICCV.2015.345
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li CL, 2022, IEEE T IMAGE PROCESS, V31, P392, DOI 10.1109/TIP.2021.3130533
   Li CL, 2019, IEEE INT CONF COMP V, P2262, DOI 10.1109/ICCVW.2019.00279
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49
   Li CL, 2018, NEUROCOMPUTING, V281, P78, DOI 10.1016/j.neucom.2017.11.068
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9
   Lu AD, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3157594
   Lu AD, 2021, IEEE T IMAGE PROCESS, V30, P5613, DOI 10.1109/TIP.2021.3087341
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P2386, DOI 10.1109/TPAMI.2020.3041332
   Luo CW, 2019, INFRARED PHYS TECHN, V99, P265, DOI 10.1016/j.infrared.2019.04.017
   Mei JT, 2021, IEEE SENS J, V21, P16915, DOI 10.1109/JSEN.2021.3078455
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Pu S, 2018, ADV NEUR IN, V31
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tu ZZ, 2022, IEEE T IMAGE PROCESS, V31, P85, DOI 10.1109/TIP.2021.3125504
   Wang CQ, 2020, PROC CVPR IEEE, P7062, DOI 10.1109/CVPR42600.2020.00709
   Wang JH, 2016, LECT NOTES COMPUT SC, V9909, P664, DOI 10.1007/978-3-319-46454-1_40
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wu Y., 2011, P IEEE 14 INT C INF, P1
   Xiao Y, 2022, AAAI CONF ARTIF INTE, P2831
   Xingping Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P378, DOI 10.1007/978-3-030-58565-5_23
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Yu J, 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Zhai SL, 2019, NEUROCOMPUTING, V334, P172, DOI 10.1016/j.neucom.2019.01.022
   Zhang H, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020393
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang LC, 2019, IEEE INT CONF COMP V, P2252, DOI 10.1109/ICCVW.2019.00278
   Zhang PY, 2022, PROC CVPR IEEE, P8876, DOI 10.1109/CVPR52688.2022.00868
   Zhang PY, 2021, INT J COMPUT VISION, V129, P2714, DOI 10.1007/s11263-021-01495-3
   Zhang PY, 2021, IEEE T IMAGE PROCESS, V30, P3335, DOI 10.1109/TIP.2021.3060862
   Zhang Q, 2021, IEEE T CIRC SYST VID, V31, P1804, DOI 10.1109/TCSVT.2020.3014663
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
   Zhang TL, 2022, IEEE T CIRC SYST VID, V32, P1403, DOI 10.1109/TCSVT.2021.3072207
   Zhang XC, 2020, SIGNAL PROCESS-IMAGE, V84, DOI 10.1016/j.image.2019.115756
   Zhang XC, 2019, IEEE ACCESS, V7, P122122, DOI 10.1109/ACCESS.2019.2936914
   Zhang YP, 2021, IEEE T MULTIMEDIA, V23, P4232, DOI 10.1109/TMM.2020.3038310
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
   Zhu YB, 2022, IEEE T CIRC SYST VID, V32, P579, DOI 10.1109/TCSVT.2021.3067997
   Zhu YB, 2021, IEEE T INTELL VEHICL, V6, P121, DOI 10.1109/TIV.2020.2980735
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 79
TC 6
Z9 6
U1 17
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3378
EP 3391
DI 10.1109/TMM.2023.3310295
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200023
DA 2024-08-05
ER

PT J
AU Gao, JL
   Li, JG
   Jia, CM
   Wang, SS
   Ma, SW
   Gao, W
AF Gao, Junlong
   Li, Jiguo
   Jia, Chuanmin
   Wang, Shanshe
   Ma, Siwei
   Gao, Wen
TI Cross Modal Compression With Variable Rate Prompt
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross modal compression; semantic fidelity; variable rate prompt
ID SEMANTIC COMMUNICATION
AB Traditional image/video compression compresses the highly redundant visual data while preserving signal fidelity. Recently, cross modal compression (CMC) is proposed to compress the data into a compact, human-comprehensible domain (such as text) with an ultra-high compression ratio while preserving semantic fidelity for machine analysis and semantic monitoring. CMC is with a constant rate because the CMC encoder can only represent the data with a fixed grain. But in practice, variable rate is necessary due to the complicated and dynamic transmission conditions, the different storage mediums, and the diverse levels of application requirements. To deal with this problem, in this paper, we propose variable rate cross modal compression (VR-CMC), where we introduce variable rate prompt to represent the data with different grains. Variable rate prompt is composed of three strategies. Specifically, 1) target length prompt (TLP) introduces the target length into the language prompt to guide the generation of the text representation; 2) decaying EOS probability (DEP) exponentially decays the probability of the EOS token with regard to the decoding step and target length, where the EOS (end-of-sequence) token is a special token indicating the end of the text; 3) text augmentation (TA) enriches the training data and makes the text representation length more balanced when training. Experimental results show that our proposed VR-CMC can effectively control the rate in the CMC framework and achieve state-of-the-art performance on MSCOCO and IM2P datasets.
C1 [Gao, Junlong; Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Li, Jiguo] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Li, Jiguo] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Jia, Chuanmin] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
C3 Peking University; Chinese Academy of Sciences; Institute of Computing
   Technology, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Peking University
RP Jia, CM (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM jlgao@pku.edu.cn; jiguo.li@vipl.ict.ac.cn; cmjia@pku.edu.cn;
   sswang@pku.edu.cn; swma@pku.edu.cn; wgao@pku.edu.cn
OI Jia, Chuanmin/0000-0002-7418-6245
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Balle J., 2017, PROC 5 INT C LEARN R
   Balle J., 2018, PROC INT C LEARNREPR
   Banerjee S., 2005, ACL WORKSH INTR EXTR, P65
   Chaorui Deng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P712, DOI 10.1007/978-3-030-58601-0_42
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2414, DOI 10.1145/3343031.3350849
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Gao CS, 2023, IEEE T MULTIMEDIA, V25, P721, DOI 10.1109/TMM.2021.3130754
   Gao L, 2020, Arxiv, DOI arXiv:2101.00027
   Gao Wen, 2014, Advanced Video Coding Systems, P4
   Güler B, 2018, IEEE T COGN COMMUN, V4, P787, DOI 10.1109/TCCN.2018.2872596
   Guo DD, 2022, INT J COMPUT VISION, V130, P1920, DOI 10.1007/s11263-022-01624-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Jiang ZB, 2020, T ASSOC COMPUT LING, V8, P423, DOI 10.1162/tacl_a_00324
   Juba B., Universal Semantic Communication
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kastner MA, 2021, IEEE ACCESS, V9, P162951, DOI 10.1109/ACCESS.2021.3131393
   Kikuchi Y., 2016, P 2016 C EMP METH NA, P1328
   Kilickaya M, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P199
   Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356
   Lester B, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3045
   Li JG, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4230, DOI 10.1145/3474085.3475558
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140
   Liu YZ, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4110
   Luo YD, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2341, DOI 10.1145/3343031.3350961
   Marcellin M. W., 2000, Proceedings DCC 2000. Data Compression Conference, P523, DOI 10.1109/DCC.2000.838192
   Melas-Kyriazi L, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P757
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Petroni F, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2463
   Radford A, 2021, PR MACH LEARN RES, V139
   Redondi A, 2016, IEEE T MOBILE COMPUT, V15, P3000, DOI 10.1109/TMC.2016.2519340
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Salimans T, 2016, ADV NEUR IN, V29
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shi GM, 2021, IEEE COMMUN MAG, V59, P44, DOI 10.1109/MCOM.001.2001239
   Shi Y., 2021, PROC ACM MULTIMEDIA, P1
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun Y., 2022, Advances in Neural Information Processing Systems, V35, P37484
   Toderici G., 2016, PROC 4 INT C LEARN R
   van den Oord A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   WALLACE GK, 1990, P SOC PHOTO-OPT INS, V1244, P220, DOI 10.1117/12.19537
   Wang J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P940
   Wang P., 2022, PMLR, P23318
   Wang SR, 2022, IEEE T MULTIMEDIA, V24, P3169, DOI 10.1109/TMM.2021.3094300
   Weaver W, 1953, ET CETERA, V10, P261
   Xu Chunpu, 2020, P 28 INT C COMP LING, P3132, DOI DOI 10.18653/V1/2020.COLING-MAIN.279
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang F, 2020, IEEE SIGNAL PROC LET, V27, P331, DOI 10.1109/LSP.2020.2970539
   Yang LC, 2021, AAAI CONF ARTIF INTE, V35, P3136
   Zhang PP, 2023, IEEE T CIRC SYST VID, V33, P4441, DOI 10.1109/TCSVT.2023.3241225
   Zhou KY, 2022, PROC CVPR IEEE, P16795, DOI 10.1109/CVPR52688.2022.01631
NR 64
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3444
EP 3456
DI 10.1109/TMM.2023.3310657
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200028
DA 2024-08-05
ER

PT J
AU Hu, B
   Zhu, G
   Li, LD
   Gan, J
   Li, WS
   Gao, XB
AF Hu, Bo
   Zhu, Guang
   Li, Leida
   Gan, Ji
   Li, Weisheng
   Gao, Xinbo
TI Blind Image Quality Index With Cross-Domain Interaction and Cross-Scale
   Integration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Feature extraction; Distortion; Image quality;
   Transformers; Databases; Indexes; Image quality assessment; deep
   learning; cross-domain interaction; cross-scale integration
ID FREE-ENERGY PRINCIPLE
AB With the assistance of Convolutional Neural Networks (CNNs), Image Quality Assessment (IQA) models have made great progress in evaluating both simulated distortion and authentic distortion. However, most of the existing IQA models only learn the features of distorted images, and thus do not make full use of the available feature representation of other domains. Furthermore, the common multi-scale fusion strategies are relatively simple, such as downsampling and concatenating, which further limits the prediction performance. To this end, we propose a novel blind image quality index with cross-domain interaction and cross-scale integration, which is designed based on the combination of CNN and Transformer. First, the hierarchical spatial-domain and gradient-domain representations are obtained through a typical CNN architecture. Then, based on the proposed gradient-query cross-attention, these two types of features are fully interacted in the Cross-Domain Interaction (CDI) module. To represent the distortion information more comprehensively, the Cross-Scale Integration (CSI) module is proposed to combine the information between different scales progressively. Finally, the quality score is obtained through a simple regression module. The experimental results on five public IQA databases of both simulated and authentic scenes show that the proposed model outperforms the compared state-of-the-art metrics. In addition, cross-database experiments show that the proposed model has strong generalization performance.
C1 [Hu, Bo; Zhu, Guang; Gan, Ji; Li, Weisheng; Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Hu, Bo; Zhu, Guang; Gan, Ji; Li, Weisheng; Gao, Xinbo] Chongqing Inst Brain & Intelligence, Guangyang Bay Lab, Chongqing 400064, Peoples R China.
   [Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Xidian University
RP Gao, XB (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.; Gao, XB (corresponding author), Chongqing Inst Brain & Intelligence, Guangyang Bay Lab, Chongqing 400064, Peoples R China.
EM hubo90@cqupt.edu.cn; s210401049@stu.cqupt.edu.cn; ldli@xidian.edu.cn;
   ganji@cqupt.edu.cn; liws@cqupt.edu.cn; gaoxb@cqupt.edu.cn
OI Gan, Ji/0000-0001-6041-588X
FU National Natural Science Foundation of China
FX No Statement Available
CR Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Dosovitskiy A., 2021, INT C LEARN REPRESEN, P1
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Gu J, 2019, PATTERN RECOGN, V91, P332, DOI 10.1016/j.patcog.2019.02.021
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hu B, 2022, SIGNAL PROCESS, V198, DOI 10.1016/j.sigpro.2022.108595
   Hu B, 2019, IEEE T MULTIMEDIA, V21, P2042, DOI 10.1109/TMM.2019.2894958
   Kang L, 2015, IEEE IMAGE PROC, P2791, DOI 10.1109/ICIP.2015.7351311
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu Y., 2020, PROC INT C HIGH PERF, P1
   Ma JP, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102895
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Madhusudana PC, 2022, IEEE T IMAGE PROCESS, V31, P4149, DOI 10.1109/TIP.2022.3181496
   Madhusudana PC, 2022, IEEE WINT CONF APPL, P93, DOI 10.1109/WACVW54805.2022.00015
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ou FZ, 2022, IEEE T MULTIMEDIA, V24, P4197, DOI 10.1109/TMM.2021.3114551
   Pan ZQ, 2022, IEEE T IMAGE PROCESS, V31, P1613, DOI 10.1109/TIP.2022.3144892
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Ren HY, 2018, AAAI CONF ARTIF INTE, P7308
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song TS, 2022, IEEE T CIRC SYST VID, V32, P7592, DOI 10.1109/TCSVT.2022.3179744
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tang ZS, 2019, IEEE T BROADCAST, V65, P138, DOI 10.1109/TBC.2018.2871376
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Wu JJ, 2018, IEEE ACCESS, V6, P12493, DOI 10.1109/ACCESS.2018.2798573
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Wu QB, 2015, J VIS COMMUN IMAGE R, V32, P205, DOI 10.1016/j.jvcir.2015.08.009
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yan J. B., 2020, PROC IEEE INT C MULT, P1
   Yan QS, 2019, IEEE T IMAGE PROCESS, V28, P2200, DOI 10.1109/TIP.2018.2883741
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yang XH, 2021, IEEE T MULTIMEDIA, V23, P4326, DOI 10.1109/TMM.2020.3040529
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhu HC, 2022, IEEE T CIRC SYST VID, V32, P1048, DOI 10.1109/TCSVT.2021.3073410
   Zhu MM, 2021, IEEE INT CONF COMP V, P1953, DOI 10.1109/ICCVW54120.2021.00222
   Zhu WH, 2019, IEEE T MULTIMEDIA, V21, P2334, DOI 10.1109/TMM.2019.2902484
NR 69
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2729
EP 2739
DI 10.1109/TMM.2023.3303725
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400003
DA 2024-08-05
ER

PT J
AU Li, YH
   Chen, H
   Xu, BW
   Zhang, ZC
   Ma, Z
AF Li, Yueheng
   Chen, Hao
   Xu, Bowei
   Zhang, Zicheng
   Ma, Zhan
TI Improving Adaptive Real-Time Video Communication via Cross-Layer
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive bitrate; cross-layer optimization; network condition; video
   encoding parameter; and video content complexity
ID QUALITY ASSESSMENT
AB Effective Adaptive Bitrate (ABR) algorithm or policy is of paramount importance for Real-Time Video Communication (RTVC) amid this pandemic to pursue uncompromised quality of experience (QoE). Existing ABR methods mainly separate the network bandwidth estimation and video encoder control, and fine-tune video bitrate towards estimated bandwidth, assuming the maximization of bandwidth utilization yields the optimal QoE. However, the QoE of an RTVC system is jointly determined by the quality of the compressed video, fluency of video playback, and interaction delay. Solely maximizing the bandwidth utilization without comprehensively considering compound impacts incurred by both transport and video application layers, does not assure a satisfactory QoE. The decoupling of the transport and application layer further exacerbates the user experience due to codec-transport incoordination. This work, therefore, proposes the Palette, a reinforcement learning-based ABR scheme that unifies the processing of transport and video application layers to directly maximize the QoE formulated as the weighted function of video quality, stalling rate, and delay. To this aim, a cross-layer optimization is proposed to derive the fine-grained compression factor of the upcoming frame(s) using cross-layer observations like network conditions, video encoding parameters, and video content complexity. As a result, Palette manages to resolve the codec-transport incoordination and to best catch up with the network fluctuation. Compared with state-of-the-art schemes in real-world tests, Palette not only reduces 3.1%-46.3% of the stalling rate, 20.2%-50.8% of the delay but also improves 0.2%-7.2% of the video quality with comparable bandwidth consumption, under a variety of application scenarios.
C1 [Li, Yueheng; Chen, Hao; Xu, Bowei; Zhang, Zicheng; Ma, Zhan] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.
C3 Nanjing University
RP Ma, Z (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.
EM yueheng.li@smail.nju.edu.cn; chenhao1210@nju.edu.cn;
   xubowei@smail.nju.edu.cn; zichengzhang@smail.nju.edu.cn;
   mazhan@nju.edu.cn
RI Chen, Hao/AGI-0052-2022
OI Chen, Hao/0000-0002-1179-8199; Li, Yueheng/0000-0002-2740-1993
FU National Natural Science Foundation of China
FX No Statement Available
CR AItrans, 2019, ACM multimedia 2019 grand challenge-live video streaming
   Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   [Anonymous], 2007, Standard ITU-R BT.1788
   Bankoski J, 2011, IEEE INT CON MULTI
   Bentaleb A., 2023, PROC IEEE INFO COM C, P1
   Bentaleb A, 2023, IEEE T MULTIMEDIA, V25, P6930, DOI 10.1109/TMM.2022.3216456
   Bentaleb A, 2022, IEEE T MULTIMEDIA, V24, P2300, DOI 10.1109/TMM.2021.3079288
   Berger T, 1998, IEEE T INFORM THEORY, V44, P2693, DOI 10.1109/18.720552
   Bienik J, 2017, ADV ELECTR ELECTRON, V15, P673, DOI 10.15598/aeee.v15i4.2387
   Cardwell N, 2017, COMMUN ACM, V60, P58, DOI 10.1145/3009824
   Carlucci G, 2017, IEEE ACM T NETWORK, V25, P2629, DOI 10.1109/TNET.2017.2703615
   Chen H, 2019, IEEE T PARALL DISTR, V30, P2849, DOI 10.1109/TPDS.2019.2922205
   Chen Y, 2018, PICT COD SYMP, P41, DOI 10.1109/PCS.2018.8456249
   Cisco, 2022, Global-2022 forecast highlights
   F. C. Commission, 2016, Raw data-measuring broadband Amer-ica
   Feng X, 2011, IEEE T BROADCAST, V57, P81, DOI 10.1109/TBC.2010.2092150
   Fouladi S, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P267
   Fujimoto S, 2018, PR MACH LEARN RES, V80
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P9, DOI 10.1109/79.952802
   Haarnoja T, 2018, PR MACH LEARN RES, V80
   Hu H, 2012, IEEE IMAGE PROC, P717, DOI 10.1109/ICIP.2012.6466960
   Huang TC, 2020, IEEE INFOCOM SER, P1967, DOI [10.1109/INFOCOM41043.2020.9155411, 10.1109/infocom41043.2020.9155411]
   Huang TC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P429, DOI 10.1145/3343031.3351014
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   Jain R. K., 1984, Eastern Res. Laboratory, Digit. Equip. Corporation, V21, P1
   Jiang ZQ, 2021, IEEE T BROADCAST, V67, P409, DOI 10.1109/TBC.2020.3028286
   Li Z., 2018, VMAF: The journey continues
   Lillicrap T.P., 2015, Continuous control with deep reinforcement learning
   Ma Z, 2012, IEEE T CIRC SYST VID, V22, P671, DOI 10.1109/TCSVT.2011.2177143
   Mao H., 2020, Ph.D. dissertation
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mnih V, 2016, PR MACH LEARN RES, V48
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Peng F, 2023, IEEE INT CON MULTI, P978, DOI 10.1109/ICME55011.2023.00172
   Qi YN, 2006, IIH-MSP: 2006 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P423
   Rassool R, 2017, IEEE INT SYM BROADB, P351
   Research and Markets, 2022, Global video conferencing market
   Riiser H., 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Schulman J, 2017, Arxiv, DOI [arXiv:1707.06347, DOI 10.48550/ARXIV.1707.06347]
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   T. L. P. Interface, 2001, TC(8)-linux manual page
   TrustRadius, 2020, Covid-19 software industry statistics
   VideoLAN, 2013, x264, the best H.264/AVC encoder-videolan
   W. P. Releases, 2021, Web real-time communications (WEBRTC) transformsthe communications landscape as it becomes a W3C recommendation andIETF standard
   Wang YL, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901772
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WebRTC, 2021, Real-time communication for the web
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang Huanhuan, 2021, MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking, P775, DOI 10.1145/3447993.3483259
   Zhang H, 2020, PROC 26 ANN INT C MO, P1, DOI [10.1145/3372224.3419186, DOI 10.1145/3372224.3419186]
   Zhang X, 2019, IEEE WIREL COMMUN, V26, P178, DOI 10.1109/MWC.2019.1800440
   Zheng QY, 2023, PROCEEDINGS OF THE 7TH ASIA-PACIFIC WORKSHOP ON NETWORKING, APNET 2023, P215, DOI 10.1145/3600061.3600069
   Zhou AF, 2019, MOBICOM'19: PROCEEDINGS OF THE 25TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, DOI 10.1145/3300061.3345430
NR 55
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5369
EP 5382
DI 10.1109/TMM.2023.3331946
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600027
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, YZ
   Xu, R
   Niu, YZ
   Guo, WZ
   Zhao, TS
AF Li, Yuezhou
   Xu, Rui
   Niu, Yuzhen
   Guo, Wenzhong
   Zhao, Tiesong
TI Perceptual Decoupling With Heterogeneous Auxiliary Tasks for Joint
   Low-Light Image Enhancement and Deblurring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Degradation; Image reconstruction; Image enhancement;
   Brightness; Image restoration; Feature extraction; Multiple
   degradations; joint solution; low-light image enhancement; image
   deblurring
AB Capturing images at night are susceptible to inadequate illumination conditions and motion blurring. Given the typical coupling of these two forms of degradation, a pioneer work takes a compact approach of brightening followed by deblurring. However, this sequential approach may compromise informative features and elevate the likelihood of generating unintended artifacts. In this paper, we observe that the co-existing low light and blurs intuitively impair multiple perceptions, making it difficult to produce visually appealing results. To meet these challenges, we propose perceptual decoupling with heterogeneous auxiliary tasks (PDHAT) for joint low-light image enhancement and deblurring. Based on the crucial perceptual properties of the two degradations, we construct two individual auxiliary tasks: coarse preview prediction (CPP) and high-frequency reconstruction (HFR), so that the perception of color, brightness, edges, and details are decoupled into heterogeneous auxiliary tasks to obtain task-specific representations for parallel assisting the main task: joint low-light enhancement and deblurring (LLE-Deblur). Furthermore, we develop dedicated modules to build the network blocks in each branch based on the exclusive properties of each task. Comprehensive experiments are conducted on LOL-Blur and Real-LOL-Blur datasets, showing that our method outperforms existing methods on quantitative metrics and qualitative results.
C1 [Li, Yuezhou; Xu, Rui; Niu, Yuzhen; Guo, Wenzhong] Fuzhou Univ, Fujian Key Lab Network Comp & Intelligent Informat, Coll Comp & Data Sci, Fuzhou 350108, Peoples R China.
   [Li, Yuezhou; Xu, Rui; Niu, Yuzhen; Guo, Wenzhong] Minist Educ, Engn Res Ctr Big Data Intelligence, Fuzhou 350108, Peoples R China.
   [Zhao, Tiesong] Fuzhou Univ, Coll Phys & Informat Engn, Fujian Key Lab Intelligent Proc & Wireless Transmi, Fuzhou 350108, Peoples R China.
C3 Fuzhou University; Fuzhou University
RP Niu, YZ (corresponding author), Fuzhou Univ, Fujian Key Lab Network Comp & Intelligent Informat, Coll Comp & Data Sci, Fuzhou 350108, Peoples R China.
EM liyuezhou.cm@gmail.com; xurui.ryan.chn@gmail.com; yuzhenniu@gmail.com;
   guowenzhong@fzu.edu.cn; t.zhao@fzu.edu.cn
RI Li, Yuezhou/JWO-9859-2024
OI Li, Yuezhou/0000-0002-7397-4661; Xu, Rui/0000-0003-3767-6816
FU National Natural Science Foundation of China
FX No Statement Available
CR Aakerberg A., 2021, PROC 35 C NEURAL INF
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2612, DOI 10.1109/ICCV48922.2021.00263
   Cheng DQ, 2022, IEEE T CIRC SYST VID, V32, P8436, DOI 10.1109/TCSVT.2022.3194169
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Cui Z., 2022, BMVC, P238
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Fu Z., 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P3057
   Fu ZQ, 2023, PROC CVPR IEEE, P22252, DOI 10.1109/CVPR52729.2023.02131
   Gao J., 2023, Collaborative brightening and amplification of low-light imagery via Bi-level adversarial learning, DOI [10.2139/ssrn.4617171, DOI 10.2139/SSRN.4617171]
   Gao JX, 2023, Arxiv, DOI arXiv:2309.05267
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Haohan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8681, DOI 10.1109/CVPR42600.2020.00871
   Hou SY, 2023, IEEE T CIRC SYST VID, V33, P7327, DOI 10.1109/TCSVT.2023.3279981
   Jiang HM, 2017, IEEE T IMAGE PROCESS, V26, P5032, DOI 10.1109/TIP.2017.2713942
   Jinno T, 2012, IEEE T IMAGE PROCESS, V21, P358, DOI 10.1109/TIP.2011.2160953
   Kong LS, 2023, PROC CVPR IEEE, P5886, DOI 10.1109/CVPR52729.2023.00570
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li H., 2023, P AAAI C ART INT, V37, P1314
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Liu Y, 2022, IEEE T MULTIMEDIA, V24, P2890, DOI 10.1109/TMM.2021.3090206
   Lu YC, 2022, IEEE T IMAGE PROCESS, V31, P2390, DOI 10.1109/TIP.2022.3155948
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Ma C, 2022, INT C PATT RECOG, P1664, DOI 10.1109/ICPR56361.2022.9956695
   Mao XT, 2023, AAAI CONF ARTIF INTE, P1905
   Naik SK, 2003, IEEE T IMAGE PROCESS, V12, P1591, DOI 10.1109/TIP.2003.819231
   Tran P, 2021, PROC CVPR IEEE, P11951, DOI 10.1109/CVPR46437.2021.01178
   Ruan L, 2023, ARXIV
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Tsai FJ, 2022, LECT NOTES COMPUT SC, V13679, P146, DOI 10.1007/978-3-031-19800-7_9
   Tu Z., 2022, IEEE C COMPUT VIS PA, P5769
   Wan RJ, 2023, IEEE T MULTIMEDIA, V25, P8006, DOI 10.1109/TMM.2022.3232206
   Wang Chenxi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P8356, DOI 10.1145/3581783.3611907
   Wang Chenxi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P7459, DOI 10.1145/3581783.3611909
   Wang T., 2023, P AAAI C ART INT, P2654, DOI [DOI 10.1609/AAAI.V37I3.25364, 10.1609/AAAI.V37I3.25364]
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P2661, DOI 10.1109/TMM.2022.3149716
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wu H., 2022, PROC IEEE INT C MULT, P1
   Wu YH, 2023, PROC CVPR IEEE, P1662, DOI 10.1109/CVPR52729.2023.00166
   Xing WZ, 2021, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR46437.2021.00351
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Yan QS, 2023, PROC CVPR IEEE, P22211, DOI 10.1109/CVPR52729.2023.02127
   Yan QS, 2023, IEEE T IMAGE PROCESS, V32, P2857, DOI 10.1109/TIP.2023.3251029
   Yang X., 2021, IEEE Trans. Geosci. Remote Sens., V60
   Yu W, 2023, NEUROCOMPUTING, V554, DOI 10.1016/j.neucom.2023.126584
   Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2023, IEEE T PATTERN ANAL, V45, P1934, DOI 10.1109/TPAMI.2022.3167175
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang XX, 2021, IEEE T MULTIMEDIA, V23, P3215, DOI 10.1109/TMM.2020.3021989
   Zhang Y, 2022, IEEE T IMAGE PROCESS, V31, P759, DOI 10.1109/TIP.2021.3135473
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou SC, 2022, LECT NOTES COMPUT SC, V13666, P573, DOI 10.1007/978-3-031-20068-7_33
NR 58
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6663
EP 6675
DI 10.1109/TMM.2024.3355634
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600029
DA 2024-08-05
ER

PT J
AU Lin, JY
   Hua, H
   Chen, M
   Li, YK
   Hsiao, JH
   Ho, CM
   Luo, JB
AF Lin, Jingyang
   Hua, Hang
   Chen, Ming
   Li, Yikang
   Hsiao, Jenhao
   Ho, Chiuman
   Luo, Jiebo
TI VideoXum: Cross-Modal Visual and Textural Summarization of Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Task analysis; Semantics; Visualization; Benchmark testing;
   Coherence; Training; Cross-modal video summarization; video captioning;
   video summarization
AB Video summarization aims to distill the most important information from a source video into either an abridged video clip or a textual narrative. Existing methods often treat the generation of video and text summaries as independent tasks, thus neglecting the semantic correlation between visual and textual summarization. In other words, these methods only study a single modality as output without considering coherent video and text as outputs. In this work, we first introduce a novel task: cross-modal video summarization. This task seeks to transfer a long video into a condensed video clip and a semantically aligned textual summary, collectively referred to as a cross-modal summary. We then establish VideoXum (X refers to different modalities), a new large-scale human-annotated video benchmark for cross-modal video summarization. VideoXum is reannotated based on ActivityNet Captions with diverse open-domain videos. In the current version, VideoXum provides 14 K long videos, with a total of 140 K pairs of aligned video and text summaries. Compared to existing datasets, VideoXum offers superior scalability while preserving a comparable level of annotation quality. To validate the dataset's quality, we provide a comprehensive analysis of VideoXum, comparing it with existing datasets. Further, we perform an extensive empirical evaluation of several state-of-the-art methods on this dataset. Our findings highlight the impressive generalization capability of the vision-language encoder-decoder framework yields on VideoXum. Particularly, we propose VTSUM-BLIP, an end-to-end framework, serving as a strong baseline for this novel benchmark. Moreover, we adapt CLIPScore for VideoXum to measure the semantic consistency of cross-modal summaries effectively.
C1 [Lin, Jingyang; Hua, Hang; Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
   [Chen, Ming; Li, Yikang; Hsiao, Jenhao; Ho, Chiuman] OPPO US Res Ctr, Palo Alto, CA 94303 USA.
C3 University of Rochester
RP Luo, JB (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
EM jlin81@ur.rochester.edu; hhua2@ur.rochester.edu; cmelf0819@gmail.com;
   lyk010632@gmail.com; mhsiao.pro@gmail.com; chiuman100@gmail.com;
   jluo@cs.rochester.edu
CR Achiam OJ, 2023, Arxiv, DOI arXiv:2303.08774
   Alayrac J.-B., 2022, NeurIPS, V35, P23716, DOI DOI 10.48550/ARXIV.2204.14198
   Arora S, 2018, PR MACH LEARN RES, V80
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Bao H., 2022, Advances in Neural Information Pro- cessing Systems, V35, P32897
   Beltagy I, 2020, Arxiv, DOI arXiv:2004.05150
   Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Chen B.-C., 2017, PROCBRIT MACH VIS C
   Chen David, 2011, ACL
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Du Yifan, 2022, P INT JOINT C ART IN, P5436
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Fu TJ, 2019, IEEE WINT CONF APPL, P1579, DOI 10.1109/WACV.2019.00173
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Grabska-Gradzinska I., 2021, PROC AUSTRALASCOMPUT, P1
   Graves A, 2012, STUD COMPUT INTELL, V385, P37
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   Hessel J, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P7514
   Hua H, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P3229
   Hua Hang, 2022, Fine-tuning Pre-trained Language Models with Noise Stability Regularization
   Irie G, 2010, P 18 ACM INT C MULTI, P839, DOI 10.1145/1873951.1874092
   Ji Z, 2020, NEUROCOMPUTING, V405, P200, DOI 10.1016/j.neucom.2020.04.132
   Ju C, 2022, LECT NOTES COMPUT SC, V13695, P105, DOI 10.1007/978-3-031-19833-5_7
   Jungin Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P647, DOI 10.1007/978-3-030-58595-2_39
   Kendall MG, 1945, BIOMETRIKA, V33, P239, DOI 10.2307/2332303
   Kim W, 2021, PR MACH LEARN RES, V139
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lal S, 2019, IEEE WINT CONF APPL, P471, DOI 10.1109/WACV.2019.00056
   Lei Jie, 2021, ADV NEURAL INFORM PR, V34, P3
   Lewis M, 2019, P 58 ANN M ASS COMP, DOI [10.18653/v1/2020.acl-main.703, DOI 10.18653/V1/2020.ACL-MAIN.703]
   Li JN, 2022, PR MACH LEARN RES
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Loshchilov I., 2017, INT C LEARNING REPRE
   Loshchilov I., 2018, INT C LEARN REPR
   Lu JS, 2019, ADV NEUR IN, V32
   Luo HS, 2020, Arxiv, DOI arXiv:2002.06353
   Narasimhan M, 2021, 35 C NEURAL INFORM P, V34
   Ni B, 2022, LECT NOTES COMPUT SC, V13664, P1, DOI 10.1007/978-3-031-19772-7_1
   Otani Mayu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7588, DOI 10.1109/CVPR.2019.00778
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Patrick M., 2021, PROC INT C LEARN REP
   Radford A, 2021, PR MACH LEARN RES, V139
   Raffel C, 2020, J MACH LEARN RES, V21
   Regneri M., 2013, T ASSOC COMPUT LING, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Ruan Q, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P1292
   Saquil Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1698, DOI 10.1109/ICCV48922.2021.00174
   Singer U, 2023, PROC INT C LEARN REP
   Singh A, 2022, PROC CVPR IEEE, P15617, DOI 10.1109/CVPR52688.2022.01519
   Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Su W., 2020, PROC INT C LEARN REP
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Touvron H, 2023, Arxiv, DOI [arXiv:2307.09288, DOI 10.48550/ARXIV.2307.09288]
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang J, 2022, Trans. Mach. Learn. Res., V2022
   Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751
   Wang K., 2019, Advances in Neural Information Processing Systems, V32, p14 648
   Wang W., 2020, Advances in Neural Information Processing Systems, V33, P5776
   Wu CF, 2021, Arxiv, DOI arXiv:2104.14806
   Xu HY, 2023, Arxiv, DOI arXiv:2302.00402
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xue H. W., 2021, ADV NEURAL INF PROCE, P4514
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu J, 2022, Trans. Mach. Learn. Res., V2022
   Yuan L., 2021, Florence: A new foundation model for computer vision
   Yuan Y, 2019, IEEE ACCESS, V7, P64676, DOI 10.1109/ACCESS.2019.2916989
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang T., 2020, PROC INT C LEARN REP
   Zhao B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P863, DOI 10.1145/3123266.3123328
   Zhao B, 2018, PROC CVPR IEEE, P7405, DOI 10.1109/CVPR.2018.00773
   Zhao X., 2019, PROC ACM TURING CELE, P1
   Zhou KY, 2018, AAAI CONF ARTIF INTE, P7582
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zoph B, 2020, ARXIV200606882, DOI DOI 10.48550/ARXIV.2006.06882
   Zwillinger Daniel, 1999, CRC standard probability and statistics tables and for-mulae
NR 89
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5548
EP 5560
DI 10.1109/TMM.2023.3335875
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600018
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, DZ
   Zhu, JH
   Fang, X
   Xiong, ZY
   Wang, H
   Li, RF
   Zhou, P
AF Liu, Daizong
   Zhu, Jiahao
   Fang, Xiang
   Xiong, Zeyu
   Wang, Huan
   Li, Renfu
   Zhou, Pan
TI Conditional Video Diffusion Network for Fine-Grained Temporal Sentence
   Grounding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal sentence grounding (TSG); diffusion models
AB Temporal sentence grounding (TSG) aims to locate a semantically related segment of an untrimmed video guided by a sentence query. Since the untrimmed videos are too long, almost all existing TSG works first sparsely down-sample each video into a shorter video of a fixed length and then conduct multimodal interactions with the query sentence for reasoning. However, this video down-sampling process may introduce a challenging issue that confuses the latter grounding process: Due to the video down-sampling, some query-related frames may be filtered out; this process may remove the specific boundary frames of the target segment and take the adjacent irrelevant frames as new boundaries, easily leading to cross-modal misalignment and introducing both boundary-bias and reasoning-bias. Therefore, it is important to keep the grounding consistency (both temporal annotations and boundary predictions) between the original and the sampled videos. To this end, in this paper, we propose a novel Conditional Video Diffusion Network (CVDN) for TSG to learn extra visual semantics to enrich and refine the biased new boundaries, which enables soft-label boundary prediction for fine-grained frame-query reasoning. Specifically, we first construct a conditional video diffusion model which is separately trained to recover the consecutive semantics of the filtered frames between the adjacent sampled frames. Through the designed stochastic interval sampling strategies in the training process, this diffusion model is able to generate absent coherent semantics between the sparsely sampled frames and in turn enrich and refine them, benefiting the integral activity understanding for TSG. In this manner, the incorrect new boundaries will be refined to be closely correlated to the original boundary frames and contain sufficient query-related information, which is crucial for accurate segment prediction. Extensive experiments on three challenging datasets demonstrate the effectiveness of CVDN.
C1 [Liu, Daizong] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
   [Zhu, Jiahao; Xiong, Zeyu; Zhou, Pan] Huazhong Univ Sci & Technol, Hubei Engn Res Ctr Big Data Secur, Sch Cyber Sci & Engn, Wuhan 430074, Peoples R China.
   [Fang, Xiang] Nanyang Technol Univ, Energy Res Inst NTU, Interdisciplinary Grad Programme, Singapore 639798, Singapore.
   [Wang, Huan] Huazhong Agr Univ, Coll Informat, Wuhan 430070, Peoples R China.
   [Li, Renfu] Huazhong Univ Sci & Technol, Sch Aerosp Engn, Wuhan 430074, Peoples R China.
C3 Peking University; Huazhong University of Science & Technology; Nanyang
   Technological University; Huazhong Agricultural University; Huazhong
   University of Science & Technology
RP Zhou, P (corresponding author), Huazhong Univ Sci & Technol, Hubei Engn Res Ctr Big Data Secur, Sch Cyber Sci & Engn, Wuhan 430074, Peoples R China.; Li, RF (corresponding author), Huazhong Univ Sci & Technol, Sch Aerosp Engn, Wuhan 430074, Peoples R China.
EM dzliu@stu.pku.edu.cn; jiahaozhu@hust.edu.cn; xfang9508@gmail.com;
   zeyuxiong@hust.edu.cn; hwang@mail.hzau.edu.cn; renfu.li@hust.edu.cn;
   panzhou@hust.edu.cn
RI Li, Renfu/S-7297-2019
OI Li, Renfu/0000-0001-6841-024X; Fang, Xiang/0000-0003-3231-5771; liu,
   daizong/0000-0001-8179-4508
FU National Natural Science Foundation of China
FX No Statement Available
CR Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cai Ruojin, 2020, EUR C COMP VIS, P364, DOI [DOI 10.1007/978-3-030-58580-822, DOI 10.1007/978-3-030-58580-8_22]
   Cao M, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9810
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen N., 2020, PROC INT C LEARN REP, P1
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chung J, 2014, PREPRINT
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang X, 2023, IEEE T MULTIMEDIA, V25, P7517, DOI 10.1109/TMM.2022.3222965
   Fang Xiang, 2020, IEEE Trans Artif Intell, V1, P233, DOI 10.1109/TAI.2021.3052425
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Gong GQ, 2023, IEEE T MULTIMEDIA, V25, P7402, DOI 10.1109/TMM.2022.3222664
   Harvey W, 2022, ADV NEURAL INF PROCE, P27953
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hertz A, 2024, arXiv
   Ho J., 2022, PROC NEURAL INF PROC, P1
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hoppe T., 2022, Trans. Mach. Learn. Res.
   Jolicoeur-Martineau R., 2020, INT C LEARN REPRESEN, P1
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Köksal A, 2024, IEEE T MULTIMEDIA, V26, P190, DOI 10.1109/TMM.2023.3262972
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Liu D., 2022, P 29 INT C COMP LING, P5532
   Liu DZ, 2023, IEEE T MULTIMEDIA, V25, P8539, DOI 10.1109/TMM.2023.3238514
   Liu DZ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4092, DOI 10.1145/3503161.3547969
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1665
   Liu DZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9292
   Liu H., 2022, IEEE Trans. Multimed., P1, DOI [10.1109/TMM.2022.3197364, DOI 10.1109/TMM.2022.3197364]
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu YX, 2023, IEEE T MULTIMEDIA, V25, P2851, DOI 10.1109/TMM.2022.3152086
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Luo S., 2022, IEEE Trans. Multime-dia, DOI [10.1109/TMM.2022.3228167.[5]H., DOI 10.1109/TMM.2022.3228167.[5]H]
   Mittal G, 2021, Arxiv, DOI [arXiv:2103.16091, 10.48550/arXiv.2103.16091, DOI 10.48550/ARXIV.2103.16091]
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Nichol A, 2022, Arxiv, DOI arXiv:2112.10741
   Nichol P., 2021, Improved denoising diffusion probabilistic models, P8162, DOI DOI 10.48550/ARXIV.2102.09672
   Nie DY, 2023, IEEE T MULTIMEDIA, V25, P6436, DOI 10.1109/TMM.2022.3208740
   Niu CH, 2020, PR MACH LEARN RES, V108, P4474
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Regneri M., 2013, T ASSOC COMPUT LING, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/wacv45572.2020.9093328, 10.1109/WACV45572.2020.9093328]
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saharia C., 2022, PROC ACM SIGGRAPH C, P1
   Shaw P., 2018, Self-attention with relative position representations, P464
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singha J, 2018, NEURAL COMPUT APPL, V29, P1129, DOI 10.1007/s00521-016-2525-z
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song S., 2019, Neural Inf. Process. Syst., P1
   Song Y., 2020, PROC INT C LEARN REP
   Song Y., 2020, Adv Neural Inf Process Syst, V33, P12438
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Tao ZL, 2023, IEEE T MULTIMEDIA, V25, P5107, DOI 10.1109/TMM.2022.3187556
   Unterthiner T, 2019, Arxiv, DOI [arXiv:1812.01717, 10.48550/arXiv.1812.01717]
   Vaswani A, 2017, ADV NEUR IN, V30
   Voleti V., 2022, Advances in neural information processing systems, V35, P23371
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P6329, DOI 10.1109/TMM.2022.3207581
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Wu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10013, DOI 10.1109/ICCV48922.2021.00988
   Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1
   Xiao SN, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P4008
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu Z, 2023, IEEE T MULTIMEDIA, V25, P6121, DOI 10.1109/TMM.2022.3205404
   Yang RH, 2022, Arxiv, DOI arXiv:2203.09481
   Yuan BW, 2024, IEEE T MULTIMEDIA, V26, P1255, DOI 10.1109/TMM.2023.3278992
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, P IEEECVF C COMPUTER, P10287
   Zeng ZL, 2023, IEEE T MULTIMEDIA, V25, P2176, DOI 10.1109/TMM.2022.3144066
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang H., 2020, P 58 ANN M ASS COMPU, P6543
   Zhang H, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P776
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhu W, 2019, Advances in Neural Information Processing Systems, P534
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
   Zhuet J., 2023, arXiv
NR 90
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5461
EP 5476
DI 10.1109/TMM.2023.3334019
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800004
DA 2024-08-05
ER

PT J
AU Liu, JM
   Wu, Y
   Gong, MG
   Liu, ZX
   Miao, QG
   Ma, WP
AF Liu, Jiaming
   Wu, Yue
   Gong, Maoguo
   Liu, Zhixiao
   Miao, Qiguang
   Ma, Wenping
TI Inter-Modal Masked Autoencoder for Self-Supervised Learning on Point
   Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Transformers; Task analysis; Standards;
   Computer architecture; Decoding; Self-supervised learning;
   Self-supervision; masked autoencoder; joint multimodality; point cloud
   understanding
AB Masked autoencoder (MAE) is a recently widely used self-supervised learning method that has achieved great success in NLP and computer vision. However, the potential advantages of masked pre-training for point cloud understanding have not been fully explored. There is preliminary work on MAE-based point clouds using the Transformer architecture to explore low-level geometric representations in 3D space, which is insufficient for fine-grained decoding completion and downstream tasks. Inspired by multimodality, we propose Inter-MAE, a inter-modal MAE method for self-supervised learning on point clouds. Specifically, we first use Point-MAE as a baseline to partition point clouds into random low percentage of visible and high percentage of masked point patches. Then, a standard Transformer-based autoencoder is built by asymmetric design and shifting mask operations, and latent features are learned from the visible point patches aiming to recover the masked point patches. In addition, we generate image features based on ViT after point cloud rendering to form inter-modal contrastive learning with the decoded features of the completed point patches. Extensive experiments show that the proposed Inter-MAE generates pre-trained models that are effective and exhibit superior results in various downstream tasks. For example, an accuracy of 85.4% is achieved on ScanObjectNN and 86.3% on ShapeNetPart, outperforming other state-of-the-art self-supervised learning methods. Notably, our work establishes for the first time the feasibility of applying image modality to masked point clouds.
C1 [Liu, Jiaming; Wu, Yue; Miao, Qiguang] Xidian Univ, Sch Comp Sci & Technol, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
   [Gong, Maoguo] Xidian Univ, Sch Elect Engn, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
   [Liu, Zhixiao] Harbin Engn Univ, Yantai Res Inst, Yantai 264006, Peoples R China.
   [Ma, Wenping] Xidian Univ, Sch Artificial Intelligence, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University; Harbin Engineering University;
   Xidian University
RP Wu, Y (corresponding author), Xidian Univ, Sch Comp Sci & Technol, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
EM ljm@stu.xidian.edu.cn; ywu@xidian.edu.cn; gong@ieee.org;
   robinliu@hrbeu.edu.cn; qgmiao@mail.xidian.edu.cn;
   wpma@mail.xidian.edu.cn
OI Liu, Jiaming/0009-0003-9699-1987; Liu, Zhixiao/0009-0007-3517-9556
FU National Natural Science Foundation of China
FX No Statement Available
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Afham M, 2022, PROC CVPR IEEE, P9892, DOI 10.1109/CVPR52688.2022.00967
   Brown T. B., 2020, P 34 INT C NEURAL IN
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eckart B, 2021, PROC CVPR IEEE, P8244, DOI 10.1109/CVPR46437.2021.00815
   El Banani M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6413, DOI 10.1109/ICCV48922.2021.00637
   Engel N, 2021, IEEE ACCESS, V9, P134826, DOI 10.1109/ACCESS.2021.3116304
   Gidaris S, 2018, ICLR, DOI DOI 10.1109/ICCV.2019.00156
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han XF, 2023, IEEE T MULTIMEDIA, V25, P5638, DOI 10.1109/TMM.2022.3198318
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hess G, 2023, IEEE WINT CONF APPL, P350, DOI 10.1109/WACVW58289.2023.00039
   Hou J, 2021, PROC CVPR IEEE, P15582, DOI 10.1109/CVPR46437.2021.01533
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Lai X, 2022, PROC CVPR IEEE, P8490, DOI 10.1109/CVPR52688.2022.00831
   Li YY, 2018, ADV NEUR IN, V31
   Li ZY, 2022, AAAI CONF ARTIF INTE, P1500
   Liu H, 2022, LECT NOTES COMPUT SC, V13662, P657, DOI 10.1007/978-3-031-20086-1_38
   Liu JH, 2023, IEEE T MULTIMEDIA, V25, P5649, DOI 10.1109/TMM.2022.3198011
   Liu MH, 2023, PROC CVPR IEEE, P21736, DOI 10.1109/CVPR52729.2023.02082
   Loshchilov I., 2017, INT C LEARNING REPRE
   Lv CL, 2022, IEEE T MULTIMEDIA, V24, P1815, DOI 10.1109/TMM.2021.3073265
   Mohammadi SS, 2021, IEEE IMAGE PROC, P3103, DOI 10.1109/ICIP42928.2021.9506426
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pang Y, 2022, LECT NOTES COMPUT SC, V13662, P604, DOI 10.1007/978-3-031-20086-1_35
   Plummer BA, 2022, IEEE T PATTERN ANAL, V44, P2155, DOI 10.1109/TPAMI.2020.3029008
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qian G. C., 2022, ADV NEURAL INFORM PR, V35, P23192, DOI [DOI 10.48550/ARXIV.2206.04670, https://doi.org/10.48550/arXiv.2206.04670]
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Pham QH, 2020, AAAI CONF ARTIF INTE, V34, P11856
   Raffel C, 2020, J MACH LEARN RES, V21
   Ran HX, 2022, PROC CVPR IEEE, P18920, DOI 10.1109/CVPR52688.2022.01837
   Ran HX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15457, DOI 10.1109/ICCV48922.2021.01519
   Rolfe J. T., 2016, PROC INT C LEARN REP
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Sauder J, 2019, ADV NEUR IN, V32
   Sun C, 2023, IEEE T MULTIMEDIA, V25, P6207, DOI 10.1109/TMM.2022.3206664
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9762, DOI 10.1109/ICCV48922.2021.00964
   Wang W, 2019, ADV NEUR IN, V32
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu Y, 2024, IEEE T MULTIMEDIA, V26, P1626, DOI 10.1109/TMM.2023.3284591
   Wu Y, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3286943
   Wu Y, 2023, IEEE T CIRC SYST VID, V33, P3585, DOI 10.1109/TCSVT.2023.3237328
   Wu Y, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3271757
   Wu Y, 2022, IEEE T NEUR NET LEAR, V33, P4257, DOI 10.1109/TNNLS.2021.3056238
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xie ZD, 2022, PROC CVPR IEEE, P9643, DOI 10.1109/CVPR52688.2022.00943
   Xu C., 2021, PROC EUR C COMPUT VI, P638
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan CX, 2024, IEEE T PATTERN ANAL, V46, P1530, DOI 10.1109/TPAMI.2021.3140070
   Yan SM, 2024, Arxiv, DOI arXiv:2304.06911
   Yan SM, 2023, Arxiv, DOI [arXiv:2201.00785, 10.48550/arXiv.2201.00785]
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Yuan X, 2021, PROC CVPR IEEE, P6991, DOI 10.1109/CVPR46437.2021.00692
   Zha YH, 2023, Arxiv, DOI arXiv:2304.07221
   Zhang C, 2022, PROC CVPR IEEE, P11789, DOI 10.1109/CVPR52688.2022.01150
   Zhang C, 2022, INT J INTELL SYST, V37, P11985, DOI 10.1002/int.23073
   Zhang R., 2022, Advances in neural information processing systems, VVolume 35, P27061
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang ZW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10232, DOI 10.1109/ICCV48922.2021.01009
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 78
TC 2
Z9 2
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3897
EP 3908
DI 10.1109/TMM.2023.3317998
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300019
DA 2024-08-05
ER

PT J
AU Liu, JW
   Wang, WN
   Chen, SH
   Zhu, XX
   Liu, J
AF Liu, Jiawei
   Wang, Weining
   Chen, Sihan
   Zhu, Xinxin
   Liu, Jing
TI Sounding Video Generator: A Unified Framework for Text-Guided Sounding
   Video Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Text-guided sounding-video generation; video-audio representation;
   contrastive learning; transformer
AB As a combination of visual and audio signals, video is inherently multi-modal. However, existing video generation methods are primarily intended for the synthesis of visual frames, whereas audio signals in realistic videos are disregarded. In this work, we concentrate on a rarely investigated problem of text-guided sounding video generation and propose the <bold>S</bold>ounding <bold>V</bold>ideo <bold>G</bold>enerator (SVG), a unified framework for generating realistic videos along with audio signals. Specifically, we present the SVG-VQGAN to transform visual frames and audio mel-spectrograms into discrete tokens. SVG-VQGAN applies a novel hybrid contrastive learning method to model inter-modal and intra-modal consistency and improve the quantized representations. A cross-modal attention module is employed to extract associated features of visual frames and audio signals for contrastive learning. Then, a Transformer-based decoder is used to model associations between texts, visual frames, and audio signals at token level for auto-regressive sounding video generation. AudioSet-Cap, a human annotated text-video-audio paired dataset, is produced for training SVG. Experimental results demonstrate the superiority of our method when compared with existing text-to-video generation methods as well as audio generation methods on Kinetics and VAS datasets.
C1 [Liu, Jiawei; Wang, Weining; Chen, Sihan; Zhu, Xinxin; Liu, Jing] Chinese Acad Sci, Inst Automat, Lab Cognitionand Decis Intelligence Complex Syst, Beijing 100190, Peoples R China.
   [Liu, Jiawei; Chen, Sihan; Liu, Jing] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100040, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Liu, J (corresponding author), Chinese Acad Sci, Inst Automat, Lab Cognitionand Decis Intelligence Complex Syst, Beijing 100190, Peoples R China.
EM liujiawei2020@ia.ac.cn; weining.wang@nlpr.ia.ac.cn;
   chensihan2019@ia.ac.cn; xinxin.zhu@nlpr.ia.ac.cn; jliu@nlpr.ia.ac.cn
RI Liu, Jing/A-7644-2016; Chen, Sihan/GRK-5890-2022
OI Chen, Sihan/0000-0003-4246-2476; liu, jing/0000-0003-0903-9131; Wang,
   Weining/0000-0001-7299-6431
FU National Key Research and Development Program of China
FX No Statement Available
CR Alwassel H., 2020, NEURIPS, V33, P9758
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Balaji Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1995
   Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI [10.1109/ICASSP40776.2020.9053174, 10.1109/icassp40776.2020.9053174]
   Chen PH, 2020, IEEE T IMAGE PROCESS, V29, P8292, DOI 10.1109/TIP.2020.3009820
   Chuang Gan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P758, DOI 10.1007/978-3-030-58621-8_44
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Di SZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2037, DOI 10.1145/3474085.3475195
   Ding M., 2021, ADV NEURAL INFORM PR, V34
   Ebert Frederik, 2017, ARXIV171005268, P344, DOI DOI 10.48550/ARXIV.1710.05268
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Gan C., 2021, PROC NEURAL INF PROC
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hensel M, 2017, ADV NEUR IN, V30
   Hong W., 2022, arXiv
   Iashin V., 2021, PROC BRIT MACH VIS C, P2
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kim CD, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P119
   Kim N, 2021, IEEE T MULTIMEDIA, V23, P3986, DOI 10.1109/TMM.2020.3035281
   Kingma D.P., 2014, Proc. of ICLR
   Kong J., 2020, Advances in Neural Information Processing Systems, V33, P17022
   Koutini K, 2022, INTERSPEECH, P2753, DOI 10.21437/Interspeech.2022-227
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar K., 2019, ADV NEURAL INFORM PR, P14881
   Li YT, 2018, AAAI CONF ARTIF INTE, P7065
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140
   Loshchilov I., 2017, INT C LEARNING REPRE
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Min S., 2021, arXiv
   Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264
   Radford A, 2021, PR MACH LEARN RES, V139
   Rakhimov R, 2021, VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL. 5: VISAPP, P101, DOI 10.5220/0010241801010112
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Razavi A, 2019, ADV NEUR IN, V32
   Ren Yi, 2019, ADV NEURAL INFORM PR, P3165
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Su K., 2020, Advances in Neural Information Processing Systems, V33, P3325
   Tian Y., 2021, PROC INT C LEARN REP
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   van den Oord A, 2017, ADV NEUR IN, V30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Wu CF, 2022, LECT NOTES COMPUT SC, V13676, P720, DOI 10.1007/978-3-031-19787-1_41
   Wu CF, 2021, Arxiv, DOI arXiv:2104.14806
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1007/s11263-019-01198-w, 10.1109/CSTIC.2018.8369274]
   Xiao FY, 2020, Arxiv, DOI arXiv:2001.08740
   Xie JH, 2023, IEEE T MULTIMEDIA, V25, P4894, DOI 10.1109/TMM.2022.3183394
   Yan W., 2021, arXiv, DOI DOI 10.48550/ARXIV.2104.10157
   Zeng Zhaoyang, 2021, ADV NEURAL INFORM PR, V34, P7025
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
NR 63
TC 1
Z9 1
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 141
EP 153
DI 10.1109/TMM.2023.3262180
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500024
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, RS
   Cheng, Y
   Huang, SF
   Li, CY
   Cheng, X
AF Liu, Renshuai
   Cheng, Yao
   Huang, Sifei
   Li, Chengyang
   Cheng, Xuan
TI Transformer-Based High-Fidelity Facial Displacement Completion for
   Detailed 3D Face Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Displacement map; face completion; face reconstruction; transformer
ID SINGLE IMAGE
AB In this paper, we tackle a special face completion task, facial displacement completion, which can offer a key component for many single image 3D face reconstruction systems. To produce a detailed 3D face with ear-to-ear complete displacement UV map, we propose a novel Displacement Completion method based on Transformer (DCT). Current transformer based image inpainting methods usually follow a two-stage scheme, which firstly recovers the masked pixels in low resolution with transformer, and then replenishes the inpainting result in high resolution with GAN. Although these methods have achieved great success, they suffer from information loss from two aspects when applied in face completion: 1) The downsampling operation makes transformer only produce a coarse appearance prior for GAN, incurring middle and low level information loss. 2) Some meaningful facial semantics can be well captured by transformer and further benefit the completion, but it's has not yet been explored. Motivated by the above considerations, we come up with three key designs in the proposed DCT: PCA tokenization, BERT-style learning, and style modulation. Firstly, we use PCA tokenization to replace the downsampling in transformer to preserve more meaningful structures. Secondly, we make transformer simulate the two tasks in BERT, Masked Language Model (MLM) and Next Sentence Prediction (NSP), for both masked pixels and facial attributes recovery. Thirdly, we encode the outcome of transformer as the latent code to guide an image translation network in the StyleGAN2 modulation way. Experments on both FaceScape dataset and in-the-wild data demonstrate DCT's better performance compared with other transformer based or GAN based completion methods.
C1 [Liu, Renshuai; Li, Chengyang; Cheng, Xuan] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
   [Cheng, Yao; Huang, Sifei] China Mobile Hangzhou Informat Technol Co Ltd, Hangzhou 311121, Peoples R China.
C3 Xiamen University
RP Cheng, X (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
EM medalwill@stu.xmu.edu.cn; chengyao@cmhi.chinamobile.com;
   huangsifei@cmhi.chinamobile.com; chengyanglee@stu.xmu.edu.cn;
   chengxuan@xmu.edu.cn
RI li, chengyang/KDN-7899-2024
OI Liu, Renshuai/0000-0002-4029-6569
FU Natural Science Foundation of Xiamen
FX No Statement Available
CR Tran AT, 2018, PROC CVPR IEEE, P3935, DOI 10.1109/CVPR.2018.00414
   Bao H., 2022, P INT C LEARN REPR
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chai ML, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818112
   Chen AP, 2019, IEEE I CONF COMP VIS, P9428, DOI 10.1109/ICCV.2019.00952
   Chen JS, 2022, AAAI CONF ARTIF INTE, P294
   Chen M, 2020, PR MACH LEARN RES, V119
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Deng Y, 2022, PROC CVPR IEEE, P10663, DOI 10.1109/CVPR52688.2022.01041
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Fan X, 2021, IEEE T MULTIMEDIA, V23, P1252, DOI 10.1109/TMM.2020.2994506
   Gecer B, 2021, PROC CVPR IEEE, P7624, DOI 10.1109/CVPR46437.2021.00754
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Hong Y, 2022, PROC CVPR IEEE, P20342, DOI 10.1109/CVPR52688.2022.01973
   Huynh L, 2018, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR.2018.00877
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Islam M. A., 2020, P 8 INT C LEARN REPR, P1
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13970, DOI 10.1109/ICCV48922.2021.01373
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Lin CH, 2019, IEEE I CONF COMP VIS, P4511, DOI 10.1109/ICCV.2019.00461
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2021, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR46437.2021.00925
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Loshchilov I., 2018, INT C LEARN REPR
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Nazeri K., 2019, arXiv
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175
   Tong Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7677, DOI 10.1109/CVPR42600.2020.00770
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4672, DOI 10.1109/ICCV48922.2021.00465
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P69, DOI 10.1145/3474085.3475436
   Zeng XX, 2019, IEEE I CONF COMP VIS, P2315, DOI 10.1109/ICCV.2019.00240
   Zhao L, 2020, PROC CVPR IEEE, P5740, DOI 10.1109/CVPR42600.2020.00578
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhou H, 2020, PROC CVPR IEEE, P5910, DOI 10.1109/CVPR42600.2020.00595
NR 52
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 799
EP 810
DI 10.1109/TMM.2023.3271816
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700049
DA 2024-08-05
ER

PT J
AU Luo, X
   Jiang, M
   Kong, J
   Tao, XF
AF Luo, Xi
   Jiang, Min
   Kong, Jun
   Tao, Xuefeng
TI Hierarchical Camera-Aware Contrast Extension for Unsupervised Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Task analysis; Training; Self-supervised learning;
   Optimization; Noise measurement; Pedestrians; Fully unsupervised
   learning; person re-identification; camera-aware contrastive learning;
   instances extension
ID ATTRIBUTE; MEMORY
AB Unsupervised person re-identification (Re-ID) targets to learn discriminative representations without annotations. Recently, clustering-based methods have shown promising performance, which utilize clustering to generate identity pseudo labels for model optimization. Large intra-class variance mainly caused by domain discrepancy among cameras could lead to noisy clustering results. However, abundant camera-aware sample pairs relations have not been exploited fully to facilitate learning of features with comprehensive knowledge, so as to tackle this issue. In this paper, we propose hierarchical camera-aware contrast extension (HCACE) for unsupervised person Re-ID. Firstly, cognitive collaboration contrast scheme (CCCS) is introduced to explore hierarchical camera-aware relations at the proxy-level, so as to collaboratively promote model to learn representative knowledge. Secondly, aggregative instance contrast extension scheme (AICES) is proposed to promote the learning of potential fine-grained knowledge by aggregating refined camera-aware inter-instance relations. Especially in AICES, hard negative instance extension (HNIE) is designed to generate extended negative instances, so as to assist the exploration of transitional cross-camera inter-instance relations. Finally, extensive experiments on three benchmark datasets validate superior performance of proposed HCACE.
C1 [Luo, Xi; Jiang, Min] Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
   [Kong, Jun; Tao, Xuefeng] Jiangnan Univ, Key Lab Adv Proc Control Light Ind, Minist Educ, Wuxi 214122, Peoples R China.
C3 Jiangnan University; Jiangnan University
RP Jiang, M (corresponding author), Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
EM minjiang@jiangnan.edu.cn
RI JIANG, MIN/KSM-4856-2024
OI Kong, Jun/0000-0003-2551-4748; Tao, Xuefeng/0000-0003-1142-619X
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Cheng D, 2022, IEEE T IMAGE PROCESS, V31, P3334, DOI 10.1109/TIP.2022.3169693
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2019, IEEE T GEOSCI REMOTE, V57, P1741, DOI 10.1109/TGRS.2018.2868851
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fu DP, 2021, PROC CVPR IEEE, P14745, DOI 10.1109/CVPR46437.2021.01451
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Han CC, 2023, IEEE T PATTERN ANAL, V45, P7319, DOI 10.1109/TPAMI.2022.3221079
   Han CC, 2021, AAAI CONF ARTIF INTE, V35, P1505
   Han XM, 2023, IEEE T IMAGE PROCESS, V32, P29, DOI 10.1109/TIP.2022.3224325
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kingma D. P., 2014, arXiv
   Kong J, 2023, IEEE T MULTIMEDIA, V25, P1903, DOI 10.1109/TMM.2022.3220115
   Li J., 2021, PROC INT C LEARN REP
   Li M., 2018, EUR C COMPUT VIS, P737
   Li MK, 2022, IEEE T IMAGE PROCESS, V31, P3606, DOI 10.1109/TIP.2022.3173163
   Li YY, 2022, IEEE T MULTIMEDIA, V24, P415, DOI 10.1109/TMM.2021.3052354
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Liu TY, 2022, IEEE T IMAGE PROCESS, V31, P4240, DOI 10.1109/TIP.2022.3181811
   Liu YX, 2023, IEEE T CIRC SYST VID, V33, P326, DOI 10.1109/TCSVT.2022.3200671
   Mekhazni Djebril, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P159, DOI 10.1007/978-3-030-58583-9_10
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Sun J, 2022, KNOWL-BASED SYST, V251, DOI 10.1016/j.knosys.2022.109162
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tao XF, 2022, IEEE T CIRC SYST VID, V32, P4404, DOI 10.1109/TCSVT.2021.3135274
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764
   Wang X., 2007, PROC IEEE 11 INT C C, P1
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu JL, 2019, IEEE I CONF COMP VIS, P8320, DOI 10.1109/ICCV.2019.00841
   Wu YM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1571, DOI 10.1145/3474085.3475296
   Wu YH, 2022, AAAI CONF ARTIF INTE, P2750
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xie JH, 2022, INT J COMPUT VISION, V130, P2994, DOI 10.1007/s11263-022-01681-x
   Xu Z, 2023, IEEE T PATTERN ANAL, V45, P11458, DOI 10.1109/TPAMI.2023.3284853
   Xuan SY, 2024, IEEE T PATTERN ANAL, V46, P1711, DOI 10.1109/TPAMI.2022.3163451
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang Shuyu, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P4492, DOI 10.1145/3581783.3611709
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8351, DOI 10.1109/ICCV48922.2021.00826
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P72, DOI 10.1007/978-3-030-58621-8_5
NR 71
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7636
EP 7648
DI 10.1109/TMM.2024.3369904
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000013
DA 2024-08-05
ER

PT J
AU Song, LY
   Chen, SY
   Meng, ZY
   Sun, MX
   Shang, XQ
AF Song, Lingyun
   Chen, Siyu
   Meng, Ziyang
   Sun, Mingxuan
   Shang, Xuequn
TI FMSA-SC: A Fine-Grained Multimodal Sentiment Analysis Dataset Based on
   Stock Comment Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Stock markets; Annotations; Task analysis; Acoustics;
   Visualization; Web sites; Multimedia databases; neural networks;
   sentiment analysis; video signal processing
ID SOCIAL MEDIA
AB Previous Sentiment Analysis (SA) studies have demonstrated that exploring sentiment cues from multiple synchronized modalities can effectively improve the SA results. Unfortunately, until now there is no publicly available dataset for multimodal SA of the stock market. Existing datasets for stock market SA only provide textual stock comments, which usually contain words with ambiguous sentiments or even sarcasm words expressing opposite sentiments of literal meaning. To address this issue, we introduce a Fine-grained Multimodal Sentiment Analysis dataset built upon 1,247 Stock Comment videos, called FMSA-SC. It provides both multimodal sentiment annotations for the videos and unimodal sentiment annotations for the textual, visual, and acoustic modalities of the videos. In addition, FMSA-SC also provides fine-grained annotations that align text at the phrase level with visual and acoustic modalities. Furthermore, we present a new fine-grained multimodal multi-task framework as the baseline for multimodal SA on the FMSA-SC.
C1 [Song, Lingyun; Chen, Siyu; Meng, Ziyang; Shang, Xuequn] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
   [Song, Lingyun; Chen, Siyu; Meng, Ziyang; Shang, Xuequn] Northwestern Polytech Univ, Key Lab Big Data Storage & Management, Minist Ind & Informat Technol, Xian 710129, Peoples R China.
   [Sun, Mingxuan] Louisiana State Univ, Sch Elect Engn & Comp Sci, Div Elect & Comp Engn, Baton Rouge, LA 70803 USA.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Louisiana State University System; Louisiana State
   University
RP Song, LY; Meng, ZY; Shang, XQ (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
EM lysong@nwpu.edu.cn; sychen@mail.nwpu.edu.cn; mzy@mail.nwpu.edu.cn;
   msun@csc.lsu.edu; shang@nwpu.edu.cn
FU National Nature Science Foundation of China
FX No Statement Available
CR Alzazah F, 2022, IEEE INT C SEMANT CO, P103, DOI 10.1109/ICSC52841.2022.00022
   Angelidis S., 2018, TACL, V6, P17, DOI 10.1162/tacl_a_00002
   Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Baker M, 2007, J ECON PERSPECT, V21, P129, DOI 10.1257/jep.21.2.129
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Chauhan DS, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4351
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   de Sousa MG, 2019, PROC INT C TOOLS ART, P1597, DOI 10.1109/ICTAI.2019.00231
   Derakhshan A, 2019, ENG APPL ARTIF INTEL, V85, P569, DOI 10.1016/j.engappai.2019.07.002
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Entezari R, 2023, Arxiv, DOI arXiv:2302.13602
   Fu ZW, 2024, FRONT COMPUT SCI-CHI, V18, DOI 10.1007/s11704-023-2444-y
   Gandhi A, 2023, INFORM FUSION, V91, P424, DOI 10.1016/j.inffus.2022.09.025
   Guo WY, 2021, IEEE T MULTIMEDIA, V23, P1785, DOI 10.1109/TMM.2020.3003648
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Jin ZG, 2020, NEURAL COMPUT APPL, V32, P9713, DOI 10.1007/s00521-019-04504-2
   KRIPPENDORFF K, 1987, QUAL QUANT, V21, P109
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Ma J, 2024, INT J COMPUT VISION, V132, P1578, DOI 10.1007/s11263-023-01954-z
   Mao HS, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P204
   Mehta P, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.476
   Meng DB, 2019, IEEE IMAGE PROC, P3866, DOI [10.1109/icip.2019.8803603, 10.1109/ICIP.2019.8803603]
   Mittal A., 2012, Standford University, CS229, V15, P2352
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI [DOI 10.1145/2070481.2070509, 10.1145/2070481.2070509]
   Paramanik Rajendra N., 2020, Procedia Computer Science, V176, P330, DOI 10.1016/j.procs.2020.08.035
   Poria S, 2018, ARXIV
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Radford A, 2021, PR MACH LEARN RES, V139
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Rosas VP, 2013, IEEE INTELL SYST, V28, P38, DOI 10.1109/MIS.2013.9
   Serrano-Guerrero J, 2015, INFORM SCIENCES, V311, P18, DOI 10.1016/j.ins.2015.03.040
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Stappen L, 2023, IEEE T AFFECT COMPUT, V14, P1334, DOI 10.1109/TAFFC.2021.3097002
   Sul HK, 2017, DECISION SCI, V48, P454, DOI 10.1111/deci.12229
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tabari N., 2018, PROC ECML PKDD WORKS, P51
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vadicamo L, 2017, IEEE INT CONF COMP V, P308, DOI 10.1109/ICCVW.2017.45
   Valle-Cruz D, 2022, COGN COMPUT, V14, P372, DOI 10.1007/s12559-021-09819-8
   Wang BQ, 2022, LECT NOTES COMPUT SC, V13141, P612, DOI 10.1007/978-3-030-98358-1_48
   Wang Chuan-Ju, 2013, P IJCNLP, P802
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P4909, DOI 10.1109/TMM.2022.3183830
   Wen MN, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-023-2689-5
   Williams J, 2018, FIRST GRAND CHALLENGE AND WORKSHOP ON HUMAN MULTIMODAL LANGUAGE (CHALLENGE-HML), P11
   Williams J, 2018, FIRST GRAND CHALLENGE AND WORKSHOP ON HUMAN MULTIMODAL LANGUAGE (CHALLENGE-HML), P64
   Wu DD, 2014, IEEE T SYST MAN CY-S, V44, P1077, DOI 10.1109/TSMC.2013.2295353
   Yadav A, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3517139
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yang Y, 2024, FRONT COMPUT SCI-CHI, V18, DOI 10.1007/s11704-023-3186-6
   Yifan Liu, 2017, Advances in Artificial Intelligence: from Theory to Practice. 30th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2017. Proceedings: LNAI 10350, P192, DOI 10.1007/978-3-319-60042-0_22
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yu WM, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3718
   Yuan ZQ, 2024, IEEE T MULTIMEDIA, V26, P529, DOI 10.1109/TMM.2023.3267882
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhang BB, 2022, INT CONF ACOUST SPEE, P6182, DOI 10.1109/ICASSP43922.2022.9746682
   Zhu T, 2023, IEEE T MULTIMEDIA, V25, P3375, DOI 10.1109/TMM.2022.3160060
   Zhu Y, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-022-1349-5
NR 63
TC 0
Z9 0
U1 13
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7294
EP 7306
DI 10.1109/TMM.2024.3363641
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000050
DA 2024-08-05
ER

PT J
AU Sun, FM
   Ren, P
   Yin, BW
   Wang, FS
   Li, HJ
AF Sun, Fuming
   Ren, Peng
   Yin, Bowen
   Wang, Fasheng
   Li, Haojie
TI CATNet: A Cascaded and Aggregated Transformer Network for RGB-D Salient
   Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Swin Transformer; salient object detection; multi-scale features;
   attention; decoder
AB Salient object detection (SOD) is an important preprocessing operation for various computer vision tasks. Most of existing RGB-D SOD models employ additive or connected strategies to directly aggregate and decode multi-scale features to predict salient maps. However, due to the large differences between the features of different scales, these aggregation strategies adopted may lead to information loss or redundancy, and few methods explicitly consider how to establish connections between features at different scales in the decoding process, which consequently deteriorates the detection performance of the models. To this end, we propose a cascaded and aggregated Transformer Network (CATNet) which consists of three key modules, i.e., attention feature enhancement module (AFEM), cross-modal fusion module (CMFM) and cascaded correction decoder (CCD). Specifically, the AFEM is designed on the basis of atrous spatial pyramid pooling to obtain multi-scale semantic information and global context information in high-level features through dilated convolution and multi-head self-attention mechanism, enhancing high-level features. The role of the CMFM is to enhance and thereafter fuse the RGB features and depth features, alleviating the problem of poor-quality depth maps. The CCD is composed of two subdecoders in a cascading fashion. It is designed to suppress noise in low-level features and mitigate the differences between features at different scales. Moreover, the CCD uses a feedback mechanism to correct and repair the output of the subdecoder by exploiting supervised features, so that the problem of information loss caused by the upsampling operation during the multi-scale features aggregation process can be mitigated. Extensive experimental results demonstrate that the proposed CATNet achieves superior performance over 14 state-of-the-art RGB-D methods on 7 challenging benchmarks.
C1 [Sun, Fuming; Ren, Peng; Yin, Bowen; Wang, Fasheng] Dalian Minzu Univ, Sch Informat & Commun Engn, Dalian 116620, Peoples R China.
   [Li, Haojie] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian 116620, Peoples R China.
C3 Dalian Minzu University; Dalian University of Technology
RP Wang, FS (corresponding author), Dalian Minzu Univ, Sch Informat & Commun Engn, Dalian 116620, Peoples R China.
EM sunfuming@dlnu.edu.cn; 724162106@qq.com; 997450910@qq.com;
   wangfasheng@dlnu.edu.cn; hjli@dlut.edu.cn
OI Ren, Peng/0009-0007-0821-1039; Sun, Fuming/0000-0003-3932-2712
FU National Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bruno A, 2020, IEEE ACCESS, V8, P121330, DOI 10.1109/ACCESS.2020.3006700
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2024, IEEE T NEUR NET LEAR, V35, P4309, DOI 10.1109/TNNLS.2022.3202241
   Chen Shaoxiang, 2020, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-58598-3_31
   Cheng MM, 2017, J COMPUT SCI TECH-CH, V32, P110, DOI 10.1007/s11390-017-1681-7
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng Y, 2014, IEEE INT CON MULTI
   Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fu KR, 2022, NEUROCOMPUTING, V512, P142, DOI 10.1016/j.neucom.2022.09.019
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kingma D. P., 2014, arXiv
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Lee M, 2022, LECT NOTES COMPUT SC, V13689, P630, DOI 10.1007/978-3-031-19818-2_36
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liang PP, 2016, IEEE SIGNAL PROC LET, V23, P949, DOI 10.1109/LSP.2016.2556706
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pang YW, 2023, IEEE T IMAGE PROCESS, V32, P892, DOI 10.1109/TIP.2023.3234702
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang WG, 2018, IEEE T CIRC SYST VID, V28, P1727, DOI 10.1109/TCSVT.2017.2701279
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XX, 2023, Arxiv, DOI arXiv:2112.01177
   Wang Zhuo, 2022, 2022 3rd International Conference on Pattern Recognition and Machine Learning (PRML), P38, DOI 10.1109/PRML56267.2022.9882262
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wu JY, 2022, EXPERT SYST APPL, V195, DOI 10.1016/j.eswa.2022.116614
   Wu YH, 2023, IEEE T PATTERN ANAL, V45, P12760, DOI 10.1109/TPAMI.2022.3202765
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P5346, DOI 10.1109/TCSVT.2022.3144852
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang M, 2023, IEEE T MULTIMEDIA, V25, P5142, DOI 10.1109/TMM.2022.3187856
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang W., 2021, P IEEE INT C MULT EX, P1
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zhu JY, 2015, IEEE T PATTERN ANAL, V37, P862, DOI 10.1109/TPAMI.2014.2353617
NR 63
TC 14
Z9 14
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2249
EP 2262
DI 10.1109/TMM.2023.3294003
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100022
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Wang, M
   Chen, XY
   Yang, X
   Peng, S
   Zhao, Y
   Xu, MW
   Xu, CQ
AF Wang, Mu
   Chen, Xingyan
   Yang, Xu
   Peng, Shuai
   Zhao, Yu
   Xu, Mingwei
   Xu, Changqiao
TI CoLive: Edge-Assisted Clustered Learning Framework for Viewport
   Prediction in 360<SUP>°</SUP> Live Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Predictive models; Behavioral sciences; Heuristic
   algorithms; Bandwidth; Servers; Computational modeling; FoV prediction;
   clustered collaborative learning; 360 (degrees) live video streaming
AB The exceptionally high bandwidth requirement for delivering high-quality live 360(degrees) video poses a significant challenge to current network capacity. Mitigating such bandwidth starvation necessitates accurate field-of-view (FoV) prediction to focus limited resources on the viewer's area of interest. However, FoV prediction for live 360(degrees) streaming can be complex due to the time-sensitive nature of live content and the limited knowledge available for model training. Our paper introduces a novel framework, CoLive, for predicting the FoV in 360(degrees) live streaming. CoLive accelerates FoV prediction by offloading model training from viewers to the edge and migrating saliency feature detection to the server side. Observations on user clustering of viewing behaviors further motivate us to propose a novel dynamic clustered learning algorithm. The algorithm dynamically groups users according to their model update gradients and enables them to train a shared model that better suits their viewing preferences. We conduct extensive experiments on the public 360(degrees) video datasets and demonstrate that CoLive outperforms state-of-the-art solutions in terms of prediction performance and bandwidth savings.
C1 [Wang, Mu; Peng, Shuai; Xu, Changqiao] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Chen, Xingyan; Yang, Xu; Zhao, Yu] Southwestern Univ Finance & Econ, Inst Digital Econ & Interdisciplinary Sci Innovat, Financial Intelligence & Financial Engn Key Lab Si, Chengdu 611130, Peoples R China.
   [Xu, Mingwei] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Xu, Mingwei] Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Southwestern
   University of Finance & Economics - China; Tsinghua University; Tsinghua
   University
RP Chen, XY (corresponding author), Southwestern Univ Finance & Econ, Inst Digital Econ & Interdisciplinary Sci Innovat, Financial Intelligence & Financial Engn Key Lab Si, Chengdu 611130, Peoples R China.
EM muwang@tsinghua.edu.cn; xychen@swufe.edu.cn; xy@swufe.edu.cn;
   pengshuai@bupt.edu.cn; zhaoyu@swufe.edu.cn; xumw@tsinghua.edu.cn;
   cqxu@bupt.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Nguyen A, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P279, DOI 10.1145/3304109.3325820
   Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chen XY, 2023, IEEE T MULTIMEDIA, V25, P8471, DOI 10.1109/TMM.2023.3237325
   Cisco, 2018, White Paper
   Eltobgy O, 2020, IEEE T MULTIMEDIA, V22, P3139, DOI 10.1109/TMM.2020.2973855
   Feng XL, 2021, IEEE T VIS COMPUT GR, V27, P2736, DOI 10.1109/TVCG.2021.3067686
   Feng XL, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P800, DOI [10.1109/VR46266.2020.1584727730619, 10.1109/VR46266.2020.00005]
   He G, 2018, IEEE COMMUN LETT, V22, P25, DOI 10.1109/LCOMM.2017.2764021
   Hou XS, 2021, IEEE T MULTIMEDIA, V23, P716, DOI 10.1109/TMM.2020.2987693
   HUAWEI, 2019, Cloud AR/VR whitepaper
   Kingma D. P., 2015, ICLR, P1
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Maniotis P, 2021, IEEE T CIRC SYST VID, V31, P4938, DOI 10.1109/TCSVT.2021.3055985
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Shietal X., 2015, Convolutional LSTM Network:A Machine Learning Approach for Precipitation Nowcasting
   Sun LY, 2023, IEEE T MULTIMEDIA, V25, P2636, DOI 10.1109/TMM.2022.3149642
   Sun LY, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P26, DOI 10.1145/3339825.3391856
   Wang M., 2022, P IEEE INT C MULT EX, P1
   Wu CL, 2020, AAAI CONF ARTIF INTE, V34, P14003
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yaqoob A, 2020, IEEE COMMUN SURV TUT, V22, P2801, DOI 10.1109/COMST.2020.3006999
   Zhong LJ, 2023, IEEE T MOBILE COMPUT, V22, P4405, DOI 10.1109/TMC.2022.3162147
NR 27
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5078
EP 5091
DI 10.1109/TMM.2023.3330112
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600013
DA 2024-08-05
ER

PT J
AU Wang, WQ
   Luo, YW
   Chen, ZQ
   Jiang, T
   Yang, Y
   Xiao, J
AF Wang, Wenqing
   Luo, Yawei
   Chen, Zhiqing
   Jiang, Tao
   Yang, Yi
   Xiao, Jun
TI Taking a Closer Look At Visual Relation: Unbiased Video Scene Graph
   Generation With Decoupled Label Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoupled label learning (DLL); long-tail problem; video scene graph
   generation (VidSGG)
ID NETWORK; IMAGE
AB Current video-based scene graph generation (VidSGG) methods have been found to perform poorly in predicting predicates that are less represented due to the inherently biased distribution of the training data. In this paper, we take a closer look at the inherent characteristics of predicates and identify that most visual relations (e.g. sit_above) involve both actional pattern (sit) and spatial pattern (above), while the distribution bias is much less severe at the pattern level. Based on this insight, we propose a decoupled label learning (DLL) paradigm to address the intractable visual relation prediction from the pattern-level perspective. Specifically, DLL decouples the predicate labels and adopts separate classifiers to learn actional and spatial patterns respectively. The patterns are then combined and mapped back to the predicate. Moreover, we propose a knowledge-level label decoupling method to transfer non-target knowledge from head predicates to tail predicates within the same pattern to calibrate the distribution of tail classes. We validate the effectiveness of DLL on the commonly used VidSGG benchmark, i.e. VidVRD. Extensive experiments demonstrate that the DLL offers a remarkably simple but highly effective solution to the long-tailed problem, achieving the state-of-the-art VidSGG performance.
C1 [Wang, Wenqing; Luo, Yawei; Chen, Zhiqing; Jiang, Tao] Zhejiang Univ, Software Coll, Hangzhou 310058, Peoples R China.
   [Yang, Yi; Xiao, Jun] Zhejiang Univ, Sch Comp Sci, Hangzhou 318000, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Luo, YW (corresponding author), Zhejiang Univ, Software Coll, Hangzhou 310058, Peoples R China.
EM wenqing07@zju.edu.cn; yaweiluo329@gmail.com; zqc@zju.edu.cn;
   zjujiangtao@zju.edu.cn; yangyics@zju.edu.cn; junx@cs.zju.edu.cn
OI Luo, Yawei/0000-0002-7037-1806
FU National Natural Science Foundation of China
FX No Statement Available
CR Aafaq N, 2023, IEEE T MULTIMEDIA, V25, P2309, DOI 10.1109/TMM.2022.3146005
   Cao QW, 2021, NEUROCOMPUTING, V432, P91, DOI 10.1016/j.neucom.2020.12.029
   Chen S., 2022, PROC INT CONF SOFT
   Chen S., 2021, PROC IEEECVF INT C, P13485
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Chenchen Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10837, DOI 10.1109/CVPR42600.2020.01085
   Chiou MJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1581, DOI 10.1145/3474085.3475297
   Cong YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16352, DOI 10.1109/ICCV48922.2021.01606
   Desjardins G., 2012, Disentangling factors of variation via generative entangling
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Esmaeili B., 2019, 22 INT C ART INT STA, P2525, DOI DOI 10.48550/ARXIV
   Feng SY, 2023, IEEE WINT CONF APPL, P5119, DOI 10.1109/WACV56688.2023.00510
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao K., 2022, PROC 11 INT C LEAR
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Guo ZC, 2023, IEEE T MULTIMEDIA, V25, P38, DOI 10.1109/TMM.2021.3120544
   Han W., 2016, SEQ-NMS for video object detection
   Han XJ, 2023, IEEE T MULTIMEDIA, V25, P5319, DOI 10.1109/TMM.2022.3190135
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Jiang JJ, 2023, IEEE T MULTIMEDIA, V25, P5002, DOI 10.1109/TMM.2022.3185900
   Jingwei Ji, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10233, DOI 10.1109/CVPR42600.2020.01025
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li L., 2023, IEEE Trans. Circuits Syst. Video Technol.
   Li RJ, 2021, PROC CVPR IEEE, P11104, DOI 10.1109/CVPR46437.2021.01096
   Li YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4091, DOI 10.1145/3474085.3475540
   Liao Y, 2020, PROC CVPR IEEE, P479, DOI 10.1109/CVPR42600.2020.00056
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin X, 2020, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR42600.2020.00380
   Liu HY, 2021, PROC CVPR IEEE, P11541, DOI 10.1109/CVPR46437.2021.01138
   Misra I, 2016, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2016.320
   Nemeth J, 2020, AAAI CONF ARTIF INTE, V34, P10243
   Newell A., 2017, PROC NEURAL INF PRO, P2168
   Peng X, 2019, PMLR, P5102
   Qian XF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P84, DOI 10.1145/3343031.3351058
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Shang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3654, DOI 10.1145/3474085.3475263
   Shang XD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1300, DOI 10.1145/3123266.3123380
   Song PP, 2023, IEEE T MULTIMEDIA, V25, P1858, DOI 10.1109/TMM.2022.3183402
   Su ZX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3127, DOI 10.1145/3394171.3413764
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Teng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13668, DOI 10.1109/ICCV48922.2021.01343
   Tsai YHH, 2019, PROC CVPR IEEE, P10416, DOI 10.1109/CVPR.2019.01067
   Wang GM, 2022, IEEE T MULTIMEDIA, V24, P1221, DOI 10.1109/TMM.2022.3142420
   Wang JY, 2021, IEEE T MULTIMEDIA, V24, P3369, DOI 10.1109/TMM.2021.3097171
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P2661, DOI 10.1109/TMM.2022.3149716
   Wang W, 2021, IEEE T MULTIMEDIA, V23, P2386, DOI 10.1109/TMM.2020.3011288
   Woo Sangmin, 2021, WHAT LOOK TEMPORAL S
   Xiang S., Disentangling style and content in anime illustrations.
   Xu L, 2022, LECT NOTES COMPUT SC, V13687, P374, DOI 10.1007/978-3-031-19812-0_22
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang S., 2022, Speech representation disentanglement with adversarial mutual information learning for one-shot voice conversion
   Yu Jing, 2020, COGTREE COGNITION TR
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
NR 59
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5718
EP 5728
DI 10.1109/TMM.2023.3338078
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100012
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yang, JC
   Ma, SK
   Zhang, Z
   Li, Y
   Xiao, S
   Wen, JB
   Lu, W
   Gao, XB
AF Yang, Jiachen
   Ma, Shukun
   Zhang, Zhuo
   Li, Yang
   Xiao, Shuai
   Wen, Jiabao
   Lu, Wen
   Gao, Xinbo
TI Say No to Redundant Information: Unsupervised Redundant Feature
   Elimination for Active Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Data models; Labeling; Costs; Training; Redundancy;
   Computational modeling; Active learning; data issues; deep learning;
   information redundancy; label noise
AB The usual active learning is to sample unlabeled set by designing efficient sample information evaluation algorithms. However, information redundancy between candidate sets is often overlooked. This can cause similar data to be labeled repeatedly, producing ineffective gains for the model. In this paper, we proposed an Unsupervised Redundant Feature Elimination Active Learning module (URFEAL), which utilizes the information feature coincidence of the unlabeled set to eliminate information redundant data, thus guaranteeing the validity of each candidate data. URFEAL consists of feature clusterer and eliminator. The feature clusterer computes class boundaries based on feature densities to discretize each class of the candidate set, and the eliminator judges data similarity by overlapping degree to eliminate redundant data features. Furthermore, we propose an anti-noise sampling strategy Outlier Feature Elimination (OFE) in URFEAL to filter mislabeled sets for relabeling in the data sampling stage. We extensively evaluate our method by image classification and perform experimental validation on CIFAR-10, CIFAR-100 and CALTECH-101. The experimental results show that the improvements we make are especially significant for most existing active learning algorithms in the low data stage, which demonstrates the effectiveness and generality of URFEAL.
C1 [Yang, Jiachen; Ma, Shukun; Zhang, Zhuo; Li, Yang; Xiao, Shuai; Wen, Jiabao] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Lu, Wen] Xidian Univ, Sch Elect Engn, Xian 710126, Peoples R China.
   [Gao, Xinbo] Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710126, Peoples R China.
C3 Tianjin University; Xidian University; Xidian University
RP Zhang, Z (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM yangjiachen@tju.edu.cn; mashukun@tju.edu.cn; z_zhuo@tju.edu.cn;
   liyang328@shzu.edu.cn; xs611@tju.edu.cn; wen_jiabao@tju.edu.cn;
   luwen.xidian@gmail.com; xbgao@mail.xidian.edu.cn
OI Wen, Jiabao/0000-0003-2303-9613; Xiao, Shuai/0000-0003-4058-8120; zhang,
   zhuo/0000-0002-3946-0720; Yang, Jiachen/0000-0003-2558-552X
FU National Natural Science Foundation of China
FX No Statement Available
CR Agarwal Sharat, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P137, DOI 10.1007/978-3-030-58517-4_9
   Aghdam HH, 2019, IEEE I CONF COMP VIS, P3671, DOI 10.1109/ICCV.2019.00377
   Ahmad M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11091136
   Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1007/BF00116828
   Beichen Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8753, DOI 10.1109/CVPR42600.2020.00878
   Beluch WH, 2018, PROC CVPR IEEE, P9368, DOI 10.1109/CVPR.2018.00976
   Bilgic Mustafa, 2009, P NIPS WORKSH AN NET
   Chang DL, 2021, PROC CVPR IEEE, P11471, DOI 10.1109/CVPR46437.2021.01131
   Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295
   Collins B, 2008, LECT NOTES COMPUT SC, V5302, P86, DOI 10.1007/978-3-540-88682-2_8
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ducoffe M, 2018, Arxiv, DOI arXiv:1802.09841
   Elhamifar E, 2013, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2013.33
   Fails J. A., 2003, IUI 03. 2003 International Conference on Intelligent User Interfaces, P39, DOI 10.1145/604045.604056
   Gal Y, 2017, PR MACH LEARN RES, V70
   Gal Y, 2016, PR MACH LEARN RES, V48
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hasan M, 2015, IEEE I CONF COMP VIS, P4543, DOI 10.1109/ICCV.2015.516
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho David Joon, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P540, DOI 10.1007/978-3-030-59722-1_52
   Joshi AJ, 2009, PROC CVPR IEEE, P2364
   Joulin A, 2016, LECT NOTES COMPUT SC, V9911, P67, DOI 10.1007/978-3-319-46478-7_5
   Kim DJ, 2021, Arxiv, DOI arXiv:2110.10906
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo WC, 2018, LECT NOTES COMPUT SC, V11072, P715, DOI 10.1007/978-3-030-00931-1_82
   Lehmann R, 2013, J SURV ENG, V139, P157, DOI 10.1061/(ASCE)SU.1943-5428.0000112
   Lewis D. D., 1995, SIGIR Forum, V29, P13, DOI 10.1145/219587.219592
   Lewis D.D., 1994, MACHINE LEARNING P 1, P148, DOI DOI 10.1016/B978-1-55860-335-6.50026-X
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li L, 2012, IEEE T MULTIMEDIA, V14, P1401, DOI 10.1109/TMM.2012.2194993
   Li X, 2014, LECT NOTES COMPUT SC, V8695, P234, DOI 10.1007/978-3-319-10584-0_16
   Lin YC, 2017, IEEE J-STSP, V11, P89, DOI 10.1109/JSTSP.2016.2632422
   Lin Yang, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10435, P399, DOI 10.1007/978-3-319-66179-7_46
   Liu C, 2021, PROC CVPR IEEE, P6807, DOI 10.1109/CVPR46437.2021.00674
   Luo Wenjie, 2013, ADV NEURAL INFORM PR, P728
   Mac Aodha O, 2014, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2014.79
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Mahapatra D, 2018, LECT NOTES COMPUT SC, V11071, P580, DOI 10.1007/978-3-030-00934-2_65
   Mayer C, 2020, IEEE WINT CONF APPL, P3060, DOI [10.1109/wacv45572.2020.9093556, 10.1109/WACV45572.2020.9093556]
   Mirza M., 2014, ARXIV
   Nguyen H. T., 2004, P 21 INT C MACH LEAR
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Parvaneh A, 2022, PROC CVPR IEEE, P12227, DOI 10.1109/CVPR52688.2022.01192
   Roth D, 2006, LECT NOTES COMPUT SC, V4212, P413
   Roy N., 2001, ICML, P441
   Sener O., 2018, INT C LEARNING REPRE, P1
   Settles B., 2008, P 2008 C EMP METH NA, P1070
   Shin Inkyu, 2021, P IEEECVF INT C COMP, P8588
   Sinha S, 2019, IEEE I CONF COMP VIS, P5971, DOI 10.1109/ICCV.2019.00607
   Snell J, 2017, ADV NEUR IN, V30
   Sohn K, 2015, ADV NEUR IN, V28
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243
   Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9
   Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879
   Wang KZ, 2018, PROC CVPR IEEE, P1605, DOI 10.1109/CVPR.2018.00173
   Xie JY, 2022, IEEE T PATTERN ANAL, V44, P4605, DOI 10.1109/TPAMI.2021.3083089
   Yang JC, 2023, NEUROCOMPUTING, V530, P104, DOI 10.1016/j.neucom.2023.01.067
   Yang JC, 2019, IEEE T MULTIMEDIA, V21, P1750, DOI 10.1109/TMM.2018.2889562
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Yoo D, 2019, PROC CVPR IEEE, P93, DOI 10.1109/CVPR.2019.00018
   Zhao Y, 2023, SIGNAL PROCESS, V203, DOI 10.1016/j.sigpro.2022.108782
   Zhu JJ, 2017, Arxiv, DOI arXiv:1702.07956
NR 66
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7721
EP 7733
DI 10.1109/TMM.2024.3371192
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RE1G6
UT WOS:001225895800004
DA 2024-08-05
ER

PT J
AU Yue, GH
   Wu, H
   Jiang, QP
   Zhou, TW
   Yan, WQ
   Wang, TF
AF Yue, Guanghui
   Wu, Honglv
   Jiang, Qiuping
   Zhou, Tianwei
   Yan, Weiqing
   Wang, Tianfu
TI Perceptual Quality Assessment of Retouched Face Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Perceptual quality; facial retouching; no-reference; image quality
   assessment; multi-task learning
ID DISTORTED IMAGES; BODY-IMAGE; MEDIA; CLASSIFICATION; STATISTICS; IMPACT
AB Nowadays, it is a common practice to retouch face images before sharing them on websites, social media, and even identification cards. In response, increased criticisms have appeared about taking photo retouching to an extreme. This naturally leads to the necessity of designing perceptual quality assessment methods that can measure how much a retouched face image has strayed from reality. However, such an issue has seldom been considered. In this paper, we conduct both subjective and objective studies to advance this field. Firstly, we construct a benchmark database (termed SZU-RFD) via subjective experiments. SZU-RFD consists of 200 high-quality images with Asian faces and 1,600 retouched images generated by three popular photo-editing tools under different settings. Secondly, considering that retouching usually distorts the image texture, we propose a novel no-reference (NR) quality assessment method, named TANet, for retouched face images by taking the textural artifact into account. Specifically, a texture enhancement module is embedded into the shallow layer to help the network focus on textural information, and a multi-task learning strategy is applied to improve the performance of the main task with the assistance of an auxiliary task, i.e., texture recognition. Extensive experiments on the constructed SZU-RFD show that our proposed TANet correlates well with subjective perceptual judgments and is superior to 19 mainstream NR image quality assessment methods in evaluating retouched face images.
C1 [Yue, Guanghui; Wu, Honglv; Zhou, Tianwei] Shenzhen Univ Med Sch, Sch Biomed Engn, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasound, Shenzhen, Peoples R China.
   [Yue, Guanghui; Wu, Honglv; Zhou, Tianwei] Shenzhen Univ, Marshall Lab Biomed Engn, Shenzhen 518060, Peoples R China.
   [Jiang, Qiuping] Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Zhou, Tianwei] Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China.
   [Yan, Weiqing] Yantai Univ, Sch Comp & Control Engn, Yantai 261400, Peoples R China.
C3 Shenzhen University; Shenzhen University; Ningbo University; Shenzhen
   University; Yantai University
RP Zhou, TW (corresponding author), Shenzhen Univ Med Sch, Sch Biomed Engn, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasound, Shenzhen, Peoples R China.
EM yueguanghui@szu.edu.cn; kailyn_wu@126.com; jiangqiuping@nbu.edu.cn;
   tianwei@szu.edu.cn; wqyan@tju.edu.cn; tfwang@szu.edu.cn
OI Qiuping, Jiang/0000-0002-6025-9343
FU National Natural Science Foundation of China
FX No Statement Available
CR Agliata D, 2004, J SOC CLIN PSYCHOL, V23, P7, DOI 10.1521/jscp.23.1.7.26988
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Bharati A, 2016, IEEE T INF FOREN SEC, V11, P1903, DOI 10.1109/TIFS.2016.2561898
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Cao YQ, 2023, IEEE T IMAGE PROCESS, V32, P1882, DOI 10.1109/TIP.2023.3251695
   Chen CJ, 2013, INT CONF BIOMETR
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Derenne JL, 2006, ACAD PSYCHIATR, V30, P257, DOI 10.1176/appi.ap.30.3.257
   Dittmar H, 2009, J SOC CLIN PSYCHOL, V28, P1, DOI 10.1521/jscp.2009.28.1.1
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Gao X. Min, 2023, IEEE Trans. Circuits Syst. Video Technol., early access, DOI [10.1109/TCSVT.2023.3295375.[76]W., DOI 10.1109/TCSVT.2023.3295375.[76]W]
   Gao YX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P997, DOI 10.1145/3503161.3547872
   Gao YX, 2023, IEEE T CIRC SYST VID, V33, P2656, DOI 10.1109/TCSVT.2022.3229839
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Gu K, 2020, IEEE T BROADCAST, V66, P127, DOI 10.1109/TBC.2019.2906768
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hou JW, 2020, IEEE IMAGE PROC, P3463, DOI 10.1109/ICIP40778.2020.9191241
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang YP, 2023, IEEE T MULTIMEDIA, V25, P7672, DOI 10.1109/TMM.2022.3225728
   Jain R., 2018, IEEE 9 INT C BIOMETR, P1
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Kee E, 2011, P NATL ACAD SCI USA, V108, P19907, DOI 10.1073/pnas.1110747108
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kose Neslihan, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163104
   Li F, 2021, IEEE T CIRC SYST VID, V31, P4798, DOI 10.1109/TCSVT.2021.3055197
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Luo Y, 2024, IEEE T CIRC SYST VID, V34, P85, DOI 10.1109/TCSVT.2023.3284856
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Nafchi HZ, 2018, IEEE T BROADCAST, V64, P518, DOI 10.1109/TBC.2018.2818402
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan Zhaoqing, 2023, IEEE Transactions on Artificial Intelligence, P148, DOI 10.1109/TAI.2022.3146804
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Rathgeb C, 2020, IEEE ACCESS, V8, P106373, DOI 10.1109/ACCESS.2020.3000254
   RIR, 2002, Int. Telecommun. Union, V4
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Sharma G., 2023, Expert Syst. Appl., V211
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Sun W, 2023, IEEE J-STSP, V17, P1178, DOI 10.1109/JSTSP.2023.3270621
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Tang W, 2022, IEEE T IMAGE PROCESS, V31, P5134, DOI 10.1109/TIP.2022.3193288
   Video Quality Experts Group, 2000, VQEG M OTT CAN MARCH
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xiongkuo Min, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457921
   Xu K, 2022, IEEE T CIRC SYST VID, V32, P4983, DOI 10.1109/TCSVT.2022.3141578
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   Yue GH, 2023, IEEE T MULTIMEDIA, V25, P6499, DOI 10.1109/TMM.2022.3209889
   Yue GH, 2019, IEEE T IND ELECTRON, V66, P3784, DOI 10.1109/TIE.2018.2851984
   Yue GH, 2017, J VIS COMMUN IMAGE R, V49, P382, DOI 10.1016/j.jvcir.2017.09.011
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang CF, 2022, IEEE T CIRC SYST VID, V32, P5011, DOI 10.1109/TCSVT.2022.3143321
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang Y, 2018, NATL SCI REV, V5, P30, DOI 10.1093/nsr/nwx105
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhou Y, 2023, IEEE T MULTIMEDIA, V25, P4177, DOI 10.1109/TMM.2022.3171684
   Zhu H., 2020, P IEEECVF C COMPUTER, P14143
NR 79
TC 3
Z9 3
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5741
EP 5752
DI 10.1109/TMM.2023.3338412
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100009
DA 2024-08-05
ER

PT J
AU Zhang, Z
   St-Hilaire, M
   Wei, X
   Dong, HW
   Saddik, AE
AF Zhang, Zhe
   St-Hilaire, Marc
   Wei, Xin
   Dong, Haiwei
   Saddik, Abdulmotaleb El
TI How to Cache Important Contents for Multi-Modal Service in Dynamic
   Networks: A DRL-Based Caching Scheme
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-modal contents; edge caching; deep reinforcement learning; dynamic
   network environments
ID POPULARITY
AB With the continuous evolution of networking technologies, multi-modal services that involve video, audio, and haptic contents are expected to become the dominant multimedia service in the near future. Edge caching is a key technology that can significantly reduce network load and content transmission latency, which is critical for the delivery of multi-modal contents. However, existing caching approaches only rely on a limited number of factors, e.g., popularity, to evaluate their importance for caching, which is inefficient for caching multi-modal contents, especially in dynamic network environments. To overcome this issue, we propose a content importance-based caching scheme which consists of a content importance evaluation model and a caching model. By leveraging dueling double deep Q networks (D3QN) model, the content importance evaluation model can adaptively evaluate contents' importance in dynamic networks. Based on the evaluated contents' importance, the caching model can easily cache and evict proper contents to improve caching efficiency. The simulation results show that the proposed content importance-based caching scheme outperforms existing caching schemes in terms of caching hit ratio (at least 15% higher), reduced network load (up to 22% reduction), average number of hops (up to 27% lower), and unsatisfied requests ratio (more than 47% reduction).
C1 [Zhang, Zhe; Wei, Xin] Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210003, Peoples R China.
   [St-Hilaire, Marc] Carleton Univ, Dept Syst & Comp Engn, Sch Informat Technol, Ottawa, ON K1S 5B6, Canada.
   [Dong, Haiwei] Ottawa Res Ctr, Huawei Technol Canada, Ottawa, ON K2K 3J1, Canada.
   [Saddik, Abdulmotaleb El] Univ Ottawa, Multimedia Commun Res Lab, Ottawa, ON K1N 6N5, Canada.
   [Saddik, Abdulmotaleb El] Mohamed Bin Zayed Univ Artificial Intelligence, Comp Vis Dept, Abu Dhabi, U Arab Emirates.
C3 Nanjing University of Posts & Telecommunications; Carleton University;
   Huawei Technologies; University of Ottawa; Mohamed Bin Zayed University
   of Artificial Intelligence
RP Wei, X (corresponding author), Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210003, Peoples R China.
EM zhezhang@njupt.edu.cn; marc_st_hilaire@carleton.ca; xwei@njupt.edu.cn;
   haiwei.dong@ieee.org; elsaddik@uottawa.ca
RI Zhang, Zhe/HMD-5984-2023; Dong, Haiwei/I-1273-2014; /D-4159-2009
OI Zhang, Zhe/0000-0002-6791-9009; Dong, Haiwei/0000-0003-1437-7805;
   /0000-0002-7690-8547
FU Natural Science Foundation of Jiangsu Province
FX No Statement Available
CR Bharath BN, 2018, IEEE T COMMUN, V66, P3837, DOI 10.1109/TCOMM.2018.2835479
   Coutinho RWL, 2022, IEEE COMMUN MAG, V60, P60, DOI 10.1109/MCOM.001.21261
   Derakhshan F., 2022, PROC IEEE INT C COMM, P1
   Fang T, 2021, IEEE INTERNET THINGS, V8, P14418, DOI 10.1109/JIOT.2021.3068601
   Fettweis GP, 2014, IEEE VEH TECHNOL MAG, V9, P64, DOI 10.1109/MVT.2013.2295069
   Fu YC, 2023, IEEE INTERNET THINGS, V10, P3587, DOI 10.1109/JIOT.2022.3222521
   Gao Y, 2021, IEEE NETWORK, V35, P236, DOI 10.1109/MNET.011.2000474
   Huang CK, 2019, PROCEEDINGS OF THE 2019 ACM MOBIHOCWORKSHOP ON PERVASIVE SYSTEMS IN THE IOT ERA (PERSIST-IOT '19), P49, DOI 10.1145/3331052.3332478
   Jiang F, 2019, IEEE ACCESS, V7, P97505, DOI 10.1109/ACCESS.2019.2927836
   Kumar S, 2020, COMPUT NETW, V179, DOI 10.1016/j.comnet.2020.107434
   Lample G, 2017, AAAI CONF ARTIF INTE, P2140
   Lekharu A, 2022, IEEE T NETW SERV MAN, V19, P1413, DOI 10.1109/TNSM.2021.3136439
   Ming ZX, 2014, 2014 23RD INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATION AND NETWORKS (ICCCN)
   Pfender J, 2019, PROCEEDINGS OF THE 2019 CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ICN '19), P100, DOI 10.1145/3357150.3357405
   Qiao GH, 2020, IEEE INTERNET THINGS, V7, P247, DOI 10.1109/JIOT.2019.2945640
   Tang FX, 2023, IEEE WIREL COMMUN, V30, P72, DOI 10.1109/MWC.019.2100721
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Vinyals O, 2019, NATURE, V575, P350, DOI 10.1038/s41586-019-1724-z
   Wang CW, 2023, FUTURE GENER COMP SY, V140, P225, DOI 10.1016/j.future.2022.10.036
   Wang YT, 2019, IEEE INT C COMMUNICA
   Wei X, 2022, IEEE T CIRC SYST VID, V32, P3991, DOI 10.1109/TCSVT.2021.3105130
   Wei X, 2021, IEEE WIREL COMMUN, V28, P182, DOI 10.1109/MWC.001.2000448
   Wu D, 2018, IEEE WIREL COMMUN, V25, P43, DOI 10.1109/MWC.2018.1700325
   Xiao H, 2022, IEEE J SEL AREA COMM, V40, P1615, DOI 10.1109/JSAC.2022.3145813
   Xu JW, 2019, IEEE T GREEN COMMUN, V3, P483, DOI 10.1109/TGCN.2019.2905225
   Yao L, 2022, IEEE T INTELL TRANSP, V23, P20230, DOI 10.1109/TITS.2022.3171071
   Zhang GQ, 2013, COMPUT NETW, V57, P3128, DOI 10.1016/j.comnet.2013.07.007
   Zhang M, 2021, IEEE INT CONF COMM, DOI 10.1109/ICCWorkshops50388.2021.9473609
   Zhang XZ, 2022, IEEE T PARALL DISTR, V33, P2597, DOI 10.1109/TPDS.2022.3147240
   Zhang XW, 2023, IEEE NETWORK, V37, P34, DOI 10.1109/MNET.107.2100461
   Zhang Z, 2023, IEEE INTERNET THINGS, V10, P1787, DOI 10.1109/JIOT.2022.3209256
   Zhang Z, 2020, IEEE T VEH TECHNOL, V69, P7955, DOI 10.1109/TVT.2020.2994181
   Zhang Z, 2017, P INT COMP SOFTW APP, P523, DOI 10.1109/COMPSAC.2017.203
   Zhao H, 2021, IEEE T CIRC SYST VID, V31, P1234, DOI 10.1109/TCSVT.2020.2991408
   Zhong C, 2020, IEEE T COGN COMMUN, V6, P48, DOI 10.1109/TCCN.2020.2968326
   Zhou H, 2021, IEEE J SEL AREA COMM, V39, P2445, DOI 10.1109/JSAC.2021.3087232
   Zhou L, 2021, IEEE J SEL AREA COMM, V39, P426, DOI 10.1109/JSAC.2020.3021543
   Zhou L, 2020, IEEE WIREL COMMUN, V27, P112, DOI 10.1109/MWC.001.1900201
   Zhu J., 2022, IEEE Trans. Cogn. Commun. Netw., V9, P345
   Zong TY, 2023, IEEE ACM T NETWORK, V31, P208, DOI 10.1109/TNET.2022.3193680
NR 40
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7372
EP 7385
DI 10.1109/TMM.2024.3366399
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000046
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhao, YC
   Chen, S
   Liu, SG
   Hu, ZH
   Xia, JW
AF Zhao, Yaochi
   Chen, Sen
   Liu, Shiguang
   Hu, Zhuhua
   Xia, Jingwen
TI Hierarchical Equalization Loss for Long-Tailed Instance Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Instance segmentation; Object detection; long-tailed distribution;
   imbalanced learning; deep learning
ID OBJECT DETECTION
AB In long-tailed image instance segmentation, the existing methods deal with imbalance problem from a single perspective, which results in the limitation of performance. Considering that imbalances exist not only between positive and negative classes, but also between foreground and background subclasses, as well as between hard and easy examples, we argue that the losses of samples should be hierarchically equalized at multi-levels (HEL). We first propose a focus based hierarchical-equalization loss (FHEL), which employs a class gradient ratio based reweighting mechanism to achieve the balance between classes, and uses a subclass-balance term and a sample-balance term to separately deal with the inter-subclass and inter-sample imbalances. FHEL can improve the performance of long-tailed instance segmentation in an end-to-end manner, avoiding the overfitting risk and manual hard division in the traditional methods. On the basis of FHEL, we further explore the relationship between inter-subclass imbalance and inter-sample imbalance, and propose a constrained-focus based hierarchical-equalization loss (CFHEL) that copes with the imbalances at multi-levels simultaneously with fewer hyperparameters. We conduct extensive experiments on LVIS v1.0 and COCO-LT datasets with different benchmarks. Both FHEL and CFHEL are superior to the existing methods. On LVIS v1.0, with ResNet50Mask R-CNN, ResNet101Mask R-CNN, ResNeXt101Mask R-CNN and ResNet101 CascadeMask R-CNN, CFHEL outperforms its baselines respectively with 19.8 % , 18.5 % , 21.6 % and 21.2 % AP r gains, and with 6.7 % , 6.6 % , 7.6 % and 6.5 % AP gains, achieving the new state-of-the-arts. On COCO-LT, our CFHEL outperforms the baseline with 13.2 % APr gains and 3.3 % AP gains, also achieving the new best performances.
C1 [Zhao, Yaochi; Liu, Shiguang] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300050, Peoples R China.
   [Zhao, Yaochi; Chen, Sen] Hainan Univ, Sch Cyberspace Secur, Haikou 570228, Peoples R China.
   [Hu, Zhuhua] Hainan Univ, Sch Informat & Commun Engn, Haikou 570228, Peoples R China.
C3 Tianjin University; Hainan University; Hainan University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300050, Peoples R China.
EM zhyc@hainanu.edu.cn; 1848339732@qq.com; lsg@tju.edu.cn;
   eagler_hu@hainanu.edu.cn; xalva575719@163.com
OI Zhao, Yaochi/0000-0003-0306-3114
FU National Natural Science Foundation of China
FX No Statement Available
CR Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3397, DOI 10.1109/ICCV48922.2021.00340
   Gao JX, 2023, IEEE T MULTIMEDIA, V25, P4764, DOI 10.1109/TMM.2022.3181789
   Guo YD, 2019, IEEE T MULTIMEDIA, V21, P2903, DOI 10.1109/TMM.2019.2912703
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YY, 2022, PROC CVPR IEEE, P6990, DOI 10.1109/CVPR52688.2022.00687
   Hsieh TI, 2021, AAAI CONF ARTIF INTE, V35, P1549
   Ji Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1349, DOI 10.1145/3343031.3351064
   Jiale Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P1, DOI 10.1007/978-3-030-58568-6_1
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168
   Li B, 2022, PROC CVPR IEEE, P6980, DOI 10.1109/CVPR52688.2022.00686
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li J, 2022, PROC CVPR IEEE, P6939, DOI 10.1109/CVPR52688.2022.00682
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JL, 2020, PROC CVPR IEEE, P2967, DOI 10.1109/CVPR42600.2020.00304
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P1, DOI DOI 10.48550/ARXIV.1310.4546
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Ren S., 2016, Faster r-cnn: Towards real-time object detection with region proposal networks
   Tan JR, 2021, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR46437.2021.00173
   Tang K., 2020, P NIPS, P1513
   Tao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P728, DOI 10.1007/978-3-030-58568-6_43
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957
   Wang T, 2021, PROC CVPR IEEE, P3102, DOI 10.1109/CVPR46437.2021.00312
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512
   Wu JL, 2022, IEEE T MULTIMEDIA, V24, P3693, DOI 10.1109/TMM.2021.3106096
   Wu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1570, DOI 10.1145/3394171.3413970
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xinting Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14042, DOI 10.1109/CVPR42600.2020.01406
   Yu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10988, DOI 10.1109/CVPR42600.2020.01100
   Yuan Z., 2020, PROC INT C LEARN REP
   Zang Y., 2021, P IEEECVF INT C COMP, P3457
   Zhao YC, 2022, IET IMAGE PROCESS, V16, P1305, DOI 10.1049/ipr2.12410
   Zhao YC, 2022, ELECTRON LETT, V58, P203, DOI 10.1049/ell2.12408
NR 50
TC 6
Z9 6
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6943
EP 6955
DI 10.1109/TMM.2024.3358080
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000037
DA 2024-08-05
ER

PT J
AU Chen, DL
   Wen, GH
   Wen, PC
   Yang, P
   Chen, R
   Li, C
AF Chen, Dongliang
   Wen, Guihua
   Wen, Pengcheng
   Yang, Pei
   Chen, Rui
   Li, Cheng
TI Cross-Domain Sample Relationship Learning for Facial Expression
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Databases; Transformers; Face recognition; Training; Task analysis;
   Target recognition; Knowledge transfer; Deep neural network; facial
   expression recognition; cross-domain; sample relationship learning;
   transformer
AB Cross-domain facial expression recognition is confronted by the problem of the large distribution discrepancy and samples inconsistencies between the source domain and target domain. To solve this problem, we propose a cross-domain sample relationship learning (CSRL) method that explores useful intrinsic sample relationships of two domains to narrow the domain discrepancy. Specifically, during the training stage, we first design inter-domain sample transformers to explore the sample similarity relationships between the source and target domains, and then deploy intra-domain sample transformers to capture the internal similar structure of the samples in each domain. Thus dual sample relationships can be learned to align the cross-domain similar samples and preserve the domain-specific information, which can facilitate both the inter-domain invariant features and intra-domain invariant features learning. Subsequently, we design a joint alignment strategy by simultaneously deploying the feature distribution alignment and cross-domain sample relationship learning. Thus, both local similar samples and global domain distribution of two domains can be well aligned to enhance the generalization ability of the model. Experimental results on several benchmark databases show the superiority of CSRL over some state-of-the-art methods.
C1 [Chen, Dongliang; Wen, Guihua; Yang, Pei] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China.
   [Wen, Pengcheng] HuBei Minzu Univ, Wuhan 430079, Hubei, Peoples R China.
   [Chen, Rui; Li, Cheng] Guangdong Second Prov Gen Hosp, Guangdong Tradit Med & Sports Injury Rehabil Res I, Guangzhou 510317, Peoples R China.
C3 South China University of Technology; Hubei Minzu University
RP Wen, GH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China.; Li, C (corresponding author), Guangdong Second Prov Gen Hosp, Guangdong Tradit Med & Sports Injury Rehabil Res I, Guangzhou 510317, Peoples R China.
EM ytucdl@foxmail.com; crghwen@scut.edu.cn; 202030203@hbmzu.edu.cn;
   yangpei@scut.edu.cn; chenr@gd2h.org.cn; lic@gd2h.org.cn
FU China National Science Foundation
FX No Statement Available
CR Abadi M., 2015, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, V1
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Bisogni C, 2022, IEEE T IND INFORM, V18, P5619, DOI 10.1109/TII.2022.3141400
   Brent R. P., 2013, Algorithms for minimization without derivatives
   Chung BD, 2012, NETW SPAT ECON, V12, P167, DOI 10.1007/s11067-011-9157-8
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Gong T, 2019, IEEE ACCESS, V7, P141627, DOI 10.1109/ACCESS.2019.2943604
   Hu Yashan, 2021, 2021 IEEE Sustainable Power and Energy Conference (iSPEC), P1692, DOI 10.1109/iSPEC53008.2021.9735563
   Huang RP, 2020, APPL SOFT COMPUT, V87, DOI 10.1016/j.asoc.2019.105978
   I. E. Agency, 2011, CO2 emissions from fuel combustion highlights, V22
   Ji YL, 2019, NEUROCOMPUTING, V333, P231, DOI 10.1016/j.neucom.2018.12.037
   Jia L., 2020, SCENARIOS PATTERNS S
   Jonkman J.K., 2009, Report No. NREL/TP-500-38060, DOI DOI 10.2172/947422
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Knaff JA, 2003, WEATHER FORECAST, V18, P80, DOI 10.1175/1520-0434(2003)018<0080:SDTCIF>2.0.CO;2
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Liebel L, 2018, Arxiv, DOI [arXiv:1805.06334, DOI 10.48550/ARXIV.1805.06334]
   Liu HX, 2019, Arxiv, DOI [arXiv:1806.09055, 10.48550/arXiv.1806.09055]
   Liu Y, 2011, IEEE T POWER SYST, V26, P145, DOI 10.1109/TPWRS.2010.2050219
   Lo L, 2022, IEEE T MULTIMEDIA, V24, P4275, DOI 10.1109/TMM.2022.3197365
   Lorca A, 2015, IEEE T POWER SYST, V30, P1702, DOI 10.1109/TPWRS.2014.2357714
   Lu XQ, 2021, ADV ATMOS SCI, V38, P690, DOI 10.1007/s00376-020-0211-7
   Lucey P., 2010, P IEEE COMP SOC C CO, P94
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Ni XY, 2022, J TRANSP ENG A-SYST, V148, DOI 10.1061/JTEPBS.0000670
   Ouyang M, 2014, STRUCT SAF, V48, P15, DOI 10.1016/j.strusafe.2014.01.001
   Ruan D., 2022, Int. J. Comput. Vis., P1
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Valstar MF, 2010, LREC 2010 - SEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, pJ65
   Wei YM, 2022, J. Beijing Inst. Technol. (Soc. Sci. Ed.), V4, P11, DOI [DOI 10.15918/J.JBITSS1009-3370.2022.1165, 10.15918/j.jbitss1009-3370.2022.1165]
   Wen GH, 2020, IEEE T MULTIMEDIA, V22, P2914, DOI 10.1109/TMM.2020.2966858
   Yan Haoyua, 2023, IEEE DataPort, DOI 10.21227/C8XX-3V17
   Yan Y, 2020, IEEE T MULTIMEDIA, V22, P2792, DOI 10.1109/TMM.2019.2962317
   Yang C., 2021, CSEE J. Power Energy Syst., DOI [10.17775/CSEEJPES.2020.03430, DOI 10.17775/CSEEJPES.2020.03430]
   Ying M, 2014, J ATMOS OCEAN TECH, V31, P287, DOI 10.1175/JTECH-D-12-00119.1
   Yperman I., 2007, The link transmission model for dynamic network loading
   Zhang FF, 2022, IEEE T MULTIMEDIA, V24, P1800, DOI 10.1109/TMM.2021.3072786
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zheng WM, 2015, INT CONF ACOUST SPEE, P1528, DOI 10.1109/ICASSP.2015.7178226
NR 40
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3788
EP 3798
DI 10.1109/TMM.2023.3316027
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300006
DA 2024-08-05
ER

PT J
AU Ding, XY
   Chen, Z
   Lin, WS
   Chen, ZZ
AF Ding, Xiaoying
   Chen, Zhao
   Lin, Weisi
   Chen, Zhenzhong
TI Towards 3D Colored Mesh Saliency: Database and Benchmarks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Saliency detection; Databases;
   Visualization; Image color analysis; Gaze tracking; Feature extraction;
   Mesh saliency detection; visual attention; 3D colored mesh; mesh
   simplification
ID OBJECT DETECTION; VISUAL-ATTENTION; HISTOGRAMS; DESCRIPTOR; IMAGES;
   ROBUST; MODEL
AB While saliency detection for 3D meshes has been extensively studied in the past decades, only a little work considers color information, and most of existing 3D mesh saliency databases are collected using meshes without color information. The lack of publicly available 3D colored mesh saliency database hinders the research progress in 3D colored mesh saliency detection. In this article, we established a novel 3D colored mesh saliency database (3DCMS) based on an eye-tracking experiment and investigated subjects' visual attention behavior towards 3D colored meshes. Based on the investigations, a novel 3D colored mesh saliency detection framework is proposed which takes both color and geometric features into consideration. To evaluate the performance of the proposed algorithm, we compare it with several relevant methods and apply it to 3D mesh simplification task. The quantitative and qualitative evaluation results demonstrate the superior performance of the proposed framework. The proposed 3DCMS database will be made publicly available.(1)
C1 [Ding, Xiaoying] Zhongnan Univ Econ & Law, Sch Informat & Safety Engn, Wuhan 430073, Peoples R China.
   [Ding, Xiaoying; Chen, Zhao; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
C3 Zhongnan University of Economics & Law; Wuhan University; Nanyang
   Technological University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
EM dingxiaoying@zuel.edu.cn; chen_zhao@whu.edu.cn; wslin@ntu.edu.sg;
   zzchen@ieee.org
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947; zhao, chen/0009-0009-6969-7938
FU Natural Science Foundation of Hubei Province, China
FX No Statement Available
CR Abid M, 2020, IEEE IMAGE PROC, P3448, DOI 10.1109/ICIP40778.2020.9191064
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Aljeri N, 2017, 2017 IEEE 28TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), DOI 10.1109/PIMRC.2017.8292722
   [Anonymous], 2005, ACM Trans. Appl. Percep.
   [Anonymous], 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7298710
   Banerjee S, 2018, INFORM SCIENCES, V424, P337, DOI 10.1016/j.ins.2017.10.011
   Castello P, 2008, INFORM SCIENCES, V178, P2375, DOI 10.1016/j.ins.2008.01.011
   Chang A.X., 2015, ArXiv
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Ding XY, 2022, NEUROCOMPUTING, V502, P120, DOI 10.1016/j.neucom.2022.06.088
   Ding XY, 2019, IEEE T IMAGE PROCESS, V28, P5379, DOI 10.1109/TIP.2019.2918735
   Dong L, 2015, IEEE T MULTIMEDIA, V17, P2174, DOI 10.1109/TMM.2015.2484221
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Garland Michael, 1997, SIGGRAPH, DOI DOI 10.1145/258734.258849
   Harel J., 2007, ADV NEURAL INF PROCE, V19, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073
   Hu SF, 2020, IEEE T MULTIMEDIA, V22, P2278, DOI 10.1109/TMM.2019.2952983
   Hu SF, 2020, NEUROCOMPUTING, V400, P11, DOI 10.1016/j.neucom.2020.02.106
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kim Y, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670676
   Konstantinides JM, 2009, IEEE T MULTIMEDIA, V11, P23, DOI 10.1109/TMM.2008.2008913
   Lavoué G, 2018, COMPUT GRAPH FORUM, V37, P191, DOI 10.1111/cgf.13353
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G, 2016, IEEE T PATTERN ANAL, V38, P2544, DOI 10.1109/TPAMI.2016.2522437
   Lin W, 2022, APSIPA TRANS SIGNAL, V11, DOI 10.1561/116.00000125
   Ma GX, 2020, IEEE T VIS COMPUT GR, V26, P3535, DOI 10.1109/TVCG.2020.3023636
   Maximo A, 2011, GRAPH MODELS, V73, P231, DOI 10.1016/j.gmod.2011.05.002
   Nouri A, 2015, IEEE IMAGE PROC, P2820, DOI 10.1109/ICIP.2015.7351317
   Papon J, 2013, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2013.264
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Reynolds JH, 2003, NEURON, V37, P853, DOI 10.1016/S0896-6273(03)00097-7
   Rossignac J., 1993, Geometric Modeling in Computer Graphics, P455
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salvucci Dario D., 2000, P 2000 S EYE TRACK R, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Shtrom E, 2013, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2013.446
   Song MK, 2022, IEEE T IMAGE PROCESS, V31, P6124, DOI 10.1109/TIP.2022.3205747
   Song R, 2021, PROC CVPR IEEE, P8849, DOI 10.1109/CVPR46437.2021.00874
   Song R, 2021, IEEE T VIS COMPUT GR, V27, P151, DOI 10.1109/TVCG.2019.2928794
   Song R, 2018, VISUAL COMPUT, V34, P323, DOI 10.1007/s00371-016-1334-9
   Souza LS, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107028
   Stavropoulos G, 2010, IEEE T MULTIMEDIA, V12, P692, DOI 10.1109/TMM.2010.2053023
   Tao PP, 2015, COMPUT GRAPH-UK, V46, P264, DOI 10.1016/j.cag.2014.09.023
   Tasse FP, 2015, IEEE I CONF COMP VIS, P163, DOI 10.1109/ICCV.2015.27
   TAVAKOLI HR, 2017, PROC CVPR IEEE, P6354, DOI DOI 10.1109/CVPR.2017.673
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Wang GT, 2021, PROC CVPR IEEE, P15114, DOI 10.1109/CVPR46437.2021.01487
   Wang X, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275094
   Wang X, 2016, IEEE COMPUT GRAPH, V36, P46, DOI 10.1109/MCG.2016.47
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   Wu ZY, 2022, IEEE T IMAGE PROCESS, V31, P6649, DOI 10.1109/TIP.2022.3214332
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
NR 60
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3580
EP 3591
DI 10.1109/TMM.2023.3312924
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200002
DA 2024-08-05
ER

PT J
AU Feng, ZR
   Zeng, ZM
   Guo, CL
   Li, Z
   Hu, L
AF Feng, Zerun
   Zeng, Zhimin
   Guo, Caili
   Li, Zheng
   Hu, Lin
TI Learning From Noisy Correspondence With Tri-Partition for Cross-Modal
   Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Semantics; Training; Semisupervised learning; Data
   models; Costs; Visualization; Cross-modal matching; noisy
   correspondence; image-text matching; video-text matching
ID TRANSFORMER
AB Due to high labeling cost, it is inevitable to introduce a certain proportion of noisy correspondence into visual-text datasets, resulting in poor model robustness for cross-modal matching. Although recent methods divide the datasets into clean and noisy pair subsets to yield promising achievements, they still suffer from deep neural networks over-fitting on noisy correspondence. In particular, the similar positive pairs with partially relevant semantic correspondence are easily partitioned into noisy pair subset by mistake without carefully selection, which brings harmful impact on robust learning. Meanwhile, the similar negative pairs with partially relevant semantic correspondence lead to ambiguous distance relations in common space learning, which also damages the stability of performance. To solve the coarse-grained dataset division problem, we propose Correspondence Tri-Partition Rectifier (CTPR) to partition the training set into clean, hard, and noisy pair subsets based on the memorization effect of neural networks and prediction inconsistency. Then, we refine the correspondence labels for each subset to indicate the real semantic correspondence between visual-text pairs. The differences between rectified labels of anchors and hard negatives are recast as the adaptive margin in the improved triplet loss for robust training in a co-teaching manner. To verify the effectiveness and robustness of our method, we conduct experiments by implementing image-text and video-text matching as two showcases. Extensive experiments on Flickr30 K, MS-COCO, MSR-VTT, and LSMDC datasets verify that our method successfully partitions the visual-text pairs according to their semantic correspondence and improves performance under noisy data training.
C1 [Feng, Zerun; Zeng, Zhimin; Li, Zheng] Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing Key Lab Network Syst Architecture & Conver, Beijing 100876, Peoples R China.
   [Guo, Caili] Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing Lab Adv Informat Networks, Beijing 100876, Peoples R China.
   [Hu, Lin] China Telecom Digital Intelligence Technol Co Ltd, Beijing 100035, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications
RP Guo, CL (corresponding author), Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing Lab Adv Informat Networks, Beijing 100876, Peoples R China.
EM fengzerun@bupt.edu.cn; zengzm@bupt.edu.cn; guocaili@bupt.edu.cn;
   lizhengzachary@bupt.edu.cn; hul12@chinatelecom.cn
OI Li, Zheng/0000-0003-2535-2523; Feng, Zerun/0000-0003-3987-0591
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR [Anonymous], 2000, Bayesian theory
   Arpit D, 2017, PR MACH LEARN RES, V70
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Chen FY, 2021, IEEE T MULTIMEDIA, V23, P3073, DOI 10.1109/TMM.2020.3019710
   Chen P., 2021, PROC INT C LEARN REP, P1
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2019, IEEE T GEOSCI REMOTE, V57, P1741, DOI 10.1109/TGRS.2018.2868851
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Faghri F., 2018, PROC BRIT MACH VIS C
   Fawzi A., 2016, ADV NEURAL INFORM PR, P1632
   Feng ZR, 2023, IEEE T CIRC SYST VID, V33, P1438, DOI 10.1109/TCSVT.2022.3207910
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Goldberger J., 2017, PROC INT C LEARN REP
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hendrycks D, 2018, ADV NEUR IN, V31
   Hu W., 2020, Open Graph Benchmark: Datasets for Machine Learning on Graphs
   Huang Z., 2021, Advances in Neural Informa- tion Processing Systems (NeurIPS-21), P29406
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Karpathy A, 2014, ADV NEUR IN, V27
   Kim S, 2019, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2019.00239
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li H., 2022, NeurIPS, P11934
   Li J., 2020, PROC INT C LEARN REP
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Liang XF, 2022, Arxiv, DOI arXiv:2202.09579
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu FY, 2020, AAAI CONF ARTIF INTE, V34, P11563
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu S, 2020, P NEUIPS DEC 6 12 VI
   Liu Y., 2019, PROC BRIT MACH VIS C
   Lu J, 2018, PR MACH LEARN RES, V80
   Ma ZY, 2011, IEEE T PATTERN ANAL, V33, P2160, DOI 10.1109/TPAMI.2011.63
   Malach E, 2017, ADV NEUR IN, V30
   Olston Christopher, 2010, Foundations and Trends in Information Retrieval, V4, P175, DOI 10.1561/1500000017
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Permuter H, 2006, PATTERN RECOGN, V39, P695, DOI 10.1016/j.patcog.2005.10.028
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Qin Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4948, DOI 10.1145/3503161.3547922
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Song G, 2021, IEEE T MULTIMEDIA, V23, P1708, DOI 10.1109/TMM.2020.3002177
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Tan C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1405, DOI 10.1145/3474085.3475622
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wray M, 2021, PROC CVPR IEEE, P3649, DOI 10.1109/CVPR46437.2021.00365
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yan Y, 2014, MACH LEARN, V95, P291, DOI 10.1007/s10994-013-5412-1
   Yang Erkun, 2022, CVPR, P7551
   Yang S, 2023, PROC CVPR IEEE, P19883, DOI 10.1109/CVPR52729.2023.01904
   Yu E, 2022, NEUROCOMPUTING, V486, P215, DOI 10.1016/j.neucom.2021.11.035
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Yu XR, 2019, PR MACH LEARN RES, V97
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang HW, 2023, IEEE T MULTIMEDIA, V25, P8632, DOI 10.1109/TMM.2023.3239183
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhang Y, 2021, PR MACH LEARN RES, V139
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 64
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3884
EP 3896
DI 10.1109/TMM.2023.3318002
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300003
DA 2024-08-05
ER

PT J
AU Li, J
   Wang, QQ
   Yang, M
   Gao, QX
   Gao, XB
AF Li, Jing
   Wang, Qianqian
   Yang, Ming
   Gao, Quanxue
   Gao, Xinbo
TI Efficient Anchor Graph Factorization for Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-view clustering; anchor graph; non-negative matrix factorisation;
   tensor Schatten p-norm
AB Due to the excellent interpretability of non-negative matrix factorization (NMF), NMF-based multi-view clustering has attracted much attention for multi-media data analysis and processing. However, the existing clustering methods leverage NMF to cluster data matrix, resulting in high computational complexity. Moreover, they are sub-optimal to exploit the complementary information between views because they all measure the between-views error pixel by pixel. To tackle this problem, inspired by orthogonal NMF and anchor graph, we present an efficient anchor graph factorization model with orthogonal, non-negative, and tensor low-rank constraints. We use an anchor graph instead of a data matrix to get an indicator matrix without post-processing, which remarkably reduces the computational complexity. To exploit the between-views complementary information well, we introduce tensor Schatten $p$-norm regularization on the third tensor, composed of soft label matrices of views. The solution can be obtained by iteratively optimizing four decoupled sub-problems, which can be solved more efficiently with good convergence. Through experimental results on the six multi-view datasets, our approach ensures the enhancement of clustering performance while improving efficiency.
C1 [Li, Jing; Wang, Qianqian; Gao, Quanxue] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
   [Yang, Ming] Harbin Engn Univ, Coll Math Sci, Harbin 150001, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; Harbin Engineering University; Chongqing University
   of Posts & Telecommunications
RP Gao, QX (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
EM muscel@qq.com; qqwang@xidian.edu.cn; yangmingmath@gmail.com;
   qxgao@xidian.edu.cn; xd_gxb_pr@163.com
OI Yang, Ming/0000-0003-1810-1566
FU National NSFC
FX No Statement Available
CR [Anonymous], 2007, ICML
   Cai X., 2013, P 23 INT JOINT C ART
   Cao XC, 2015, IEEE T IMAGE PROCESS, V24, P4381, DOI 10.1109/TIP.2015.2463223
   Chen X., 2011, P 25 AAAI C ART INT, P313
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Ding C., 2006, P 12 ACM SIGKDD INT, P126, DOI DOI 10.1145/1150402.1150420
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Gao QX, 2021, IEEE T PATTERN ANAL, V43, P2133, DOI 10.1109/TPAMI.2020.3017672
   Greene D, 2009, LECT NOTES ARTIF INT, V5781, P423, DOI 10.1007/978-3-642-04180-8_45
   Han JW, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1809
   Han JW, 2017, AAAI CONF ARTIF INTE, P1969
   He J., 2010, ICML, DOI DOI 10.1007/s11263-007-0090-8
   Hu ZX, 2020, INFORM FUSION, V55, P251, DOI 10.1016/j.inffus.2019.09.005
   Hua W., 2011, IJCAI P INT JOINT C, P1553, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-261
   Jiang Y, 2014, MACH VISION APPL, V25, P1635, DOI 10.1007/s00138-013-0556-3
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Kumar A., 2011, Advances in Neural Information Processing Systems, V24, DOI DOI 10.5555/2986459.2986617
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CK, 2016, IEEE IMAGE PROC, P4042, DOI 10.1109/ICIP.2016.7533119
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6323, DOI 10.1109/TNNLS.2018.2829867
   Liu J., 2013, PROC 13 SIAM INT C, P252, DOI DOI 10.1137/1.9781611972832.28
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107207
   Nie FP, 2012, IEEE DATA MINING, P566, DOI 10.1109/ICDM.2012.160
   Runwu Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14607, DOI 10.1109/CVPR42600.2020.01463
   Saha Moumita, 2013, Pattern Recognition and Machine Intelligence. 5th International Conference, PReMI 2013. Proceedings: LNCS 8251, P128, DOI 10.1007/978-3-642-45062-4_17
   Sakai T, 2009, LECT NOTES ARTIF INT, V5632, P372, DOI 10.1007/978-3-642-03070-3_28
   Semerci O, 2014, IEEE T IMAGE PROCESS, V23, P1678, DOI 10.1109/TIP.2014.2305840
   Tang C, 2023, SCI CHINA INFORM SCI, V66, DOI 10.1007/s11432-022-3579-1
   Tang C, 2023, IEEE T KNOWL DATA EN, V35, P6449, DOI 10.1109/TKDE.2022.3172687
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tao ZQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2843
   Trosten DJ, 2023, PROC CVPR IEEE, P23976, DOI 10.1109/CVPR52729.2023.02296
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang J, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3253246
   Wang M, 2016, IEEE T KNOWL DATA EN, V28, P1864, DOI 10.1109/TKDE.2016.2535367
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Xia W, 2023, IEEE T PATTERN ANAL, V45, P5187, DOI 10.1109/TPAMI.2022.3187976
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Xie Y, 2016, IEEE T IMAGE PROCESS, V25, P4842, DOI 10.1109/TIP.2016.2599290
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang B, 2021, IEEE T IMAGE PROCESS, V30, P2575, DOI 10.1109/TIP.2020.3045631
   Yang HZ, 2022, IEEE T IMAGE PROCESS, V31, P3591, DOI 10.1109/TIP.2022.3171411
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
NR 55
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5834
EP 5845
DI 10.1109/TMM.2023.3340095
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100028
DA 2024-08-05
ER

PT J
AU Li, X
   Lu, YT
   Chen, ZB
AF Li, Xin
   Lu, Yiting
   Chen, Zhibo
TI FreqAlign: Excavating Perception-Oriented Transferability for Blind
   Image Quality Assessment From a Frequency Perspective
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Frequency-domain analysis; Feature extraction; Image
   quality; Task analysis; Adaptation models; Frequency synthesizers; Blind
   image quality assessment; unsupervised domain adaptation; frequency
   alignment
ID STATISTICS
AB Blind Image Quality Assessment (BIQA) is susceptible to poor transferability when the distribution shift occurs, e.g., from synthesis degradation to authentic degradation. To mitigate this, some studies have attempted to design unsupervised domain adaptation (UDA) based schemes for BIQA, which intends to eliminate the domain shift through adversarial-based feature alignment. However, the feature alignment is usually taken at the low-frequency space of features since the global average pooling operation. This ignores the transferable perception knowledge in other frequency components and causes the sub-optimal solution for the UDA of BIQA. To overcome this, from a novel frequency perspective, we propose an effective alignment strategy, i.e., Frequency Alignment (dubbed FreqAlign), to excavate the perception-oriented transferability of BIQA in the frequency space. Concretely, we study what frequency components of features are more proper for perception-oriented alignment. Based on this, we propose to improve the perception-oriented transferability of BIQA by performing feature frequency decomposition and selecting the frequency components that contained the most transferable perception knowledge for alignment. To achieve a stable and effective frequency selection, we further propose the frequency movement with a sliding window to find the optimal frequencies for alignment, which is composed of three strategies, i.e., warm up with pre-training, frequency movement-based selection, and perturbation-based finetuning. Extensive experiments under different domain adaptation settings of BIQA have validated the effectiveness of our proposed method.
C1 [Li, Xin; Lu, Yiting; Chen, Zhibo] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, ZB (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
EM lixin666@mail.ustc.edu.cn; luyt31415@mail.ustc.edu.cn;
   chenzhibo@ustc.edu.cn
FU NSFC
FX No Statement Available
CR Chen BL, 2021, IEEE T IMAGE PROCESS, V30, P5463, DOI 10.1109/TIP.2021.3084750
   Chen P., 2021, P IEEECVF INT C COMP, P5178
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Damodaran B. B., 2018, P EUR C COMP VIS ECC, P447, DOI DOI 10.1007/978-3-030-01225-028
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ganin Y., 2016, The Journal ofMachine Learning Research, V17, P2096
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Haohan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8681, DOI 10.1109/CVPR42600.2020.00871
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Shuai, 2022, P 31 INT JOINT C ART, P942, DOI DOI 10.24963/IJCAI.2022/132
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Huang Jiaxing, 2021, P IEEECVF INT C COMP, P8988
   Jang E., 2016, INT C LEARN REPR
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jin X, 2021, PROC IEEECVF INT C C, P9174
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Kingma D. P., 2014, arXiv
   Kundu JN, 2022, AAAI CONF ARTIF INTE, P1220
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee S, 2012, SIGNAL PROCESS-IMAGE, V27, P31, DOI 10.1016/j.image.2011.08.002
   Li X., 2022, arXiv
   Li X, 2021, AAAI CONF ARTIF INTE, V35, P1975
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Liu Hong, 2021, ADV NEURAL INFORM PR, V34
   Liu JZ, 2023, IEEE T MULTIMEDIA, V25, P5358, DOI 10.1109/TMM.2022.3190700
   Liu JZ, 2022, Arxiv, DOI arXiv:2207.08124
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lu YT, 2022, Arxiv, DOI arXiv:2207.14489
   Luo C, 2022, PROC CVPR IEEE, P15294, DOI 10.1109/CVPR52688.2022.01488
   Madhusudana PC, 2022, IEEE T IMAGE PROCESS, V31, P4149, DOI 10.1109/TIP.2022.3181496
   Marichal X., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P386, DOI 10.1109/ICIP.1999.822923
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mittal A, 2012, IEEE SIGNAL PROC LET, V19, P75, DOI 10.1109/LSP.2011.2179293
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Muijs R., 2005, 2005 13th European Signal Processing Conference, P1
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nabavi S, 2021, Arxiv, DOI arXiv:2112.06806
   Ong EP, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P469, DOI 10.1109/ISSPA.2003.1224741
   Pan ZQ, 2022, IEEE T IMAGE PROCESS, V31, P1613, DOI 10.1109/TIP.2022.3144892
   Paszke A, 2019, ADV NEUR IN, V32
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Ponomarenko N., 2009, Advances of Modern Radioelectronics, V10, P30
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Rao Yongming, 2021, Advances in neural information processing systems, V34
   Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Saad MA, 2010, IEEE SIGNAL PROC LET, V17, P583, DOI 10.1109/LSP.2010.2045550
   Sazzad Z. M. P., 2007, PROC IEEE INTCONF IM
   Shang RH, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12050872
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shuyang Gu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P369, DOI 10.1007/978-3-030-58621-8_22
   Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Tliba M, 2022, IEEE IMAGE PROC, P3692, DOI 10.1109/ICIP46576.2022.9897600
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P2624, DOI 10.1109/TMM.2022.3149629
   Wang ZH, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109296
   Wei GQ, 2021, PROC CVPR IEEE, P16638, DOI 10.1109/CVPR46437.2021.01637
   Xie JH, 2022, Arxiv, DOI arXiv:2206.07706
   Xie Wenbin, 2021, P IEEECVF INT C COMP, P4308, DOI DOI 10.1109/ICCV48922.2021.00427
   Xu ZQJ, 2020, COMMUN COMPUT PHYS, V28, P1746, DOI 10.4208/cicp.OA-2020-0085
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yang JH, 2021, PROC CVPR IEEE, P10363, DOI 10.1109/CVPR46437.2021.01023
   Yang JY, 2023, IEEE WINT CONF APPL, P520, DOI 10.1109/WACV56688.2023.00059
   Yang Q, 2022, PROC CVPR IEEE, P21147, DOI 10.1109/CVPR52688.2022.02050
   Yang XH, 2021, IEEE T MULTIMEDIA, V23, P4326, DOI 10.1109/TMM.2020.3040529
   Yang YC, 2020, PROC CVPR IEEE, P4084, DOI 10.1109/CVPR42600.2020.00414
   Yin GH, 2022, AAAI CONF ARTIF INTE, P3134
   Yingxue Pang, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P468, DOI 10.1007/978-3-030-67070-2_28
   Yu ZQ, 2023, IEEE T CIRC SYST VID, V33, P4232, DOI 10.1109/TCSVT.2023.3242614
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang C, 2022, IEEE T MULTIMEDIA, V24, P2246, DOI 10.1109/TMM.2021.3078141
   Zhang JY, 2022, PROC CVPR IEEE, P9819, DOI 10.1109/CVPR52688.2022.00960
   Zhang K, 2023, IEEE Transactions on Multimedia
   Zhang RH, 2022, IEEE T MULTIMEDIA, V24, P1735, DOI 10.1109/TMM.2021.3070138
   Zhang WX, 2023, IEEE T PATTERN ANAL, V45, P2864, DOI 10.1109/TPAMI.2022.3178874
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang Youshan, 2021, arXiv
   Zhu YC, 2019, NEURAL NETWORKS, V119, P214, DOI 10.1016/j.neunet.2019.07.010
   Zou W, 2022, IEEE Trans. Multimedia
NR 95
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4652
EP 4666
DI 10.1109/TMM.2023.3325755
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100020
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, YT
   Gu, K
   Cao, JC
   Wang, SQ
   Zhai, GT
   Dong, JY
   Kwong, S
AF Liu, Yutao
   Gu, Ke
   Cao, Jingchao
   Wang, Shiqi
   Zhai, Guangtao
   Dong, Junyu
   Kwong, Sam
TI UIQI: A Comprehensive Quality Evaluation Index for Underwater Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality; Feature extraction; Image color analysis; Indexes;
   Predictive models; Colored noise; Visualization; Underwater image; image
   quality assessment (IQA); no-reference (NR); objective metric;
   statistical modeling
ID FREE-ENERGY PRINCIPLE; SIMILARITY; VISIBILITY
AB Due to the light absorption and scattering in waterbodies, acquired underwater images frequently suffer from color cast, blur, low contrast, noise, etc., which seriously degrade the image quality and affect their subsequent applications. Therefore, it is necessary to propose a reliable and practical underwater image quality assessment (IQA) model that can faithfully evaluate underwater image quality. To this end, in this article, we establish a novel quality assessment model for underwater images by in-depth analysis and characterization of multiple image properties. Specifically, we propose characterizing the image luminance, color cast, sharpness, contrast, fog density and noise to comprehensively describe the image quality to evaluate the underwater image quality more accurately. Dedicated features are elaborately investigated to characterize those quality-aware image properties. After feature extraction, we employ support vector regression (SVR) to integrate all the quality-aware features and regress them onto the underwater image quality score. Extensive tests performed on standard underwater image quality databases demonstrate the superior prediction performance of the proposed underwater IQA model to state-of-the-art congeneric quality assessment models.
C1 [Liu, Yutao; Cao, Jingchao; Dong, Junyu] Ocean Univ China, Sch Comp Sci & Technol, Qingdao 266100, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Wang, Shiqi; Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
C3 Ocean University of China; Beijing University of Technology; City
   University of Hong Kong; Shanghai Jiao Tong University
RP Cao, JC; Dong, JY (corresponding author), Ocean Univ China, Sch Comp Sci & Technol, Qingdao 266100, Peoples R China.
EM liuyutao@ouc.edu.cn; guke@bjut.edu.cn; caojingchao@ouc.edu.cn;
   shiqwang@cityu.edu.hk; zhaiguangtao@gmail.com; dongjunyu@ouc.edu.cn;
   cssamk@cityu.edu.hk
RI Cao, Jingchao/KVC-1193-2024; Kwong, Sam/C-9319-2012; Zhai,
   Guangtao/X-5949-2019
OI Kwong, Sam/0000-0001-7484-7261; Zhai, Guangtao/0000-0001-8165-9322;
   Dong, Junyu/0000-0001-7012-2087
FU National Science Foundation of China
FX No Statement Available
CR Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   COHEN A, 1992, COMMUN PUR APPL MATH, V45, P485, DOI 10.1002/cpa.3160450502
   Fu ZQ, 2022, SIGNAL PROCESS-IMAGE, V102, DOI 10.1016/j.image.2021.116622
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   Gu G., 2012, ASIA PACSIGNAL INF P, P1
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2013, IEEE INT SYMP CIRC S, P1095, DOI 10.1109/ISCAS.2013.6572041
   Guo C., 2023, AAAI, V37, P702
   Guo PF, 2022, IEEE T MULTIMEDIA, V24, P1980, DOI 10.1109/TMM.2021.3074825
   Guo Pengfei, 2022, IEEE T MULTIMEDIA, V2022
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li F, 2012, 2012 IEEE FIFTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P662, DOI 10.1109/ICACI.2012.6463249
   Li M, 2022, IEEE Trans. Geosci. Remote Sens., V60
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li Z., 2016, PROC IEEE OCEANS, P1
   Ling SY, 2021, IEEE T MULTIMEDIA, V23, P4245, DOI 10.1109/TMM.2020.3038305
   Liu YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414837
   Liu YT, 2020, IEEE ACCESS, V8, P84105, DOI 10.1109/ACCESS.2020.2991842
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu YT, 2017, J VIS COMMUN IMAGE R, V46, P70, DOI 10.1016/j.jvcir.2017.03.007
   Liu YT, 2016, IEEE INT SYMP CIRC S, P1586, DOI 10.1109/ISCAS.2016.7538867
   Lu HM, 2016, IEEE IMAGE PROC, P1998, DOI 10.1109/ICIP.2016.7532708
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Panetta A., 2014, J. Biomed.Imag., V2014, P1
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Schechner YY, 2005, IEEE J OCEANIC ENG, V30, P570, DOI 10.1109/JOE.2005.850871
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tang SQ, 2020, 2020 13TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2020), P378, DOI 10.1109/CISP-BMEI51763.2020.9263539
   Venkatanath N, 2015, NATL CONF COMMUN
   Video Quality Experts Group, 2000, VQEG M OTT CAN MARCH
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2023, IEEE T IMAGE PROCESS, V32, P1442, DOI 10.1109/TIP.2023.3244647
   Wang ZY, 2023, IEEE T CIRC SYST VID, V33, P1123, DOI 10.1109/TCSVT.2022.3212788
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Xu HB, 2021, MATER CORROS, V72, P720, DOI 10.1002/maco.202011886
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang M, 2014, IEEE SIGNAL PROC LET, V21, P1215, DOI 10.1109/LSP.2014.2330848
   Yang N, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116218
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhai GT, 2013, PICT COD SYMP, P29, DOI 10.1109/PCS.2013.6737675
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zoran D, 2009, IEEE I CONF COMP VIS, P2209, DOI 10.1109/ICCV.2009.5459476
NR 67
TC 9
Z9 9
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2560
EP 2573
DI 10.1109/TMM.2023.3301226
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100033
DA 2024-08-05
ER

PT J
AU Ma, JW
   Liang, M
   Chen, L
   Tian, S
   Chen, SL
   Qin, JY
   Yin, XC
AF Ma, Jia-Wei
   Liang, Min
   Chen, Lei
   Tian, Shu
   Chen, Song-Lu
   Qin, Jingyan
   Yin, Xu-Cheng
TI Sample Weighting with Hierarchical Equalization Loss for Dense Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; label assignment; hierarchical equalization; weighted
   loss
AB Label assignment (LA) is one of the essential phases in the object detection paradigm and aims to classify samples as foreground or background. Current LA strategies generally discriminate samples by explicit thresholds and then calculate weighted losses based on their significances. However, existing methods mostly neglect to consider the importance of samples comprehensively due to the uneven distribution of objects and the limitations of detector structures. In this paper, we propose a hierarchical equalization loss (HEL) by reconsidering the underlying factors affecting sample weights. First, we mitigate sample imbalance at three progressive levels. (1) Task level. We propose task-reconciled weights (TRW) to overcome the effects caused by inter-task inconsistencies (i.e., the inherent differences of classification and localization). (2) Instance level. We propose instance-aware normalization (IAN) for reconstructing the distribution of sample weights within an instance to suppress environmental noise. (3) Pyramid level. We propose hierarchical modulation (HM) to alleviate the unbalanced distribution of multi-scale objects on feature pyramids. Then, we stack the above three mechanisms and formulate the effective weighted loss. Moreover, we propose a staggered candidate bag construction (SCBC) mechanism to further improve the robustness of our method. Without adding any extra overhead, HEL can improve the performance of representative detectors by an impressive margin. Equipped with HEL, a single "ResNet-50+FPN+Head" detector can achieve a performance of 41.9 AP on COCO under 1x schedule, outperforming other existing LA methods. Extensive experiments conducted on multiple backbones and datasets demonstrate the effectiveness of our method.
C1 [Ma, Jia-Wei; Liang, Min; Chen, Lei; Tian, Shu; Chen, Song-Lu; Yin, Xu-Cheng] Univ Sci & Technol Beijing, Dept Comp Sci & Technol, Beijing 100083, Peoples R China.
   [Ma, Jia-Wei; Chen, Song-Lu; Yin, Xu-Cheng] Univ Sci & Technol Beijing, USTB EEasyTech Joint Lab Arti?cial Intelligence, Beijing 100083, Peoples R China.
   [Qin, Jingyan] Univ Sci & Technol Beijing, Dept Ind Design, Beijing 100083, Peoples R China.
C3 University of Science & Technology Beijing; University of Science &
   Technology Beijing; University of Science & Technology Beijing
RP Chen, L (corresponding author), Univ Sci & Technol Beijing, Dept Comp Sci & Technol, Beijing 100083, Peoples R China.; Qin, JY (corresponding author), Univ Sci & Technol Beijing, Dept Ind Design, Beijing 100083, Peoples R China.
EM mjw20151001@hotmail.com; lm20200126@hotmail.com;
   chenlei2022@ustb.edu.cn; shutian@ustb.edu.cn; chenslvs7@gmail.com;
   qinjingyanking@foxmail.com; xuchengyin@ustb.edu.cn
RI Chen, Song-Lu/GXH-7004-2022
OI Chen, Song-Lu/0000-0002-0780-658X; Yin, Xucheng/0000-0003-0023-0220; Ma,
   Jia-Wei/0000-0002-7628-6047; Chen, Lei/0000-0002-4279-3892
FU National Key Research and Development Program of China
FX No Statement Available
CR Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Carion N., 2020, EUR C COMP VIS, P213
   Chen KA, 2021, IEEE T PATTERN ANAL, V43, P3782, DOI 10.1109/TPAMI.2020.2991457
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P1558, DOI 10.1109/TMM.2021.3067439
   Chen Y., 2020, Adv. Neural Inf. Process. Syst., V33, P5621
   Chen ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4939, DOI 10.1145/3474085.3475351
   Cong RM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3123984
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai XY, 2021, PROC CVPR IEEE, P7369, DOI 10.1109/CVPR46437.2021.00729
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3397, DOI 10.1109/ICCV48922.2021.00340
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Gao ZT, 2022, PROC CVPR IEEE, P5354, DOI 10.1109/CVPR52688.2022.00529
   Gao ZT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3621, DOI 10.1109/ICCV48922.2021.00362
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Guanglu Song, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11560, DOI 10.1109/CVPR42600.2020.01158
   Hengduo Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10585, DOI 10.1109/CVPR42600.2020.01060
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jia D, 2023, PROC CVPR IEEE, P19702, DOI 10.1109/CVPR52729.2023.01887
   Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li B, 2022, PROC CVPR IEEE, P6980, DOI 10.1109/CVPR52688.2022.00686
   Li S, 2022, PROC CVPR IEEE, P9377, DOI 10.1109/CVPR52688.2022.00917
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S., 2022, PROC INT C LEARN REP
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu ST, 2019, Arxiv, DOI arXiv:1911.09516
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3133956
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma YC, 2021, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR46437.2021.00176
   Oksuz K, 2020, P ADV NEUR INF PROC, V33, P15534
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Qi Cai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14161, DOI 10.1109/CVPR42600.2020.01418
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren J., 2020, ADV NEURAL INFORM PR, V33, P4175, DOI DOI 10.48550/ARXIV.2007.10740
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Singh B, 2018, 32 C NEURAL INFORM P
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tan JR, 2021, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR46437.2021.00173
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang KY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3611, DOI 10.1109/ICCV48922.2021.00361
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wei Ke, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10203, DOI 10.1109/CVPR42600.2020.01022
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1570, DOI 10.1145/3394171.3413970
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Yuhang Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11580, DOI 10.1109/CVPR42600.2020.01160
   Zhang H., 2023, PROC INT C LEARN REP
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhiwei Dong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10516, DOI 10.1109/CVPR42600.2020.01053
   Zhu BJ, 2020, Arxiv, DOI arXiv:2007.03496
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
NR 77
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5846
EP 5859
DI 10.1109/TMM.2023.3340065
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100018
DA 2024-08-05
ER

PT J
AU Mao, YQ
   Yan, XQ
   Liu, JM
   Ye, YD
AF Mao, Yiqiao
   Yan, Xiaoqiang
   Liu, Jiaming
   Ye, Yangdong
TI ConGMC: Consistency-Guided Multimodal Clustering via Mutual Information
   Maximin
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Consistent information; multimodal alignment; multimodal clustering;
   superfluous information
ID MULTIVIEW
AB Aligning multiple heterogeneous modalities in a parameter-sharing encoder to mine consistent information is a core idea of multimodal learning. However, two drawbacks hinder the development of such methods for clustering tasks: 1) each modality contains a considerable amount of superfluous information that cannot be aligned, impeding the mining of consistent information and 2) one-to-one alignment is contradictory to the clustering principle of minimum intra-cluster distance, leading to suboptimal clustering results. In this paper, we propose a novel Consistency-Guided Multimodal Clustering method (ConGMC) to remove superfluous information within the modalities unsupervised through information theory while improving one-to-one alignment for the clustering task. ConGMC contains multiple unimodal encoders and a multimodal shared encoder, where the former learns unimodal representation while the latter aligns multiple modalities to learn the cluster partition. Specifically, we first construct a mutual information maximin function to distinguish consistent information from superfluous information, in which the consistent and superfluous information are maximally retained and removed, respectively. Then a Clustering-Friendly Alignment strategy (CF-Align) is designed to address the contradiction between the alignment and clustering tasks. CF-Align dynamically adjusts the set of negative samples according to the learned cluster partition to avoid increasing the intra-cluster distance. Finally, we consider the cluster partition as a consistent constraint to optimize the multimodal shared encoder, enabling consistent information to guide the training process iteratively. Moreover, a variational optimization algorithm is proposed to ensure that ConGMC converges to a local optimum. Numerous experimental results on twelve real-world datasets validate that the proposed ConGMC method outperforms the state-of-the-art multimodal clustering methods.
C1 [Mao, Yiqiao; Yan, Xiaoqiang; Liu, Jiaming; Ye, Yangdong] Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450052, Peoples R China.
C3 Zhengzhou University
RP Ye, YD (corresponding author), Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450052, Peoples R China.
EM ieyqmao@gs.zzu.edu.cn; iexqyan@zzu.edu.cn; iejmliu@gs.zzu.edu.cn;
   ieydye@zzu.edu.cn
OI Liu, Jiaming/0000-0003-1619-5482
FU National Natural Science Foundation of China
FX No Statement Available
CR Asano Y. M., 2020, ICLR, P1
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Chen B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7992, DOI 10.1109/ICCV48922.2021.00791
   Chen MS, 2020, AAAI CONF ARTIF INTE, V34, P3513
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen YB, 2021, PROC CVPR IEEE, P7012, DOI 10.1109/CVPR46437.2021.00694
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Cover T.M., 1999, ELEMENTS INFORM THEO
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Federici A., 2020, P INT C LEARN REPR, P1
   Grubinger M., 2006, INT WORKSHOP ONTOIMA, P13
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu D, 2019, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR.2019.00947
   Huiskes M.J., 2008, P ACM INT C MULT INF, P39, DOI 10.1145/1460096
   Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996
   Jiang Y., 2019, P INT C NEUR INF PRO, P5880
   Do K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9908, DOI 10.1109/ICCV48922.2021.00978
   Le Q., 2014, INT C MACH LEARN
   Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063
   Li YF, 2021, AAAI CONF ARTIF INTE, V35, P8547
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Madani O, 2013, MACH LEARN, V92, P457, DOI 10.1007/s10994-013-5377-0
   Mao YQ, 2021, AAAI CONF ARTIF INTE, V35, P8893
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Shapiro A, 2003, HDBK OPER R, V10, P353
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sridharan K., 2008, P ANN C LEARN THEOR, P403
   Tan JP, 2021, IEEE T MULTIMEDIA, V23, P2943, DOI 10.1109/TMM.2020.3019683
   Trigeorgis G, 2017, IEEE T PATTERN ANAL, V39, P417, DOI 10.1109/TPAMI.2016.2554555
   Trosten DJ, 2021, PROC CVPR IEEE, P1255, DOI 10.1109/CVPR46437.2021.00131
   Tsai Y. H., 2021, P INT C LEARN REPR, P1
   Turc I, 2019, Arxiv, DOI arXiv:1908.08962
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244
   von Ahn Luis., 2005, KNOWLEDGE COLLECTION, P91
   Wang QQ, 2021, IEEE T MULTIMEDIA, V23, P3483, DOI 10.1109/TMM.2020.3025666
   Wang Z, 2021, IEEE T MULTIMEDIA, V23, P1855, DOI 10.1109/TMM.2020.3003747
   Wen J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3230
   Xian Y., 2020, Adv. Neural Inf. Process. Syst, V33, P4660
   Xu C, 2013, Arxiv, DOI arXiv:1304.5634
   Xu J, 2023, IEEE T KNOWL DATA EN, V35, P7470, DOI 10.1109/TKDE.2022.3193569
   Xu J, 2022, PROC CVPR IEEE, P16030, DOI 10.1109/CVPR52688.2022.01558
   Yan XQ, 2020, INFORM FUSION, V56, P15, DOI 10.1016/j.inffus.2019.10.006
   Yang ZY, 2021, IEEE T CYBERNETICS, V51, P3249, DOI 10.1109/TCYB.2020.2984552
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhao MY, 2023, INFORM SCIENCES, V632, P324, DOI 10.1016/j.ins.2023.03.016
   Zhou L., 2018, P BRIT MACH VIS C, P1
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhukov D, 2019, PROC CVPR IEEE, P3532, DOI 10.1109/CVPR.2019.00365
NR 55
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5131
EP 5146
DI 10.1109/TMM.2023.3330093
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800007
DA 2024-08-05
ER

PT J
AU Shi, HY
   Wang, L
   Zhou, SP
   Hua, G
   Tang, W
AF Shi, Haoyue
   Wang, Le
   Zhou, Sanping
   Hua, Gang
   Tang, Wei
TI Abnormal Ratios Guided Multi-Phase Self-Training for Weakly-Supervised
   Video Anomaly Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anomaly detection; Training; Annotations; Labeling; Road accidents;
   Feature extraction; Adaptation models; weakly-supervised video anomaly
   detection; multiple instance learning
AB Weakly-supervised Video Anomaly Detection (W-VAD) aims to detect abnormal events in videos given only video-level labels for training. Recent methods relying on multiple instance learning (MIL) and self-training achieve good performance, but they tend to focus on learning easy abnormal patterns while ignoring hard ones, e.g., unusual driving trajectory or over-speeding driving. How to detect hard anomalies is a critical but largely ignored problem in W-VAD. To tackle this challenge, we propose a novel framework, termed Abnormal Ratios guided Multi-phase Self-training (ARMS), for W-VAD. It includes a new abnormal ratio-based MIL (AR-MIL) loss and a new multi-phase self-training paradigm. The AR-MIL loss guides the learning of hard anomalies by enforcing a minimum ratio of abnormal snippets in an abnormal video and no abnormal snippets in a normal video. Our multi-phase self-training paradigm sequentially performs bootstrapping, hard anomalies mining, and adaptive self-training so as to address pseudo labeling on easy anomalies, detect hard anomalies, and setting adaptive abnormal ratios for different videos in a unified framework. Experimental results on three benchmark datasets, i.e., ShanghaiTech, UCF-Crime, and XD-Violence, show that ARMS outperforms all previous state-of-the-art methods and has a great advantage in detecting hard anomalies.
C1 [Shi, Haoyue; Wang, Le; Zhou, Sanping] Xi An Jiao Tong Univ, Natl Engn Res Ctr Visual Informat & Applicat, Natl Key Lab Human Machine Hybrid Augmented Intell, Xian 710049, Shaanxi, Peoples R China.
   [Shi, Haoyue; Wang, Le; Zhou, Sanping] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Shi, Haoyue; Tang, Wei] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
   [Hua, Gang] Wormpex AI Res, Bellevue, WA 98004 USA.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; University of
   Illinois System; University of Illinois Chicago; University of Illinois
   Chicago Hospital
RP Wang, L (corresponding author), Xi An Jiao Tong Univ, Natl Engn Res Ctr Visual Informat & Applicat, Natl Key Lab Human Machine Hybrid Augmented Intell, Xian 710049, Shaanxi, Peoples R China.; Wang, L (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
EM shyern@stu.xjtu.edu.cn; lewang@mail.xjtu.edu.cn;
   spzhou@mail.xjtu.edu.cn; ganghua@gmail.com; tangw@uic.edu
OI Wang, Le/0000-0001-6636-6396
FU National Key Ramp;D Program of China
FX No Statement Available
CR Cai RC, 2021, AAAI CONF ARTIF INTE, V35, P938
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang SN, 2022, IEEE T MULTIMEDIA, V24, P4067, DOI 10.1109/TMM.2021.3112814
   Feng JC, 2021, PROC CVPR IEEE, P14004, DOI 10.1109/CVPR46437.2021.01379
   Georgescu MI, 2021, PROC CVPR IEEE, P12737, DOI 10.1109/CVPR46437.2021.01255
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Guansong Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12170, DOI 10.1109/CVPR42600.2020.01219
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Hyunjong Park, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14360, DOI 10.1109/CVPR42600.2020.01438
   Kim J., 2020, ADV NEUR IN, V33, P14567
   Kingma D. P., 2014, arXiv
   Lee Dong-Hyun, 2013, P ICML WORKSH CHALL
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li S, 2022, AAAI CONF ARTIF INTE, P1395
   Liu DY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3522714
   Liu K, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1490, DOI 10.1145/3343031.3350998
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Lv H, 2021, IEEE T IMAGE PROCESS, V30, P4505, DOI 10.1109/TIP.2021.3072863
   Nag S, 2022, LECT NOTES COMPUT SC, V13663, P663, DOI 10.1007/978-3-031-20062-5_38
   Peng Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P322, DOI 10.1007/978-3-030-58577-8_20
   Purwanto D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P173, DOI 10.1109/ICCV48922.2021.00024
   Sapkota H, 2022, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR52688.2022.00321
   Sohrab F, 2018, INT C PATT RECOG, P722, DOI 10.1109/ICPR.2018.8545819
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Tian Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4955, DOI 10.1109/ICCV48922.2021.00493
   Vaswani A., 2017, C WORKSHOP NEURAL IN, P6000
   Wan BY, 2021, IET IMAGE PROCESS, V15, P3454, DOI 10.1049/ipr2.12258
   Wan BY, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102722
   Wang J, 2019, IEEE I CONF COMP VIS, P8200, DOI 10.1109/ICCV.2019.00829
   Wu L, 2022, IEEE T IMAGE PROCESS, V31, P4803, DOI 10.1109/TIP.2022.3186746
   Wu L, 2020, IEEE T IMAGE PROCESS, V29, P1233, DOI 10.1109/TIP.2019.2940684
   Wu P, 2021, IEEE T IMAGE PROCESS, V30, P3513, DOI 10.1109/TIP.2021.3062192
   Xia K., 2023, IEEE Trans. Multimedia
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang S., 2020, P ASIAN C COMPUTER V, P124
   Ying Kaining, 2023, 2023 IEEE/CVF International Conference on Computer Vision (ICCV), P899, DOI 10.1109/ICCV51070.2023.00089
   Yu G, 2022, PROC CVPR IEEE, P13967, DOI 10.1109/CVPR52688.2022.01360
   Zaheer Muhammad Zaigham, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P358, DOI 10.1007/978-3-030-58542-6_22
   Zaheer MZ, 2022, PROC CVPR IEEE, P14724, DOI 10.1109/CVPR52688.2022.01433
   Zhai YH, 2022, IEEE T MULTIMEDIA, V24, P1857, DOI 10.1109/TMM.2021.3073235
   Zhang JG, 2019, IEEE IMAGE PROC, P4030, DOI [10.1109/ICIP.2019.8803657, 10.1109/icip.2019.8803657]
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhu Y., 2019, PROC BRIT MACH VIS C
NR 48
TC 0
Z9 0
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5575
EP 5587
DI 10.1109/TMM.2023.3336576
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600033
DA 2024-08-05
ER

PT J
AU Wang, ZB
   Hou, SH
   Zhang, M
   Liu, X
   Cao, CS
   Huang, YZ
AF Wang, Zengbin
   Hou, Saihui
   Zhang, Man
   Liu, Xu
   Cao, Chunshui
   Huang, Yongzhen
TI GaitParsing: Human Semantic Parsing for Gait Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gait recognition; human semantic parsing; partial features
AB Gait recognition is a soft biotechnology to identify pedestrians observed from different camera views based on specific walking patterns. However, various dressing and wearing conditions bring great challenges to realistic gait recognition. Most existing methods take holistic gait silhouette as input and focus on local areas through horizontal strip division or attention map. We consider that this processing may contain mixed or incomplete information about multiple body parts so that gait information is misused or underutilized. In this paper, we propose a parsing-guided framework for gait recognition, named GaitParsing, which explores human semantic parsing to dissect human body into a set of specific and complete body parts. Correspondingly, a simple yet effective dual-branch feature extraction network is adopted to process holistic gait and distinct body parts. To maximize the use of highly discriminated gait frames, we propose a self-occlusion frame assessment to measure the self-occlusion in a gait sequence. Since there is no human parsing modality in current gait datasets, we further develop a general human parsing pipeline specifically tailored for gait datasets. This single training enables widespread application across various gait datasets. Extensive experiments with ablation analyses demonstrate competitive performance even in the most challenging conditions, e.g., Cloth-Changing (CC+5.9%). Especially, It is gratifying to see that our model can be easily applied to existing methods and significantly outperform the original architecture, even without much modification.
C1 [Wang, Zengbin; Zhang, Man] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
   [Hou, Saihui; Huang, Yongzhen] Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
   [Hou, Saihui; Liu, Xu; Cao, Chunshui; Huang, Yongzhen] Watrix Technol Ltd Co Ltd, Beijing 100088, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing Normal
   University
RP Zhang, M (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
EM wzb1@bupt.edu.cn; housaihui@bnu.edu.cn; zhangman@bupt.edu.cn;
   xu.liu@watrix.ai; chunshui.cao@watrix.ai; huangyongzhen@bnu.edu.cn
OI Hou, Saihui/0000-0003-4689-2860; zhang, man/0000-0003-3043-2122; Liu,
   Xu/0000-0002-0401-1343
FU National Natural Science Foundation of China
FX No Statement Available
CR Bashir K, 2010, PATTERN RECOGN LETT, V31, P2052, DOI 10.1016/j.patrec.2010.05.027
   Chai TR, 2022, PROC CVPR IEEE, P20217, DOI 10.1109/CVPR52688.2022.01961
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Deng MQ, 2024, IEEE T MULTIMEDIA, V26, P117, DOI 10.1109/TMM.2023.3262131
   Dou HZ, 2024, IEEE T IMAGE PROCESS, V33, P1464, DOI 10.1109/TIP.2022.3164543
   Dou HZ, 2023, PROC CVPR IEEE, P5578, DOI 10.1109/CVPR52729.2023.00540
   Dou HZ, 2022, LECT NOTES COMPUT SC, V13665, P357, DOI 10.1007/978-3-031-20065-6_21
   Fan C, 2023, PROC CVPR IEEE, P9707, DOI 10.1109/CVPR52729.2023.00936
   Fu S., 2023, ICCV, P19595
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   Guo R, 2022, IEEE T MULTIMEDIA, V24, P1583, DOI 10.1109/TMM.2021.3068609
   Huang Xiaohu, 2021, P IEEECVF INT C COMP, P12909, DOI DOI 10.48550/ARXIV.2204.03270
   Huang Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14900, DOI 10.1109/ICCV48922.2021.01465
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Kusakunniran W, 2020, IET BIOMETRICS, V9, P238, DOI 10.1049/iet-bmt.2020.0103
   Li N, 2023, IEEE T MULTIMEDIA, V25, P3046, DOI 10.1109/TMM.2022.3154609
   Li N, 2023, APPL INTELL, V53, P1517, DOI 10.1007/s10489-022-03474-8
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Liao RJ, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107069
   Lin BB, 2022, Arxiv, DOI arXiv:2208.01380
   Lin BB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14628, DOI 10.1109/ICCV48922.2021.01438
   Ma K, 2023, PROC CVPR IEEE, P22076, DOI 10.1109/CVPR52729.2023.02114
   Maggie M., 2019, iMaterialist (fashion)2019 at FGVC6
   Manssor SAF, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21134323
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Paszke A, 2019, ADV NEUR IN, V32
   Pinyoanuntapong A., 2023, IEEE INT C ACOUST SP, P1
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Ren XQ, 2024, Arxiv, DOI arXiv:2207.11720
   Rida I, 2019, IET BIOMETRICS, V8, P14, DOI 10.1049/iet-bmt.2018.5063
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Saihui Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P382, DOI 10.1007/978-3-030-58545-7_22
   Shen C, 2023, PROC CVPR IEEE, P1054, DOI 10.1109/CVPR52729.2023.00108
   Shiraga K, 2016, INT CONF BIOMETR
   Sivarathinabala M, 2017, STUD COMPUT INTELL, V660, P227, DOI 10.1007/978-3-319-44790-2_11
   Song CF, 2023, IEEE T PATTERN ANAL, V45, P2801, DOI 10.1109/TPAMI.2022.3183288
   Takemura Noriko, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0039-6
   Teepe T, 2022, IEEE COMPUT SOC CONF, P1568, DOI 10.1109/CVPRW56347.2022.00163
   Teepe T, 2021, IEEE IMAGE PROC, P2314, DOI 10.1109/ICIP42928.2021.9506717
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3508, DOI 10.1109/TPAMI.2021.3055780
   Weizhi An, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P421, DOI 10.1109/TBIOM.2020.3008862
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xu K, 2021, IEEE T MULTIMEDIA, V24, P3265, DOI 10.1109/TMM.2021.3095809
   Yan Ye, 2019, 2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC), P540, DOI 10.1109/IMCEC46724.2019.8984154
   Yu SQ, 2006, INT C PATT RECOG, P441
   Zhang SX, 2021, PROC CVPR IEEE, P9091, DOI 10.1109/CVPR46437.2021.00898
   Zhang X.-P., 2023, Expert Syst., V40
   Zhang ZY, 2022, IEEE T PATTERN ANAL, V44, P345, DOI 10.1109/TPAMI.2020.2998790
   Zhang ZY, 2019, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR.2019.00484
   Zheng JK, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6136, DOI 10.1145/3503161.3547897
   Zheng JK, 2022, PROC CVPR IEEE, P20196, DOI 10.1109/CVPR52688.2022.01959
   Zhong CK, 2023, IEEE T MULTIMEDIA, V25, P7076, DOI 10.1109/TMM.2022.3217392
   Zhou S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10367, DOI 10.1109/ICCV48922.2021.01022
   ZHU K, 2020, ECCV, DOI DOI 10.1007/978-3-030-58580-8_21
   Zhu Z., 2021, ICCV, p14 789, DOI DOI 10.1109/ICCV48922.2021.01452
NR 58
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4736
EP 4748
DI 10.1109/TMM.2023.3325962
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100013
DA 2024-08-05
ER

PT J
AU Wei, JW
   Yang, Y
   Guan, X
   Xu, X
   Wang, GQ
   Shen, HT
AF Wei, Jiwei
   Yang, Yang
   Guan, Xiang
   Xu, Xing
   Wang, Guoqing
   Shen, Heng Tao
TI Runge-Kutta Guided Feature Augmentation for Few-Sample Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Runge-Kutta method; feature augmentation; few-sample learning
ID DYNAMICAL-SYSTEMS; IMAGE; NETWORK
AB Deep Neural Networks (DNNs) have primarily been demonstrated to be successful when large-scale labeled data are available. However, DNNs usually fail when tasked in few-sample learning scenarios, and the results will be much worse when the limited data show large intra-class variation and inter-class similarity (a.k.a fine-grained classification). To solve this challenging task, the idea of carrying out feature augmentation is visited and better achieved by exploring the merit of the forward Euler method in solving ordinary differential equations (ODEs), and a novel high-order feature augmentation (HFA) model with ResNet is proposed. Specifically, the proposed method leverages the stacked residual structure to model the direction of feature change over the initial state, and uses the triplet loss as constraint to model the step size of change in an adaptive manner. As a result, the initial features can then be augmented by a residual structure with a forward Eulerian form to generate features of the same subcategory with a similar representation as the input image. Furthermore, the proposed augmentation mechanism enjoys two additional benefits: a) it can help avoid the over-fitting issue when learned with insufficient training data; b) it can be used seamlessly with any residual structure-based classification network, and the ResNet used in this paper remains unchanged during testing. Extensive experiments are carried out on fine-grained visual categorization benchmarks, and the results demonstrate that our approach can significantly improve the categorization performance when the training data is highly insufficient.
C1 [Wei, Jiwei; Yang, Yang; Xu, Xing; Wang, Guoqing; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.
   [Wei, Jiwei; Yang, Yang; Xu, Xing; Wang, Guoqing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Guan, Xiang] Huawei Technol Co Ltd, Shenzhen 518129, Peoples R China.
   [Shen, Heng Tao] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Huawei Technologies; Peng
   Cheng Laboratory
RP Yang, Y (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.; Yang, Y (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM dlyyang@gmail.com
RI Shen, Heng Tao/ABD-5331-2021
OI Wei, Jiwei/0000-0003-3912-1742
FU National Natural Science Foundation of China
FX No Statement Available
CR Balduzzi D, 2017, PR MACH LEARN RES, V70
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Chen RTQ, 2018, 32 C NEURAL INFORM P, V31
   Chen T. Q., PROC ANN C NEURAL IN
   Chen TI, 2023, IEEE T MULTIMEDIA, V25, P291, DOI 10.1109/TMM.2021.3125195
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Chen ZT, 2019, IEEE T IMAGE PROCESS, V28, P4594, DOI 10.1109/TIP.2019.2910052
   Deng HQ, 2021, AAAI CONF ARTIF INTE, V35, P11462
   Gebru T, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1877, DOI 10.1145/3025453.3025930
   Guan X., 2021, PROC 2 ACM INT C MUL, P1
   Guan X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5011, DOI 10.1145/3474085.3475184
   Guan X, 2022, IEEE T NEUR NET LEAR, V33, P1507, DOI 10.1109/TNNLS.2020.3042537
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1372, DOI 10.1145/3240508.3240557
   Hu T, 2019, Arxiv, DOI arXiv:1901.09891
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Huang HX, 2019, IEEE INT CON MULTI, P91, DOI 10.1109/ICME.2019.00024
   Iserles A., 1996, The first course in the numerical analysis of differential equations
   Jiwei Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13002, DOI 10.1109/CVPR42600.2020.01302
   Khosla A., 2011, PROC 1 WORKSHOP FINE, P125
   Li QX, 2018, PR MACH LEARN RES, V80
   Li QX, 2018, J MACH LEARN RES, V18
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Lu YP, 2018, PR MACH LEARN RES, V80
   Luo ZB, 2022, COMPLEX INTELL SYST, V8, P3395, DOI 10.1007/s40747-022-00671-3
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Muehlebach M, 2019, PR MACH LEARN RES, V97
   Orvieto A, 2019, 33 C NEURAL INFORM P, V32
   Pahde F., 2018, PROC IEEE C COMPUT V, P1
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sonoda S, 2019, J MACH LEARN RES, V20
   Su WJ, 2016, J MACH LEARN RES, V17
   Tan M, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3492221
   Vobecky A, 2021, AAAI CONF ARTIF INTE, V35, P2692
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang YJ, 1998, IEEE T NEURAL NETWOR, V9, P294, DOI 10.1109/72.661124
   Wang Z, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102130
   Wei J., 2019, PROC ACM INT C MULTI, P1
   Wei JW, 2023, IEEE T CIRC SYST VID, V33, P5271, DOI 10.1109/TCSVT.2023.3249754
   Wei JW, 2022, J VIS COMMUN IMAGE R, V88, DOI 10.1016/j.jvcir.2022.103629
   Wei J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3835, DOI 10.1145/3474085.3475451
   Wei JW, 2022, IEEE T PATTERN ANAL, V44, P6534, DOI 10.1109/TPAMI.2021.3088863
   Weinan E, 2017, COMMUN MATH STAT, V5, P1, DOI 10.1007/s40304-017-0103-z
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Yue KY, 2018, ADV NEUR IN, V31
   Zhang HG, 2023, IEEE T MULTIMEDIA, V25, P2111, DOI 10.1109/TMM.2022.3142955
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
   Zhu YX, 2020, NEURAL NETWORKS, V122, P174, DOI 10.1016/j.neunet.2019.10.009
NR 53
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7349
EP 7358
DI 10.1109/TMM.2024.3366404
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000039
DA 2024-08-05
ER

PT J
AU Xu, JY
   Yang, J
   Kimishima, F
   Taniguchi, I
   Zhou, JJ
AF Xu, Jiayao
   Yang, Jian
   Kimishima, Fuma
   Taniguchi, Ittetsu
   Zhou, Jinjia
TI Compressive Sensing Based Image Codec With Partial Pre-Calculation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Image reconstruction; Decoding; Compressed sensing;
   Codecs; Quantization (signal); Reconstruction algorithms; Compressive
   sensing; image compression; codec design; reconstruction algorithm
ID SIGNAL RECONSTRUCTION; INTRA PREDICTION; MATRIX DESIGN; CODING SYSTEM;
   ALGORITHM
AB Compressive Sensing (CS) surpasses the limitations of the sampling theorem by reducing signal dimensions during sampling. Recent works integrate measurement coding into CS to enhance the compression ratio. However, these works significantly decrease image quality, and both encoding and decoding become time-consuming. This article proposes a Compressive Sensing based Image Codec with Partial Pre-calculation (CSCP) to solve these issues. The CSCP separates the original reconstruction procedure into two parts: reconstructing the frequency domain data and the inverse calculation. Depending on the feature of the chosen deterministic sensing matrix, the complex reconstruction procedure is reduced to twice matrix-based multiplications, resulting in a low time cost. Moreover, we can further optimize the reconstruction process by moving the frequency domain data reconstruction to the encoder, referred to as the partial pre-calculation process. Then compressing the sparse data in the frequency domain. This approach has two main benefits: 1) it reduces the complexity of the decoder, and 2) it results in less degradation in quality compared to existing measurement coding methods. Additionally, this work proposes the One-Row-Two-Tables strategy for defining Huffman Coding units. This approach leverages the quantized data distribution to improve compression efficiency while maintaining low complexity. In the decoder, the sequence of operations includes Huffman decoding, dequantization, and inverse calculation. Compared to the state-of-the-art, this work decreases 22.61% bpp with 17.72% increased quality. Meanwhile, time speeds up to 649.13x on the encoder, 11.03x on the decoder, and 288.46x in total.
C1 [Xu, Jiayao; Yang, Jian; Kimishima, Fuma; Zhou, Jinjia] Hosei Univ, Grad Sch Sci & Engn, Koganei 1848584, Japan.
   [Taniguchi, Ittetsu] Osaka Univ, Suita 5650871, Japan.
C3 Hosei University; Osaka University
RP Zhou, JJ (corresponding author), Hosei Univ, Grad Sch Sci & Engn, Koganei 1848584, Japan.
EM jiayao.xu.5k@stu.hosei.ac.jp; jian.yang.4f@stu.hosei.ac.jp;
   fuma.kimishima.3c@stu.hosei.ac.jp; i-tanigu@ist.osaka-u.ac.jp;
   zhou@hosei.ac.jp
OI Yang, Jian/0000-0001-8858-4378; Taniguchi, Ittetsu/0000-0002-7843-5907
FU JSPS KAKENHI
FX No Statement Available
CR Baraniuk RG, 2017, IEEE SIGNAL PROC MAG, V34, P52, DOI 10.1109/MSP.2016.2602099
   Candes E., 2005, L1 MAGIC RECOVERY SP, V4, P14
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Chen B, 2022, IEEE T IMAGE PROCESS, V31, P5412, DOI 10.1109/TIP.2022.3195319
   Chen WJ, 2022, INT CONF ACOUST SPEE, P2460, DOI 10.1109/ICASSP43922.2022.9746648
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   Do TT, 2008, CONF REC ASILOMAR C, P581, DOI 10.1109/ACSSC.2008.5074472
   Dolatabadi HM, 2019, IEEE SIGNAL PROC LET, V26, P1501, DOI 10.1109/LSP.2019.2923843
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fardad M, 2018, IEEE T CIRCUITS-I, V65, P3349, DOI 10.1109/TCSI.2018.2803627
   Ge X, 2019, IEEE T VLSI SYST, V27, P611, DOI 10.1109/TVLSI.2018.2879884
   HOWARD PG, 1994, P IEEE, V82, P857, DOI 10.1109/5.286189
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Jiayao Xu, 2020, ICVIP 2020: Proceedings of 2020 4th International Conference on Video and Image Processing, P247, DOI 10.1145/3447450.3447489
   Jinyao Z., 2023, P DAT COMPR C, P346
   Kimishima F, 2022, LECT NOTES COMPUT SC, V13231, P247, DOI 10.1007/978-3-031-06427-2_21
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Li J, 2021, IEEE T VLSI SYST, V29, P259, DOI 10.1109/TVLSI.2020.3030906
   Li LX, 2020, IEEE T MULTIMEDIA, V22, P82, DOI 10.1109/TMM.2019.2923111
   Lotfi M, 2020, IEEE T SIGNAL PROCES, V68, P3008, DOI 10.1109/TSP.2020.2990154
   Ma JW, 2019, IEEE I CONF COMP VIS, P10222, DOI 10.1109/ICCV.2019.01032
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Moshtaghpour A, 2020, IEEE T INFORM THEORY, V66, P7253, DOI 10.1109/TIT.2020.2992852
   Obermeier R, 2019, IEEE T COMPUT IMAG, V5, P27, DOI 10.1109/TCI.2018.2884291
   Obermeier R, 2017, IEEE T COMPUT IMAG, V3, P217, DOI 10.1109/TCI.2017.2671398
   Peetakul J, 2021, IEEE ACCESS, V9, P56031, DOI 10.1109/ACCESS.2021.3068579
   Peetakul J, 2019, IEEE DATA COMPR CONF, P599, DOI 10.1109/DCC.2019.00111
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Tran Thuy T. T., 2020, IEEE INT WORKSH MULT, P1, DOI [10.1109/MMSP48831.2020.9287074, DOI 10.1109/mmsp48831.2020.9287074]
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Vaz PG, 2020, OPT EXPRESS, V28, P11666, DOI 10.1364/OE.387612
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wan RT, 2021, IEEE T MULTIMEDIA, V24, P3558, DOI 10.1109/TMM.2021.3102394
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu JY, 2022, IEEE INT SYMP CIRC S, P2978, DOI 10.1109/ISCAS48785.2022.9937930
   Xu JY, 2022, LECT NOTES COMPUT SC, V13141, P518, DOI 10.1007/978-3-030-98358-1_41
   You D., 2021, P IEEE INT C MULT EX, P1, DOI DOI 10.1109/ICME51207.2021.9428249
   You D, 2021, IEEE T IMAGE PROCESS, V30, P6066, DOI 10.1109/TIP.2021.3091834
   Yu NY, 2018, IEEE T INF FOREN SEC, V13, P1722, DOI 10.1109/TIFS.2018.2800726
   Yu WK, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19235135
   Yu WK, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194122
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang J, 2023, IEEE SIGNAL PROC MAG, V40, P58, DOI 10.1109/MSP.2022.3208394
   Zhang YX, 2019, MEAS SCI TECHNOL, V30, DOI 10.1088/1361-6501/aaf4e7
   Zhou JB, 2017, IEICE T FUND ELECTR, VE100A, P2869, DOI 10.1587/transfun.E100.A.2869
NR 45
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4871
EP 4883
DI 10.1109/TMM.2023.3327534
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600020
DA 2024-08-05
ER

PT J
AU Xu, ZY
   Jiang, XQ
   Gao, XY
   Gao, R
   Gu, CJ
   Zhang, QN
   Li, WS
   Gao, XB
AF Xu, Zongyi
   Jiang, Xinqi
   Gao, Xinyu
   Gao, Rui
   Gu, Changjun
   Zhang, Qianni
   Li, Weisheng
   Gao, Xinbo
TI IGReg: Image-Geometry-Assisted Point Cloud Registration via Selective
   Correlation Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Feature extraction; Correlation;
   Three-dimensional displays; Reliability; Geometry; Iterative methods;
   Low-geometry area; multimodal point cloud registration; repetitive
   patterns
AB Point cloud registration suffers from repeated patterns and low geometric structures in indoor scenes. The recent transformer utilises attention mechanism to capture the global correlations in feature space and improves the registration performance. However, for indoor scenarios, global correlation loses its advantages as it cannot distinguish real useful features and noise. To address this problem, we propose an image-geometry-assisted point cloud registration method by integrating image information into point features and selectively fusing the geometric consistency with respect to reliable salient areas. Firstly, an Intra-Image-Geometry fusion module is proposed to integrate the texture and structure information into the point feature space by the cross-attention mechanism. Initial corresponding superpoints are acquired as salient anchors in the source and target. Then, a selective correlation fusion module is designed to embed the correlations between the salient anchors and points. During training, the saliency location and selective correlation fusion modules exchange information iteratively to identify the most reliable salient anchors and achieve effective feature fusion. The obtained distinctive point cloud features allow for accurate correspondence matching, leading to the success of indoor point cloud registration. Extensive experiments are conducted on 3DMatch and 3DLoMatch datasets to demonstrate the outstanding performance of the proposed approach compared to the state-of-the-art, particularly in those geometrically challenging cases such as repetitive patterns and low-geometry regions.
C1 [Xu, Zongyi; Jiang, Xinqi; Gao, Xinyu; Gao, Rui; Gu, Changjun; Li, Weisheng; Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Zhang, Qianni] Queen Mary Univ London, Dept Elect Engn & Comp Sci, London E1 4NS, England.
C3 Chongqing University of Posts & Telecommunications; University of
   London; Queen Mary University London
RP Gao, XB (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
EM xuzy@cqupt.edu.cn; gaoxb@cqupt.edu.cn
OI Zhang, Qianni/0000-0001-7685-2187
FU National Natural Science Foundation of China
FX No Statement Available
CR Ao S, 2021, PROC CVPR IEEE, P11748, DOI 10.1109/CVPR46437.2021.01158
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Bai XY, 2021, PROC CVPR IEEE, P15854, DOI 10.1109/CVPR46437.2021.01560
   Bai XY, 2020, PROC CVPR IEEE, P6358, DOI 10.1109/CVPR42600.2020.00639
   BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Campbell D, 2015, IEEE I CONF COMP VIS, P4292, DOI 10.1109/ICCV.2015.488
   Chen H., 2022, PROC ACM SIGGRAPH C, P1
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Deng HW, 2019, PROC CVPR IEEE, P3239, DOI 10.1109/CVPR.2019.00336
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Gu CJ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3039641
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Huang XS, 2022, IEEE ROBOT AUTOM LET, V7, P12323, DOI 10.1109/LRA.2022.3214789
   Jauer P, 2019, IEEE T PATTERN ANAL, V41, P1102, DOI 10.1109/TPAMI.2018.2831670
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Li Jiahao, 2020, PROC EUR C COMPUT VI, P378, DOI [DOI 10.1007/978-3-030-58586-0_23, 10.1007/978-3-030-58586-0_23]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu WX, 2019, IEEE I CONF COMP VIS, P12, DOI 10.1109/ICCV.2019.00010
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Nguyen AD, 2024, IEEE T NEUR NET LEAR, V35, P6613, DOI 10.1109/TNNLS.2022.3211929
   Paszke A., 2017, PROC NIPSAUTODIFF WO
   Pavlov AL, 2018, IEEE INT CONF ROBOT, P3407
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Rusinkiewicz S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323037
   Saleh M, 2020, INT CONF 3D VISION, P241, DOI 10.1109/3DV50981.2020.00034
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sarode V, 2019, Arxiv, DOI arXiv:1908.07906
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HP, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1630, DOI 10.1145/3503161.3548023
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang YJ, 2023, IEEE T PATTERN ANAL, V45, P1135, DOI 10.1109/TPAMI.2022.3148308
   Wu Q., 2023, IEEE Trans. Multimedia, early access, DOI [10.1109/TMM.2023.3283881, DOI 10.1109/TMM.2023.3283881]
   Xiaoshui Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11363, DOI 10.1109/CVPR42600.2020.01138
   Xu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3112, DOI 10.1109/ICCV48922.2021.00312
   Xu ZY, 2018, MULTIMEDIA SYST, V24, P257, DOI 10.1007/s00530-017-0541-1
   Yang H, 2021, IEEE T ROBOT, V37, P314, DOI 10.1109/TRO.2020.3033695
   Yew ZJ, 2018, LECT NOTES COMPUT SC, V11219, P630, DOI 10.1007/978-3-030-01267-0_37
   Yu H, 2021, ADV NEUR IN, V34
   Yu YG, 2023, IEEE T MULTIMEDIA, V25, P2993, DOI 10.1109/TMM.2022.3154160
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhang J, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107952
   Zhang JY, 2022, IEEE T PATTERN ANAL, V44, P3450, DOI 10.1109/TPAMI.2021.3054619
   Zhang Y, 2022, LECT NOTES COMPUT SC, V13670, P443, DOI 10.1007/978-3-031-20080-9_26
   Zhao L, 2023, IEEE T CIRC SYST VID, V33, P1854, DOI 10.1109/TCSVT.2022.3218076
   Zheng YC, 2022, IEEE T INTELL TRANSP, V23, P22312, DOI 10.1109/TITS.2022.3153133
NR 56
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7475
EP 7489
DI 10.1109/TMM.2024.3368913
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000007
DA 2024-08-05
ER

PT J
AU Yang, L
   Wang, RD
   Xu, DW
   Dong, L
   He, SH
AF Yang, Lin
   Wang, Rangding
   Xu, Dawen
   Dong, Li
   He, Songhan
TI Centralized Error Distribution-Preserving Adaptive Steganography for
   HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Centralized error; CER distribution preserving; distortion drift; HEVC
   steganography
ID DATA HIDING ALGORITHM; VIDEO STEGANOGRAPHY; H.264/AVC VIDEO;
   STEGANALYSIS; CNN
AB Distortion compensation method is a common way to cope with the distortion drift problem in coefficient domain HEVC steganography. However, it will leave obvious steganographic traces called centralized error (CER). The current coefficient domain HEVC steganography is fragile to CER-based steganalysis. In this article, a novel adaptive HEVC steganography that can resist CER-based steganalysis is proposed. First, the difference of CER between H.264/AVC and HEVC is introduced, and the CER feature in HEVC is re-modeled. Then, from two aspects of overall average distribution and single-frame distribution, we conclude that there is a strong correlation among four components of the CER feature. Last, an adaptive cost function is proposed by maintaining one component distribution to resist steganalysis. Experimental results show that the proposed cost function can effectively improve the security compared with other coefficient-based HEVC steganography. In addition, the proposed steganography outperforms other HEVC steganography in visual quality and bit rate increase.
C1 [Yang, Lin; Wang, Rangding; Dong, Li; He, Songhan] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315211, Peoples R China.
   [Xu, Dawen] Ningbo Univ Technol, Sch Cyber Sci & Engn, Ningbo 315211, Peoples R China.
C3 Ningbo University; Ningbo University of Technology
RP Wang, RD (corresponding author), Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315211, Peoples R China.; Xu, DW (corresponding author), Ningbo Univ Technol, Sch Cyber Sci & Engn, Ningbo 315211, Peoples R China.
EM 2011082340@nbu.edu.cn; wangrangding@nbu.edu.cn; dawenxu@126.com;
   dongli@nbu.edu.cn; 2111082349@nbu.edu.cn
RI he, songhan/KVY-0154-2024
OI Dong, Li/0000-0003-2002-8249; xu, dawen/0000-0002-9619-8407
FU National Natural Science Foundation of China
FX No Statement Available
CR Cao Y, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P23, DOI 10.1145/3206004.3206014
   Chang PC, 2014, J VIS COMMUN IMAGE R, V25, P239, DOI 10.1016/j.jvcir.2013.10.007
   Chen YL, 2021, IEEE T DEPEND SECURE, V18, P1320, DOI 10.1109/TDSC.2019.2932983
   Chen Y, 2022, IEEE T DEPEND SECURE, V19, P2405, DOI 10.1109/TDSC.2021.3058134
   Cui Y., 2020, PROC IEEE 22 INT WOR, P1
   Dong Y, 2023, IEEE T DEPEND SECURE, V20, P769, DOI 10.1109/TDSC.2022.3144139
   Dong Y, 2023, IEEE T MULTIMEDIA, V25, P2698, DOI 10.1109/TMM.2022.3150180
   Dong Y, 2019, LECT NOTES COMPUT SC, V11378, P233, DOI 10.1007/978-3-030-11389-6_18
   Dong Y, 2017, LECT NOTES COMPUT SC, V10431, P149, DOI 10.1007/978-3-319-64185-0_12
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Ghasempour M, 2020, IEEE T CIRC SYST VID, V30, P4009, DOI 10.1109/TCSVT.2019.2947545
   He SH, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103549
   Holub V, 2015, IEEE T INF FOREN SEC, V10, P219, DOI 10.1109/TIFS.2014.2364918
   Jia XQ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2720, DOI 10.1109/ICASSP39728.2021.9413728
   Jiang B., 2015, J. Comput. Inf. Syst., V11, P2121
   Li ZH, 2023, IEEE T DEPEND SECURE, V20, P606, DOI 10.1109/TDSC.2022.3140899
   Liu JD, 2022, IEEE T MULTIMEDIA, V24, P2084, DOI 10.1109/TMM.2021.3075858
   Liu SW, 2021, IEEE SIGNAL PROC LET, V28, P1843, DOI 10.1109/LSP.2021.3111565
   Liu Y, 2022, IEEE T CIRC SYST VID, V32, P4905, DOI 10.1109/TCSVT.2021.3135384
   Liu YX, 2019, MULTIMED TOOLS APPL, V78, P6459, DOI 10.1007/s11042-018-6320-y
   Ma XJ, 2010, IEEE T CIRC SYST VID, V20, P1320, DOI 10.1109/TCSVT.2010.2070950
   Shi HN, 2021, INT J DIGIT CRIME FO, V13, P19, DOI 10.4018/IJDCF.20210501.oa2
   Si Liu, 2021, Intelligent Computing Theories and Application: 17th International Conference, ICIC 2021, Proceedings. Lecture Notes in Computer Science, Information Systems and Applications, incl. Internet/Web, and HCI (12836), P327, DOI 10.1007/978-3-030-84522-3_26
   Wang J, 2019, IEEE ACCESS, V7, P119393, DOI 10.1109/ACCESS.2019.2936614
   Wang PP, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P123, DOI 10.1145/3082031.3083245
   Wang Y., 2017, PROC 12 INT C INSCRY, P472
   Wang Y, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P97, DOI 10.1145/3206004.3206020
   Xue YM, 2019, SIGNAL PROCESS-IMAGE, V76, P22, DOI 10.1016/j.image.2019.04.012
   Yang J, 2018, MULTIMED TOOLS APPL, V77, P11979, DOI 10.1007/s11042-017-4844-1
   Yang L., 2022, P INT WORKSH DIG FOR, P20
   Yang L, 2023, J INF SECUR APPL, V73, DOI 10.1016/j.jisa.2023.103442
   Yang YY, 2019, MULTIMED TOOLS APPL, V78, P8423, DOI 10.1007/s11042-018-6859-7
   Yao YZ, 2015, MULTIMED TOOLS APPL, V74, P11163, DOI 10.1007/s11042-014-2223-8
   Zhai LM, 2020, IEEE T INF FOREN SEC, V15, P1762, DOI 10.1109/TIFS.2019.2949428
   Zhao Y., 2015, INT WORKSH DIG FOR W, P25
   Zhou AJ, 2021, 2021 14TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2021), DOI 10.1109/CISP-BMEI53629.2021.9624353
   Zhou AJ, 2021, IEEE INT CONF TRUST, P967, DOI 10.1109/TrustCom53373.2021.00135
NR 38
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4255
EP 4270
DI 10.1109/TMM.2023.3321496
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100006
DA 2024-08-05
ER

PT J
AU Zhang, RH
   Yu, JZ
   Chen, JZ
   Li, GF
   Lin, L
   Wang, DW
AF Zhang, Ronghui
   Yu, Jiongze
   Chen, Junzhou
   Li, Guofa
   Lin, Liang
   Wang, Danwei
TI A Prior Guided Wavelet-Spatial Dual Attention Transformer Framework for
   Heavy Rain Image Restoration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Rain; Transformers; Wavelet transforms; Task analysis; Feature
   extraction; Image restoration; Visualization; Heavy rain; image
   deraining; transformer; wavelet attention; spatial attention
ID REMOVAL; MODEL
AB Heavy rain significantly reduces image visibility, hindering tasks like autonomous driving and video surveillance. Many existing rain removal methods, while effective in light rain, falter under heavy rain due to their reliance on purely spatial features. Recognizing this challenge, we introduce the Wavelet-Spatial Dual Attention Transformer Framework (WSDformer). This innovative architecture adeptly captures both frequency and spatial characteristics, anchored by the wavelet-spatial dual attention (WSDA) mechanism. While the spatial attention zeroes in on intricate local details, the wavelet attention leverages wavelet decomposition to encompass diverse frequency information, augmenting the spatial representations. Furthermore, addressing the persistent issue of incomplete structural detail restoration, we integrate the PriorFormer Block (PFB). This unique module, underpinned by the Prior Fusion Attention (PFA), synergizes residual channel prior features with input features, thereby enhancing background structures and guiding precise rain feature extraction. To navigate the intrinsic constraints of U-shaped transformers, such as semantic discontinuities and subdued multi-scale interactions from skip connections, our Cross Interaction U-Shaped Transformer Network is introduced. This design empowers superior semantic layers to streamline the extraction of their lower-tier counterparts, optimizing network learning. Empirical analysis reveals our method's leading prowess across rainy image datasets and achieves state-of-the-art performance, with notable supremacy in heavy rainfall conditions. This superiority extends to diverse visual challenges and real-world rainy scenarios, affirming its broad applicability and robustness.
C1 [Zhang, Ronghui; Yu, Jiongze; Chen, Junzhou] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangdong Prov Key Lab Intelligent Transport Syst, Guangzhou 510275, Peoples R China.
   [Li, Guofa] Chongqing Univ, Coll Mech & Vehicle Engn, Chongqing 400044, Peoples R China.
   [Lin, Liang] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
   [Wang, Danwei] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Sun Yat Sen University; Chongqing University; Sun Yat Sen University;
   Nanyang Technological University
RP Chen, JZ (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangdong Prov Key Lab Intelligent Transport Syst, Guangzhou 510275, Peoples R China.
EM zhangrh25@mail.sysu.edu.cn; yujz3@mail2.sysu.edu.cn;
   chenjunzhou@mail.sysu.edu.cn; liguofa@cqu.edu.cn; linliang@ieee.org;
   edwwang@ntu.edu.sg
RI ; Liang, Lin/IQR-8601-2023; Li, Guofa/Q-1176-2016
OI CHEN, Junzhou/0000-0002-3388-3503; Zhang, Ronghui/0000-0001-6107-4044;
   Wang, Danwei/0000-0003-3400-0079; Liang, Lin/0000-0003-2248-3755; Li,
   Guofa/0000-0002-7889-4695
FU National Natural Science Foundation of China
FX No Statement Available
CR Agbodike O, 2023, IEEE ACCESS, V11, P99470, DOI 10.1109/ACCESS.2023.3313946
   Bae W, 2017, IEEE COMPUT SOC CONF, P1141, DOI 10.1109/CVPRW.2017.152
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen X, 2023, PROC CVPR IEEE, P5896, DOI 10.1109/CVPR52729.2023.00571
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Frants V, 2024, IEEE T MULTIMEDIA, V26, P789, DOI 10.1109/TMM.2023.3271829
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Guo TT, 2017, IEEE COMPUT SOC CONF, P1100, DOI 10.1109/CVPRW.2017.148
   He ZY, 2022, IEEE T MULTIMEDIA, V24, P2877, DOI 10.1109/TMM.2021.3090166
   Huang HB, 2021, INT J COMPUT VISION, V129, P1282, DOI 10.1007/s11263-020-01421-z
   Jiang Kui, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P7065, DOI 10.1145/3581783.3611697
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Li QF, 2021, IEEE T IMAGE PROCESS, V30, P7074, DOI 10.1109/TIP.2021.3101395
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin X, 2021, IEEE T MULTIMEDIA, V23, P664, DOI 10.1109/TMM.2020.2987703
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma HC, 2020, IEEE T MULTIMEDIA, V22, P1667, DOI 10.1109/TMM.2019.2957990
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Pan Z., 2022, Adv. Neural Inf. Process. Syst, V35, P14541
   Qiu YW, 2023, IEEE I CONF COMP VIS, P12756, DOI 10.1109/ICCV51070.2023.01176
   Que Y, 2021, IEEE T MULTIMEDIA, V23, P3059, DOI 10.1109/TMM.2020.3019680
   Wang YL, 2021, IEEE T MULTIMEDIA, V23, P2481, DOI 10.1109/TMM.2020.3013383
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Xiao J, 2023, IEEE T PATTERN ANAL, V45, P12978, DOI 10.1109/TPAMI.2022.3183612
   Xu JZ, 2023, IEEE T MULTIMEDIA, V25, P6258, DOI 10.1109/TMM.2022.3207330
   Yang HH, 2020, IEEE IMAGE PROC, P883, DOI [10.1109/ICIP40778.2020.9190720, 10.1109/icip40778.2020.9190720]
   Yang WH, 2021, IEEE T PATTERN ANAL, V43, P4059, DOI 10.1109/TPAMI.2020.2995190
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Yang WH, 2019, IEEE T IMAGE PROCESS, V28, P2948, DOI 10.1109/TIP.2019.2892685
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yang Y, 2022, IEEE T MULTIMEDIA, V24, P1622, DOI 10.1109/TMM.2021.3068833
   Yao T, 2022, LECT NOTES COMPUT SC, V13685, P328, DOI 10.1007/978-3-031-19806-9_19
   Yu L, 2023, IEEE T MULTIMEDIA, V25, P443, DOI 10.1109/TMM.2021.3127360
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
   Zou WB, 2023, IEEE T MULTIMEDIA, V25, P4623, DOI 10.1109/TMM.2022.3179926
NR 44
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7043
EP 7057
DI 10.1109/TMM.2024.3359480
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000032
DA 2024-08-05
ER

PT J
AU Zhang, Z
   Wang, HF
   Geng, J
   Deng, XY
   Jiang, W
AF Zhang, Zhuo
   Wang, Hongfei
   Geng, Jie
   Deng, Xinyang
   Jiang, Wen
TI A New Data Augmentation Method Based on Mixup and Dempster-Shafer Theory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dempster-Shafer theory; uncertainty; mixup; data augmentation; deep
   neural network
ID NETWORK; COMBINATION; VALUES
AB To improve the performance of deep neural networks, the Mixup method has been proposed to alleviate their memorization issues and sensitivity to adversarial samples. This provides networks with better generalization abilities. The learning principle of Mixup is essentially to train deep neural networks for regularization tasks with a convex combination of the original feature vectors and their labels. However, soft labels are generated directly using the mixing ratio without dealing with the uncertain information generated during the mixing process. Therefore, this paper proposes a new data augmentation method based on Mixup and Dempster-Shafer theory called DS-Mixup, which is a regularizer that can express and deal with the uncertainty caused by ambiguity. This method uses interval numbers to generate mass functions of mixed samples to model the distribution of set-valued random variables; then, ambiguous decision spaces are constructed, and soft labels with single-element subsets and multielement subsets are generated to further improve the delineation of decision boundaries during the training process. In addition, an evidence neural network with DS-Mixup is designed in this paper to accomplish recognition or classification tasks. Experimental results obtained on multimedia datasets, including attribute, image, text and signal data, show that the proposed method achieves more effective data augmentation effects and further improves the performance of deep neural networks.
C1 [Zhang, Zhuo; Wang, Hongfei; Geng, Jie; Deng, Xinyang; Jiang, Wen] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
C3 Northwestern Polytechnical University
RP Jiang, W (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
EM zhangzhuonwpu@126.com; wanghongfeinwpu@126.com; gengjie@nwpu.edu.cn;
   xinyang.deng@nwpu.edu.cn; jiangwen@nwpu.edu.cn
RI zhang, min/IYI-9869-2023
OI jiang, wen/0000-0001-5429-2748
FU National Natural Science Foundation of China
FX No Statement Available
CR Bacchi S, 2019, STROKE, V50, P758, DOI 10.1161/STROKEAHA.118.024124
   Biswas B, 2020, APPL SOFT COMPUT, V86, DOI 10.1016/j.asoc.2019.105889
   Chapelle J., 2000, ANN C NEURAL INF PRO, P1
   Cheng RL, 2022, IEEE T FUZZY SYST, V30, P5196, DOI 10.1109/TFUZZ.2022.3170208
   DEMPSTER AP, 1967, ANN MATH STAT, V38, P325, DOI 10.1214/aoms/1177698950
   Deng XY, 2021, INFORM SCIENCES, V580, P398, DOI 10.1016/j.ins.2021.08.083
   Deng Y. Cui, An ECR-PCR rule for fusion of evidencesdefined on a non-exclusive framework of discernment
   Deng Y, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-020-3006-9
   Denoeux T, 2021, INFORM SCIENCES, V572, P297, DOI 10.1016/j.ins.2021.05.011
   Denoeux T, 2021, INFORM FUSION, V65, P179, DOI 10.1016/j.inffus.2020.09.001
   DeVries T., 2017, ARXIV
   Dezert J, 2020, INT J INTELL SYST, V35, P1105, DOI 10.1002/int.22236
   Evermann J, 2017, DECIS SUPPORT SYST, V100, P129, DOI 10.1016/j.dss.2017.04.003
   Fan XJ, 2022, CHINESE J AERONAUT, V35, P179, DOI 10.1016/j.cja.2021.08.003
   Fernandes FE Jr, 2019, SWARM EVOL COMPUT, V49, P62, DOI 10.1016/j.swevo.2019.05.010
   Ferson Scott., 2015, Dependence in probabilistic modeling dempster-shafer theory and probability bounds analysis
   Gao L, 2022, IEEE T MULTIMEDIA, V24, P1503, DOI 10.1109/TMM.2021.3066118
   Gao L, 2020, IEEE SIGNAL PROC LET, V27, P1894, DOI 10.1109/LSP.2020.3028006
   Gao XZA, 2022, ENG APPL ARTIF INTEL, V108, DOI 10.1016/j.engappai.2021.104584
   Geng J, 2020, ISPRS J PHOTOGRAMM, V167, P201, DOI 10.1016/j.isprsjprs.2020.07.007
   He HS, 2020, KNOWL-BASED SYST, V193, DOI 10.1016/j.knosys.2019.105426
   He HS, 2017, CHIN CONTR CONF, P5496, DOI 10.23919/ChiCC.2017.8028229
   Hong MN, 2021, PROC CVPR IEEE, P14857, DOI 10.1109/CVPR46437.2021.01462
   Jiang W, 2020, IEEE T FUZZY SYST, V28, P1585, DOI 10.1109/TFUZZ.2019.2918999
   Jiang W, 2018, INT J APPROX REASON, V103, P94, DOI 10.1016/j.ijar.2018.09.001
   Jiang W, 2017, APPL SOFT COMPUT, V57, P672, DOI 10.1016/j.asoc.2017.04.008
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Jing MM, 2021, IEEE T CYBERNETICS, V51, P3390, DOI 10.1109/TCYB.2020.2974106
   Kang BY, 2020, J AMB INTEL HUM COMP, V11, P2041, DOI 10.1007/s12652-019-01228-y
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041
   Kim Yoon, 2014, EMNLP, P1746, DOI 10.3115/v1/D14-1181
   Le N, 2022, ARTIF INTELL REV, V55, P2733, DOI 10.1007/s10462-021-10061-9
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li JJ, 2021, IEEE T KNOWL DATA EN, V33, P194, DOI 10.1109/TKDE.2019.2924656
   Li Q, 2023, IEEE T MULTIMEDIA, V25, P3420, DOI 10.1109/TMM.2022.3160589
   Liu F, 2021, IEEE T FUZZY SYST, V29, P986, DOI 10.1109/TFUZZ.2020.2966182
   Liu K, 2021, NEUROCOMPUTING, V466, P190, DOI 10.1016/j.neucom.2021.09.034
   Liu ZG, 2021, IEEE T SYST MAN CY-S, V51, P5129, DOI 10.1109/TSMC.2019.2945808
   Miao W, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3140485
   Moore RE., 1979, Methods and Applications of Interval Analysis
   Otter DW, 2021, IEEE T NEUR NET LEAR, V32, P604, DOI 10.1109/TNNLS.2020.2979670
   Panagakis Y, 2021, P IEEE, V109, P863, DOI 10.1109/JPROC.2021.3074329
   Peñafiel S, 2020, EXPERT SYST APPL, V148, DOI 10.1016/j.eswa.2020.113262
   Rodríguez JJ, 2020, EXPERT SYST APPL, V151, DOI 10.1016/j.eswa.2020.113376
   Sentz K., 2002, Tech. Rep. SAND2002-0835
   Shafer G., 1976, A mathematical theory of evidence
   Shang QY, 2022, IEEE T SYST MAN CY-S, V52, P5602, DOI 10.1109/TSMC.2021.3130187
   Shin W, 2022, COMPUT IND ENG, V167, DOI 10.1016/j.cie.2022.107996
   Simard PY, 2000, INT J IMAG SYST TECH, V11, P181, DOI 10.1002/1098-1098(2000)11:3<181::AID-IMA1003>3.0.CO;2-E
   Singh R, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108111
   Smets P., 1994, ADV DEMPSTER SHAFER, P5
   Szegedy C., 2014, P 2 INT C LEARN REPR, P1
   Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640
   Verma V, 2019, PR MACH LEARN RES, V97
   Verma V, 2022, NEURAL NETWORKS, V145, P90, DOI 10.1016/j.neunet.2021.10.008
   Xiao FY, 2021, IEEE T NEUR NET LEAR, V32, P1525, DOI 10.1109/TNNLS.2020.2984918
   Xiao FY, 2021, IEEE T SYST MAN CY-S, V51, P3980, DOI 10.1109/TSMC.2019.2958635
   Xiong LH, 2021, INFORM SCIENCES, V580, P408, DOI 10.1016/j.ins.2021.08.088
   Yager RR, 2022, INFORM FUSION, V78, P86, DOI 10.1016/j.inffus.2021.07.020
   Yager RR, 2021, ENG APPL ARTIF INTEL, V101, DOI 10.1016/j.engappai.2021.104201
   Yager RR, 2008, STUD FUZZ SOFT COMP, V219, P291
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang CQ, 2022, IEEE T PATTERN ANAL, V44, P2402, DOI 10.1109/TPAMI.2020.3037734
   Zhang CQ, 2020, IEEE T CYBERNETICS, V50, P2837, DOI 10.1109/TCYB.2019.2894985
   Zhang CY, 2021, COMMUN ACM, V64, P107, DOI 10.1145/3446776
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang XG, 2022, KNOWL-BASED SYST, V243, DOI 10.1016/j.knosys.2022.108418
   Zhang XG, 2017, RELIAB ENG SYST SAFE, V162, P111, DOI 10.1016/j.ress.2017.01.009
   Zhang Z, 2022, ENG APPL ARTIF INTEL, V109, DOI 10.1016/j.engappai.2021.104610
NR 69
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4998
EP 5013
DI 10.1109/TMM.2023.3330106
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800006
DA 2024-08-05
ER

PT J
AU Zhong, YH
   Zhang, CX
   Yang, X
   Wang, SS
AF Zhong, Yuanhong
   Zhang, Chenxu
   Yang, Xun
   Wang, Shanshan
TI Video Compressed Sensing Reconstruction via an Untrained Network with
   Low-Rank Regularization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Video sequences; Electronics packaging;
   Correlation; Compressed sensing; Training; Loss measurement; Deep image
   prior; latent space and data space; low-rank regularization;
   spatiotemporal correlation; Video compressed sensing
ID REPRESENTATION; RECOVERY; SPARSITY; IMAGES; MODELS
AB Deep image prior (DIP) is an emerging technology that indicates that the structure of an untrained network can serve as an excellent prior for image restoration. It bridges the gap between training-based and training-free methods and exhibits considerable potential in image compressed sensing (CS) reconstruction. In this article, we extend DIP and propose a novel Low-Rank Regularization Video Compressed Sensing Network for CS video reconstruction (dubbed LRR-VCSNet). We explore the application of a low-rank latent tensor with an untrained network for global low-rank regularization on video reconstruction, and the interframe low-rank approximation for framewise nonlocal low-rank regularization in the data space is also exploited. In addition, we design the structure of the untrained network based on the encoder-decoder architecture to improve the performance. Extensive experiments on six standard CIF video sequences show that LLR-VCSNet significantly outperforms traditional video CS methods and achieves competitive results when compared with the state-of-the-art training-based video CS method.
C1 [Zhong, Yuanhong; Zhang, Chenxu] Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400030, Peoples R China.
   [Yang, Xun] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Wang, Shanshan] Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230039, Peoples R China.
C3 Chongqing University; Chinese Academy of Sciences; University of Science
   & Technology of China, CAS; Anhui University
RP Zhong, YH (corresponding author), Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400030, Peoples R China.
EM zhongyh@cqu.edu.cn; zhangchenxu@cqu.edu.cn; xyang21@ustc.edu.cn;
   wang.shanshan@ahu.edu.cn
OI Wang, Shanshan/0000-0002-3824-687X; YANG, Xun/0000-0003-0201-1638;
   Zhang, Chenxu/0000-0002-7079-7284; Zhong, Yuanhong/0000-0001-5689-1146
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2006, P PICT COD S
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chen B, 2022, IEEE T IMAGE PROCESS, V31, P5412, DOI 10.1109/TIP.2022.3195319
   Chen Z, 2018, IEEE T MULTIMEDIA, V20, P1610, DOI 10.1109/TMM.2017.2774004
   Chi L., 2020, ADV NEURAL INF PROCE, V33, P4479, DOI DOI 10.5555/3495724.3496100
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fowler JE, 2010, FOUND TRENDS SIGNAL, V4, P297, DOI 10.1561/2000000033
   Hadizadeh H, 2021, IEEE T MULTIMEDIA, V23, P12, DOI 10.1109/TMM.2020.2975420
   He LH, 2009, IEEE T SIGNAL PROCES, V57, P3488, DOI 10.1109/TSP.2009.2022003
   Hyder R, 2020, IEEE T SIGNAL PROCES, V68, P1688, DOI 10.1109/TSP.2020.2977256
   Dinh KQ, 2017, IEEE T CIRC SYST VID, V27, P2294, DOI 10.1109/TCSVT.2016.2587398
   Kim YK, 2010, IEEE IMAGE PROC, P3365, DOI 10.1109/ICIP.2010.5652744
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Li LX, 2020, IEEE T MULTIMEDIA, V22, P82, DOI 10.1109/TMM.2019.2923111
   Lingala SG, 2011, IEEE T MED IMAGING, V30, P1042, DOI 10.1109/TMI.2010.2100850
   Liu JM, 2019, INT CONF ACOUST SPEE, P7715, DOI 10.1109/ICASSP.2019.8682856
   Liu Y, 2013, IEEE T CIRC SYST VID, V23, P438, DOI 10.1109/TCSVT.2012.2207269
   Mun S, 2011, IEEE DATA COMPR CONF, P183, DOI 10.1109/DCC.2011.25
   Shi WZ, 2021, IEEE T CIRC SYST VID, V31, P425, DOI 10.1109/TCSVT.2020.2978703
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Song JC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4249, DOI 10.1145/3474085.3475562
   Sun YB, 2020, IEEE T MULTIMEDIA, V22, P3236, DOI 10.1109/TMM.2020.2973862
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Tramel EW, 2011, IEEE DATA COMPR CONF, P193, DOI 10.1109/DCC.2011.26
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Van Veen D, 2020, Arxiv, DOI arXiv:1806.06438
   Yang Y, 2016, 30 C NEURAL INFORM P, V29
   You D, 2021, IEEE T IMAGE PROCESS, V30, P6066, DOI 10.1109/TIP.2021.3091834
   You J. Xie, 2021, P INT C MULT EXP, P1
   Zha ZY, 2023, IEEE SIGNAL PROC MAG, V40, P32, DOI 10.1109/MSP.2022.3217936
   Zha ZY, 2021, IEEE T IMAGE PROCESS, V30, P5223, DOI 10.1109/TIP.2021.3078329
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8960, DOI 10.1109/TIP.2020.3021291
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8561, DOI 10.1109/TIP.2020.3015545
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P5094, DOI 10.1109/TIP.2020.2972109
   Zhang B, 2021, IEEE T MULTIMEDIA, V23, P2656, DOI 10.1109/TMM.2020.3014489
   Zhang J, 2023, IEEE SIGNAL PROC MAG, V40, P58, DOI 10.1109/MSP.2022.3208394
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang XY, 2018, PROC CVPR IEEE, P8232, DOI 10.1109/CVPR.2018.00859
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P1182, DOI 10.1109/TCSVT.2016.2527181
   Zheng S, 2019, IEEE T MULTIMEDIA, V21, P1905, DOI 10.1109/TMM.2019.2891415
NR 46
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4590
EP 4601
DI 10.1109/TMM.2023.3324490
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100033
DA 2024-08-05
ER

PT J
AU Huang, YH
   Juefei-Xu, F
   Guo, Q
   Pu, GG
   Liu, Y
AF Huang, Yihao
   Juefei-Xu, Felix
   Guo, Qing
   Pu, Geguang
   Liu, Yang
TI Natural & Adversarial Bokeh Rendering via Circle-of-Confusion Predictive
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adversarial attack; bokeh rendering; circle-of-confusion
AB Bokeh effect is a natural shallow depth-of-field phenomenon that blurs the out-of-focus part in photography. In recent years, a series of works have proposed automatic and realistic bokeh rendering methods for artistic and aesthetic purposes. They usually employ cutting-edge data-driven deep generative networks with complex training strategies and network architectures. However, these works neglect that the bokeh effect can inevitably affect the subsequent visual intelligent tasks like recognition, and their data-driven nature prevents them from studying the influence of bokeh-related physical parameters (i.e., depth-of-the-field) on the intelligent tasks. To fill this gap, we study a totally new problem, i.e., natural & adversarial bokeh rendering, which consists of two objectives: rendering realistic and natural bokeh and fooling the visual perception models (i.e., bokeh-based adversarial attack). Specifically, we propose the circle-of-confusion predictive network (CoCNet) by taking the all-in-focus image and depth image as inputs to estimate circle-of-confusion parameters for each pixel, which are employed to render the final image through a well-known physical model of bokeh. Moreover, we propose the adversarial bokeh attack by fixing the CoCNet while optimizing the depth map w.r.t. the visual perception tasks. Then, we are able to study the vulnerability of deep neural networks according to the depth variations in the real world. The extensive experiments show that our method produces more realistic bokeh than the state-of-the-art methods while fooling the powerful deep neural networks with a high accuracy drop.
C1 [Huang, Yihao; Liu, Yang] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Juefei-Xu, Felix] NYU, New York 10003, NY USA.
   [Guo, Qing] ASTAR, Inst High Performance Comp IHPC, Singapore 639798, Singapore.
   [Guo, Qing] ASTAR, Ctr Frontier AI Res CFAR, Singapore 639798, Singapore.
   [Pu, Geguang] East China Normal Univ, Shanghai 200241, Peoples R China.
   [Pu, Geguang] Shanghai Ind Control Safety Innovat Technol Co Ltd, Shanghai 200241, Peoples R China.
C3 Nanyang Technological University; New York University; Agency for
   Science Technology & Research (A*STAR); A*STAR - Institute of High
   Performance Computing (IHPC); Agency for Science Technology & Research
   (A*STAR); East China Normal University
RP Guo, Q (corresponding author), ASTAR, Inst High Performance Comp IHPC, Singapore 639798, Singapore.; Guo, Q (corresponding author), ASTAR, Ctr Frontier AI Res CFAR, Singapore 639798, Singapore.
EM huangyihao22@gmail.com; juefei.xu@gmail.com; tsingqguo@ieee.org;
   ggpu@sei.ecnu.edu.cn; yangliu@ntu.edu.sg
OI Guo, Qing/0000-0003-0974-9299; Huang, Yihao/0000-0002-5784-770X
FU National Key Research and Development Program
FX No Statement Available
CR Abuolaim Abdullah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P111, DOI 10.1007/978-3-030-58607-2_7
   Abuolaim A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2269, DOI 10.1109/ICCV48922.2021.00229
   Busam B, 2019, IEEE INT CONF COMP V, P3295, DOI 10.1109/ICCVW.2019.00411
   Cheng YP, 2022, IEEE T MULTIMEDIA, V24, P3807, DOI 10.1109/TMM.2021.3108009
   Dutta S, 2021, IEEE COMPUT SOC CONF, P2398, DOI 10.1109/CVPRW53098.2021.00272
   Dutta S, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103089
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Endo K, 2020, IEEE IMAGE PROC, P1691, DOI [10.1109/icip40778.2020.9191087, 10.1109/ICIP40778.2020.9191087]
   Engstrom L, 2019, PR MACH LEARN RES, V97
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Guo Qing, 2020, Advances in Neural Information Processing Systems, V33, P2
   Haeberli P., 1990, Computer Graphics, V24, P309, DOI 10.1145/97880.97913
   Thao NH, 2021, Arxiv, DOI arXiv:2106.13864
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Ignatov A, 2020, IEEE COMPUT SOC CONF, P1676, DOI 10.1109/CVPRW50498.2020.00217
   Ignatov A, 2019, IEEE INT CONF COMP V, P3591, DOI 10.1109/ICCVW.2019.00444
   Jeong Y, 2022, IEEE T VIS COMPUT GR, V28, P1373, DOI 10.1109/TVCG.2020.3014474
   Kurakin A, 2018, SPRING SER CHALLENGE, P195, DOI 10.1007/978-3-319-94042-7_11
   Lee J, 2021, PROC CVPR IEEE, P2034, DOI 10.1109/CVPR46437.2021.00207
   Lee S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778802
   Madry A., 2017, PROC INT C LEARN REP
   Meng QL, 2023, IEEE T MULTIMEDIA, V25, P9044, DOI 10.1109/TMM.2023.3244398
   Ming Qian, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P229, DOI 10.1007/978-3-030-67070-2_14
   Nagasubramaniam H., 2022, Inst. Elect. Electron. Eng., DOI [10.36227/techrxiv.17714849.v1, DOI 10.36227/TECHRXIV.17714849.V1]
   Pei YT, 2021, IEEE T PATTERN ANAL, V43, P1239, DOI 10.1109/TPAMI.2019.2950923
   Purohit K, 2019, IEEE INT CONF COMP V, P3417, DOI 10.1109/ICCVW.2019.00424
   Seizinger T., 2023, PROC IEEECVF C COMPU, P1633
   Shamsabadi AS, 2020, PROC CVPR IEEE, P1148, DOI 10.1109/CVPR42600.2020.00123
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Shen XY, 2016, COMPUT GRAPH FORUM, V35, P93, DOI 10.1111/cgf.12814
   Soler C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516529
   Tan HC, 2023, IEEE T MULTIMEDIA, V25, P8620, DOI 10.1109/TMM.2023.3238554
   Wadhwa N, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201329
   Wan C, 2023, IEEE T MULTIMEDIA, V25, P9572, DOI 10.1109/TMM.2023.3255742
   Wan RJ, 2023, IEEE T PATTERN ANAL, V45, P1424, DOI 10.1109/TPAMI.2022.3168560
   Wang LJ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275013
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZF, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103580
   Wu JZ, 2010, VISUAL COMPUT, V26, P555, DOI 10.1007/s00371-010-0459-5
   Xianrui Luo, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P245, DOI 10.1007/978-3-030-67070-2_15
   Xiao L, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214769
   Xu J, 2021, IEEE T MULTIMEDIA, V23, P2222, DOI 10.1109/TMM.2021.3070972
   Xu XY, 2018, LECT NOTES COMPUT SC, V11213, P36, DOI 10.1007/978-3-030-01240-3_3
   Yang Zhihao, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1542, DOI 10.1109/CVPRW59228.2023.00157
   Yu XA, 2010, COMPUT GRAPH FORUM, V29, P2099, DOI 10.1111/j.1467-8659.2010.01797.x
   Zhang J, 2023, IEEE T MULTIMEDIA, V25, P8988, DOI 10.1109/TMM.2023.3243659
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SH, 2023, IEEE T MULTIMEDIA, V25, P4296, DOI 10.1109/TMM.2022.3173533
   Zhang XY, 2021, IEEE T MULTIMEDIA, V24, P3240, DOI 10.1109/TMM.2021.3096009
   Zhao Z., 2020, PROC 31 BRIT MACH VI
NR 50
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5729
EP 5740
DI 10.1109/TMM.2023.3338413
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100008
DA 2024-08-05
ER

PT J
AU Lan, GP
   Xiao, S
   Yang, JC
   Zhou, YS
   Wen, JB
   Lu, W
   Gao, XB
AF Lan, Guipeng
   Xiao, Shuai
   Yang, Jiachen
   Zhou, Yanshuang
   Wen, Jiabao
   Lu, Wen
   Gao, Xinbo
TI Image Aesthetics Assessment Based on Hypernetwork of Emotion Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image aesthetics assessment; image emotion
AB Research in psychology demonstrates that visual features and semantic content can convey various emotions. Furthermore, studies have proved that image emotion and aesthetics are inextricably linked. During the image aesthetic assessment process (IAA), images elicit emotional responses from individuals, leading to emotional resonance and influencing the evaluation of images. This article proposes an image aesthetics assessment method based on hypernetwork of emotion fusion (HNEF). Our method incorporates the emotions depicted in images into the process of IAA. To accomplish this, we extract both aesthetic and emotional features from the images. Additionally, we employed the self-attention mechanism of the transformer to comprehensively investigate the intimate connection between aesthetics and emotion. Additionally, the hypernetwork is designed to establish perception rules governing the high-level semantic information in images. The experimental results validate the strong correlation between emotion and aesthetics. Furthermore, the proposed method exhibits a significantly competitive advantage when compared to existing methods on the Aesthetic Visual Analysis (AVA) dataset.
C1 [Lan, Guipeng; Xiao, Shuai; Yang, Jiachen; Zhou, Yanshuang; Wen, Jiabao] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Lu, Wen] Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Tianjin University; Xidian University; Chongqing University of Posts &
   Telecommunications
RP Xiao, S (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM lgp@tju.edu.cn; xs611@tju.edu.cn; yangjiachen@tju.edu.cn;
   2020234337@tju.edu.cn; wen_jiabao@tju.edu.cn; luwen@mail.xidian.edu.cn;
   gaoxb@cqupt.edu.cn
RI chen, xi/GXH-3653-2022
OI Wen, Jiabao/0000-0003-2303-9613; Lan, Guipeng/0000-0001-7321-7460; Xiao,
   Shuai/0000-0003-4058-8120; Yang, Jiachen/0000-0003-2558-552X
FU National Natural Science Foundation of China
FX No Statement Available
CR Alaluf Y, 2022, PROC CVPR IEEE, P18490, DOI 10.1109/CVPR52688.2022.01796
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen Q., 2020, CVPR, P14102
   Chen Y., 2021, P 6 INT C MULT SYST, DOI [10.1145/3471261.3471263, DOI 10.1145/3471261.3471263]
   Chew YH, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901699
   Ching JH, 2020, IEEE IMAGE PROC, P2246, DOI [10.1109/ICIP40778.2020.9191130, 10.1109/icip40778.2020.9191130]
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Datta R, 2008, IEEE IMAGE PROC, P105, DOI 10.1109/ICIP.2008.4711702
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Hanbury A., 2010, P 18 ACM INT C MULTI, V18, P83, DOI DOI 10.1145/1873951.1873965
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XY, 2018, NEUROCOMPUTING, V291, P187, DOI 10.1016/j.neucom.2018.02.073
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P1124, DOI 10.1109/TIP.2016.2514499
   Hou JW, 2023, IEEE T MULTIMEDIA, V25, P5263, DOI 10.1109/TMM.2022.3189276
   Jiang QP, 2022, IEEE T IMAGE PROCESS, V31, P2279, DOI 10.1109/TIP.2022.3154588
   Jiang QP, 2022, IEEE T BROADCAST, V68, P43, DOI 10.1109/TBC.2021.3113280
   Jiang ZQ, 2020, IEEE T BROADCAST, V66, P814, DOI 10.1109/TBC.2020.2977513
   Jin X, 2019, IET COMPUT VIS, V13, P206, DOI 10.1049/iet-cvi.2018.5249
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Ke Y., 2006, CVPR, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Lan GP, 2024, IEEE INTELL SYST, V39, P29, DOI 10.1109/MIS.2022.3217391
   Lee HJ, 2017, IEEE T MULTIMEDIA, V19, P1921, DOI 10.1109/TMM.2017.2687759
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Li LD, 2019, IEEE INT CON MULTI, P430, DOI 10.1109/ICME.2019.00081
   Liu H, 2018, IEEE T PATTERN ANAL, V40, P2546, DOI 10.1109/TPAMI.2017.2734779
   Liu Z., 2020, Convtransformer: A Convolutional Transformer Network for Video Frame Synthesis
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Lv P, 2023, IEEE T MULTIMEDIA, V25, P736, DOI 10.1109/TMM.2021.3130752
   Lv P, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1328, DOI 10.1145/3240508.3240635
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Miao HT, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9121437
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nguyen TV, 2013, IEEE T MULTIMEDIA, V15, P1910, DOI 10.1109/TMM.2013.2272919
   Paudyal P, 2019, IEEE T BROADCAST, V65, P152, DOI 10.1109/TBC.2019.2892092
   Pei JL, 2023, IEEE T MULTIMEDIA, V25, P1964, DOI 10.1109/TMM.2022.3141891
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   She DY, 2021, PROC CVPR IEEE, P8471, DOI 10.1109/CVPR46437.2021.00837
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Wang K, 2021, IEEE IMAGE PROC, P1, DOI [10.4018/IJCINI.20211001.oa14, 10.1109/ICIP42928.2021.9506621]
   Wang WN, 2019, IEEE IMAGE PROC, P1875, DOI [10.1109/ICIP.2019.8803119, 10.1109/icip.2019.8803119]
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Xiao SQ, 2023, INT J PAVEMENT ENG, V24, DOI [10.1080/10298436.2022.2027415, 10.1109/IECON49645.2022.9968611]
   Xiao SK, 2023, INT J APPL CERAM TEC, V20, P2055, DOI [10.1111/ijac.14370, 10.1109/TMM.2023.3279993]
   Xiao S, 2022, DIGIT COMMUN NETW, V8, P877, DOI 10.1016/j.dcan.2022.07.010
   Yang JC, 2022, COMPUT ELECTR ENG, V103, DOI 10.1016/j.compeleceng.2022.108322
   Yang JC, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22134697
   Yang JC, 2022, IEEE T CIRC SYST VID, V32, P4854, DOI 10.1109/TCSVT.2021.3133859
   Yang JC, 2021, FUTURE GENER COMP SY, V125, P127, DOI 10.1016/j.future.2021.06.043
   Yang JC, 2020, IEEE T CIRC SYST VID, V30, P3608, DOI 10.1109/TCSVT.2019.2948383
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang YZ, 2022, PROC CVPR IEEE, P19829, DOI 10.1109/CVPR52688.2022.01924
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   Zhang CS, 2020, Arxiv, DOI arXiv:1810.05749
   Zhang H, 2022, NEURAL COMPUT APPL, V34, P14107, DOI 10.1007/s00521-022-07139-y
   Zhao D., 2020, P 4 WORKSH MET NEURI, P1
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu HC, 2022, IEEE T CYBERNETICS, V52, P1798, DOI 10.1109/TCYB.2020.2984670
   Zhu HC, 2023, IEEE T MULTIMEDIA, V25, P179, DOI 10.1109/TMM.2021.3123468
NR 66
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3640
EP 3650
DI 10.1109/TMM.2023.3313507
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200032
DA 2024-08-05
ER

PT J
AU Liu, JF
   Wang, XS
   Wang, C
   Gao, Y
   Liu, MY
AF Liu, Jinfu
   Wang, Xinshun
   Wang, Can
   Gao, Yuan
   Liu, Mengyuan
TI Temporal Decoupling Graph Convolutional Network for Skeleton-Based
   Gesture Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Gesture recognition; Topology; Feature extraction;
   Convolutional neural networks; Network topology; Convolution; Graph
   convolutional network; gesture recognition; skeleton sequence
ID NEURAL-NETWORKS; LSTM
AB Skeleton-based gesture recognition methods have achieved high success using Graph Convolutional Network (GCN), which commonly uses an adjacency matrix to model the spatial topology of skeletons. However, previous methods use the same adjacency matrix for skeletons from different frames, which limits the flexibility of GCN to model temporal information. To solve this problem, we propose a Temporal Decoupling Graph Convolutional Network (TD-GCN), which applies different adjacency matrices for skeletons from different frames. The main steps of each convolution layer in our proposed TD-GCN are as follows. To extract deep spatiotemporal information from skeleton joints, we first extract high-level spatiotemporal features from skeleton data. Then, channel-dependent and temporal-dependent adjacency matrices corresponding to different channels and frames are calculated to capture the spatiotemporal dependencies between skeleton joints. Finally, to fuse topology information from neighbor skeleton joints, spatiotemporal features of skeleton joints are fused based on channel-dependent and temporal-dependent adjacency matrices. To the best of our knowledge, we are the first to use temporal-dependent adjacency matrices for temporal-sensitive topology learning from skeleton joints. The proposed TD-GCN effectively improves the modeling ability of GCN and achieves state-of-the-art results on gesture datasets including SHREC'17 Track and DHG-14/28.
C1 [Liu, Jinfu; Wang, Xinshun] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou 510275, Peoples R China.
   [Liu, Jinfu] Hangzhou GOTHEN Technol Co Ltd, Sch Intelligent Syst Engn, Hangzhou 310000, Peoples R China.
   [Wang, Can] Univ Kiel, Dept Comp Sci, Multimedia Informat Proc Lab, Kiel D-24118, Germany.
   [Gao, Yuan] Tampere Univ, Fac Informat Technol & Commun Sci ITC, Tampere, Finland.
   [Wang, Can] Peking Univ, Adv Inst Informat Technol, Key Lab Machine Percept, Beijing 100871, Peoples R China.
   [Wang, Can] Hangzhou Linxrobot Co Ltd, Hangzhou 310003, Peoples R China.
   [Gao, Yuan] Tampere Univ, Fac Informat Technol & Commun Sci ITC, Tampere 33100, Finland.
   [Liu, Mengyuan] Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing 100871, Peoples R China.
C3 Sun Yat Sen University; University of Kiel; Tampere University; Peking
   University; Tampere University; Peking University
RP Liu, MY (corresponding author), Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing 100871, Peoples R China.
EM liujf69@mail2.sysu.edu.cn; wangxsh36@mail2.sysu.edu.cn;
   wangcan@linxrobot.com; yuan.gao@tuni.fi; nkliuyifang@gmail.com
RI 刘, 梦媛/KIH-9841-2024
FU National Natural Science Foundation of China
FX No Statement Available
CR Avola D, 2019, IEEE T MULTIMEDIA, V21, P234, DOI 10.1109/TMM.2018.2856094
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   De Smedt Q, 2019, COMPUT VIS IMAGE UND, V181, P60, DOI 10.1016/j.cviu.2019.01.008
   De Smedt Q, 2016, IEEE COMPUT SOC CONF, P1206, DOI 10.1109/CVPRW.2016.153
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ke Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P536, DOI 10.1007/978-3-030-58586-0_32
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Korban Matthew, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P761, DOI 10.1007/978-3-030-58565-5_45
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li C, 2022, IEEE T NEUR NET LEAR, V33, P4800, DOI 10.1109/TNNLS.2021.3061115
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li C, 2017, IEEE T IMAGE PROCESS, V26, P2149, DOI 10.1109/TIP.2017.2670782
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li M, 2022, IEEE T MULTIMEDIA, V24, P1488, DOI 10.1109/TMM.2021.3066115
   Li R, 2022, IEEE T CIRC SYST VID, V32, P2647, DOI 10.1109/TCSVT.2021.3057992
   Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572
   Liu H, 2017, Arxiv, DOI [arXiv:1705.08106, DOI 10.48550/ARXIV.1705.08106]
   Liu JB, 2020, PROC CVPR IEEE, P5750, DOI 10.1109/CVPR42600.2020.00579
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Mostafa A, 2022, IEEE IMAGE PROC, P3301, DOI 10.1109/ICIP46576.2022.9897522
   Ng W, 2022, IEEE T MULTIMEDIA, V24, P1678, DOI 10.1109/TMM.2021.3070127
   Nguyen XS, 2019, PROC CVPR IEEE, P12028, DOI 10.1109/CVPR.2019.01231
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Peng W, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1432, DOI 10.1145/3394171.3413910
   Peng W, 2021, NEUROCOMPUTING, V454, P45, DOI 10.1016/j.neucom.2021.05.004
   Peng W, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107921
   Peng W, 2021, IEEE SIGNAL PROC LET, V28, P244, DOI 10.1109/LSP.2021.3049691
   Peng WT, 2020, IEEE INT C ELECTR TA, DOI 10.1109/icce-taiwan49838.2020.9258245
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Quentin De Smedt J-PV, 2017, SHREC 17 TRACK 3D HA
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L., 2020, P AS C COMP VIS, P38, DOI DOI 10.1007/978-3-030-69541-5_3
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Song JH, 2022, IEEE T CIRC SYST VID, V32, P6227, DOI 10.1109/TCSVT.2022.3165069
   Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328
   Song YF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1625, DOI 10.1145/3394171.3413802
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang SG, 2022, IEEE T MULTIMEDIA, V24, P4433, DOI 10.1109/TMM.2021.3117124
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wu GL, 2017, IEEE T MULTIMEDIA, V19, P1730, DOI 10.1109/TMM.2017.2691538
   Xia RJ, 2022, IEEE T MULTIMEDIA, V24, P2648, DOI 10.1109/TMM.2021.3086758
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang H, 2022, IEEE T IMAGE PROCESS, V31, P164, DOI 10.1109/TIP.2021.3129117
   Ye FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P55, DOI 10.1145/3394171.3413941
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2018, LECT NOTES COMPUT SC, V11213, P136, DOI 10.1007/978-3-030-01240-3_9
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhang YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3229, DOI 10.1145/3474085.3475473
   Zhao R, 2019, IEEE I CONF COMP VIS, P6881, DOI 10.1109/ICCV.2019.00698
NR 63
TC 18
Z9 18
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 811
EP 823
DI 10.1109/TMM.2023.3271811
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700044
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Liu, YM
   Zhang, MX
   Jiang, B
   Hou, B
   Liu, D
   Chen, J
   Lian, HQ
AF Liu, Yiming
   Zhang, Mengxi
   Jiang, Bo
   Hou, Bo
   Liu, Dan
   Chen, Jie
   Lian, Heqing
TI Flexible Alignment Super-Resolution Network for Multi-Contrast Magnetic
   Resonance Imaging
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Superresolution; Magnetic resonance imaging; Semantics; Feature
   extraction; Hafnium; Task analysis; Image reconstruction; Feature
   alignment; feature fusion; magnetic resonance imaging; reference-based
   image super-resolution
ID MRI; SINGLE
AB Super-resolution is essential in improving the image quality of Magnetic Resonance Imaging (MRI). Existing MRI Super-Resolution methods leverage multi-contrast MRI and achieve satisfied effects. However, these methods perform alignment by calculating the similarity of single-scale semantic features between reference images and low-resolution images, which causes misalignment and limits the performance of MRI Super-Resolution. To tackle this problem, we propose the Flexible Alignment Super-resolution Network (FASR-Net) for multi-contrast MRI Super-resolution, which explores the interaction of multi-scale features. To this end, we first use the feature extractor to generate multi-scale features, including hierarchical features and semantic pyramid features. Subsequently, we introduce the Hierarchical-Feature Alignment (HF) module and the Semantic-Pyramid-Feature Alignment (SF) module to align hierarchical features and semantic pyramid features, respectively. Finally, the Cross-Hierarchical Progressive Fusion (CHPF) module fuses these aligned features at different scales, which further improves the model's performance. Extensive experiments on FastMRI and IXI datasets show that FASR-net achieves the most competitive results over state-of-the-art approaches.
C1 [Liu, Yiming; Lian, Heqing] Xiao Ying AI Lab, Beijing 100085, Peoples R China.
   [Zhang, Mengxi] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Jiang, Bo; Hou, Bo; Liu, Dan] Peking Union Med Coll Hosp, Beijing 100730, Peoples R China.
   [Chen, Jie] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Tianjin University; Chinese Academy of Medical Sciences - Peking Union
   Medical College; Peking Union Medical College Hospital; Peking
   University
RP Lian, HQ (corresponding author), Xiao Ying AI Lab, Beijing 100085, Peoples R China.; Chen, J (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM liuyiming@xiaoyingai.com; mengxizhang@tju.edu.cn; jbpumch@163.com;
   houbo97@pumch.cn; liud2104@163.com; jiechen2019@pku.edu.cn;
   lianheqing@xiaoyingai.com
OI Zhang, Mengxi/0000-0002-6011-1218; Liu, Yiming/0000-0002-0387-6329
FU China National Funds for Distinguished Young Scientists
FX No Statement Available
CR brain-development, 2007, IXI
   Cao JZ, 2022, LECT NOTES COMPUT SC, V13678, P325, DOI 10.1007/978-3-031-19797-0_19
   Chen CQ, 2022, IEEE T MULTIMEDIA, V24, P202, DOI 10.1109/TMM.2021.3050092
   Chen W, 2015, KNEE SURG SPORT TR A, V23, P198, DOI 10.1007/s00167-014-3035-0
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Feng CM, 2021, LECT NOTES COMPUT SC, V12906, P140, DOI 10.1007/978-3-030-87231-1_14
   Gao QQ, 2019, LECT NOTES COMPUT SC, V11362, P527, DOI 10.1007/978-3-030-20890-5_34
   Gao SC, 2023, PROC CVPR IEEE, P10021, DOI 10.1109/CVPR52729.2023.00966
   Gyumin Shim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8422, DOI 10.1109/CVPR42600.2020.00845
   Jiang YM, 2021, PROC CVPR IEEE, P2103, DOI 10.1109/CVPR46437.2021.00214
   Knoll F, 2020, RADIOL-ARTIF INTELL, V2, DOI 10.1148/ryai.2020190007
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li HY, 2022, NEUROCOMPUTING, V479, P47, DOI 10.1016/j.neucom.2022.01.029
   Li YH, 2022, LECT NOTES COMPUT SC, V13669, P280, DOI 10.1007/978-3-031-20077-9_17
   Liang J, 2021, PROC CVPR IEEE, P9387, DOI 10.1109/CVPR46437.2021.00927
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lu LY, 2021, PROC CVPR IEEE, P6364, DOI 10.1109/CVPR46437.2021.00630
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Plenge E, 2012, MAGN RESON MED, V68, P1983, DOI 10.1002/mrm.24187
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Van Reeth E, 2012, CONCEPT MAGN RESON A, V40A, P306, DOI 10.1002/cmr.a.21249
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wonkyung Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P465, DOI 10.1007/978-3-030-58586-0_28
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Zbontar J, 2019, Arxiv, DOI arXiv:1811.08839
   Zeng K, 2018, COMPUT BIOL MED, V99, P133, DOI 10.1016/j.compbiomed.2018.06.010
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zheng HT, 2018, LECT NOTES COMPUT SC, V11210, P87, DOI 10.1007/978-3-030-01231-1_6
NR 38
TC 0
Z9 0
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5159
EP 5169
DI 10.1109/TMM.2023.3330085
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600048
DA 2024-08-05
ER

PT J
AU Pei, YJ
   Wang, ZY
   Li, N
   Chen, HL
   Huang, BJ
   Tu, WP
AF Pei, Yingjiao
   Wang, Zhongyuan
   Li, Na
   Chen, Heling
   Huang, Baojin
   Tu, Weiping
TI Deep Hashing Network With Hybrid Attention and Adaptive Weighting for
   Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive weighting learning; channel and spatial attention; deep hashing
   network; image retrieval
ID QUANTIZATION
AB Due to the low computational cost of Hamming distance, hashing-based image retrieval has been universally acknowledged. Therefore, it is becoming increasingly important to quickly generate high-precision hash codes (also hash features) from images. However, the existing deep hashing methods are vulnerable to image content variations; that is, it is difficult to generate stable and consistent hash codes for similar images. In addition, generating hash codes of different lengths requires retraining the model, which is expensive in training time. To address these problems, this paper proposes a deep hashing network (DHN) with a hybrid attention mechanism and adaptive weighting (HAAW) learning. It mainly consists of a feature extraction module, feature refinement module, classification layer, hash layer and an adaptive weight layer. In particular, the hybrid attention mechanism combines bottom-up pixel saliency and top-down semantic constraints, in which the former is achieved through channel and spatial attention (CSA) and the latter is supervised by classification labels. In this way, it encourages the network to focus on dominant semantic features without being disturbed by irrelevant objects so that semantically similar images can be mapped to approximate hash codes. We further propose an adaptive weighting learning algorithm to generate weights for each bit of the hash code generated by the deep network. Then, we directly generate shorter hash codes from the available long hash code according to the importance of bits represented by the weights. This avoids retraining the network for learning hash codes of different lengths. Extensive experiments on public CIFAR-10, NUS_WIDE and ImageNet datasets show that our method has achieved substantial improvements over the counterparts in terms of precision and speed.
C1 [Pei, Yingjiao; Wang, Zhongyuan; Chen, Heling; Huang, Baojin; Tu, Weiping] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Li, Na] Wuhan Univ, Arch Wuhan Univ, Wuhan 430072, Peoples R China.
C3 Wuhan University; Wuhan University
RP Wang, ZY (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.; Li, N (corresponding author), Wuhan Univ, Arch Wuhan Univ, Wuhan 430072, Peoples R China.
EM peiyingjiao@whu.edu.cn; wzy_hope@163.com; wrxln_hope@sina.com;
   helingchen@whu.edu.cn; huangbaojin@whu.edu.cn; tuweiping@whu.edu.cn
OI Wang, Zhongyuan/0000-0002-9796-488X; Huang, Baojin/0000-0002-4882-5787
FU National Natural Science Foundation of China
FX No Statement Available
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   Bai JL, 2019, IEEE T MULTIMEDIA, V21, P3178, DOI 10.1109/TMM.2019.2920601
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gu JZ, 2022, INT CONF ACOUST SPEE, P3199, DOI 10.1109/ICASSP43922.2022.9746429
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Kim S., 2012, IEEE 11 INT C DATA M, P1122
   Krizhevsky A., 2009, Learning multiple layers of features from tiny images
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li Q, 2017, ADV NEUR IN, V30
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Li X, 2022, IEEE T CIRC SYST VID, V32, P933, DOI 10.1109/TCSVT.2021.3070129
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Meyer-Bäse A, 2004, OPT ENG, V43, P1012, DOI 10.1117/1.1683885
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P3128, DOI 10.1109/TMM.2020.2974326
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Norouzi M., 2011, P 28 INT C MACH LEAR, P353
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen XB, 2022, IEEE T MULTIMEDIA, V24, P1116, DOI 10.1109/TMM.2021.3119868
   Shi WW, 2022, IEEE T NEUR NET LEAR, V33, P3713, DOI 10.1109/TNNLS.2021.3054386
   Shi Y, 2021, IEEE T MULTIMEDIA, V23, P3778, DOI 10.1109/TMM.2020.3031092
   Szegedy C., 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594]
   van der Maaten G. E., 2008, J. Mach. Learn. Res., V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang YT, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107785
   Wang YB, 2020, IEEE T MULTIMEDIA, V22, P1458, DOI 10.1109/TMM.2019.2947197
   Weiss A., 2008, NeurIPS, P1753
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang Z, 2019, IEEE ACCESS, V7, P11209, DOI 10.1109/ACCESS.2019.2891894
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P212, DOI 10.1109/TCSVT.2017.2771332
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zheng XT, 2020, NEUROCOMPUTING, V403, P224, DOI 10.1016/j.neucom.2020.04.037
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 45
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4961
EP 4973
DI 10.1109/TMM.2023.3328197
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600045
DA 2024-08-05
ER

PT J
AU Qin, YL
   Pu, N
   Wu, HZ
AF Qin, Yalan
   Pu, Nan
   Wu, Hanzhou
TI EDMC: Efficient Multi-View Clustering via Cluster and Instance Space
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clustering algorithms; Tensors; Scalability; Representation learning;
   Optimization; Dimensionality reduction; Complexity theory; Multi-view
   subspace clustering; cluster representation; instance representation;
   anchor learning; a unified framework
AB Multi-view subspace clustering aims to cluster the data lying in a union of subspaces with low dimensions. The commonly used spectral clustering performs the final clustering based on an n x n affinity graph, which suffers from relative high time and space complexity. Some existing works have chosen key anchors with uniform sampling strategy or K-means for dealing with large-scale datasets. However, few of them pay attention to the physical meaning of cluster representation in the column of the dataset for learning informative anchors, which is independent from the instance representation. In this paper, we propose efficient dual multi-view clustering (EDMC) with relative low complexity. To be specific, EDMC makes full use of cluster representation space in the column of the dataset to help produce informative anchors, which has a clear physical meaning and is independent of instance representation in the row. It simultaneously explores the cluster and instance subspace representations to learn anchors for large-scale datasets. We perform anchor learning and efficient multi-view clustering in a unified framework and then adopt an alternative optimization strategy for solving the formulated problem. Extensive experiments performed on different datasets in terms of several metrics validate the superiority of the proposed method.
C1 [Qin, Yalan; Wu, Hanzhou] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Pu, Nan] Univ Trento, Dept Informat Engn & Comp Sci, I-40128 Trento, Italy.
C3 Shanghai University; University of Trento
RP Wu, HZ (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.; Pu, N (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, I-40128 Trento, Italy.
EM ylqin@shu.edu.cn; n.pu@liacs.leidenuniv.nl; hanzhou@shu.edu.cn
RI ; Wu, Hanzhou/C-8755-2013
OI Pu, Nan/0000-0002-2179-8301; Qin, Yalan/0000-0002-4479-5680; Wu,
   Hanzhou/0000-0002-1599-7232
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen MS, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3774, DOI 10.1145/3503161.3547940
   Chen MS, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P127, DOI 10.1145/3534678.3539282
   Cheng ZQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P90, DOI 10.1145/3240508.3240518
   Cheng ZQ, 2017, PROC CVPR IEEE, P4169, DOI 10.1109/CVPR.2017.444
   Cheng Zhi-Qi, 2017, P 2017ACM INT C MULT, P287
   Hu H, 2014, PROC CVPR IEEE, P3834, DOI 10.1109/CVPR.2014.484
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P9595, DOI 10.1109/TPAMI.2023.3247939
   Huang SY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2049, DOI 10.1145/3240508.3240588
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Kang Z, 2020, NEURAL NETWORKS, V122, P279, DOI 10.1016/j.neunet.2019.10.010
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Liu SY, 2022, AAAI CONF ARTIF INTE, P7576
   Luo SR, 2018, AAAI CONF ARTIF INTE, P3730
   Nguyen P. A., 2017, TREC Video Retrieval Eval.
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Pu Nan, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P541, DOI 10.1145/3503161.3548234
   Pu N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2149, DOI 10.1145/3394171.3413673
   Pu Nan, 2021, P IEEECVF C COMPUTER, P7901
   Qin H., 2022, Pattern Recognit., V130
   Qin N., 2023, IEEE Trans. Knowl. Data Eng.,early access
   Qin YL, 2023, IEEE T IMAGE PROCESS, V32, P175, DOI 10.1109/TIP.2022.3226408
   Qin YL, 2023, IEEE T CYBERNETICS, V53, P832, DOI 10.1109/TCYB.2022.3165550
   Qin YL, 2023, IEEE T PATTERN ANAL, V45, P2652, DOI 10.1109/TPAMI.2022.3168882
   Qin YL, 2023, IEEE T KNOWL DATA EN, V35, P2313, DOI 10.1109/TKDE.2021.3113943
   Qin YL, 2022, IEEE T IMAGE PROCESS, V31, P1, DOI 10.1109/TIP.2021.3128325
   Qin YL, 2021, SIGNAL PROCESS, V186, DOI 10.1016/j.sigpro.2021.108115
   Qin Y, 2024, Arxiv, DOI arXiv:2308.09911
   Qin Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4948, DOI 10.1145/3503161.3547922
   Qin Z., 2023, IEEE T KNOWL DATA EN
   Sun MJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3528, DOI 10.1145/3474085.3475516
   Tang C, 2023, IEEE T KNOWL DATA EN, V35, P6449, DOI 10.1109/TKDE.2022.3172687
   Wang J, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3253246
   Wang QQ, 2021, IEEE T IMAGE PROCESS, V30, P1771, DOI 10.1109/TIP.2020.3048626
   Wang SW, 2022, IEEE T IMAGE PROCESS, V31, P556, DOI 10.1109/TIP.2021.3131941
   Wang WR, 2016, Arxiv, DOI arXiv:1602.01024
   Wang YM, 2023, IEEE T MULTIMEDIA, V25, P6551, DOI 10.1109/TMM.2022.3210376
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhao B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P383, DOI 10.1145/3240508.3240536
   Zhi-Qi Cheng, 2017, IEEE Transactions on Multimedia, V19, P1170, DOI 10.1109/TMM.2016.2647386
NR 43
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5273
EP 5283
DI 10.1109/TMM.2023.3331197
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600016
DA 2024-08-05
ER

PT J
AU Teng, SH
   Li, JB
   Teng, LY
   Fei, LK
   Wu, NQ
   Zhang, W
AF Teng, Shaohua
   Li, Jiangbo
   Teng, Luyao
   Fei, Lunke
   Wu, Naiqi
   Zhang, Wei
TI Scalable Discrete and Asymmetric Unequal Length Hashing Learning for
   Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unequal length encoding; cross-modal hashing (CMH); dual semantic
   embedding learning
AB Due to the high computational efficiency and low storage cost, cross-modal hashing retrieval attracts great deal of attention. However, as heterogeneous data from different modalities often have distinct physical meanings and underlying structures, learning encoding with equal length for different modalities may result in an insurmountable semantic gap. In addition, there are other issues to be addressed in this field such as how to combine label and sample information to learn hash codes effectively, how to reduce the time consumption caused by computing $n \times n$ similarity matrix, and how to effectively solve the complex discrete optimization problem. To cope with the above challenges, this study proposes a novel model called Scalable Discrete and Asymmetric Unequal Length Hashing (SDAULH). First, SDAULH constructs a novel hash model that utilizes unequal length encoding schemes to narrow the semantic gap between heterogeneous modalities. Second, SDAULH develops a dual semantic embedding learning scheme, which combines pairwise similarity between label and sample data to generate a more discriminative hash code. Third, SDAULH associates with both hash codes and label information by an asymmetric relaxation strategy. Furthermore, SDAULH solves directly the discrete optimization problem by generating discrete hash codes. Experimental results on four benchmark datasets demonstrate the promising performance of SDAULH.
C1 [Teng, Shaohua; Li, Jiangbo; Fei, Lunke; Zhang, Wei] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Teng, Shaohua; Li, Jiangbo; Fei, Lunke; Zhang, Wei] Jieyang Ctr, Guangdong Prov Lab Chem & Fine Chem Engn, Jieyang 515200, Peoples R China.
   [Teng, Luyao] Guangzhou Panyu Polytech, Sch Informat Engn, Guangzhou 511483, Peoples R China.
   [Wu, Naiqi] Macau Univ Sci & Technol, Inst Syst Engn, Collaborat Lab Intelligent Sci & Syst, Macau 999078, Peoples R China.
C3 Guangdong University of Technology; Guangzhou Panyu Polytechnic; Macau
   University of Science & Technology
RP Zhang, W (corresponding author), Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
EM shteng@gdut.edu.cn; 2112105115@mail2.gdut.edu.cn; Luna.teng@qq.com;
   flksxm@126.com; nqwu@must.edu.mo; weizhang@gdut.edu.cn
RI ; Wu, Naiqi/C-2953-2017
OI Fei, Lunke/0000-0001-6072-7875; Wu, Naiqi/0000-0001-6782-458X; TENG,
   Shaohua/0000-0002-7204-1288; teng, luyao/0000-0002-3872-4085
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2009, NIPS
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen Y, 2020, IEEE T IMAGE PROCESS, V29, P3596, DOI 10.1109/TIP.2020.2963952
   Chen ZD, 2020, IEEE T CIRC SYST VID, V30, P2262, DOI 10.1109/TCSVT.2019.2911359
   Chen ZD, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1694, DOI 10.1145/3343031.3350862
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Cong Bai, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P525, DOI 10.1145/3372278.3390711
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Fang XZ, 2022, IEEE T CYBERNETICS, V52, P11780, DOI 10.1109/TCYB.2021.3081615
   Huiskes M.J., 2008, P ACM INT C MULT INF, P39, DOI 10.1145/1460096
   Ji J, 2014, arXiv, DOI [10.48550/ARXIV.1408.2927, DOI 10.48550/ARXIV.1408.2927]
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Li HX, 2023, IEEE T KNOWL DATA EN, V35, P1185, DOI 10.1109/TKDE.2021.3102119
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Liu LC, 2018, LECT NOTES COMPUT SC, V10828, P606, DOI 10.1007/978-3-319-91458-9_37
   Liu W., 2014, P ADV NEUR INF PROC, V27
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu X, 2023, IEEE T MULTIMEDIA, V25, P3811, DOI 10.1109/TMM.2022.3166668
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Mandal D, 2019, IEEE T IMAGE PROCESS, V28, P102, DOI 10.1109/TIP.2018.2863040
   Qin JY, 2022, IEEE T IMAGE PROCESS, V31, P5343, DOI 10.1109/TIP.2022.3195059
   Rasiwasia Nikhil, 2010, P 18 ACM INT C MULT, P251
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Tan WT, 2023, IEEE T MULTIMEDIA, V25, P8499, DOI 10.1109/TMM.2023.3238308
   Tong S., 2023, IEEE Trans Comput. Social Syst., V10, P577
   Wang D, 2019, IEEE T PATTERN ANAL, V41, P2466, DOI 10.1109/TPAMI.2018.2861000
   Wang I, 2021, Pattern Recognit., V111
   Wang L, 2022, IEEE T MULTIMEDIA, V24, P3665, DOI 10.1109/TMM.2021.3105824
   Wang S, 2021, KNOWL-BASED SYST, V228, DOI 10.1016/j.knosys.2021.107252
   Wang YX, 2021, IEEE T KNOWL DATA EN, V33, P3507, DOI 10.1109/TKDE.2020.2974825
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yi J., 2021, PROC IEEE INT C MULT, P1
   Zhan YW, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108262
   Zhang DL, 2023, IEEE T KNOWL DATA EN, V35, P1365, DOI 10.1109/TKDE.2021.3099125
   Zhang DL, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108343
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang PF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1762, DOI 10.1145/3123266.3123320
   Zhang W, 2023, WORLD WIDE WEB, V26, P1093, DOI 10.1007/s11280-022-01072-9
   Zhu L, 2022, IEEE MULTIMEDIA, V29, P17, DOI 10.1109/MMUL.2022.3144138
   Zhu L, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3365841
NR 41
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7917
EP 7932
DI 10.1109/TMM.2024.3372876
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000008
DA 2024-08-05
ER

PT J
AU Wang, HY
   Cheng, YH
   Liu, XM
   Wang, XS
AF Wang, Haoyu
   Cheng, Yuhu
   Liu, Xiaomin
   Wang, Xuesong
TI Reinforcement Learning Based Markov Edge Decoupled Fusion Network for
   Fusion Classification of Hyperspectral and LiDAR
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Laser radar; Task analysis; Topology; Data mining;
   Data integration; Remote sensing; Hyperspectral image (HSI); light
   detection and ranging (LiDAR); fusion classification; multimodal fusion;
   reinforcement learning; graph learning
AB Hyperspectral images (HSIs) and light detection and ranging (LiDAR) are two critical and frequently used types of remote sensing data, each containing rich spectral and elevation information. Fusing HSI and LiDAR can exploit the complementary properties of the two modalities for ground object classification. The performance of existing fusion classification methods is often limited by the difficulty of adapting feature extraction operators to complex spatial distributions, and the correlation and specificity between different modalities are not reasonably exploited. Therefore, the reinforcement learning-based markov edge decoupled fusion network (MEDFN) is proposed. This network can intelligently compose graphs based on different modal characteristics and tasks to adapt to complex spatial distributions; it can also suppress noise to complete fusion classification while fully utilizing complementary information of different modalities. First, a reinforcement learning-based graph construction subnetwork (RLGN) is proposed to learn a two-modal graph construction strategy suitable for classification tasks by transforming regular multimodal data into irregular graph data. Second, a multimodal edge attention module (MEAM) is proposed to extract edge features between spatial neighboring nodes and model the importance of each node, thereby capturing the spatial topology information encompassed in the multimodal data. Finally, the decoupled multimodal fusion module (DMFM) is proposed to decouple multimodal features into shared and unshared parts and enhance the model's ability to distinguish features by targeting the modal-shared feature between modalities and modal-specific feature. The experimental results based on three well-known HSI and LiDAR datasets demonstrate the effectiveness of the proposed MEDFN in fusion classification tasks.
C1 [Wang, Haoyu; Cheng, Yuhu; Liu, Xiaomin; Wang, Xuesong] China Univ Min & Technol, Engn Res Ctr Intelligent Control Underground Space, Minist Educ, Xuzhou 221116, Peoples R China.
   [Wang, Haoyu; Cheng, Yuhu; Liu, Xiaomin; Wang, Xuesong] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
   [Wang, Haoyu; Cheng, Yuhu; Liu, Xiaomin; Wang, Xuesong] China Univ Min & Technol, Xuzhou Key Lab Artificial Intelligence & Big Data, Xuzhou 221116, Peoples R China.
C3 China University of Mining & Technology; China University of Mining &
   Technology; China University of Mining & Technology
RP Wang, XS (corresponding author), China Univ Min & Technol, Engn Res Ctr Intelligent Control Underground Space, Minist Educ, Xuzhou 221116, Peoples R China.
EM wanghaoyucumt@163.com; chengyuhu@163.com; xiaominliu@cumt.edu.cn;
   wangxuesongcumt@163.com
OI Wang, Haoyu/0000-0002-8905-822X
FU National Natural Science Foundation of China
FX No Statement Available
CR Dian RW, 2021, IEEE T NEUR NET LEAR, V32, P1124, DOI 10.1109/TNNLS.2020.2980398
   Ding KX, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3216319
   Dong WQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3179737
   Du B, 2017, IEEE T MULTIMEDIA, V19, P67, DOI 10.1109/TMM.2016.2608780
   Du XQ, 2021, IEEE T GEOSCI REMOTE, V59, P10062, DOI [10.1109/TGRS.2020.3047130, 10.4018/IJCINI.295808]
   Ghamisi P, 2019, IEEE GEOSC REM SEN M, V7, P6, DOI 10.1109/MGRS.2018.2890023
   Hang RL, 2020, IEEE T GEOSCI REMOTE, V58, P4939, DOI 10.1109/TGRS.2020.2969024
   Hong DF, 2021, IEEE T GEOSCI REMOTE, V59, P5966, DOI 10.1109/TGRS.2020.3015157
   Hong DF, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3017414
   Jia S, 2021, IEEE T GEOSCI REMOTE, V59, P1437, DOI 10.1109/TGRS.2020.2996599
   Kotzagiannidis MS, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3112298
   Li HC, 2020, IEEE J-STARS, V13, P738, DOI 10.1109/JSTARS.2020.2968930
   Li JJ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3213513
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Lu T, 2023, INFORM FUSION, V93, P118, DOI 10.1016/j.inffus.2022.12.020
   Morchhale S, 2016, 2016 8TH WORKSHOP ON HYPERSPECTRAL IMAGE AND SIGNAL PROCESSING: EVOLUTION IN REMOTE SENSING (WHISPERS)
   Pedergnana M, 2012, IEEE J-STSP, V6, P856, DOI 10.1109/JSTSP.2012.2208177
   Peng YS, 2022, IEEE J-STARS, V15, P1454, DOI 10.1109/JSTARS.2022.3144312
   Roy SK, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3177633
   Shi C, 2020, IEEE T MULTIMEDIA, V22, P487, DOI 10.1109/TMM.2019.2928491
   Wan S, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3075223
   Wang XH, 2022, INFORM FUSION, V82, P1, DOI 10.1016/j.inffus.2021.12.008
   Wu X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3124913
   Xiu D, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3210398
   Yang JQ, 2021, IEEE T GEOSCI REMOTE, V59, P10328, DOI 10.1109/TGRS.2020.3046757
   Yang Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3169163
   Zhang HY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3163326
   Zhang MM, 2023, IEEE T CYBERNETICS, V53, P3153, DOI 10.1109/TCYB.2022.3169773
   Zhang MM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3093334
   Zhang MM, 2020, IEEE T CYBERNETICS, V50, P100, DOI 10.1109/TCYB.2018.2864670
   Zhao CY, 2016, IEEE T GEOSCI REMOTE, V54, P4052, DOI 10.1109/TGRS.2016.2535538
   Zhao XD, 2024, IEEE T NEUR NET LEAR, V35, P2314, DOI 10.1109/TNNLS.2022.3189994
   Zhao XD, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3065507
   Zhao XD, 2020, IEEE T GEOSCI REMOTE, V58, P7355, DOI 10.1109/TGRS.2020.2982064
   Zhu QQ, 2022, IEEE T CYBERNETICS, V52, P11709, DOI 10.1109/TCYB.2021.3070577
NR 35
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7174
EP 7187
DI 10.1109/TMM.2024.3360717
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RE1G6
UT WOS:001225895800003
DA 2024-08-05
ER

PT J
AU Wang, L
   Xie, T
   Zhang, XY
   Jiang, ZQ
   Yang, LQ
   Zhang, HM
   Li, XY
   Ren, YL
   Yu, HY
   Li, J
   Liu, HP
AF Wang, Li
   Xie, Tao
   Zhang, Xinyu
   Jiang, Zhiqiang
   Yang, Linqi
   Zhang, Haoming
   Li, Xiaoyu
   Ren, Yilong
   Yu, Haiyang
   Li, Jun
   Liu, Huaping
TI Auto-Points: Automatic Learning for Point Cloud Analysis With Neural
   Architecture Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud; neural architecture search; deep learning
ID 3D OBJECT DETECTION
AB Pure point-based neural networks have recently shown tremendous promise for point cloud tasks, including 3D object classification, 3D object part segmentation, 3D semantic segmentation, and 3D object detection. Nevertheless, it is a laborious process to construct a network for each task due to the artificial parameters and hyperparameters involved, e.g., the depths and widths of the network and the number of sampled points at each stage. In this work, we propose Auto-Points, a novel one-shot search framework that automatically seeks the optimal architecture configuration for point cloud tasks. Technically, we introduce a set abstraction mixer (SAM) layer that is capable of scaling up flexibly along the depth and width of the network. Each SAM layer consists of numerous child candidates, which simplifies architecture search and enables us to discover the optimum design for each point cloud task pursuant to resource constraint from an enormous search space. To fully optimize the child candidates, we develop a weight-entwinement neural architecture search (NAS) technique that entwines the weights of different candidates in the same layer during supernet training such that all candidates can be extremely optimized. Benefiting from the proposed techniques, the trained supernet allows the searched subnets to be exceptionally well-optimized without further retraining or finetuning. In particular, the searched models deliver superior performances on multiple extensively employed benchmarks, 93.9% overall accuracy (OA) on ModelNet40, 89.1% OA on ScanObjectNN, 87.1% instance average IoU on ShapeNetPart, 69.1% mIoU on S3DIS, 70.4% mAP@0.25 on ScanNet V2, and 64.4% mAP@0.25 on SUN RGB-D.
C1 [Wang, Li; Zhang, Xinyu; Li, Jun] Tsinghua Univ, Sch Vehicle & Mobil, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R China.
   [Wang, Li] State Key Lab Robot & Syst HIT, Harbin 150001, Peoples R China.
   [Xie, Tao; Jiang, Zhiqiang; Yang, Linqi; Zhang, Haoming; Li, Xiaoyu] Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150006, Peoples R China.
   [Ren, Yilong; Yu, Haiyang] Beihang Univ, Beijing Key Lab Cooperat Vehicle Infrastruct Syst, Beijing 100191, Peoples R China.
   [Ren, Yilong; Yu, Haiyang] Zhongguancun Lab, Beijing 100094, Peoples R China.
   [Liu, Huaping] Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
C3 Tsinghua University; Harbin Institute of Technology; Beihang University;
   Zhongguancun Laboratory; Tsinghua University
RP Zhang, XY (corresponding author), Tsinghua Univ, Sch Vehicle & Mobil, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R China.
EM wangli_thu@mail.tsinghua.edu.cn; xietao1997@hit.edu.cn;
   xyzhang@tsinghua.edu.cn; 22s008043@stu.hit.edu.cn;
   1190303319@stu.hit.edu.cn; 18092433532@163.com;
   22s108236@stu.hit.edu.cn; yilongren@buaa.edu.cn; hyyu@buaa.edu.cn;
   lj19580324@126.com; hpliu@tsinghua.edu.cn
OI li, xiaoyu/0009-0008-3062-9594; Zhang, Haoming/0000-0002-0610-4113; Xie,
   Tao/0000-0001-7223-9553
FU National High Technology Research and Development Program of China
FX No Statement Available
CR ang H., 2020, EUR C COMP VIS, P685, DOI DOI 10.1007/978-3-030-58604-1_41
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Arvanitis G, 2022, IEEE T MULTIMEDIA, V24, P2230, DOI 10.1109/TMM.2021.3089838
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Billinghurst Mark, 2015, Foundations and Trends in Human-Computer Interaction, V8, P73, DOI 10.1561/1100000049
   Chen MH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12250, DOI 10.1109/ICCV48922.2021.01205
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chenfeng Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P1, DOI 10.1007/978-3-030-58604-1_1
   Cheng BW, 2021, PROC CVPR IEEE, P8959, DOI 10.1109/CVPR46437.2021.00885
   Cheng SL, 2021, IEEE T IMAGE PROCESS, V30, P4436, DOI 10.1109/TIP.2021.3072214
   Chu XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12219, DOI 10.1109/ICCV48922.2021.01202
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Duan Y, 2022, PROC CVPR IEEE, P16959, DOI 10.1109/CVPR52688.2022.01647
   Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Fernandes D, 2021, INFORM FUSION, V68, P161, DOI 10.1016/j.inffus.2020.11.002
   González A, 2015, IEEE INT VEH SYM, P356, DOI 10.1109/IVS.2015.7225711
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   Han XF, 2023, IEEE T MULTIMEDIA, V25, P5638, DOI 10.1109/TMM.2022.3198318
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Huang TX, 2023, IEEE T MULTIMEDIA, V25, P5903, DOI 10.1109/TMM.2022.3200851
   Jiahui Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P702, DOI 10.1007/978-3-030-58571-6_41
   Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053
   Lai X, 2022, PROC CVPR IEEE, P8490, DOI 10.1109/CVPR52688.2022.00831
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li B, 2016, Arxiv, DOI arXiv:1608.07916
   Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li YY, 2018, ADV NEUR IN, V31
   Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2929, DOI 10.1109/ICCV48922.2021.00294
   Liu ZJ, 2019, ADV NEUR IN, V32
   Lv CL, 2022, IEEE T MULTIMEDIA, V24, P1815, DOI 10.1109/TMM.2021.3073265
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Ma X., 2022, PROC INT C LEARN REP, P1
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Peng H., 2020, Advances in Neural Information Processing Systems, V33, P17955
   Pham H, 2018, PR MACH LEARN RES, V80
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi ZK, 2023, Arxiv, DOI arXiv:2302.02318
   Qian G. C., 2022, ADV NEURAL INFORM PR, V35, P23192, DOI [DOI 10.48550/ARXIV.2206.04670, https://doi.org/10.48550/arXiv.2206.04670]
   Qian GC, 2021, ADV NEUR IN, V34
   Qiu S, 2021, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR46437.2021.00180
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Qiu S, 2021, IEEE WINT CONF APPL, P3812, DOI 10.1109/WACV48630.2021.00386
   Ran HX, 2022, PROC CVPR IEEE, P18920, DOI 10.1109/CVPR52688.2022.01837
   Real E, 2017, PR MACH LEARN RES, V70
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Wang DL, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Wang HY, 2022, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR52688.2022.00118
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xie Q., 2020, P IEEE CVF C COMP VI, P10447, DOI 10.1109/cvpr42600.2020.01046
   Xie Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3692, DOI 10.1109/ICCV48922.2021.00369
   Xie S., 2018, PROC INT C LEARN REP
   Xie T, 2023, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR52729.2023.00125
   Xie T, 2024, IEEE T MULTIMEDIA, V26, P1027, DOI 10.1109/TMM.2023.3275366
   Xue L, 2023, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR52729.2023.00120
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Yang L, 2020, ROBOT CIM-INT MANUF, V64, DOI 10.1016/j.rcim.2019.101929
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   You S, 2020, PROC CVPR IEEE, P1996, DOI 10.1109/CVPR42600.2020.00207
   Yu JH, 2019, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2019.00189
   Zhang WX, 2023, IEEE T VIS COMPUT GR, V29, P4229, DOI 10.1109/TVCG.2022.3185247
   Zaiwei Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P311, DOI [10.1061/9780784482933.027, 10.1007/978-3-030-58610-2_19]
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhou D., 2020, P IEEE CVF C COMP VI, P11396
   Zichao Guo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P544, DOI 10.1007/978-3-030-58517-4_32
   Zoph B., 2017, Neural architecture search with reinforcement learning
NR 87
TC 2
Z9 2
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2878
EP 2893
DI 10.1109/TMM.2023.3304892
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400011
DA 2024-08-05
ER

PT J
AU Wei, L
   Wan, S
   Wang, ZC
   Yang, FZ
AF Wei, Lei
   Wan, Shuai
   Wang, Zhecheng
   Yang, Fuzheng
TI Near-Lossless Compression of Point Cloud Attribute Using Quantization
   Parameter Cascading and Rate-Distortion Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; near-lossless; quantization parameter; rate
   distortion optimization
ID BIT ALLOCATION
AB Near-lossless compression of point clouds is suitable for the application scenarios with low distortion tolerance and certain requirements on the rate. Near-lossless attribute compression usually adopts a level-of-detail structure, where the dependencies between the layers make it possible to improve the rate-distortion (R-D) performance by using different quantization parameters for different layers. In this work, a theoretical analysis of the dependencies between adjacent layers is carried out, based on which the dependent Distortion-Quantization and Rate-Quantization models are established for point cloud attribute compression. Then an algorithm for quantization parameter cascading based on R-D optimization is proposed and implemented for near-lossless compression of point cloud attributes. The experimental results show that the proposed method has a superior performance gain compared to state-of-the-art for the Hausdorff R-D performance. At the same time, the proposed method improves subjective quality and is well adapted to various categories of point clouds.
C1 [Wei, Lei; Wan, Shuai; Wang, Zhecheng] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Shaanxi, Peoples R China.
   [Wan, Shuai] RMIT Univ, Sch Engn, Melbourne, Vic 3000, Australia.
   [Yang, Fuzheng] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
C3 Northwestern Polytechnical University; Royal Melbourne Institute of
   Technology (RMIT); Xidian University
RP Wan, S (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Shaanxi, Peoples R China.
EM l.wei@mail.nwpu.edu.cn; swan@nwpu.edu.cn;
   zhecheng.wang@mail.nwpu.edu.cn; fzhyang@mail.xidian.edu.cn
OI Wei, Lei/0000-0002-0958-4597; wan, shuai/0000-0001-8617-149X
FU Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2020, G-PCCTest Model v12 User Manual, Standard ISO/IEC JTCI/SC29/WG7 MPEG 3D Graphics Coding, N00005
   [Anonymous], 2022, Standard ISO/IEC JTC1/SC29/WG7 MPEG 3D Graphics Coding, N00271
   Asif MT, 2015, IEEE T INTELL TRANSP, V16, P1817, DOI 10.1109/TITS.2014.2374335
   Berger T., 1971, RATE DISTORTION THEO
   Bjontegaard G., 2001, ITUT SG.16 Q.6 VCEG-M33
   Chen LQ, 2019, IEEE T CIRC SYST VID, V29, P3754, DOI 10.1109/TCSVT.2018.2881040
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   CLARK JH, 1976, COMMUN ACM, V19, P547, DOI 10.1145/360349.360354
   Common Test Conditions for G-PCC, 2021, Standard ISO/IEC JTCI/SC29/WG7 MPEG 3D Graphics Coding, N00106, Virtual
   Cover T.M., 1999, ELEMENTS INFORM THEO
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   dEon E., 2017, Standard ISO/IEC JTC1/SC29 JointWG11/WG1 (MPEG/JPEG), M40059(WG11)
   Gao P., 2021, IEEE Trans, Circuits Syst. Video Technol, V33, P2424, DOI [10.1109/TCSVT2022.3223898, DOI 10.1109/TCSVT2022.3223898]
   Gao Z., 2019, Standard ISO/IEC JTCI/SC29/WG11 MPEG, M51011
   Gong YC, 2021, IEEE T IMAGE PROCESS, V30, P5692, DOI 10.1109/TIP.2021.3087413
   Gong YC, 2017, IEEE T CIRC SYST VID, V27, P1304, DOI 10.1109/TCSVT.2016.2539718
   Kamaci N, 2005, IEEE T CIRC SYST VID, V15, P994, DOI 10.1109/TCSVT.2005.852400
   Krivokuca M, 2020, IEEE T IMAGE PROCESS, V29, P2217, DOI 10.1109/TIP.2019.2957853
   Li L, 2023, IEEE T MULTIMEDIA, V25, P2007, DOI 10.1109/TMM.2022.3142528
   li li. V., 2020, IEEE Trans. Image Process, V29, P6277, DOI 10.110/TIP:2020.2989576
   Liu JY, 2010, IEEE T CIRC SYST VID, V20, P967, DOI 10.1109/TCSVT.2010.2045924
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Liu S., 2022, IEEE Trans. Multimedia, carly access, DOI [10.1109/TMM2022.3167810, DOI 10.1109/TMM2022.3167810]
   Luchke D., 2002, Level of Detail for 3D Graphics
   Mammou K., 2018, Standard ISO/IEC JTCI/SC29/WG7 MPEG 3D Graphics Cod-ing, M43781
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   Ribas-Corbera J, 1998, J ELECTRON IMAGING, V7, P155, DOI 10.1117/1.482636
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sheng XH, 2022, IEEE T MULTIMEDIA, V24, P2617, DOI 10.1109/TMM.2021.3086711
   Sim JY, 2008, IEEE T MULTIMEDIA, V10, P305, DOI 10.1109/TMM.2008.917349
   Wan S., 2014, New Eijfficient Video Coding 11.265/HEVC: Principle, Standand and Implementation
   Yuan H., 2021, StandardISO/IEC JTC/SC29/WG7 MPEG 3D Graphics Coding, M55859
   Yuan H., 2020, Standard ISO/IECITCI/SC29/WG7 MPEG 30 Graphics Coding, M55035
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhao TS, 2016, IEEE T IMAGE PROCESS, V25, P2997, DOI 10.1109/TIP.2016.2556941
   Zhou ML, 2023, IEEE T IMAGE PROCESS, V32, P219, DOI 10.1109/TIP.2022.3224876
NR 40
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3317
EP 3330
DI 10.1109/TMM.2023.3309550
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200014
DA 2024-08-05
ER

PT J
AU Wu, WH
   Yi, WF
   Li, JH
   Chen, MY
   Zheng, XP
AF Wu, Wenhan
   Yi, Wenfeng
   Li, Jinghai
   Chen, Maoyin
   Zheng, Xiaoping
TI Automatic Identification of Human Subgroups in Time-Dependent Pedestrian
   Flow Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video analysis; human subgroups; automatic identification; network
   science; crowd behavior
ID COMMUNITY STRUCTURE
AB The study of identifying human subgroups from videos is a significant topic, which has received a lot of attention in multiple disciplines. So far, however, there has been little consideration about combining it with relevant conceptions in network science. Therefore, this article proposes a novel method for the automatic identification of human subgroups in dynamic pedestrian flows. The spatial proximity and temporal continuity are combined to calculate the interaction intensity between pedestrians, by which a time-dependent pedestrian flow network is constructed. Based on the objective function of weighted partition density, the optimal threshold is used to determine community structures that correspond to human subgroups in frame images. Numerical experiments demonstrate that our method achieves high identification accuracy under various evaluation datasets, and exhibits better performance than existing methods in terms of different crowd densities, various numbers of subgroup members, and certain levels of trajectory noise. Furthermore, this work provides valuable implications for the understanding of subgroup behaviors and the modeling of subgroup movements.
C1 [Wu, Wenhan; Yi, Wenfeng; Chen, Maoyin; Zheng, Xiaoping] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Dept Automat, Beijing 100084, Peoples R China.
   [Li, Jinghai] Beijing Univ Chem Technol, Sch Mech & Elect Engn, Beijing 100029, Peoples R China.
C3 Tsinghua University; Beijing University of Chemical Technology
RP Zheng, XP (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Dept Automat, Beijing 100084, Peoples R China.
EM wwh19@mails.tsinghua.edu.cn; ywf19@mails.tsinghua.edu.cn;
   ljhai725@163.com; mychen@mail.tsinghua.edu.cn;
   asean@mail.tsinghua.edu.cn
RI Wu, Wenhan/IAO-6875-2023
OI Wu, Wenhan/0000-0003-4487-9283; Xiaoping, Zheng/0000-0002-2935-9514
FU National Major Scientific Research Instrument Development Project
FX No Statement Available
CR Ahn YY, 2010, NATURE, V466, P761, DOI 10.1038/nature09182
   Bandini S, 2014, PATTERN RECOGN LETT, V44, P16, DOI 10.1016/j.patrec.2013.10.003
   BROWN RG, 1961, OPER RES, V9, P672
   Calovi DS, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005933
   CAMPBELL DT, 1958, BEHAV SCI, V3, P14, DOI 10.1002/bs.3830030103
   Clauset A, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066111
   DELAMATER J, 1974, SMALL GROUP BEHAV, V5, P30, DOI 10.1177/104649647400500103
   Dong HR, 2020, IEEE T INTELL TRANSP, V21, P1849, DOI 10.1109/TITS.2019.2915014
   Du H, 2018, IEEE T MOBILE COMPUT, V17, P884, DOI 10.1109/TMC.2017.2694839
   Escobedo R, 2020, PHILOS T R SOC B, V375, DOI 10.1098/rstb.2019.0380
   Feng Y, 2021, BUILD ENVIRON, V187, DOI 10.1016/j.buildenv.2020.107329
   Fortunato S, 2007, P NATL ACAD SCI USA, V104, P36, DOI 10.1073/pnas.0605965104
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   Helbing D, 2005, TRANSPORT SCI, V39, P1, DOI 10.1287/trsc.1040.0108
   James J, 1953, AM SOCIOL REV, V18, P569, DOI 10.2307/2087444
   Javed MA, 2018, J NETW COMPUT APPL, V108, P87, DOI 10.1016/j.jnca.2018.02.011
   Karamouzas I, 2012, IEEE T VIS COMPUT GR, V18, P394, DOI 10.1109/TVCG.2011.133
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li A, 2017, SCIENCE, V358, P1042, DOI 10.1126/science.aai7488
   Li M, 2023, TRANSPORTMETRICA A, V19, DOI 10.1080/23249935.2021.1976877
   Lu LL, 2017, TRANSPORT RES C-EMER, V81, P317, DOI 10.1016/j.trc.2016.08.018
   Mankad S, 2013, PHYS REV E, V88, DOI 10.1103/PhysRevE.88.042812
   MCPHAIL C, 1982, SOCIOL METHOD RES, V10, P347, DOI 10.1177/0049124182010003007
   Moussaïd M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010047
   Moussaïd M, 2009, P ROY SOC B-BIOL SCI, V276, P2755, DOI 10.1098/rspb.2009.0405
   Newman MEJ, 2013, PHYS REV E, V88, DOI 10.1103/PhysRevE.88.042822
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.026113
   Nicolas A, 2023, TRANSPORTMETRICA A, V19, DOI 10.1080/23249935.2021.1970651
   Palla G, 2005, NATURE, V435, P814, DOI 10.1038/nature03607
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Qin Z, 2016, IEEE T PATTERN ANAL, V38, P2082, DOI 10.1109/TPAMI.2015.2505292
   REICHER SD, 1984, EUR J SOC PSYCHOL, V14, P1, DOI 10.1002/ejsp.2420140102
   Shen HW, 2009, PHYSICA A, V388, P1706, DOI 10.1016/j.physa.2008.12.021
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shi XM, 2018, J ADV TRANSPORT, DOI 10.1155/2018/1063043
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Singh H, 2009, APPL MATH MODEL, V33, P4408, DOI 10.1016/j.apm.2009.03.020
   Solera F, 2016, IEEE T PATTERN ANAL, V38, P995, DOI 10.1109/TPAMI.2015.2470658
   Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725
   Do T, 2016, TRANSPORT RES REC, P13, DOI 10.3141/2540-02
   Turner J. C., 1985, ADV GROUP PROCESSES, V2, P72
   Wu L, 2020, IEEE ACCESS, V8, P96016, DOI 10.1109/ACCESS.2020.2996001
   Wu YP, 2018, IEEE T MULTIMEDIA, V20, P1418, DOI 10.1109/TMM.2017.2771477
   Yang FK, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P313, DOI 10.1145/3267851.3267877
   Yang TB, 2011, MACH LEARN, V82, P157, DOI 10.1007/s10994-010-5214-7
   You L, 2016, PHYS LETT A, V380, P3340, DOI 10.1016/j.physleta.2016.08.012
   Yücel Z, 2013, SENSORS-BASEL, V13, P875, DOI 10.3390/s130100875
   Zaki MH, 2018, IEEE T INTELL TRANSP, V19, P1880, DOI 10.1109/TITS.2017.2747516
   Zanlungo F, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0187253
   Zanlungo F, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.062810
   Zhou C, 2019, NAT HUM BEHAV, V3, P847, DOI 10.1038/s41562-019-0618-2
NR 53
TC 4
Z9 4
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 166
EP 177
DI 10.1109/TMM.2023.3262975
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500007
DA 2024-08-05
ER

PT J
AU Wu, YX
   Zhao, GS
   Li, MD
   Zhang, ZC
   Qian, XM
AF Wu, Yuxia
   Zhao, Guoshuai
   Li, Mingdi
   Zhang, Zhuocheng
   Qian, Xueming
TI Reason Generation for Point of Interest Recommendation Via a
   Hierarchical Attention-Based Transformer Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Explainable recommendation; natural language generation;
   personalization; recommender system
ID MECHANISMS
AB Existing point-of-interest (POI) recommendation methods only show the direct recommendation results and lack the proper reasons for recommendation. In recent years, explainable recommendation has become an increasingly important subfield in recommendation systems. The aim of explainable recommendation is to provide a reason why an item is recommended to a user. In this way, it helps to improve the transparency, persuasiveness and user satisfaction of recommendation systems. The explainable recommendation should indicate users' preferences for POIs, such as the category and the price. In addition, to increase the diversity of the results, we take emotional intensity into account in our model to generate more vivid reasons. To this end, we propose a hierarchical attention-based transformer model to generate reasons with specific topics and different emotions. With a hierarchical attention mechanism, we can capture the word-level and attribute-level preferences of users. In addition, we also learn the latent representation of the emotion score to generate diverse recommendation reasons. We evaluate the proposed model on a new real-world dataset collected from three travel service websites. The experimental results demonstrate that our method outperforms the related approaches for reason generation.
C1 [Wu, Yuxia; Li, Mingdi; Zhang, Zhuocheng] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Zhao, Guoshuai] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University; Xi'an Jiaotong University
RP Zhao, GS (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
EM wuyuxia@stu.xjtu.edu.cn; guoshuai.zhao@xjtu.edu.cn;
   limingdi@stu.xjtu.edu.cn; zhuocheng_zhang@163.com;
   qianxm@mail.xjtu.edu.cn
OI , Yuxia Wu/0000-0003-3873-3982
FU National Natural Science Foundation of China
FX No Statement Available
CR Aksenov P., 2014, Procedia Environ. Sci., V22, P257
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Belinkov Y, 2019, T ASSOC COMPUT LING, V7, P49, DOI 10.1162/tacl_a_00254
   Bengio S, 2015, ADV NEUR IN, V28
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bowman S.R., 2016, P 20 SIGNLL C COMPUT, P10, DOI [10.18653/v1/K16-1002, DOI 10.18653/V1/K16-1002]
   Chen C, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1583, DOI 10.1145/3178876.3186070
   Chen Yen-Chun, 2020, P 58 ANN M ASS COMP, P7893, DOI DOI 10.18653/V1/2020.ACL-MAIN.705
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Doulamis N., 2013, IEEE 14THINT S A WOR, P1
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   Gehring J, 2017, PR MACH LEARN RES, V70
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo Daya, 2020, P 58 ANN M ASS COMP, P6118
   Hao JM, 2021, IEEE T MULTIMEDIA, V24, P3381, DOI 10.1109/TMM.2021.3097186
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He X., 2020, INT C LEARN REPRESEN, P1
   Hu ZT, 2017, P MACHINE LEARNING R, V70
   Huang Chu-Ren., 2007, Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, P69
   Islam MA, 2022, NEUROCOMPUTING, V472, P306, DOI 10.1016/j.neucom.2021.05.114
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Li PJ, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P345, DOI 10.1145/3077136.3080822
   Li XQ, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P122, DOI 10.1145/3366423.3380100
   Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P8425
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Nallapati R, 2017, AAAI CONF ARTIF INTE, P3075
   Padia P, 2019, IEEE INT CONF BIG DA, P900, DOI 10.1109/BigData47090.2019.9006442
   Parikh A., 2016, P 2016 C EMP METH NA, P2249, DOI DOI 10.18653/V1/D16-1244
   Semeniuta S., 2017, P 2017 C EMP METH NA, P627
   Shaw P., 2018, Self-attention with relative position representations, P464
   Sun K, 2020, AAAI CONF ARTIF INTE, V34, P214
   Sun PJ, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P837, DOI 10.1145/3366423.3380164
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XY, 2015, IEEE T MULTIMEDIA, V17, P409, DOI 10.1109/TMM.2014.2385473
   Webster J.J., 1992, Proceedings of the 14th Conference on Computational Linguistics-Volume, V4, P1106, DOI [10.3115/992424.992434, DOI 10.3115/992424.992434]
   Wu YX, 2022, IEEE T KNOWL DATA EN, V34, P1944, DOI 10.1109/TKDE.2020.3002531
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu ZX, 2017, IEEE T MULTIMEDIA, V19, P1933, DOI 10.1109/TMM.2017.2688928
   Yiakoumettis C, 2014, GEOINFORMATICA, V18, P27, DOI 10.1007/s10707-013-0176-0
   Yin HY, 2020, AAAI CONF ARTIF INTE, V34, P9466
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu A. W., 2018, PROC INT C LEARN REP, P1
   Yu L, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI 10.14722/ndss.2017.23241
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2762, DOI 10.1109/TMM.2019.2912124
   Zhang T., 2019, INT C LEARN REPRE SE, P1
   Zhang YF, 2020, FOUND TRENDS INF RET, V14, P1, DOI 10.1561/1500000066
   Zhao GS, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3337967
   Zhao GS, 2019, IEEE T MULTIMEDIA, V21, P771, DOI 10.1109/TMM.2018.2863598
   Zhao KZ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3216
   Zhao PP, 2021, IEEE T KNOWL DATA EN, V33, P1708, DOI 10.1109/TKDE.2019.2943854
   Zhao YB, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P732
   Zheng XZ, 2023, IEEE T KNOWL DATA EN, V35, P5400, DOI 10.1109/TKDE.2022.3148485
   Zhou YP, 2018, IEEE T MULTIMEDIA, V20, P2153, DOI 10.1109/TMM.2017.2781364
   Zhu JH, 2023, IEEE T KNOWL DATA EN, V35, P4940, DOI 10.1109/TKDE.2022.3146178
NR 57
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5511
EP 5522
DI 10.1109/TMM.2023.3335886
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600007
DA 2024-08-05
ER

PT J
AU Ye, XC
   Guo, YJ
   Sun, BL
   Xu, R
   Wang, ZH
   Li, HJ
AF Ye, Xinchen
   Guo, Yanjun
   Sun, Baoli
   Xu, Rui
   Wang, Zhihui
   Li, Haojie
TI C<SUP>2</SUP>ANet: Cross-Scale and Cross-Modality Aggregation Network
   for Scene Depth Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color; Image color analysis; Task analysis; Feature extraction; Image
   reconstruction; Spatial resolution; Aggregates; Alignment;
   cross-attention; cross-modality; cross-scale; depth super-resolution
AB Existing depth super-resolution (DSR) methods typically utilize an additional high-resolution (HR) color image of the same scene as assistance to recover the low-resolution (LR) depth map. Although these color-guided methods have achieved impressive progress, they easily face with color image under-utilization and misutilization issues. In this article, we deeply investigate the above problems and further propose a novel DSR framework to alleviate them. Specifically, we propose a Cross-scale and Cross-modality Aggregation Network (C(2)ANet) to learn abundant and accurate complementarity from color images to help recover the degraded depth map. Our C-2 ANet can simultaneously extract multi-scale representations from color images with parallel network hierarchies, and effectively aggregate cross-scale and cross-modality contexts to boost HR representations in each hierarchy. Then, to appropriately use the guided color image, we further design a Feature Aggregation Module ( FAM ) to adaptively select and fuse task-relevant features, which consists of (1) a feature alignment block to learn transformation offsets and align upsampled features with targeted HR features, and (2) a feature fusion block based on cross-attention mechanism to maintain strong structural context and suppress texture distraction. Experimental results on synthetic and real-world benchmark datasets demonstrate the superiority of our proposed method in comparison with other state-of-the-art DSR methods.
C1 [Ye, Xinchen; Guo, Yanjun; Sun, Baoli; Xu, Rui; Wang, Zhihui] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian 116024, Peoples R China.
   [Ye, Xinchen; Guo, Yanjun; Sun, Baoli; Xu, Rui; Wang, Zhihui] Key Lab Ubiquitous Network & Serv Software Liaonin, Dalian 116620, Peoples R China.
   [Li, Haojie] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266590, Shandong, Peoples R China.
C3 Dalian University of Technology; Shandong University of Science &
   Technology
RP Li, HJ (corresponding author), Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266590, Shandong, Peoples R China.
EM yexch@dlut.edu.cn; guoyanjun@mail.dlut.edu.cn; baoli@mail.dlut.edu.cn;
   xurui@dlut.edu.cn; zhwang@dlut.edu.cn; hjli@dlut.edu.cn
OI Sun, Baoli/0000-0002-2861-4288; Guo, Yanjun/0000-0003-3357-8872; wang,
   zhihui/0000-0002-5011-9726; Xu, Rui/0000-0003-0516-3629
FU National Natural Science Foundation of China
FX No Statement Available
CR Arif S, 2019, J INF SCI ENG, V35, P851, DOI 10.6688/JISE.201907_35(4).0009
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Doersch C., 2020, NeurIPS, V33, P21981
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LZ, 2021, PROC CVPR IEEE, P9225, DOI 10.1109/CVPR46437.2021.00911
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Huang YX, 2022, PROC CVPR IEEE, P5921, DOI 10.1109/CVPR52688.2022.00584
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Ke L, 2021, ADV NEUR IN, V34
   Keighrey C, 2021, IEEE T MULTIMEDIA, V23, P333, DOI 10.1109/TMM.2020.2982046
   Kim B, 2019, Arxiv, DOI arXiv:1903.11286
   Kim B, 2021, INT J COMPUT VISION, V129, P579, DOI 10.1007/s11263-020-01386-z
   Li GY, 2022, PROC CVPR IEEE, P20604, DOI 10.1109/CVPR52688.2022.01998
   Li YJ, 2019, IEEE T PATTERN ANAL, V41, P1909, DOI 10.1109/TPAMI.2018.2890623
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Lin JR, 2022, IEEE T MULTIMEDIA, V24, P1707, DOI 10.1109/TMM.2021.3070106
   Liu J, 2022, IEEE T MULTIMEDIA, V24, P4212, DOI 10.1109/TMM.2021.3115039
   Reading C, 2021, PROC CVPR IEEE, P8551, DOI 10.1109/CVPR46437.2021.00845
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song XB, 2020, PROC CVPR IEEE, P5630, DOI 10.1109/CVPR42600.2020.00567
   Su YZ, 2022, BIOMED SIGNAL PROCES, V78, DOI 10.1016/j.bspc.2022.103903
   Sun BL, 2021, PROC CVPR IEEE, P7788, DOI 10.1109/CVPR46437.2021.00770
   Sun K, 2019, Arxiv, DOI [arXiv:1904.04514, DOI 10.48550/ARXIV.1904.04514]
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tang JX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4390, DOI 10.1145/3474085.3475584
   Tang Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2148, DOI 10.1145/3474085.3475373
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Ye XC, 2020, IEEE T IMAGE PROCESS, V29, P7427, DOI 10.1109/TIP.2020.3002664
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Zhao ZX, 2022, PROC CVPR IEEE, P5687, DOI 10.1109/CVPR52688.2022.00561
   Zhong ZW, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3584860
   Zhong ZW, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3253472
   Zhong ZW, 2022, IEEE T IMAGE PROCESS, V31, P648, DOI 10.1109/TIP.2021.3131041
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 40
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2574
EP 2584
DI 10.1109/TMM.2023.3301240
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400001
DA 2024-08-05
ER

PT J
AU Yu, Q
   Irie, G
   Aizawa, K
AF Yu, Qing
   Irie, Go
   Aizawa, Kiyoharu
TI Self-Labeling Framework for Open-Set Domain Adaptation With Few Labeled
   Samples
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Mutual information; Prototypes; Annotations; Training;
   Target recognition; Self-supervised learning; Cross-domain learning;
   domain adaptation; open-set recognition
ID ALIGNMENT
AB Unsupervised domain adaptation (UDA) is extremely effective for transferring knowledge from a label-rich source domain to a label-scarce target domain. Because the target domain is unlabeled and may contain additional novel classes, open-set domain adaptation (ODA) has been suggested as a possible solution to detect these novel classes in the training phase. However, existing ODA methods rely heavily on abundant fully labeled source data, which are expensive to collect in specific applications and may also contain novel classes. In this study, we propose a novel self-labeling framework with prototypical contrastive learning and mutual information maximization to achieve ODA even when the amount of labeled data is very small, which is a new problem setting named few-shot ODA (FODA). We use self-supervised prototypical contrastive learning to train the network to learn the representations of source and target samples and maximize the mutual information between labels and input data to simultaneously recognize known and novel classes in the source and target domains. We evaluated our strategy in several domain adaptation environments and found that our method performed far better than existing approaches.
C1 [Yu, Qing; Aizawa, Kiyoharu] Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138656, Japan.
   [Irie, Go] Tokyo Univ Sci, Dept Informat & Comp Technol, Tokyo 1258585, Japan.
C3 University of Tokyo; Tokyo University of Science
RP Yu, Q (corresponding author), Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138656, Japan.
EM yu@hal.t.u-tokyo.ac.jp; goirie@ieee.org; aizawa@hal.t.u-tokyo.ac.jp
OI Irie, Go/0000-0002-4309-4700; Yu, Qing/0000-0001-6965-9581
FU JST AIP Acceleration Research
FX No Statement Available
CR Asano Y., 2019, P INT C LEARN REPR
   Bachman P, 2019, ADV NEUR IN, V32
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Bucci Silvia, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P422, DOI 10.1007/978-3-030-58517-4_25
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Grandvalet Y, 2004, Proceedings of NIPS, V17
   Grill JB, 2020, P 34 INT C NEUR INF, V33, P21271
   Han K., 2020, INT C LEARN REPR ICL
   Han K, 2019, IEEE I CONF COMP VIS, P8400, DOI 10.1109/ICCV.2019.00849
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hjelm R. D., 2019, ICLR
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kim D, 2020, Arxiv, DOI arXiv:2003.08264
   Li J., 2021, P INT C LEARN REPR
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li XH, 2022, LECT NOTES COMPUT SC, V13694, P1, DOI 10.1007/978-3-031-19830-4_1
   Liu H, 2019, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2019.00304
   Long MS, 2018, ADV NEUR IN, V31
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Perone CS, 2019, NEUROIMAGE, V194, P1, DOI 10.1016/j.neuroimage.2019.03.026
   Qu SQ, 2023, PROC CVPR IEEE, P20019, DOI 10.1109/CVPR52729.2023.01917
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K., 2020, Advances in Neural Information Processing Systems, V33, P16282
   Saito K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8980, DOI 10.1109/ICCV48922.2021.00887
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Snell J., 2017, Advances in Neural Information Processing Systems, V30, P4077
   Taigman Yaniv, 2017, INT C LEARN REPR
   Tschannen M., 2017, P INT C LEARN REPR
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang P, 2023, IEEE T MULTIMEDIA, V25, P6026, DOI 10.1109/TMM.2022.3203574
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Yu Q, 2022, AAAI CONF ARTIF INTE, P3161
   Yue XY, 2021, PROC CVPR IEEE, P13829, DOI 10.1109/CVPR46437.2021.01362
   Zhan XH, 2020, PROC CVPR IEEE, P6687, DOI 10.1109/CVPR42600.2020.00672
   Zhuo JB, 2023, IEEE T MULTIMEDIA, V25, P6157, DOI 10.1109/TMM.2022.3205457
NR 62
TC 0
Z9 0
U1 17
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1474
EP 1487
DI 10.1109/TMM.2023.3282538
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700005
DA 2024-08-05
ER

PT J
AU Zeng, HM
   Huang, J
   Li, JC
   Xiong, ZW
AF Zeng, Huimin
   Huang, Jie
   Li, Jiacheng
   Xiong, Zhiwei
TI Region-Aware Portrait Retouching With Sparse Interactive Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Portrait retouching; image editing; user interaction
ID IMAGE QUALITY ASSESSMENT
AB Portrait retouching aims to improve the aesthetic quality of input portrait photos and especially requires human-region priority. The deep learning-based methods largely elevate the retouching efficiency and provide promising retouched results. However, existing portrait retouching methods focus on automatic retouching, which treats all human-regions equally and ignores users' preferences for specific individuals, thus suffering from limited flexibility in interactive scenarios. In this work, we emphasize the importance of users' intents and explore the interactive portrait retouching task. Specifically, we propose a region-aware retouching framework with two branches: an automatic branch and an interactive branch. The automatic branch involves an encoding-decoding process, which searches region candidates and performs automatic region-aware retouching without user guidance. The interactive branch encodes sparse user guidance into a priority condition vector and modulates latent features with a region selection module to further emphasize the user-specified regions. Experimental results show that our interactive branch effectively captures users' intents and generalizes well to unseen scenes with sparse user guidance, while our automatic branch also outperforms the state-of-the-art retouching methods due to improved region-awareness.
C1 [Zeng, Huimin; Huang, Jie; Li, Jiacheng; Xiong, Zhiwei] Univ Sci & Technol China, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xiong, ZW (corresponding author), Univ Sci & Technol China, Hefei 230026, Peoples R China.
EM zenghuimin@mail.ustc.edu.cn; hj0117@mail.ustc.edu.cn;
   jclee@mail.ustc.edu.cn; zwxiong@ustc.edu.cn
RI Zeng, Huimin/AAJ-7592-2021
OI Li, Jiacheng/0000-0002-4215-6754; Zeng, Huimin/0009-0002-7101-0495;
   Huang, Jie/0000-0002-3518-3404
FU National Natural Science Foundation of China
FX No Statement Available
CR Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chai Y, 2020, IEEE WINT CONF APPL, P981, DOI 10.1109/WACV45572.2020.9093321
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Deng YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P870, DOI 10.1145/3240508.3240531
   Gao YT, 2023, IEEE T MULTIMEDIA, V25, P3190, DOI 10.1109/TMM.2022.3156812
   Ge SM, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3536426
   Ge SM, 2020, IEEE T CIRC SYST VID, V30, P3387, DOI 10.1109/TCSVT.2020.2967754
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Ge SM, 2017, PROC CVPR IEEE, P426, DOI 10.1109/CVPR.2017.53
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo Z., 2021, P IEEE CVF INT C COM, P14870
   Han-Ul Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P339, DOI 10.1007/978-3-030-58595-2_21
   Han-Ul Kim, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P374, DOI 10.1007/978-3-030-58577-8_23
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P4002, DOI 10.1109/TMM.2021.3111501
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Jingwen He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P679, DOI 10.1007/978-3-030-58601-0_40
   Kim H., 2021, P IEEECVF INT C COMP, P4459
   Li CY, 2020, Arxiv, DOI arXiv:2010.13412
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liang J, 2021, PROC CVPR IEEE, P653, DOI 10.1109/CVPR46437.2021.00071
   Liang J, 2021, PROC CVPR IEEE, P9387, DOI 10.1109/CVPR46437.2021.00927
   Lin J, 2021, PROC CVPR IEEE, P14981, DOI 10.1109/CVPR46437.2021.01474
   Ling H., 2021, Advances in Neural Information Processing Systems (NIPS), P16331
   Ling J, 2021, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR46437.2021.00924
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy A. K., 2009, IEEE Signal Process. Lett, V17, P7
   Moran Sean, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12823, DOI 10.1109/CVPR42600.2020.01284
   Moran S, 2021, INT C PATT RECOG, P9796, DOI 10.1109/ICPR48806.2021.9412677
   Ni ZK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1697, DOI 10.1145/3394171.3413839
   Ni ZK, 2020, IEEE T IMAGE PROCESS, V29, P9140, DOI 10.1109/TIP.2020.3023615
   Santos MS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392403
   Serra D., 2018, International Conference on Machine Learning, P4548
   Sofiiuk K, 2022, IEEE IMAGE PROC, P3141, DOI 10.1109/ICIP46576.2022.9897365
   Song Y., 2021, P IEEE CVF INT C COM, P4126
   Tang H, 2022, IEEE T MULTIMEDIA, V24, P2963, DOI 10.1109/TMM.2021.3091847
   Tsai YH, 2017, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2017.299
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2245, DOI 10.1109/TMM.2021.3087026
   Wang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2451, DOI 10.1109/ICCV48922.2021.00247
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xin Tong, 2020, 2020 International Conference on Computer Vision, Image and Deep Learning (CVIDL), P265, DOI 10.1109/CVIDL51233.2020.00-88
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang R., 2017, ACM Transactions on Graphics (TOG), V9
   Zhang YB, 2020, IEEE T IMAGE PROCESS, V29, P1101, DOI 10.1109/TIP.2019.2938347
   Zhao NX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447647
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
NR 61
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 127
EP 140
DI 10.1109/TMM.2023.3262185
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500011
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, K
   Jiang, HL
   Zhang, J
   Huang, QM
   Fan, JP
   Yu, J
   Han, WD
AF Zhang, Ke
   Jiang, Hanliang
   Zhang, Jian
   Huang, Qingming
   Fan, Jianping
   Yu, Jun
   Han, Weidong
TI Semi-Supervised Medical Report Generation via Graph-Guided Hybrid
   Feature Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge graph; mean teacher; medical report generation;
   semi-supervised learning
AB Medical report generation generates the corresponding report according to the given radiology image, which has been attracting increasing research interest. However, existing methods mainly adopt supervised training which rely on large amount of medical reports that are actually unavailable owing to the labor-intensive labeling process and privacy protection protocol. In the meanwhile, the intrinsic relationships between local pathological changes in the image are often ignored, which actually are important hints to high quality report generation. To this end, we propose a Relation-Aware Mean Teacher (RAMT) framework, which follows a standard mean teacher paradigm for semi-supervised report generation. The key to the encoder of the backbone network is the Graph-guided Hybrid Feature Encoding (GHFE) module, which exploits a prior disease knowledge graph to encode the intrinsic relations between pathological changes into the graph embedding and learns a word dictionary to retrieve the semantic embedding for each potential pathological change. GHFE combines the graph embedding, semantic embedding and visual features to form hybrid features, which are sent to a Transformer-based decoder for report generation. Extensive experiments on the MIMIC-CXR and IU X-Ray datasets demonstrate the effectiveness of our proposed approach.
C1 [Zhang, Ke; Yu, Jun] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.
   [Jiang, Hanliang] Zhejiang Univ, Sir Run Run Shaw Hosp, Natl Inst Resp Dis, Coll Med,Reg Med Ctr, Hangzhou 310016, Peoples R China.
   [Zhang, Jian] Hangzhou Normal Univ, Sch Informat Sci & Technol, Hangzhou 310030, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Fan, Jianping] Lenovo Res, AI Lab, Beijing 100094, Peoples R China.
   [Han, Weidong] Zhejiang Univ, Sir Run Run Shaw Hosp, Coll Med, Dept Med Oncol, Hangzhou 310016, Peoples R China.
   [Han, Weidong] Zhejiang Normal Univ, Coll Math Med, Jinhua 321017, Peoples R China.
C3 Hangzhou Dianzi University; Zhejiang University; Hangzhou Normal
   University; Chinese Academy of Sciences; University of Chinese Academy
   of Sciences, CAS; Legend Holdings; Lenovo; Zhejiang University; Zhejiang
   Normal University
RP Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.; Han, WD (corresponding author), Zhejiang Univ, Sir Run Run Shaw Hosp, Coll Med, Dept Med Oncol, Hangzhou 310016, Peoples R China.
EM ke.zhang@hdu.edu.cn; aock@zju.edu.cn; jeyzhang@outlook.com;
   qmhuang@ucas.ac.cn; jfan1@lenovo.com; yujun@hdu.edu.cn; hanwd@zju.edu.cn
RI ZHANG, KE/ABG-3727-2022; Huang, Qingming/GLR-3473-2022
OI Huang, Qingming/0000-0002-3025-7099; Han, Weidong/0000-0001-7227-3671;
   Fan, Jianping/0000-0003-2290-1785; Zhang, Jian/0000-0001-6478-9192;
   Zhang, Ke/0000-0002-9855-003X
FU National Natural Science Foundation of China
FX No Statement Available
CR Aila T, 2017, INT C LEARN REPR
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Aviles-Rivero AI, 2019, LECT NOTES COMPUT SC, V11769, P504, DOI 10.1007/978-3-030-32226-7_56
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Bannur S, 2023, Arxiv, DOI [arXiv:2301.04558, 10.48550/arXiv.2301.04558, DOI 10.48550/ARXIV.2301.04558]
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chartsias A, 2018, LECT NOTES COMPUT SC, V11071, P490, DOI 10.1007/978-3-030-00934-2_55
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen Z., 2021, Long Papers, P5904
   Chen ZH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1439
   Delrue L., 2011, Comparative Interpretation of CT and Standard Radiography of the Chest
   Demner-Fushman D, 2016, J AM MED INFORM ASSN, V23, P304, DOI 10.1093/jamia/ocv080
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Johnson AEW, 2019, Arxiv, DOI arXiv:1901.07042
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang QB, 2022, IEEE T MULTIMEDIA, V24, P2004, DOI 10.1109/TMM.2021.3074803
   Huang SC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3922, DOI 10.1109/ICCV48922.2021.00391
   Irvin J, 2019, AAAI CONF ARTIF INTE, P590
   Jing BY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6570
   Jing BY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2577
   Li CY, 2019, AAAI CONF ARTIF INTE, P6666
   Li CY, 2018, 32 C NEURAL INFORM P
   Li XM, 2021, IEEE T NEUR NET LEAR, V32, P523, DOI 10.1109/TNNLS.2020.2995319
   Liu FL, 2021, PROC CVPR IEEE, P13748, DOI 10.1109/CVPR46437.2021.01354
   Liu QD, 2020, IEEE T MED IMAGING, V39, P3429, DOI 10.1109/TMI.2020.2995518
   Luo XD, 2021, AAAI CONF ARTIF INTE, V35, P8801
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qin H, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P448
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wang D, 2020, PROC CVPR IEEE, P3950, DOI 10.1109/CVPR42600.2020.00401
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XS, 2018, PROC CVPR IEEE, P9049, DOI 10.1109/CVPR.2018.00943
   Xue Y, 2018, LECT NOTES COMPUT SC, V11070, P457, DOI 10.1007/978-3-030-00928-1_52
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Yuan JB, 2019, LECT NOTES COMPUT SC, V11769, P721, DOI 10.1007/978-3-030-32226-7_80
   Zhang Y., 2022, P 7 MACHINE LEARNING, P2
   Zhang YX, 2020, AAAI CONF ARTIF INTE, V34, P12910
   Zhang ZJ, 2022, IEEE T MULTIMEDIA, V24, P3101, DOI 10.1109/TMM.2021.3093725
   Zhou HY, 2023, Arxiv, DOI arXiv:2301.13155
   Zhou HY, 2022, NAT MACH INTELL, V4, P32, DOI 10.1038/s42256-021-00425-9
NR 43
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 904
EP 915
DI 10.1109/TMM.2023.3273390
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700052
DA 2024-08-05
ER

PT J
AU Zhang, YZ
   Zhang, T
   Wu, CY
   Tao, R
AF Zhang, Yunzuo
   Zhang, Tian
   Wu, Cunyu
   Tao, Ran
TI Multi-Scale Spatiotemporal Feature Fusion Network for Video Saliency
   Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video saliency prediction; multi-scale spatiotemporal features; feature
   fusion; attention mechanism
ID NEURAL-NETWORK; MODEL
AB Recently, video saliency prediction has attracted increasing attention, yet the improvement of its accuracy is still subject to the insufficient use of multi-scale spatiotemporal features. To address this issue, we propose a 3D convolutional Multi-scale Spatiotemporal Feature Fusion Network (MSFF-Net) to achieve the full utilization of spatiotemporal features. Specifically, we propose a Bi-directional Temporal-Spatial Feature Pyramid (BiTSFP), the first application of bi-directional fusion architectures in this field, which adds the flow of shallow location information on the basis of the previous flow of deep semantic information. Then, different from simple addition and concatenation, we design an Attention-Guided Fusion (AGF) mechanism that can adaptively learn the fusion weights of adjacent features to integrate them appropriately. Moreover, a Frame-wise Attention (FA) module is introduced to selectively emphasize the useful frames, augmenting the multi-scale temporal features to be fused. Our model is simple but effective, and it can run in real-time. Experimental results on the DHF1K, Hollywood-2, and UCF-sports datasets demonstrate that the proposed MSFF-Net outperforms existing state-of-the-art methods in accuracy.
C1 [Zhang, Yunzuo; Zhang, Tian; Wu, Cunyu] Shijiazhuang Tiedao Univ, Shijiazhuang 050043, Peoples R China.
   [Tao, Ran] Beijing Inst Technol, Beijing 100081, Peoples R China.
C3 Shijiazhuang Tiedao University; Beijing Institute of Technology
RP Zhang, YZ (corresponding author), Shijiazhuang Tiedao Univ, Shijiazhuang 050043, Peoples R China.
EM zhangyunzuo888@sina.com; zhangtian0809@sina.com; wucunyu1410@sina.com;
   rantao@bit.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Bellitto G, 2021, INT J COMPUT VISION, V129, P3216, DOI 10.1007/s11263-021-01519-y
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang QY, 2023, COGN COMPUT, V15, P856, DOI 10.1007/s12559-023-10114-x
   Chen JZ, 2021, NEUROCOMPUTING, V462, P59, DOI 10.1016/j.neucom.2021.07.088
   Chen J, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107615
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Gorji S, 2018, PROC CVPR IEEE, P7501, DOI 10.1109/CVPR.2018.00783
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hadizadeh H, 2021, IEEE T MULTIMEDIA, V23, P12, DOI 10.1109/TMM.2020.2975420
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jain S, 2021, IEEE INT C INT ROBOT, P3520, DOI 10.1109/IROS51168.2021.9635989
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Linardos P., 2019, PROC BRIT MACH VIS C, P1
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Ma C, 2022, IEEE T CIRC SYST VID, V32, P6850, DOI 10.1109/TCSVT.2022.3172971
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Mao YD, 2022, IEEE T MULTIMEDIA, V24, P2435, DOI 10.1109/TMM.2021.3081260
   Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Qiao ML, 2021, IEEE T MULTIMEDIA, V23, P748, DOI 10.1109/TMM.2020.2987682
   Qiu Y, 2023, IEEE T MULTIMEDIA, V25, P1991, DOI 10.1109/TMM.2022.3141933
   Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Souza LS, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107028
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tong YB, 2011, COGN COMPUT, V3, P241, DOI 10.1007/s12559-010-9094-8
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang ZQ, 2023, IEEE T MULTIMEDIA, V25, P1161, DOI 10.1109/TMM.2021.3139743
   Wu XY, 2020, AAAI CONF ARTIF INTE, V34, P12410
   Wu Z, 2019, IEEE T CIRC SYST VID, V29, P2960, DOI 10.1109/TCSVT.2018.2870954
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xiong JW, 2023, PROC CVPR IEEE, P6441, DOI 10.1109/CVPR52729.2023.00623
   Xue H, 2022, NEUROCOMPUTING, V468, P233, DOI 10.1016/j.neucom.2021.10.024
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yu Y, 2017, PROC CVPR IEEE, P6119, DOI 10.1109/CVPR.2017.648
   Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844
   Zhang K, 2021, IEEE T IMAGE PROCESS, V30, P572, DOI 10.1109/TIP.2020.3036749
   Zhang Y., 2023, PROC IEEE INT C ACOU, P1
   Zhao ZJ, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108120
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou XF, 2023, IEEE T CIRC SYST VID, V33, P7696, DOI 10.1109/TCSVT.2023.3278410
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhu SP, 2022, NEURAL COMPUT APPL, V34, P7955, DOI 10.1007/s00521-022-06895-1
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
   Zou WB, 2021, PATTERN RECOGN LETT, V147, P78, DOI 10.1016/j.patrec.2021.04.010
NR 62
TC 1
Z9 1
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4183
EP 4193
DI 10.1109/TMM.2023.3321394
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100057
DA 2024-08-05
ER

PT J
AU Bai, ZY
   Xu, HL
   Zhang, XY
   Ding, QC
AF Bai, Zhongyu
   Xu, Hongli
   Zhang, Xiangyue
   Ding, Qichuan
TI GCSANet: Arbitrary Style Transfer With Global Context Self-Attentional
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Semantics; Task analysis; Computational modeling;
   Context modeling; Transforms; Visualization; Arbitrary style transfer;
   Contrastive learning; Self-attentional model; Global contextual feature
ID IMAGE
AB Arbitrary style transfer is attracting increasing attention in the computer vision community due to its application flexibility. Existing approaches directly fuse deep style features with deep content features or adaptively normalize content features for global statistical matching. Although effective, it is prone to suffer from local unnatural outputs and artifacts owing to the lack of exploring the global contextual semantic distribution of style image features. In this article, a novel global context self-attentional network (GCSANet) is proposed to efficiently generate high-quality stylized results based on the global semantic spatial distributions of style images. First, a context modeling module is proposed to aggregate the depth features of style images into global context features. Then, channel-wise interdependencies are captured with the feature transform module. Finally, the style features are appropriately aggregated to each location of the content image. In addition, novel external contrastive losses are proposed to balance the distribution of content and style features to ensure the reasonableness of the texture patterns in the stylized images. The ablation studies validate the effectiveness of the proposed components. Various quantitative and qualitative experiments demonstrate the superiority of our method for real-time arbitrary image/video style transfer.
C1 [Bai, Zhongyu; Xu, Hongli; Zhang, Xiangyue; Ding, Qichuan] Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110819, Peoples R China.
C3 Northeastern University - China
RP Xu, HL (corresponding author), Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110819, Peoples R China.
EM zhongyubai@stumail.neu.edu.cn; xuhongli@mail.neu.edu.cn;
   zhangxiangyue@mail.neu.edu.cn; dingqichuan@mail.neu.edu.cn
RI Bai, Zhongyu/JRW-5846-2023
OI Bai, Zhongyu/0009-0006-7186-2873; ding, qi chuan/0000-0002-7221-4003
FU National Natural Science Foundation of China
FX No Statement Available
CR An J, 2021, PROC CVPR IEEE, P862, DOI 10.1109/CVPR46437.2021.00092
   An J, 2020, AAAI CONF ARTIF INTE, V34, P10443
   Baek Kyungjune, 2021, P IEEE CVF INT C COM, P14154
   Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen HB, 2021, ADV NEUR IN, V34
   Cui J, 2021, NEUROCOMPUTING, V465, P114, DOI 10.1016/j.neucom.2021.08.088
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gao LL, 2023, IEEE T MULTIMEDIA, V25, P7248, DOI 10.1109/TMM.2022.3219677
   Gao PC, 2021, IEEE T MULTIMEDIA, V23, P926, DOI 10.1109/TMM.2020.2991507
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han JL, 2021, IEEE COMPUT SOC CONF, P746, DOI 10.1109/CVPRW53098.2021.00084
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang YJ, 2022, IEEE T MULTIMEDIA, V24, P3978, DOI 10.1109/TMM.2021.3111515
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Li YJ, 2017, ADV NEUR IN, V30
   Lin J., 2020, EUR C COMP VIS, P18, DOI DOI 10.1007/978-3-030-58548-8_2
   Lin TW, 2021, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR46437.2021.00510
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S., 2022, arXiv
   Liu SG, 2022, IEEE T MULTIMEDIA, V24, P1299, DOI 10.1109/TMM.2021.3063605
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Luo X, 2022, Arxiv, DOI arXiv:2201.02233
   Ma YN, 2023, IEEE WINT CONF APPL, P331, DOI 10.1109/WACV56688.2023.00041
   Mao WD, 2023, IEEE T MULTIMEDIA, V25, P6485, DOI 10.1109/TMM.2022.3209870
   Mitheran JS, 2022, OPTIK, V267, DOI 10.1016/j.ijleo.2022.169656
   Mnih V, 2014, ADV NEUR IN, V27
   Mun H, 2022, IEEE T MULTIMEDIA, V24, P3823, DOI 10.1109/TMM.2021.3108401
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Phillips F, 2011, ISS ACCOUNT EDUC, V26, P593, DOI 10.2308/iace-50038
   Santa Cruz R, 2019, IEEE T PATTERN ANAL, V41, P3100, DOI 10.1109/TPAMI.2018.2873701
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh Aaditya, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), DOI 10.1109/ICME51207.2021.9428124
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Vaswani A, 2017, ADV NEUR IN, V30
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2245, DOI 10.1109/TMM.2021.3087026
   Wang H, 2020, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR42600.2020.00193
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao Y., 2020, Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, P800, DOI DOI 10.1007/978-3-030-58545-746
   Zhizhong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7786, DOI 10.1109/CVPR42600.2020.00781
   Zhu JC, 2023, IEEE T PATTERN ANAL, V45, P3311, DOI 10.1109/TPAMI.2022.3186752
NR 63
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1407
EP 1420
DI 10.1109/TMM.2023.3282489
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700014
DA 2024-08-05
ER

PT J
AU Cai, B
   Lu, GF
   Li, H
   Song, WH
AF Cai, Bing
   Lu, Gui-Fu
   Li, Hua
   Song, Weihong
TI Tensorized Scaled Simplex Representation for Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scaled simplex representation; low-rank tensor constraint; adaptive
   weights; multi-view clustering
ID SUBSPACE; ROBUST
AB Tensor-based multi-view clustering, which incorporates high-order correlations among views, has emerged as a promising research direction. These methods aim to capture intrinsic structure through a tensor-based constraint and then construct an affinity matrix. However, when constructing the affinity matrix, the negative entries in the coefficient matrices are forced to be positive via absolute operation, which can inadvertently destroy the inherent relationships within the data. Furthermore, existing methods may lack the flexibility to effectively handle and fuse multiple views. To address these issues, we propose a novel approach called Tensorized Scaled Simplex Representation (TSSR) for multi-view clustering. TSSR leverages a low-rank tensor constraint to capture the consensus and complementary information among the views. Besides, it introduces the scaled simplex representation, ensuring non-negative coefficient matrices, thus preserving inherent relationships and enhancing flexibility. Thirdly, TSSR extends the scaling range of the affine constraint to capture authentic structural information. Finally, an auto-weighted strategy assigns ideal weights to diverse views, enabling them to contribute appropriately. We integrate these techniques into a unified framework solved by an iterative algorithm. Experimental results demonstrate that TSSR outperforms state-of-the-art methods in terms of performance and efficiency.
C1 [Cai, Bing; Li, Hua; Song, Weihong] Anhui Inst Informat Technol, Sch Comp & Software Engn, Wuhu 241000, Peoples R China.
   [Cai, Bing; Lu, Gui-Fu] Anhui Polytech Univ, Sch Comp & Informat, Wuhu 241000, Peoples R China.
C3 Anhui Polytechnic University
RP Lu, GF (corresponding author), Anhui Polytech Univ, Sch Comp & Informat, Wuhu 241000, Peoples R China.
EM bingly@foxmail.com; lu-guifu@ahpu.edu.cn; 313110644@qq.com;
   weihong.song@qq.com
RI Cai, Bing/GRF-3461-2022
OI Cai, Bing/0000-0002-2360-2579
FU NSFC of China
FX No Statement Available
CR [Anonymous], 2011, INT C NEURAL INF PRO
   Boyd L., 2004, Convex Optimization
   Chen YY, 2022, IEEE T NEUR NET LEAR, V33, P4712, DOI 10.1109/TNNLS.2021.3059874
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Du YF, 2023, INFORM SCIENCES, V631, P429, DOI 10.1016/j.ins.2023.02.089
   Duchi J., 2008, Proceedings of the International Conference on Machine learning, P272
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fu LL, 2022, INFORM SCIENCES, V606, P877, DOI 10.1016/j.ins.2022.05.091
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Gao QX, 2020, AAAI CONF ARTIF INTE, V34, P3930
   Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569
   Huang YX, 2022, NEURAL PROCESS LETT, V54, P265, DOI 10.1007/s11063-021-10634-3
   Ji P, 2017, ADV NEUR IN, V30
   Kang Z, 2020, KNOWL-BASED SYST, V189, DOI 10.1016/j.knosys.2019.105102
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Li JN, 2022, PR MACH LEARN RES
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Lin Z., 2011, ADV NEURAL INFORM PR, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu J., 2013, PROC 13 SIAM INT C, P252, DOI DOI 10.1137/1.9781611972832.28
   Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26
   Lu RK, 2021, NEUROCOMPUTING, V435, P186, DOI 10.1016/j.neucom.2021.01.011
   Nie FP, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2022, DOI 10.1145/3219819.3220049
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Patterson S., 2013, Advances in neural information processing systems, P3102
   Radford A, 2021, PR MACH LEARN RES, V139
   Song K, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2751
   Song K, 2022, IEEE T PATTERN ANAL, V44, P4591, DOI 10.1109/TPAMI.2021.3073587
   Song K, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107560
   Tang YQ, 2022, IEEE T CYBERNETICS, V52, P9179, DOI 10.1109/TCYB.2021.3053057
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wan ZZ, 2021, NEUROCOMPUTING, V462, P123, DOI 10.1016/j.neucom.2021.07.090
   Wen J, 2018, NEURAL NETWORKS, V108, P83, DOI 10.1016/j.neunet.2018.08.007
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Xia W, 2023, IEEE T PATTERN ANAL, V45, P5187, DOI 10.1109/TPAMI.2022.3187976
   Xia W, 2022, IEEE T CYBERNETICS, V52, P8962, DOI 10.1109/TCYB.2021.3052352
   Xiao XL, 2021, IEEE T MULTIMEDIA, V23, P4555, DOI 10.1109/TMM.2020.3045259
   Xie DY, 2021, NEURAL NETWORKS, V133, P57, DOI 10.1016/j.neunet.2020.10.010
   Xie Y, 2021, IEEE T NEUR NET LEAR, V32, P868, DOI 10.1109/TNNLS.2020.2979685
   Xie Y, 2020, IEEE T CYBERNETICS, V50, P572, DOI 10.1109/TCYB.2018.2869789
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Xu J, 2021, IEEE T CYBERNETICS, V51, P1493, DOI 10.1109/TCYB.2019.2943691
   Yao L, 2022, NEURAL NETWORKS, V151, P168, DOI 10.1016/j.neunet.2022.03.039
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang GY, 2022, APPL INTELL, V52, P716, DOI 10.1007/s10489-021-02365-8
   Zhang GY, 2021, EXPERT SYST APPL, V166, DOI 10.1016/j.eswa.2020.113913
   Zhao JB, 2022, APPL INTELL, V52, P15899, DOI 10.1007/s10489-021-03146-z
   Zhao JB, 2022, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109118
   Zhao YJ, 2022, NEUROCOMPUTING, V468, P257, DOI 10.1016/j.neucom.2021.09.052
   Zheng QH, 2023, INFORM FUSION, V89, P198, DOI 10.1016/j.inffus.2022.08.014
   Zheng QH, 2020, NEUROCOMPUTING, V379, P89, DOI 10.1016/j.neucom.2019.10.074
   Zhu XF, 2019, IEEE T KNOWL DATA EN, V31, P2022, DOI 10.1109/TKDE.2018.2873378
NR 57
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6621
EP 6631
DI 10.1109/TMM.2024.3355649
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600058
DA 2024-08-05
ER

PT J
AU Cao, YJ
   Yao, LN
   Pan, L
   Sheng, QZ
   Chang, XJ
AF Cao, Yuanjiang
   Yao, Lina
   Pan, Le
   Sheng, Quan Z.
   Chang, Xiaojun
TI Guided Image-to-Image Translation by Discriminator-Generator
   Communication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Training; Generative adversarial networks; Adaptation
   models; Games; Computational modeling; Markov processes; Image-to-image
   translation; domain adaption; generative adversarial networks
AB The goal of Image-to-image (I2I) translation is to transfer an image from a source domain to a target domain, which has recently drawn increasing attention. One major branch of this research is to formulate I2I translation based on Generative Adversarial Network (GAN). As a zero-sum game, GAN can be reformulated as a Partially-observed Markov Decision Process (POMDP) for generators, where generators cannot access full state information of their environments. This formulation illustrates the information insufficiency in the GAN training. To mitigate this problem, we propose to add a communication channel between discriminators and generators. We explore multiple architecture designs to integrate the communication mechanism into the I2I translation framework. To validate the performance of the proposed approach, we have conducted extensive experiments on various benchmark datasets. The experimental results confirm the superiority of our proposed method.
C1 [Cao, Yuanjiang; Yao, Lina; Pan, Le] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
   [Yao, Lina] CSIROs, Data61, Sydney, NSW 2015, Australia.
   [Sheng, Quan Z.] Macquarie Univ, Dept Comp, Sydney, NSW 2109, Australia.
   [Chang, Xiaojun] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
C3 University of New South Wales Sydney; Commonwealth Scientific &
   Industrial Research Organisation (CSIRO); Macquarie University;
   University of Technology Sydney
RP Cao, YJ (corresponding author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
EM yuanjiang.cao@unsw.edu.au; lina.yao@unsw.edu.au; le.pan@unsw.edu.au;
   michael.sheng@mq.edu.au; xiaojun.chang@uts.edu.au
RI Sheng, Quan Z./ITV-5105-2023; Chang, Xiaojun/A-2055-2015
OI Sheng, Quan Z./0000-0002-3326-4147; Chang, Xiaojun/0000-0002-7778-8807
CR Benaim S, 2017, ADV NEURAL INFORM PR, P752
   Cazenavette M. L., 2021, CoRR, Vabs/2105.14110
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen XY, 2018, LECT NOTES COMPUT SC, V11206, P167, DOI 10.1007/978-3-030-01216-8_11
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Das A, 2019, PR MACH LEARN RES, V97
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Foerster JN, 2016, ADV NEUR IN, V29
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gronauer S, 2022, ARTIF INTELL REV, V55, P895, DOI 10.1007/s10462-021-09996-w
   Guo X, 2020, NEUROCOMPUTING, V394, P127, DOI 10.1016/j.neucom.2019.01.115
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoshen Y, 2017, ADV NEUR IN, V30
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P4002, DOI 10.1109/TMM.2021.3111501
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang JC, 2018, ADV NEUR IN, V31
   Jiang L, 2021, PROC CVPR IEEE, P16504, DOI 10.1109/CVPR46437.2021.01624
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim M., 2020, 8 INT C LEARN REPRES
   Kingma D. P., 2014, arXiv
   Liu MY, 2017, ADV NEUR IN, V30
   Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47
   Mejjati Youssef Alami, 2018, ADV NEURAL INFORM PR, P3697
   Oliehoek C., 2016, A Concise Introduction to DecentralizedPOMDPs
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Peng P, 2017, Arxiv, DOI arXiv:1703.10069
   Rao Kanishka, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11154, DOI 10.1109/CVPR42600.2020.01117
   Royer A., 2020, Domain Adapt. Vis. Underst, V2020, P33
   Singh A, 2018, Arxiv, DOI [arXiv:1812.09755, 10.48550/ARXIV.1812.09755]
   Sukhbaatar S, 2016, ADV NEUR IN, V29
   Sutherland M., 2018, 6 INT C LEARN REPRES
   Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Yang SQ, 2022, KNOWL-BASED SYST, V257, DOI 10.1016/j.knosys.2022.109852
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zheng CX, 2021, PROC CVPR IEEE, P16402, DOI 10.1109/CVPR46437.2021.01614
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zheng ZQ, 2022, IEEE T MULTIMEDIA, V24, P480, DOI 10.1109/TMM.2021.3053775
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu LC, 2022, IEEE T PATTERN ANAL, V44, P273, DOI 10.1109/TPAMI.2020.3007511
NR 49
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1528
EP 1538
DI 10.1109/TMM.2023.3282869
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Chiang, JC
   Wu, YT
   Hsieh, HY
   Tsai, YC
AF Chiang, Jui-Chiu
   Wu, Yu-Tze
   Hsieh, Hsin-Yun
   Tsai, Yun-Chang
TI Enhanced Temporal Consistency for Global Patch Allocation in Video-Based
   Point Cloud Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Three-dimensional displays; Geometry; Encoding;
   Image coding; Resource management; Transforms; Global patch allocation;
   point cloud; video-based point cloud compression
ID ATTRIBUTE COMPRESSION; TRANSFORM; MODEL
AB Video-based point cloud compression (V-PCC) is a promising technique for compressing 3D point clouds. V-PCC projects the 3D point cloud into patches and encodes the generated 2D images using state-of-the-art video codecs. To maintain temporal consistency between frames, V-PCC supports global patch packing methods and one notable approach is Global Patch Allocation (GPA), which packs the global matched patches into the same location in each frame across the sequence. Additionally, frames are subdivided into groups (i.e., sub-contexts) to balance packing compactness and patch similarity within the groups. While video coding typically employs a Group of Picture (GOP) as the basic unit for encoding, GPA in V-PCC currently does not consider the reference relationship between images within or between GOPs, resulting in limited similarity between the current and the reference images, ultimately leading to reduced encoding efficiency. This paper presents an improved technique for GPA. We propose a dynamic sub-context and GOP determination technique, enhancing the similarity between images within the same GOP. Furthermore, we introduce a priority-based patch packing (PBPP) technique to reduce differences between frames in adjacent GOPs. Experimental results demonstrate the superiority of our proposed method over the anchor, achieving an average BD-rate savings of 3.09%, 3.04%, and 2.33% for D1-PSNR, D2-PSNR, and Y-PSNR, respectively.
C1 [Chiang, Jui-Chiu] Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621301, Taiwan.
   [Chiang, Jui-Chiu] Natl Chung Cheng Univ, Adv Inst Mfg High Tech Innovat, Chiayi 621301, Taiwan.
   [Wu, Yu-Tze; Hsieh, Hsin-Yun; Tsai, Yun-Chang] Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 62130, Taiwan.
C3 National Chung Cheng University; National Chung Cheng University;
   National Chung Cheng University
RP Chiang, JC (corresponding author), Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621301, Taiwan.; Chiang, JC (corresponding author), Natl Chung Cheng Univ, Adv Inst Mfg High Tech Innovat, Chiayi 621301, Taiwan.
EM rachel@ccu.edu.tw; uniotw@gmail.com; yaabb881212@gmail.com;
   jacky19990408@gmail.com
FU National Science and Technology Council, Taiwan
FX No Statement Available
CR Akhtar A, 2022, IEEE T MULTIMEDIA, V24, P2866, DOI 10.1109/TMM.2021.3090148
   [Anonymous], 2022, ISO/IEC JTC 1/SC 29/WG 7 N00271
   [Anonymous], 2021, ISO/IEC JTC 1/SC 29/WG 7
   [Anonymous], 2020, ISO/IEC JTC 1/SC 29/WG 7/N0038
   [Anonymous], 2017, ISO/IEC JTC1/SC29/WG11 MPEG2017/N16763
   Bjontegaard G., 2001, Calculation of Average PSNR Differences between RDcurves
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   d'Eon E., 2016, ISO/IEC JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG) m38673/M72012
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3507, DOI 10.1109/TIP.2017.2699922
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   Garcia DC, 2018, IEEE IMAGE PROC, P1807, DOI 10.1109/ICIP.2018.8451802
   Garcia DC, 2017, IEEE IMAGE PROC, P1412, DOI 10.1109/ICIP.2017.8296514
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Graziosi D., 2018, ISO/IEC JTC1/SC29/WG11 m43680
   Graziosi D., 2019, document ISO/IEC JTC1/SC29/WG11,MPEG2019/m46212
   Gu S, 2020, IEEE T IMAGE PROCESS, V29, P796, DOI 10.1109/TIP.2019.2936738
   Herglotz C, 2022, IEEE T CIRC SYST VID, V32, P7996, DOI 10.1109/TCSVT.2022.3185026
   Hou JH, 2017, INT CONF ACOUST SPEE, P2926, DOI 10.1109/ICASSP.2017.7952692
   Hui Yuan, 2021, Image and Graphics: 11th International Conference, ICIG 2021, Proceedings. Lecture Notes in Computer Science, Image Processing, Computer Vision, Pattern Recognition, and Graphics (12888), P735, DOI 10.1007/978-3-030-87355-4_61
   Jia W, 2022, IEEE T MULTIMEDIA, V24, P2352, DOI 10.1109/TMM.2021.3079698
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Kim J, 2021, IEEE ACCESS, V9, P80088, DOI 10.1109/ACCESS.2021.3084180
   Li L, 2021, IEEE T CIRC SYST VID, V31, P326, DOI 10.1109/TCSVT.2020.2966118
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P289, DOI 10.1109/TIP.2019.2931621
   Liu H, 2020, IEEE T BROADCAST, V66, P701, DOI 10.1109/TBC.2019.2957652
   Liu JQ, 2019, IEEE INT CON MULTI, P904, DOI 10.1109/ICME.2019.00160
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Liu Q, 2020, IEEE INT CONF MULTI
   Lopes E, 2019, IEEE INT CON MULTI, P49, DOI 10.1109/ICME.2019.00017
   mpegx.intevry, 2022, TMC2-10.0
   Santos C, 2021, IEEE IMAGE PROC, P3388, DOI 10.1109/ICIP42928.2021.9506355
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Shi HY, 2022, IEEE OPEN J SIGNAL P, V3, P155, DOI 10.1109/OJSP.2022.3160392
   Song F, 2022, IEEE SIGNAL PROC LET, V29, P922, DOI 10.1109/LSP.2022.3161868
   Souto AL, 2023, IEEE T IMAGE PROCESS, V32, P2428, DOI 10.1109/TIP.2023.3265264
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong J, 2023, IEEE T MULTIMEDIA, V25, P3329, DOI 10.1109/TMM.2022.3158809
   Xiong J, 2022, IEEE T CIRC SYST VID, V32, P813, DOI 10.1109/TCSVT.2021.3063501
   Xu YZ, 2019, INT CONF ACOUST SPEE, P2287, DOI 10.1109/ICASSP.2019.8682413
   Xu YQ, 2021, IEEE T CIRC SYST VID, V31, P1968, DOI 10.1109/TCSVT.2020.3015901
   Yi X., 2017, ISO/IEC JTC1/SC29/WG11 (MPEG/JPEG) m41658
   Yuan H, 2021, IEEE INT WORKSH MULT, DOI 10.1109/MMSP53017.2021.9733714
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhang D., 2018, ISO/IEC JTC1/SC29/WG11 MPEG2018/m44769
NR 48
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6917
EP 6930
DI 10.1109/TMM.2024.3358076
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000044
DA 2024-08-05
ER

PT J
AU Cong, RX
   Sheng, H
   Yang, D
   Cui, ZL
   Chen, RS
AF Cong, Ruixuan
   Sheng, Hao
   Yang, Da
   Cui, Zhenglong
   Chen, Rongshan
TI Exploiting Spatial and Angular Correlations With Deep Efficient
   Transformers for Light Field Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Computational modeling; Superresolution; Spatial
   resolution; Feature extraction; Light fields; Convolution; Light field;
   transformer; super-resolution; sub-sampling spatial modeling;
   multi-scale angular modeling
ID NETWORK
AB Global context information is particularly important for comprehensive scene understanding. It helps clarify local confusions and smooth predictions to achieve fine-grained and coherent results. However, most existing light field processing methods leverage convolution layers to model spatial and angular information. The limited receptive field restricts them to learn long-range dependency in LF structure. In this article, we propose a novel network based on deep efficient transformers (i.e., LF-DET) for LF spatial super-resolution. It develops a spatial-angular separable transformer encoder with two modeling strategies termed as sub-sampling spatial modeling and multi-scale angular modeling for global context interaction. Specifically, the former utilizes a sub-sampling convolution layer to alleviate the problem of huge computational cost when capturing spatial information within each sub-aperture image. In this way, our model can cascade more transformers to continuously enhance feature representation with limited resources. The latter processes multi-scale macro-pixel regions to extract and aggregate angular features focusing on different disparity ranges to well adapt to disparity variations. Besides, we capture strong similarities among surrounding pixels by dynamic positional encodings to fill the gap of transformers that lack of local information interaction. The experimental results on both real-world and synthetic LF datasets confirm our LF-DET achieves a significant performance improvement compared with state-of-the-art methods. Furthermore, our LF-DET shows high robustness to disparity variations through the proposed multi-scale angular modeling.
C1 [Cong, Ruixuan; Sheng, Hao; Yang, Da; Cui, Zhenglong; Chen, Rongshan] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Cong, Ruixuan; Sheng, Hao; Yang, Da; Cui, Zhenglong; Chen, Rongshan] Beihang Hangzhou Innovat Inst Yuhang, Hangzhou 310023, Peoples R China.
   [Cong, Ruixuan; Sheng, Hao; Yang, Da; Cui, Zhenglong; Chen, Rongshan] Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.
C3 Beihang University; Macao Polytechnic University
RP Sheng, H (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM congrx@buaa.edu.cn; shenghao@buaa.edu.cn; da.yang@buaa.edu.cn;
   zhenglong.cui@buaa.edu.cn; rongshan@buaa.edu.cn
RI sheng, hao/AAM-9149-2020; Ruixuan, Cong/GRE-7371-2022
OI Ruixuan, Cong/0000-0001-6410-5248; Cui, Zhenglong/0000-0003-4796-6382
FU National Key Ramp;D Program of China
FX No Statement Available
CR Alain M, 2018, IEEE IMAGE PROC, P2501, DOI 10.1109/ICIP.2018.8451162
   Alexey D., 2021, P 9 INT C LEARN REPR
   Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen YY, 2023, IEEE T VIS COMPUT GR, V29, P4183, DOI 10.1109/TVCG.2022.3184047
   Chu X., 2023, P 11 INT C LEARN REP
   Chu XX, 2021, ADV NEUR IN
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Egiazarian K, 2015, EUR SIGNAL PR CONF, P2849, DOI 10.1109/EUSIPCO.2015.7362905
   Glorot X., Understanding the difficulty of training deep feedforward neural networks, V9, P249
   Guo MT, 2022, IEEE T PATTERN ANAL, V44, P6094, DOI 10.1109/TPAMI.2021.3087485
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Jia Chen, 2022, 2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD), P239, DOI 10.1109/CSCWD54268.2022.9776140
   Jin J, 2020, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR42600.2020.00233
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Ko K, 2021, IEEE T IMAGE PROCESS, V30, P4114, DOI 10.1109/TIP.2021.3069291
   Le Pendu M, 2018, IEEE T IMAGE PROCESS, V27, P1981, DOI 10.1109/TIP.2018.2791864
   Li M, 2023, IEEE T MULTIMEDIA, V25, P919, DOI 10.1109/TMM.2021.3134839
   Li YW, 2021, Arxiv, DOI arXiv:2104.05707
   Liang ZY, 2022, IEEE SIGNAL PROC LET, V29, P563, DOI 10.1109/LSP.2022.3146798
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu GS, 2023, IEEE T MULTIMEDIA, V25, P256, DOI 10.1109/TMM.2021.3124385
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Mitra K., 2012, 2012 IEEE COMPUTER S, P22
   Ramachandran P, 2019, ADV NEUR IN, V32
   Rerabek M., 2016, P INT C QUAL MULT EX, P2
   Rossi P., 2017, P IEEE 19 INT WORKSH, P1
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Sheng H, 2022, IEEE T CIRC SYST VID, V32, P7880, DOI 10.1109/TCSVT.2022.3187664
   Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaish A., 2008, Comput.Graph. Lab., Stanford Univ., V6
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang SZ, 2022, AAAI CONF ARTIF INTE, P2522
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YQ, 2023, IEEE T PATTERN ANAL, V45, P425, DOI 10.1109/TPAMI.2022.3152488
   Wang YQ, 2021, IEEE T IMAGE PROCESS, V30, P1057, DOI 10.1109/TIP.2020.3042059
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wanner S., 2013, INT S VIS MOD VIS, V13, P225
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Yingqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P290, DOI 10.1007/978-3-030-58592-1_18
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang S, 2021, IEEE T IMAGE PROCESS, V30, P5956, DOI 10.1109/TIP.2021.3079805
   Zhang S, 2019, PROC CVPR IEEE, P11038, DOI 10.1109/CVPR.2019.01130
   Zhang S, 2016, COMPUT VIS IMAGE UND, V145, P148, DOI 10.1016/j.cviu.2015.12.007
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 59
TC 40
Z9 40
U1 21
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1421
EP 1435
DI 10.1109/TMM.2023.3282465
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700050
HC Y
HP Y
DA 2024-08-05
ER

PT J
AU Ge, FW
   Zhang, YZ
   Wang, L
   Coleman, S
   Kerr, D
AF Ge, Fawei
   Zhang, Yunzhou
   Wang, Li
   Coleman, Sonya
   Kerr, Dermot
TI Double-Domain Adaptation Semantics for Retrieval-Based Long-Term Visual
   Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Changing environment; domain adaptation; image retrieval; semantic
   information; visual localization
ID SLAM; MAP
AB Due to seasonal and illumination variance, long-term visual localization tasks in dynamic environments is a crucial problem in the field of autonomous driving and robotics. At present, image-based retrieval is an effective method to solve this problem. However, it is difficult to completely distinguish changes in the same location over times by relying on content information alone. In order to solve these above problems, a double-domain network model combining semantic information and content information is proposed for visual localization task. In addition, this approach only needs to use the virtual KITTI 2 dataset for training. To reduce the domain difference between real scene and virtual image, the cross-predictive semantic segmentation mechanism is introduced to solve this problem. In addition, the obtained model achieves good domain adaptation and further has well generalization on other real datasets by introducing a domain loss function and a triplet semantic loss function. A series of experiments on the Extended CMU-Seasons dataset and the Oxford RobotCar-Seasons dataset demonstrates that the proposed network model outperformes the state-of-the-art baselines for retrieval-based visual localization in challenging environments.
C1 [Ge, Fawei; Zhang, Yunzhou; Wang, Li] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
   [Coleman, Sonya; Kerr, Dermot] Univ Ulster, Intelligent Syst Res Ctr, Derry BT52 1SA, North Ireland.
C3 Northeastern University - China; Ulster University
RP Zhang, YZ (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
EM gefawei0822@163.com; zhangyunzhou@mail.neu.edu.cn; wangli7682@163.com;
   sa.coleman@ulster.ac.uk; d.kerr@ulster.ac.uk
FU National Natural Science Foundation of China
FX No Statement Available
CR Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI [10.1109/ICRA.2019.8794387, 10.1109/icra.2019.8794387]
   Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Badino D. H. H., 2016, The CMU visual localization data set
   Benbihi A, 2020, IEEE INT CONF ROBOT, P3032, DOI [10.1109/ICRA40945.2020.9197529, 10.1109/icra40945.2020.9197529]
   Cabon Y, 2020, Arxiv, DOI arXiv:2001.10773
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Chen ZT, 2018, IEEE ROBOT AUTOM LET, V3, P4015, DOI 10.1109/LRA.2018.2859916
   Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961
   Doan AD, 2019, IEEE I CONF COMP VIS, P9318, DOI 10.1109/ICCV.2019.00941
   Duh PJ, 2021, IEEE T MULTIMEDIA, V23, P1567, DOI 10.1109/TMM.2020.3001500
   Fan B, 2023, IEEE T MULTIMEDIA, V25, P1713, DOI 10.1109/TMM.2022.3154165
   Fan B, 2022, IEEE T IMAGE PROCESS, V31, P4842, DOI 10.1109/TIP.2022.3187565
   Gao P, 2020, IEEE INT CONF ROBOT, P1070, DOI [10.1109/icra40945.2020.9196906, 10.1109/ICRA40945.2020.9196906]
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hu HJ, 2023, IEEE INT C INT ROBOT, P11384, DOI 10.1109/IROS55552.2023.10341917
   Hu HJ, 2022, IEEE-CAA J AUTOMATIC, V9, P313, DOI 10.1109/JAS.2021.1003907
   Hu HJ, 2021, IEEE T IMAGE PROCESS, V30, P1342, DOI 10.1109/TIP.2020.3043875
   Hu HJ, 2019, IEEE INT C INT ROBOT, P3684, DOI [10.1109/iros40897.2019.8968047, 10.1109/IROS40897.2019.8968047]
   Humenberger M, 2022, INT J COMPUT VISION, V130, P1811, DOI 10.1007/s11263-022-01615-7
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jenicek T, 2019, IEEE I CONF COMP VIS, P9695, DOI 10.1109/ICCV.2019.00979
   Ji RR, 2020, IEEE T PATTERN ANAL, V42, P2410, DOI 10.1109/TPAMI.2019.2936024
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Kundu JN, 2018, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2018.00281
   Larsson M, 2019, IEEE I CONF COMP VIS, P31, DOI 10.1109/ICCV.2019.00012
   Larsson M, 2019, PROC CVPR IEEE, P9524, DOI 10.1109/CVPR.2019.00976
   Li DJ, 2020, IEEE INT C INT ROBOT, P4958, DOI 10.1109/IROS45743.2020.9340907
   Liu ZG, 2017, IEEE T MULTIMEDIA, V19, P874, DOI 10.1109/TMM.2016.2636750
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long MS, 2013, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2013.59
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Piasco N, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01363-6
   Piasco N, 2019, IEEE INT CONF ROBOT, P9094, DOI [10.1109/icra.2019.8794221, 10.1109/ICRA.2019.8794221]
   Porav H, 2018, IEEE INT CONF ROBOT, P1011, DOI 10.1109/ICRA.2018.8462894
   Qin W, 2023, IEEE T MULTIMEDIA, V25, P1033, DOI 10.1109/TMM.2021.3136717
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Salarian M, 2018, IEEE T MULTIMEDIA, V20, P3298, DOI 10.1109/TMM.2018.2839893
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395
   Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300
   Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342
   Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897
   Schubert S, 2020, IEEE INT CONF ROBOT, P4372, DOI 10.1109/icra40945.2020.9197044
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi W, 2019, IEEE-CAA J AUTOMATIC, V6, P917, DOI 10.1109/JAS.2019.1911561
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Toft C, 2022, IEEE T PATTERN ANAL, V44, P2074, DOI 10.1109/TPAMI.2020.3032010
   Tonioni A, 2020, IEEE T PATTERN ANAL, V42, P2396, DOI 10.1109/TPAMI.2019.2940948
   Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790
   Tsintotas KA, 2022, IEEE T INTELL TRANSP, V23, P19929, DOI 10.1109/TITS.2022.3175656
   Warburg Frederik, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P2623, DOI 10.1109/CVPR42600.2020.00270
   Xin Z, 2019, IEEE INT CONF ROBOT, P5979, DOI [10.1109/icra.2019.8794383, 10.1109/ICRA.2019.8794383]
   Yang BH, 2022, IEEE T MULTIMEDIA, V24, P3947, DOI 10.1109/TMM.2021.3110667
   Yin P, 2019, IEEE INT CONF ROBOT, P319, DOI [10.1109/ICRA.2019.8793752, 10.1109/icra.2019.8793752]
   Zhang XW, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107760
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao SS, 2019, PROC CVPR IEEE, P9780, DOI 10.1109/CVPR.2019.01002
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu YY, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P821, DOI 10.1145/3397271.3401176
NR 63
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6050
EP 6064
DI 10.1109/TMM.2023.3345138
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100007
DA 2024-08-05
ER

PT J
AU Jiang, YY
   Yin, JQ
   Dang, YH
AF Jiang, Yuanyuan
   Yin, Jianqin
   Dang, Yonghao
TI Leveraging the Video-Level Semantic Consistency of Event for
   Audio-Visual Event Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual learning; event localization; video understanding;
   weakly-supervised learning
ID ATTENTION NETWORK
AB Audio-visual event (AVE) localization has attracted much attention in recent years. Most existing methods are often limited to independently encoding and classifying each video segment separated from the full video (which can be regarded as the segment-level representations of events). However, they ignore the semantic consistency of the event within the same full video (which can be considered as the video-level representations of events). In contrast to existing methods, we propose a novel video-level semantic consistency guidance network for the AVE localization task. Specifically, we propose an event semantic consistency modeling (ESCM) module to explore video-level semantic information for semantic consistency modeling. It consists of two components: a cross-modal event representation extractor (CERE) and an intra-modal semantic consistency enhancer (ISCE). CERE is proposed to obtain the event semantic information at the video level. Furthermore, ISCE takes video-level event semantics as prior knowledge to guide the model to focus on the semantic continuity of an event within each modality. Moreover, we propose a new negative pair filter loss to encourage the network to filter out the irrelevant segment pairs and a new smooth loss to further increase the gap between different categories of events in the weakly-supervised setting. We perform extensive experiments on the public AVE dataset and outperform the state-of-the-art methods in both fully- and weakly-supervised settings, thus verifying the effectiveness of our method.
C1 [Jiang, Yuanyuan; Yin, Jianqin; Dang, Yonghao] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Yin, JQ (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
EM jyy@bupt.edu.cn; jqyin@bupt.edu.cn; dyh2018@bupt.edu.cn
OI Dang, Yonghao/0000-0002-1118-5587
FU National Natural Science Foundation of China
FX No Statement Available
CR Afouras T., 2020, Lecture Notes in Computer Science, P208
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Cheng Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3884, DOI 10.1145/3394171.3413869
   Cho K., 2014, ARXIV14061078, V1406, P1078, DOI DOI 10.3115/V1/D14-1179
   Feng F., 2023, IEEE Trans. Multimedia, early access, DOI [10.1109/TMM.2023.3270624, DOI 10.1109/TMM.2023.3270624]
   Gan C, 2019, IEEE I CONF COMP VIS, P7052, DOI 10.1109/ICCV.2019.00715
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Geng TT, 2023, PROC CVPR IEEE, P22942, DOI 10.1109/CVPR52729.2023.02197
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hu D, 2019, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR.2019.00947
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee J.-T., 2021, PROC INT C LEARN REP, P1
   Lin Y.-B., 2020, PROC ASIAN C COMPUT, P274
   Lin YB, 2019, INT CONF ACOUST SPEE, P2002, DOI 10.1109/icassp.2019.8683226
   Lin YB, 2023, COMPUT VIS IMAGE UND, V227, DOI 10.1016/j.cviu.2022.103602
   Liu S, 2023, IEEE T MULTIMEDIA, V25, P2734, DOI 10.1109/TMM.2022.3150469
   Mahmud T., 2023, P IEEECVF WINTER C A, P5158
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Ramaswamy J, 2020, INT CONF ACOUST SPEE, P4372, DOI [10.1109/icassp40776.2020.9053895, 10.1109/ICASSP40776.2020.9053895]
   Ramaswamy J, 2020, IEEE WINT CONF APPL, P2959, DOI 10.1109/WACV45572.2020.9093616
   Rui Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P292, DOI 10.1007/978-3-030-58565-5_18
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Zengjie, 2022, P IEEE CVF C COMP VI, P3222
   Stein B E, 1989, J Cogn Neurosci, V1, P12, DOI 10.1162/jocn.1989.1.1.12
   Tian YP, 2021, PROC CVPR IEEE, P2744, DOI 10.1109/CVPR46437.2021.00277
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Valverde FR, 2021, PROC CVPR IEEE, P11607, DOI 10.1109/CVPR46437.2021.01144
   Wang H, 2021, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR46437.2021.00695
   Wang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4116, DOI 10.1145/3394171.3413975
   Wu P, 2023, IEEE T MULTIMEDIA, V25, P1674, DOI 10.1109/TMM.2022.3147369
   Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639
   Xia Y, 2022, PROC CVPR IEEE, P19957, DOI 10.1109/CVPR52688.2022.01936
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xu HM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3893, DOI 10.1145/3394171.3413581
   Xuan HY, 2021, IEEE T IMAGE PROCESS, V30, P7878, DOI 10.1109/TIP.2021.3106814
   Xuan HY, 2020, AAAI CONF ARTIF INTE, V34, P279
   Xue C, 2023, IEEE T MULTIMEDIA, V25, P418, DOI 10.1109/TMM.2021.3127029
   Zellers R, 2022, PROC CVPR IEEE, P16354, DOI 10.1109/CVPR52688.2022.01589
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zhou JX, 2021, PROC CVPR IEEE, P8432, DOI 10.1109/CVPR46437.2021.00833
NR 46
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4617
EP 4627
DI 10.1109/TMM.2023.3324498
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100041
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, WH
   Yang, S
   Li, Q
   Li, XY
   Liu, AA
AF Li, Wenhui
   Yang, Song
   Li, Qiang
   Li, Xuanya
   Liu, An-An
TI Commonsense-Guided Semantic and Relational Consistencies for Image-Text
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Correlation; Task analysis; Commonsense
   reasoning; Oceans; Sea measurements; Commonsense knowledge; semantic
   consistency; relational consistency; image-text retrieval
AB Image-text retrieval, as a fundamental task in the cross-modal field, aims to explore the relationship between visual and textual modalities. Recent methods address this task only by learning the conceptual and syntactical correspondences between cross-modal fragments, but these correspondences inevitably contain noise without considering external knowledge. To solve this issue, we propose a novel Commonsense-Guided Semantic and Relational Consistencies (CSRC) for image-text retrieval that can simultaneously expand the semantics and relations to reduce the cross-modal differences under the assumption that the semantics and relations of the true image-text pair should be consistent between two modalities. Specifically, we first explore commonsense knowledge to expand the specific concepts for visual and textual graphs and optimize the semantic consistency by minimizing the differences in cross-modal semantic importance. Then, we extend the same relations for cross-modal concept pairs with semantic consistency, which serves to implement relational consistency. After that, we combine external commonsense knowledge with internal correlation to enhance concept representation and further optimize relational consistency by regularizing the importance differences between association-enhanced concepts. Extensive experimental results on two popular image-text retrieval datasets demonstrate the effectiveness of our proposed method.
C1 [Li, Wenhui; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Yang, Song; Li, Qiang] Tianjin Univ, Sch Microelect, Tianjin 300072, Peoples R China.
   [Li, Xuanya] Baidu Inc, Beijing 100085, Peoples R China.
   [Liu, An-An] Chinese Acad Sci, Inst Artificial Intelligence, Hefei Comprehens Natl Sci Ctr, Beijing 100045, Peoples R China.
   [Liu, An-An] Chinese Acad Sci, Key Lab Electromagnet Space Informat, Beijing 100045, Peoples R China.
C3 Tianjin University; Tianjin University; Baidu; Chinese Academy of
   Sciences; Chinese Academy of Sciences
RP Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Yang, S (corresponding author), Tianjin Univ, Sch Microelect, Tianjin 300072, Peoples R China.
EM liwenhui@tju.edu.cn; songyang@tju.edu.cn; liqiang@tju.edu.cn;
   lixuanya@baidu.com; anan0422@gmail.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Chen Ran, 2022, ARXIV
   Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Vo DM, 2022, PROC CVPR IEEE, P17979, DOI 10.1109/CVPR52688.2022.01747
   Faghri Fartash, 2018, BMVC
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   Hong WX, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1379, DOI 10.1145/3404835.3462838
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Zhong, 2021, P 30 INT JOINT C ART, P765, DOI DOI 10.24963/IJCAI.2021/106
   Jiang Huaizu, 2020, P IEEE CVF C COMP VI, P10267, DOI DOI 10.1109/CVPR42600.2020.01028
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P543, DOI 10.1109/TMM.2021.3128744
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Chunxiao, 2020, CVPR, P10918, DOI DOI 10.1109/CVPR42600.2020.01093
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Luo H., 2019, arXiv
   Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Plummer BA, 2017, INT J COMPUT VISION, V123, P74, DOI 10.1007/s11263-016-0965-7
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5182
   Tan HC, 2022, IEEE T MULTIMEDIA, V24, P832, DOI 10.1109/TMM.2021.3060291
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wendell T. C., 1992, Proceedings. Fifth Annual IEEE Symposium on Computer-Based Medical Systems (Cat. No.92CH3117-9), P704, DOI 10.1109/CBMS.1992.245041
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x
   Yang S, 2022, IEEE T CIRC SYST VID, V32, P8037, DOI 10.1109/TCSVT.2022.3182426
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yongzhi Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12783, DOI 10.1109/CVPR42600.2020.01280
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zeng PP, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2205, DOI 10.1145/3474085.3475380
   Zhang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3367, DOI 10.1145/3394171.3413917
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhou H, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4623
NR 51
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1867
EP 1880
DI 10.1109/TMM.2023.3289753
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800016
DA 2024-08-05
ER

PT J
AU Liu, AQ
   Li, SM
   Chang, YL
   Zhang, WL
   Hou, YH
AF Liu, Anqi
   Li, Sumei
   Chang, Yongli
   Zhang, Wenlin
   Hou, Yonghong
TI Coarse-to-Fine Cross-View Interaction Based Accurate Stereo Image
   Super-Resolution Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention transfer loss; coarse-to-fine structure; modified parallax
   attention; stereo image super-resolution
ID PARALLAX ATTENTION; MODULE
AB Recently, parallax attention based stereo image super-resolution (SR) methods, which can better explore cross-view information, have been widely studied. Despite the impressive performance of these methods, almost all of them calculate parallax attention maps at a single low resolution, which will lead to ambiguous stereo correspondence. Besides, the widely used parallax attention module (PAM) cannot handle the illuminance variations in stereo image pairs, and cannot distinguish the contribution of the captured cross-view features to the reconstruction of the target view. To this end, in this paper, we propose a coarse-to-fine cross-view interaction based network (C2FNet) to achieve more accurate cross-view information capturing. Firstly, in C2FNet, a coarse-to-fine cascaded parallax attention structure (C2F-CPAS), which conforms with the human visual mechanism, is constructed to gradually perform parallax attention from the low-resolution to high-resolution level. Thus, richer textures can be used to learn more reliable stereo correspondence. Meanwhile, a multi-level attention transfer loss is designed to further calibrate the accuracy of stereo correspondence at each level. Secondly, we propose a modified PAM (MPAM) to alleviate the limitations of common PAM so that illuminance-robust stereo correspondence can be learned and more important cross-view information can be selected. Extensive experimental results show that our proposed C2FNet outperforms the state-of-the-art methods on various datasets.
C1 [Liu, Anqi; Li, Sumei; Chang, Yongli; Hou, Yonghong] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Zhang, Wenlin] Tianjin Univ, Tianjin Int Engn Inst, Tianjin, Peoples R China.
C3 Tianjin University; Tianjin University
RP Li, SM (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM liuanqi@tju.edu.cn; lisumei@tju.edu.cn; chang_yli@163.com;
   zwl1998@tju.edu.cn; houroy@tju.edu.cn
OI Li, Sumei/0000-0002-4793-3161; Zhang, Wenlin/0009-0008-0171-8897
FU National Natural Science Foundation of China
FX No Statement Available
CR Bo Yan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13176, DOI 10.1109/CVPR42600.2020.01319
   Chen CQ, 2022, IEEE T MULTIMEDIA, V24, P202, DOI 10.1109/TMM.2021.3050092
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chu XJ, 2022, IEEE COMPUT SOC CONF, P1238, DOI 10.1109/CVPRW56347.2022.00130
   Dai QY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1985, DOI 10.1145/3474085.3475356
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dan JW, 2021, IEEE SIGNAL PROC LET, V28, P1285, DOI 10.1109/LSP.2021.3088050
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gao QQ, 2019, LECT NOTES COMPUT SC, V11362, P527, DOI 10.1007/978-3-030-20890-5_34
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hua Y., 2020, PROC IEEE C COMPUT V, P1
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jeon DS, 2018, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2018.00185
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D., 2015, P INT C LEARN REPR S, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li JC, 2024, Arxiv, DOI arXiv:2109.14335
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang ZF, 2021, IEEE T PATTERN ANAL, V43, P300, DOI 10.1109/TPAMI.2019.2928550
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin JX, 2023, IEEE T MULTIMEDIA, V25, P8396, DOI 10.1109/TMM.2023.3236845
   Mallot HA, 1996, BIOL CYBERN, V74, P95, DOI 10.1007/BF00204198
   Menz MD, 2003, NAT NEUROSCI, V6, P59, DOI 10.1038/nn986
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Simonyan K., 2014, C TRACK P
   Song W, 2020, AAAI CONF ARTIF INTE, V34, P12031
   Takahashi N, 2021, PROC CVPR IEEE, P993, DOI 10.1109/CVPR46437.2021.00105
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Wang LG, 2019, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR.2019.01253
   Wang YQ, 2021, IEEE COMPUT SOC CONF, P766, DOI 10.1109/CVPRW53098.2021.00086
   Wang YQ, 2019, IEEE INT CONF COMP V, P3852, DOI 10.1109/ICCVW.2019.00478
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xu QY, 2021, IEEE SIGNAL PROC LET, V28, P613, DOI 10.1109/LSP.2021.3066125
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P496, DOI 10.1109/LSP.2020.2973813
   Zhang XD, 2022, LECT NOTES COMPUT SC, V13677, P649, DOI 10.1007/978-3-031-19790-1_39
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang Z, 2023, IEEE T CIRC SYST VID, V33, P2048, DOI 10.1109/TCSVT.2022.3220412
   Zhu XY, 2022, IEEE T MULTIMEDIA, V24, P3074, DOI 10.1109/TMM.2021.3092571
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zou WB, 2023, IEEE T MULTIMEDIA, V25, P4623, DOI 10.1109/TMM.2022.3179926
NR 51
TC 1
Z9 1
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7321
EP 7334
DI 10.1109/TMM.2024.3364492
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000043
DA 2024-08-05
ER

PT J
AU Moniruzzaman, M
   Yin, ZZ
AF Moniruzzaman, Md
   Yin, Zhaozheng
TI Feature Weakening, Contextualization, and Discrimination for Weakly
   Supervised Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal action localization; Feature weakening; Feature
   contextualization; Feature discrimination
ID ACTION RECOGNITION; VIDEOS
AB Weakly-supervised Temporal Action Localization (W-TAL) aims to train a model to localize all action instances potentially from different classes in an untrimmed video, using a training dataset that has video-level action class labels but has no detailed annotations on the start and end timestamps of action instances. We propose to solve the W-TAL problem from the feature learning aspect, with a new architecture, termed F3-Net, which includes (1) a Feature Weakening (FW) module that can identify and randomly weaken either the most discriminative action or the most discriminative background features over the training iterations to force the network to precisely localize the action instances in both discriminative and ambiguous action-related frames, without spreading to the background intervals; (2) a Feature Contextualization (FC) module that can infer the global contexts among video segments and attentionally fuse them with the local contexts from individual video segments to generate more representative features; and (3) a Feature Discrimination (FD) module that can highlight the most discriminative video segments/classes corresponding to each class/segment, respectively, for localizing multiple action instances from different classes within a video. Experimental results on THUMOS14 and ActivityNet1.3 demonstrate the state-of-the-art performance of our F3-Net, and the FW and FC are also effective plug-in modules to improve other methods. This project will be available at https://moniruzzamanmd.github.io/F3-Net/https://moniruzzamanmd.github.io/F3-Net/
C1 [Moniruzzaman, Md; Yin, Zhaozheng] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Yin, Zhaozheng] SUNY Stony Brook, Dept Biomed Informat, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Yin, ZZ (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM mmoniruzzama@cs.stonybrook.edu; zyin@cs.stonybrook.edu
RI Moniruzzaman, Md/AFD-5446-2022
OI Yin, Zhaozheng/0000-0002-9602-6488; Moniruzzaman, Md/0000-0003-3217-5094
FU National Science Foundation
FX No Statement Available
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao JY, 2022, PROC CVPR IEEE, P19967, DOI 10.1109/CVPR52688.2022.01937
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   He B, 2022, PROC CVPR IEEE, P13915, DOI 10.1109/CVPR52688.2022.01355
   Hou QB, 2018, ADV NEUR IN, V31
   Huang LJ, 2022, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR52688.2022.00327
   Huang LJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7982, DOI 10.1109/ICCV48922.2021.00790
   Huang LJ, 2020, AAAI CONF ARTIF INTE, V34, P11053
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Islam A, 2021, AAAI CONF ARTIF INTE, V35, P1637
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li JJ, 2022, PROC CVPR IEEE, P19882, DOI 10.1109/CVPR52688.2022.01929
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu XL, 2022, IEEE T IMAGE PROCESS, V31, P5427, DOI 10.1109/TIP.2022.3195321
   Liu ZY, 2021, AAAI CONF ARTIF INTE, V35, P2233
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Luo W, 2021, PROC CVPR IEEE, P9964, DOI 10.1109/CVPR46437.2021.00984
   Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17
   Moniruzzaman M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2166, DOI 10.1145/3394171.3413687
   Moniruzzaman M, 2022, IEEE T MULTIMEDIA, V24, P689, DOI 10.1109/TMM.2021.3058050
   Narayan S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13588, DOI 10.1109/ICCV48922.2021.01335
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Pardo A, 2021, IEEE WINT CONF APPL, P3318, DOI 10.1109/WACV48630.2021.00336
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Qu S., 2021, ACM-Net: Action context modeling network for weakly-supervised temporal action localization
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yang K, 2018, AAAI CONF ARTIF INTE, P7477
   Yang WF, 2021, PROC CVPR IEEE, P53, DOI 10.1109/CVPR46437.2021.00012
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yu TZ, 2019, IEEE T MULTIMEDIA, V21, P2504, DOI 10.1109/TMM.2019.2907060
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zhang C, 2021, PROC CVPR IEEE, P16005, DOI 10.1109/CVPR46437.2021.01575
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhekun Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P729, DOI 10.1007/978-3-030-58526-6_43
   Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511
   Zhu ZX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13496, DOI 10.1109/ICCV48922.2021.01326
NR 64
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 270
EP 283
DI 10.1109/TMM.2023.3263965
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500021
DA 2024-08-05
ER

PT J
AU Ni, DD
   Jia, ZH
   Yang, J
   Kasabov, NK
AF Ni, Dongdong
   Jia, Zhenhong
   Yang, Jie
   Kasabov, Nikola k.
TI Online Low-Light Sand-Dust Video Enhancement Using Adaptive Dynamic
   Brightness Correction and a Rolling Guidance Filter
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low-light sand-dust video; adaptive dynamic brightness correction;
   rolling guidance filter; dual-threshold interframe detection strategy
ID QUALITY ASSESSMENT; IMAGES
AB Sand-dust videos obtained in a low-light environment are characterized by low contrast, nonuniform illumination, color cast, and considerable noise. To realize sand-dust removal and brightness enhancement simultaneously, this article proposes an online low-light sand-dust video enhancement method using adaptive dynamic brightness correction and a rolling guidance filter. The proposed dual-threshold interframe detection strategy involves two methods to treat low-light sand-dust video frames. The first method involves two components: an adaptive dynamic brightness correction algorithm to correct the color deviation of the low-light video frame and improve its brightness and a rolling guidance filter combined with guided image filtering to enhance the frame details. The second method enhances the quality of the incoming frame by reducing the amount of calculation. The first frame of the video is processed using the first method. The processing method of each subsequent frame is determined according to its interframe detection value with the buffer frame. Through qualitative and quantitative comprehensive experiments on low-light sand-dust images and videos, the performance of the proposed method is compared with those of state-of-the-art methods. The proposed method for frame quality improvement achieves the best visual effect in enhancing the quality of low-light sand-dust images, as indicated by the best objective evaluation indicators. Moreover, compared with the framewise enhancement method, the video processing efficiency associated with the dual-threshold interframe detection strategy is 2.77 times higher.
C1 [Ni, Dongdong; Jia, Zhenhong] Xinjiang Univ, Coll Informat Sci & Engn, Urumqi 830046, Peoples R China.
   [Yang, Jie] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 200400, Peoples R China.
   [Kasabov, Nikola k.] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
C3 Xinjiang University; Shanghai Jiao Tong University; Auckland University
   of Technology
RP Jia, ZH (corresponding author), Xinjiang Univ, Coll Informat Sci & Engn, Urumqi 830046, Peoples R China.
EM ni_dongdong@yeah.net; jzhh@xju.edu.cn; jieyang@sjtu.edu.cn;
   nkasabov@aut.ac.nz
OI Kasabov, Nikola/0000-0003-4433-7521
FU National Science Foundation of China
FX No Statement Available
CR Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen QL, 2023, NEURAL COMPUT APPL, V35, P8647, DOI 10.1007/s00521-022-07564-z
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Ding BS, 2022, IEEE ACCESS, V10, P90092, DOI 10.1109/ACCESS.2022.3200163
   Dong X, 2011, IEEE INT CON MULTI
   Gao GX, 2020, IEEE PHOTONICS J, V12, DOI 10.1109/JPHOT.2020.2975833
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jiang H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1428, DOI 10.1109/ICASSP.2018.8462182
   Lei XZ, 2023, IEEE T MULTIMEDIA, V25, P4439, DOI 10.1109/TMM.2022.3175634
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li Z, 2021, INT C PATT RECOG, P8267, DOI 10.1109/ICPR48806.2021.9412595
   Liang PW, 2023, VISUAL COMPUT, V39, P1829, DOI 10.1007/s00371-022-02448-8
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu PJ, 2019, IEEE T IMAGE PROCESS, V28, P2212, DOI 10.1109/TIP.2018.2823424
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Ni DD, 2022, J REAL-TIME IMAGE PR, V19, P1181, DOI 10.1007/s11554-022-01248-6
   Ou FZ, 2022, IEEE T MULTIMEDIA, V24, P4197, DOI 10.1109/TMM.2021.3114551
   Park TH, 2021, IEEE ACCESS, V9, P19749, DOI 10.1109/ACCESS.2021.3054899
   Porikli F., 2008, 2008 IEEE C COMP VIS, DOI [10.1109/CVPR.2008.4587843, DOI 10.1109/CVPR.2008.4587843]
   Prakash J, 2019, IEEE T BIO-MED ENG, V66, P2604, DOI 10.1109/TBME.2019.2892842
   Qi SX, 2016, INFRARED PHYS TECHN, V77, P440, DOI 10.1016/j.infrared.2016.06.026
   Rustagi Shubham, 2018, 2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA), P464, DOI 10.1109/ICECA.2018.8474787
   Salazar-Colores S, 2019, IEEE T IMAGE PROCESS, V28, P2357, DOI 10.1109/TIP.2018.2885490
   Shen WW, 2022, IET IMAGE PROCESS, V16, P681, DOI 10.1049/ipr2.12286
   Shi ZH, 2020, IET IMAGE PROCESS, V14, P747, DOI 10.1049/iet-ipr.2019.0992
   Shi ZH, 2019, IEEE ACCESS, V7, P116722, DOI 10.1109/ACCESS.2019.2936444
   Shlens J, 2014, A Tutorial on Principal Component Analysis, V51, P1, DOI 10.13140/2.1.1593.1684
   Si YZ, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103638
   Si YZ, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-17530-3
   Srinivas K, 2021, IEEE T IMAGE PROCESS, V30, P5391, DOI 10.1109/TIP.2021.3083448
   Su Wang, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P979, DOI 10.1109/CISP.2011.6100338
   Tsakanikas V, 2018, COMPUT ELECTR ENG, V70, P736, DOI 10.1016/j.compeleceng.2017.11.011
   Wang QR, 2019, IEEE INT CONF BIG DA, P5080, DOI 10.1109/BigData47090.2019.9005603
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wang ZX, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3217035
   Wei KX, 2022, IEEE T PATTERN ANAL, V44, P8520, DOI 10.1109/TPAMI.2021.3103114
   Wu XM, 2017, IEEE IMAGE PROC, P3190, DOI 10.1109/ICIP.2017.8296871
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
   Yang Y, 2020, MULTIDIM SYST SIGN P, V31, P619, DOI 10.1007/s11045-019-00678-z
   Yu SY, 2019, IEEE T CIRC SYST VID, V29, P28, DOI 10.1109/TCSVT.2017.2763180
   Zhang JW, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3196050
   Zhang L, 2005, IEEE T CIRC SYST VID, V15, P469, DOI 10.1109/TCSVT.2005.844456
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang ZY, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116308
   Zhou Y, 2019, J VIS COMMUN IMAGE R, V60, P158, DOI 10.1016/j.jvcir.2019.02.028
   Zhu ZQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024335
NR 52
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2192
EP 2206
DI 10.1109/TMM.2023.3293276
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100005
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Qian, TW
   Cui, R
   Chen, JJ
   Peng, P
   Guo, XW
   Jiang, YG
AF Qian, Tianwen
   Cui, Ran
   Chen, Jingjing
   Peng, Pai
   Guo, Xiaowei
   Jiang, Yu-Gang
TI Locate Before Answering: Answer Guided Question Localization for Video
   Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video question answering; video grounding; cross-modal learning
AB Video question answering (VideoQA) is an essential task in vision-language understanding, which has attracted numerous research attention recently. Nevertheless, existing works mostly achieve promising performances on short videos of duration within 15 seconds. For VideoQA on minute-level long-term videos, those methods are likely to fail because of lacking the ability to deal with noise and redundancy caused by scene changes and multiple actions in the video. Considering the fact that the question often remains concentrated in a short temporal range, we propose to first locate the question to a segment in the video and then infer the answer using the located segment only. Under this scheme, we propose "Locate before Answering" (LocAns), a novel approach that integrates a question localization module and an answer prediction module into an end-to-end model. During the training phase, the available answer label not only serves as the supervision signal of the answer prediction module, but also is used to generate pseudo temporal labels for the question localization module. Moreover, we design a decoupled alternative training strategy to update the two modules separately. In the experiments, LocAns achieves state-of-the-art performance on three modern long-term VideoQA datasets, NExT-QA, ActivityNet-QA, and AGQA. Its qualitative examples show the reliable performance of the question localization.
C1 [Qian, Tianwen; Chen, Jingjing; Jiang, Yu-Gang] Fudan Univ, Shanghai 200433, Peoples R China.
   [Cui, Ran] Australian Natl Univ, Canberra, ACT 2600, Australia.
   [Peng, Pai; Guo, Xiaowei] Bilibili Co, Shanghai 200433, Peoples R China.
C3 Fudan University; Australian National University
RP Chen, JJ (corresponding author), Fudan Univ, Shanghai 200433, Peoples R China.
EM twqian19@fudan.edu.cn; ran.cui@anu.edu.au; chenjingjing@fudan.edu.cn;
   pengpai@bilibili.com; weide@bilibili.com; ygj@fudan.edu.cn
FU National Key Ramp;D Program of China
FX No Statement Available
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Dang Long Hoang, 2021, P 30 INT JOINT C ART, P636, DOI DOI 10.24963/IJCAI.2021/88
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao LL, 2019, AAAI CONF ARTIF INTE, P6391
   Grunde-McLaughlin M, 2021, PROC CVPR IEEE, P11282, DOI 10.1109/CVPR46437.2021.01113
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Huang J., 2021, P IEEE CVF INT C COM, P7199
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Jiang JW, 2020, AAAI CONF ARTIF INTE, V34, P11101
   Jiang P, 2020, AAAI CONF ARTIF INTE, V34, P11109
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Khurana K, 2021, IEEE ACCESS, V9, P43799, DOI 10.1109/ACCESS.2021.3058248
   Kingma D., 2015, P INT C LEARN REPR S, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Lei J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1369
   Malinowski M, 2014, ADV NEUR IN, V27
   Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Nair V., 2010, ICML, P807
   Patel Devshree, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12662), P339, DOI 10.1007/978-3-030-68790-8_27
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qian TW, 2023, IEEE T MULTIMEDIA, V25, P3950, DOI 10.1109/TMM.2022.3169065
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Seo A., 2021, P 59 ANN M ASS COMP, V1, P6167, DOI 10.18653/v1/2021.acllong.481
   Seo PH, 2021, PROC CVPR IEEE, P16872, DOI 10.1109/CVPR46437.2021.01660
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Thao Minh Le, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9969, DOI 10.1109/CVPR42600.2020.00999
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JY, 2021, IEEE T MULTIMEDIA, V24, P3369, DOI 10.1109/TMM.2021.3097171
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Xiao JB, 2021, PROC CVPR IEEE, P9772, DOI 10.1109/CVPR46437.2021.00965
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu Z, 2023, IEEE T MULTIMEDIA, V25, P6121, DOI 10.1109/TMM.2022.3205404
   Yang A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1666, DOI 10.1109/ICCV48922.2021.00171
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yu T, 2021, IEEE T CIRC SYST VID, V31, P931, DOI 10.1109/TCSVT.2020.2995959
   Yu T, 2020, IEEE T IMAGE PROCESS, V29, P1204, DOI 10.1109/TIP.2019.2940677
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2019, AAAI CONF ARTIF INTE, P9127
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7
   Zhuang YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366710
NR 57
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4554
EP 4563
DI 10.1109/TMM.2023.3323878
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Qian, XY
   Xue, W
   Zhang, QQ
   Tao, RJ
   Li, HZ
AF Qian, Xinyuan
   Xue, Wei
   Zhang, Qiquan
   Tao, Ruijie
   Li, Haizhou
TI Deep Cross-Modal Retrieval Between Spatial Image and Acoustic Speech
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; space-acoustics correspondence; contrastive
   learning; reverberation
AB Cross-modal Retrieval (CMR) is formulated for the scenarios where the queries and retrieval results are of different modalities. Existing Cross-modal Retrieval (CMR) studies mainly focus on the common contextualized information between text transcripts and images, and the synchronized event information in audio-visual recordings. Unlike all previous works, in this article, we investigate the geometric correspondence between images and speech recordings captured in the same space and formulate a novel CMR task, called Spatial Image-Acoustic Retrieval (SIAR). To this end, we first design a novel speech encoder that consists of convolution neural networks and transformer layers, to learn space-aware speech representations. Then, to eliminate the cross-modal inherent discrepancy, we propose the Contrastive Speech Image Retrieval (CSIR) method which uses supervised contrastive learning to attract the same-space cross-modal features while repelling the ones from different spaces. Finally, image and speech features are directly compared and we predict the SIAR result with the maximum similarity. Extensive experiments demonstrate that our proposed speech encoder can recognize space from human speeches with superior performance over the other prevailing networks. It also sets our penultimate goal of speech-to-speech retrieval. Furthermore, our CSIR proposal can successfully perform bi-directional SIAR between spatial images and reverberant speeches with promising results. Code and data will be available.
C1 [Qian, Xinyuan] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 10083, Peoples R China.
   [Xue, Wei] Hong Kong Univ Sci & Technol, Div Emerging Interdisciplinary Areas, Hong Kong, Peoples R China.
   [Zhang, Qiquan] Univ New South Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia.
   [Tao, Ruijie; Li, Haizhou] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Li, Haizhou] Chinese Univ Hong Kong, Shenzhen Res Inst Big Data, Sch Data Sci, Shenzhen 518172, Peoples R China.
C3 University of Science & Technology Beijing; Hong Kong University of
   Science & Technology; University of New South Wales Sydney; National
   University of Singapore; Shenzhen Research Institute of Big Data; The
   Chinese University of Hong Kong, Shenzhen
RP Zhang, QQ (corresponding author), Univ New South Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia.
EM qianxy@ustb.edu.cn; weixue@ust.hk; qiquan.zhang@unsw.edu.au;
   ruijie.tao@u.nus.edu; haizhouli@cuhk.edu.cn
RI qian, xinyuan/ADN-5425-2022; Haizhou, LI/ITT-8410-2023
OI Haizhou, LI/0009-0001-1165-6131; Li, Haizhou/0000-0001-9158-9401; Qian,
   Xinyuan/0000-0002-9511-6713
FU National Natural Science Foundation of China
FX No Statement Available
CR Akaho S., 2001, PROC INT M PSYCHOMET, P1
   [Anonymous], 2013, P ACM INT C MULTIMED
   Bahdanau D., 2015, P INT C LEARN REPR, P246
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081
   Chen C., 2021, P INT C LEARN REPR, P9622
   Chen CA, 2022, PROC CVPR IEEE, P18836, DOI 10.1109/CVPR52688.2022.01829
   Chen CG, 2021, PROC CVPR IEEE, P15511, DOI 10.1109/CVPR46437.2021.01526
   Chen Changan, 2020, LNCS, P17, DOI [DOI 10.1007/978-3-030-58539-6_2, 10.1007/978-3-030-58539-62]
   Chen T, 2020, PR MACH LEARN RES, V119
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Desai K, 2021, PROC CVPR IEEE, P11157, DOI 10.1109/CVPR46437.2021.01101
   Biten AF, 2019, PROC CVPR IEEE, P12458, DOI 10.1109/CVPR.2019.01275
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gildenblat Jacob, 2021, Pytorch Library for Cam Methods
   Girdhar R, 2023, PROC CVPR IEEE, P15180, DOI 10.1109/CVPR52729.2023.01457
   Gu CB, 2022, NEUROCOMPUTING, V496, P166, DOI 10.1016/j.neucom.2022.01.078
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Jia C, 2021, PR MACH LEARN RES, V139
   Jiang XY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P69, DOI 10.1145/2733373.2806240
   Kang PP, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P226, DOI 10.1145/3323873.3325029
   Kon H, 2020, ACOUST SCI TECHNOL, V41, P675, DOI 10.1250/ast.41.675
   Kon H, 2019, J AUDIO ENG SOC, V67, P540, DOI 10.17743/jaes.2018.0069
   Kong QQ, 2020, IEEE-ACM T AUDIO SPE, V28, P2880, DOI 10.1109/TASLP.2020.3030497
   Li Dongge, 2003, P 11 ACM INT C MULT, P604, DOI DOI 10.1145/957013.957143
   Li K, 2017, IEEE T PATTERN ANAL, V39, P1825, DOI 10.1109/TPAMI.2016.2610969
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Liu Haohe, 2021, arXiv
   Liu YX, 2023, IEEE T MULTIMEDIA, V25, P2851, DOI 10.1109/TMM.2022.3152086
   Majumder S., 2022, P INT C NEURAL INF P, P658
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Ngiam A., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Parida KK, 2021, PROC CVPR IEEE, P8264, DOI 10.1109/CVPR46437.2021.00817
   Park DS, 2019, INTERSPEECH, P2613, DOI 10.21437/Interspeech.2019-2680
   PAUL DB, 1992, SPEECH AND NATURAL LANGUAGE, P357
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Purushwalkam Senthil, 2021, P IEEE CVF INT C COM, P1183
   Radford A, 2021, PR MACH LEARN RES, V139
   Rasiwasia Nikhil, 2010, P 18 ACM INT C MULT, P251
   Savioja L, 2015, J ACOUST SOC AM, V138, P708, DOI 10.1121/1.4926438
   Shvetsova N, 2022, PROC CVPR IEEE, P19988, DOI 10.1109/CVPR52688.2022.01939
   Singh N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P286, DOI 10.1109/ICCV48922.2021.00035
   Srivastava N., 2012, P INT C NEUR INF PRO, P233
   Stan GB, 2002, J AUDIO ENG SOC, V50, P249
   Straub J, 2019, Arxiv, DOI arXiv:1906.05797
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Valin J.-M., 2006, P IEEE INT C AC SPEE, P134
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Werner S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11031150
   Wu F, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107335
   Wu HH, 2022, INT CONF ACOUST SPEE, P4563, DOI 10.1109/ICASSP43922.2022.9747669
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yu E, 2022, NEUROCOMPUTING, V486, P215, DOI 10.1016/j.neucom.2021.11.035
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Zhai XH, 2022, PROC CVPR IEEE, P18102, DOI 10.1109/CVPR52688.2022.01759
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
NR 62
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4480
EP 4489
DI 10.1109/TMM.2023.3323876
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100034
DA 2024-08-05
ER

PT J
AU Wang, MD
   Li, XM
   Chen, SQ
   Zhang, XL
   Ma, L
   Zhang, Y
AF Wang, Mingdao
   Li, Xueming
   Chen, Siqi
   Zhang, Xianlin
   Ma, Lei
   Zhang, Yue
TI Learning Representations by Contrastive Spatio-Temporal Clustering for
   Skeleton-Based Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive learning; skeleton-based action recognition; spatio-temporal
   clustering
ID NETWORK; LSTM
AB Self-supervised representation learning has proven constructive for skeleton-based action recognition. For better performance, existing methods mainly focus on 1) multi-modal data augmentations and 2) triplet contrastive samples construction. However, designing these strategies is always heuristics and hard. Instead of exploring more similar strategies, this paper addresses this issue with a different view and proposes a novel Contrastive Spatio-Temporal Clustering (CSTC) module. CSTC constructs a supervised signal (pseudo-label) of action sequences in an online clustering manner, and it is complementary to the recent data augmentations or triplet contrastive samples construction strategies. Specifically, CSTC can be formulated as an optimal transport problem. we introduce the spatio-temporal regularizations into the original optimal transport term to guide the pseudo-label generation, i.e., a semantic regularization learned by frame index is proposed to constrain the frame order, and a prior normal distribution regularization based on sampling characteristics of samples is proposed to maintain the dependability of spatial cluster assignments. Furthermore, to enhance the learning of latent features, we propose a Bidirectional Cross-modal Clustering Consistency Objective (B3CO) to enforce cluster assignments consistency for different modalities of the same sample. Last, since fusing spatial and temporal clustering losses directly during back-propagation will confuse the learned dimension-specific semantics, we propose a simple yet effective training strategy to fix it by training the model using these two losses alternately. By integrating the above designs into the MoCo framework, we propose a Contrastive Spatio-Temporal Clustering Network (CSTCN), which can excavate cross-modal discriminative spatio-temporal features in the clustering space. Experimental results on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets show that CSTCN achieves state-of-the-art performance in both single- and multi-modal models, especially in the KNN and semi-supervised evaluation protocols. Besides, the key module CSTC shows good generalization capability, and achieves consistent performance improvement on the basis of several state-of-the-art methods which focus on data augmentations and triplet contrastive samples construction.
C1 [Wang, Mingdao; Chen, Siqi] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
   [Li, Xueming; Zhang, Xianlin; Zhang, Yue] Beijing Univ Posts & Telecommun, Sch Digital Media & Design Arts, Beijing 100876, Peoples R China.
   [Ma, Lei] Univ Alberta, Dept Elect & Comp Engn, Edmonton T6G 2H5, AB, Canada.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications; University of Alberta
RP Zhang, Y (corresponding author), Beijing Univ Posts & Telecommun, Sch Digital Media & Design Arts, Beijing 100876, Peoples R China.
EM wmingdao@bupt.edu.cn; lxm@bupt.edu.cn; sqchen@bupt.edu.cn;
   zxlin@bupt.edu.cn; lma7@ualberta.ca; zhangyuereal@163.com
RI Chen, Siqi/HKN-8779-2023
OI Zhang, Yue/0000-0002-6327-5023; Wang, Mingdao/0009-0004-2907-086X; ,
   Xianlin/0000-0003-3905-2062
FU National Key Ramp;D Program of China Cultural Technology and Modern
   Service Industry Key Special Project
FX No Statement Available
CR Avola D, 2020, IEEE T MULTIMEDIA, V22, P2481, DOI 10.1109/TMM.2019.2960588
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen YX, 2022, LECT NOTES COMPUT SC, V13686, P185, DOI 10.1007/978-3-031-19809-0_11
   Chen Zhan, 2022, arXiv
   Chenyang Si, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P35, DOI 10.1007/978-3-030-58571-6_3
   Cuturi M., 2013, Ad-vances in Neural Information Processing Systems, V26, P1
   Devillers L, 2005, NEURAL NETWORKS, V18, P407, DOI 10.1016/j.neunet.2005.03.007
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Gao XH, 2023, IEEE T MULTIMEDIA, V25, P405, DOI 10.1109/TMM.2021.3127040
   Gidaris S, 2018, Arxiv, DOI arXiv:1803.07728
   Gidaris S, 2020, PROC CVPR IEEE, P6926, DOI 10.1109/CVPR42600.2020.00696
   GOOD IJ, 1963, ANN MATH STAT, V34, P911, DOI 10.1214/aoms/1177704014
   Guo TY, 2022, AAAI CONF ARTIF INTE, P762
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jones S, 2014, PROC CVPR IEEE, P604, DOI 10.1109/CVPR.2014.84
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kim B, 2022, LECT NOTES COMPUT SC, V13664, P209, DOI 10.1007/978-3-031-19772-7_13
   Kumar S, 2022, PROC CVPR IEEE, P20142, DOI 10.1109/CVPR52688.2022.01954
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Li LG, 2021, PROC CVPR IEEE, P4739, DOI 10.1109/CVPR46437.2021.00471
   Liang DH, 2019, IEEE COMPUT SOC CONF, P934, DOI 10.1109/CVPRW.2019.00123
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu CH, 2017, Arxiv, DOI arXiv:1703.07475
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu Q, 2022, LECT NOTES COMPUT SC, V13664, P137, DOI 10.1007/978-3-031-19772-7_9
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Luo DZ, 2020, AAAI CONF ARTIF INTE, V34, P11701
   Mao YY, 2022, LECT NOTES COMPUT SC, V13663, P734, DOI 10.1007/978-3-031-20062-5_42
   Asano YM, 2020, Arxiv, DOI arXiv:1911.05371
   Mu N, 2022, LECT NOTES COMPUT SC, V13686, P529, DOI 10.1007/978-3-031-19809-0_30
   Pang C, 2023, IEEE T MULTIMEDIA, V25, P8699, DOI 10.1109/TMM.2023.3239751
   Peng B, 2020, IEEE T IND INFORM, V16, P555, DOI 10.1109/TII.2019.2937514
   Qiang Nie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P102, DOI 10.1007/978-3-030-58529-7_7
   Radford A, 2021, PR MACH LEARN RES, V139
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sudha MR, 2017, INT J AMBIENT COMPUT, V8, P1, DOI 10.4018/IJACI.2017100101
   Sun ZH, 2023, IEEE T PATTERN ANAL, V45, P3200, DOI 10.1109/TPAMI.2022.3183112
   Thoker FM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1655, DOI 10.1145/3474085.3475307
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang P, 2022, IEEE T IMAGE PROCESS, V31, P6224, DOI 10.1109/TIP.2022.3207577
   Xia RJ, 2022, IEEE T MULTIMEDIA, V24, P2648, DOI 10.1109/TMM.2021.3086758
   Xu SH, 2023, IEEE T MULTIMEDIA, V25, P624, DOI 10.1109/TMM.2021.3129616
   Xu SA, 2021, IEEE INTERNET THINGS, V8, P15990, DOI 10.1109/JIOT.2020.3042986
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yan XT, 2020, PROC CVPR IEEE, P6508, DOI 10.1109/CVPR42600.2020.00654
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Yang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13403, DOI 10.1109/ICCV48922.2021.01317
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang H, 2022, LECT NOTES COMPUT SC, V13664, P36, DOI 10.1007/978-3-031-19772-7_3
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
NR 55
TC 1
Z9 2
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3207
EP 3220
DI 10.1109/TMM.2023.3307933
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200010
DA 2024-08-05
ER

PT J
AU Wu, KL
   Huang, J
   Ma, Y
   Fan, F
   Ma, JY
AF Wu, Kangle
   Huang, Jun
   Ma, Yong
   Fan, Fan
   Ma, Jiayi
TI Cycle-Retinex: Unpaired Low-Light Image Enhancement via Retinex-Inline
   CycleGAN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low light enhancement; CycleGAN; Retinex theory; unsupervised learning
ID HISTOGRAM EQUALIZATION; FUSION
AB Low-light image enhancement aims to recover normal-light images from the images captured under dim environments. Most existing methods could just improve the light appearance globally whereas failing to handle other degradation such as dense noise, color offset and extremely low-light. Moreover, unsupervised methods proposed in recent years lack reliable physical model as the basis, thus universality is greatly limited. To address these problems, we propose a novel low-light image enhancement method via Retinex-inline cycle-consistent generative adversarial network named Cycle-Retinex, whose training is totally dependent on unpaired datasets. Specifically, we organically combine Retinex theory with CycleGAN, by which we decouple low-light image enhancement task into two sub-tasks, i.e. illumination map enhancement and reflectance map restoration. Retinex theory helps CycleGAN simplify low-light image enhancement problem and CycleGAN provides synthetic paired images to guide the training of Retinex decomposition network. We further introduce a self-augmented method to address the color distortion and noise problem, thus making the network learn to enhance low-light images adaptively. Extensive experiments show that the proposed method can achieve promising results.
C1 [Wu, Kangle; Huang, Jun; Ma, Yong; Fan, Fan; Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Huang, J (corresponding author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
EM wukangle@cug.edu.cn; junhwong@whu.edu.cn; mayong@whu.edu.cn;
   fanfan@whu.edu.cn; jyma2010@gmail.com
RI Ma, Jiayi/Y-2470-2019
OI Ma, Jiayi/0000-0003-3264-3265; Huang, Jun/0000-0001-5893-4090; Wu,
   Kangle/0000-0002-0147-756X; Wu, Kangle/0009-0009-3469-047X
FU National Natural Science Foundation of China
FX No Statement Available
CR Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Huang HF, 2022, IEEE T IMAGE PROCESS, V31, P1391, DOI 10.1109/TIP.2022.3140610
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jin X, 2019, IEEE IMAGE PROC, P2761, DOI [10.1109/icip.2019.8803238, 10.1109/ICIP.2019.8803238]
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lei XZ, 2023, IEEE T MULTIMEDIA, V25, P4439, DOI 10.1109/TMM.2022.3175634
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma L, 2023, IEEE T MULTIMEDIA, V25, P3573, DOI 10.1109/TMM.2022.3162493
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Sakaridis C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10745, DOI 10.1109/ICCV48922.2021.01059
   Shen L, 2017, Arxiv, DOI arXiv:1711.02488
   Tang LF, 2024, IEEE T NEUR NET LEAR, V35, P2694, DOI 10.1109/TNNLS.2022.3190880
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang WC, 2022, SIGNAL PROCESS-IMAGE, V106, DOI 10.1016/j.image.2022.116742
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wei C., 2018, PROC BRIT MACH VIS C, P1
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Yang WH, 2020, IEEE T IMAGE PROCESS, V29, P5737, DOI 10.1109/TIP.2020.2981922
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhu AQ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102962
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 47
TC 1
Z9 1
U1 56
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1213
EP 1228
DI 10.1109/TMM.2023.3278385
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700028
DA 2024-08-05
ER

PT J
AU Xun, ZZ
   Di, SZ
   Gao, YL
   Tang, ZH
   Wang, G
   Liu, S
   Li, B
AF Xun, Zizheng
   Di, Shangzhe
   Gao, Yulu
   Tang, Zongheng
   Wang, Gang
   Liu, Si
   Li, Bo
TI Linker: Learning Long Short-term Associations for Robust Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Long short-term; visual object tracking; transformer
AB Siamese and Transformer trackers have demonstrated exceptional performance in visual object tracking. These methods utilize initial and potentially online templates to locate the target in subsequent frames. Despite their success, these trackers are vulnerable to changes in the target's appearance due to slow template updates and interference from similar objects, resulting from the absence of scene information. To address these issues, we introduce a reference region within our tracker. The reference region is updated rapidly, providing short-term scene information. By associating the initial template, reference region, and current search region, we enhance the tracker's ability to adapt to changes in target appearance and discriminate between the target and other objects. Additionally, we propose a novel Reference-Enhance (RE) module, which aggregates contextually relevant information from the reference region to enhance the template feature. Extensive experiments show our method achieves state-of-the-art performance on six popular visual object tracking benchmarks while running at over 40 FPS.
C1 [Xun, Zizheng; Di, Shangzhe; Gao, Yulu; Tang, Zongheng; Liu, Si; Li, Bo] Beihang Univ, Inst Artificial Intelligence, Beijing 100083, Peoples R China.
   [Wang, Gang] Beijing Inst Basic Med Sci, Beijing 100850, Peoples R China.
   [Wang, Gang] Chinese Inst Brain Res, Beijing 100850, Peoples R China.
C3 Beihang University; Academy of Military Medical Sciences - China;
   Chinese Institute for Brain Research, Beijing
RP Wang, G (corresponding author), Beijing Inst Basic Med Sci, Beijing 100850, Peoples R China.; Wang, G (corresponding author), Chinese Inst Brain Res, Beijing 100850, Peoples R China.
EM xunzz@buaa.edu.cn; shangzhe.di@gmail.com; gyl97@buaa.edu.cn;
   tzhhhh123@buaa.edu.cn; g_wang@foxmail.com; liusi@buaa.edu.cn;
   boli@buaa.edu.cn
OI liu, si/0000-0002-9180-2935; WANG, Gang/0000-0002-1916-6110; Di,
   Shangzhe/0009-0005-6977-4332
FU Beijing Nova Program
FX No Statement Available
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Carion N., 2020, EUR C COMP VIS, P213
   Chen BY, 2022, LECT NOTES COMPUT SC, V13682, P375, DOI 10.1007/978-3-031-20047-2_22
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Cui Y., 2022, arXiv
   Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Gao SY, 2022, LECT NOTES COMPUT SC, V13682, P146, DOI 10.1007/978-3-031-20047-2_9
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P486, DOI 10.1109/TMM.2021.3128047
   Law J., CornerNet: Detecting objects as paired keypoints
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Lin L., 2021, P ADV NEUR INF PROC, P16743
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Mayer C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13424, DOI 10.1109/ICCV48922.2021.01319
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Nie JH, 2023, IEEE T MULTIMEDIA, V25, P6194, DOI 10.1109/TMM.2022.3206668
   Ren S., 2015, Advances in neural information processing systems, P91
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   van Heusden E, 2018, J NEUROSCI, V38, P8243, DOI 10.1523/JNEUROSCI.0736-18.2018
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang X, 2021, PROC CVPR IEEE, P13758, DOI 10.1109/CVPR46437.2021.01355
   Xie F, 2022, PROC CVPR IEEE, P8741, DOI 10.1109/CVPR52688.2022.00855
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Ye BT, 2022, Arxiv, DOI arXiv:2203.11991
   Yuan D, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3266837
   Yuan D, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486678
   Zhang ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13319, DOI 10.1109/ICCV48922.2021.01309
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhou ZK, 2022, PROC CVPR IEEE, P8751, DOI 10.1109/CVPR52688.2022.00856
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 47
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6228
EP 6237
DI 10.1109/TMM.2023.3347644
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600021
DA 2024-08-05
ER

PT J
AU Yan, H
   Zhang, HJ
   Zhang, Z
AF Yan, Han
   Zhang, Haijun
   Zhang, Zhao
TI Learning to Disentangle the Colors, Textures, and Shapes of Fashion
   Items: A Unified Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Image color analysis; Generators; Feature extraction; Task
   analysis; Training; Image synthesis; Intelligent design; fashion
   disentanglement; generative adversarial network; image-to-image
   translation
AB Today,fashion design can be readily performed by most people due to the rapid development of design tools. However, not everyone possesses the professional skills to produce an aesthetically pleasing design. In order to assist an inexperienced user during the design process, this research explored a new fashion-related disentanglement task, with the goal of creating novel fashion items with controllable attributes. The key idea is to develop a unified framework, called CTS-GAN, by disentangling the colors, textures, and shapes of fashion items simultaneously using a generative adversarial network (GAN). Specifically, we first introduced a fashion attribute encoder to decompose input fashion items into three latent spaces, i.e., color, texture, and shape. A fashion item pattern-making module (FIPM)-based generator was then proposed to control the corresponding parameters of color and texture in FIPMs independently and combine them with the shape features in order to accomplish the final generation of new fashion items. Furthermore, three independent pathways were introduced to extract the representations of color, texture, and shape in fashion items to optimize our CTS-GAN in an unsupervised manner. Extensive experimental results demonstrate the effectiveness of our CTS-GAN and suggest that it can generate diverse, novel fashion images by taking full advantage of the controllability of the colors, textures, and shapes of different fashion items.
C1 [Yan, Han; Zhang, Haijun] Harbin Inst Technol, Dept Comp Sci, Shenzhen 518055, Peoples R China.
   [Zhang, Zhao] Hefei Univ Technol, Dept Comp Sci, Hefei 230601, Peoples R China.
C3 Harbin Institute of Technology; Hefei University of Technology
RP Zhang, HJ (corresponding author), Harbin Inst Technol, Dept Comp Sci, Shenzhen 518055, Peoples R China.
EM 20b351014@stu.hit.edu.cn; hjzhang@hit.edu.cn; cszzhang@gmail.com
RI Zhang, Zhao/B-5136-2010
OI Zhang, Zhao/0000-0002-5703-7969
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Afifi M, 2021, PROC CVPR IEEE, P7937, DOI 10.1109/CVPR46437.2021.00785
   Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cui YR, 2018, COMPUT GRAPH FORUM, V37, P109, DOI 10.1111/cgf.13552
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Gui J, 2023, IEEE T KNOWL DATA EN, V35, P3313, DOI 10.1109/TKDE.2021.3130191
   Harkonen E, 2020, C NEUR INF PROC SYST
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ji YZ, 2018, NEUROCOMPUTING, V322, P130, DOI 10.1016/j.neucom.2018.09.061
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li YJ, 2017, ADV NEUR IN, V30
   Liu GH, 2013, PATTERN RECOGN, V46, P188, DOI 10.1016/j.patcog.2012.06.001
   Liu M.-Y., 2017, ADV NEURAL INFORM PR, V30, P700, DOI DOI 10.48550/ARXIV.1703.00848
   Song GX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459771
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang TF, 2022, PROC CVPR IEEE, P11369, DOI 10.1109/CVPR52688.2022.01109
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Yan H, 2023, IEEE T MULTIMEDIA, V25, P2323, DOI [10.1109/TCSS.2022.3161996, 10.1109/TMM.2022.3146010]
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2022, P ACM SIGGRAPH C, P1
   Zhang Z, 2021, ADV NEUR IN
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 40
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5615
EP 5629
DI 10.1109/TMM.2023.3338050
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600025
DA 2024-08-05
ER

PT J
AU Yang, X
   Wang, X
   Liu, LC
   Wang, NN
   Gao, XB
AF Yang, Xi
   Wang, Xian
   Liu, Liangchen
   Wang, Nannan
   Gao, Xinbo
TI STFE: A Comprehensive Video-Based Person Re-Identification Network Based
   on Spatio-Temporal Feature Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion feature extraction; spatio-temporal information learning;
   video-based person re-identification (Re-ID)
AB Video-based person re-identification (Re-ID) is designed to retrieve target pedestrians in video sequences under non-overlapping cameras. At present, mainstream approaches post-process the feature map extracted by the convolutional neural network backbone to obtain a global representation or a fine-grained local representation for higher accuracy. However, they still suffer from challenges, such as information loss for global-based methods and spatio-temporal feature fragmentation for local-based methods. To alleviate these problems, this article proposes a Spatio-Temporal Feature Enhancement (STFE) network from a spatio-temporal comprehensive perspective, combining the advantages of the above methods to obtain more comprehensive information from video tracklets. STFE consists of two main modules: Feature Space Projection Module (FSPM) and Global Low-frequency Enhancement Module (GLEM). FSPM mathematically converts continuous video information into a discrete feature space and selectively retains more useful information, thus avoiding spatio-temporal information loss. Meanwhile, FSPM applies global features instead of dividing feature maps spatially, thereby avoiding spatio-temporal feature fragmentation. In addition, GLEM which is based on transformer, acts as a broadband low-pass filter to mine richer global comprehensive information. Finally, by combining FSPM with GLEM, STFE can obtain spatio-temporal comprehensive video representation. Extensive experiments were conducted on two widely-used video Re-ID datasets. The experimental results verify our idea and demonstrate the effectiveness of the proposed STFE with 95.5% Rank-1 accuracy on MARS benchmarks, which surpasses previous state-of-the-arts by a large margin of +4%.
C1 [Yang, Xi; Liu, Liangchen; Wang, Nannan] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Wang, Xian] Xidian Univ, Hangzhou Inst Technol, Hangzhou 311231, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Gao, Xinbo] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University; Chongqing University of Posts &
   Telecommunications; Xidian University
RP Gao, XB (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
EM yangx@xidian.edu.cn; xwang_01@stu.xidian.edu.cn;
   lcliu79xidian@gmail.com; nnwang@xidian.edu.cn; gaoxb@cqupt.edu.cn
OI Wang, Nannan/0000-0002-4695-6134
FU National Natural Science Foundation of China
FX No Statement Available
CR Aich A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P152, DOI 10.1109/ICCV48922.2021.00022
   [Anonymous], 2018, IEEE Trans. Image Process., V27, P5683
   Cao B., 2024, Assoc. Advance. Artif. Intell. (AAAI)
   Cao B, 2023, IEEE I CONF COMP VIS, P23498, DOI 10.1109/ICCV51070.2023.02153
   Cao B, 2024, IEEE T NEUR NET LEAR, V35, P9728, DOI 10.1109/TNNLS.2023.3236486
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Ding CX, 2022, IEEE T PATTERN ANAL, V44, P1474, DOI 10.1109/TPAMI.2020.3024900
   Ehrlich M, 2019, IEEE I CONF COMP VIS, P3483, DOI 10.1109/ICCV.2019.00358
   Eom C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12016, DOI 10.1109/ICCV48922.2021.01182
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8287
   Gao JY, 2018, Arxiv, DOI arXiv:1805.02104
   Gu XQ, 2022, IEEE T IMAGE PROCESS, V31, P3908, DOI 10.1109/TIP.2022.3175593
   Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P660, DOI 10.1007/978-3-030-58598-3_39
   Haohan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8681, DOI 10.1109/CVPR42600.2020.00871
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He TY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1470, DOI 10.1109/ICCV48922.2021.00152
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Lee H., 2009, P 26 ANN INT C MACH, P609
   Li JN, 2019, IEEE I CONF COMP VIS, P3957, DOI 10.1109/ICCV.2019.00406
   Li JN, 2020, IEEE T IMAGE PROCESS, V29, P4461, DOI 10.1109/TIP.2020.2972108
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu C.-T., 2019, PROC BRIT MACH VIS C
   Liu H, 2018, IEEE T CIRC SYST VID, V28, P2788, DOI 10.1109/TCSVT.2017.2715499
   Liu JW, 2021, PROC CVPR IEEE, P4368, DOI 10.1109/CVPR46437.2021.00435
   Liu JW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231741
   Liu LC, 2023, IEEE T IMAGE PROCESS, V32, P4287, DOI 10.1109/TIP.2023.3296901
   Liu LC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P227, DOI 10.1145/3474085.3475566
   Liu XH, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3271353
   Liu XH, 2021, Arxiv, DOI arXiv:2104.01745
   Liu XH, 2021, PROC CVPR IEEE, P13329, DOI 10.1109/CVPR46437.2021.01313
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Lu JX, 2023, IEEE T IMAGE PROCESS, V32, P949, DOI 10.1109/TIP.2023.3236144
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Meng JK, 2019, PATTERN RECOGN, V93, P430, DOI 10.1016/j.patcog.2019.04.008
   Pan Z., 2022, Adv. Neural Inf. Process. Syst., V35
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Ranzato M., 2007, ADV NEURAL INFORM PR, V20, P1185, DOI DOI 10.1109/CVPR.2007.383157
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruibing Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P388, DOI 10.1007/978-3-030-58595-2_24
   Shao ZY, 2023, IEEE I CONF COMP VIS, P11140, DOI 10.1109/ICCV51070.2023.01026
   Si C., 2022, Adv. Neural Inf. Process. Syst, V35, P23495
   Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065
   Tan WT, 2024, IEEE T MULTIMEDIA, V26, P1600, DOI 10.1109/TMM.2023.3283878
   Tang ZY, 2023, IEEE T MULTIMEDIA, V25, P7917, DOI 10.1109/TMM.2022.3231103
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K, 2021, IEEE T IMAGE PROCESS, V30, P3405, DOI 10.1109/TIP.2021.3060909
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P3416, DOI 10.1109/TIP.2019.2959923
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang PY, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3209537
   Wang XD, 2023, IEEE T MULTIMEDIA, V25, P9597, DOI 10.1109/TMM.2023.3256092
   Wang YQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12006, DOI 10.1109/ICCV48922.2021.01181
   Wu YM, 2020, IEEE T IMAGE PROCESS, V29, P8821, DOI 10.1109/TIP.2020.3001693
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xie QK, 2023, IEEE T MULTIMEDIA, V25, P6384, DOI 10.1109/TMM.2022.3207949
   Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507
   Yan YC, 2016, LECT NOTES COMPUT SC, V9910, P701, DOI 10.1007/978-3-319-46466-4_42
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Yin D, 2019, ADV NEUR IN, V32
   Zhang GW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P516, DOI 10.1145/3474085.3475202
   Zhang RM, 2019, IEEE T IMAGE PROCESS, V28, P4870, DOI 10.1109/TIP.2019.2911488
   Zhang ZZ, 2020, INT CONF CONDIT MON, P404, DOI 10.1109/CVPR42600.2020.01042
   Zhang Z, 2024, IEEE T MULTIMEDIA, V26, P2303, DOI 10.1109/TMM.2023.3294816
   Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zhu K, 2023, IEEE T NEUR NET LEAR, V34, P8531, DOI 10.1109/TNNLS.2022.3151487
   Zhu XR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1706, DOI 10.1145/3394171.3413843
NR 72
TC 3
Z9 3
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7237
EP 7249
DI 10.1109/TMM.2024.3362136
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000029
DA 2024-08-05
ER

PT J
AU Yu, S
   Zhai, DH
   Xia, YQ
   Li, D
   Zhao, SQ
AF Yu, Sheng
   Zhai, Di-Hua
   Xia, Yuanqing
   Li, Dong
   Zhao, Shiqi
TI CatTrack: Single-Stage Category-Level 6D Object Pose Tracking via
   Convolution and Vision Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE pose tracking; transformer; Pose estimation
AB In the current research, many researchers have focused on instance-level pose tracking, which requires a 3D model of the object in advance, making it challenging to apply in practice. To address this limitation, some researchers have proposed the category-level object pose tracking method. Achieving accurate and speedy monocular category-level pose tracking is an essential research goal. In this article, we propose CatTrack, a new single-stage keypoints-based monocular category-level multi-object pose tracking network. A significant issue in object pose tracking tasks is utilizing the information from the previous frame to guide pose estimation for the next frame. However, as the object poses and camera information in each frame are different, we need to remove irrelevant information and emphasize useful features. To this end, we propose a transformer-based temporal information capture module to leverage the position information of keypoints from the previous frame. Furthermore, we propose a new keypoint matching module to enable the grouping and matching of object keypoints in complex scenes. We have successfully applied CatTrack to the Objectron dataset and achieved superior results in comparison to existing methods. Furthermore, we have also evaluated the generalization of CatTrack and successfully applied it to track the 6D pose of unseen real-world objects.
C1 [Yu, Sheng; Zhai, Di-Hua; Xia, Yuanqing] Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China.
   [Zhai, Di-Hua] Beijing Inst Technol, Yangtze Delta Reg Acad, Jiaxing 314001, Peoples R China.
   [Li, Dong; Zhao, Shiqi] China Unicom Res Inst, Beijing 102676, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology
RP Zhai, DH (corresponding author), Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China.
EM yusheng@bit.edu.cn; zhaidih@bit.edu.cn; xia_yuanqing@bit.edu.cn;
   lid61@chinaunicom.cn; zhaosq82@chinaunicom.cn
RI yu, sheng/IUO-6047-2023; Zhao, Shiqi/AAT-2160-2020
OI yu, sheng/0009-0002-0709-1024; Zhao, Shiqi/0000-0002-2360-956X; xia,
   yuqin/0009-0007-4022-9175; Zhao, Shiqi/0009-0006-2508-7108
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdel-Aziz YI, 2015, PHOTOGRAMM ENG REM S, V81, P103, DOI 10.14358/PERS.81.2.103
   Ahmadyan A, 2021, PROC CVPR IEEE, P7818, DOI 10.1109/CVPR46437.2021.00773
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cao Z, 2016, IEEE INT CONF ROBOT, P2441, DOI 10.1109/ICRA.2016.7487396
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2753, DOI 10.1109/ICCV48922.2021.00277
   Chen Wang, 2020, 2020 IEEE International Conference on Robotics and Automation (ICRA), P10059, DOI 10.1109/ICRA40945.2020.9196679
   Chen W, 2021, PROC CVPR IEEE, P1581, DOI 10.1109/CVPR46437.2021.00163
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Choi C, 2013, IEEE INT C INT ROBOT, P1084, DOI 10.1109/IROS.2013.6696485
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2968, DOI 10.1109/ICCV48922.2021.00298
   Deng XK, 2021, IEEE T ROBOT, V37, P1328, DOI 10.1109/TRO.2021.3056043
   Dengsheng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11970, DOI 10.1109/CVPR42600.2020.01199
   Dong XP, 2023, IEEE T PATTERN ANAL, V45, P8049, DOI 10.1109/TPAMI.2022.3230064
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Gao TZ, 2022, IEEE T INTELL VEHICL, V7, P240, DOI 10.1109/TIV.2022.3143954
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He K., 2017, P IEEE INT C COMP VI, P2961
   He W., 2020, P IEEE CVF C COMP VI, P11629, DOI [10.1109/CVPR42600.2020.01165, DOI 10.1109/CVPR42600.2020.01165]
   Hinterstoisser S., 2012, ACCV, P548
   Hou T., 2020, MobilePose: Real-time pose estimation for unseen objects with weak shape supervision
   Huang WL, 2022, IEEE T MULTIMEDIA, V24, P3025, DOI 10.1109/TMM.2021.3092149
   Irshad MZ, 2022, IEEE INT CONF ROBOT, P10632, DOI 10.1109/ICRA46639.2022.9811799
   Issac J, 2016, IEEE INT CONF ROBOT, P608, DOI 10.1109/ICRA.2016.7487184
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kumar M, 2006, P AMER CONTR CONF, V1-12, P2078, DOI 10.1109/ACC.2006.1656526
   Lee T, 2021, IEEE ROBOT AUTOM LET, V6, P8575, DOI 10.1109/LRA.2021.3110538
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li Y, 2011, IEEE T PATTERN ANAL, V33, P1860, DOI 10.1109/TPAMI.2011.40
   Li Y, 2019, INT J COMPUT VISION, pNIL_20
   Li ZG, 2019, IEEE I CONF COMP VIS, P7677, DOI 10.1109/ICCV.2019.00777
   Lin JH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3540, DOI 10.1109/ICCV48922.2021.00354
   Lin Liting, 2022, ADV NEUR IN
   Lin Yunzhi, 2022, 2022 International Conference on Robotics and Automation (ICRA), P1258, DOI 10.1109/ICRA46639.2022.9811720
   Lin YZ, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022), P1547, DOI 10.1109/ICRA46639.2022.9812299
   Liu L, 2018, IEEE T INTELL TRANSP, V19, P2432, DOI 10.1109/TITS.2017.2749409
   Liu YP, 2019, IEEE T MULTIMEDIA, V21, P2776, DOI 10.1109/TMM.2019.2913321
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P2386, DOI 10.1109/TPAMI.2020.3041332
   Meng Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P530, DOI 10.1007/978-3-030-58589-1_32
   Meyer GP, 2020, IEEE INT C INT ROBOT, P10521, DOI 10.1109/IROS45743.2020.9341623
   Oberweger M, 2018, LECT NOTES COMPUT SC, V11219, P125, DOI 10.1007/978-3-030-01267-0_8
   Pauwels K, 2015, IEEE INT C INT ROBOT, P1300, DOI 10.1109/IROS.2015.7353536
   Pavlakos G., 2017, ICRA, P2011
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413
   Rünz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024
   Sahin C., 2018, P EUR C COMP VIS WOR, P1
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051
   Su YZ, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P222, DOI 10.1109/ISMAR-Adjunct.2019.00-42
   Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038
   Tjaden H, 2017, IEEE I CONF COMP VIS, P124, DOI 10.1109/ICCV.2017.23
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang DM, 2022, IEEE T MULTIMEDIA, V24, P4394, DOI 10.1109/TMM.2021.3117092
   Wang H, 2019, PROC CVPR IEEE, P2637, DOI 10.1109/CVPR.2019.00275
   Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wen BW, 2021, IEEE INT C INT ROBOT, P8067, DOI 10.1109/IROS51168.2021.9635991
   Wen BW, 2020, IEEE INT C INT ROBOT, P10367, DOI 10.1109/IROS45743.2020.9341314
   Weng YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13189, DOI 10.1109/ICCV48922.2021.01296
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wüthrich M, 2013, IEEE INT C INT ROBOT, P3195, DOI 10.1109/IROS.2013.6696810
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yin JB, 2023, IEEE T PATTERN ANAL, V45, P9822, DOI 10.1109/TPAMI.2021.3125981
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zakharov S, 2019, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2019.00203
   Zhang ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13319, DOI 10.1109/ICCV48922.2021.01309
   Zheng Minghang, 2020, End-to-end object detection with adaptive clustering transformer
   Zhou Qian-Yi, 2018, ARXIV180109847
   Zhou X, 2019, PSYCHOL HEALTH, V34, P811, DOI 10.1080/08870446.2019.1574348
NR 86
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1665
EP 1680
DI 10.1109/TMM.2023.3284598
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800026
DA 2024-08-05
ER

PT J
AU Zeng, K
   Chen, KJ
   Zhang, WM
   Wang, YF
AF Zeng, Kai
   Chen, Kejiang
   Zhang, Weiming
   Wang, Yaofei
TI Upward Robust Steganography Based on Overflow Alleviation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE OSNs; robust steganography; asymmetric distortion; dither modulation;
   overflow
ID RESISTING JPEG COMPRESSION; DITHER MODULATION; STEGANALYSIS;
   WATERMARKING; NETWORK
AB Images with low quality factor (QF) are widely available and apposite as steganography cover, which will be JPEG recompressed with a preset larger QF when uploaded to online social networks. This scenario is known as "Upward Robust," which is currently a hotspot of robust steganography. The state-of-the-art algorithm is Generalized dither Modulation-based robust Adaptive Steganography (GMAS). However, GMAS can only realize limited resistance to detection and compression due to robust domain selection. To overcome this problem, we meticulously explore three lossy operations in JPEG recompression and discover that the key problem is spatial overflow. Then, two preprocessing methods, overall scaling (OS) and specific truncation (ST), were presented to remove overflow before message embedding and generate a reference image. After pre-processing, the stability of the image coefficients during JPEG recompression will be significantly enhanced. Therefore, we no longer need robust domain selection and all coefficients are eligible as cover, which improves security and embedding capacity. Additionally, the reference image was employed as guidance to build asymmetric distortion for removing overflow during embedding. Experimental results show that the proposed methods significantly surpass GMAS in terms of security and achieve comparable robustness.
C1 [Zeng, Kai; Chen, Kejiang; Zhang, Weiming] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
   [Zeng, Kai; Chen, Kejiang; Zhang, Weiming] Anhui Prov Key Lab oratory Cyberspace Secur Situat, Hefei, Peoples R China.
   [Wang, Yaofei] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230002, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Hefei University of Technology
RP Chen, KJ (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
EM zk0128@mail.ustc.edu.cn; chenkj@ustc.edu.cn; zhangwm@ustc.edu.cn;
   yaofei@mail.ustc.edu.cn
OI Zhang, Weiming/0000-0001-5576-6108; Zeng, Kai/0009-0007-6275-9767; Chen,
   Kejiang/0000-0002-9868-3414
FU National Natural Science Foundation of China
FX No Statement Available
CR Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Dong Y, 2023, IEEE T MULTIMEDIA, V25, P2698, DOI 10.1109/TMM.2022.3150180
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Guo LJ, 2015, IEEE T INF FOREN SEC, V10, P2669, DOI 10.1109/TIFS.2015.2473815
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2015, IEEE T INF FOREN SEC, V10, P219, DOI 10.1109/TIFS.2014.2364918
   Kin-Cleaves C, 2018, IEEE INT WORKS INFOR
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Kodovsky J, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P63
   Li B, 2014, IEEE T INF FOREN SEC, V9, P1264, DOI 10.1109/TIFS.2014.2326954
   Li CL, 2015, NEUROCOMPUTING, V166, P404, DOI 10.1016/j.neucom.2015.03.039
   Li WX, 2020, IEEE T CIRC SYST VID, V30, P2288, DOI 10.1109/TCSVT.2019.2925118
   Li WX, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P5, DOI 10.1145/3206004.3206008
   Lu W, 2021, IEEE T CIRC SYST VID, V31, P2909, DOI 10.1109/TCSVT.2020.3027843
   Luo WQ, 2010, IEEE T INF FOREN SEC, V5, P480, DOI 10.1109/TIFS.2010.2051426
   Miyazaki A, 2001, INT CONF ACOUST SPEE, P1969, DOI 10.1109/ICASSP.2001.941333
   Niu YK, 2019, SIGNAL PROCESS-IMAGE, V76, P89, DOI 10.1016/j.image.2019.04.016
   Pevny T, 2007, PROC SPIE, V6505, DOI 10.1117/12.696774
   Solanki K, 2004, IEEE T IMAGE PROCESS, V13, P1627, DOI 10.1109/TIP.2004.837557
   Solanki K, 2007, LECT NOTES COMPUT SC, V4567, P16
   Sun WW, 2021, IEEE T CIRC SYST VID, V31, P1208, DOI 10.1109/TCSVT.2020.2998476
   Tao JY, 2019, IEEE T CIRC SYST VID, V29, P594, DOI 10.1109/TCSVT.2018.2881118
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wang YF, 2021, IEEE T CIRC SYST VID, V31, P2082, DOI 10.1109/TCSVT.2020.3010554
   Wang YF, 2021, IEEE T INF FOREN SEC, V16, P1117, DOI 10.1109/TIFS.2020.3029908
   Yang JQ, 2014, IEEE T INF FOREN SEC, V9, P1933, DOI 10.1109/TIFS.2014.2359368
   Yu XZ, 2020, SIGNAL PROCESS, V168, DOI 10.1016/j.sigpro.2019.107343
   Zhang WM, 2007, IEEE SIGNAL PROC LET, V14, P848, DOI 10.1109/LSP.2007.903255
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zhang Y, 2018, MULTIMED TOOLS APPL, V77, P17913, DOI 10.1007/s11042-017-4506-3
   Zhang Y, 2016, SECUR COMMUN NETW, V9, P2957, DOI 10.1002/sec.1502
   Zhao ZZ, 2019, IEEE T INF FOREN SEC, V14, P1843, DOI 10.1109/TIFS.2018.2885438
NR 34
TC 4
Z9 4
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 299
EP 312
DI 10.1109/TMM.2023.3264628
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500013
DA 2024-08-05
ER

PT J
AU Zhou, Y
   Gong, WK
   Sun, YJ
   Li, LD
   Gu, K
   Wu, JJ
AF Zhou, Yu
   Gong, Weikang
   Sun, Yanjing
   Li, Leida
   Gu, Ke
   Wu, Jinjian
TI Quality Assessment for Stitched Panoramic Images via Patch Registration
   and Bidimensional Feature Aggregation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment; stitched panoramic images; patch registration;
   bidimensional feature aggregation
ID RANDOM GRAPHS; HODGERANK
AB Quality assessment for stitched panoramic images (SPIQA) is of great significance for the stitching algorithm optimization. By contrast, this task is much more challenging and arduous than traditional IQA task due to the high resolution of stitched panoramic images and the particularity and complexity of stitching distortions. For this task, we propose an effective method based on patch registration and bidimensional feature aggregation (PRBFA). First, inspired by the attention mechanism of the human visual system and the limited range of human vision, a soft patch segmentation and selection method is presented to determine the key patches in panoramic images to participate in the following patch matching and feature alignment stages, achieving patch registration between the panoramic image and the corresponding constituent images. Further, to fully simulate the human visual perception process from local viewport to panorama, the feature exploration is successively performed from local to global, which is also adaptive to the complexity of the distortions in stitched panoramic images. For performance testification, extensive experiments are conducted on the publicly released SPIQA database, the results of which prove the performance superiority of the PRBFA method.
C1 [Zhou, Yu; Gong, Weikang; Sun, Yanjing] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
   [Zhou, Yu] Xuzhou First Peoples Hosp, Xuzhou 221116, Peoples R China.
   [Li, Leida; Wu, Jinjian] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Gu, Ke] Beijing Lab Smart Environm Protect, Fac Informat Technol, Engn Res Ctr Intelligent Percept & Autonomous Cont, Beijing Key Lab Computat Intelligence & Intelligen, Beijing 100124, Peoples R China.
   [Wu, Jinjian] Beijing Univ Technol, Beijing Artificial IntelligenceInstitute, Beijing 100124, Peoples R China.
C3 China University of Mining & Technology; Xidian University; Beijing
   University of Technology
RP Sun, YJ (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
EM zhouy@cumt.edu.cn; gongweikang1998@163.com; yjsun@cumt.edu.cn;
   ldli@xidian.edu.cn; guke.doctor@gmail.com; jinjian.wu@mail.xidian.edu.cn
OI Sun, Yanjing/0000-0002-1389-3958
FU National Natural Science Foundation of China
FX No Statement Available
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Chen SJ, 2018, IEEE INT CON MULTI
   Chu X. X., 2023, PROC IEEE INT C LEAR, P517
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duan HY, 2023, IEEE J-STSP, V17, P1150, DOI 10.1109/JSTSP.2023.3250956
   Hou JW, 2020, IEEE IMAGE PROC, P3463, DOI 10.1109/ICIP40778.2020.9191241
   Jiang H, 2021, IEEE T IMAGE PROCESS, V30, P2364, DOI 10.1109/TIP.2021.3052073
   Kim HG, 2020, IEEE T CIRC SYST VID, V30, P917, DOI 10.1109/TCSVT.2019.2898732
   Kingma D. P., 2014, arXiv
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P2757, DOI 10.1109/TMM.2020.3016124
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Lim HT, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6737, DOI 10.1109/ICASSP.2018.8461317
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Ling SY, 2021, IEEE T MULTIMEDIA, V23, P4245, DOI 10.1109/TMM.2020.3038305
   Ling SY, 2018, IEEE INT CON MULTI
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Madhusudana PC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921858
   Meng CL, 2021, IEEE T MULTIMEDIA, V24, P3193, DOI 10.1109/TMM.2021.3096071
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Okarma K, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23111525
   Shaw P., 2018, Self-attention with relative position representations, P464
   Shuai X., 2008, PROC IEEE 19 INT C P, P1
   Sun W, 2020, IEEE J-STSP, V14, P64, DOI 10.1109/JSTSP.2019.2955024
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Tian C. Z., 2021, J. Vis. Commun. ImageRepresentation, V81, P1
   Tian CZ, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XJ, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2021.103051
   Wang XJ, 2021, IEEE T MULTIMEDIA, V23, P692, DOI 10.1109/TMM.2020.2986583
   Xu JH, 2021, IEEE T CIRC SYST VID, V31, P1724, DOI 10.1109/TCSVT.2020.3015186
   XU Q, 2013, ACM INT C MULT, P00043
   Xu QQ, 2014, IEEE T MULTIMEDIA, V16, P373, DOI 10.1109/TMM.2013.2292568
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang LY, 2017, IEEE INT CONF COMP V, P2487, DOI 10.1109/ICCVW.2017.293
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zakharchenko V, 2016, PROC SPIE, V9970, DOI 10.1117/12.2235885
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang Q, 2023, IEEE T MULTIMEDIA, V25, P5386, DOI 10.1109/TMM.2022.3190697
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhao WL, 2013, IEEE T IMAGE PROCESS, V22, P980, DOI 10.1109/TIP.2012.2226043
   Zheng XL, 2020, IEEE ACCESS, V8, P31647, DOI 10.1109/ACCESS.2020.2972158
   Zhou Y, 2023, IEEE T MULTIMEDIA, V25, P4177, DOI 10.1109/TMM.2022.3171684
   Zhou Y, 2022, IEEE T CIRC SYST VID, V32, P1767, DOI 10.1109/TCSVT.2021.3081162
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
   Zhou Y, 2018, IEEE T MULTIMEDIA, V20, P3019, DOI 10.1109/TMM.2018.2829607
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
NR 55
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3354
EP 3365
DI 10.1109/TMM.2023.3310276
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200011
DA 2024-08-05
ER

PT J
AU Zhu, GS
   Qin, Z
   Ding, Y
   Liu, Y
   Qin, ZG
AF Zhu, Guosong
   Qin, Zhen
   Ding, Yi
   Liu, Yao
   Qin, Zhiguang
TI MFNet:Real-Time Motion Focus Network for Video Frame Interpolation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interpolation; Optical imaging; Task analysis; Optical distortion;
   Memory management; Streaming media; Dynamics; Memory consumption
   reduction; motion focus; time efficiency improvement; video frame
   interpolation
AB As a popular research topic in computer vision, video frame interpolation is widely used in video processing tasks. However, this task is often limited by slow processing speed or high memory consumption in practical applications. To address these drawbacks, a frame interpolation network focusing on motion regions named MFNet is proposed, which consists of a sampler for adaptive and efficient separation of motion regions from the background, a fine-grained module for direct approximation of intermediate streams, and a lightweight module for bi-directional optical stream fusion. Extensive experiments show that our MFNet achieves optimal accuracy on some frame interpolation tasks and is much faster than other state-of-the-art methods. In addition, transplantation of the core components of MFNet to other frame interpolation networks can significantly improve the performance.
C1 [Zhu, Guosong; Qin, Zhen; Ding, Yi; Liu, Yao; Qin, Zhiguang] Univ Elect Sci & Technol China, Network & Data Secur Key Lab Sichuan Prov, Chengdu 610056, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Qin, Z (corresponding author), Univ Elect Sci & Technol China, Network & Data Secur Key Lab Sichuan Prov, Chengdu 610056, Peoples R China.
EM 202111090804@std.uestc.edu.cn; qinzhen@uestc.edu.cn;
   yi.ding@uestc.edu.cn; ly@uestc.edu.cn; qinzg@uestc.edu.cn
RI Zhu, Guosong/KIE-1691-2024
OI Ding, Yi/0000-0003-3406-9770
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980257
   [Anonymous], 2015, P C NEUR INF PROC SY
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Brooks T, 2019, PROC CVPR IEEE, P6833, DOI 10.1109/CVPR.2019.00700
   Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714
   Cheng XH, 2020, AAAI CONF ARTIF INTE, V34, P10607
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Ding TY, 2021, PROC CVPR IEEE, P7997, DOI 10.1109/CVPR46437.2021.00791
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Huang ZW, 2022, LECT NOTES COMPUT SC, V13674, P624, DOI 10.1007/978-3-031-19781-9_36
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Junheum Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P109, DOI 10.1007/978-3-030-58568-6_7
   Kalluri T, 2023, IEEE WINT CONF APPL, P2070, DOI 10.1109/WACV56688.2023.00211
   Karargyris A, 2011, IEEE T MED IMAGING, V30, P957, DOI 10.1109/TMI.2010.2098882
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Li HP, 2020, INT CONF ACOUST SPEE, P2613, DOI [10.1109/icassp40776.2020.9053987, 10.1109/ICASSP40776.2020.9053987]
   Li Y, 2022, INT J COMPUT VISION, V130, P2980, DOI 10.1007/s11263-022-01683-9
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCV.2017.478, 10.1109/ICCVW.2017.361]
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Park J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14519, DOI 10.1109/ICCV48922.2021.01427
   Peng X., 2022, P 31 INT JOINT C ART, P1349
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Reda F, 2022, LECT NOTES COMPUT SC, V13667, P250, DOI 10.1007/978-3-031-20071-7_15
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Shi ZH, 2022, IEEE T MULTIMEDIA, V24, P426, DOI 10.1109/TMM.2021.3052419
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Xu Xiangyu, 2019, P ADV NEUR INF PROC
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 35
TC 2
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3251
EP 3262
DI 10.1109/TMM.2023.3308442
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200007
DA 2024-08-05
ER

PT J
AU Assefa, M
   Jiang, W
   Zhan, JY
   Gedamu, K
   Yilma, G
   Ayalew, M
   Adhikari, D
AF Assefa, Maregu
   Jiang, Wei
   Zhan, Jinyu
   Gedamu, Kumie
   Yilma, Getinet
   Ayalew, Melese
   Adhikari, Deepak
TI Audio-Visual Contrastive and Consistency Learning for Semi-Supervised
   Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; audio-visual learning; contrastive learning;
   semi-supervised learning
AB Semi-supervised video learning is an increasingly popular approach for improving video understanding tasks by utilizing large-scale unlabeled videos along with a few labels. Recent studies have shown that multimodal contrastive learning and consistency regularization are effective techniques for generating high-quality pseudo-labels for semi-supervised action recognition. However, existing pseudo-labeling approaches are solely based on the model's class predictions and can suffer from confirmation biases due to the accumulation of false predictions. To address this issue, we propose exploiting audio-visual feature correlations to achieve high-quality pseudo-labels instead of relying on model confidence. To achieve this goal, we introduce Audio-visual Contrastive and Consistency Learning (AvCLR) for semi-supervised action recognition. AvCLR generates reliable pseudo-labels from audio-visual feature correlations using deep embedded clustering to mitigate confirmation biases. Additionally, AvCLR introduces two contrastive modules: intra-modal contrastive learning (ImCL) and cross-modal contrastive learning (XmCL) to discover complementary information from audio-visual alignments. The ImCL module learns informative representations within audio and video independently, while the XmCL module aims to leverage global high-level features of audio-visual information. Furthermore, the XmCL is constrained by introducing intra-instance negatives from one modality to the other. We jointly optimize the model with ImCL, XmCL, and consistency regularization in an end-to-end semi-supervised manner. Experimental results have demonstrated that the proposed AvCLR framework is effective in reducing confirmation biases and outperforms existing confidence-based semi-supervised action recognition methods.
C1 [Assefa, Maregu; Jiang, Wei; Zhan, Jinyu; Ayalew, Melese; Adhikari, Deepak] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Peoples R China.
   [Gedamu, Kumie] Sichuan Artificial Intelligence Res Inst, Chengdu, Peoples R China.
   [Gedamu, Kumie] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Peoples R China.
   [Yilma, Getinet] Adama Sci & Technol Univ, Dept Comp Sci & Engn, Adama 1888, Ethiopia.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Adama Science & Technology
   University
RP Zhan, JY (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Peoples R China.
EM maregu2006@gmail.com; weijiang@uestc.edu.cn; zhanjy@uestc.edu.cn;
   alemugedamu@gmail.com; getinetyilma@gmail.com; meleawima@gmail.com;
   deepakadhikari@std.uestc.edu.cn
RI A, Getinet Yilma/GPP-2884-2022
OI A, Getinet Yilma/0000-0001-5577-3201; Assefa,
   Maregu/0000-0003-2815-7993; Jiang, Wei/0000-0001-6181-3900; Adhikari,
   Deepak/0000-0002-3768-0666
FU National Natural Science Foundation of China
FX No Statement Available
CR Alwassel H., 2020, NEURIPS, V33, P9758
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Arazo E, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207304
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Assefa M., 2023, IEEE T CIRCUITS APR, DOI [10.1109/TCSVT2023,3267178, DOI 10.1109/TCSVT2023,3267178]
   Assefa M, 2022, PROC INT C TOOLS ART, P660, DOI 10.1109/ICTAI56018.2022.00102
   Assefa M, 2022, J CIRCUIT SYST COMP, V31, DOI 10.1142/S0218126622501596
   Assefaet M., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM.20223193559, DOI 10.1109/TMM.20223193559]
   Ayalew Melese, 2021, 2021 18th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP), P601, DOI 10.1109/ICCWAMTIP53232.2021.9674096
   Berthelot D., P INT C LEARN REPR 2, P1
   Berthelot D, 2019, ADV NEUR IN, V32
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Carreira J, 2019, Arxiv, DOI arXiv:1907.06987
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gedamu K, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108043
   Han T., 2020, Advances in Neural Information Processing Systems, V33, P5679
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hsiao JH, 2021, IEEE INT CONF COMP V, P3151, DOI 10.1109/ICCVW54120.2021.00354
   Jing T., 2021, P IEEE CVF WINT C AP, P1110
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Korbar B, 2018, ADV NEUR IN, V31
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lee Dong-Hyun, 2013, P ICML WORKSH CHALL
   Li JN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9455, DOI 10.1109/ICCV48922.2021.00934
   Liu YZ, 2022, IEEE T IMAGE PROCESS, V31, P4104, DOI 10.1109/TIP.2022.3180585
   Ma Shuang, 2021, INT C LEARN REPR
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Morgado P, 2021, PROC CVPR IEEE, P12470, DOI 10.1109/CVPR46437.2021.01229
   Pan T, 2021, PROC CVPR IEEE, P11200, DOI 10.1109/CVPR46437.2021.01105
   Park DS, 2019, INTERSPEECH, P2613, DOI 10.21437/Interspeech.2019-2680
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Romaissa BD, 2021, INT C PATT RECOG, P5859, DOI 10.1109/ICPR48806.2021.9412863
   Sime DM, 2023, IEEE T IND INFORM, V19, P9535, DOI 10.1109/TII.2022.3230785
   Singh A, 2021, PROC CVPR IEEE, P10384, DOI 10.1109/CVPR46437.2021.01025
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Soomro K., 2012, ARXIV
   Tong AY, 2023, IEEE T CIRC SYST VID, V33, P1305, DOI 10.1109/TCSVT.2022.3210271
   Tu ZG, 2023, IEEE T MULTIMEDIA, V25, P1819, DOI 10.1109/TMM.2022.3168137
   Wang JP, 2021, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR46437.2021.01163
   Wu JL, 2023, IEEE T IMAGE PROCESS, V32, P2215, DOI 10.1109/TIP.2023.3265261
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao JF, 2022, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR52688.2022.00325
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xie Q., 2020, Advances in Neural Information Processing Systems, V33, P6256
   Xiong B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7189, DOI 10.1109/ICCV48922.2021.00712
   Xu YH, 2022, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR52688.2022.00297
   Yan S, 2022, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR52688.2022.00333
   Yang S., 2021, ADV NEUR IN, V34
   Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658
   Zeng Zhaoyang, 2021, ADV NEURAL INFORM PR, V34, P7025
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zheng MK, 2022, PROC CVPR IEEE, P14451, DOI 10.1109/CVPR52688.2022.01407
   Zhong X., 2023, P IEEE INT C AC SPEE
NR 60
TC 2
Z9 2
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3491
EP 3504
DI 10.1109/TMM.2023.3312856
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200008
DA 2024-08-05
ER

PT J
AU Chun, D
   Lee, SG
   Kim, H
AF Chun, Dayoung
   Lee, Seungil
   Kim, Hyun
TI USD: Uncertainty-Based One-Phase Learning to Enhance Pseudo-Label
   Reliability for Semi-Supervised Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Annotations; Uncertainty; Object detection; Reliability; Task
   analysis; Predictive models; Deep learning; object detection;
   pseudo-labeling; semi-supervised learning; uncertainty
AB With the ease of accessing large unlabeled datasets, studies on semi-supervised learning for object detection (SSOD) have become increasingly popular. Among these SSOD studies, the pseudo-labeling method significantly depends on the accuracy of the pseudo-labels; thus, inaccurate annotations must be filtered to prevent performance degradation. This study classifies annotation errors that occur in pseudo-labeling methods as false negative (FN) and false positive (FP), and solutions to address each type of error are proposed using uncertainty information obtained through Gaussian modeling. Network performance is improved by preventing the background learning of the FN objects based on the uncertainty of the network output. In addition, based on the uncertainty of the annotations, low-reliability annotations are filtered out, and the learning reflectivity of FP objects is determined. Considering the network performance improvement and training complexity, the proposed method employs one-phase learning, including a single pseudo-label update, to achieve maximum performance with the minimum learning process. Moreover, an algorithm is proposed for an optimal update point search to increase the expected performance improvement. Experiments on the Pascal VOC, COCO, and Cityscapes datasets show that the SSD network improves accuracy by 3.3%, 4.7%, and 4.1%, respectively, with negligible computational complexity compared to the baseline.
C1 [Chun, Dayoung] Seoul Natl Univ, Interuniv Semicond Res Ctr, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Lee, Seungil; Kim, Hyun] Seoul Natl Univ Sci & Technol, Res Ctr Elect & Informat Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.
C3 Seoul National University (SNU); Seoul National University of Science &
   Technology
RP Kim, H (corresponding author), Seoul Natl Univ Sci & Technol, Res Ctr Elect & Informat Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.
EM jjeonda@capp.snu.ac.kr; seungil66@seoultech.ac.kr;
   hyunkim@seoultech.ac.kr
OI Chun, Dayoung/0009-0004-5741-6004
FU Institute of Information and Communications Technology Planning and
   Evaluation
FX No Statement Available
CR Alonso O, 2015, ACM J DATA INF QUAL, V6, DOI 10.1145/2724721
   Arazo E, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207304
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chen JS, 2019, P IEEE, V107, P1655, DOI 10.1109/JPROC.2019.2921977
   Choi J, 2019, IEEE I CONF COMP VIS, P502, DOI 10.1109/ICCV.2019.00059
   Choi J, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P16, DOI [10.1109/AICAS48895.2020.9073907, 10.1109/aicas48895.2020.9073907]
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gaff BM, 2014, COMPUTER, V47, P7
   Gal Y, 2016, PR MACH LEARN RES, V48
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Guo CA, 2017, PR MACH LEARN RES, V70
   Guo CX, 2015, ACM SIGCOMM COMP COM, V45, P139, DOI 10.1145/2785956.2787496
   He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300
   Hong HJ, 2016, IEEE INTERNET THINGS, V3, P299, DOI 10.1109/JIOT.2016.2519502
   Hu Shu, 2022, IEEE CVF C COMP VIS, P4390
   Jeong J, 2019, ADV NEUR IN, V32
   Jeong J, 2021, PROC CVPR IEEE, P11597, DOI 10.1109/CVPR46437.2021.01143
   Jing Y, 2018, IEEE INTERNET THINGS, V5, P3452, DOI 10.1109/JIOT.2017.2762003
   Kendall A., 2017, Adv Neural Inf Process Syst, V30
   Kim JU, 2021, IEEE T CIRC SYST VID, V31, P3529, DOI 10.1109/TCSVT.2020.3042219
   Laine S, 2016, INT C LEARN REPR
   Lau Y., 2021, J. Appl. Technol.Innov., V5
   Lee D.H., 2013, WORKSH CHALL REPR LE, P07
   Lee SI, 2022, INT C PATT RECOG, P3851, DOI 10.1109/ICPR56361.2022.9956515
   Li HD, 2022, AAAI CONF ARTIF INTE, P1314
   Liang-Chieh Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P695, DOI 10.1007/978-3-030-58545-7_40
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y., 2020, P INT C LEARN REPR, P1
   Liu YC, 2022, PROC CVPR IEEE, P9809, DOI 10.1109/CVPR52688.2022.00959
   Luo C, 2018, INT CONF SYST INFORM, P361, DOI 10.1109/ICSAI.2018.8599448
   Ma JX, 2022, IEEE COMPUT SOC CONF, P4849, DOI 10.1109/CVPRW56347.2022.00532
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Redmon J., 2018, CoRR
   Ren S., 2016, Faster r-cnn: Towards real-time object detection with region proposal networks
   Russakovsky O, 2015, PROC CVPR IEEE, P2121, DOI 10.1109/CVPR.2015.7298824
   Sajjadi M, 2016, ADV NEUR IN, V29
   Sohn K, 2020, Arxiv, DOI [arXiv:2005.04757, 10.48550/arXiv.2005.04757]
   Solovyev R, 2021, IMAGE VISION COMPUT, V107, DOI 10.1016/j.imavis.2021.104117
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Uddin M, 2012, RENEW SUST ENERG REV, V16, P4078, DOI 10.1016/j.rser.2012.03.014
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang ZY, 2021, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR46437.2021.00454
   Wu Z., 2020, P 30 BRIT MACH VIS C
   Xu MD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3040, DOI 10.1109/ICCV48922.2021.00305
   Yang QZ, 2021, PROC CVPR IEEE, P5937, DOI 10.1109/CVPR46437.2021.00588
   You KC, 2019, Arxiv, DOI arXiv:1908.01878
   Yu LQ, 2019, LECT NOTES COMPUT SC, V11765, P605, DOI 10.1007/978-3-030-32245-8_67
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang FY, 2022, AAAI CONF ARTIF INTE, P3252
   Zhang YH, 2023, IEEE T MULTIMEDIA, V25, P1749, DOI 10.1109/TMM.2022.3158069
   Zhou HY, 2022, LECT NOTES COMPUT SC, V13669, P35, DOI 10.1007/978-3-031-20077-9_3
   Zhou Q, 2021, PROC CVPR IEEE, P4079, DOI 10.1109/CVPR46437.2021.00407
NR 56
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6336
EP 6347
DI 10.1109/TMM.2023.3348662
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600010
DA 2024-08-05
ER

PT J
AU He, LZ
   Li, F
   Cong, RM
   Zhao, Y
AF He, Lingzhi
   Li, Feng
   Cong, Runmin
   Zhao, Yao
TI Reflection Intensity Guided Single Image Reflection Removal and
   Transmission Recovery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reflection; Task analysis; Cameras; Glass; Estimation; Optimization;
   Interference; Single image reflection removal; reflection intensity
   guided; joint learning; transmission recovery
ID SEPARATION
AB Single image reflection removal (SIRR) aims at eliminating unwanted interference caused by the reflection of transparent or smooth surfaces and obtaining an estimation of a clear transmission layer. Existing data-driven methods typically rely on decomposing the observed image into transmission and reflection layers, which neglects the physical generation principles of an image with reflections, thus leading to unsatisfactory results, especially in strong reflection regions. To address this issue, in this work, we analyze the imaging process of reflection image from the physical perspective and derive a conclusion that the physical quantity: illuminance of the reflection layer determines the reflection intensity. Then a two-stage reflection intensity-guided network (RINet) is proposed for reflection removal and transmission recovery. The key lies in the first stage are the parallel modules that generate the reflection intensity map and transmission layer. In the second stage, besides utilizing such intensity map as the guidance, we additionally calculate the gradient field as the other prior to facilitate the final reflection removal. Specifically, we design a dual-flow joint learning module (JLM) comprised of a transmission recovery branch and a gradient optimization branch that jointly optimizes image structures and details by exploiting the interactions between transmission and gradient features. In particular, guided by the reflection intensity map, the transmission recovery branch can dynamically focus on removing reflections. Equipped with the two-stage framework, our RINet constitutes a divide-and-conquer process to achieve effective transmission recovery and reflection removal. Experimental results on public datasets demonstrate the superiority of the proposed method over recent state-of-the-art methods.
C1 [He, Lingzhi; Zhao, Yao] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Li, Feng] Hefei Univ Technol, Sch Comp Sci & Engn, Hefei 230009, Peoples R China.
   [Cong, Runmin] Shandong Univ, Sch Control Sci & Engn, Shandong 250100, Peoples R China.
C3 Beijing Jiaotong University; Hefei University of Technology; Shandong
   University
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
EM lingzhihe@bjtu.edu.cn; fengli@hfut.edu.cn; rmcong@sdu.edu.cn;
   yzhao@bjtu.edu.cn
OI He, Lingzhi/0009-0001-9000-7278; Li, Feng/0000-0001-9862-0432; Zhao,
   Yao/0000-0002-8581-9554
FU National Key Ramp;D Program of China
FX No Statement Available
CR Be'ery E, 2008, IEEE T IMAGE PROCESS, V17, P340, DOI 10.1109/TIP.2007.915548
   Chang YK, 2021, IEEE T CYBERNETICS, V51, P5836, DOI 10.1109/TCYB.2019.2959381
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Cong RM, 2023, IEEE T IMAGE PROCESS, V32, P4472, DOI 10.1109/TIP.2023.3286263
   Dong Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4997, DOI 10.1109/ICCV48922.2021.00497
   Duan HY, 2023, IEEE T MULTIMEDIA, V25, P4267, DOI 10.1109/TMM.2022.3172882
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Feynman RP., 1965, The Feynman Lectures on Physics, V33, P750, DOI 10.1119/1.1972241
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   He LZ, 2021, PROC CVPR IEEE, P9225, DOI 10.1109/CVPR46437.2021.00911
   Heydecker D, 2019, IEEE T IMAGE PROCESS, V28, P6185, DOI 10.1109/TIP.2019.2923559
   Hong Y., 2022, IEEE Trans. Multimedia, V22, P2012
   Hong Y., 2021, P IEEE CVF C COMP VI, P7762
   Horn B.K.P., 1974, Computer Graphics and Image Processing, V3, P277, DOI [10.1016/0146-664X(74)90022-7, DOI 10.1016/0146-664X(74)90022-7]
   Hu QM, 2021, ADV NEUR IN
   Jiang QP, 2022, IEEE T INTELL TRANSP, V23, P19440, DOI 10.1109/TITS.2022.3165176
   Khezri HR, 2023, IEEE T MULTIMEDIA, V25, P4958, DOI 10.1109/TMM.2022.3185929
   Kim S, 2020, PROC CVPR IEEE, P5163, DOI 10.1109/CVPR42600.2020.00521
   Kingma D. P., 2014, arXiv
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lei CY, 2021, PROC CVPR IEEE, P14806, DOI 10.1109/CVPR46437.2021.01457
   Lei CY, 2020, PROC CVPR IEEE, P1747, DOI 10.1109/CVPR42600.2020.00182
   Levin A., 2002, Proceedings of the neural information processing systems conference (NIPS), P1247
   Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106
   Li C, 2020, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR42600.2020.00362
   Li CY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1478, DOI 10.1145/3394171.3413928
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Li F, 2023, IEEE T MULTIMEDIA, V25, P2825, DOI 10.1109/TMM.2022.3152090
   Li TL, 2019, IEEE SIGNAL PROC LET, V26, P1237, DOI 10.1109/LSP.2019.2926828
   Li TT, 2021, IEEE T IMAGE PROCESS, V30, P68, DOI 10.1109/TIP.2020.3031184
   Li TT, 2019, IEEE T IMAGE PROCESS, V28, P1798, DOI 10.1109/TIP.2018.2880510
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Li Y, 2013, IEEE I CONF COMP VIS, P2432, DOI 10.1109/ICCV.2013.302
   Peng YT, 2022, IEEE SIGNAL PROC LET, V29, P568, DOI 10.1109/LSP.2022.3148668
   Prasad BHP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2370, DOI 10.1109/ICCV48922.2021.00239
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Ronneberger P., 2015, MEDICAL IMAGE COMPUT, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]
   Shi XJ, 2015, ADV NEUR IN, V28
   Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song BB, 2022, IEEE T CIRC SYST VID, V32, P6515, DOI 10.1109/TCSVT.2022.3168828
   Sun J, 2019, IEEE SIGNAL PROC LET, V26, P1011, DOI 10.1109/LSP.2019.2915560
   Tang Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2148, DOI 10.1145/3474085.3475373
   Wan RJ, 2023, IEEE T PATTERN ANAL, V45, P1424, DOI 10.1109/TPAMI.2022.3168560
   Wan RJ, 2020, IEEE T PATTERN ANAL, V42, P2969, DOI 10.1109/TPAMI.2019.2921574
   Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502
   Wan RJ, 2018, IEEE T IMAGE PROCESS, V27, P2927, DOI 10.1109/TIP.2018.2808768
   Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wen Q, 2019, PROC CVPR IEEE, P3766, DOI 10.1109/CVPR.2019.00389
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM.2022.3216115, DOI 10.1109/TMM.2022.3216115]
   Wu ZQ, 2022, IEEE T MULTIMEDIA, V24, P3782, DOI 10.1109/TMM.2021.3107688
   Xue TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766940
   Yang J, 2018, LECT NOTES COMPUT SC, V11207, P675, DOI 10.1007/978-3-030-01219-9_40
   Zhang DY, 2024, IEEE T CIRC SYST VID, V34, P2934, DOI 10.1109/TCSVT.2023.3307438
   Zhang HD, 2020, IEEE T MULTIMEDIA, V22, P2012, DOI 10.1109/TMM.2019.2951461
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zheng Q, 2021, PROC CVPR IEEE, P13390, DOI 10.1109/CVPR46437.2021.01319
   Zheng Q, 2020, PROC CVPR IEEE, P3019, DOI 10.1109/CVPR42600.2020.00309
   Zhu C., 2022, P ADV NEUR INF PROC, V35, P38994
   Zhu YR, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3270938
NR 67
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5026
EP 5039
DI 10.1109/TMM.2023.3330107
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600009
DA 2024-08-05
ER

PT J
AU He, P
   Jiao, LC
   Liu, F
   Liu, X
   Shang, RH
   Wang, S
AF He, Pei
   Jiao, Licheng
   Liu, Fang
   Liu, Xu
   Shang, Ronghua
   Wang, Shuang
TI Cross-Domain Scene Unsupervised Learning Segmentation With Dynamic
   Subdomains
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation models; Uncertainty; Training; Estimation; Semantic
   segmentation; Data models; Bayes methods; Scene Segmentation;
   Inter-domain and Intra-domain Adaptation; Dynamic Subdomains;
   Statistical Priors; Structural Priors
ID ALIGNMENT
AB Unsupervised cross-domain scene segmentation approach adapts the source model to the target domain, which utilizes two-stage strategies to minimize the inter-domain and intra-domain gap. However, the accumulation of errors in the previous stages affects the training of the subsequent stages. In this paper, a framework called statistical and structural domain adaptation (SSDA) is proposed to optimize inter-domain and intra-domain adaptation jointly. Firstly, the statistical inter-domain adaptation (StaIA) is proposed to model dynamic subdomains, which continuously adjust seed samples during the process of domain adaptation to mitigate error accumulation. The dynamic subdomains are modeled by exploring Bayesian uncertainty statistics and global balance statistics, which alleviate the imbalance problem in uncertainty estimation. StaIA encourages the model to transfer comprehensive and genuine knowledge through the seed loss for inter-domain adaptation. Secondly, the structural intra-domain adaptation (StrIA) is proposed to align the intra-domain gap among dynamic subdomains by the structural priors. Specifically, the StrIA models structural priors by truncated conditional random field (TruCRF) loss within the neighborhood, which constrains intra-domain semantic consistency to reduce the intra-domain gap. Experimental results demonstrate the effectiveness of the proposed cross-domain scene segmentation approaches on two commonly-used unsupervised domain adaptation benchmarks.
C1 [He, Pei; Jiao, Licheng; Liu, Fang; Liu, Xu; Shang, Ronghua; Wang, Shuang] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Minist Educ China, Int Res Ctr Intelligent Percept & Comp,Sch Artific, Xian 710071, Peoples R China.
C3 Xidian University
RP Jiao, LC (corresponding author), Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Minist Educ China, Int Res Ctr Intelligent Percept & Comp,Sch Artific, Xian 710071, Peoples R China.
EM hepei@stu.xidian.edu.cn; lchjiao@mail.xidian.edu.cn; f63liu@163.com;
   xuliu361@163.com; rhshang@mail.xidian.edu.cn; shwang@mail.xidian.edu.cn
OI He, Pei/0000-0002-5645-8489
FU Key Scientific Technological Innovation Research Project by Ministry of
   Education
FX No Statement Available
CR Araslanov N, 2021, PROC CVPR IEEE, P15379, DOI 10.1109/CVPR46437.2021.01513
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Chen T., 2022, IEEE Trans. Multimedia, V24, P202
   Chen T. Q., 2016, PROC ADV NEURAL INF, P1
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Das A, 2023, PROC CVPR IEEE, P15434, DOI 10.1109/CVPR52729.2023.01481
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Gal Y, 2017, PR MACH LEARN RES, V70
   Gong R, 2023, PROC CVPR IEEE, P7225, DOI 10.1109/CVPR52729.2023.00698
   Gong R, 2021, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR46437.2021.00824
   Guan DY, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107764
   Guo XQ, 2021, PROC CVPR IEEE, P3926, DOI 10.1109/CVPR46437.2021.00392
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3179379
   Heetal P, 2023, IEEE Trans. Neural Netw. Learn. Syst., P1
   Hong WX, 2018, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2018.00145
   Houlsby Neil, 2011, stat, V1050, P24
   Hoyer L, 2022, LECT NOTES COMPUT SC, V13690, P372, DOI 10.1007/978-3-031-20056-4_22
   Hoyer L, 2022, PROC CVPR IEEE, P9914, DOI 10.1109/CVPR52688.2022.00969
   Huang HS, 2018, LECT NOTES COMPUT SC, V11220, P611, DOI 10.1007/978-3-030-01270-0_36
   Jinyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P480, DOI 10.1007/978-3-030-58583-9_29
   Kang Guoliang, 2020, NEURIPS
   Kendall A., 2017, Adv Neural Inf Process Syst, V30
   Kim M, 2021, AAAI CONF ARTIF INTE, V35, P1799
   Krahenbuhl P., 2011, NeurIPS, V24
   Li RH, 2022, PROC CVPR IEEE, P11583, DOI 10.1109/CVPR52688.2022.01130
   Li T., 2023, P IEEECVF C COMPUTER, P4868
   Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710
   Lian Q, 2019, IEEE I CONF COMP VIS, P6757, DOI 10.1109/ICCV.2019.00686
   Liu Y, 2021, PROC CVPR IEEE, P1215, DOI 10.1109/CVPR46437.2021.00127
   Lu XQ, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3272552
   Lu XQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3220755
   Luo YW, 2019, IEEE I CONF COMP VIS, P6777, DOI 10.1109/ICCV.2019.00688
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Ma HY, 2021, PROC CVPR IEEE, P4050, DOI 10.1109/CVPR46437.2021.00404
   Melas-Kyriazi L, 2021, PROC CVPR IEEE, P12430, DOI 10.1109/CVPR46437.2021.01225
   Myeongjin Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12972, DOI 10.1109/CVPR42600.2020.01299
   Obukhov S., 2019, INT CONFNEURAL INF P, P1
   Pan F, 2020, PROC CVPR IEEE, P3763, DOI 10.1109/CVPR42600.2020.00382
   Paszke A., 2017, P 31 C NEUR INF PROC, P1, DOI DOI 10.1017/CB09781107707221.009
   Qiao FC, 2021, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR46437.2021.00672
   Ren QH, 2024, IEEE T MULTIMEDIA, V26, P501, DOI 10.1109/TMM.2023.3266892
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Roy S, 2021, PROC CVPR IEEE, P5347, DOI 10.1109/CVPR46437.2021.00531
   Shen FY, 2023, PROC CVPR IEEE, P15866, DOI 10.1109/CVPR52729.2023.01523
   Sun L, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3278133
   Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31
   Teja SP, 2021, PROC CVPR IEEE, P9608, DOI 10.1109/CVPR46437.2021.00949
   Truong T.-D., 2021, P IEEE CVF INT C COM, P8548
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Vemulapalli R, 2016, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2016.351
   Vu TH, 2019, IEEE I CONF COMP VIS, P7363, DOI 10.1109/ICCV.2019.00746
   Wang Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8495, DOI 10.1109/ICCV48922.2021.00840
   Xie B., 2022, PROC IEEECVF C COMPU, P8068
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yan ZZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P404, DOI 10.1145/3474085.3475174
   Yang JY, 2023, IEEE T MED IMAGING, V42, P3229, DOI 10.1109/TMI.2023.3278461
   Yiheng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9618, DOI 10.1109/CVPR42600.2020.00964
   You C, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3300706
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P10754
   Zhang J., 2019, INT CONFNEURAL INF P, P1
   Zhang P, 2021, PROC CVPR IEEE, P12409, DOI 10.1109/CVPR46437.2021.01223
   Zhang Y, 2020, IEEE T PATTERN ANAL, V42, P1823, DOI 10.1109/TPAMI.2019.2903401
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng ZD, 2021, INT J COMPUT VISION, V129, P1106, DOI 10.1007/s11263-020-01395-y
   Zhonghao Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12632, DOI 10.1109/CVPR42600.2020.01265
   Zhou DW, 2021, IEEE T MULTIMEDIA, V24, P3469, DOI 10.1109/TMM.2021.3099297
   Zhuang W., 2022, PROC IEEE INT C MULT, P1
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
   Zuo YK, 2023, IEEE T MULTIMEDIA, V25, P9057, DOI 10.1109/TMM.2023.3245420
NR 77
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6770
EP 6784
DI 10.1109/TMM.2024.3355629
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600046
DA 2024-08-05
ER

PT J
AU Huang, JJ
   Du, CB
   Zhu, XQ
   Ma, SQ
   Nepal, S
   Xu, C
AF Huang, Jiajun
   Du, Chengbin
   Zhu, Xinqi
   Ma, Siqi
   Nepal, Surya
   Xu, Chang
TI Anti-Compression Contrastive Facial Forgery Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial forgery detection; contrastive Learning; anti-compression
AB Forgery of facial images and videos has increased the concern about digital security. It has led to the significant development of detecting forgery data recently. However, the data, especially the videos published on the Internet, are usually compressed with lossy compression algorithms such as H.264. The compressed data could significantly degrade the performance of recent detection algorithms. The existing anti-compression algorithms focus on enhancing the performance in detecting heavily compressed data but less consider the compression adaption to the data from various compression levels. We believe creating a forgery detection capable of handling data compressed with unknown levels is important. To enhance the performance of such models, we consider the weak compressed and strong compressed data as two views of the original data and they should have similar representation and relationships with other samples. We propose a novel anti-compression forgery detection framework by maintaining closer relations within data under different compression levels. Specifically, our algorithm measures the pair-wise similarity within data as the relations, ensuring that relationships between weakly and strongly compressed data remain consistent. This enhances the discriminative power for detecting highly compressed data. To achieve a better strong compressed data relation guided by the less compressed one, we apply video-level contrastive learning for weak compressed data, which forces the model to produce similar representations within the same video and far from the negative samples. The experiment results show that the proposed algorithm could boost performance for strong compressed data while improving the accuracy rate when detecting clean data.
C1 [Huang, Jiajun; Du, Chengbin; Zhu, Xinqi; Xu, Chang] Univ Sydney, Sch Comp Sci, Camperdown, NSW 2006, Australia.
   [Ma, Siqi] UNSW Canberra, Sch Engn & Informat Technol, Canberra, ACT 2609, Australia.
   [Nepal, Surya] CSIRO Data61, Eveleigh, NSW 2015, Australia.
C3 University of Sydney; University of New South Wales Sydney; Commonwealth
   Scientific & Industrial Research Organisation (CSIRO)
RP Xu, C (corresponding author), Univ Sydney, Sch Comp Sci, Camperdown, NSW 2006, Australia.
EM jhua7177@uni.sydney.edu.au; chdu5632@uni.sydney.edu.au;
   xinqi.zhu@uni.sydney.edu.au; siqi.ma@adfa.edu.au;
   surya.nepal@data61.csiro.au; c.xu@uni.sydney.edu.au
RI Nepal, Surya/B-7523-2011
OI Nepal, Surya/0000-0002-3289-6599
FU Australian Research Council
FX No Statement Available
CR Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8
   Burkov E., 2020, P IEEE CVF C COMP VI, P13786
   Buslaev A, 2020, INFORMATION, V11, DOI 10.3390/info11020125
   Cao SH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1929, DOI 10.1145/3474085.3475347
   Chen HT, 2020, AAAI CONF ARTIF INTE, V34, P3585
   Chen L, 2022, PROC CVPR IEEE, P18689, DOI 10.1109/CVPR52688.2022.01815
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Deng Zhengjie, 2022, 2022 9th International Conference on Digital Home (ICDH), P251, DOI 10.1109/ICDH57206.2022.00046
   github, Github-iperov/deepfacelab: Deepfacelab is the leading software for creatingdeepfakes
   github, Github-deepfakes/faceswap: Deepfakes software for all
   github, Github-dfaker/df: Larger resolution face masked, weirdly warped, deepfake
   github, Github-shaoanlu/faceswap-gan: A denoising autoencoder adversarial losses and attention mechanisms for face swapping
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grill J-B, 2020, PROC 34 INT C NEURAL
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   Guo X, 2023, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR52729.2023.00308
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YA, 2021, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR46437.2021.00434
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Jiajun Huang, 2021, 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), P1973, DOI 10.1109/ICCVW54120.2021.00224
   John Jerry, 2022, 2022 Sixth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), P516, DOI 10.1109/I-SMAC55078.2022.9987265
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D., 2014, P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR P 3 INT C LEARN REPR
   Kumar A., IEEE 8 INT WORKSH BI, P1
   Li J, 2017, IEEE T IMAGE PROCESS, V26, P3113, DOI 10.1109/TIP.2017.2651379
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   [李艳歌 Li Yange], 2018, [高分子通报, Polymer Bulletin], P46
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   malavida, Fakeapp 2.2.0-download for PC free
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Natsume T., 2018, P ASS COMP MACH SIGG, P1
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Rana MS, 2020, 2020 7TH IEEE INTERNATIONAL CONFERENCE ON CYBER SECURITY AND CLOUD COMPUTING (CSCLOUD 2020)/2020 6TH IEEE INTERNATIONAL CONFERENCE ON EDGE COMPUTING AND SCALABLE CLOUD (EDGECOM 2020), P70, DOI 10.1109/CSCloud-EdgeCom49738.2020.00021
   Ricker S., 2023, P 11 INT C LEARN REP, P1
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Shao ZW, 2021, IEEE T IMAGE PROCESS, V30, P4610, DOI 10.1109/TIP.2021.3073857
   Siarohin A, 2019, ADV NEUR IN, V32
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Wang T.-H., 2018, ARXIV
   Wang YH, 2020, IEEE INT CONF AUTOMA, P515, DOI 10.1109/FG47880.2020.00089
   Xu Y, 2019, P ADV NEUR INF PROC, V32, P1
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zagoruyko S., 2017, P INT C LEARN REPR, P1
   Zhang DC, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5833, DOI 10.1145/3503161.3547913
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhao TC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15003, DOI 10.1109/ICCV48922.2021.01475
   Zheng Mingkai, 2021, ADV NEURAL INF PROCE, P2543
   Zhou TF, 2021, PROC CVPR IEEE, P5774, DOI 10.1109/CVPR46437.2021.00572
NR 56
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6166
EP 6177
DI 10.1109/TMM.2023.3347103
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600009
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ji, SL
   Yang, XY
AF Ji, Shulei
   Yang, Xinyu
TI EmoMusicTV: Emotion-Conditioned Symbolic Music Generation With
   Hierarchical Transformer VAE
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Symbolic music generation; lead sheets; emotional condition; conditional
   variational autoencoder; transformer
AB Emotion is one of the most crucial attributes of music. However, due to the scarcity of emotional music datasets, emotion-conditioned symbolic music generation using deep learning techniques has not been investigated in depth. In particular, no study explores conditional music generation with the guidance of emotion, and few studies adopt time-varying emotional conditions. To address these issues, first, we endow three public lead sheet datasets with fine-grained emotions by automatically computing the valence labels from the chord progressions. Second, we propose a novel and effective encoder-decoder architecture named EmoMusicTV to explore the impact of emotional conditions on multiple music generation tasks and to capture the rich variability of musical sequences. EmoMusicTV is a transformer-based variational autoencoder (VAE) that contains a hierarchical latent variable structure to model holistic properties of the music segments and short-term variations within bars. The piece-level and bar-level emotional labels are embedded in their corresponding latent spaces to guide music generation. Third, we pretrain EmoMusicTV with the lead sheet continuation task to further improve its performance on conditional melody or harmony generation. Experimental results demonstrate that EmoMusicTV outperforms previous methods on three tasks, i.e., melody harmonization, melody generation given harmony, and lead sheet generation. Ablation studies verify the significant roles of emotional conditions and hierarchical latent variable structure on conditional music generation. Human listening shows that the lead sheets generated by EmoMusicTV are closer to the ground truth (GT) and perform slightly worse than the GT in conveying emotional polarity.
C1 [Ji, Shulei; Yang, Xinyu] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Yang, XY (corresponding author), Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China.
EM taylorji@stu.xjtu.edu.cn; yxyphd@mail.xjtu.edu.cn
OI Ji, Shulei/0000-0002-9908-136X
CR Bao Q., 2022, IEEE Trans. Mul-timedia, DOI [10.1109/TMM.2022.3163543.[18]Y.-C., DOI 10.1109/TMM.2022.3163543.[18]Y.-C]
   Bowman S.R., 2016, P 20 SIGNLL C COMPUT, P10, DOI [10.18653/v1/K16-1002, DOI 10.18653/V1/K16-1002]
   Briot J.P., 2020, Deep Learning Techniques for Music Generation, DOI DOI 10.1007/978-3-319-70163-9
   Chen Yi-Wei, 2021, PROC 22 INT SOC MUSI, P105
   Chih-Fang Huang, 2020, 2020 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE), P220, DOI 10.1109/ECICE50847.2020.9301934
   Dieng AB, 2019, PR MACH LEARN RES, V89
   Ferreira L. N., 2020, P AAAI C ART INT INT, P59
   Ferreira L. N., 2022, P AAAI C ARTIFICIAL, P163
   Ferreira L N, 2019, Proceedings of the 20th International Society for Music Information Retrieval Conference (Delft, The Netherlands), P384, DOI [DOI 10.5281/ZENODO.3527824, 10 . 5281 / zenodo.3527824]
   Fu H, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P240
   Grekow J, 2021, IEEE SYS MAN CYBERN, P1941, DOI 10.1109/SMC52423.2021.9658604
   Holtzman J., 2020, 8 INT C LEARN REPRES
   Hsiao WY, 2021, AAAI CONF ARTIF INTE, V35, P178
   Huang YSA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1180, DOI 10.1145/3394171.3413671
   Hung H., 2021, ISMIR, P318
   Ji S., 2020, A comprehensive survey on deep music generation: Multi-level representations algorithms evaluations and future directions
   Jiang JY, 2020, INT CONF ACOUST SPEE, P516, DOI [10.1109/icassp40776.2020.9054554, 10.1109/ICASSP40776.2020.9054554]
   KRAUSE B., 2017, PROC 5 INT C LEARN R, P2872
   Madaghiele Vincenzo, 2021, 22 INT SOC MUSIC INF, P412
   Makris K. R., 2021, IEEE INT JOINTCONF N, P1
   Neves P. L. T., 2022, P 23 INT SOC MUS INF, P717, DOI [10.5281/zenodo.7316763, DOI 10.5281/ZENODO.7316763]
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pangestu S., 2021, IEEE INT C COMPUT SC, P1
   Radford, 2018, OPENAI BLOG
   Radford A., 2019, OpenAI blog, V1, P9
   Rempe D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11468, DOI 10.1109/ICCV48922.2021.01129
   Renjie Huang, 2021, EITCE 2021: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering, P1598, DOI 10.1145/3501409.3501691
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Sonderby CK, 2016, ADV NEUR IN, V29
   Sun CE, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4145, DOI 10.1109/ICASSP39728.2021.9414281
   Tseng BW, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P86, DOI 10.1109/ICASSP39728.2021.9413365
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z., 2021, P 22 INT SOC MUS INF, P722, DOI DOI 10.5281/ZENODO.5624387
   Wu SL, 2021, Arxiv, DOI [arXiv:2105.04090, DOI 10.1109/TASLP.2023.3270726]
   Wu Y., 2020, PROC INT SOC MUSIC I, P142
   Wu YC, 2016, IEEE-ACM T AUDIO SPE, V24, P2277, DOI 10.1109/TASLP.2016.2603006
   Yang L.-C., 2017, ISMIR, P324
   Yang LC, 2020, NEURAL COMPUT APPL, V32, P4773, DOI 10.1007/s00521-018-3849-7
   Yang XP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4539
   Yeh YC, 2021, J NEW MUSIC RES, V50, P37, DOI 10.1080/09298215.2021.1873392
   Zeng ML, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P791
   Zhang KJ, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P135, DOI 10.1145/3206025.3206037
   Zhao K, 2019, PROCEEDINGS OF 2019 IEEE 3RD INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC 2019), P2039, DOI [10.1109/ITNEC.2019.8729266, 10.1109/itnec.2019.8729266]
NR 44
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1076
EP 1088
DI 10.1109/TMM.2023.3276177
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700034
DA 2024-08-05
ER

PT J
AU Liu, AA
   Zhai, YC
   Xu, N
   Tian, HS
   Nie, WZ
   Zhang, YD
AF Liu, An-An
   Zhai, Yingchen
   Xu, Ning
   Tian, Hongshuo
   Nie, Weizhi
   Zhang, Yongdong
TI Event-Aware Retrospective Learning for Knowledge-Based Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image captioning; external knowledge; visual facts; retrospective
   learning
ID TRANSFORMER
AB External knowledge has been widely applied in image captioning tasks to enrich the generated sentences. However, existing methods retrieve knowledge by considering only semantic relevance while ignoring whether they are useful for captioning. For example, when querying "person" in external knowledge, the most relevant concepts may be "wearing shirt" or "riding horse" statistically, which are not consistent with image contents and introduce noise to generated sentences. Intuitively, we humans can iteratively correlate visual clues with corresponding knowledge to distinguish useful clues from noise. Therefore, we propose an event-aware retrospective learning network for knowledge-based image captioning, which employs a retrospective validation mechanism on captioning models to align the retrieved knowledge with visual contents. This approach is an event-aware perspective and helps select useful knowledge that corresponds to visual facts. To better align images and knowledge, 1) we design an event-aware retrieval algorithm that clusters word-centered knowledge into triplet-centered knowledge (i.e., from " < subject - predicate - object >" to "< triplet A> - edge - < triplet B >"), which provides an event context to facilitate knowledge retrieval and validation. 2) We revisit image contents to retrospectively validate retrieved knowledge by aligning the visual representation between knowledge and image. We summarize the visual characteristics of each knowledge event from the visual genome dataset to help learn which knowledge does not exist in the visual scene and should be discarded. 3) We adopt a dynamic knowledge fusion module that calibrates image and knowledge representations for sentence generation, which includes a knowledge-controlled gate unit that jointly calculates visual and semantic features in event-aware patterns. Compared to current knowledge-based captioning methods, the proposed network retrospectively learns the visual facts by event-aware retrieval and knowledge-image visual alignment, which regularizes the knowledge-incorporated captioning with visual evidence. Extensive experiments on the MS-COCO dataset demonstrate the effectiveness of our method. Ablation studies and visualization demonstrate the advantages of each component of the proposed model.
C1 [Liu, An-An; Zhai, Yingchen; Xu, Ning; Tian, Hongshuo; Nie, Weizhi] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Hefei 230052, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Xu, N (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM anan0422@gmail.com; zhaiyingchen@tju.edu.cn; ningxu@tju.edu.cn;
   kellyeden@tju.edu.cn; weizhinie@tju.edu.cn; zhyd73@ustc.edu.cn
OI nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], 2016, P 24 ACM INT C MULT
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Bao JW, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P967
   Barraco M, 2022, IEEE COMPUT SOC CONF, P4661, DOI 10.1109/CVPRW56347.2022.00512
   Cao S, 2022, IEEE T CIRC SYST VID, V32, P7005, DOI 10.1109/TCSVT.2022.3178844
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen Q, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2406
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Vo DM, 2022, PROC CVPR IEEE, P17979, DOI 10.1109/CVPR52688.2022.01747
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207
   Han XJ, 2023, IEEE T MULTIMEDIA, V25, P5319, DOI 10.1109/TMM.2022.3190135
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Huang FC, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P535, DOI 10.1145/3340531.3411948
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jiang WH, 2018, AAAI CONF ARTIF INTE, P6959
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kuo CW, 2022, PROC CVPR IEEE, P17948, DOI 10.1109/CVPR52688.2022.01744
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Li GH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1227, DOI 10.1145/3394171.3413943
   Li X., 2020, Oscar: Object-Semantics Aligned Pre-training for VisionLanguage Tasks, DOI 10.1007/978-3-030-58577-8_8
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YA, 2022, PROC CVPR IEEE, P17969, DOI 10.1109/CVPR52688.2022.01746
   Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Lv SW, 2020, AAAI CONF ARTIF INTE, V34, P8449
   Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10
   Narasimhan M, 2018, LECT NOTES COMPUT SC, V11212, P460, DOI 10.1007/978-3-030-01237-3_28
   Nie LQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3234, DOI 10.1145/3503161.3548180
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qian TW, 2023, IEEE T MULTIMEDIA, V25, P3950, DOI 10.1109/TMM.2022.3169065
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Schuster Sebastian, 2015, P 4 WORKSH VIS LANG, P70, DOI DOI 10.18653/V1/W15-2812
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shen S., 2021, arXiv
   Shi Z, 2020, Arxiv, DOI arXiv:2006.11807
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Tan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10757, DOI 10.1109/CVPR42600.2020.01077
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Nguyen VQ, 2022, LECT NOTES COMPUT SC, V13696, P167, DOI 10.1007/978-3-031-20059-5_10
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vrandecic D, 2014, COMMUN ACM, V57, P78, DOI 10.1145/2629489
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wei YW, 2023, IEEE T KNOWL DATA EN, V35, P11153, DOI 10.1109/TKDE.2022.3231352
   Wu JL, 2022, AAAI CONF ARTIF INTE, P2712
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao XC, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P956
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P606, DOI 10.1007/978-3-030-58592-1_36
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhang YF, 2021, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR46437.2021.00141
   Zhang Y, 2021, PATTERN RECOGN LETT, V143, P43, DOI 10.1016/j.patrec.2020.12.020
   Zhang ZJ, 2022, IEEE T MULTIMEDIA, V24, P3101, DOI 10.1109/TMM.2021.3093725
   Zhao WT, 2024, IEEE T MULTIMEDIA, V26, P2659, DOI 10.1109/TMM.2023.3301279
   Zhong YW, 2022, PROC CVPR IEEE, P16772, DOI 10.1109/CVPR52688.2022.01629
   Zhou YM, 2019, IEEE WINT CONF APPL, P283, DOI 10.1109/WACV.2019.00036
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 81
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4898
EP 4911
DI 10.1109/TMM.2023.3327537
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800001
DA 2024-08-05
ER

PT J
AU Lu, YW
   Huang, HY
   Zeng, BQ
   Lai, ZH
   Li, XL
AF Lu, Yuwu
   Huang, Haoyu
   Zeng, Biqing
   Lai, Zhihui
   Li, Xuelong
TI Multi-Source and Multi-Target Domain Adaptation Based on Dynamic
   Generator with Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple targets; dynamic feature generator; multi-source domain
   adaptation; attention mechanism
ID CONVOLUTIONAL NETWORK; RECOGNITION; MODEL
AB As a branch of domain adaptation (DA), multi-source DA (MSDA) is a challenging issue that aims to transfer knowledge from multiple well-labeled source domains to a target domain for target tasks. However, most existing related works focus on single-target domain adaptation, and multiple target domain adaptation is not accounted for. We believe that multiple target domains provide valuable knowledge. Meanwhile, in multi-source and multi-target adaptation scenarios, feature generators with static parameters have difficulty generating deep features of each individual domain. In this article, we propose a Dynamic Generator With Attention (DGWA) method for multi-source and multi-target domain adaptation to adapt domain-agnostic deep features in multi source and multi target domain scenarios. The feature generator with dynamic parameters can dynamically change its parameters with data input from different domains, which greatly improves the generalization of the feature pools. An attention mechanism is used in our DGWA to learn more transferable information from different domains. To demonstrate the performance of DGWA, we conduct extensive experiments on several popular domain adaptation datasets, including the digits, Office+Caltech10, Office-Home, and ImageCLEF-DA datasets. The experimental results demonstrate that our method performs better than state-of-the-art methods.
C1 [Lu, Yuwu; Huang, Haoyu; Zeng, Biqing] South China Normal Univ, Sch Software, Foshan 528225, Peoples R China.
   [Lu, Yuwu] Hong Kong Polytech Univ, Inst Text & Clothing, Hong Kong, Peoples R China.
   [Lai, Zhihui] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518055, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
C3 South China Normal University; Hong Kong Polytechnic University;
   Shenzhen University; Northwestern Polytechnical University
RP Zeng, BQ (corresponding author), South China Normal Univ, Sch Software, Foshan 528225, Peoples R China.
EM luyuwu2008@163.com; hyhuang99@163.com; zengbiqing137@163.com;
   lai_zhi_hui@163.com; xuelong_li@nwpu.edu.cn
RI Li, Xuelong/Z-3785-2019
OI Lai, Zhihui/0000-0002-4388-3080; Huang, Haoyu/0009-0006-5648-4237;
   Biqing, Zeng/0000-0001-9088-4759
FU National Natural Science Foundation of China
FX No Statement Available
CR Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bousmalis K, 2016, ADV NEUR IN, V29
   Caputo Barbara, 2014, INT C CROSS LANG EV, P192, DOI DOI 10.1007/978-3-319-11382-118
   Dai QY, 2023, IEEE T KNOWL DATA EN, V35, P4908, DOI 10.1109/TKDE.2022.3144250
   Deng C, 2018, PATTERN RECOGN, V77, P306, DOI 10.1016/j.patcog.2017.10.007
   Deng ZY, 2022, IEEE T IMAGE PROCESS, V31, P4585, DOI 10.1109/TIP.2022.3186531
   Fang Z, 2021, IEEE T NEUR NET LEAR, V32, P4309, DOI 10.1109/TNNLS.2020.3017213
   Feng Y, 2023, IEEE T NEUR NET LEAR, V34, P3082, DOI 10.1109/TNNLS.2021.3111732
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gholami B, 2020, IEEE T IMAGE PROCESS, V29, P3993, DOI 10.1109/TIP.2019.2963389
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Griffin G., 2007, Caltech-256 object category dataset
   Gu X, 2024, IEEE T PATTERN ANAL, V46, P1757, DOI 10.1109/TPAMI.2022.3158637
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jie ZQ, 2021, IEEE T PATTERN ANAL, V43, P1875, DOI 10.1109/TPAMI.2019.2959322
   Kang Q, 2021, IEEE T NEUR NET LEAR, V32, P3919, DOI 10.1109/TNNLS.2020.3016180
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li KQY, 2023, IEEE T KNOWL DATA EN, V35, P4727, DOI 10.1109/TKDE.2022.3144423
   Li KQY, 2022, IEEE T NEUR NET LEAR, V33, P5293, DOI 10.1109/TNNLS.2021.3069982
   Li S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9082, DOI 10.1109/ICCV48922.2021.00897
   Li YS, 2021, PROC CVPR IEEE, P10993, DOI 10.1109/CVPR46437.2021.01085
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Ma A, 2022, IEEE T NEUR NET LEAR, V33, P6263, DOI 10.1109/TNNLS.2021.3073119
   Mesgaran M, 2021, IEEE T MULTIMEDIA, V23, P3931, DOI 10.1109/TMM.2020.3034530
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Paszke A, 2019, ADV NEUR IN, V32
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Ren CX, 2022, IEEE T IMAGE PROCESS, V31, P2122, DOI 10.1109/TIP.2022.3152052
   Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang JW, 2022, IEEE T MULTIMEDIA, V24, P230, DOI 10.1109/TMM.2021.3050057
   Wang XM, 2019, AAAI CONF ARTIF INTE, P5345
   Wang YX, 2021, IEEE T IMAGE PROCESS, V30, P892, DOI 10.1109/TIP.2020.3031161
   Westfechtel T, 2023, IEEE WINT CONF APPL, P392, DOI 10.1109/WACV56688.2023.00047
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Wu ZH, 2023, APPL INTELL, V53, P3766, DOI 10.1007/s10489-022-03638-6
   Xia WH, 2022, IEEE T EMERG TOP COM, V10, P962, DOI 10.1109/TETC.2021.3056031
   Xia Y, 2022, MECH SYST SIGNAL PR, V168, DOI 10.1016/j.ymssp.2021.108697
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xu DQ, 2023, APPL INTELL, V53, P10766, DOI 10.1007/s10489-022-04077-z
   Xu MH, 2024, IEEE T PATTERN ANAL, V46, P1727, DOI 10.1109/TPAMI.2022.3172372
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Yang X, 2022, IEEE T PATTERN ANAL, V44, P1992, DOI 10.1109/TPAMI.2020.3026079
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
   Zhang ZL, 2022, IEEE T KNOWL DATA EN, V34, P2335, DOI 10.1109/TKDE.2020.3005952
   Zhao H., 2018, Advances in neural information processing systems, P8559
   Zhou LH, 2022, IEEE T NEUR NET LEAR, V33, P5308, DOI 10.1109/TNNLS.2021.3070085
   Zuo YK, 2021, IEEE T IMAGE PROCESS, V30, P3793, DOI 10.1109/TIP.2021.3065254
NR 60
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6891
EP 6905
DI 10.1109/TMM.2024.3358062
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000035
DA 2024-08-05
ER

PT J
AU Nie, WZ
   Bao, YR
   Zhao, Y
   Liu, AA
AF Nie, Weizhi
   Bao, Yuru
   Zhao, Yue
   Liu, Anan
TI Long Dialogue Emotion Detection Based on Commonsense Knowledge Graph
   Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion detection; growing graph; commonsense knowledge graph; topic
   module
ID SPEECH; RECOGNITION
AB Dialogue emotion detection is always challenging due to human subjectivity and the randomness of dialogue content. In a conversation, the emotion of each person often develops via a cumulative process, which can be influenced by many elements of uncertainty. Much commonsense knowledge influences people's emotions imperceptibly, such as experiential or habitual knowledge. In the process of conversation, this commonsense knowledge information can be used to enrich the semantic information of each utterance and improve the accuracy of emotion recognition. In this paper, we propose a growing graph model for dialogues emotion detection based on retrieval of external knowledge atlas ATOMIC from local and global respectively, which can effectively represent the dialogues as a process variable in a sequence and the correlation among utterances also can be represented by the graph model. In particular, 1) we introduce a common sense knowledge graph for linking the commonsense knowledge retrieved from external knowledge atlas ATOMIC, which can effectively add auxiliary information to improve the performance of each utterance's representation. 2) We propose a novel self-supervised learning method for extracting the latent topic of each dialogue. Based on this design, we also propose an effective optimization mechanism to make the representation (embedding) of latent topic has a better distinction for the next operation. 3) Finally, the cross-attention module is utilized to combine the utterances' features and the latent conversation topic information. The attention mechanism can effectively use topic information to supplement the representation of utterances and improve recognition performance. The model is tested on three popular datasets in dialogue emotion detection and is empirically demonstrated to outperform the state-of-the-art approaches. Meanwhile, to demonstrate the performance of our approach, we also build a long dialogue dataset. The average length of each conversation is over 50 utterances. The final experimental results also demonstrate the superior performance of our approach.
C1 [Nie, Weizhi; Bao, Yuru; Zhao, Yue; Liu, Anan] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Zhao, Y; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM weizhinie@tju.edu.cn; baoyuru@tju.edu.cn; zhaoyue_tju@163.com;
   anan0422@gmail.com
OI nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdelwahab M, 2018, IEEE-ACM T AUDIO SPE, V26, P2423, DOI 10.1109/TASLP.2018.2867099
   Bosselut A, 2019, Arxiv, DOI arXiv:1906.05317
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cai L, 2022, IEEE T PATTERN ANAL, V44, P5103, DOI 10.1109/TPAMI.2021.3080635
   Chen HF, 2021, IEEE T MULTIMEDIA, V23, P4171, DOI 10.1109/TMM.2020.3037496
   Chen Sheng-Yeh, 2018, Emotion-lines: An emotion corpus of multi-party conversations
   Choi J, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0256039
   Choi YJ, 2021, BIG DATA-US, V9, P279, DOI 10.1089/big.2020.0274
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   Datcu D, 2015, EMOTION RECOGNITION: A PATTERN ANALYSIS APPROACH, P411
   Defferrard M, 2016, ADV NEUR IN, V29
   Devillers L, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P801
   EKMAN P, 1993, AM PSYCHOL, V48, P384, DOI 10.1037/0003-066X.48.4.384
   Gat I, 2022, INT CONF ACOUST SPEE, P7342, DOI 10.1109/ICASSP43922.2022.9747460
   Ghosal D, 2019, arXiv
   Ghosal D, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P2470
   Ghosh S, 2023, Arxiv, DOI arXiv:2203.16799
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hazarika D, 2021, INFORM FUSION, V65, P1, DOI 10.1016/j.inffus.2020.06.005
   Hazarika D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2594
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   Hu D., 2021, arXiv
   Huang XD, 2021, IEEE MULTIMEDIA, V28, P76, DOI 10.1109/MMUL.2021.3065678
   Huang ZC, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1329
   Huang ZH, 2015, Arxiv, DOI arXiv:1508.01991
   Ishiwatari T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P7360
   Jalal MA, 2020, INTERSPEECH, P4113, DOI 10.21437/Interspeech.2020-3007
   Jiao WX, 2020, AAAI CONF ARTIF INTE, V34, P8002
   Jin Q, 2015, INT CONF ACOUST SPEE, P4749, DOI 10.1109/ICASSP.2015.7178872
   Khare A, 2021, IEEE W SP LANG TECH, P381, DOI 10.1109/SLT48900.2021.9383618
   Khare SK, 2021, IEEE T NEUR NET LEAR, V32, P2901, DOI 10.1109/TNNLS.2020.3008938
   Kim J., 2020, P 3 WORKSH COMP MOD, P64
   Lee CC, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P1955
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Li C, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102185
   Li DY, 2021, KNOWL-BASED SYST, V232, DOI 10.1016/j.knosys.2021.107449
   Li RN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5060
   Li W, 2022, NEUROCOMPUTING, V467, P73, DOI 10.1016/j.neucom.2021.09.057
   Li Y, 2017, Long Papers, V1, P986
   Lian Z, 2020, INTERSPEECH, P394, DOI 10.21437/Interspeech.2020-1705
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Lu X., 2020, P 28 INT C COMP LING, P4078, DOI DOI 10.18653/V1/2020.COLING-MAIN.360
   Majumder N, 2019, AAAI CONF ARTIF INTE, P6818
   Mansouri-Benssassi E, 2020, AAAI CONF ARTIF INTE, V34, P1351
   Metallinou A, 2012, IEEE T AFFECT COMPUT, V3, P184, DOI 10.1109/T-AFFC.2011.40
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Narayanan S, 2013, P IEEE, V101, P1203, DOI 10.1109/JPROC.2012.2236291
   Parmar N, 2018, PR MACH LEARN RES, V80
   Petroni F, 2019, Arxiv, DOI [arXiv:1909.01066, DOI 10.48550/ARXIV.1909.01066]
   Poria S, 2019, Arxiv, DOI arXiv:1810.02508
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Reimers N, 2019, Arxiv, DOI [arXiv:1908.10084, DOI 10.48550/ARXIV.1908.10084]
   Sap M, 2019, AAAI CONF ARTIF INTE, P3027
   Shao Zhuang, 2022, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2022.3152990
   Shen W, 2021, arXiv
   Shirian A, 2022, IEEE T MULTIMEDIA, V24, P780, DOI 10.1109/TMM.2021.3059169
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Wagner J, 2023, IEEE T PATTERN ANAL, V45, P10745, DOI 10.1109/TPAMI.2023.3263585
   Wang H., 2020, P NEURIPS, V33, P19839
   Wang KX, 2020, NEUROCOMPUTING, V398, P257, DOI 10.1016/j.neucom.2020.02.085
   Wang LF, 2022, INT J INTELL SYST, V37, P5643, DOI 10.1002/int.22805
   Wang Yingzhi, 2021, arXiv
   Wöllmer M, 2010, IEEE J-STSP, V4, P867, DOI 10.1109/JSTSP.2010.2057200
   Wu F, 2019, PR MACH LEARN RES, V97
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu XR, 2020, Arxiv, DOI arXiv:1909.11334
   Yu E.-Y., 2021, arXiv
   Zhang D. Z., 2020, P 28 INT C COMPUTATI, P4429
   Zhang D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P503, DOI 10.1145/3394171.3413949
   Zhang D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5415
   Zhong PX, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P165
   Zhu L., 2021, arXiv
NR 73
TC 32
Z9 32
U1 41
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 514
EP 528
DI 10.1109/TMM.2023.3267295
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000024
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Umam, A
   Yang, CK
   Chuang, JH
   Lin, YY
AF Umam, Ardian
   Yang, Cheng-Kun
   Chuang, Jen-Hui
   Lin, Yen-Yu
TI Unsupervised Point Cloud Co-Part Segmentation via Co-Attended Superpoint
   Generation and Aggregation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Semantics; Shape; Image segmentation; Task
   analysis; Three-dimensional displays; Annotations; Point cloud
   segmentation; co-part segmentation; co-segmentation; unsupervised
   learning
ID SALIENCY DETECTION; REPRESENTATION
AB We propose a co-part segmentation method that takes a set of point clouds of the same category as input where neither a ground truth label nor a prior network is required. With difficulties caused by the label absence, we formulate the co-part segmentation task into two subtasks, including superpoint generation and part aggregation. In the first subtask, our superpoint generation network divides each point cloud into homogeneous partitions, each called superpoint, while in the second subtask, these superpoints are further aggregated into a few semantic parts via our part aggregation network. We introduce the coupled attention blocks in the part aggregation network to explicitly enforce semantic consistency in the segmentation by exploiting intra-, inter-, and paired-cloud geometrical information by minimizing the devised intra-, inter-, and paired-cloud losses, respectively. The intra-cloud loss triggers a semantic segmentation in each point cloud, while the inter-cloud loss considers all clouds to enforce their semantic consistency. The paired-cloud loss is designed to ensure that each part of one point cloud can be discriminatively reconstructed from the superpoints of another point cloud. We perform experiments on two benchmark datasets, ShapeNet part and COSEG, and provide quantitative and qualitative results to demonstrate the superiority of our method over existing methods. We also show that the proposed method can help several downstream tasks, including semi-supervised part segmentation and data augmentation for shape classification.
C1 [Umam, Ardian; Chuang, Jen-Hui; Lin, Yen-Yu] Natl Yang Ming Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
   [Yang, Cheng-Kun] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
C3 National Yang Ming Chiao Tung University; National Taiwan University
RP Lin, YY (corresponding author), Natl Yang Ming Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
EM ardianumam.ee09@nycu.edu.tw; d08922002@csie.ntu.edu.tw;
   jchuang@cs.nctu.edu.tw; lin@cs.nycu.edu.tw
OI Lin, Yen-Yu/0000-0002-7183-6070
FU National Science and Technology Council
FX No Statement Available
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Carion N., 2020, EUR C COMP VIS, P213
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012
   Chen ZQ, 2019, IEEE I CONF COMP VIS, P8489, DOI 10.1109/ICCV.2019.00858
   Choudhury S., 2021, ADV NEUR IN, P28104
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Demantké J, 2011, INT ARCH PHOTOGRAMM, V38-5, P97
   Deng B., 2022, P ADV NEUR INF PROC, V35, P37837
   Deng S, 2022, IEEE INT CONF ROBOT, P9214, DOI 10.1109/ICRA46639.2022.9811904
   Duh PJ, 2021, IEEE T MULTIMEDIA, V23, P1567, DOI 10.1109/TMM.2020.3001500
   Fan ZX, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1810, DOI 10.1109/ICASSP39728.2021.9414867
   Gadelha Matheus, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P473, DOI 10.1007/978-3-030-58607-2_28
   Guo B, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3322579
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hu RZ, 2012, COMPUT GRAPH FORUM, V31, P1703, DOI 10.1111/j.1467-8659.2012.03175.x
   Hui L., 2022, P ADV NEUR INF PROC, P36804
   Hui L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5490, DOI 10.1109/ICCV48922.2021.00546
   Hung WC, 2019, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2019.00096
   Jerripothula KR, 2018, IEEE T MULTIMEDIA, V20, P2466, DOI 10.1109/TMM.2018.2798294
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P3193, DOI 10.1109/TMM.2020.3021251
   Jiang TP, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3158362
   Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kaashki NN, 2023, IEEE T MULTIMEDIA, V25, P831, DOI 10.1109/TMM.2021.3132487
   Khan A. A., 2022, IEEE Transactions on Multimedia, DOI DOI 10.1109/TMM.2022.3230330
   Kim S., 2021, PROC BRIT MACH VIS C
   Landrieu L, 2019, PROC CVPR IEEE, P7432, DOI 10.1109/CVPR.2019.00762
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Landrieu L, 2017, SIAM J IMAGING SCI, V10, P1724, DOI 10.1137/17M1113436
   Le-Khac PH, 2020, IEEE ACCESS, V8, P193907, DOI 10.1109/ACCESS.2020.3031549
   Lei JH, 2023, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR52729.2023.00475
   Li JL, 2023, PROC CVPR IEEE, P21694, DOI 10.1109/CVPR52729.2023.02078
   Li TP, 2022, IEEE T MULTIMEDIA, V24, P492, DOI 10.1109/TMM.2021.3054526
   Li XL, 2023, PROC CVPR IEEE, P21781, DOI 10.1109/CVPR52729.2023.02086
   Lin C., 2022, Proc. NeurIPS, V35, P15217
   Liu DS, 2022, NEURAL COMPUT APPL, V34, P17371, DOI 10.1007/s00521-022-07379-y
   Liu GZ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174275
   Liu SL, 2021, PROC CVPR IEEE, P8351, DOI 10.1109/CVPR46437.2021.00825
   Liu WX, 2022, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR52688.2022.00270
   Lu ZM, 2019, IEEE ACCESS, V7, P62875, DOI 10.1109/ACCESS.2019.2917152
   Makuch M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12101542
   Mamou K., 2016, GAME ENGINE GEMS, V3, P141
   Michel O, 2022, PROC CVPR IEEE, P13482, DOI 10.1109/CVPR52688.2022.01313
   Muralikrishnan S, 2018, PROC CVPR IEEE, P2926, DOI 10.1109/CVPR.2018.00309
   Nekrasov A, 2021, INT CONF 3D VISION, P116, DOI 10.1109/3DV53792.2021.00022
   Paschalidou D, 2020, PROC CVPR IEEE, P1057, DOI 10.1109/CVPR42600.2020.00114
   Paschalidou D, 2019, PROC CVPR IEEE, P10336, DOI 10.1109/CVPR.2019.01059
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Rother C., 2006, IEEE Computer Society Conference on, V1, P993
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Rubio JC, 2012, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2012.6247745
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Shu ZY, 2016, COMPUT AIDED GEOM D, V43, P39, DOI 10.1016/j.cagd.2016.02.015
   Sidi O, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024160
   Song Z., 2022, P ADV NEUR INF PROC, P30798
   Sun C, 2023, IEEE T MULTIMEDIA, V25, P6207, DOI 10.1109/TMM.2022.3206664
   Sung M, 2018, ADV NEUR IN, V31
   Sung M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130821
   Umam A, 2022, LECT NOTES COMPUT SC, V13689, P596, DOI 10.1007/978-3-031-19818-2_34
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Wen MX, 2022, IEEE INT C INT ROBOT, P7841, DOI 10.1109/IROS47612.2022.9981281
   Wu XY, 2023, PROC CVPR IEEE, P9415, DOI 10.1109/CVPR52729.2023.00908
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu MT, 2021, AAAI CONF ARTIF INTE, V35, P3056
   Yang CK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7315, DOI 10.1109/ICCV48922.2021.00724
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yu C, 2022, PROC CVPR IEEE, P4022, DOI 10.1109/CVPR52688.2022.00400
   Yuan YJ, 2021, J COMPUT SCI TECH-CH, V36, P520, DOI 10.1007/s11390-021-1414-9
   Zanjani FG, 2019, PR MACH LEARN RES, V102, P557
   Zanjani FG, 2019, LECT NOTES COMPUT SC, V11768, P128, DOI 10.1007/978-3-030-32254-0_15
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhu Chenyang, 2020, P IEEECVF C COMPUTER, P8543
   Ziegler A, 2022, PROC CVPR IEEE, P14482, DOI 10.1109/CVPR52688.2022.01410
NR 80
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7775
EP 7786
DI 10.1109/TMM.2024.3371294
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000005
DA 2024-08-05
ER

PT J
AU Wang, LX
   Li, HL
   Zhang, MJ
   Qiu, HQ
   Meng, FM
   Wu, QB
   Xu, LF
AF Wang, Lanxiao
   Li, Hongliang
   Zhang, Minjian
   Qiu, Heqian
   Meng, Fanman
   Wu, Qingbo
   Xu, Linfeng
TI CrowdCaption plus plus : Collective-Guided Crowd Scenes Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Charge coupled devices; Decoding;
   Task analysis; Image analysis; Behavioral sciences; CrowdCaption; crowd
   scenes captioning; CrowdCaption plus plus; collectives guidance;
   double-query attention
ID DEEP
AB Crowd scenes analysis plays an important role in various fields, including public security, smart cities, and intelligent transportation systems. However, traditional crowd scenes captioning methods mainly focus on a single and prominent crowd collective, which limits their ability to describe the different crowd collectives in complex crowd scenes. To address this issue, we propose a collective-guided crowd scenes captioning model (CrowdCaption++) to explore a more comprehensive and detailed description. We design a crowd features encoder (CFE) including double-query features encoder and foreground crowd features encoder, which uses double-query attention module (DQ-ATT) to capture more representative visual features and extracts foreground crowd features to avoid interference from background for collectives prediction. Moreover, we build a collective-guided captioning decoder (CCD) to generate captions of different crowd collectives without requiring extra alignment between crowd collectives and captions. To achieve this, we first design a crowd collectives predictor to identify multiple potential crowd collectives and create crowd collectives guidance information. Finally, we use the crowd collectives guidance information to merge useful visual features and further generate corresponding caption. We evaluate our approach on the latest crowd scenes dataset CrowdCaption and demonstrate that our model can achieve a comprehensive understanding and describe the different crowd collectives in complex crowd scenes.
C1 [Wang, Lanxiao; Li, Hongliang; Zhang, Minjian; Qiu, Heqian; Meng, Fanman; Wu, Qingbo; Xu, Linfeng] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, HL; Qiu, HQ (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM lanxiao.wang@std.uestc.edu.cn; hlli@uestc.edu.cn;
   mjzhang_ivip@std.uestc.edu.cn; hqqiu@std.uestc.edu.cn;
   fmmeng@uestc.edu.cn; qbwu@uestc.edu.cn; lfxu@uestc.edu.cn
RI Wu, Qingbo/M-5065-2015; Xu, Linfeng/HME-1913-2023
OI Wu, Qingbo/0000-0003-2936-6340; Qiu, Heqian/0000-0002-0963-0311; Xu,
   Linfeng/0000-0002-9934-0958
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Armstrong JW, 1927, SOC FORCES, V5, P583, DOI 10.2307/3004622
   Beichen Zhang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P1112, DOI 10.1145/3394171.3413885
   Chen L, 2021, PROC CVPR IEEE, P16841, DOI 10.1109/CVPR46437.2021.01657
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denkowski M., 2014, P WMT ACL, P376
   Ding GC, 2023, IEEE T MULTIMEDIA, V25, P4665, DOI 10.1109/TMM.2022.3180222
   Dong XZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2615, DOI 10.1145/3474085.3475439
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Hu N., IEEE Trans. Multimedia
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang X., 2020, CVPR, P10747
   Jia Q, 2021, AAAI CONF ARTIF INTE, V35, P13125
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Jin L, 2023, IEEE T MULTIMEDIA, V25, P3364, DOI 10.1109/TMM.2022.3159111
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2531, DOI 10.1109/TMM.2019.2908350
   Li Q, 2023, IEEE T MULTIMEDIA, V25, P1521, DOI 10.1109/TMM.2023.3248144
   Li YA, 2022, PROC CVPR IEEE, P17969, DOI 10.1109/CVPR52688.2022.01746
   Li YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3799, DOI 10.1145/3474085.3478331
   Li YK, 2018, IEEE T MULTIMEDIA, V20, P3289, DOI 10.1109/TMM.2018.2834873
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632
   Liu X., 2021, P IEEE CVF INT C COM, P3215
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qiu HQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4435, DOI 10.1145/3503161.3547765
   Reddy MKK, 2022, IEEE T MULTIMEDIA, V24, P1008, DOI 10.1109/TMM.2021.3062481
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shao S, 2018, Arxiv, DOI arXiv:1805.00123
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Song ZL, 2021, AAAI CONF ARTIF INTE, V35, P2584
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang LX, 2023, IEEE T MULTIMEDIA, V25, P5400, DOI 10.1109/TMM.2022.3192729
   Wang MJ, 2023, IEEE T MULTIMEDIA, V25, P2074, DOI 10.1109/TMM.2022.3142398
   Wang N, 2023, AAAI CONF ARTIF INTE, P2617
   Wang WX, 2019, AAAI CONF ARTIF INTE, P8957
   Wang YY, 2022, AAAI CONF ARTIF INTE, P2585
   Wen LY, 2021, PROC CVPR IEEE, P7808, DOI 10.1109/CVPR46437.2021.00772
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang JL, 2021, IEEE T MULTIMEDIA, V23, P3085, DOI 10.1109/TMM.2020.3020691
   Zou SH, 2023, IEEE T MULTIMEDIA, V25, P3560, DOI 10.1109/TMM.2022.3162469
NR 55
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4974
EP 4986
DI 10.1109/TMM.2023.3328189
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600017
DA 2024-08-05
ER

PT J
AU Weng, SW
   Zhu, TG
   Zhang, TC
   Zhang, CY
AF Weng, Shaowei
   Zhu, Tangguo
   Zhang, Tiancong
   Zhang, Chunyu
TI UCM-Net: A U-Net-Like Tampered-Region-Related Framework for Copy-Move
   Forgery Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE ASPP; CMFD; cross-layer connections; FEM; RSU; self-correlation; UCM-Net
ID NETWORKS
AB Copy-move forgery causes a big challenge to copy-move forgery detection (CMFD) due to that the photometrical characteristics of genuine and tampered regions in the same image remain highly consistent. A novel U-Net-like architecture with multiple asymmetric cross-layer connections associated with self-correlation and atrous spatial pyramid pooling (ASPP) between feature extraction module (FEM) and tampered region localization module (TRLM), called UCM-Net, is proposed in this article. Different from existing deep learning based CMFD networks which indiscriminately process large or small tampered regions without considering the statistical characteristics of regions, FEM differentially treats large or small tampered regions by exploiting deep backbone networks to extract high-level features with rich semantic information for large tampered regions while utilizing lightweight backbone networks to extract low-level features for small tampered regions. Multiple cross-layer connections between two modules utilize the self-correlation calculation and ASPP to remove as much irrelevant semantic information as possible while retaining multi-scale tampered features from shallow to deep convolutional layers of FEM. Unlike the previous CMFD networks, which cannot capture multi-scale features because of simply stacking convolution blocks in the upsampling step, TRLM exploits multiple U-shaped residual U-block modules with different depths to change the receptive field of each point in the tampered feature maps so as to capture global and local information, greatly improving the localization accuracy of tampered regions. Experimental results on three publicly available databases demonstrate that UCM-Net outperforms several state-of-the-art algorithms in terms of various evaluation metrics.
C1 [Weng, Shaowei] Fujian Univ Technol, Fujian Prov Key Lab Big Data Min & Applicat, Fuzhou 350108, Peoples R China.
   [Weng, Shaowei] Fujian Univ Technol, Sch Comp Sci & Math, Fuzhou 350118, Peoples R China.
   [Zhu, Tangguo; Zhang, Tiancong] Fujian Univ Technol, Sch Elect Elect Engn & Phys, Fuzhou 350108, Peoples R China.
   [Zhang, Chunyu] Xizang Minzu Univ, Coll Informat Engn, Xianyang 712082, Peoples R China.
C3 Fujian University of Technology; Fujian University of Technology; Fujian
   University of Technology; Xizang Minzu University
RP Zhang, TC (corresponding author), Fujian Univ Technol, Sch Elect Elect Engn & Phys, Fuzhou 350108, Peoples R China.
EM wswweiwei@126.com; 2978064409@qq.com; kushentian@163.com;
   zcy@xzmu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Ardizzone E, 2015, IEEE T INF FOREN SEC, V10, P2084, DOI 10.1109/TIFS.2015.2445742
   Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Bunk J, 2017, IEEE COMPUT SOC CONF, P1881, DOI 10.1109/CVPRW.2017.235
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen XR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14165, DOI 10.1109/ICCV48922.2021.01392
   Costanzo A, 2014, IEEE T INF FOREN SEC, V9, P1450, DOI 10.1109/TIFS.2014.2337654
   Dong CB, 2023, IEEE T PATTERN ANAL, V45, P3539, DOI 10.1109/TPAMI.2022.3180556
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang DY, 2017, MULTIMED TOOLS APPL, V76, P1509, DOI 10.1007/s11042-015-3152-x
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Liu XH, 2022, IEEE T CIRC SYST VID, V32, P7505, DOI 10.1109/TCSVT.2022.3189545
   Liu YQ, 2022, IEEE T IMAGE PROCESS, V31, P541, DOI 10.1109/TIP.2021.3132828
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Mahmood T, 2016, MATH PROBL ENG, V2016, DOI 10.1155/2016/8713202
   NIST, 2016, nimble 2016 datasets
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ryu SJ, 2010, LECT NOTES COMPUT SC, V6387, P51, DOI 10.1007/978-3-642-16435-4_5
   Shivakumar B., 2011, International Journal of Computer Science Issues (IJCSI), V8, P199
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tralic Dijana, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P49
   Wang JK, 2022, PROC CVPR IEEE, P2354, DOI 10.1109/CVPR52688.2022.00240
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Wu Y, 2018, LECT NOTES COMPUT SC, V11210, P170, DOI 10.1007/978-3-030-01231-1_11
   Wu Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1480, DOI 10.1145/3123266.3123411
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
   Zhu Y, 2020, IEEE T IND INFORM, V16, P6714, DOI 10.1109/TII.2020.2982705
   Zhu Y, 2016, MULTIMED TOOLS APPL, V75, P3221, DOI 10.1007/s11042-014-2431-2
NR 34
TC 5
Z9 5
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 750
EP 763
DI 10.1109/TMM.2023.3270629
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000023
DA 2024-08-05
ER

PT J
AU Wu, TT
   Ding, X
   Zhang, H
   Gao, JL
   Tang, MJ
   Du, L
   Qin, B
   Liu, T
AF Wu, Tingting
   Ding, Xiao
   Zhang, Hao
   Gao, Jinglong
   Tang, Minji
   Du, Li
   Qin, Bing
   Liu, Ting
TI DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples
   Discrimination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Machine learning; deep learning; noisy label; label noise; robust
   methods
AB Given data with label noise (i.e., incorrect data), deep neural networks would gradually memorize the label noise and impair model performance. To relieve this issue, curriculum learning is proposed to improve model performance and generalization by ordering training samples in a meaningful (e.g., easy to hard) sequence. Previous work takes incorrect samples as generic hard ones without discriminating between hard samples (i.e., hard samples in correct data) and incorrect samples. Indeed, a model should learn from hard samples to promote generalization rather than overfit to incorrect ones. In this article, we address this problem by appending a novel loss function DiscrimLoss, on top of the existing task loss. Its main effect is to automatically and stably estimate the importance of easy samples and difficult samples (including hard and incorrect samples) at the early stages of training to improve the model performance. Then, during the following stages, DiscrimLoss is dedicated to discriminating between hard and incorrect samples to improve the model generalization. Such a training strategy can be formulated dynamically in a self-supervised manner, effectively mimicking the main principle of curriculum learning. Experiments on image classification, image regression, text sequence regression, and event relation reasoning demonstrate the versatility and effectiveness of our method, particularly in the presence of diversified noise levels.
C1 [Wu, Tingting; Ding, Xiao; Zhang, Hao; Gao, Jinglong; Tang, Minji; Du, Li; Qin, Bing; Liu, Ting] Harbin Inst Technol, Fac Comp, Harbin 150000, Peoples R China.
C3 Harbin Institute of Technology
RP Ding, X (corresponding author), Harbin Inst Technol, Fac Comp, Harbin 150000, Peoples R China.
EM ttwu@ir.hit.edu.cn; xding@ir.hit.edu.cn; zhh1000@hit.edu.cn;
   jlgao@ir.hit.edu.cn; mjtang@ir.hit.edu.cn; ldu@ir.hit.edu.cn;
   bqin@ir.hit.edu.cn; tliu@ir.hit.edu.cn
RI Wu, Tingting/IWM-4143-2023
OI Wu, Tingting/0000-0003-3437-8899; Ding, Xiao/0000-0002-5838-0320; Gao,
   Jinglong/0000-0002-4466-0516; Zhang, Hao/0000-0002-6769-2115
FU Technological Innovation 2030 Megaproject New Generation Artificial
   Intelligence of China
FX No Statement Available
CR Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Bergstra J., 2013, International Conference on Machine Learning, P115
   Birodkar V, 2019, Arxiv, DOI arXiv:1901.11409
   Castells T., 2020, Advances in Neural Information Processing Systems, P4308
   Chaudhary C, 2020, IEEE T MULTIMEDIA, V22, P897, DOI 10.1109/TMM.2019.2937181
   Chen P., 2021, PROC INT C LEARN REP, P1
   Chitta K, 2022, IEEE T INTELL TRANSP, V23, P14741, DOI 10.1109/TITS.2021.3133268
   Cirik V., 2016, Visualizing and understanding curriculum learning for long short-term memory networks
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4
   Feng YL, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1295
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Guo Y, 2020, PR MACH LEARN RES, V119
   Hacohen G, 2019, PR MACH LEARN RES, V97
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374
   Huang YY, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P389
   Jamieson K, 2016, JMLR WORKSH CONF PRO, V51, P240
   Jiang L, 2015, AAAI CONF ARTIF INTE, P2694
   Kalantidis Y., 2020, ADV NEURAL INFORM PR, V33, P21798
   Krizhevsky A, 2009, CIFAR-10 dataset
   Kumar M, 2010, Advances in Neural Information Processing Systems, V23
   LeCun Y., 1998, The MNIST Database of Handwritten Digits
   Li J., 2020, PROC INT C LEARN REP
   Li L, 2016, Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization, DOI [10.48550/arxiv.1603.06560, DOI 10.1007/978-1-4899-7687-1_100200]
   Li ZY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3629
   Lin BY, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2829
   Lin YH, 2021, IEEE T MULTIMEDIA, V23, P1605, DOI 10.1109/TMM.2020.3001521
   Liu Yinhan, 2020, INT C LEARN REPR
   Loshchilov I., 2018, INT C LEARN REPR
   Lu J, 2018, PR MACH LEARN RES, V80
   Lyu Y., 2020, PROC INT C LEARN REP
   Mei Hongyuan, 2015, PROC C N AM CHAPTER, P720
   Nguyen D. T, 2020, PROC INT C LEARN REP
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Paul S., 2021, Advances in Neural Information Processing Systems, V34, P20596
   Radford A., 2019, OpenAI blog, V1, P9
   Ren MY, 2018, PR MACH LEARN RES, V80
   Saxena S., 2019, Advances in Neural Information Processing Systems, P11095
   Sorscher Ben, 2022, Advances in Neural Information Processing Systems, V35, P19523
   Soviany P, 2020, IEEE WINT CONF APPL, P3452, DOI [10.1109/WACV45572.2020.9093408, 10.1109/wacv45572.2020.9093408]
   Spitkovsky Valentin I, 2010, HUMAN LANGUAGE TECHN, P751
   Suh Y, 2019, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2019.00742
   Toneva M., 2019, PROC INT C LEARN REP
   Wang CY, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3728
   Wang H, 2014, IEEE T MULTIMEDIA, V16, P1282, DOI 10.1109/TMM.2014.2312251
   Xia X., 2021, PROC INT C LEARN REP
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xu B., 2020, P 58 ANN M ASS COMP, P6095, DOI DOI 10.18653/V1/2020.ACL-MAIN.542
   Yang ZL, 2019, ADV NEUR IN, V32
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yu XR, 2019, PR MACH LEARN RES, V97
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang L, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4630
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhao MJ, 2020, AAAI CONF ARTIF INTE, V34, P9652
   Zheng W, 2020, PATTERN RECOGN LETT, V132, P4, DOI 10.1016/j.patrec.2018.06.029
NR 59
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1957
EP 1968
DI 10.1109/TMM.2023.3290477
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800002
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xie, T
   Wang, L
   Wang, K
   Li, RF
   Zhang, XY
   Zhang, HM
   Yang, LQ
   Liu, HP
   Li, J
AF Xie, Tao
   Wang, Li
   Wang, Ke
   Li, Ruifeng
   Zhang, Xinyu
   Zhang, Haoming
   Yang, Linqi
   Liu, Huaping
   Li, Jun
TI FARP-Net: Local-Global Feature Aggregation and Relation-Aware Proposals
   for 3D Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D object detection; deep learning; point cloud
ID POINT; NETWORK
AB In this work, we introduce FARP-Net, an adaptive local-global feature aggregation and relation-aware proposal network for high-quality 3D object detection from pure point clouds. Our key insight is that learning adaptive local-global feature aggregation from an irregular yet sparse point cloud and generating superb proposals are both pivotal for detection. Technically, we propose a novel local-global feature aggregation layer (LGFAL) that fully exploits the complementary correlation between local features and global features, and fuses their strengths adaptively via an attention-based fusion module. Furthermore, we incorporate a lightweight feature affine module (LFAM) into LGFAL to map the local features into a normal distribution, thus acquiring fine-grained features of each local region in a weight-sharing manner. During object proposal generation, we propose a weighted relation-aware proposal module (WRPM) that uses an objectness-aware formalism to weigh the relation importance among object candidates for a clear and principal context, thereby facilitating the generation of high-quality proposals. The WRPM challenges the traditional practice of extracting contextual information among all object candidates, which is inefficient as object candidates are always noisy and redundant. Experimentally, FARP-Net delivers superior performance on two widely used benchmarks with fewer parameters, 64.0% mAP@0.25 on the SUN RGB-D dataset and 70.9% mAP@0.25 on the ScanNet V2 dataset. We further validate that the proposed LGFAL and WRPM can be integrated into both indoor and outdoor detectors to boost performance.
C1 [Xie, Tao; Wang, Ke; Li, Ruifeng; Zhang, Haoming; Yang, Linqi] Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150006, Peoples R China.
   [Wang, Li; Zhang, Xinyu; Li, Jun] Tsinghua Univ, Sch Vehicle & Mobil, Beijing 100084, Peoples R China.
   [Li, Jun] Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China.
   [Liu, Huaping] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Harbin Institute of Technology; Tsinghua University; Harbin Institute of
   Technology; Tsinghua University
RP Wang, K; Li, RF (corresponding author), Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150006, Peoples R China.; Zhang, XY (corresponding author), Tsinghua Univ, Sch Vehicle & Mobil, Beijing 100084, Peoples R China.
EM xietao1997@hit.edu.cn; wangli_thu@mail.tsinghua.edu.cn;
   wangke@hit.edu.cn; lrf100@hit.edu.cn; xyzhang@tsinghua.edu.cn;
   1180611005@stu.hit.edu.cn; 1190303319@hit.edu.cn; hpliu@tsinghua.edu.cn;
   lj19580324@126.com
RI zhou, chen/KHW-8121-2024
OI Zhang, Haoming/0000-0002-0610-4113
FU Special Project for Research and Development in Key areas of Guangdong
   Province
FX No Statement Available
CR Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Bansal M, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV
   Billinghurst Mark, 2015, Foundations and Trends in Human-Computer Interaction, V8, P73, DOI 10.1561/1100000049
   Carmigniani J, 2011, HANDBOOK OF AUGMENTED REALITY, P3, DOI 10.1007/978-1-4614-0064-6_1
   Chen C, 2022, AAAI CONF ARTIF INTE, P221
   Chen JT, 2020, PROC CVPR IEEE, P389, DOI 10.1109/CVPR42600.2020.00047
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Cheng BW, 2021, PROC CVPR IEEE, P8959, DOI 10.1109/CVPR46437.2021.00885
   Cheng SL, 2021, IEEE T IMAGE PROCESS, V30, P4436, DOI 10.1109/TIP.2021.3072214
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Duan Y, 2022, PROC CVPR IEEE, P16959, DOI 10.1109/CVPR52688.2022.01647
   Engelmann F, 2017, IEEE INT CONF COMP V, P716, DOI 10.1109/ICCVW.2017.90
   Engelmann Francis, 2020, P IEEE CVF C COMP VI, P9031
   Feng MT, 2021, IEEE T IMAGE PROCESS, V30, P92, DOI 10.1109/TIP.2020.3031371
   Fernandes D, 2021, INFORM FUSION, V68, P161, DOI 10.1016/j.inffus.2020.11.002
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang TY, 2021, IEEE INT CONF ROBOT, P13408, DOI 10.1109/ICRA48506.2021.9561597
   Jin ZC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7169, DOI 10.1109/ICCV48922.2021.00710
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li Y, 2020, IEEE T GEOSCI REMOTE, V58, P3588, DOI 10.1109/TGRS.2019.2958517
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39
   Liu M, 2016, IEEE T CYBERNETICS, V46, P1217, DOI 10.1109/TCYB.2015.2430526
   Liu XH, 2020, COMPUT AIDED GEOM D, V79, DOI 10.1016/j.cagd.2020.101859
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2929, DOI 10.1109/ICCV48922.2021.00294
   Ma X., 2022, P 10 INT C LEARN REP
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Noh J, 2021, PROC CVPR IEEE, P14600, DOI 10.1109/CVPR46437.2021.01437
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Ran HX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15457, DOI 10.1109/ICCV48922.2021.01519
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vaswani A, 2017, ADV NEUR IN, V30
   Wald J, 2020, PROC CVPR IEEE, P3960, DOI 10.1109/CVPR42600.2020.00402
   Wang DQ, 2019, IEEE INT C INT ROBOT, P2876, DOI [10.1109/IROS40897.2019.8967897, 10.1109/iros40897.2019.8967897]
   Wang HY, 2022, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR52688.2022.00118
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Zhoutao, 2021, P IEEE CVF INT C COM, P3101
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie Q., 2020, P IEEE CVF C COMP VI, P10447, DOI 10.1109/cvpr42600.2020.01046
   Xie Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3692, DOI 10.1109/ICCV48922.2021.00369
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang B., 2018, PROC C ROBOT LE, P146
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yang L, 2020, ROBOT CIM-INT MANUF, V64, DOI 10.1016/j.rcim.2019.101929
   Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25
   Yu R. R., 2016, P BRIT MACH VIS C YO
   Yuan YH, 2021, INT J COMPUT VISION, V129, P2375, DOI 10.1007/s11263-021-01465-9
   Yue KY, 2018, ADV NEUR IN, V31
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhang YF, 2022, PROC CVPR IEEE, P18931, DOI 10.1109/CVPR52688.2022.01838
   Zaiwei Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P311, DOI [10.1061/9780784482933.027, 10.1007/978-3-030-58610-2_19]
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zheng W, 2021, AAAI CONF ARTIF INTE, V35, P3555
   Zheng ZL, 2020, INT CONF UNMAN AIRCR, P790, DOI [10.1109/ICUAS48674.2020.9213894, 10.1109/icuas48674.2020.9213894]
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 73
TC 9
Z9 9
U1 21
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1027
EP 1040
DI 10.1109/TMM.2023.3275366
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700054
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Xu, WJ
   Long, CJ
   Nie, YW
   Wang, GH
AF Xu, Wenju
   Long, Chengjiang
   Nie, Yongwei
   Wang, Guanghui
TI Disentangled Representation Learning for Controllable Person Image
   Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Disentangled representation; Transformer; controllable person synthesize
ID NETWORKS
AB In this paper, we propose a novel framework named DRL-CPG to learn disentangled latent representation for controllable person image generation, which can produce realistic person images with desired poses and human attributes (e.g. pose, head, upper clothes, and pants) provided by various source persons. Unlike the existing works leveraging the semantic masks to obtain the representation of each component, we propose to generate disentangled latent code via a novel attribute encoder with transformers trained in a manner of curriculum learning from a relatively easy step to a gradually hard one. A random component mask-agnostic strategy is introduced to randomly remove component masks from the person segmentation masks, which aims at increasing the difficulty of training and promoting the transformer encoder to recognize the underlying boundaries between each component. This enables the model to transfer both the shape and texture of the components. Furthermore, we propose a novel attribute decoder network to integrate multi-level attributes (e.g. the structure feature and the attribute representation) with well-designed Dual Adaptive Denormalization (DAD) residual blocks. Extensive experiments strongly demonstrate that the proposed approach is able to transfer both the texture and shape of different human parts and yield realistic results. To our knowledge, we are the first to learn disentangled latent representations with transformers for person image generation.
C1 [Xu, Wenju] AMAZON, Palo Alto, CA 94301 USA.
   [Long, Chengjiang] META Real Labs, Burlingame, CA 94010 USA.
   [Nie, Yongwei] South China Univ Technol, Guangzhou 510006, Peoples R China.
   [Wang, Guanghui] Toronto Metropolitan Univ, Dept Comp Sci, Toronto, ON M5B 2K3, Canada.
C3 Amazon.com; South China University of Technology; Toronto Metropolitan
   University
RP Wang, GH (corresponding author), Toronto Metropolitan Univ, Dept Comp Sci, Toronto, ON M5B 2K3, Canada.
EM xuwenju@amazon.com; cjfykx@gmail.com; nieyongwei@scut.edu.cn;
   wangcs@torontomu.ca
RI Guanghui, Wang/IVH-6475-2023
OI Guanghui, Wang/0000-0003-3166-1613; Wang, Guanghui/0000-0003-3182-104X
CR Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Carion N., 2020, EUR C COMP VIS, P213
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen X., 2016, Advances in Neural Information Processing Systems NIPS, P2180
   Dong XZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2615, DOI 10.1145/3474085.3475439
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dumoulin J., 2016, INT C LEARN REPRESEN
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Li B, 2022, IEEE T MULTIMEDIA, V24, P4077, DOI 10.1109/TMM.2021.3113786
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lv ZY, 2021, PROC CVPR IEEE, P10801, DOI 10.1109/CVPR46437.2021.01066
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Ren YR, 2022, PROC CVPR IEEE, P13525, DOI 10.1109/CVPR52688.2022.01317
   REN Z, 2017, PROC CVPR IEEE, P1151, DOI DOI 10.1109/CVPR.2017.128
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi MJ, 2016, LECT NOTES COMPUT SC, V9909, P105, DOI 10.1007/978-3-319-46454-1_7
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Tang H, 2022, IEEE T MULTIMEDIA, V24, P2963, DOI 10.1109/TMM.2021.3091847
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SZ, 2022, AAAI CONF ARTIF INTE, P2531
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao J, 2021, IEEE T MULTIMEDIA, V23, P3454, DOI 10.1109/TMM.2020.3025661
   Xiao YB, 2022, AAAI CONF ARTIF INTE, P2822
   Xie E., 2021, PROC INT JOINT C ART
   Xu WJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6363, DOI 10.1109/ICCV48922.2021.00632
   Xu WJ, 2019, IEEE T MULTIMEDIA, V21, P2387, DOI 10.1109/TMM.2019.2898777
   Yang YH, 2023, IEEE T MULTIMEDIA, V25, P280, DOI 10.1109/TMM.2021.3125134
   Yu JQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480565
   Zhai ZJ, 2023, PROC CVPR IEEE, P22086, DOI 10.1109/CVPR52729.2023.02115
   Zhang JS, 2021, PROC CVPR IEEE, P7978, DOI 10.1109/CVPR46437.2021.00789
   Zhang KH, 2022, IEEE T MULTIMEDIA, V24, P1378, DOI 10.1109/TMM.2021.3064273
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng L., 2015, PROC IEEE INT C COMP, P2530
   Zhou XY, 2022, LECT NOTES COMPUT SC, V13675, P161, DOI 10.1007/978-3-031-19784-0_10
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
   Zhu Z, 2020, PROC CVPR IEEE, P5466, DOI 10.1109/CVPR42600.2020.00551
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 56
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6065
EP 6077
DI 10.1109/TMM.2023.3345180
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100002
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yin, NZ
   Liu, CX
   Tian, RH
   Qian, XM
AF Yin, Nengzhong
   Liu, Chengxu
   Tian, Ruhao
   Qian, Xueming
TI SDPDet: Learning Scale-Separated Dynamic Proposals for End-to-End
   Drone-View Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Drone-view image; activation pyramid; scale-separated learnable
   proposals; object detection
AB Detecting objects in large-scale drone-view images is notoriously challenging due to their uneven distribution and scale variation caused by photoing angles. Common approaches promote drone-view object detection by two-step detection (i.e., detecting sub-regions first) and multi-scale input. However, all these methods suffer from onerous computational costs since the high model complexity and input resolution. In this paper, we propose a novel one-step detector, called SDPDet, to enable effective object learning in drone-view images. In particular, a Scale-separated Activation Pyramid (SAP) serves to focus on the regions with objects aggregated at each scale, and a Scale-separated Learnable Proposals (SLP) mechanism learns proposal boxes and corresponding features on these regions. By such design, the quantity of learnable proposals allows dynamic adjustment at each scale separately, which facilitates the objects learning of various distributions and scales with less computational costs. Experiments demonstrate SDPDet can significantly outperform the state-of-the-art one-step detectors on three widely-used benchmarks. On the most challenging VisDrone dataset, SDPDet with ResNet50 gains 5.4% AP and 6.9% AP s improvements while running 1.9x faster than previous models.
C1 [Yin, Nengzhong; Liu, Chengxu] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Tian, Ruhao] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University; Xi'an Jiaotong University
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
EM yinnz@foxmail.com; liuchx97@gmail.com; ruhaot2020@stu.xjtu.edu.cn;
   qianxm@mail.xjtu.edu.cn
RI Liu, Chengxu/ABI-2926-2022
OI Liu, Chengxu/0000-0001-8023-9465
FU NSFC
FX No Statement Available
CR Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Carion N., 2020, EUR C COMP VIS, P213
   Chen CR, 2019, IEEE INT CONF COMP V, P100, DOI 10.1109/ICCVW.2019.00018
   Contributors S, 2022, Spconv: Spatially sparse convolution library
   Deng ST, 2021, IEEE T IMAGE PROCESS, V30, P1556, DOI 10.1109/TIP.2020.3045636
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23
   Duan CZ, 2021, IEEE INT CONF COMP V, P2789, DOI 10.1109/ICCVW54120.2021.00313
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu C, 2017, arXiv
   Fu K, 2020, Arxiv, DOI arXiv:2009.00833
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Hong QH, 2022, PROC CVPR IEEE, P4713, DOI 10.1109/CVPR52688.2022.00468
   Huang YC, 2022, AAAI CONF ARTIF INTE, P1026
   Jin HY, 2023, IEEE T MULTIMEDIA, V25, P6356, DOI 10.1109/TMM.2022.3207895
   Kisantal M, 2019, Arxiv, DOI arXiv:1902.07296
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Koyun OC, 2022, SIGNAL PROCESS-IMAGE, V104, DOI 10.1016/j.image.2022.116675
   Leng JX, 2023, IEEE T CIRC SYST VID, V33, P1320, DOI 10.1109/TCSVT.2022.3210207
   Li CL, 2020, IEEE COMPUT SOC CONF, P737, DOI 10.1109/CVPRW50498.2020.00103
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li X, 2022, IEEE T CIRC SYST VID, V32, P1792, DOI 10.1109/TCSVT.2021.3082635
   Liao JJ, 2021, IEEE J-STARS, V14, P11204, DOI 10.1109/JSTARS.2021.3122152
   Lim JS, 2021, 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN INFORMATION AND COMMUNICATION (IEEE ICAIIC 2021), P181, DOI 10.1109/ICAIIC51459.2021.9415217
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu CX, 2024, IEEE T NEUR NET LEAR, V35, P1584, DOI 10.1109/TNNLS.2022.3184075
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z., 2021, PROC IEEE INT C MULT, P1
   Meethal Akhil, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P2046, DOI 10.1109/CVPRW59228.2023.00198
   Noh J, 2019, IEEE I CONF COMP VIS, P9724, DOI 10.1109/ICCV.2019.00982
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shao S, 2019, IEEE I CONF COMP VIS, P8429, DOI 10.1109/ICCV.2019.00852
   Shen W, 2019, IEEE INT CONF COMP V, P82, DOI 10.1109/ICCVW.2019.00016
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tang XW, 2021, IEEE T MULTIMEDIA, V23, P2398, DOI 10.1109/TMM.2020.3011319
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang JF, 2021, PROC CVPR IEEE, P15844, DOI 10.1109/CVPR46437.2021.01559
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang YM, 2022, AAAI CONF ARTIF INTE, P2567
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Xu JT, 2023, IEEE T MULTIMEDIA, V25, P4598, DOI 10.1109/TMM.2022.3178871
   Yang CHY, 2022, PROC CVPR IEEE, P13658, DOI 10.1109/CVPR52688.2022.01330
   Yang F, 2019, IEEE I CONF COMP VIS, P8310, DOI 10.1109/ICCV.2019.00840
   Yi Wang, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P651, DOI 10.1007/978-3-030-66823-5_39
   Zhang FB, 2021, IEEE T MULTIMEDIA, V23, P1410, DOI 10.1109/TMM.2020.2997193
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhu PF, 2022, IEEE T PATTERN ANAL, V44, P7380, DOI 10.1109/TPAMI.2021.3119563
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
NR 56
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7812
EP 7822
DI 10.1109/TMM.2024.3371892
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000025
DA 2024-08-05
ER

PT J
AU Zeng, Y
   Mai, SJ
   Yan, WJ
   Hu, HF
AF Zeng, Ying
   Mai, Sijie
   Yan, Wenjun
   Hu, Haifeng
TI Multimodal Reaction: Information Modulation for Cross-Modal
   Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Catalysts; Impurities; Representation learning; Purification; Bit error
   rate; Noise measurement; Information filters; knowledge distillation;
   sequence learning; multimodal learning
ID SENTIMENT
AB In multimodal machine learning, proper handling of cross-modal information is essential for obtaining an ideal joint embedding. Despite the progress made by recent fusion strategies, we hold that before the fusion stage, the unimodal representation inevitably contains noise that may hinder the correct learning of cross-modal dynamics and affect multimodal fusion. It is worthwhile to investigate how the information is being utilized and how to make the full use of it. Rethinking the process of leveraging multiple modalities for the joint embedding, multimodal learning can be regarded as a chemical reaction process and two steps may benefit learning: 1) purification to filter impurity, and 2) catalyst to facilitate learning. In this paper, we propose a Multimodal Information Modulation (MIM) learning framework to modulate the contribution and utilization of the cross-modal information, which identifies and handles the 'impurity' and 'catalyst' in multimodal learning. Specifically, a Unimodal Purification Network (UPN) is proposed to identify and explicitly filter out the impurity within each modality before fusion, which reduces the possibility of learning incorrect cross-modal dynamics. Besides, based on the intuition that useful information has the potential in the guidance of model updating, it plays a role to facilitate learning, which is achieved by the design of the Knowledge Guidance Scheme (KGS) considering both the intra- and inter-modal scenarios. Different to a majority of works that emphasize the role of useful information in the fusion and inference stage, KGS considers its potential role in assisting the representation learning of weaker components. Besides, it fully considers the modality dominance problem and sample variations for optimization. In short, MIM manages to modulate the useless/useful information to minimize/emphasize their contribution. Experimental results verify the effectiveness of the proposed method.
C1 [Zeng, Ying; Mai, Sijie; Yan, Wenjun; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
EM zengy268@mail2.sysu.edu.cn; maisj@mail2.sysu.edu.cn;
   yanwj23@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China
FX No Statement Available
CR Akhtar MS, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P370
   Beyer L, 2022, PROC CVPR IEEE, P10915, DOI 10.1109/CVPR52688.2022.01065
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Chauhan DS, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5647
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Cho JW, 2021, IEEE COMPUT SOC CONF, P1592, DOI 10.1109/CVPRW53098.2021.00175
   Dai R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13033, DOI 10.1109/ICCV48922.2021.01281
   Dastidar S. G., 2021, P INT C COMP INT COM, P150
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Gao H, 2022, PROC CVPR IEEE, P9903, DOI 10.1109/CVPR52688.2022.00968
   George A, 2021, PROC CVPR IEEE, P7878, DOI 10.1109/CVPR46437.2021.00779
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2019, IEEE T MULTIMEDIA, V21, P2850, DOI 10.1109/TMM.2019.2911456
   Huang X, 2020, IEEE T CYBERNETICS, V50, P1047, DOI 10.1109/TCYB.2018.2879846
   Kampman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P606
   Kang Z, 2020, KNOWL-BASED SYST, V189, DOI 10.1016/j.knosys.2019.105102
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim Y., 2016, EMNLP, DOI 10.18653/v1/d16-1139
   Lee C, 2021, PR MACH LEARN RES, V130
   Li QC, 2021, INFORM FUSION, V65, P58, DOI 10.1016/j.inffus.2020.08.006
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Liang PP, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P150
   Liang PP, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1569
   Liu C, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P1001
   Liu YF, 2023, IEEE T PATTERN ANAL, V45, P3378, DOI 10.1109/TPAMI.2022.3185317
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   Louizos C., 2018, LEARNING SPARSE NEUR
   Lu JS, 2016, ADV NEUR IN, V29
   Mai Sijie, 2023, IEEE Transactions on Multimedia, P4121, DOI 10.1109/TMM.2022.3171679
   Mai SJ, 2023, IEEE T AFFECT COMPUT, V14, P2276, DOI 10.1109/TAFFC.2022.3172360
   Mai SJ, 2021, IEEE-ACM T AUDIO SPE, V29, P1424, DOI 10.1109/TASLP.2021.3068598
   Mai SJ, 2020, AAAI CONF ARTIF INTE, V34, P164
   Mai SJ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P481
   Mnih A, 2014, PR MACH LEARN RES, V32, P1791
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   OLSON DR, 1977, HARVARD EDUC REV, V47, P257, DOI 10.17763/haer.47.3.8840364413869005
   Pan YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3487042
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2017, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2017.134
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.0055, 10.1109/ICDM.2016.178]
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Rozgic V, 2012, ASIAPAC SIGN INFO PR
   Saporta A, 2022, IEEE COMPUT SOC CONF, P3750, DOI 10.1109/CVPRW56347.2022.00419
   Schlichtkrull M. S., 2020, P INT C LEARN REPR
   Shenoy A, 2020, PROCEEDINGS OF THE SECOND GRAND CHALLENGE AND WORKSHOP ON MULTIMODAL LANGUAGE (CHALLENGE-HML), VOL 1, P19
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tang JX, 2021, Arxiv, DOI arXiv:2002.03532
   Thoker FM, 2019, IEEE IMAGE PROC, P6, DOI [10.1109/icip.2019.8802909, 10.1109/ICIP.2019.8802909]
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   [王锦荟 Wang Jinhui], 2022, [中国科学. 技术科学, Scientia Sinica Technologica], V52, P713
   Wang T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P12, DOI 10.1145/3343031.3350875
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wöllmer M, 2013, IEEE INTELL SYST, V28, P46, DOI 10.1109/MIS.2013.34
   Wu CH, 2011, IEEE T AFFECT COMPUT, V2, P10, DOI 10.1109/T-AFFC.2010.16
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Yang ZL, 2019, ADV NEUR IN, V32
   Yoon Jeongbeen, 2022, P IEEECVF WINTER C A, P1978
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zeng Y, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P1262
   Zhang D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P148, DOI 10.1145/3343031.3350987
   Zhang D, 2019, IEEE INT CON MULTI, P730, DOI 10.1109/ICME.2019.00131
   Zhu T, 2023, IEEE T MULTIMEDIA, V25, P3375, DOI 10.1109/TMM.2022.3160060
   Zhu T, 2023, IEEE T MULTIMEDIA, V25, P6868, DOI 10.1109/TMM.2022.3214989
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 77
TC 1
Z9 1
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2178
EP 2191
DI 10.1109/TMM.2023.3293335
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100035
DA 2024-08-05
ER

PT J
AU Zhang, YL
   Liu, YM
   Hu, RT
   Wu, Q
   Zhang, J
AF Zhang, Yongle
   Liu, Yimin
   Hu, Ruotong
   Wu, Qiang
   Zhang, Jian
TI Mutual Dual-Task Generator With Adaptive Attention Fusion for Image
   Inpainting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image inpainting; semantic segmentation guidance; attention fusion
AB Image segmentation can reveal the semantic structure information in an image, which is helpful guidance information for image inpainting. Notably, it can help mitigate the artifacts on the boundaries of different semantic regions during the inpainting process. Existing semantic guidance-based image inpainting provides one-way guidance from the semantic segmentation task to the image inpainting task. There is no feedback from the inpainting results to adjust the guidance process, which causes inferior performance. To tackle this issue, this work proposes mutual dual-task generators to establish the interaction between image segmentation and image inpainting tasks. Thus, semantic segmentation guides image inpainting and also receives feedback from image inpainting. These two processes interact with each other and progressively improve the inpainting quality. The mutual dual-task generator consists of a shared encoder and mutual decoders with the bidirectional Cross-domain Feature DeNormalization (CFDN) module inside, which hierarchically models the Segmentation-guided image Texture (ST) generation and Texture-guided semantic Segmentation (TS) generation. At the end of mutual decoders, an Adaptive Attention Fusion (AAF) module is proposed to augment the texture and semantic class affinity between pixels, further refining the inpainted results. Experimental results demonstrate that the proposed mutual dual-task generator pipeline achieves superior inpainting performances over the state of the arts on three public datasets.
C1 [Zhang, Yongle; Wu, Qiang; Zhang, Jian] Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.
   [Liu, Yimin] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
   [Hu, Ruotong] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
C3 University of Technology Sydney; Hefei University of Technology; Ocean
   University of China
RP Wu, Q (corresponding author), Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.
EM yongle.zhang@student.uts.edu.au; yiminliu@mail.hfut.edu.cn;
   hrt@stu.ouc.edu.cn; qiang.wu@uts.edu.au; jian.zhang@uts.edu.au
RI Hu, Ruotong/KCL-4499-2024
OI Hu, Ruotong/0009-0008-4459-2155; Wu, Qiang/0000-0001-5641-2483; Zhang,
   Jian/0000-0002-7240-3541; Liu, Yimin/0000-0002-5543-4602; Zhang,
   Yongle/0000-0002-9236-3479
FU China Scholarship Council
FX No Statement Available
CR Ardino P, 2021, INT C PATT RECOG, P9280, DOI 10.1109/ICPR48806.2021.9412690
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo XF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14114, DOI 10.1109/ICCV48922.2021.01387
   Hensel M, 2017, ADV NEUR IN, V30
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Karras T., 2018, INT C LEARNING REPRE
   Li JY, 2019, IEEE I CONF COMP VIS, P5961, DOI 10.1109/ICCV.2019.00606
   Liang Liao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P683, DOI 10.1007/978-3-030-58583-9_41
   Liao L, 2021, PROC CVPR IEEE, P6535, DOI 10.1109/CVPR46437.2021.00647
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Quan WZ, 2022, IEEE T IMAGE PROCESS, V31, P2405, DOI 10.1109/TIP.2022.3152624
   Song Y., 2018, P BRIT MACH VIS C, P971
   Sun HY, 2023, IEEE T MULTIMEDIA, V25, P4240, DOI 10.1109/TMM.2022.3174413
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang N, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3748
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13729, DOI 10.1109/ICCV48922.2021.01349
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HW, 2022, IEEE T MULTIMEDIA, V24, P4016, DOI 10.1109/TMM.2021.3111491
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zeng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14144, DOI 10.1109/ICCV48922.2021.01390
   Zhang R., IEEE Trans. Multimedia, DOI [10.1109/TMM2022.3219728, DOI 10.1109/TMM2022.3219728]
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2020, NEUROCOMPUTING, V396, P1, DOI 10.1016/j.neucom.2020.01.068
NR 40
TC 2
Z9 2
U1 22
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1539
EP 1550
DI 10.1109/TMM.2023.3282892
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700038
DA 2024-08-05
ER

PT J
AU Zhou, CY
   Wang, ZM
   Zhang, XP
   Du, B
AF Zhou, Chaoyang
   Wang, Zengmao
   Zhang, Xiaoping
   Du, Bo
TI Domain Complementary Adaptation by Leveraging Diversity and
   Discriminability From Multiple Sources
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge engineering; Feature extraction; Prototypes; Loss measurement;
   Faces; Adaptation models; Task analysis; Unsupervised domain adaptation;
   diversity; contrastive learning; complementary learning
AB Due to the lack of labeled data in many real-world applications, unsupervised domain adaptation has attracted a great deal of attention in the machine learning community through its use of labeled data from source domains. However, how to make full use of the discriminative information from different sources remains a challenge due to various domain gaps. In this article, we propose a domain complementary adaptation method by leveraging the diversity between sources and the discriminability of each source with contrastive learning. In the proposed method, we adopt several branch networks, denoted as domain branch networks, to learn different views of discriminative domain-invariant features from each source. Moreover, an ensemble classification network trained with domain-invariant features from all domain branch networks is adopted to guide the domain branch networks in providing diverse knowledge. We design a domain mutual contrastive loss by forcing the domain branch networks to be different from one another and be consistent with the ensemble classification network to learn diverse domain-invariant features. To further improve the discriminability of domain branch networks, a domain structure-oriented contrastive loss is proposed to learn the discriminative intrinsic neighborhood structure across each source and target domain. Extensive experiments on the Office-31, Office-Home and DomainNet datasets show that the proposed method outperforms state-of-the-art methods.
C1 [Zhou, Chaoyang] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Wang, Zengmao; Du, Bo] Wuhan Univ, Artificial Intelligence Inst, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430079, Peoples R China.
   [Wang, Zengmao; Du, Bo] Hubei Luo Jia Lab, Wuhan 430079, Peoples R China.
   [Zhang, Xiaoping] Wuhan Univ, Sch Civil & Architectural Engn, Wuhan 430079, Peoples R China.
C3 Wuhan University; Wuhan University; Wuhan University
RP Wang, ZM (corresponding author), Wuhan Univ, Artificial Intelligence Inst, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430079, Peoples R China.; Wang, ZM (corresponding author), Hubei Luo Jia Lab, Wuhan 430079, Peoples R China.
EM zhoucy@whu.edu.cn; wangzengmao@whu.edu.cn; jxhkzhang@163.com;
   dubo@whu.edu.cn
OI wang, zengmao/0000-0002-9326-0316
FU National Natural Science Foundation of China
FX No Statement Available
CR Cai Tianle, 2021, INT C MACH LEARN, P1170
   Chen T, 2020, PR MACH LEARN RES, V119
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Ding FF, 2024, IEEE T MULTIMEDIA, V26, P1179, DOI 10.1109/TMM.2023.3277275
   Feng ZY, 2019, IEEE I CONF COMP VIS, P3244, DOI 10.1109/ICCV.2019.00334
   Fengchun Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12553, DOI 10.1109/CVPR42600.2020.01257
   Fu Y, 2021, P CVPR, P16654
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JX, 2022, PROC CVPR IEEE, P1193, DOI 10.1109/CVPR52688.2022.00127
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kong Lingjing, 2022, INT C MACHINE LEARNI, V162, P11455
   Li Y., 2020, Adv. Neural Inf. Process. Syst., V33, P789
   Li YS, 2021, PROC CVPR IEEE, P10993, DOI 10.1109/CVPR46437.2021.01085
   Lin HB, 2023, IEEE T MULTIMEDIA, V25, P8767, DOI 10.1109/TMM.2023.3241539
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Mengxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13933, DOI 10.1109/CVPR42600.2020.01395
   Park G. Y., 2021, P IEEE CVF INT C COM, P9214
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Qu SQ, 2023, PROC CVPR IEEE, P20019, DOI 10.1109/CVPR52729.2023.01917
   Ren CX, 2022, IEEE T IMAGE PROCESS, V31, P2122, DOI 10.1109/TIP.2022.3152052
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sarfraz MS, 2019, PROC CVPR IEEE, P8926, DOI 10.1109/CVPR.2019.00914
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shen J, 2018, AAAI CONF ARTIF INTE, P4058
   Singh A, 2021, Advances in Neural Information Processing Systems, V34
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun H, 2023, NEURAL NETWORKS, V163, P10, DOI 10.1016/j.neunet.2023.03.017
   Sun SL, 2015, INFORM FUSION, V24, P84, DOI 10.1016/j.inffus.2014.12.003
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wan J, 2023, IEEE T IMAGE PROCESS, V32, P1966, DOI 10.1109/TIP.2023.3261749
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wang Z., 2022, PROC 31 INT JOINT C, P3551
   Wang ZM, 2020, IEEE T NEUR NET LEAR, V31, P2387, DOI 10.1109/TNNLS.2019.2935608
   Wei Colin, 2019, ARXIV191004284
   Wilson G, 2023, IEEE T PATTERN ANAL, V45, P14208, DOI 10.1109/TPAMI.2023.3298346
   Wu XF, 2023, IEEE T NEUR NET LEAR, V34, P2896, DOI 10.1109/TNNLS.2021.3110109
   Xu MH, 2024, IEEE T PATTERN ANAL, V46, P1727, DOI 10.1109/TPAMI.2022.3172372
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yang SQ, 2021, ADV NEUR IN, V34
   Yang Shiqi, 2022, Advances in Neural Information Processing Systems (NeurIPS), V35, P5802
   Yeh CH, 2022, LECT NOTES COMPUT SC, V13686, P668, DOI 10.1007/978-3-031-19809-0_38
   Zhao SC, 2020, AAAI CONF ARTIF INTE, V34, P12975
   Zhou J, 2023, COMPANION OF THE WORLD WIDE WEB CONFERENCE, WWW 2023, P523, DOI 10.1145/3543873.3584659
   Zhu YC, 2019, AAAI CONF ARTIF INTE, P5989
   Zhuang CX, 2019, IEEE I CONF COMP VIS, P6001, DOI 10.1109/ICCV.2019.00610
NR 53
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4490
EP 4501
DI 10.1109/TMM.2023.3323868
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100037
DA 2024-08-05
ER

PT J
AU Cao, B
   Cao, HF
   Liu, JX
   Zhu, PF
   Zhang, CQ
   Hu, QH
AF Cao, Bing
   Cao, Haifang
   Liu, Jiaxu
   Zhu, Pengfei
   Zhang, Changqing
   Hu, Qinghua
TI Autoencoder-Based Collaborative Attention GAN for Multi-Modal Image
   Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image synthesis; Collaboration; Task analysis; Generative adversarial
   networks; Feature extraction; Data models; Image reconstruction;
   Multi-modal image synthesis; collaborative attention; single-modal
   attention; multi-modal attention
ID TRANSLATION; NETWORK
AB Multi-modal images are required in a wide range of practical scenarios, from clinical diagnosis to public security. However, certain modalities may be incomplete or unavailable because of the restricted imaging conditions, which commonly leads to decision bias in many real-world applications. Despite the significant advancement of existing image synthesis techniques, learning complementary information from multi-modal inputs remains challenging. To address this problem, we propose an autoencoder-based collaborative attention generative adversarial network (ACA-GAN) that uses available multi-modal images to generate the missing ones. The collaborative attention mechanism deploys a single-modal attention module and a multi-modal attention module to effectively extract complementary information from multiple available modalities. Considering the significant modal gap, we further developed an autoencoder network to extract the self-representation of target modality, guiding the generative model to fuse target-specific information from multiple modalities. This considerably improves cross-modal consistency with the desired modality, thereby greatly enhancing the image synthesis performance. Quantitative and qualitative comparisons for various multi-modal image synthesis tasks highlight the superiority of our approach over several prior methods by demonstrating more precise and realistic results.
C1 [Cao, Bing; Cao, Haifang; Liu, Jiaxu; Zhu, Pengfei; Zhang, Changqing; Hu, Qinghua] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300403, Peoples R China.
   [Cao, Bing] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710000, Peoples R China.
   [Cao, Haifang; Liu, Jiaxu; Zhu, Pengfei; Zhang, Changqing; Hu, Qinghua] Tianjin Univ, Haihe Lab Informat echnol Applicat Innovat, Tianjin 300403, Peoples R China.
C3 Tianjin University; Xidian University; Tianjin University
RP Zhu, PF (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300403, Peoples R China.
EM caobing@tju.edu.cn; caohaifang@tju.edu.cn; realliujiaxu@tju.edu.cn;
   zhupengfei@tju.edu.cn; zhangchangqing@tju.edu.cn; huqinghua@tju.edu.cn
RI zhang, chao/IXD-9965-2023
OI Hu, Qinghua/0000-0001-7765-8095
FU National Key Ramp;D Program of China
FX No Statement Available
CR [Anonymous], 2020, ARXIV
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bakas S, 2019, Arxiv, DOI arXiv:1811.02629
   Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Bi ZW, 2022, IEEE T IMAGE PROCESS, V31, P6664, DOI 10.1109/TIP.2022.3214336
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Burgos N, 2014, IEEE T MED IMAGING, V33, P2332, DOI 10.1109/TMI.2014.2340135
   Calhoun VD, 2009, IEEE T INF TECHNOL B, V13, P711, DOI 10.1109/TITB.2008.923773
   Cao B., 2023, IEEE Trans. Neural Netw. Learn. Syst.
   Cao B, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108446
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Cheng ZX, 2020, IEEE T MULTIMEDIA, V22, P860, DOI 10.1109/TMM.2019.2938345
   Cheng ZQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3272, DOI 10.1145/3503161.3547943
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fang B, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020159
   Fukui H, 2019, PROC CVPR IEEE, P10697, DOI 10.1109/CVPR.2019.01096
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Hensel M, 2017, ADV NEUR IN, V30
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang SY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P623
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jog A, 2017, MED IMAGE ANAL, V35, P475, DOI 10.1016/j.media.2016.08.009
   Kermi A, 2019, LECT NOTES COMPUT SC, V11384, P37, DOI 10.1007/978-3-030-11726-9_4
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kingma D.P., 2014, Proc. of ICLR
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Lee D, 2019, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2019.00259
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liang XC, 2019, IEEE SENS J, V19, P7107, DOI 10.1109/JSEN.2019.2913281
   Liming Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P206, DOI 10.1007/978-3-030-58580-8_13
   Liu ZS, 2021, IEEE T CIRC SYST VID, V31, P1351, DOI 10.1109/TCSVT.2020.3003832
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Ma S, 2018, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2018.00593
   Makhzani A., 2016, P 4 INT C LEARN REPR
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Mejjati YA, 2018, ADV NEUR IN, V31
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Mnih V, 2014, ADV NEUR IN, V27
   Nair V., 2010, ICML, P807
   Ng A., 2011, CS294A Lecture notes, V2011, P1
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Salimans T, 2016, ADV NEUR IN, V29
   Song H, 2020, IEEE T MULTIMEDIA, V22, P2138, DOI 10.1109/TMM.2019.2950530
   Sun YP, 2016, IEEE T MULTIMEDIA, V18, P171, DOI 10.1109/TMM.2015.2496246
   Tanenbaum LN, 2017, AM J NEURORADIOL, V38, P1103, DOI 10.3174/ajnr.A5227
   Tang H, 2023, IEEE T NEUR NET LEAR, V34, P1972, DOI 10.1109/TNNLS.2021.3105725
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2015, IEEE I CONF COMP VIS, P630, DOI 10.1109/ICCV.2015.79
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang YK, 2022, PROC CVPR IEEE, P12176, DOI 10.1109/CVPR52688.2022.01187
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu K., 2015, PROC INT C MACH LEAR, P2048
   Xu WJ, 2019, IEEE T MULTIMEDIA, V21, P2387, DOI 10.1109/TMM.2019.2898777
   Yang X, 2020, IEEE J BIOMED HEALTH, V24, P855, DOI 10.1109/JBHI.2019.2922986
   Yang X, 2019, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2019.00419
   Zhang DL, 2020, IEEE T CYBERNETICS, V50, P3033, DOI 10.1109/TCYB.2019.2905157
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang LL, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P627, DOI 10.1145/2671188.2749321
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P383, DOI 10.1145/3240508.3240536
   Zhou T, 2020, IEEE T MED IMAGING, V39, P2772, DOI 10.1109/TMI.2020.2975344
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 76
TC 3
Z9 3
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 995
EP 1010
DI 10.1109/TMM.2023.3274990
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700007
DA 2024-08-05
ER

PT J
AU Chen, ZH
   Yang, CHY
   Chang, JH
   Zhao, F
   Zha, ZJ
   Wu, F
AF Chen, Zehui
   Yang, Chenhongyi
   Chang, Jiahao
   Zhao, Feng
   Zha, Zheng-Jun
   Wu, Feng
TI DDOD: Dive Deeper into the Disentanglement of Object Detector
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Disentanglement; feature representation learning; imbalance learning;
   object detection
AB Compared to many other dense prediction tasks, object detection plays a fundamental role in visual perception and scene understanding. Dense object detection, aiming at localizing objects directly from the feature map, has drawn great attention due to its low cost and high efficiency. Though it has been developed for a long time, the training pipeline of dense object detectors is still compromised to lots of conjunctions. In this paper, we demonstrate the existence of three conjunctions lying in the current paradigm of one-stage detectors: 1) only samples assigned as positive in classification head are used to train the regression head; 2) classification and regression share the same input feature and computational fields defined by the parallel head architecture; and 3) samples distributed in different feature pyramid layers are treated equally when computing the loss. Based on this, we propose Disentangled Dense Object Detector (DDOD), a simple, direct, and efficient framework for 2D detection with strong performance. We derive two DDOD variants (i.e., DR-CNN, and DDETR) following the basic one-stage/two-stage and recently developed transformer-based pipelines. Specifically, we develop three effective disentanglement mechanisms and integrate them into the current state-of-the-art object detectors. Extensive experiments on MS COCO benchmark show that our approach obtains significant enhancements with negligible extra overhead on various detectors. Notably, our best model reaches 55.4 mAP on the COCO test-dev set, achieving new state-of-the-art performance on this competitive benchmark. Additionally, we validate our model on several challenging tasks including small object detection and crowded object detection. The experimental results further prove the superiority of disentanglement on these conjunctions.
C1 [Chen, Zehui; Chang, Jiahao; Zhao, Feng; Zha, Zheng-Jun; Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
   [Yang, Chenhongyi] Univ Edinburgh, Sch Engn, Edinburgh EH8 9YL, Scotland.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Edinburgh
RP Zhao, F (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
EM lovesnow@mail.ustc.edu.cn; chenhongyi.yang@ed.ac.uk;
   changjh@mail.ustc.edu.cn; fzhao956@ustc.edu.cn; zhazj@ustc.edu.cn;
   fengwu@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020; Wu, Feng/KCY-3017-2024; Chen,
   Zehui/HDN-3605-2022; Zhao, Feng/C-8367-2009
OI Zhao, Feng/0000-0001-6767-8105
FU JKW Research Funds
FX No Statement Available
CR Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen Yu., 2020, P ADV NEUR INF PROC
   Chen ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4939, DOI 10.1145/3474085.3475351
   Chen ZH, 2022, Arxiv, DOI arXiv:2211.09386
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai XY, 2021, PROC CVPR IEEE, P7369, DOI 10.1109/CVPR46437.2021.00729
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Deng JK, 2019, Arxiv, DOI [arXiv:1905.00641, DOI 10.48550/ARXIV.1905.00641]
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Gao P., 2021, P IEEE CVF C COMP VI, P3621
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guanglu Song, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11560, DOI 10.1109/CVPR42600.2020.01158
   Han Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P549, DOI 10.1007/978-3-030-58452-8_32
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hengduo Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10585, DOI 10.1109/CVPR42600.2020.01060
   Huang LC, 2015, Arxiv, DOI arXiv:1509.04874
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Jiaqi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P403, DOI 10.1007/978-3-030-58548-8_24
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li F, 2022, PROC CVPR IEEE, P13609, DOI 10.1109/CVPR52688.2022.01325
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Liu S., 2022, arXiv
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Meng DP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3631, DOI 10.1109/ICCV48922.2021.00363
   Najibi M, 2019, IEEE I CONF COMP VIS, P9744, DOI 10.1109/ICCV.2019.00984
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Redmon J., 2018, CoRR
   Ren S., 2015, P INT C ADV NEUR INF, P1
   Shao S, 2018, Arxiv, DOI arXiv:1805.00123
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Sun Peize, 2020, ABS201215460 CORR
   Sun Peize, 2021, PROC INT C MACH LEAR, P9934
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang C.Y., 2020, Scaled-YOLOv4: Scaling Cross Stage Partial Network, DOI [DOI 10.48550/ARXIV.2011.08036, DOI 10.48550/ARXIV.2011.08036.ARXIV]
   Wang JF, 2021, PROC CVPR IEEE, P15844, DOI 10.1109/CVPR46437.2021.01559
   Wang W., 2020, P 28 INT C COMP LING, P6019
   Wang YM, 2022, AAAI CONF ARTIF INTE, P2567
   Wu S, 2022, IEEE T MULTIMEDIA, V24, P2058, DOI 10.1109/TMM.2021.3075323
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu YJ, 2021, IEEE T IMAGE PROCESS, V30, P5782, DOI 10.1109/TIP.2021.3085208
   Yang C., 2021, arXiv
   Yang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13043, DOI 10.1109/CVPR42600.2020.01306
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yao ZY, 2021, Arxiv, DOI arXiv:2104.01318
   Ye SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6423, DOI 10.1109/ICCV48922.2021.00638
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zha ZJ, 2022, IEEE T PATTERN ANAL, V44, P710, DOI 10.1109/TPAMI.2019.2909864
   Zhang B, 2020, Arxiv, DOI arXiv:2003.11228
   Zhang F, 2019, Arxiv, DOI arXiv:1905.01585
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang SF, 2021, IEEE T PATTERN ANAL, V43, P4008, DOI 10.1109/TPAMI.2020.2997456
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhong YW, 2022, PROC CVPR IEEE, P16772, DOI 10.1109/CVPR52688.2022.01629
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11212, P122, DOI 10.1007/978-3-030-01237-3_8
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Zhu Y, 2020, arXiv
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 75
TC 6
Z9 6
U1 20
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 284
EP 298
DI 10.1109/TMM.2023.3264008
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500010
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Du, ZX
   He, D
   Wang, X
   Wang, Q
AF Du, Zexing
   He, Di
   Wang, Xue
   Wang, Qing
TI Learning Semantics-Guided Representations for Scoring Figure Skating
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Figure skating videos; sports video analysis; multi-modality
   representation learning; teacher-student network; action quality
   assessment
AB This paper explores semantic-aware representations for scoring figure skating videos. Most existing approaches to sports video analysis only focus on reasoning action scores based on visual input, limiting their ability to depict high-level semantic representations. Here, we propose a teacher-student-based network with an attention mechanism to realize an adaptive knowledge transfer from the semantic domain to the visual domain, which is termed semantics-guided network (SGN). Specifically, we use a set of learnable atomic queries in the student branch to mimic the semantic-aware distribution in the teacher branch, which is represented by the visual and semantic inputs. In addition, we propose three auxiliary losses to align features in different domains. With aligned feature representations, the adapted teacher is capable of transferring the semantic knowledge to the student. To verify the effectiveness of our method, we collect a new dataset OlympicFS for scoring figure skating. Besides action scores, OlympicFS also provides professional comments on actions for learning semantic representations. By evaluating four challenging datasets, our method achieves state-of-the-art performance.
C1 [Du, Zexing; He, Di; Wang, Xue; Wang, Qing] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
C3 Northwestern Polytechnical University
RP Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
EM duzexing@mail.nwpu.edu.cn; hedy@mail.nwpu.edu.cn; xwang@nwpu.edu.cn;
   qwang@nwpu.edu.cn
OI Wang, Xue/0009-0003-5224-906X; Wang, Qing/0000-0003-3439-0644
FU National Natural Science Foundation of China
FX No Statement Available
CR Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bai Y, 2022, LECT NOTES COMPUT SC, V13664, P422, DOI 10.1007/978-3-031-19772-7_25
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carion N., 2020, EUR C COMP VIS, P213
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen T, 2020, PR MACH LEARN RES, V119
   Clarke A, 2015, TRENDS COGN SCI, V19, P677, DOI 10.1016/j.tics.2015.08.008
   Deliege A, 2021, IEEE COMPUT SOC CONF, P4503, DOI 10.1109/CVPRW53098.2021.00508
   Devereux BJ, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-28865-1
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2020, PROC INT C LEARN RE, P1, DOI DOI 10.48550/ARXIV
   Dou ZY, 2022, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52688.2022.01763
   Doughty H, 2019, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2019.00805
   Doughty H, 2018, PROC CVPR IEEE, P6057, DOI 10.1109/CVPR.2018.00634
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1408, DOI 10.1109/ICCV48922.2021.00146
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao JB, 2023, INT J COMPUT VISION, V131, P659, DOI 10.1007/s11263-022-01695-5
   Giancola S, 2018, IEEE COMPUT SOC CONF, P1792, DOI 10.1109/CVPRW.2018.00223
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lee Sangho, 2020, INT C LEARN REPR
   Li M, 2022, LECT NOTES COMPUT SC, V13664, P457, DOI 10.1007/978-3-031-19772-7_27
   Liu DC, 2021, PROC CVPR IEEE, P9517, DOI 10.1109/CVPR46437.2021.00940
   Liu SL, 2020, NEUROCOMPUTING, V413, P360, DOI 10.1016/j.neucom.2020.06.108
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mal ZY, 2022, PROC CVPR IEEE, P14054, DOI 10.1109/CVPR52688.2022.01368
   Merler M, 2019, IEEE T MULTIMEDIA, V21, P1147, DOI 10.1109/TMM.2018.2876046
   Nakano T, 2020, Arxiv, DOI arXiv:2007.01089
   Nekoui M, 2021, IEEE WINT CONF APPL, P394, DOI 10.1109/WACV48630.2021.00044
   Pan JH, 2019, IEEE I CONF COMP VIS, P6340, DOI 10.1109/ICCV.2019.00643
   Parmar P, 2019, PROC CVPR IEEE, P304, DOI 10.1109/CVPR.2019.00039
   Parmar P, 2019, IEEE WINT CONF APPL, P1468, DOI 10.1109/WACV.2019.00161
   Parmar P, 2017, IEEE COMPUT SOC CONF, P76, DOI 10.1109/CVPRW.2017.16
   Pirsiavash H, 2014, LECT NOTES COMPUT SC, V8694, P556, DOI 10.1007/978-3-319-10599-4_36
   Radford A, 2021, PR MACH LEARN RES, V139
   Rui Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P208, DOI 10.1007/978-3-030-58598-3_13
   Tejero-de-Pablos A, 2018, IEEE T MULTIMEDIA, V20, P2000, DOI 10.1109/TMM.2018.2794265
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang SL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4902, DOI 10.1145/3474085.3475438
   Wu F, 2023, IEEE T MULTIMEDIA, V25, P7943, DOI 10.1109/TMM.2022.3232034
   Wu M., 2023, P AAAI C ARTIFICIAL, V37, P2839, DOI 10.1609/aaai.v37i3.25385
   Xia J., 2023, Proc. AAAI Conf. Artif. Intell., V37, P2901
   Xu AC, 2022, PROC CVPR IEEE, P3222, DOI 10.1109/CVPR52688.2022.00323
   Xu CM, 2020, IEEE T CIRC SYST VID, V30, P4578, DOI 10.1109/TCSVT.2019.2927118
   Xu Jinglin, 2022, P IEEECVF C COMPUTER, P2949
   Yang A, 2023, PROC CVPR IEEE, P10714, DOI 10.1109/CVPR52729.2023.01032
   Yansong Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9836, DOI 10.1109/CVPR42600.2020.00986
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7899, DOI 10.1109/ICCV48922.2021.00782
   Yuan HJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7456, DOI 10.1109/ICCV48922.2021.00738
   Zeng LA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2526, DOI 10.1145/3394171.3413560
NR 52
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4987
EP 4997
DI 10.1109/TMM.2023.3328180
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800010
DA 2024-08-05
ER

PT J
AU Hu, HZ
   Pu, JF
   Zhou, WG
   Fang, H
   Li, HQ
AF Hu, Hezhen
   Pu, Junfu
   Zhou, Wengang
   Fang, Hang
   Li, Houqiang
TI Prior-Aware Cross Modality Augmentation Learning for Continuous Sign
   Language Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross modality augmentation learning; editing with prior incorporated;
   continuous sign language recognition
ID FRAMEWORK
AB Continuous sign language recognition (CSLR) aims to map a sign video into a sentence of text words in the same order as the signs. Generally, word error rate (WER), i.e., editing distance, is adopted as the main evaluation metric. Since this metric is not differentiable, current deep-learning-based CSLR methods usually resort to connectionist temporal classification (CTC) loss during optimization, which maximizes the posterior probability over the sequential alignment. Due to the optimization gap between CTC loss and WER, the decoded sequence with the maximum probability in CTC may not be the one with the lowest WER. To tackle this issue, we propose a novel prior-aware cross modality augmentation learning method. In our approach, we first generate the pseudo video-text pair by cross modality editing, i.e., substitution, deletion and insertion on the paired real video-text data. To ensure the pseudo data quality, we guide the editing with both textual grammar prior and visual pose transition consistency prior. In this way, the generated pseudo video and text sentence follow the underlying distribution of the sign language data, and sever as more genuine hard examples for the cross modality representation learning of our CSLR task. Based on the real and generated pseudo data, we optimize our CSLR framework with three loss terms. We evaluate our approach on popular large-scale CSLR datasets and extensive experiments demonstrate the effectiveness of our method.
C1 [Hu, Hezhen; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
   [Pu, Junfu] TencentARCLab, Shenzhen, Peoples R China.
   [Fang, Hang] Anhui Univ, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Anhui University
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
EM alexhu@mail.ustc.edu.cn; pjh@mail.ustc.edu.cn; zhwg@ustc.edu.cn;
   fanghang2021@163.com; lihq@ustc.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Adaloglou N, 2022, IEEE T MULTIMEDIA, V24, P1750, DOI 10.1109/TMM.2021.3070438
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Buehler P, 2009, PROC CVPR IEEE, P2953, DOI 10.1109/CVPRW.2009.5206523
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang CY, 2019, PROC CVPR IEEE, P3541, DOI 10.1109/CVPR.2019.00366
   Chen X, 2014, INT C PATT RECOG, P411, DOI 10.1109/ICPR.2014.79
   Cho K., 2014, EMNLP, DOI 10.3115/v1/w14-4012
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Cuturi M, 2017, PR MACH LEARN RES, V70
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Feng SY, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P968
   Goyal K., 2022, P INT C LEARN REPR, P1
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Graves A, 2006, ICML, P369, DOI DOI 10.1145/1143844.1143891
   Guo D, 2020, IEEE T IMAGE PROCESS, V29, P1575, DOI 10.1109/TIP.2019.2941267
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hu H, 2021, ACM T MULTIM COMPUT, P1
   Hu HZ, 2023, IEEE T MULTIMEDIA, V25, P7559, DOI 10.1109/TMM.2022.3223260
   Hu HZ, 2021, PROC CVPR IEEE, P16423, DOI 10.1109/CVPR46437.2021.01616
   Hu HZ, 2021, AAAI CONF ARTIF INTE, V35, P1558
   Hu Hezhen, 2021, ICCV, P11087
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Ka Leong Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P697, DOI 10.1007/978-3-030-58586-0_41
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2016, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR.2016.412
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Koller Oscar, 2016, BMVC, DOI DOI 10.5244/C.30.136
   Lampiris G, 2020, ANN OPER RES, V294, P225, DOI 10.1007/s10479-019-03337-5
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Liu Landong, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13808), P256, DOI 10.1007/978-3-031-25085-9_15
   M. Contributors, 2020, OpenMMLab pose estimation toolbox and benchmark
   Melamud O., 2016, Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, P51
   Min YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11522, DOI 10.1109/ICCV48922.2021.01134
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pfister T., 2013, P BRIT MACH VIS C, P1
   Pu JF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1497, DOI 10.1145/3394171.3413931
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Qiu ZF, 2019, PROC CVPR IEEE, P12048, DOI 10.1109/CVPR.2019.01233
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Radford, 2018, OPENAI BLOG
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811
   Szegedy C., 2015, P IEEE CVFINT C COMP, P1
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang Alex, 2019, Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, DOI [DOI 10.18653/V1/W19-2304, 10.18653/v1/W19-2304]
   Wang HJ, 2019, IEEE T MULTIMEDIA, V21, P2806, DOI 10.1109/TMM.2019.2915032
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Xie P, 2022, IEEE T MULTIMEDIA, V24, P3908, DOI 10.1109/TMM.2021.3109665
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang JH, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552950
   Zhao W., 2023, P AAAI C ART INT, P1
   Zhe Niu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P172, DOI 10.1007/978-3-030-58517-4_11
   Zhou H, 2021, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR46437.2021.00137
   Zhou H, 2022, IEEE T MULTIMEDIA, V24, P768, DOI 10.1109/TMM.2021.3059098
NR 68
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 593
EP 606
DI 10.1109/TMM.2023.3268368
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000025
DA 2024-08-05
ER

PT J
AU Li, B
   Lin, X
   Liu, B
   He, ZF
   Lai, YK
AF Li, Bo
   Lin, Xiao
   Liu, Bin
   He, Zhi-Fen
   Lai, Yu-Kun
TI Lightweight Text-Driven Image Editing With Disentangled Content and
   Attributes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Task analysis; Standards; Semantics; Image reconstruction;
   Feature extraction; Decoding; Interactive image editing; text-driven;
   disentanglement; cycle-consistency
AB Text-driven image editing aims to manipulate images with the guidance of natural language description. Text is much more natural and intuitive than many other interaction modes, and attracts more attention recently. However, compared with classical supervised learning tasks, there is no standard benchmark dataset for text-driven interactive image editing up to now. Therefore, it is hard to train an end-to-end model for pixel-aligned interactive image editing driven by text. Some methods follow the paradigm of text-to-image models by incorporating the target image into the process of text-to-image generation. However, these methods relying on cross-modal text-to-image generation involve complicated and expensive models, which can lead to inconsistent editing effects. In this article, a novel text-driven image editing method is proposed. Our key observation is that this task can be more efficiently learned using image-to-image translation. To ensure effective learning for image editing, our framework takes paired text and the corresponding images for training, and disentangles each image into content and attributes, such that the content is maintained while the attributes are modified according to the text. Our network is a lightweight encoder-decoder architecture that accomplishes pixel-aligned end-to-end training via cycle-consistent supervision. Quantitative and qualitative experimental results show that the proposed method achieves state-of-the-art performance.
C1 [Li, Bo; Lin, Xiao; Liu, Bin; He, Zhi-Fen] Nanchang Hangkong Univ, Sch Math & Informat Sci, Nanchang 330063, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales.
C3 Nanchang Hangkong University; Cardiff University
RP Liu, B (corresponding author), Nanchang Hangkong Univ, Sch Math & Informat Sci, Nanchang 330063, Peoples R China.
EM libonchu@outlook.com; laxyion7@gmail.com; nyliubin@nchu.edu.cn;
   zfhe323@163.com; laiy4@cardiff.ac.uk
RI Lai, Yu-Kun/D-2343-2010
OI Lin, Xiao/0009-0006-8716-2601; Lai, Yukun/0000-0002-2094-5680; Liu,
   Bin/0000-0002-5023-7167
FU National Natural Science Foundation of China
FX No Statement Available
CR Avrahami O, 2022, PROC CVPR IEEE, P18187, DOI 10.1109/CVPR52688.2022.01767
   Chandramouli K. V., 33RDBRIT MACH VIS C
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Crowson K, 2022, LECT NOTES COMPUT SC, V13697, P88, DOI 10.1007/978-3-031-19836-6_6
   Dash A, 2017, Arxiv, DOI arXiv:1703.06412
   Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383
   Dhamo H, 2020, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR42600.2020.00526
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Gao LL, 2023, IEEE T MULTIMEDIA, V25, P7248, DOI 10.1109/TMM.2022.3219677
   Hessel J, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P7514
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang WM, 2019, PR MACH LEARN RES, V101, P284
   Huang X, 2019, 2019 16TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2019), P73, DOI 10.1109/CRV.2019.00018
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Li B., 2020, 2020 IEEE CVF C COMP, P7877
   Li B., 2020, Proc. Adv. Neural Inf. Process. Syst, P22020
   Li BW, 2019, ADV NEUR IN, V32
   Liu YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1357, DOI 10.1145/3394171.3413505
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Nam S, 2018, ADV NEURAL INFORM PR, P42
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Olszewski Kyle, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7444, DOI 10.1109/CVPR42600.2020.00747
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Reed S, 2016, PR MACH LEARN RES, V48
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Salimans T, 2016, ADV NEUR IN, V29
   Shuai Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P601, DOI 10.1007/978-3-030-58555-6_36
   Song JK, 2022, IEEE T MULTIMEDIA, V24, P791, DOI 10.1109/TMM.2021.3059336
   Su ST, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1784, DOI 10.1145/3474085.3475326
   Tan HC, 2021, IEEE T IMAGE PROCESS, V30, P1275, DOI 10.1109/TIP.2020.3026728
   Tan HC, 2019, IEEE I CONF COMP VIS, P10500, DOI 10.1109/ICCV.2019.01060
   Wah S., 2011, TR 2010-001
   Wu FX, 2023, IEEE T MULTIMEDIA, V25, P6219, DOI 10.1109/TMM.2022.3207000
   Xihui Liu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P89, DOI 10.1007/978-3-030-58621-8_6
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhu DW, 2021, NEURAL NETWORKS, V136, P207, DOI 10.1016/j.neunet.2020.09.002
NR 50
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1829
EP 1841
DI 10.1109/TMM.2023.3289755
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800036
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Lu, HF
   Gou, SP
   Li, RM
AF Lu, Haofan
   Gou, Shuiping
   Li, Ruimin
TI SPMHand: Segmentation-Guided Progressive Multi-Path 3D Hand Pose and
   Shape Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Shape; Image segmentation; Solid modeling;
   Task analysis; Optimization; Transformers; Hand morphology attention;
   hand pose estimation; progressive hand regression; segmentation-guided
   deocclusion
AB Hand pose and shape estimation plays an important role in numerous applications. A cost-effective and practical-friendly approach is to perform accurate hand estimation from a single RGB image, but this task is challenging due to ubiquitous hand self-occlusion and hand-object interaction occlusions. In this paper, we propose a novel SPMHand network to alleviate the effect of occlusions, inspired by the process that humans infer the whole hand when the hand is occluded. The proposed SPMHand consists of two main modules to generate hand segmentations as guidance and conduct hand regressions in a progressive multi-path manner. The segmentation-guided deocclusion module enables the network to "see" the occluded hand by inferring the whole hand segmentation. Specifically, the visible hand segmentation is first obtained and then a hand morphology attention block is introduced to infer the whole hand segmentation by fusing the visible information with the learned hand features. The progressive multi-path regression module is designed to gradually regress the fine hand with intermediate supervisions. Features from deep to shallow are utilized for the hand regressions from coarse to decent. Subsequently, the structure feature, joint heatmaps and segmentations that provide guidance for deocclusion are embedded and fused for the final fine hand regression. Experiments on four challenging datasets illustrate that the proposed SPMHand outperforms the state-of-the-arts in both real-world and synthetic scenes, especially in the present of severe hand-object occlusions.
C1 [Lu, Haofan; Gou, Shuiping] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Xian 710071, Shaanxi, Peoples R China.
   [Li, Ruimin] Xidian Univ, Acad Adv Interdisciplinary Res, Xian 710071, Shaanxi, Peoples R China.
C3 Xidian University; Xidian University
RP Li, RM (corresponding author), Xidian Univ, Acad Adv Interdisciplinary Res, Xian 710071, Shaanxi, Peoples R China.
EM hflu@stu.xidian.edu.cn; shpgou@mail.xidian.edu.cn; rmli@xidian.edu.cn
OI LI, RUIMIN/0000-0003-2393-2225
FU National Natural Science Foundation of China
FX No Statement Available
CR Baek S, 2019, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2019.00116
   Cao Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12397, DOI 10.1109/ICCV48922.2021.01219
   Chao YW, 2021, PROC CVPR IEEE, P9040, DOI 10.1109/CVPR46437.2021.00893
   Chen X, 2022, P IEEECVF C COMPUTER, P20544
   Chen XY, 2021, PROC CVPR IEEE, P13269, DOI 10.1109/CVPR46437.2021.01307
   Chen YJ, 2021, IEEE T IMAGE PROCESS, V30, P4008, DOI 10.1109/TIP.2021.3068645
   Choi Hongsuk, 2020, COMPUTER VISION ECCV
   Gao DH, 2021, IEEE INT CONF COMP V, P2266, DOI 10.1109/ICCVW54120.2021.00256
   Grubert J, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P151, DOI 10.1109/VR.2018.8446250
   Gyeongsik Moon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P752, DOI 10.1007/978-3-030-58571-6_44
   Hampali S., 2021, arXiv
   Hampali S, 2022, PROC CVPR IEEE, P11080, DOI 10.1109/CVPR52688.2022.01081
   Hampali S, 2020, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR42600.2020.00326
   Handa A, 2020, IEEE INT CONF ROBOT, P9164, DOI [10.1109/ICRA40945.2020.9197124, 10.1109/icra40945.2020.9197124]
   Hasson Yana, 2019, CVPR, P11807
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu HY, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555421
   Hua GL, 2023, IEEE T MULTIMEDIA, V25, P1832, DOI 10.1109/TMM.2022.3171102
   Karunratanakul K, 2020, INT CONF 3D VISION, P333, DOI 10.1109/3DV50981.2020.00043
   Kingma D. P., 2014, arXiv
   Li B, 2023, NEUROCOMPUTING, V536, P59, DOI 10.1016/j.neucom.2023.02.046
   LI M, 2022, P IEEE CVF C COMP VI, P2761
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Lin K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12919, DOI 10.1109/ICCV48922.2021.01270
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu SW, 2021, PROC CVPR IEEE, P14682, DOI 10.1109/CVPR46437.2021.01445
   Lv W., 2021, BRIT MACH VIS C, P1
   Meng H, 2022, LECT NOTES COMPUT SC, V13666, P380, DOI 10.1007/978-3-031-20068-7_22
   Park J, 2022, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR52688.2022.00155
   Paszke A, 2019, ADV NEUR IN, V32
   Radkowski R., 2012, P 2012 INT C ADV COM, P303
   Ravi N, 2020, Arxiv, DOI arXiv:2007.08501
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seeber M, 2021, INT CONF 3D VISION, P22, DOI 10.1109/3DV53792.2021.00013
   Spurr A., 2020, EUR C COMP VIS, P211
   Tang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11678, DOI 10.1109/ICCV48922.2021.01149
   Tkach A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980226
   Tse THE, 2022, PROC CVPR IEEE, P1654, DOI 10.1109/CVPR52688.2022.00171
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Yang L, 2020, P BRIT MACH VIS C, P1
   Yang LX, 2022, PROC CVPR IEEE, P2740, DOI 10.1109/CVPR52688.2022.00277
   Yang LX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11077, DOI 10.1109/ICCV48922.2021.01091
   Zhang BW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11334, DOI 10.1109/ICCV48922.2021.01116
   Zhang H, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322998
   Zhang XY, 2022, IEEE T MULTIMEDIA, V24, P166, DOI 10.1109/TMM.2020.3047552
   Zhang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11261, DOI 10.1109/ICCV48922.2021.01109
   Zhang X, 2019, IEEE I CONF COMP VIS, P2354, DOI 10.1109/ICCV.2019.00244
   Zhou YX, 2020, PROC CVPR IEEE, P5345, DOI 10.1109/CVPR42600.2020.00539
NR 52
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6822
EP 6833
DI 10.1109/TMM.2024.3355652
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600011
DA 2024-08-05
ER

PT J
AU Lu, W
   Zhai, YJ
   Han, JZ
   Jing, PG
   Liu, Y
   Su, YT
AF Lu, Wei
   Zhai, Yujia
   Han, Jiaze
   Jing, Peiguang
   Liu, Yu
   Su, Yuting
TI VMemNet: A Deep Collaborative Spatial-Temporal Network With Attention
   Representation for Video Memorability Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video memorability; Attention mechanism; Spatial-temporal features
ID MEMORY; MODELS
AB Video memorability measures the degree to which a video is remembered by different viewers and has shown great potential in various contexts, including advertising, education, and health care. While extensive research has been conducted on image memorability, the study of video memorability is still in its early stages. Existing methods in this field primarily focus on coarse-grained spatial feature representation and decision fusion strategies, overlooking the crucial interactions between spatial and temporal domains. Therefore, we propose an end-to-end collaborative spatial-temporal network called VMemNet, which incorporates targeted attention mechanisms and intermediation fusion strategies. This enables VMemNet to capture the intricate relationships between spatial and temporal information and uncover more elements of memorability within video visual features. VMemNet integrates spatially and semantically guided attention modules into a dual-stream network architecture, allowing it to simultaneously capture static local cues and dynamic global cues in videos. Specifically, the spatial attention module is used to aggregate more memorable elements from spatial locations, and the semantically guided attention module is used to achieve semantic alignment and intermediate fusion of the local and global cues. In addition, two types of loss functions with complementary decision rules are associated with the corresponding attention modules to guide the training process of the proposed network. Experimental results obtained on a publicly available dataset verify that the proposed VMemNet approach outperforms all current single- and multi-modal methods in terms of video memorability prediction.
C1 [Lu, Wei; Zhai, Yujia; Han, Jiaze; Jing, Peiguang; Su, Yuting] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Jing, Peiguang] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
   [Liu, Yu] Tianjin Univ, Sch Microelect, Tianjin 300072, Peoples R China.
C3 Tianjin University; Guangxi Normal University; Tianjin University
RP Jing, PG (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM luwei@tju.edu.cn; zyj211@tju.edu.cn; 3015204341@tju.edu.cn;
   pgjing@tju.edu.cn; liuyu@tju.edu.cn; ytsu@tju.edu.cn
OI Liu, Yu/0000-0002-5949-6587; Jing, Peiguang/0000-0003-2648-7358
FU Research Fund of Guangxi Key Lab of Multi-source Information Mining amp;
   Security
FX No Statement Available
CR Agarla M., 2023, PROC MEDIAEVAL MULTI, V1
   [Anonymous], 2011, Advances in Neural Information Processing Systems (NIPS), DOI DOI 10.1167/12.9.1082
   Azcona D., 2020, PROC CEUR WORKSHOP
   Bainbridge W. A., 2017, Alzheimer's Dement., V13, pP287
   Bainbridge WA, 2019, ALZH DEMENT-DADM, V11, P610, DOI 10.1016/j.dadm.2019.07.005
   Bainbridge WA, 2013, J EXP PSYCHOL GEN, V142, P1323, DOI 10.1037/a0033872
   Bampis CG, 2018, IEEE T IMAGE PROCESS, V27, P3316, DOI 10.1109/TIP.2018.2815842
   Baveye Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P491, DOI 10.1145/2964284.2967269
   Bigne E, 2020, J HOSP TOUR MANAG, V45, P309, DOI 10.1016/j.jhtm.2020.08.019
   Nguyen C, 2020, J RETAIL CONSUM SERV, V55, DOI 10.1016/j.jretconser.2020.102080
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Cohendet R., 2018, PROC MEDIAEVAL
   Cohendet R, 2019, IEEE I CONF COMP VIS, P2531, DOI 10.1109/ICCV.2019.00262
   Constantin M. G., 2019, PROC MEDIAEVAL
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fajtl J, 2019, LECT NOTES COMPUT SC, V11367, P39, DOI 10.1007/978-3-030-21074-8_4
   Fajtl J, 2018, PROC CVPR IEEE, P6363, DOI 10.1109/CVPR.2018.00666
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Guinaudeau C., 2023, PROC WORK NOTES PROC
   Gupta R., 2018, PROC MEDIAEVAL
   Han JW, 2015, IEEE T CYBERNETICS, V45, P1692, DOI 10.1109/TCYB.2014.2358647
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Isola P, 2014, IEEE T PATTERN ANAL, V36, P1469, DOI 10.1109/TPAMI.2013.200
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Jing PG, 2021, IEEE T MULTIMEDIA, V23, P2259, DOI 10.1109/TMM.2020.3009485
   Jing PG, 2018, IEEE T KNOWL DATA EN, V30, P1519, DOI 10.1109/TKDE.2017.2785784
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Khosla A, 2015, IEEE I CONF COMP VIS, P2390, DOI 10.1109/ICCV.2015.275
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Leyva R., 2019, PROC MEDIAEVAL
   Leyva R, 2021, IEEE IMAGE PROC, P2488, DOI 10.1109/ICIP42928.2021.9506411
   Li Y, 2023, INFORM SCIENCES, V630, P356, DOI 10.1016/j.ins.2022.11.111
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Luna D, 2013, J CONSUM PSYCHOL, V23, P36, DOI 10.1016/j.jcps.2012.02.003
   Needell C. D., 2022, Computational Brain & Behavior, V5, P168, DOI [10.1007/s42113-022-00126-5, DOI 10.1007/S42113-022-00126-5]
   Newman Anelise, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P223, DOI 10.1007/978-3-030-58517-4_14
   Praveen A, 2021, PEERJ COMPUT SCI, V7, DOI 10.7717/peerj-cs.767
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shekhar S, 2017, IEEE INT CONF COMP V, P2730, DOI 10.1109/ICCVW.2017.321
   Shi JA, 2024, IEEE T MULTIMEDIA, V26, P2608, DOI 10.1109/TMM.2023.3301225
   STANDING L, 1970, PSYCHON SCI, V19, P73, DOI 10.3758/BF03337426
   Su YT, 2023, IEEE T MULTIMEDIA, V25, P1243, DOI 10.1109/TMM.2022.3140892
   Sweeney L, 2021, INT WORK CONTENT MUL, P23, DOI 10.1109/CBMI50038.2021.9461903
   Tibshirani R, 2011, J R STAT SOC B, V73, P273, DOI 10.1111/j.1467-9868.2011.00771.x
   Tran L.-V., 2019, PROC MEDIAEVAL
   Vaswani A, 2017, ADV NEUR IN, V30
   Viola A., 2019, POC MEDIA EVAL
   Wang S., 2018, PROC MEDIAEVAL
   Wang S., 2019, PROC MEDIAEVAL
   Weiss A., 2018, PROC MEDIA EVAL
   Wolfe JM, 1998, CURR BIOL, V8, pR303, DOI 10.1016/S0960-9822(98)70192-7
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yue Fumei, 2021, PROC DIGIT TV WIRELE, P239, DOI DOI 10.1007/978-981-16-1194-0_21
   Zhang DY, 2021, IEEE T GEOSCI REMOTE, V59, P5183, DOI 10.1109/TGRS.2020.3009918
   Zhang HZ, 2020, IEEE T MULTIMEDIA, V22, P3210, DOI 10.1109/TMM.2020.2973828
   Zhao T., 2020, PROC CEUR WORKSHOP
NR 61
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4926
EP 4937
DI 10.1109/TMM.2023.3327861
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600004
DA 2024-08-05
ER

PT J
AU Lu, Y
   Ni, FY
   Wang, HF
   Guo, XF
   Zhu, LC
   Yang, ZX
   Song, RH
   Cheng, LL
   Yang, Y
AF Lu, Yu
   Ni, Feiyue
   Wang, Haofan
   Guo, Xiaofeng
   Zhu, Linchao
   Yang, Zongxin
   Song, Ruihua
   Cheng, Lele
   Yang, Yi
TI Show Me a Video: A Large-Scale Narrated Video Dataset for Coherent Story
   Illustration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Semantics; Motion pictures; Context
   modeling; Coherence; Atmospheric modeling; Image story visualization;
   story illustration; text-to-video retrieval
ID FRAMEWORK
AB Illustrating a multi-sentence story with visual content is a significant challenge in multimedia research. While previous works have focused on sequential story-to-visual representations at the image level or representing a single sentence with a video clip, illustrating a long multi-sentence story with coherent videos remains an under-explored area. In this paper, we propose the task of video-based story illustration that focuses on the goal of visually illustrating a story with retrieved video clips. To support this task, we first create a large-scale dataset of coherent video stories in each sample, consisting of 85 K narrative stories with 60 pairs of consistent clips and texts. We then propose the Story Context-Enhanced Model, which leverages local and global contextual information within the story, inspired by sequence modeling in language understanding. Through comprehensive quantitative experiments, we demonstrate the effectiveness of our baseline model. In addition, qualitative results and detailed user studies reveal that our method can retrieve coherent video sequences from stories.
C1 [Lu, Yu; Zhu, Linchao; Yang, Zongxin; Yang, Yi] Zhejiang Univ, Coll Comp Sci & Technol, CCAI, Hangzhou 310027, Peoples R China.
   [Ni, Feiyue; Song, Ruihua] Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing 100872, Peoples R China.
   [Wang, Haofan; Guo, Xiaofeng; Cheng, Lele] Kuaishou Technol, Beijing 100085, Peoples R China.
C3 Zhejiang University; Renmin University of China
RP Zhu, LC (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, CCAI, Hangzhou 310027, Peoples R China.
EM aniki.yulu@gmail.com; nifeiyue@ruc.edu.cn; haofanwang.ai@gmail.com;
   xiaofengguo2010@gmail.com; zhulinchao@zju.edu.cn;
   yangzongxin@zju.edu.cn; songruihua_bloon@outlook.com;
   lelecheng05@126.com; yangyics@zju.edu.cn
RI Wang, Haofan/HZJ-8545-2023
OI Wang, Haofan/0000-0002-8614-9353
FU National Natural Science Foundation of China
FX No Statement Available
CR Bain Max, 2021, Computer Vision - ACCV 2020 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12626), P460, DOI 10.1007/978-3-030-69541-5_28
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen Qian, 2018, COLING, P1815
   Chen SZ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2236, DOI 10.1145/3343031.3350571
   Cheng Xing, 2021, arXiv
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dogan P, 2018, PROC CVPR IEEE, P8749, DOI 10.1109/CVPR.2018.00912
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Du YN, 2020, Arxiv, DOI arXiv:2009.09941
   Gella S, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P968
   Gulati A, 2020, INTERSPEECH, P5036, DOI 10.21437/Interspeech.2020-3015
   Huang T, 2016, LECT NOTES ARTIF INT, V9652, P233, DOI 10.1007/978-3-319-31750-2_19
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D.P., 2014, Proc. of ICLR
   Li BW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3548034
   Li BW, 2022, LECT NOTES COMPUT SC, V13696, P347, DOI 10.1007/978-3-031-20059-5_20
   Li JN, 2020, IEEE T MULTIMEDIA, V22, P554, DOI 10.1109/TMM.2019.2930041
   Li YT, 2019, PROC CVPR IEEE, P6322, DOI 10.1109/CVPR.2019.00649
   Lin RC, 2019, LECT NOTES COMPUT SC, V11132, P206, DOI 10.1007/978-3-030-11018-5_19
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Radford A, 2021, PR MACH LEARN RES, V139
   Ravi H, 2018, PROC CVPR IEEE, P7613, DOI 10.1109/CVPR.2018.00794
   Rohrbach A, 2017, INT J COMPUT VISION, V123, P94, DOI 10.1007/s11263-016-0987-1
   Vaswani A, 2017, ADV NEUR IN, V30
   Vicol P, 2018, PROC CVPR IEEE, P8581, DOI 10.1109/CVPR.2018.00895
   Wang XH, 2023, IEEE T MULTIMEDIA, V25, P6079, DOI 10.1109/TMM.2022.3204444
   Wang Z, 2021, IEEE T MULTIMEDIA, V23, P4027, DOI 10.1109/TMM.2020.3037461
   Xiong Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5407, DOI 10.1145/3503161.3548268
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang XS, 2018, IEEE T MULTIMEDIA, V20, P2360, DOI 10.1109/TMM.2018.2807588
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yun-Zhu Song, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P18, DOI 10.1007/978-3-030-58520-4_2
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503
NR 36
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2456
EP 2466
DI 10.1109/TMM.2023.3296944
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100025
DA 2024-08-05
ER

PT J
AU Ma, WF
   Li, JH
   Li, B
   Lu, Y
AF Ma, Wufei
   Li, Jiahao
   Li, Bin
   Lu, Yan
TI Uncertainty-Aware Deep Video Compression With Ensembles
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Uncertainty; Predictive models; Video compression; Decoding; Vectors;
   Optical flow; Quantization (signal); Deep video compression; motion
   estimation; prediction; uncertainty
AB Deep learning-based video compression is a challenging task, and many previous state-of-the-art learning-based video codecs use optical flows to exploit the temporal correlation between successive frames and then compress the residual error. Although these two-stage models are end-to-end optimized, the epistemic uncertainty in the motion estimation and the aleatoric uncertainty from the quantization operation lead to errors in the intermediate representations and introduce artifacts in the reconstructed frames. This inherent flaw limits the potential for higher bit rate savings. To address this issue, we propose an uncertainty-aware video compression model that can effectively capture the predictive uncertainty with deep ensembles. Additionally, we introduce an ensemble-aware loss to encourage the diversity among ensemble members and investigate the benefits of incorporating adversarial training in the video compression task. Experimental results on 1080p sequences show that our model can effectively save bits by more than 20% compared to DVC Pro.
C1 [Ma, Wufei; Li, Jiahao; Li, Bin; Lu, Yan] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Ma, Wufei] Johns Hopkins Univ, Baltimore, MD 21218 USA.
C3 Microsoft; Microsoft Research Asia; Johns Hopkins University
RP Li, JH (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM wma27@jhu.edu; li.jiahao@microsoft.com; libin@microsoft.com;
   yanlu@microsoft.com
CR Abbasi M., 2017, PROC STH INT C LEARN
   Agustsson Eirikur, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8500, DOI 10.1109/CVPR42600.2020.00853
   Ballé J, 2016, PICT COD SYMP, DOI 10.1109/pcs.2016.7906310
   Bjontegaard G., 2001, ITU SG16 Dec. VCEG-M33
   Chen HB, 2021, ADV NEUR IN, V34
   cisco, Cisco annual internet report-cisco annual internet report (2018-2023) white paper
   Fort S, 2020, Arxiv, DOI [arXiv:1912.02757, 10.48550/arXiv.1912.02757, DOI 10.48550/ARXIV.1912.02757]
   Gal Y., 2016, PhD Thesis
   Gal Y, 2016, PR MACH LEARN RES, V48
   Garipov T, 2018, ADV NEUR IN, V31
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Guo Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P456, DOI 10.1007/978-3-030-58536-5_27
   Habibian A, 2019, IEEE I CONF COMP VIS, P7032, DOI 10.1109/ICCV.2019.00713
   HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871
   He PS, 2021, IEEE T MULTIMEDIA, V23, P3179, DOI 10.1109/TMM.2020.3021234
   Hendrycks D., 2017, PROC 5 INT C LEARN R, P1
   Kendall A, 2017, 31 ANN C NEURAL INFO, V30
   Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020
   Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lee SF, 2015, Arxiv, DOI [arXiv:1511.06314, 10.48550/arXiv.1511.06314, DOI 10.48550/ARXIV.1511.06314]
   Loshchilov I., 2018, INT C LEARN REPR
   Lu G, 2021, IEEE T PATTERN ANAL, V43, P3292, DOI 10.1109/TPAMI.2020.2988453
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Lu M, 2023, IEEE T MULTIMEDIA, V25, P2097, DOI 10.1109/TMM.2022.3142414
   Luo FL, 2019, IEEE T MULTIMEDIA, V21, P851, DOI 10.1109/TMM.2018.2867260
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448
   Mercat A, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P297, DOI 10.1145/3339825.3394937
   Neal Radford M, 1995, PhD thesis,
   NIX DA, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P55, DOI 10.1109/ICNN.1994.374138
   Perrone M., 1993, PROC NEURAL NETW SPE, P342, DOI [DOI 10.1142/9789812795885_0025, 10.1142/9789812795885_0025]
   Pessoa J, 2020, IEEE WRK SIG PRO SYS, P276, DOI 10.1109/sips50750.2020.9195249
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Rippel O, 2019, IEEE I CONF COMP VIS, P3453, DOI 10.1109/ICCV.2019.00355
   Schapire RE, 1998, ANN STAT, V26, P1651
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Szegedy C., 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594]
   Szegedy C., 2014, PROC 2 INT C LEARN R
   Tomar S., 2006, Linux J, V2006, P10
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang SQ, 2017, IEEE T MULTIMEDIA, V19, P660, DOI 10.1109/TMM.2016.2625276
   Wang YF, 2021, IEEE T CIRC SYST VID, V31, P1193, DOI 10.1109/TCSVT.2020.3000331
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Wong E., 2020, PROC 8 INT C LEARN R
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhihao Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P193, DOI 10.1007/978-3-030-58536-5_12
NR 48
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7863
EP 7872
DI 10.1109/TMM.2024.3372352
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000064
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Mumtaz, D
   Sadbhawna
   Jakhetiya, V
   Subudhi, BN
   Lin, W
AF Mumtaz, Deebha
   Sadbhawna
   Jakhetiya, Vinit
   Subudhi, Badri N.
   Lin, Weisi
TI Non-Subsampled Contourlet Transform and Ground-Truth Score Generation
   Based Quality Assessment for DIBR-Synthesized Views
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ground-truth scores; depth image-based rendering; perceptual quality;
   quality assessment; NSCT coefficients
ID GEOMETRIC DISTORTIONS; IMAGES
AB In recent years, there have been advancements in developing Depth-Image-Based Rendering (DIBR) views. However, the quality of these synthesized views is often degraded by inefficient in-painting techniques and synthesis procedures, leading to geometric and structural distortions. This paper introduces two novel approaches to evaluate the quality of DIBR synthesized views, using full reference (FR) and no-reference (NR) metrics. The proposed FR quality assessment (QA) metric is based on the observation that the deep features of the Non-Subsampled Contourlet Transform (NSCT) maps capture the perceptually important characteristics of the images. By calculating the difference between these deep feature vectors of the reference and distorted views, we determine the quality of the image. Moreover, a lot of existing NR metrics typically divide an image into blocks and assign the same subjective quality scores to each block for training a deep learning model. However, this approach is not suitable for DIBR synthesized views, as distortions are often localized in specific areas rather than affecting the entire view. Consequently, the performance of existing block-based deep-learning algorithms suffers due to the absence of accurate ground truth scores for each image block. To address this limitation, this work proposes an innovative method for calculating ground truth scores for individual image blocks. This process is similar to the proposed FR metric. Firstly, we obtain the deep features of NSCT map of an image block and the quality score for each block is calculated using its and the reference block's feature vector. These block-wise ground truth scores are used to train a deep learning model which serves as an NR metric for estimating the quality of a given test block. Finally, the predicted block-level quality values are aggregated to determine the overall quality of the entire image. Experimental results demonstrate that both the proposed algorithms perform better than the existing objective metrics for DIBR synthesized views.
C1 [Mumtaz, Deebha] Natl Inst Technol, Srinagar 190006, India.
   Malaviya Natl Inst Technol Jaipur, Jaipur 302017, India.
   [Jakhetiya, Vinit] Indian Inst Technol Jammu, Dept Comp Sci & Engn, Jammu 181221, India.
   [Subudhi, Badri N.] Indian Inst Technol Jammu, Dept Elect Engn, Jammu 181221, India.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 National Institute of Technology (NIT System); Malaviya National
   Institute of Technology Jaipur; Indian Institute of Technology System
   (IIT System); Indian Institute of Technology (IIT) Jammu; Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) Jammu; Nanyang Technological University
RP Jakhetiya, V (corresponding author), Indian Inst Technol Jammu, Dept Comp Sci & Engn, Jammu 181221, India.
EM deebhamumtaz@gmail.com; sadbhawnathakur@gmail.com;
   vinit.jakhetiya@iitjammu.ac.in; subudhi.badri@iitjammu.ac.in;
   wslin@ntu.edu.sg
RI Lin, Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947; Subudhi, Badri
   Narayan/0000-0002-4378-0065
FU TIH iHub Drishti
FX No Statement Available
CR Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Chaudhary S, 2021, IEEE IMAGE PROC, P2628, DOI 10.1109/ICIP42928.2021.9506607
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Christian S., 2017, PROC AAAICONF ARTIF, V31
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   de Oliveira AQ, 2018, IEEE SIGNAL PROC LET, V25, P1705, DOI 10.1109/LSP.2018.2870342
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding KY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2483, DOI 10.1145/3474085.3475419
   Ding KY, 2021, INT J COMPUT VISION, V129, P1258, DOI 10.1007/s11263-020-01419-7
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Farid MS, 2017, IEEE INT CON MULTI, P505, DOI 10.1109/ICME.2017.8019307
   Farid MS, 2015, IEEE IMAGE PROC, P3720, DOI 10.1109/ICIP.2015.7351499
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gu K, 2020, IEEE T BROADCAST, V66, P127, DOI 10.1109/TBC.2019.2906768
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   Gu K, 2014, IEEE IMAGE PROC, P506, DOI 10.1109/ICIP.2014.7025101
   Jakhetiya V, 2021, IEEE T IND ELECTRON, V68, P423, DOI 10.1109/TIE.2020.2965469
   Jakhetiya V, 2019, IEEE T IND INFORM, V15, P4120, DOI 10.1109/TII.2018.2888861
   Jin CC, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23060770
   Jung YJ, 2016, IEEE T CIRC SYST VID, V26, P1201, DOI 10.1109/TCSVT.2015.2430632
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lao SS, 2022, IEEE COMPUT SOC CONF, P1139, DOI 10.1109/CVPRW56347.2022.00123
   Li L., 2019, CVPR WORKSHOPS, P17
   Li LD, 2021, IEEE T CIRC SYST VID, V31, P2509, DOI 10.1109/TCSVT.2020.3024882
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P320, DOI 10.1109/TMM.2020.2980185
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Li YM, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P685, DOI 10.1109/ICDSP.2016.7868646
   Ling SY, 2021, IEEE T MULTIMEDIA, V23, P4245, DOI 10.1109/TMM.2020.3038305
   Ling SY, 2019, IEEE IMAGE PROC, P1735, DOI [10.1109/ICIP.2019.8803105, 10.1109/icip.2019.8803105]
   Ling SY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1157, DOI 10.1145/3123266.3123329
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Madhusudana PC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921858
   Mahmoudpour S, 2022, IEEE ACCESS, V10, P59026, DOI 10.1109/ACCESS.2022.3179693
   Mahmoudpour S, 2020, IEEE SIGNAL PROC LET, V27, P1650, DOI 10.1109/LSP.2020.3024109
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Morin L., 2012, Proc. SPIE, V8288, P557
   Mustafa A, 2022, IEEE WINT CONF APPL, P21, DOI 10.1109/WACV51458.2022.00010
   Iandola FN, 2016, Arxiv, DOI [arXiv:1602.07360, 10.48550/arXiv.1602.07360]
   Peng ZY, 2022, IEEE T CIRC SYST VID, V32, P3422, DOI 10.1109/TCSVT.2021.3112933
   Sadbhawna, 2022, IEEE T IMAGE PROCESS, V31, P2027, DOI 10.1109/TIP.2022.3147981
   Sadbhawna, 2022, IEEE T IMAGE PROCESS, V31, P1737, DOI 10.1109/TIP.2022.3145997
   Sadbhawna, 2020, IEEE INT WORKSH MULT
   Sandic-Stankovic D, 2015, INT WORK QUAL MULTIM
   Sandic-Stankovic D, 2016, J ELECTR ENG-SLOVAK, V67, P3, DOI 10.1515/jee-2016-0001
   Sandic-Stankovic DD, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919416
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sui XJ, 2021, SIGNAL PROCESS-IMAGE, V92, DOI 10.1016/j.image.2020.116096
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Thakur S, 2023, IEEE T MULTIMEDIA, V25, P6183, DOI 10.1109/TMM.2022.3206660
   Thummerer A, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/abb1d6
   Tian S., 2018, Electron. Imag., V2018, P1
   Tian S., 2018, PROC IEEE VISUAL COM, P1
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   Tian SS, 2017, INT CONF ACOUST SPEE, P1248, DOI 10.1109/ICASSP.2017.7952356
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Wang XC, 2019, IEEE IMAGE PROC, P435, DOI [10.1109/icip.2019.8802943, 10.1109/ICIP.2019.8802943]
   Wang XJ, 2021, IEEE T MULTIMEDIA, V23, P1173, DOI 10.1109/TMM.2020.2993942
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan JB, 2020, IEEE T IMAGE PROCESS, V29, P7443, DOI 10.1109/TIP.2020.3003218
   Yue GH, 2019, IEEE T IMAGE PROCESS, V28, P2075, DOI 10.1109/TIP.2018.2875913
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhou J., 2005, IEEE INT C IM PROC, P469, DOI DOI 10.1109/ICIP.2005.1529789
   Zhou Wei, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P934, DOI 10.1145/3503161.3547899
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
NR 74
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7873
EP 7886
DI 10.1109/TMM.2024.3372837
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000069
DA 2024-08-05
ER

PT J
AU Pang, SM
   Zeng, YY
   Zhao, JW
   Xue, JR
AF Pang, Shanmin
   Zeng, Yueyang
   Zhao, Jiawei
   Xue, Jianru
TI A Mutually Textual and Visual Refinement Network for Image-Text Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Vectors; Cameras; Image segmentation; Feature
   extraction; Image coding; Cross-modal retrieval; image-text matching;
   contextual enhancement; semantic alignment enhancement
AB Image-text matching is vital important in the field of multi-modal intelligence. Recently, it is advocated in a way that decomposes images and texts into local fragments and followed by region-word aligning. As a result, the image-text relevance score is given by aggregating semantic similarities between matched region-word pairs. Despite effectiveness, this strategy fails to express data relations exactly. From the perspective of the text side, text words decomposed from a concise language sentence usually have limited contextual information, which can result in semantic identical but actually false text-region alignments. From the perspective of the image side, semantic ambiguity that multiple objects share the same semantic meaning can further exacerbate this problem. In this manuscript, we introduce a mutually Textual and Visual Refinement Network (TVRN), to tackle the inaccurate cross-modal alignment problem. In a nutshell, TVRN improves inter-modal matching by improving contextual information in sentences meanwhile reduces semantic ambiguity in images to capture the maximized relevant relations. More specifically, we develop a new module that integrates visual contextual clues into the text modality to generate informational text features with richer geometric contexts. Mutually, we further design a semantic alignment enhancement module that leverages consensus affinity of local image and text features to guide deeper semantic image embedding with the supervision of global image vectors. At the image-text matching stage, similarities at the local and global levels are integrated to capture coarse-grained and fine-grained interactions between vision and language. A large number of experiments on Flickr30 K and MS-COCO benchmarks demonstrate that TVRN is superior to existing methods.
C1 [Pang, Shanmin; Zeng, Yueyang; Zhao, Jiawei] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Xue, Jianru] Xi An Jiao Tong Univ, Sch Artificial Intelligence, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Zeng, YY (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
EM pangsm@xjtu.edu.cn; d11285201110@gmail.com; jvzhao@stu.xjtu.edu.cn;
   jrxue@xjtu.edu.cn
OI Xue, Jianru/0000-0002-4994-9343
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Chen C, 2023, IEEE T MULTIMEDIA, V25, P8933, DOI 10.1109/TMM.2023.3243665
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Cheng YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499027
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Faghri Fartash, 2018, BMVC
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Fu X, 2020, IEEE T MULTIMEDIA, V22, P2354, DOI 10.1109/TMM.2019.2957948
   Fu ZR, 2023, PROC CVPR IEEE, P15159, DOI 10.1109/CVPR52729.2023.01455
   Ge XR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5185, DOI 10.1145/3474085.3475634
   Guo J, 2023, IEEE T MULTIMEDIA, V25, P9189, DOI 10.1109/TMM.2023.3248160
   Hu ZB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P789
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Zhong, 2021, P 30 INT JOINT C ART, P765, DOI DOI 10.24963/IJCAI.2021/106
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim D, 2023, PROC CVPR IEEE, P23422, DOI 10.1109/CVPR52729.2023.02243
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li JH, 2021, ADV NEUR IN, V34
   Li JN, 2022, PR MACH LEARN RES
   Li KP, 2023, IEEE T PATTERN ANAL, V45, P641, DOI 10.1109/TPAMI.2022.3148470
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Long SQ, 2022, IEEE WINT CONF APPL, P2463, DOI 10.1109/WACV51458.2022.00252
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Messina N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451390
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Pan ZX, 2023, PROC CVPR IEEE, P19275, DOI 10.1109/CVPR52729.2023.01847
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Rao JF, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1913, DOI 10.1145/2983323.2983872
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P3362, DOI 10.1109/TMM.2020.3024822
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu E, 2022, NEUROCOMPUTING, V486, P215, DOI 10.1016/j.neucom.2021.11.035
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Zeng Yan, 2022, P MACHINE LEARNING R
   Zhang HT, 2022, AAAI CONF ARTIF INTE, P3262
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
NR 56
TC 0
Z9 0
U1 16
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7555
EP 7566
DI 10.1109/TMM.2024.3369968
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000016
DA 2024-08-05
ER

PT J
AU Song, WF
   Chu, TL
   Li, S
   Li, NN
   Hao, AM
   Qin, H
AF Song, Wenfeng
   Chu, Tangli
   Li, Shuai
   Li, Nannan
   Hao, Aimin
   Qin, Hong
TI Joints-Centered Spatial-Temporal Features Fused Skeleton Convolution
   Network for Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Feature extraction; Convolution; Visualization; Task analysis;
   Joints; Data mining; Skeleton-based action recognition; spatial-temporal
   feature fusion; PDE diffusion
ID NEURAL-NETWORKS; GRAPH; REPRESENTATION; DESCRIPTOR; DIFFUSION; FUSION
AB Skeleton-based action recognition is crucial for natural human-computer interaction, dynamic behavior analysis, and behavior surveillance. The key challenge is to effectively capture the intrinsic local-global clues of the activity. However, it remains challenging to efficiently leverage multidimensional information related to joints' local visual appearances, global spatial relationships, and coherent temporal cues. To address this challenge, we propose a joints-centered spatial-temporal feature-fused framework for action recognition, which exploits skeleton-based graph diffusion and convolution. Specifically, we employ Partial Differential Equation (PDE) based skeleton graph diffusion to automatically activate and diffuse the salient appearance features of joints. This approach simultaneously integrates the joints' appearance clues and their hierarchical relationships at both the super-pixel level and structure level. The diffused appearance-related features of the joints are further fused with skeleton-related spatial-temporal features, and the resulting fused features are fed into a skeleton convolution network for action recognition. Our method was extensively evaluated on two public datasets (NTU-RGBD and UWA3D), and the results demonstrate the improved accuracy and effectiveness of our approach. Our code will be public.
C1 [Song, Wenfeng] Beijing Informat Sci & Technol Univ, Comp Sch, Beijing 100101, Peoples R China.
   [Chu, Tangli; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Shuai] Zhongguancun Lab, Beijing, Peoples R China.
   [Hao, Aimin] Chinese Acad Med Sci, Res Unit Virtual Body & Virtual Surg Technol, 2019RU004, Beijing, Peoples R China.
   [Li, Nannan] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116024, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beijing Information Science & Technology University; Beihang University;
   Zhongguancun Laboratory; Chinese Academy of Medical Sciences - Peking
   Union Medical College; Dalian Maritime University; State University of
   New York (SUNY) System; State University of New York (SUNY) Stony Brook
RP Li, S (corresponding author), Zhongguancun Lab, Beijing, Peoples R China.; Qin, H (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM songwenfenga@gmail.com; tang0704@buaa.edu.cn; lishuai@buaa.edu.cn;
   nannanli@dlmu.edu.cn; ham@buaa.edu.cn; qin@cs.stonybrook.edu
OI QIN, HONG/0000-0001-7699-1355
FU National Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahn D, 2023, IEEE WINT CONF APPL, P3319, DOI 10.1109/WACV56688.2023.00333
   Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056
   Cai JM, 2021, IEEE WINT CONF APPL, P2734, DOI 10.1109/WACV48630.2021.00278
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Ding ZH, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY AND SECURITY COMPANION (QRS-C), P610, DOI 10.1109/QRS-C.2017.134
   Duan HD, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P7351, DOI 10.1145/3503161.3548546
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Duhme M, 2021, DAGM GERMAN C PATTER, P265, DOI [10.1007/978-3-030-92659-5_17, DOI 10.1007/978-3-030-92659-5_17]
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Hu LY, 2023, EXPERT SYST APPL, V232, DOI 10.1016/j.eswa.2023.120683
   Huang ZX, 2023, IEEE T CIRC SYST VID, V33, P1868, DOI 10.1109/TCSVT.2022.3217763
   Ju C, 2023, IEEE T MULTIMEDIA, V25, P6688, DOI 10.1109/TMM.2022.3213478
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim S, 2023, P IEEECVF INT C COMP, P10265
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Li B, 2019, AAAI CONF ARTIF INTE, P8561
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li JN, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107356
   Li MS, 2022, IEEE T PATTERN ANAL, V44, P3316, DOI 10.1109/TPAMI.2021.3053765
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li NN, 2019, COMPUT AIDED GEOM D, V72, P111, DOI 10.1016/j.cagd.2019.04.020
   Li S, 2023, IEEE T PATTERN ANAL, V45, P8477, DOI 10.1109/TPAMI.2023.3238411
   Li S, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109072
   Li S, 2020, INT J COMPUT VISION, V128, P2936, DOI 10.1007/s11263-020-01349-4
   Li ZC, 2023, IEEE T CIRC SYST VID, V33, P3333, DOI 10.1109/TCSVT.2022.3232373
   Liu H, 2017, Arxiv, DOI [arXiv:1705.08106, DOI 10.48550/ARXIV.1705.08106]
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu RS, 2016, IEEE T PATTERN ANAL, V38, P2457, DOI 10.1109/TPAMI.2016.2522415
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Niepert M, 2016, PR MACH LEARN RES, V48
   Pang C, 2023, IEEE T MULTIMEDIA, V25, P8699, DOI 10.1109/TMM.2023.3239751
   Peng W, 2020, AAAI CONF ARTIF INTE, V34, P2669
   Purwanto D, 2019, IEEE T MULTIMEDIA, V21, P3122, DOI 10.1109/TMM.2019.2919434
   Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389
   Rahmani H, 2014, LECT NOTES COMPUT SC, V8690, P742, DOI 10.1007/978-3-319-10605-2_48
   Ren B, 2024, Arxiv, DOI arXiv:2002.05907
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Song SJ, 2020, IEEE T IMAGE PROCESS, V29, P3957, DOI 10.1109/TIP.2020.2967577
   Song SJ, 2018, IEEE INT CON MULTI
   Song WF, 2023, IEEE T MULTIMEDIA, V25, P6132, DOI 10.1109/TMM.2022.3205456
   Song YF, 2023, IEEE T PATTERN ANAL, V45, P1474, DOI 10.1109/TPAMI.2022.3157033
   Song YF, 2019, IEEE IMAGE PROC, P1, DOI [10.1109/TFUZZ.2019.2910714, 10.1109/ICIP.2019.8802917, 10.1109/icip.2019.8802917]
   Thakkar K., 2018, PROC BRIT MACH VIS C
   Wang H, 2021, PROC CVPR IEEE, P14651, DOI 10.1109/CVPR46437.2021.01442
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wenfeng S, 2023, INT J COMPUT VISION, V131, P2816, DOI 10.1007/s11263-023-01839-1
   Wu C, 2024, IEEE T CIRC SYST VID, V34, P34, DOI 10.1109/TCSVT.2023.3236430
   Wu MY, 2021, AAAI CONF ARTIF INTE, V35, P2934
   Xu SH, 2023, IEEE T MULTIMEDIA, V25, P624, DOI 10.1109/TMM.2021.3129616
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang C, 2021, IEEE T VIS COMPUT GR, V27, P4520, DOI 10.1109/TVCG.2020.3003994
   Yang D, 2020, Arxiv, DOI arXiv:2003.03007
   Yang H, 2022, IEEE T IMAGE PROCESS, V31, P164, DOI 10.1109/TIP.2021.3129117
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yang ZJ, 2022, IEEE T VIS COMPUT GR, V28, P4558, DOI 10.1109/TVCG.2021.3092877
   Yun K., 2012, 2012 IEEE COMP SOC C, DOI DOI 10.1109/CVPRW.2012.6239234
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhao R, 2017, IEEE INT C INT ROBOT, P4260, DOI 10.1109/IROS.2017.8206288
NR 73
TC 0
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4602
EP 4616
DI 10.1109/TMM.2023.3324835
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100039
DA 2024-08-05
ER

PT J
AU Wang, ZY
   Yang, C
   Jiang, B
   Yuan, JS
AF Wang, Zhiyu
   Yang, Chao
   Jiang, Bin
   Yuan, Junsong
TI A Dual Reinforcement Learning Framework for Weakly Supervised Phrase
   Grounding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Grounding; Task analysis; Training; Reinforcement learning;
   Optimization; Image reconstruction; Proposals; Weakly supervised phrase
   grounding; visual grounding; dual learning; reinforcement learning
ID NETWORK; LANGUAGE
AB Weakly-supervised phrase grounding aims to localize a specific region in an image that corresponds to the given textual phrase, where the mapping between noun phrases and image regions is not available in the training stage. Previous methods typically exploit an additional proxy task (e.g., phrase reconstruction or image-phrase alignment) to provide supervision for training, since the lack of region-level annotations in the weakly-supervised setting. However, there exists a significant gap in optimization objectives between the proxy tasks and the target grounding task, which may result in low-efficient optimization for the target model. Therefore, in this paper, we propose a novel dual reinforcement learning framework to directly optimize the phrase grounding model. Specifically, we consider the duality of phrase grounding and phrase generation tasks. These two tasks form a closed loop that can provide quality feedback signals to measure the performance of each other. In this way, we can measure the correctness of the localized regions and thus be able to optimize the grounding model directly. We design two reward functions to quantify the feedback signals and train the models via reinforcement learning. In addition, to relieve the training difficulty of our framework, we present a heuristic algorithm to generate pseudo region-phrase pairs to warm-start our models. We perform experiments on two popular phrase grounding datasets: ReferItGame and Flickr30K Entities, and the results demonstrate that our method outperforms the previous methods by a large margin.
C1 [Wang, Zhiyu; Yang, Chao; Jiang, Bin] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Yuan, Junsong] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 Hunan University; State University of New York (SUNY) System; State
   University of New York (SUNY) Buffalo
RP Yang, C (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
EM wangzhiyu.wzy1@gmail.com; yangchaoedu@hnu.edu.cn; jiangbin@hnu.edu.cn;
   jsyuan@buffalo.edu
RI Yuan, Junsong/A-5171-2011
OI Yuan, Junsong/0000-0002-7901-8793; Jiang, Bin/0000-0002-5840-9664
FU National Natural Science Foundation of China
FX No Statement Available
CR Akbari H, 2019, PROC CVPR IEEE, P12468, DOI 10.1109/CVPR.2019.01276
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Arbelle A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1781, DOI 10.1109/ICCV48922.2021.00182
   Bajaj M, 2019, IEEE I CONF COMP VIS, P4280, DOI 10.1109/ICCV.2019.00438
   Chen H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1749, DOI 10.1145/3343031.3351055
   Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425
   Chen K, 2017, IEEE I CONF COMP VIS, P824, DOI 10.1109/ICCV.2017.95
   Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121
   Datta S, 2019, IEEE I CONF COMP VIS, P2601, DOI 10.1109/ICCV.2019.00269
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gupta T., 2020, EUR C COMP VIS, P752
   He D, 2016, ADV NEUR IN, V29
   Honnibal M., 2017, spaCy 2: Natural language understanding with Bloom embeddings, convolu- tional neural networks and incremental parsing, V7, P411, DOI DOI 10.3233/978-1-60750-588-4-1080
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Ivan Krasin N. A., 2017, Openimages: A public dataset for large-scale multilabel and multi-class image classification
   Jain A, 2022, LECT NOTES COMPUT SC, V13696, P417, DOI 10.1007/978-3-031-20059-5_24
   Javed SA, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P796
   Jiang HJ, 2022, PROC CVPR IEEE, P15492, DOI 10.1109/CVPR52688.2022.01507
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Kottur S, 2018, LECT NOTES COMPUT SC, V11219, P160, DOI 10.1007/978-3-030-01267-0_10
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li YK, 2018, PROC CVPR IEEE, P6116, DOI 10.1109/CVPR.2018.00640
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XJ, 2023, IEEE T PATTERN ANAL, V45, P3003, DOI 10.1109/TPAMI.2022.3186410
   Liu YF, 2021, PROC CVPR IEEE, P5608, DOI 10.1109/CVPR46437.2021.00556
   Lu JS, 2016, ADV NEUR IN, V29
   Luo FL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5116
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Qu MX, 2022, LECT NOTES COMPUT SC, V13695, P546, DOI 10.1007/978-3-031-19833-5_32
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Sibei Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P589, DOI 10.1007/978-3-030-58529-7_35
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sun MY, 2023, IEEE T MULTIMEDIA, V25, P2446, DOI 10.1109/TMM.2022.3147385
   Sun MJ, 2023, IEEE T MULTIMEDIA, V25, P1611, DOI 10.1109/TMM.2021.3139467
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang J, 2019, IEEE I CONF COMP VIS, P4662, DOI 10.1109/ICCV.2019.00476
   Wang LW, 2021, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR46437.2021.01387
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang QX, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2030
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P6329, DOI 10.1109/TMM.2022.3207581
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Wei BL, 2019, ADV NEUR IN, V32
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu X, 2018, IEEE INT CON MULTI
   Yang L, 2022, PROC CVPR IEEE, P9489, DOI 10.1109/CVPR52688.2022.00928
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Ye JB, 2022, PROC CVPR IEEE, P15481, DOI 10.1109/CVPR52688.2022.01506
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zagoruyko S., 2016, Wide residual networks, DOI DOI 10.5244/C.30.87
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhao F, 2018, PROC CVPR IEEE, P5696, DOI 10.1109/CVPR.2018.00597
   Zhao W, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P29, DOI 10.1145/3132847.3132920
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu CY, 2022, LECT NOTES COMPUT SC, V13695, P598, DOI 10.1007/978-3-031-19833-5_35
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 73
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 394
EP 405
DI 10.1109/TMM.2023.3265816
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000027
DA 2024-08-05
ER

PT J
AU Wu, Y
   Liu, JM
   Gong, MG
   Gong, PR
   Fan, XL
   Qin, AK
   Miao, QG
   Ma, WP
AF Wu, Yue
   Liu, Jiaming
   Gong, Maoguo
   Gong, Peiran
   Fan, Xiaolong
   Qin, A. K.
   Miao, Qiguang
   Ma, Wenping
TI Self-Supervised Intra-Modal and Cross-Modal Contrastive Learning for
   Point Cloud Understanding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Three-dimensional displays; Task analysis;
   Feature extraction; Self-supervised learning; Image color analysis;
   Visualization; Self-supervision; cross-modal learning; joint 3D-2D;
   point cloud understanding
ID NETWORK
AB Learning effective representations from unlabeled data is a challenging task for point cloud understanding. As the human visual system can map concepts learned from 2D images to the 3D world, and inspired by recent multimodal research, we introduce data from point cloud modality and image modality for joint learning. Based on the properties of point clouds and images, we propose CrossNet, a comprehensive intra- and cross-modal contrastive learning method that learns 3D point cloud representations. The proposed method achieves 3D-3D and 3D-2D correspondences of objectives by maximizing the consistency of point clouds and their augmented versions, and with the corresponding rendered images in invariant space. We further distinguish the rendered images into RGB and grayscale images to extract color and geometric features, respectively. These training objectives combine feature correspondences between modalities to combine rich learning signals from point clouds and images. Our CrossNet is simple: we add a feature extraction module and a projection head module to the point cloud and image branches, respectively, to train the backbone network in a self-supervised manner. After the network is pretrained, only the point cloud feature extraction module is required for fine-tuning and directly predicting results for downstream tasks. Our experiments on multiple benchmarks demonstrate improved point cloud classification and segmentation results, and the learned representations can be generalized across domains.
C1 [Wu, Yue; Liu, Jiaming; Gong, Peiran; Miao, Qiguang] Xidian Univ, Sch Comp Sci & Technol, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
   [Gong, Maoguo; Fan, Xiaolong] Xidian Univ, Sch Elect Engn, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
   [Qin, A. K.] Swinburne Univ Technol, Dept Comp Sci & Software Engn, Hawthorn, Vic 3122, Australia.
   [Ma, Wenping] Xidian Univ, Sch Artificial Intelligence, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University; Swinburne University of
   Technology; Xidian University
RP Gong, MG (corresponding author), Xidian Univ, Sch Elect Engn, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
EM ywu@xidian.edu.cn; ljm@stu.xidian.edu.cn; gong@ieee.org;
   gpr@stu.xidian.edu.cn; xiaolongfan@outlook.com; kqin@swin.edu.au;
   qgmiao@mail.xidian.edu.cn; wpma@mail.xidian.edu.cn
RI Qin, A. K./AAH-4943-2021
OI Qin, A. K./0000-0001-6631-1651; Liu, Jiaming/0009-0003-9699-1987; Miao,
   Qiguang/0000-0002-2872-388X
FU National Natural Science Foundation of China
FX No Statement Available
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Afham M, 2022, PROC CVPR IEEE, P9892, DOI 10.1109/CVPR52688.2022.00967
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Assefa M, 2023, IEEE T MULTIMEDIA, V25, P5500, DOI 10.1109/TMM.2022.3193559
   Banani El, 2021, P IEEECVF INT C COMP, P6433
   Braham NAA, 2022, INT GEOSCI REMOTE SE, P267, DOI 10.1109/IGARSS46834.2022.9884494
   Chang Angel X., 2015, arXiv
   Chang SN, 2022, IEEE T MULTIMEDIA, V24, P4067, DOI 10.1109/TMM.2021.3112814
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Desai K, 2021, PROC CVPR IEEE, P11157, DOI 10.1109/CVPR46437.2021.01101
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Fan HH, 2022, PROC CVPR IEEE, P6367, DOI 10.1109/CVPR52688.2022.00627
   Grill JB, 2020, P 34 INT C NEUR INF, V33, P21271
   Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Hjelm R. D., 2019, ICLR
   Huang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6515, DOI 10.1109/ICCV48922.2021.00647
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kingma D. P., 2014, arXiv
   Komodakis N., 2018, P INT C LEARN REPR A
   Krahenbuhl C., 2016, P INT C LEARN REPR
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li YZ, 2018, ADV NEUR IN, V31
   Li ZY, 2022, AAAI CONF ARTIF INTE, P1500
   Liu FY, 2022, INT CONF 3D VISION, P42, DOI 10.1109/3DV57658.2022.00017
   Liu Y.-C, 2021, Learning from 2 D: Contrastive pixel-to-point knowledge transfer for 3D pretraining
   Luciano L, 2019, VISUAL COMPUT, V35, P1171, DOI 10.1007/s00371-019-01668-9
   Marcus M.P., 1993, COMPUTATIONAL LINGUISTICS, V19, P313
   Mersch Benedikt, 2022, C ROB LEARN, P1444
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Oord A.v.d., Representation learning with contrastive predictive coding
   Ople JJM, 2023, IEEE T MULTIMEDIA, V25, P1125, DOI 10.1109/TMM.2021.3139215
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Peng XK, 2022, PROC CVPR IEEE, P8228, DOI 10.1109/CVPR52688.2022.00806
   Plummer BA, 2022, IEEE T PATTERN ANAL, V44, P2155, DOI 10.1109/TPAMI.2020.3029008
   Poursaeed O, 2020, INT CONF 3D VISION, P1018, DOI 10.1109/3DV50981.2020.00112
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Radford A, 2021, PR MACH LEARN RES, V139
   Rao YM, 2020, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR42600.2020.00542
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Sauder J, 2019, ADV NEUR IN, V32
   Su M., 2018, P EUR C COMP VIS WOR
   Sun C, 2023, IEEE T MULTIMEDIA, V25, P6207, DOI 10.1109/TMM.2022.3206664
   Sutton MA, 2000, TOP APPL PHYS, V77, P323
   Tao WX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5266, DOI 10.1145/3474085.3475645
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang HC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9762, DOI 10.1109/ICCV48922.2021.00964
   Wang WY, 2019, ADV NEUR IN, V32
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu Y, 2023, IEEE Trans. Circuits Syst. Video Technol.
   Wu Y, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3271757
   Wu Y, 2023, IEEE T CIRC SYST VID, V33, P1413, DOI 10.1109/TCSVT.2022.3213592
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiao A., IEEE Tran. Pattern Anal. Mach. Intell.
   Xu CF, 2022, LECT NOTES COMPUT SC, V13697, P638, DOI 10.1007/978-3-031-19836-6_36
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Ye ZP, 2023, IEEE T MULTIMEDIA, V25, P2033, DOI 10.1109/TMM.2022.3142387
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Zeng YH, 2023, PROC CVPR IEEE, P15244, DOI 10.1109/CVPR52729.2023.01463
   Zhang J., 2020, Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIV 16, P644
   Zhang L, 2019, INT CONF 3D VISION, P395, DOI 10.1109/3DV.2019.00051
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang RR, 2022, PROC CVPR IEEE, P8542, DOI 10.1109/CVPR52688.2022.00836
   Zhang ZW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10232, DOI 10.1109/ICCV48922.2021.01009
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
NR 82
TC 14
Z9 14
U1 29
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1626
EP 1638
DI 10.1109/TMM.2023.3284591
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800008
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Wu, ZN
   He, TY
   Xia, XB
   Yu, J
   Shen, X
   Liu, TL
AF Wu, Zhengning
   He, Tianyu
   Xia, Xiaobo
   Yu, Jun
   Shen, Xu
   Liu, Tongliang
TI Conditional Consistency Regularization for Semi-Supervised Multi-Label
   Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Predictive models; Image classification; Data models; Motorcycles;
   Manifolds; Training; Deep learning; semi-supervised multi-label
   learning; consistency regularization
AB Consistency regularization has achieved great successes in Semi-Supervised Single-Label Image Classification (SS-SLC) with deep learning models, while few effort has been devoted to Semi-Supervised Multi-Label Image Classification (SS-MLC) with deep learning models. One intuitive solution for introducing consistency regularization to SS-MLC is to regularize model predictions to be invariant to different augmented data of the same input image. However, the solution lacks the consideration of label relations, which are key elements in multi-label image classification. In this article, we go beyond the consistency regularization for multi-view input images, and propose Conditional Consistency Regularization (CCR) that is tailored for SS-MLC. Specifically, for two augmented input images, we make the two model predictions conditioned on different label states (i.e., positive, negative, or unknown for each class). By encouraging the two predictions to be consistent, the model is able to build relations between the given two different label states, which helps to make use of label relations for boosting image classification. The experiments on large-scale real-world SS-MLC benchmarks demonstrate that the proposed method can surpass state-of-the-art methods by a large margin.
C1 [Wu, Zhengning; Xia, Xiaobo; Liu, Tongliang] Univ Sydney, Sydney AI Ctr, Darlington, NSW 2006, Australia.
   [He, Tianyu] Microsoft Res Asia MSRA, Beijing 100080, Peoples R China.
   [Yu, Jun] Univ Sci & Technol China, Dept Automat, Hefei, Peoples R China.
   [Shen, Xu] DAMO Acad, Alibaba Grp, Hangzhou 311121, Peoples R China.
C3 University of Sydney; Chinese Academy of Sciences; University of Science
   & Technology of China, CAS; Alibaba Group
RP Yu, J (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei, Peoples R China.
EM zhwu2112@uni.sydney.edu.au; deeptimhe@gmail.com;
   xxia5420@uni.sydney.edu.au; harryjun@ustc.edu.cn; shenxuustc@gmail.com;
   tongliang.liu@sydney.edu.au
RI Liu, Tongliang/AAA-1506-2021
OI Liu, Tongliang/0000-0002-9640-6472; Xia, Xiaobo/0000-0003-3615-0919
FU Australian Research Council
FX No Statement Available
CR Belkin M, 2002, ADV NEUR IN, V14, P585
   Berthelot D., 2020, Paper 914
   Berthelot D, 2019, ADV NEUR IN, V32
   Beyer L, 2020, Arxiv, DOI [arXiv:2006.07159, DOI 10.48550/ARXIV.2006.07159]
   Bhatia K, 2015, 29 ANN C NEURAL INFO, V28
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Chen G., 2008, P 2008 SIAM INT C DA, P410, DOI DOI 10.1137/1.9781611972788.37
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen ZM, 2023, IEEE T PATTERN ANAL, V45, P6969, DOI 10.1109/TPAMI.2021.3063496
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cheng W., 2010, INT C MACH LEARN, P279
   Chu HM, 2018, LECT NOTES COMPUT SC, V11206, P409, DOI 10.1007/978-3-030-01216-8_25
   Cole E, 2021, PROC CVPR IEEE, P933, DOI 10.1109/CVPR46437.2021.00099
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Dai ZH, 2017, 31 ANN C NEURAL INFO, V30
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fei Wu, 2015, IEEE Transactions on Big Data, V1, P109, DOI 10.1109/TBDATA.2015.2497270
   Feng L, 2019, AAAI CONF ARTIF INTE, P3550
   Gao BB, 2021, IEEE T IMAGE PROCESS, V30, P5920, DOI 10.1109/TIP.2021.3088605
   Gong C, 2016, AAAI CONF ARTIF INTE, P1610
   Gupta N, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P527, DOI 10.1145/3447548.3467426
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu S., 2021, ICCV, P7649
   Huang C. Xue, 2021, INT C NEURAL INF PRO, P26714
   Huang Z, 2023, IEEE T MULTIMEDIA, V25, P1844, DOI 10.1109/TMM.2022.3179895
   Ioffe S., 2014, PROC INT C LEARN REP
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Jing LP, 2015, PROC CVPR IEEE, P1483, DOI 10.1109/CVPR.2015.7298755
   Ke JC, 2019, LECT NOTES COMPUT SC, V11936, P40, DOI 10.1007/978-3-030-36204-1_3
   Kingma D. P., 2014, P 2 INT C LEARNING R, P1
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kong XN, 2013, IEEE T KNOWL DATA EN, V25, P704, DOI 10.1109/TKDE.2011.141
   Kumar A, 2017, ADV NEUR IN, V30
   Lanchantin J, 2021, PROC CVPR IEEE, P16473, DOI 10.1109/CVPR46437.2021.01621
   Lee D.H., 2013, WORKSH CHALL REPR LE, P07
   Li Q, 2016, PROC CVPR IEEE, P2977, DOI 10.1109/CVPR.2016.325
   Lin GF, 2017, PATTERN RECOGN, V68, P14, DOI 10.1016/j.patcog.2017.03.014
   Lin HB, 2023, IEEE T MULTIMEDIA, V25, P8767, DOI 10.1109/TMM.2023.3241539
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu QS, 2017, IEEE T IMAGE PROCESS, V26, P452, DOI 10.1109/TIP.2016.2621671
   Liu WW, 2022, IEEE T PATTERN ANAL, V44, P7955, DOI 10.1109/TPAMI.2021.3119334
   Liu Y, 2020, PROC CVPR IEEE, P5719, DOI 10.1109/CVPR42600.2020.00576
   Luo Y, 2015, IEEE T IMAGE PROCESS, V24, P2355, DOI 10.1109/TIP.2015.2421309
   Luo Y, 2013, IEEE T IMAGE PROCESS, V22, P523, DOI 10.1109/TIP.2012.2218825
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Nam J., 2017, Adv. Neural Inf. Process. Syst, V30, P5413
   Qiao SY, 2018, LECT NOTES COMPUT SC, V11219, P142, DOI 10.1007/978-3-030-01267-0_9
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Rasmus A, 2015, ADV NEUR IN, V28
   Read J, 2009, LECT NOTES ARTIF INT, V5782, P254, DOI 10.1007/978-3-642-04174-7_17
   Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015
   Sajjadi M, 2016, ADV NEUR IN, V29
   Samuli L., 2017, INT C LEARN REPR ICL, V4, P6
   Shankar V., 2020, P INT C MACH LEARN, P1
   Sohn K., 2020, NEURIPS
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tarvainen A., 2017, P 31 ANN C NEUR INF, P1195, DOI DOI 10.1137/0330046
   Tong Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P162, DOI 10.1007/978-3-030-58548-8_10
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang B, 2013, IEEE I CONF COMP VIS, P425, DOI 10.1109/ICCV.2013.60
   Wang H., 2020, P NEURIPS, V33, P19839
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang LC, 2021, IEEE T IMAGE PROCESS, V30, P9125, DOI 10.1109/TIP.2021.3122003
   Wang X, 2021, IEEE T IMAGE PROCESS, V30, P1639, DOI 10.1109/TIP.2020.3044220
   Wu ZN, 2022, IEEE T MULTIMEDIA, V24, P1080, DOI 10.1109/TMM.2021.3116417
   Xia R, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1054
   Xia X., 2020, P NEUIPS DEC 6 12 VI, V33, P7597
   Xia XB, 2019, ADV NEUR IN, V32
   Xie MK, 2022, IEEE T PATTERN ANAL, V44, P3676, DOI 10.1109/TPAMI.2021.3059290
   Xie Q., 2020, Advances in Neural Information Processing Systems, V33, P6256
   Xu C, 2016, IEEE T IMAGE PROCESS, V25, P1495, DOI 10.1109/TIP.2016.2524207
   Xu JH, 2021, IEEE T MULTIMEDIA, V23, P1696, DOI 10.1109/TMM.2020.3002185
   Yang XL, 2023, IEEE T KNOWL DATA EN, V35, P8934, DOI 10.1109/TKDE.2022.3220219
   Yeh CK, 2017, AAAI CONF ARTIF INTE, P2838
   Yun S, 2021, PROC CVPR IEEE, P2340, DOI 10.1109/CVPR46437.2021.00237
   Zhan W, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1305, DOI 10.1145/3097983.3098141
   Zhang BW, 2021, 35 C NEURAL INFORM P, V34
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang LH, 2020, PROC CVPR IEEE, P3911, DOI 10.1109/CVPR42600.2020.00397
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
NR 84
TC 1
Z9 1
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4206
EP 4216
DI 10.1109/TMM.2023.3324132
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100059
DA 2024-08-05
ER

PT J
AU Xiang, JJ
   Chen, P
   Dang, YJ
   Liang, RH
   Jiang, GY
AF Xiang, Jianjun
   Chen, Peng
   Dang, Yuanjie
   Liang, Ronghua
   Jiang, Gangyi
TI Pseudo Light Field Image and 4D Wavelet-Transform-Based
   Reduced-Reference Light Field Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Wavelet transforms; Data mining; Feature extraction; Redundancy; Wavelet
   domain; Spatial resolution; Light fields; 4D wavelet transform; pseudo
   light field image; reduced-reference light field image quality
   assessment; spatial-angular weighting; view synthesis
ID SCREEN CONTENT IMAGES; VIDEO
AB Reduced-reference light field image (LFI) quality assessment (RR LFIQA) automatically assesses image quality with only partial information about the reference LFI is available. Existing RR LFIQA has difficulty extracting effective RR information and perceptual features to represent the LFI quality. In this article, we propose an RR LFIQA model based on pseudo LFI (PLFI) and four-dimensional (4D) wavelet transform. To extract RR information related to LFI perceptual quality, a PLFI is created as the RR information of the LFI using a view synthesis algorithm. Considering that the high-dimensional characteristics of the PLFI, 4D wavelet transform is used to decompose the original and distorted PLFIs. The 4D wavelet transform essentially performs a continuous 1D wavelet transform for the 4D signal to enable the local 4D structure of the PLFIs to be characterized effectively in the 4D wavelet domain. A novel spatial-angular weighting strategy is proposed to describe the importance of each location for quality evaluation, to further improve the performance of the proposed method. Experimental results on four benchmark datasets show that the proposed model performs better than the representative 2DIQA and LFIQA models.
C1 [Xiang, Jianjun; Chen, Peng; Dang, Yuanjie; Liang, Ronghua] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 31002, Peoples R China.
   [Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
C3 Zhejiang University of Technology; Ningbo University
RP Chen, P (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 31002, Peoples R China.
EM xjj180209@163.com; chenpeng@zjut.edu.cn; dangyj@zjut.edu.cn;
   rhliang@zjut.edu.cn; jianggangyi@nbu.edu.cn
RI jiang, gang/KII-8233-2024
OI Dang, Yuanjie/0000-0002-8302-1338
FU National Natural Science Foundation of China
FX No Statement Available
CR Alves G, 2018, IEEE IMAGE PROC, P1148, DOI 10.1109/ICIP.2018.8451583
   Chandramouli P, 2022, IEEE T PATTERN ANAL, V44, P1712, DOI 10.1109/TPAMI.2020.3039841
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P3722, DOI 10.1109/TMM.2021.3106775
   Cheraaqee P, 2022, IEEE T CIRC SYST VID, V32, P566, DOI 10.1109/TCSVT.2021.3067627
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Han K, 2022, IEEE T PATTERN ANAL, V44, P8022, DOI 10.1109/TPAMI.2021.3105523
   Huang HL, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P348, DOI 10.1109/MIPR49039.2020.00077
   Huang ZJ, 2019, INT SYM NETWO COMP, DOI 10.1109/isncc.2019.8909170
   Jin J, 2022, IEEE T PATTERN ANAL, V44, P1819, DOI 10.1109/TPAMI.2020.3026039
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752
   Liu M, 2017, IEEE T BROADCAST, V63, P71, DOI 10.1109/TBC.2016.2597545
   Liu XK, 2015, IEEE T IMAGE PROCESS, V24, P4847, DOI 10.1109/TIP.2015.2469140
   Liu Y, 2023, IEEE T CIRC SYST VID, V33, P4435, DOI 10.1109/TCSVT.2023.3237702
   Lv XQ, 2021, IEEE T VIS COMPUT GR, V27, P3597, DOI 10.1109/TVCG.2020.2982158
   Meng CL, 2021, IEEE T MULTIMEDIA, V24, P3193, DOI 10.1109/TMM.2021.3096071
   Meng CL, 2020, IEEE SIGNAL PROC LET, V27, P525, DOI 10.1109/LSP.2020.2982060
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Min XK, 2018, SIGNAL PROCESS, V145, P127, DOI 10.1016/j.sigpro.2017.10.025
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Ni ZK, 2018, IEEE T IMAGE PROCESS, V27, P4516, DOI 10.1109/TIP.2018.2839890
   Paudyal P, 2019, IEEE T BROADCAST, V65, P152, DOI 10.1109/TBC.2019.2892092
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Shan L, 2019, IEEE ACCESS, V7, P127217, DOI 10.1109/ACCESS.2019.2940093
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi LK, 2020, IEEE T CIRC SYST VID, V30, P4114, DOI 10.1109/TCSVT.2019.2955011
   Shi LK, 2019, IEEE IMAGE PROC, P3781, DOI [10.1109/icip.2019.8803559, 10.1109/ICIP.2019.8803559]
   Shi LK, 2018, IEEE IMAGE PROC, P41, DOI 10.1109/ICIP.2018.8451077
   Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tian Y, 2020, IEEE T IMAGE PROCESS, V29, P7945, DOI 10.1109/TIP.2020.3008856
   Tian Y, 2018, J VIS COMMUN IMAGE R, V57, P212, DOI 10.1016/j.jvcir.2018.11.005
   V. Q. E. Group, 2000, P VQEG M, P1
   Viola I, 2018, INT WORK QUAL MULTIM, P189
   Wang HY, 2021, IEEE SENS J, V21, P13417, DOI 10.1109/JSEN.2021.3065374
   Wang SQ, 2018, IEEE T CIRC SYST VID, V28, P1, DOI 10.1109/TCSVT.2016.2602764
   Wang YQ, 2023, IEEE T PATTERN ANAL, V45, P425, DOI 10.1109/TPAMI.2022.3152488
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu GC, 2019, IEEE T PATTERN ANAL, V41, P1681, DOI 10.1109/TPAMI.2018.2845393
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Wu JJ, 2018, IEEE ACCESS, V6, P12493, DOI 10.1109/ACCESS.2018.2798573
   Wu JJ, 2016, INFORM SCIENCES, V351, P18, DOI 10.1016/j.ins.2016.02.043
   Wu JJ, 2015, IEEE T IMAGE PROCESS, V24, P4602, DOI 10.1109/TIP.2015.2460467
   Xiang JJ, 2023, IEEE T MULTIMEDIA, V25, P457, DOI 10.1109/TMM.2021.3127398
   Xiang JJ, 2021, IEEE T CIRC SYST VID, V31, P2575, DOI 10.1109/TCSVT.2020.3030049
   Yan JB, 2020, IEEE T IMAGE PROCESS, V29, P7443, DOI 10.1109/TIP.2020.3003218
   Yue GH, 2019, IEEE T IMAGE PROCESS, V28, P2075, DOI 10.1109/TIP.2018.2875913
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
   Zhu WH, 2019, IEEE T MULTIMEDIA, V21, P2334, DOI 10.1109/TMM.2019.2902484
NR 59
TC 2
Z9 2
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 929
EP 943
DI 10.1109/TMM.2023.3273855
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700055
DA 2024-08-05
ER

PT J
AU Zhao, ZW
   Liu, B
   Lu, Y
   Chu, Q
   Yu, NH
   Chen, CW
AF Zhao, Zhiwei
   Liu, Bin
   Lu, Yan
   Chu, Qi
   Yu, Nenghai
   Chen, Chang Wen
TI Joint Identity-Aware Mixstyle and Graph-Enhanced Prototype for
   Clothes-Changing Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Prototypes; Training; Pedestrians; Task analysis; Robustness;
   Measurement; Clothes-changing person Re-ID; graph-enhanced prototype;
   identity-aware mixstyle; long-term; robustness
AB In recent years, considerable progress has been witnessed in the person re-identification (Re-ID). However, in a more realistic long-term scenario, the appearance shift arising from the clothes-changing inevitably deteriorates the conventional methods that heavily depend on the clothing color. Although the current clothes-changing person Re-ID methods introduce external human knowledge (i.e, contour, mask) and sophisticated feature decoupling strategy to alleviate the clothing shift, they still face the risk of overfitting to clothing due to the limited clothing diversity of training set. To more efficiently and effectively promote the clothes-irrelevant feature learning, we present a novel joint Identity-aware Mixstyle and Graph-enhanced Prototype method for clothes-changing person Re-ID. Specifically, by treating the cloth-changing as fine-grained domain/style shift, the identity-aware mixstyle (IMS) is proposed from the perspective of domain generalization, which mixes the instance-level feature statistics of samples within each identity to synthesize novel and diverse clothing styles, while retaining the correspondence between synthesized samples and latent label space. By incorporating the IMS module, the more diverse styles can be exploited to train a clothing-shift robust model. To further reduce the feature discrepancy caused by clothing variations, the graph-enhanced prototype constraint (GEP) module is proposed to explore the graph similarity structure of style-augmented samples across memory bank to build informative and robust prototypes, which serve as powerful exemplars for better clothing-irrelevant metric learning. The two modules are integrated into a joint learning framework and benefit each other. The extensive experiments conducted on clothes-changing person Re-ID datasets validate the superiority and effectiveness of our method. In addition, our method also shows good universality and corruption robustness on other Re-ID tasks.
C1 [Zhao, Zhiwei; Liu, Bin; Lu, Yan; Chu, Qi; Yu, Nenghai] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230052, Peoples R China.
   [Zhao, Zhiwei; Liu, Bin; Lu, Yan; Chu, Qi; Yu, Nenghai] Chinese Acad Sci, Key Lab Electromagnet Space Informat, Beijing 100864, Peoples R China.
   [Chen, Chang Wen] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Hong Kong Polytechnic
   University
RP Liu, B (corresponding author), Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230052, Peoples R China.
EM zwzhao98@mail.ustc.edu.cn; flowice@ustc.edu.cn;
   luyan17@mail.ustc.edu.cn; qchu@ustc.edu.cn; ynh@ustc.edu.cn;
   changwen.chen@polyu.edu.hk
RI Zhao, Zhiwei/KRQ-2715-2024
OI Lu, Yan/0009-0002-1449-5174; Chen, Chang Wen/0000-0002-6720-234X
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai Y, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P474
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P4285, DOI 10.1109/TMM.2021.3114539
   Chen JX, 2021, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR46437.2021.00805
   Chen M., 2021, P NEUR INF PROC SYST, P1
   Cui ZY, 2023, IEEE T CIRC SYST VID, V33, P4415, DOI 10.1109/TCSVT.2023.3241988
   Eom C., 2021, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), V44, P8975
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   Guan'an Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P275, DOI 10.1007/978-3-030-58598-3_17
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A., 2017, ARXIV
   Hong PX, 2021, PROC CVPR IEEE, P10508, DOI 10.1109/CVPR46437.2021.01037
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11875, DOI 10.1109/ICCV48922.2021.01168
   Jia XM, 2022, LECT NOTES COMPUT SC, V13142, P121, DOI 10.1007/978-3-030-98355-0_11
   Jin X, 2020, Arxiv, DOI arXiv:2006.12009
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11165
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li SC, 2019, IEEE I CONF COMP VIS, P6101, DOI 10.1109/ICCV.2019.00620
   Li YJ, 2021, IEEE WINT CONF APPL, P2431, DOI 10.1109/WACV48630.2021.00248
   Liu FY, 2019, IEEE I CONF COMP VIS, P6638, DOI 10.1109/ICCV.2019.00674
   Liu XC, 2016, IEEE INT CON MULTI
   Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Qian Xuelin, 2020, PROC ASIAN C COMPUT, P71
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Shu XJ, 2022, IEEE T CIRC SYST VID, V32, P4390, DOI 10.1109/TCSVT.2021.3128214
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wan FB, 2020, IEEE COMPUT SOC CONF, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang JD, 2023, IEEE T KNOWL DATA EN, V35, P8052, DOI 10.1109/TKDE.2022.3178128
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xu P., 2021, arXiv
   Xu Wanlu, 2021, PROC 13 INT JOINT C, P1201, DOI DOI 10.24963/IJCAI
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu SJ, 2020, PROC CVPR IEEE, P3397, DOI 10.1109/CVPR42600.2020.00346
   Yue XY, 2019, IEEE I CONF COMP VIS, P2100, DOI 10.1109/ICCV.2019.00219
   Zhang H., 2018, INT C LEARNING REPRE
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou Kaiyang, 2021, P INT C LEARN REPR
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
NR 54
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3457
EP 3468
DI 10.1109/TMM.2023.3311143
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200009
DA 2024-08-05
ER

PT J
AU Kim, NJ
   Kim, H
AF Kim, Nam Joon
   Kim, Hyun
TI Trunk Pruning: Highly Compatible Channel Pruning for Convolutional
   Neural Networks Without Fine-Tuning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional Neural Network (CNN); Pruning; Regularization; Fine-Tuning
ID MODEL
AB Channel pruning can efficiently reduce the computation and memory footprint within a reasonable accuracy drop by removing unnecessary channels from convolutional neural networks (CNNs). Among the various channel pruning approaches, sparsity training is the most popular because of its convenient implementation and end-to-end training. It automatically identifies the optimal network structures by applying regularization to parameters. Although this sparsity training has achieved a remarkable performance in terms of the trade-off between accuracy and network size reduction, it needs to be accompanied by a time-consuming fine-tuning process. Moreover, although activation functions with high performance are being continuously developed, the existing sparsity training does not display remarkable scalability for these new activation functions. To address these problems, this study proposes a novel pruning method, trunk pruning, which can produce a compact network by minimizing the accuracy drop during inference even without the fine-tuning process. In the proposed method, one kernel of the next convolutional layer absorbs all the information of the kernels to be pruned, considering the effects of the batch normalization (BN) shift parameters remaining after the sparsity training. Therefore, it is possible to eliminate the fine-tuning process because trunk pruning can effectively reproduce the output of the unpruned network after the sparsity training by removing the pruning loss. Furthermore, because trunk pruning is a technique that can effectively control only the shift parameters of the BN in the CONV layer, it has the significant advantage of being compatible with all BN-based sparsity training schemes and can address various activation functions.
C1 [Kim, Nam Joon; Kim, Hyun] Seoul Natl Univ Sci & Technol, Res Ctr Elect & Informat Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.
C3 Seoul National University of Science & Technology
RP Kim, H (corresponding author), Seoul Natl Univ Sci & Technol, Res Ctr Elect & Informat Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.
EM rlarla2626@seoultech.ac.kr; hyunkim@seoultech.ac.kr
FU Ministry of Science and ICT, South Korea
FX No Statement Available
CR Bailin Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P639, DOI 10.1007/978-3-030-58536-5_38
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Cai H., 2020, PROC 8 INT CONFLEARN
   Choi J, 2019, IEEE I CONF COMP VIS, P502, DOI 10.1109/ICCV.2019.00059
   Choi J, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P16, DOI [10.1109/AICAS48895.2020.9073907, 10.1109/aicas48895.2020.9073907]
   Ding X., 2019, P INT C MACH LEARN, P1607
   Ding XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4490, DOI 10.1109/ICCV48922.2021.00447
   Ding XH, 2019, PROC CVPR IEEE, P4938, DOI 10.1109/CVPR.2019.00508
   Ding Xiaohan, 2019, Advances in Neural Information Processing Systems (NeurIPS), P6382
   Nguyen DT, 2021, IEEE T CIRC SYST VID, V31, P2450, DOI 10.1109/TCSVT.2020.3020569
   Nguyen DT, 2019, IEEE T VLSI SYST, V27, P1861, DOI 10.1109/TVLSI.2019.2905242
   Nguyen DT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351021
   Frankle J., 2019, P 7 INT C LEARN REPR
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo JY, 2021, IEEE T CIRC SYST VID, V31, P1114, DOI 10.1109/TCSVT.2020.2996231
   Han S., 2017, PROC 5 INT C LEARN R
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2023, IEEE T NEUR NET LEAR, V34, P8044, DOI 10.1109/TNNLS.2022.3149332
   He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234
   He Y, 2020, PROC CVPR IEEE, P2006, DOI 10.1109/CVPR42600.2020.00208
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Zehao., 2018, Data-driven sparse structure selection for deep neural networks, P304
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Kang HJ, 2020, IEEE T CIRC SYST VID, V30, P2093, DOI 10.1109/TCSVT.2019.2911674
   Kang M., 2020, PR MACH LEARN RES, P5122
   Kim NJ, 2023, IEEE T MULTIMEDIA, V25, P5279, DOI 10.1109/TMM.2022.3189496
   Kim S, 2021, IEEE ACCESS, V9, P20828, DOI 10.1109/ACCESS.2021.3054879
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Li H, 2016, INT C LEARNING REPRE
   Lin MB, 2022, IEEE T NEUR NET LEAR, V33, P7357, DOI 10.1109/TNNLS.2021.3084856
   Lin MB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P673
   Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160
   Liu J, 2022, IEEE T PATTERN ANAL, V44, P4035, DOI 10.1109/TPAMI.2021.3066410
   Liu M., 2019, INT C LEARN REPRESEN, P1
   Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339D\, 10.1109/ICCV.2019.00339]
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Misra D., 2019, BRIT MACH VIS C, DOI DOI 10.48550/ARXIV.1908.08681
   Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152
   Peng HY, 2019, PR MACH LEARN RES, V97
   Ramachandran P, 2017, Arxiv, DOI arXiv:1710.05941
   Redmon J., 2018, CoRR
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Wang CY, 2021, PROC CVPR IEEE, P13024, DOI 10.1109/CVPR46437.2021.01283
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang H., 2021, PROC INT C LEARN REP, P1
   Wang JW, 2022, IEEE T MULTIMEDIA, V24, P230, DOI 10.1109/TMM.2021.3050057
   Wang ZZ, 2020, IEEE T MULTIMEDIA, V22, P2126, DOI 10.1109/TMM.2019.2950523
   Wang ZD, 2022, IEEE INTERNET THINGS, V9, P24506, DOI 10.1109/JIOT.2022.3190873
   Xu YH, 2020, IEEE T MULTIMEDIA, V22, P1874, DOI 10.1109/TMM.2019.2949857
   Xuefei Ning, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P592, DOI 10.1007/978-3-030-58580-8_35
   Yang TJ, 2018, LECT NOTES COMPUT SC, V11214, P289, DOI 10.1007/978-3-030-01249-6_18
   Ye X., 2018, 6 INT C LEARN REPRES
   You Z., 2019, ADV NEURAL INFORM PR, P2133
   Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958
   Zhao CL, 2019, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2019.00289
   Zhuang T., 2020, NEURIPS, P9865
NR 63
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5588
EP 5599
DI 10.1109/TMM.2023.3338052
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800009
DA 2024-08-05
ER

PT J
AU Li, D
   Rahardja, S
AF Li, Di
   Rahardja, Susanto
TI Learning Deep Representations for Photo Retouching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive learning; deep learning; generative adversarial nets; image
   enhancement; unsupervised learning
ID ENHANCEMENT
AB Photo enhancement is a long-standing and challenging problem in image processing community. Despite having witnessed significant achievements in recent years, many of them are built upon supervised learning theories and thus required expertise in constructing a huge collection of paired data, which is well-known to be a problem as the acquisition of such data in real life can be impractical. We address this issue by proposing a multi-scale GAN framework that can be trained in an unsupervised fashion. Notably, we unify the design principle of the generator and discriminator in our framework so as to maximize the ability to learn deep latent representations. Specifically, rather than maintaining the content consistency through complicated two-way loss, we present a one-way loss that measures the content distance between multi-scale latent representations of inputs and outputs to speed up the training by 1.7x. Furthermore, we redesign the discriminator into a multi-scale-multi-stage manner to strengthen the adversarial learning, where the multiple latent features with varying scales are produced by the main discriminator and these features are then sent to auxiliary discriminators for final recognition. Extensive experiments have been conducted in the well-known MIT-Adobe-fivek and HDR+ datasets, and the results demonstrated that the proposed multi-scale representation learning framework shows outstanding performance in photo enhancement task.
C1 [Li, Di; Rahardja, Susanto] Northwestern Polytech Univ, Ctr Intelligent Acoust & Immers Commun, Sch Marine Sci & Technol, Xian 710072, Peoples R China.
   [Rahardja, Susanto] Singapore Inst Technol, Singapore 138683, Singapore.
C3 Northwestern Polytechnical University; Singapore Institute of Technology
RP Rahardja, S (corresponding author), Northwestern Polytech Univ, Ctr Intelligent Acoust & Immers Commun, Sch Marine Sci & Technol, Xian 710072, Peoples R China.
EM xslidi@mail.nwpu.edu.cn; susantorahardja@ieee.org
CR Abdal Y., 2020, P IEEE CVF C COMP VI, P8296
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gonzalez R.C., 2018, Digital Image Processing
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Kim H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4439, DOI 10.1109/ICCV48922.2021.00442
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Kosugi S, 2020, AAAI CONF ARTIF INTE, V34, P11296
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Liu S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P82, DOI 10.1145/3123266.3123431
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Ni ZK, 2020, IEEE T IMAGE PROCESS, V29, P9140, DOI 10.1109/TIP.2020.3023615
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shao XN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6526, DOI 10.1109/ICCV48922.2021.00648
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Sun YL, 2020, IEEE T INF FOREN SEC, V15, P2679, DOI 10.1109/TIFS.2020.2975921
   Teterwak P, 2019, IEEE I CONF COMP VIS, P10520, DOI 10.1109/ICCV.2019.01062
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4086, DOI 10.1109/ICCV48922.2021.00407
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 36
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3153
EP 3163
DI 10.1109/TMM.2023.3307903
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200025
DA 2024-08-05
ER

PT J
AU Li, P
   Zhang, CH
   Xu, XH
AF Li, Ping
   Zhang, Chenhan
   Xu, Xianghua
TI Fast Fourier Inception Networks for Occluded Video Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamics; Convolutional codes; Task analysis; Spatiotemporal phenomena;
   Predictive models; Streaming media; Decoding; Video prediction;
   occlusion; temporal dynamics; inpainting; Fourier transform
ID TIME
AB Video prediction is a pixel-level task that generates future frames by employing the historical frames. There often exist continuous complex motions, such as object overlapping and scene occlusion in video, which poses great challenges to this task. Previous works either fail to well capture the long-term temporal dynamics or do not handle the occlusion masks. To address these issues, we develop the fully convolutional Fast Fourier Inception Networks for video prediction, termed FFINet, which includes two primary components, i.e., the occlusion inpainter and the spatiotemporal translator. The former adopts the fast Fourier convolutions to enlarge the receptive field, such that the missing areas (occlusion) with complex geometric structures are filled by the inpainter. The latter employs the stacked Fourier transform inception module to learn the temporal evolution by group convolutions and the spatial movement by channel-wise Fourier convolutions, which captures both the local and the global spatiotemporal features. This encourages generating more realistic and high-quality future frames. To optimize the model, the recovery loss is imposed to the objective, i.e., minimizing the mean square error between the ground-truth frame and the recovery frame. Both quantitative and qualitative experimental results on five benchmarks, including Moving MNIST, TaxiBJ, Human3.6 M, Caltech Pedestrian, and KTH, have demonstrated the superiority of the proposed approach.
C1 [Li, Ping; Zhang, Chenhan; Xu, Xianghua] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Li, Ping] Guangdong Lab Artificial Intelligence & Digital Ec, Shenzhen 518132, Peoples R China.
C3 Hangzhou Dianzi University; Guangming Laboratory
RP Li, P (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
EM patriclouis.lee@gmail.com; zch2020@hdu.edu.cn; xhxu@hdu.edu.cn
OI Li, Ping/0000-0002-8515-7773
FU Zhejiang Provincial Natural Science Foundation of China
FX No Statement Available
CR Babacizadeh M., 2018, P INT C LEARN REPR
   Bei XZ, 2021, PROC CVPR IEEE, P902, DOI 10.1109/CVPR46437.2021.00096
   BRIGHAM EO, 1967, IEEE SPECTRUM, V4, P63, DOI 10.1109/MSPEC.1967.5217220
   Byeon W, 2018, LECT NOTES COMPUT SC, V11220, P781, DOI 10.1007/978-3-030-01270-0_46
   Castrejon L, 2019, IEEE I CONF COMP VIS, P7607, DOI 10.1109/ICCV.2019.00770
   Chang Z, 2021, ADV NEUR IN, V34
   Chang Z, 2023, IEEE T MULTIMEDIA, V25, P2354, DOI 10.1109/TMM.2022.3146721
   Chang Z, 2022, PROC CVPR IEEE, P13926, DOI 10.1109/CVPR52688.2022.01356
   Chen G, 2022, PROC CVPR IEEE, P10718, DOI 10.1109/CVPR52688.2022.01046
   Cheng ZQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3272, DOI 10.1145/3503161.3547943
   Chi L., 2020, ADV NEURAL INF PROCE, V33, P4479, DOI DOI 10.5555/3495724.3496100
   De Brabandere B, 2016, ADV NEUR IN, V29
   Denton Emily, 2018, P MACHINE LEARNING R, V80
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Gao H, 2019, IEEE I CONF COMP VIS, P9005, DOI 10.1109/ICCV.2019.00910
   Gao XJ, 2021, IEEE INT C INT ROBOT, P5908, DOI 10.1109/IROS51168.2021.9636874
   Gao ZY, 2022, PROC CVPR IEEE, P3160, DOI 10.1109/CVPR52688.2022.00317
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geng D, 2022, PROC CVPR IEEE, P3355, DOI 10.1109/CVPR52688.2022.00336
   Goodfellow I., 2014, ADV NEURAL INF PROCE, P1724
   Guen V. L., 2020, P IEEE COMP SOC C CO, P11474, DOI 10.1109/CVPR42600.2020.01149
   Hao ZK, 2018, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2018.00819
   He JY, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P810
   Huang JH, 2022, LECT NOTES COMPUT SC, V13676, P546, DOI 10.1007/978-3-031-19787-1_31
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jin BB, 2020, PROC CVPR IEEE, P4553, DOI 10.1109/CVPR42600.2020.00461
   Katznelson Y., 2004, An introduction to harmonic analysis
   Kim N, 2021, IEEE T MULTIMEDIA, V23, P3986, DOI 10.1109/TMM.2020.3035281
   Kingma D.P., 2015, PROC INT C LEARN RE
   Kwon YH, 2019, PROC CVPR IEEE, P1811, DOI 10.1109/CVPR.2019.00191
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee A. X., 2019, P 7 INT C LEARN REPR
   Lee S, 2021, PROC CVPR IEEE, P3053, DOI 10.1109/CVPR46437.2021.00307
   Li C., 2023, P IEEE INT C AC SPEE, P1
   Li Z, 2022, PROC CVPR IEEE, P17541, DOI 10.1109/CVPR52688.2022.01704
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCV.2017.478, 10.1109/ICCVW.2017.361]
   Lotter W., 2017, ICLR, DOI 10.48550/arXiv.1605.08104
   Oliu M, 2018, LECT NOTES COMPUT SC, V11218, P745, DOI 10.1007/978-3-030-01264-9_44
   Park S, 2021, AAAI CONF ARTIF INTE, V35, P2412
   Rakhimov R, 2021, VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL. 5: VISAPP, P101, DOI 10.5220/0010241801010112
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shi XJ, 2015, ADV NEUR IN, V28
   Smith LN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2520589
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Su Jiahao, 2020, arXiv:2002.09131, V33, P13714
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Villegas R., 2017, ICLR
   Wang Y., 2019, INT C LEARN REPR
   WANG Y, 2018, P INT C MACHINE LEAR, P5110
   Wang YB, 2023, IEEE T PATTERN ANAL, V45, P2208, DOI 10.1109/TPAMI.2022.3165153
   Wang YB, 2019, PROC CVPR IEEE, P9146, DOI 10.1109/CVPR.2019.00937
   Wang YB, 2017, ADV NEUR IN, V30
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZQ, 2023, IEEE T MULTIMEDIA, V25, P1161, DOI 10.1109/TMM.2021.3139743
   Weissenborn D., 2020, P INT C LEARN REPR, P1
   Wu HX, 2021, PROC CVPR IEEE, P15430, DOI 10.1109/CVPR46437.2021.01518
   Wu Y, 2022, PROC CVPR IEEE, P17793, DOI 10.1109/CVPR52688.2022.01729
   Xu JW, 2021, INT J COMPUT VISION, V129, P601, DOI 10.1007/s11263-020-01389-w
   Yang Z, 2022, Arxiv, DOI arXiv:2112.01085
   Ye ZP, 2023, IEEE T MULTIMEDIA, V25, P2033, DOI 10.1109/TMM.2022.3142387
   Yu W., 2020, P INT C LEARN REPR A
   Yu W, 2022, PROC CVPR IEEE, P3595, DOI 10.1109/CVPR52688.2022.00359
   Zhang JB, 2017, AAAI CONF ARTIF INTE, P1655
NR 67
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3418
EP 3429
DI 10.1109/TMM.2023.3310330
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200012
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, YS
   Liang, HJ
   Yu, R
AF Li, Yanshan
   Liang, Huajie
   Yu, Rui
TI BI-CAM: Generating Explanations for Deep Neural Networks Using Bipolar
   Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary reformation; class activation mapping (CAM); interpretation;
   point-wise mutual information (PMI)
ID MODEL
AB The higher requirements for deep neural networks are driving researchers to have a deeper understanding of the internals of neural networks. The class activation map (CAM) based methods can provide a convincing interpretation of the features extracted by the neural network from both visual and quantitative perspectives. However, the existing CAM methods do not take into account that the non-target region also contains target-related activation, which results in the generated saliency map containing noise from unrelated regions. In addition, the soft mask with continuous value not only contains more non-target regions for gradient-free CAM, but also causes the characteristics and distribution of the target region to be disturbed. This paper proposed a novel CAM method named Bipolar Information CAM (BI-CAM) to interpret convolutional neural networks (CNNs) and graph convolutional networks (GCNs). Firstly, dual-stream information is proposed to precisely quantify the relationship between the target region and the non-target region for an image/graph. Secondly, binary reformation is also proposed to generate a hard mask that can retain the original features and regions. Finally, we propose to use concise and effective Point-wise Mutual Information (PMI) to measure the quantitative relationship between the image and the local region with respect to the label. The results of the experiment show that the proposed BI-CAM achieves significantly better performance in the faithfulness evaluation from the perspectives of visualization and quantitative analysis than other competitive interpretation methods.
C1 [Li, Yanshan; Liang, Huajie; Yu, Rui] Shenzhen Univ, Inst Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Li, Yanshan; Liang, Huajie; Yu, Rui] Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
C3 Shenzhen University; Shenzhen University
RP Li, YS; Liang, HJ (corresponding author), Shenzhen Univ, Inst Intelligent Informat Proc, Shenzhen 518060, Peoples R China.; Li, YS; Liang, HJ (corresponding author), Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM lys@szu.edu.cn; lianghuajie2019@email.szu.edu.cn;
   yurui2020@email.szu.edu.cn
RI Yu, Rui/KRO-9318-2024
OI Yu, Rui/0000-0003-1782-6258
FU National Natural Science Foundation of China
FX No Statement Available
CR Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Baldassarre F, 2019, Arxiv, DOI arXiv:1905.13686
   Bau D, 2020, P NATL ACAD SCI USA, V117, P30071, DOI 10.1073/pnas.1907375117
   Binder A, 2016, LECT NOTES COMPUT SC, V9887, P63, DOI 10.1007/978-3-319-44781-0_8
   Bruna J., 2014, ABS13126203 CORR, P1, DOI DOI 10.48550/ARXIV.1312.6203
   Chang E., 2019, P 7 INT C LEARN REPR
   Cheng L, 2022, IEEE T IMAGE PROCESS, V31, P2529, DOI 10.1109/TIP.2022.3157149
   Cui XR, 2020, IEEE T MULTIMEDIA, V22, P1847, DOI 10.1109/TMM.2020.2976985
   Cui XR, 2019, IEEE T MULTIMEDIA, V21, P2263, DOI 10.1109/TMM.2019.2902099
   Dabkowski P, 2017, ADV NEUR IN, V30
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Huang Q., 2020, arXiv preprint arXiv:2001.06216
   Iandola S., 2017, arXiv
   Ji CJ, 2022, NEUROCOMPUTING, V493, P59, DOI 10.1016/j.neucom.2022.04.070
   Jung H., 2021, arXiv
   Jung H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1316, DOI 10.1109/ICCV48922.2021.00137
   Kipf T., 2016, ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai QX, 2021, IEEE T MULTIMEDIA, V23, P2086, DOI 10.1109/TMM.2020.3007321
   Li X., 2020, P INT CROSS DOM C MA, P57
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Lucic A., 2022, INT C ARTIFICIAL INT, V151, P4499
   Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008
   Omeiza Daniel, 2019, arXiv
   Petsiuk A., BRIT MACH VIS C
   Pope PE, 2019, PROC CVPR IEEE, P10764, DOI 10.1109/CVPR.2019.01103
   Qin Z., 2021, Informative class activation maps
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Ruiz A., 2021, P IEEE CVF INT C COM, P905
   Sattarzadeh S, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1775, DOI 10.1109/ICASSP39728.2021.9415064
   Schnake Thomas, 2020, Higher-order explanations of graph neural networks via relevant walks
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi XW, 2021, INT C PATT RECOG, P10289, DOI 10.1109/ICPR48806.2021.9412980
   Simonyan K., 2014, 13126034 ARXIV
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vu M., 2020, ADV NEURAL INFORM PR, V33, p12 225
   Wagner J, 2019, PROC CVPR IEEE, P9089, DOI 10.1109/CVPR.2019.00931
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Wang YL, 2020, IEEE T MULTIMEDIA, V22, P1796, DOI 10.1109/TMM.2019.2949872
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang H, 2022, IEEE T IMAGE PROCESS, V31, P164, DOI 10.1109/TIP.2021.3129117
   Yi JH, 2021, Arxiv, DOI arXiv:2009.11150
   Ying R, 2019, 33 C NEURAL INFORM P, V32
   Zhang QL, 2021, Arxiv, DOI arXiv:2103.13859
   Zhang QS, 2017, AAAI CONF ARTIF INTE, P2898
   Zhang Y, 2021, AIES '21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P1042, DOI 10.1145/3461702.3462562
   Zheng Q, 2022, LECT NOTES COMPUT SC, V13672, P459, DOI 10.1007/978-3-031-19775-8_27
   Zhou BL, 2019, IEEE T PATTERN ANAL, V41, P2131, DOI 10.1109/TPAMI.2018.2858759
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 51
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 568
EP 580
DI 10.1109/TMM.2023.3267884
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000031
DA 2024-08-05
ER

PT J
AU Liang, QH
   Wang, Y
AF Liang, Qihao
   Wang, Ye
TI Drawlody: Sketch-Based Melody Creation With Enhanced Usability and
   Interpretability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Usability; Feature extraction; Rhythm; Computational modeling;
   Transformers; Market research; Human computer interaction; Music
   generation; interactive music creation; melody contour
ID MUSIC
AB Sketch-based melody creation systems enable people to compose melodies by converting human-sketched melody contours into coherent melodies that fit the depicted contours. This remains one of the most intuitive approaches to interactive music creation. However, previous studies are still stagnating in limitations regarding usability and interpretability, which hinders effective interactions between people and AI. For one thing, these studies entail additional complex musical conditions as auxiliary inputs (e.g. chord progressions, contextual melodies, and predetermined rhythms), supporting only fixed-length and rule-based melody generation. This makes existing systems less usable, with generated melodies lacking diversity and coherence. Moreover, users without enough musical expertise might find it difficult to define appropriate inputs and to interpret the role of these inputs in guiding melody generation. To address these limitations, we present Drawlody, a novel sketch-based melody creation system with enhanced usability and interpretability. Specifically, Drawlody simplifies user input requirements by excluding all complex musical conditions, using only a simplified melody contour representation named Generalised Melody Contour (GMC) as input. This simplification clarifies the role of user controls, making the system more usable for people without musical training. To guide coherent melody generation from GMC, we propose FlexMIDI music representation, which simulates the tonal structure of melodies and faithfully explains how human-sketched contours guide melody generation. We employ a CNN-Transformer-based architecture as the foundation model to achieve arbitrary-length melody generation. Drawlody is evaluated by both objective and subjective music quality studies, as well as a usability and interpretability study. The results support its enhanced usability, interpretability, and high-quality melody generation capabilities.
C1 [Liang, Qihao; Wang, Ye] Natl Univ Singapore, Sch Comp, Singapore 118425, Singapore.
C3 National University of Singapore
RP Wang, Y (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 118425, Singapore.
EM qihao.liang@u.nus.edu; wangye@comp.nus.edu.sg
RI Wang, Ye/KGL-6405-2024
OI Wang, Ye/0000-0002-0123-1260
FU Ministry of Education - Singapore
FX No Statement Available
CR Amershi S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300233
   [Anonymous], 2010, PROC ISMIR
   Bao CH, 2023, IEEE T MULTIMEDIA, V25, P3602, DOI 10.1109/TMM.2022.3163543
   Bazin T., 2019, P 10 INT C COMP CREA, P89
   Benedetto A, 2020, J COGNITIVE NEUROSCI, V32, P187, DOI 10.1162/jocn_a_01436
   Benetatos C., 2022, Trans. Int. Soc. Music Inf. Retrieval, V5, P141
   Bertin-Mahieux T., 2011, P 12 INT C MUS INF R
   Bock Sebastian, 2016, ISMIR, P255, DOI [10.5281/zenodo.1415836, DOI 10.5281/ZENODO.1415836]
   Chen K, 2019, 2019 INTERNATIONAL WORKSHOP ON MULTILAYER MUSIC REPRESENTATION AND PROCESSING (MMRP 2019), P77, DOI [10.1109/MMRP.2019.00022, 10.1109/MMRP.2019.8665362]
   Choi K, 2016, Arxiv, DOI arXiv:1604.05358
   Choi K, 2021, IEEE ACCESS, V9, P42071, DOI 10.1109/ACCESS.2021.3065831
   Clauhs M., 2020, Journal of Popular Music Education, V4, P237, DOI [DOI 10.1386/JPME000271, DOI 10.1386/JPME_00027_1]
   Clay A., 2012, P 12 INT C NEW INT M
   Dadman S, 2022, IEEE ACCESS, V10, P125679, DOI 10.1109/ACCESS.2022.3225689
   Dai SQ, 2022, J NEW MUSIC RES, V51, P69, DOI 10.1080/09298215.2023.2166848
   Dai Shuqi, 2021, P 22 INT SOC MUSIC I, P143
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Di SZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2037, DOI 10.1145/3474085.3475195
   Disch S, 2011, INT CONF ACOUST SPEE, P29
   Dong YZ, 2019, IEEE T MULTIMEDIA, V21, P3150, DOI 10.1109/TMM.2019.2918739
   Doshi-Velez F, 2017, Arxiv, DOI [arXiv:1702.08608, DOI 10.48550/ARXIV.1702.08608, 10.48550/arXiv.1702.08608]
   Dowling W. J., 2014, Psychology and Music: The Understanding of Melody and Rhythm
   Dua Mohit, 2020, Procedia Computer Science, V171, P465, DOI 10.1016/j.procs.2020.04.049
   Dyer SA, 2001, IEEE INSTRU MEAS MAG, V4, P44, DOI 10.1109/5289.911175
   Ens Jeffrey, 2020, arXiv, DOI [10.48550/arXiv.2008.06048, DOI 10.48550/ARXIV.2008.06048]
   Gwenaelle C., 2015, P 3 INT C HUM AG INT, P213
   Hadjeres G, 2020, NEURAL COMPUT APPL, V32, P995, DOI 10.1007/s00521-018-3868-4
   Han DH, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-021-0569-4
   Hejna D., 1991, Tech. Rep.
   Hsiao WY, 2021, AAAI CONF ARTIF INTE, V35, P178
   Hu ZJ, 2023, IEEE T MULTIMEDIA, V25, P2296, DOI 10.1109/TMM.2022.3146002
   Hua Zhu, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P345, DOI 10.1109/CSSE.2008.1203
   Huang C. -Z. A., 2019, P 7 INT C LEARN REPR
   Huang YSA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1180, DOI 10.1145/3394171.3413671
   Hung H., 2021, ISMIR, P318
   Ji SL, 2024, IEEE T MULTIMEDIA, V26, P1076, DOI 10.1109/TMM.2023.3276177
   Kitahara T., 2017, P MUS TECHN SWING 13, P509
   Kitahara T., 2022, P 4 ACM INT C MULT A, P1
   Kumar Sandeep, 2022, 2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS), P294, DOI 10.1109/ICTACS56270.2022.9988652
   Large EW, 2023, FRONT COMPUT NEUROSC, V17, DOI 10.3389/fncom.2023.1151895
   Li SY, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11051111
   Martineau J., 2008, The Elements of Music: Melody, Rhythm, and Harmony
   Namgyal T, 2023, Arxiv, DOI arXiv:2305.11605
   Ou LS, 2023, PROCEEDINGS OF THE 61ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL 2023, VOL 1, P447
   Pangestu M A., 2021, P IEEE INT C COMP SC, P1
   Pati A., 2019, PROC 20 INT SOC MUSI, P343
   Payne C, 2019, MUSENET
   Prétet L, 2023, IEEE T MULTIMEDIA, V25, P2898, DOI 10.1109/TMM.2022.3152598
   Rangarajan Rohit, 2015, 2015 Tenth International Conference on Digital Information Management (ICDIM). Proceedings, P85, DOI 10.1109/ICDIM.2015.7381853
   Ren Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1198, DOI 10.1145/3394171.3413721
   Sajad S., 2021, P IEEE INT C INN COM, P1
   Schenker H., 2001, New Musical Theories and Fantasies, V1
   Shih YJ, 2023, IEEE T MULTIMEDIA, V25, P3495, DOI 10.1109/TMM.2022.3161851
   Simon I, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P725
   Thalmann F, 2020, IEEE T MULTIMEDIA, V22, P2645, DOI 10.1109/TMM.2019.2961207
   Tianyu Jiang, 2019, 2019 IEEE 2nd International Conference on Electronics Technology (ICET), P564, DOI 10.1109/ELTECH.2019.8839399
   Tsai TJ, 2020, IEEE T MULTIMEDIA, V22, P3115, DOI 10.1109/TMM.2020.2973831
   Vaswani A, 2017, ADV NEUR IN, V30
   VIDYAMURTHY G, 1992, COMPUT MUSIC J, V16, P45, DOI 10.2307/3680715
   Smith JW, 2022, IEEE T MULTIMEDIA, V24, P2315, DOI 10.1109/TMM.2021.3079695
   Wang DJ, 2022, IEEE T MULTIMEDIA, V24, P4170, DOI 10.1109/TMM.2021.3114545
   Wang ZH, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1057, DOI 10.1145/3503161.3548368
   Wei SQ, 2022, INT CONF ACOUST SPEE, P186, DOI 10.1109/ICASSP43922.2022.9747817
   Whig P., 2022, Demystifying Federated Learning for Blockchain and Industrial Internet of Things, P94
   Wu XX, 2016, IEEE T MULTIMEDIA, V18, P1305, DOI 10.1109/TMM.2016.2557722
   Xu W., 2019, Interactions, V26, P42, DOI DOI 10.1145/3328485
   Xu W, 2023, INT J HUM-COMPUT INT, V39, P494, DOI [10.1109/IECON49645.2022.9968424, 10.1080/10447318.2022.2041900]
   Yang LC, 2020, NEURAL COMPUT APPL, V32, P4773, DOI 10.1007/s00521-018-3849-7
   Yang YH, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168754
   Zhang KJ, 2023, Arxiv, DOI arXiv:2301.04488
   Zhao JW, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P5878
   Zhao K, 2019, PROCEEDINGS OF 2019 IEEE 3RD INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC 2019), P2039, DOI [10.1109/ITNEC.2019.8729266, 10.1109/itnec.2019.8729266]
NR 72
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7074
EP 7088
DI 10.1109/TMM.2024.3360695
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RE1G6
UT WOS:001225895800002
DA 2024-08-05
ER

PT J
AU Liu, HQ
   Chen, CLP
   Gong, XR
   Zhang, T
AF Liu, Haiqi
   Chen, C. L. Philip
   Gong, Xinrong
   Zhang, Tong
TI Robust Saliency-Aware Distillation for Few-Shot Fine-Grained Visual
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot fine-grained visual recognition; few-shot learning; saliency
   detection; mutual learning
ID NETWORK; ATTENTION; FILTER
AB Recognizing novel sub-categories with scarce samples is an essential and challenging research topic in computer vision. Existing literature addresses this challenge by employing local-based representation approaches, which may not sufficiently facilitate meaningful object-specific semantic understanding, leading to a reliance on apparent background correlations. Moreover, they primarily rely on high-dimensional local descriptors to construct complex embedding space, potentially limiting the generalization. To address the above challenges, this article proposes a novel model, Robust Saliency-aware Distillation (RSaD), for few-shot fine-grained visual recognition. RSaD introduces additional saliency-aware supervision via saliency detection to guide the model toward focusing on the intrinsic discriminative regions. Specifically, RSaD utilizes the saliency detection model to emphasize the critical regions of each sub-category, providing additional object-specific information for fine-grained prediction. RSaD transfers such information with two symmetric branches in a mutual learning paradigm. Furthermore, RSaD exploits inter-regional relationships to enhance the informativeness of the representation and subsequently summarize the highlighted details into contextual embeddings to facilitate the effective transfer, enabling quick generalization to novel sub-categories. The proposed approach is empirically evaluated on three widely used benchmarks, demonstrating its superior performance.
C1 [Liu, Haiqi; Chen, C. L. Philip; Gong, Xinrong; Zhang, Tong] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Liu, Haiqi; Chen, C. L. Philip; Gong, Xinrong; Zhang, Tong] Brain & Affect Cognit Res Ctr, Pazhou Lab, Guangzhou 510335, Peoples R China.
   [Chen, C. L. Philip; Zhang, Tong] Minist Educ Hlth Intelligent Percept & Paralleled, Engn Res Ctr, Guangzhou 510006, Peoples R China.
C3 South China University of Technology; Pazhou Lab
RP Zhang, T (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM tony@scut.edu.cn
RI Zhang, tong/IAP-2587-2023; Chen, C. L. Philip/O-2657-2016
OI Gong, Xinrong/0000-0001-5821-6283; Zhang, Tong/0000-0002-7025-6365
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen DF, 2022, PROC CVPR IEEE, P11923, DOI 10.1109/CVPR52688.2022.01163
   Chen DF, 2021, AAAI CONF ARTIF INTE, V35, P7028
   Chen RT, 2018, Isolating sources of disentanglement in variational autoencoders
   Chen W-Y, 2019, ARXIV
   Chen YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9042, DOI 10.1109/ICCV48922.2021.00893
   Chen ZS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P569, DOI 10.1109/ICCV48922.2021.00063
   DEUTSCH JA, 1963, PSYCHOL REV, V70, P80, DOI 10.1037/h0039515
   Ding YF, 2021, IEEE T IMAGE PROCESS, V30, P2826, DOI 10.1109/TIP.2021.3055617
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Duan K, 2012, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2012.6248089
   Fu CH, 2020, IEEE T GEOSCI REMOTE, V58, P8940, DOI 10.1109/TGRS.2020.2992301
   Gosselin PH, 2014, PATTERN RECOGN LETT, V49, P92, DOI 10.1016/j.patrec.2014.06.011
   Guo C, 2022, KNOWL-BASED SYST, V235, DOI 10.1016/j.knosys.2021.107651
   Han YQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3123666
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   He J, 2022, AAAI CONF ARTIF INTE, P852
   Hinton G., 2014, NEURIPS WORKSHOPS
   Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374
   Hou RB, 2019, ADV NEUR IN, V32
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Huang HX, 2022, IEEE T CIRC SYST VID, V32, P853, DOI 10.1109/TCSVT.2021.3065693
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Ji W., 2023, PROC CVPR VIS WORKSH
   Khosla A., 2011, CVPR WORKSH, V2
   Kirillov A., 2023, P IEEE CVF INT C COM, P4015
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Li XX, 2021, IEEE T IMAGE PROCESS, V30, P1318, DOI 10.1109/TIP.2020.3043128
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10553, DOI 10.1109/ICCV48922.2021.01040
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Snell J, 2017, ADV NEUR IN, V30
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Sorscher B, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2200800119
   Stanton S, 2021, 35 C NEURAL INFORM P, V34
   Sun HB, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5853, DOI 10.1145/3503161.3548308
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3560, DOI 10.1109/ICCV48922.2021.00356
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang CF, 2021, NEUROCOMPUTING, V466, P16, DOI 10.1016/j.neucom.2021.09.016
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang Y, 2019, Arxiv, DOI arXiv:1911.04623
   Wei X.-S., 2022, Adv. Neural Inf. Process. Syst., V35, P14489
   Wei XS, 2024, IEEE T PATTERN ANAL, V46, P2091, DOI 10.1109/TPAMI.2023.3333528
   Wei XS, 2023, IEEE T PATTERN ANAL, V45, P13904, DOI 10.1109/TPAMI.2023.3299563
   Wei XS, 2022, IEEE T PATTERN ANAL, V44, P8927, DOI 10.1109/TPAMI.2021.3126648
   Wei XS, 2019, IEEE T IMAGE PROCESS, V28, P6116, DOI 10.1109/TIP.2019.2924811
   Wertheimer D, 2021, PROC CVPR IEEE, P8008, DOI 10.1109/CVPR46437.2021.00792
   Wu YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P107, DOI 10.1145/3474085.3475532
   Xu JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8792, DOI 10.1109/ICCV48922.2021.00869
   Xu SL, 2022, AAAI CONF ARTIF INTE, P2911
   Yang X, 2020, IEEE T GEOSCI REMOTE, V58, P3667, DOI 10.1109/TGRS.2019.2959838
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Ye HJ, 2024, IEEE T PATTERN ANAL, V46, P1425, DOI 10.1109/TPAMI.2022.3160362
   Ying LF, 2022, IEEE T EMERG TOP COM, V10, P973, DOI 10.1109/TETC.2021.3057395
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang B, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2135, DOI 10.1145/3503161.3547961
   Zhang HG, 2023, IEEE T MULTIMEDIA, V25, P2111, DOI 10.1109/TMM.2022.3142955
   Zhang HG, 2019, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR.2019.00288
   Zhang HG, 2019, IEEE WINT CONF APPL, P1185, DOI 10.1109/WACV.2019.00131
   Zhang J, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104423
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhang Y, 2022, INT CONF ACOUST SPEE, P3234, DOI 10.1109/ICASSP43922.2022.9747591
   Zhao BR, 2022, PROC CVPR IEEE, P11943, DOI 10.1109/CVPR52688.2022.01165
   Zhao L., 2021, PROC IEEE INT C MULT, P1
   Zhao LL, 2022, NEUROCOMPUTING, V507, P412, DOI 10.1016/j.neucom.2022.08.028
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8382, DOI 10.1109/ICCV48922.2021.00829
   Zhu YH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1090
   Zou X., 2023, PROC 37 C NEURAL INF
NR 84
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7529
EP 7542
DI 10.1109/TMM.2024.3369870
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000009
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, Y
   Xu, Y
   Wu, PP
   Wang, WW
AF Liu, Yang
   Xu, Yong
   Wu, Peipei
   Wang, Wenwu
TI Labelled Non-Zero Diffusion Particle Flow SMC-PHD Filtering for
   Multi-Speaker Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual tracking; SMC-PHD filter; particle flow
ID RANDOM FINITE SETS; MULTITARGET; LOCALIZATION; CORPUS
AB Particle flow (PF) is a method originally proposed for single target tracking, and used recently to address the weight degeneracy problem of the sequential Monte Carlo probability hypothesis density (SMC-PHD) filter for audio-visual (AV) multi-speaker tracking, where the particle flow is calculated by using only the measurements near the particle, assuming that the target is detected, as in a recent method based on non-zero particle flow (NPF), i.e. the AV-NPF-SMC-PHD filter. This, however, can be problematic when occlusion happens and the occluded speaker may not be detected. To address this issue, we propose a new method where the labels of the particles are estimated using the likelihood function, and the particle flow is calculated in terms of the selected particles with the same labels. As a result, the particles associated with detected speakers and undetected speakers are distinguished based on the particle labels. With this novel method, named as AV-LPF-SMC-PHD, the speaker states can be estimated as the weighted mean of the labelled particles, which is computationally more efficient than using a clustering method as in the AV-NPF-SMC-PHD filter. The proposed algorithm is compared systematically with several baseline tracking methods using the AV16.3, AVDIAR and CLEAR datasets, and is shown to offer improved tracking accuracy with a lower computational cost.
C1 [Liu, Yang; Wu, Peipei; Wang, Wenwu] Univ Surrey, Ctr Vision Speech & Signal Proc, Guildford GU2 7XH, England.
   [Liu, Yang] Meta, Seattle, WA 98101 USA.
   [Xu, Yong] Tencent AI Lab, Seattle, WA 98004 USA.
C3 University of Surrey
RP Wang, WW (corresponding author), Univ Surrey, Ctr Vision Speech & Signal Proc, Guildford GU2 7XH, England.
EM yangliuav@gmail.com; lucayongxu@global.tencent.com; p.wu@surrey.ac.uk;
   w.wang@surrey.ac.uk
OI Wang, Wenwu/0000-0002-8393-5703
FU SIGNetS
FX No Statement Available
CR [Anonymous], 2007, Surveillance performance evaluation initiative (SPEVI) audiovisual people dataset
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Vo BN, 2014, IEEE T SIGNAL PROCES, V62, P6554, DOI 10.1109/TSP.2014.2364014
   Vo BT, 2013, IEEE T SIGNAL PROCES, V61, P3460, DOI 10.1109/TSP.2013.2259822
   Ban YT, 2021, IEEE T PATTERN ANAL, V43, P1761, DOI 10.1109/TPAMI.2019.2953020
   Ban YT, 2019, IEEE SIGNAL PROC LET, V26, P798, DOI 10.1109/LSP.2019.2908376
   Blackman SS, 2004, IEEE AERO EL SYS MAG, V19, P5, DOI 10.1109/MAES.2004.1263228
   Bromiley P., 2003, TINA VIS MEMO, V3, P1
   Brostrom Mikel, 2022, Real-time multi-camera multi-object tracker using YOLOv5 and StrongSORT with OSNet
   Bunch P, 2016, J AM STAT ASSOC, V111, P748, DOI 10.1080/01621459.2015.1038387
   Cameron AC, 1997, J ECONOMETRICS, V77, P329
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Daum F., 2011, Proc. SPIE, V7697, P92
   Daum F, 2008, PROC SPIE, V6969, DOI 10.1117/12.764909
   Daum F, 2014, PROC SPIE, V9091, DOI 10.1117/12.2044123
   Daum F, 2013, PROC SPIE, V8745, DOI 10.1117/12.2009363
   Daum F, 2012, PROC SPIE, V8393, DOI 10.1117/12.915183
   Deleforge A, 2015, IEEE-ACM T AUDIO SPE, V23, P718, DOI 10.1109/TASLP.2015.2405475
   DiBiase J. H., 2000, Ph.D. thesis
   Escudero P., 2015, P INT C PHON SCI, P1
   Gatica-Perez D, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P25
   Gebru ID, 2018, IEEE T PATTERN ANAL, V40, P1086, DOI 10.1109/TPAMI.2017.2648793
   Gebru ID, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P702, DOI 10.1109/ICCVW.2015.96
   Gehrig T, 2005, IEEE WORK APPL SIG, P118, DOI 10.1109/ASPAA.2005.1540183
   Hampapur I, 2005, IEEE SIGNAL PROC MAG, V22, P38
   Heng J, 2021, J ROY STAT SOC B, V83, P156, DOI 10.1111/rssb.12404
   Hoel P. G., 1960, Elementary Statistics
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Kadanoff L.P., 2000, Statistical physics: statics, dynamics and renormalization
   Kilic V, 2016, IEEE T MULTIMEDIA, V18, P2417, DOI 10.1109/TMM.2016.2599150
   Kim DY, 2017, INT CONF CONTR AUTO, P38, DOI 10.1109/ICCAIS.2017.8217590
   KONG A, 1994, J AM STAT ASSOC, V89, P278, DOI 10.2307/2291224
   Lathoud G, 2005, LECT NOTES COMPUT SC, V3361, P182
   Li TC, 2016, SIGNAL PROCESS, V119, P115, DOI 10.1016/j.sigpro.2015.07.013
   Li XF, 2019, IEEE J-STSP, V13, P88, DOI 10.1109/JSTSP.2019.2903472
   Li YP, 2016, INT CONF ACOUST SPEE, P3979, DOI 10.1109/ICASSP.2016.7472424
   Lin SF, 2020, INTERSPEECH, P3082, DOI 10.21437/Interspeech.2020-1969
   Liu Y, 2020, IEEE T MULTIMEDIA, V22, P934, DOI 10.1109/TMM.2019.2937185
   Liu Y, 2019, INT CONF ACOUST SPEE, P5197, DOI 10.1109/ICASSP.2019.8683399
   Liu Y, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4304, DOI 10.1109/ICASSP.2018.8461791
   Liu Y, 2017, INT CONF ACOUST SPEE, P4371, DOI 10.1109/ICASSP.2017.7952982
   Liu Y, 2017, LECT NOTES COMPUT SC, V10169, P344, DOI 10.1007/978-3-319-53547-0_33
   Lu WL, 2013, IEEE T PATTERN ANAL, V35, P1704, DOI 10.1109/TPAMI.2012.242
   Mahler R, 2007, IEEE T AERO ELEC SYS, V43, P1523, DOI 10.1109/TAES.2007.4441756
   Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119
   Minotto VP, 2015, IEEE T MULTIMEDIA, V17, P1694, DOI 10.1109/TMM.2015.2463722
   Okuma K, 2004, LECT NOTES COMPUT SC, V3021, P28, DOI 10.1007/978-3-540-24670-1_3
   Ooi MH, 2007, J CLIN MICROBIOL, V45, P1858, DOI 10.1128/JCM.01394-06
   Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179
   Qian XY, 2019, IEEE T MULTIMEDIA, V21, P2576, DOI 10.1109/TMM.2019.2902489
   Qian XY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3071, DOI 10.1109/ICASSP.2018.8461323
   Ristic B, 2011, IEEE T SIGNAL PROCES, V59, P3452, DOI 10.1109/TSP.2011.2140111
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Saucan AA, 2017, INT CONF ACOUST SPEE, P4381, DOI 10.1109/ICASSP.2017.7952984
   Sung Y, 2022, IEEE T AUTOM SCI ENG, V19, P2122, DOI 10.1109/TASE.2021.3073938
   Theodoridis S., 1999, Pattern recognition
   Thiollière R, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3179
   van der Merwe R, 2001, ADV NEUR IN, V13, P584
   Vermaak J, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P741, DOI 10.1109/ICCV.2001.937600
   Vo BN, 2006, IEEE T SIGNAL PROCES, V54, P4091, DOI 10.1109/TSP.2006.881190
   Vo BN, 2005, IEEE T AERO ELEC SYS, V41, P1224, DOI 10.1109/TAES.2005.1561884
   Welch G., 1995, An introduction to the kalman filter
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Yeo HS, 2015, MULTIMED TOOLS APPL, V74, P2687, DOI 10.1007/s11042-013-1501-1
   Zhao L., 2016, Proc. SPIE, V9842, P106
NR 66
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2544
EP 2559
DI 10.1109/TMM.2023.3301221
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100016
DA 2024-08-05
ER

PT J
AU Lyu, Y
   Zhang, H
   Li, Y
   Liu, HY
   Yang, YF
   Yuan, D
AF Lyu, Yixuan
   Zhang, Hong
   Li, Yan
   Liu, Hanyang
   Yang, Yifan
   Yuan, Ding
TI UEDG:Uncertainty-Edge Dual Guided Camouflage Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camouflage object detection; multi-auxiliary tasks fusion; uncertainty
   deduction
AB According to Darwinian evolutionary theory, numerous species in the wild have developed remarkable adaptive mechanisms, involving pattern rearrangement and environmental assimilation, to evade predators. These obfuscation strategies pose significant challenges for both individuals and algorithms when performing the Camouflage Object Detection (COD) task in complex and intricate scenarios. Inspired by human strategies in the COD task, which involve assigning uncertainties to the entire input and then focusing on highly uncertain areas with the aid of prior knowledge such as boundary information, we propose the Uncertainty-Edge Dual Guide (UEDG) architecture. UEDG effectively combines probabilistic-derived uncertainty and deterministic-derived edge information to accurately detect concealed objects. The architecture consists of two independent branches dedicated to uncertainty reasoning and edge inference, which are subsequently integrated into a feature fusion module utilizing recursion feedback and feature-reuse techniques. This novel COD framework leverages the benefits of Bayesian learning and convolution-based learning, resulting in a powerful multi-task guided approach. Extensive experiments conducted on four widely employed datasets demonstrate the superior performance of UEDG compared to 12 state-of-the-art approaches, while maintaining an acceptable level of computational complexity. Overall, UEDG presents a promising solution for addressing the challenges of COD in complex environments by combining evolutionary-inspired strategies with advanced computer vision techniques.
C1 [Lyu, Yixuan; Zhang, Hong; Li, Yan; Liu, Hanyang; Yuan, Ding] Beihang Univ, Sch Astronaut, Beijing 100191, Peoples R China.
   [Yang, Yifan] Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Yang, YF (corresponding author), Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China.
EM yixuan.lyu@nyu.edu; dmrzhang@buaa.edu.cn; yanliz@buaa.edu.cn;
   by2015102@buaa.edu.cn; stephenyoung@buaa.edu.cn; dyuan@buaa.edu.cn
OI Liu, Hanyang/0009-0009-8956-1245
FU National Natural Science Foundation of China
FX No Statement Available
CR BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   Bi HB, 2022, IEEE T CIRC SYST VID, V32, P5708, DOI 10.1109/TCSVT.2021.3124952
   Blundell C, 2015, PR MACH LEARN RES, V37, P1613
   Chen TY, 2022, KNOWL-BASED SYST, V248, DOI 10.1016/j.knosys.2022.108901
   Damianou A., 2013, Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, P207
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Enze Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P696, DOI 10.1007/978-3-030-58601-0_41
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu X., 2023, AAAI, P881
   Ioannou Y, 2017, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2017.633
   Ji GP, 2023, MACH INTELL RES, V20, P92, DOI 10.1007/s11633-022-1365-9
   Jia Q, 2022, PROC CVPR IEEE, P4703, DOI 10.1109/CVPR52688.2022.00467
   Kendall A., 2017, Adv Neural Inf Process Syst, V30
   Kingma D. P., 2014, arXiv
   Li YY, 2023, IEEE T MULTIMEDIA, V25, P5234, DOI 10.1109/TMM.2022.3189250
   Liu JW, 2022, IEEE WINT CONF APPL, P2613, DOI 10.1109/WACV51458.2022.00267
   Liu ZY, 2022, INT C PATT RECOG, P140, DOI 10.1109/ICPR56361.2022.9956724
   Loshchilov I., 2016, arXiv
   Lv YQ, 2021, PROC CVPR IEEE, P11586, DOI 10.1109/CVPR46437.2021.01142
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Skurowski P., 2018, Unpubl Manuscr, V2, P7
   Stevens M, 2009, PHILOS T R SOC B, V364, P423, DOI 10.1098/rstb.2008.0217
   Sun Y., 2021, P INT JOINT C ART IN, P1025
   Sun Y., 2022, P INT JOINT C ART IN, P1335, DOI DOI 10.24963/IJCAI.2022/186
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Yu TY, 2019, IEEE I CONF COMP VIS, P552, DOI 10.1109/ICCV.2019.00064
   Zhai W, 2023, IEEE T MULTIMEDIA, V25, P5155, DOI 10.1109/TMM.2022.3188401
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhong YJ, 2022, PROC CVPR IEEE, P4494, DOI 10.1109/CVPR52688.2022.00446
NR 42
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4050
EP 4060
DI 10.1109/TMM.2023.3295095
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100002
DA 2024-08-05
ER

PT J
AU Ning, TY
   Zhong, BE
   Liang, QH
   Tang, ZJ
   Li, XX
AF Ning, Tian Yu
   Zhong, Bineng
   Liang, Qihua
   Tang, Zhenjun
   Li, Xianxian
TI Robust Tracking via Bidirectional Transduction With Mask Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention; bidirectional transduction; robust tracking; visual tracking
ID SIAMESE NETWORKS
AB In the tracking literature, foreground and background information have been extensively investigated to discriminate a target from its surrounding background. However, both foreground and background possess their own spatial-temporal correlation relationship that provide significant information to separate the target from its surrounding background, which has been usually ignored by existing work. To address this issue, we propose a bidirectional transductive network based tracker, which incorporates long-range spatial-temporal and bidirectional constraints. Specifically, our tracker consists of two modules, namely the mask generation module (MGM) and the transduction attention module (TAM). MGM aggregates long-range interdependencies of a target along the history frames for generating accurate target masks. TAM retrieves back to the history frames to find patches similar to the current frame, which are then forwarded along with the target masks generated by MGM. In this manner, each position in the current frame can determine its own identity, whether belonging to either the background or the foreground, hence accurately distinguishing the target from its distractors. We conduct systematically experiments and achieve state-of-the-art performance on several benchmarks, obtaining 69.2% AO on GOT-10k and 82.1% on TrackingNet.
C1 [Ning, Tian Yu; Zhong, Bineng; Liang, Qihua; Tang, Zhenjun; Li, Xianxian] Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.
   [Ning, Tian Yu; Zhong, Bineng; Liang, Qihua; Tang, Zhenjun; Li, Xianxian] Guangxi Normal Univ, Guangxi Key Lab Multisource Informa t Min & Secur, Guilin 541004, Peoples R China.
   [Ning, Tian Yu] Huaqiao Univ, Dept Comp Sci & Technol, Minist Educ, Xiamen 361021, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University; Huaqiao University
RP Zhong, BE (corresponding author), Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.; Zhong, BE (corresponding author), Guangxi Normal Univ, Guangxi Key Lab Multisource Informa t Min & Secur, Guilin 541004, Peoples R China.
EM 20014083032@stu.hqu.edu.cn; bnzhong@gxnu.edu.cn; qhliang@gxnu.edu.cn;
   tangzj230@163.com; lixx@gxnu.edu.cn
OI Tang, Zhenjun/0000-0003-3664-1363
FU Guangxi Science and Technology
FX No Statement Available
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng SY, 2021, PROC CVPR IEEE, P4419, DOI 10.1109/CVPR46437.2021.00440
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Fan C, 2023, IEEE T CIRC SYST VID, V33, P186, DOI 10.1109/TCSVT.2021.3102886
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan JQ, 2021, IEEE T CIRC SYST VID, V31, P1296, DOI 10.1109/TCSVT.2020.2987601
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Han G, 2023, IEEE T MULTIMEDIA, V25, P430, DOI 10.1109/TMM.2021.3127357
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jiang M, 2021, IEEE T CIRC SYST VID, V31, P3154, DOI 10.1109/TCSVT.2020.3037947
   Junliang Xing, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1698, DOI 10.1109/ICPR.2010.420
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Lee KH, 2015, IEEE T MULTIMEDIA, V17, P1429, DOI 10.1109/TMM.2015.2455418
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Nie Z., 2022, early access
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Szegedy C., 2015, PROC IEEECVFCONF COM, P1
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xi M, 2022, IEEE T MULTIMEDIA, V24, P2791, DOI 10.1109/TMM.2021.3087340
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411
   Zhang TL, 2022, IEEE T CIRC SYST VID, V32, P1403, DOI 10.1109/TCSVT.2021.3072207
   Zhang YZ, 2020, PROC CVPR IEEE, P6947, DOI 10.1109/CVPR42600.2020.00698
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao MJ, 2021, Arxiv, DOI [arXiv:2105.03817, DOI 10.48550/ARXIV.2105.03817]
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
   Zhu Z, 2017, IEEE INT CONF COMP V, P1973, DOI 10.1109/ICCVW.2017.231
NR 54
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4308
EP 4319
DI 10.1109/TMM.2023.3321497
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100023
DA 2024-08-05
ER

PT J
AU Pan, RJ
   Yang, H
   Li, CY
   Yang, JH
AF Pan, Renjie
   Yang, Hua
   Li, Cunyan
   Yang, Jinhai
TI Joint Intra &amp; Inter-Grained Reasoning: A New Look Into Semantic
   Consistency of Image-Text Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-text retrieval; cross-modal attention; joint intra and
   inter-grained alignment
ID NETWORK
AB Multimodal understanding aims at constructing semantic correlations among modalities of data while performing various downstream tasks. As one of the primary multimodal downstream tasks, image-text retrieval imposes a high demand on semantic alignment because of the independent expression paradigms of images and text. Existing methods mainly construct a joint embedding space at a single granularity level (either global or local). However, such single reasoning paradigms lack granularity interaction, resulting in semantic inconsistency and cross-domain catastrophes. To address these issues, we design a novel Joint Intra and Inter-grained Network (JIIGNet), focusing on not only intra- but also inter-grained interaction between modalities by combining scene information (global) with region-level (local) instances. Specifically, we simultaneously initiate three specific alignment modules, i.e., global-grained, local-grained, and cross-grained alignment modules, followed by Triplet Attention Refinement to better refine the fused embedding at the alignment-level with proper self and cross attention. For different scenarios, a Style Adaptation Head is further designed to smartly accommodate different samples. We validate JIIGNet through extensive experiments conducted on two widely used datasets: Flickr-30 K and MS-COCO, demonstrating the effectiveness of our proposed method.
C1 [Pan, Renjie; Yang, Hua; Yang, Jinhai] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Yang, Hua] Shanghai Jiao Tong Univ, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
   [Li, Cunyan] Shanghai Jiao Tong Univ, AI Inst, MoE, Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai
   Jiao Tong University
RP Yang, H (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Dept Elect Engn, Shanghai 200240, Peoples R China.; Yang, H (corresponding author), Shanghai Jiao Tong Univ, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
EM rjpan21@sjtu.edu.cn; hyang@sjtu.edu.cn; licunyan@sjtu.edu.cn;
   youngjh@sjtu.edu.cn
OI yang, hua/0000-0002-0417-234X; Yang, Jinhai/0000-0003-1101-6705; Pan,
   Renjie/0000-0002-0688-0227
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Chen TL, 2020, AAAI CONF ARTIF INTE, V34, P10583
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Faghri F, 2018, Arxiv, DOI arXiv:1707.05612
   Fan ZH, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P137, DOI 10.1145/3512527.3531368
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Huang Y, 2019, AAAI CONF ARTIF INTE, P8489
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z, 2019, IEEE I CONF COMP VIS, P5753, DOI 10.1109/ICCV.2019.00585
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li JT, 2022, AAAI CONF ARTIF INTE, P1323
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu Chunxiao, 2020, CVPR, P10918, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Ma YW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3547910
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Pan Renjie, 2022, Digital TV and Wireless Multimedia Communications: 18th International Forum, IFTC 2021, Revised Selected Papers. Communications in Computer and Information Science (1560), P266, DOI 10.1007/978-981-19-2266-4_21
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Qin Jin, 2016, P 24 ACM INT C MULT, P1087, DOI [DOI 10.1145/2964284.2984065, 10.1145/2964284.2984065]
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramanishka V, 2017, PROC CVPR IEEE, P3135, DOI 10.1109/CVPR.2017.334
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Shi BT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5182
   Vaswani A., 2017, C WORKSHOP NEURAL IN, P6000
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P3362, DOI 10.1109/TMM.2020.3024822
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xu BQ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3247103
   Xu YH, 2023, IEEE T MULTIMEDIA, V25, P8346, DOI 10.1109/TMM.2023.3235495
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Zhang HT, 2022, AAAI CONF ARTIF INTE, P3262
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu T, 2023, IEEE T MULTIMEDIA, V25, P3375, DOI 10.1109/TMM.2022.3160060
NR 56
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4912
EP 4925
DI 10.1109/TMM.2023.3327645
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600041
DA 2024-08-05
ER

PT J
AU Tan, WT
   Ding, CX
   Wang, PF
   Gong, MM
   Jia, K
AF Tan, Wentao
   Ding, Changxing
   Wang, Pengfei
   Gong, Mingming
   Jia, Kui
TI Style Interleaved Learning for Generalizable Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain generalization; interleaved learning; person re-identification
ID DOMAIN; NETWORK
AB Domain generalization (DG) for person re-identification (ReID) is a challenging problem, as access to target domain data is not permitted during the training process. Most existing DG ReID methods update the feature extractor and classifier parameters based on the same features. This common practice causes the model to overfit to existing feature styles in the source domain, resulting in sub-optimal generalization ability on target domains. To solve this problem, we propose a novel style interleaved learning (IL) framework. Unlike conventional learning strategies, IL incorporates two forward propagations and one backward propagation for each iteration. We employ the features of interleaved styles to update the feature extractor and classifiers using different forward propagations, which helps to prevent the model from overfitting to certain domain styles. To generate interleaved feature styles, we further propose a new feature stylization approach. It produces a wide range of meaningful styles that are both different and independent from the original styles in the source domain, which caters to the IL methodology. Extensive experimental results show that our model not only consistently outperforms state-of-the-art methods on large-scale benchmarks for DG ReID, but also has clear advantages in computational efficiency.
C1 [Tan, Wentao] South China Univ Technol, Sch Future Technol, Guangzhou 510000, Peoples R China.
   [Ding, Changxing; Jia, Kui] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Peoples R China.
   [Ding, Changxing] PazhouLab, Guangzhou 510330, Peoples R China.
   [Wang, Pengfei] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China.
   [Gong, Mingming] Univ Melbourne, Sch Math & Stat, Melbourne, Vic 3010, Australia.
C3 South China University of Technology; South China University of
   Technology; Hong Kong Polytechnic University; University of Melbourne
RP Ding, CX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Peoples R China.
EM ftwentaotan@mail.scut.edu.cn; chxding@scut.edu.cn;
   pengfei.wang@connect.polyu.hk; mingming.gong@unimelb.edu.au;
   kuijia@scut.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai ZC, 2021, PROC CVPR IEEE, P12909, DOI 10.1109/CVPR46437.2021.01272
   Balaji Y, 2018, ADV NEUR IN, V31
   Carpenter SK, 2013, MEM COGNITION, V41, P671, DOI 10.3758/s13421-012-0291-4
   Choi S, 2021, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR46437.2021.00343
   Dai YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11844, DOI 10.1109/ICCV48922.2021.01165
   Dai YX, 2021, PROC CVPR IEEE, P16140, DOI 10.1109/CVPR46437.2021.01588
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gao Z, 2021, IEEE T MULTIMEDIA, V23, P3332, DOI 10.1109/TMM.2020.3023784
   Ge Y., 2020, ICLR
   Halpern D., 2007, Assoc. Psychol. Sci. Taskforce Lifelong Learn. Work Home, V25
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiao BL, 2022, LECT NOTES COMPUT SC, V13674, P285, DOI 10.1007/978-3-031-19781-9_17
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Kang J, 2022, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR52688.2022.00699
   Kim J, 2022, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR52688.2022.00431
   Lee S, 2022, PROC CVPR IEEE, P9926, DOI 10.1109/CVPR52688.2022.00970
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li H, 2020, ADV NEURAL INFORM PR, V33, P3118, DOI DOI 10.5555/3495724.3495986
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li X., 2022, PROC INT C LEARN REP
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Li YJ, 2019, PR MACH LEARN RES, V97
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Lv FR, 2022, PROC CVPR IEEE, P8036, DOI 10.1109/CVPR52688.2022.00788
   Mahajan D, 2021, PR MACH LEARN RES, V139
   Mancini Massimiliano, 2020, P ECCV, P466, DOI DOI 10.1007/978-3-030-58592
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Muandet K., 2013, INT C MACHINE LEARNI, P10
   Ni H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2002, DOI 10.1145/3474085.3475361
   Ni H, 2022, PROC CVPR IEEE, P2477, DOI 10.1109/CVPR52688.2022.00252
   Nuriel O, 2021, PROC CVPR IEEE, P9477, DOI 10.1109/CVPR46437.2021.00936
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Pandey P, 2021, PROC CVPR IEEE, P12919, DOI 10.1109/CVPR46437.2021.01273
   Pashler H., 2007, Organizing instruction and study to improve student learning (NCER 2007-2004)
   Qi L, 2023, IEEE T MULTIMEDIA, V25, P4856, DOI 10.1109/TMM.2022.3183393
   Sarawagi Sunita, 2018, INT C LEARN REPR
   Shengcai Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P456, DOI 10.1007/978-3-030-58621-8_27
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Shi YC, 2020, PROC CVPR IEEE, P6816, DOI 10.1109/CVPR42600.2020.00685
   Song JF, 2019, PROC CVPR IEEE, P719, DOI 10.1109/CVPR.2019.00081
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Volpi R, 2019, IEEE I CONF COMP VIS, P7979, DOI 10.1109/ICCV.2019.00807
   Wan CQ, 2022, PROC CVPR IEEE, P4672, DOI 10.1109/CVPR52688.2022.00464
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang PF, 2023, PROC CVPR IEEE, P3769, DOI 10.1109/CVPR52729.2023.00367
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P2624, DOI 10.1109/TMM.2022.3149629
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xu BQ, 2022, LECT NOTES COMPUT SC, V13674, P372, DOI 10.1007/978-3-031-19781-9_22
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yu SJ, 2021, Arxiv, DOI arXiv:2105.12355
   Zan Gao, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P3703, DOI 10.1145/3503161.3547884
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhang PY, 2022, LECT NOTES COMPUT SC, V13674, P215, DOI 10.1007/978-3-031-19781-9_13
   Zhang X., 2022, P IEEE CVF C COMP VI, P7369
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao YY, 2021, PROC CVPR IEEE, P6273, DOI 10.1109/CVPR46437.2021.00621
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou K., 2021, PROC INT C LEARN REP
   Zhou KY, 2022, IEEE T PATTERN ANAL, V44, P5056, DOI 10.1109/TPAMI.2021.3069237
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
NR 75
TC 6
Z9 6
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1600
EP 1612
DI 10.1109/TMM.2023.3283878
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800001
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Tan, XD
   Hu, MH
   Zhai, GT
   Zhu, Y
   Li, WF
   Zhang, XP
AF Tan, Xudong
   Hu, Menghan
   Zhai, Guangtao
   Zhu, Yan
   Li, Wenfang
   Zhang, Xiao-Ping
TI Lightweight Video-Based Respiration Rate Detection Algorithm: An
   Application Case on Intensive Care
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Optical flow; Estimation; Temperature measurement;
   Signal detection; Videos; Cameras; ICU application; non-contact
   detection; physiological signals; respiratory rate measurements
AB The video-based non-contact respiration detection technology can be used in many application scenarios to unobtrusively and ubiquitously monitor the physical state of living beings, and various researchers are currently working on this technology. The optical flow method in tandem with crossover point method is rather effective for respiration rate extraction. However, each method has one disadvantage: 1) the redundant feature points in the traditional optical flow method increase the computational effort and reduce the estimation accuracy; and 2) the traditional crossover point method suffers from crossover points unrelated to breathing movements. For these two challenges, two optimization points are proposed in this work: 1) optimize feature point space by combining spatio-temporal information; and 2) use negative feedback design to adaptively remove crossovers that are not related to respiratory movements. The performance of the proposed algorithm is validated by the Large-scale Bedside Respiration Dataset for Intensive Care (LBRD-IC), which is established using the actual surveillance videos acquired from ICU wards. The validity of the above two optimization points is verified by the ablation experiments. The influential analysis of computation time and video resolution on the performance of the proposed algorithm demonstrates that the proposed algorithm can be deployed to various application terminals to monitor the respiration rate of living organisms in real-time and with high accuracy. In addition, field measurements in the ICU ward have shown that our algorithm can measure respiratory signals of the single patient and multiple patients when only one surveillance camera is present.
C1 [Tan, Xudong; Hu, Menghan] East China Normal Univ, Sch Commun & Elect Engn, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200241, Peoples R China.
   [Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Zhai, Guangtao] Naval Mil Med Univ, Shanghai Changzheng Hosp, Dept Emergency & Crit Care, Shanghai 200003, Peoples R China.
   [Zhu, Yan; Li, Wenfang] Naval Mil Med Univ, Shanghai Changzheng Hosp, Dept Emergency & Crit Care, Shanghai 200003, Peoples R China.
   [Zhang, Xiao-Ping] Tsinghua Berkeley Shenzhen Inst, Shenzhen 518071, Peoples R China.
   [Zhang, Xiao-Ping] Toronto Metropolitan Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B2K3, Canada.
C3 East China Normal University; Shanghai Jiao Tong University; Naval
   Medical University; Naval Medical University; Tsinghua Shenzhen
   International Graduate School; Toronto Metropolitan University
RP Hu, MH (corresponding author), East China Normal Univ, Sch Commun & Elect Engn, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200241, Peoples R China.
EM shawntannnn@gmail.com; mhhu@ce.ecnu.edu.cn; zhaiguangtao@sjtu.edu.cn;
   mise169@163.com; chzhedlwf@163.com; xzhang@ee.ryerson.ca
RI Zhai, Guangtao/X-5949-2019; Hu, Menghan/AAK-7153-2021; Zhang,
   Xiaoping/AFW-5367-2022; Zhang, Xiaoping/AAX-7947-2021
OI Zhai, Guangtao/0000-0001-8165-9322; Hu, Menghan/0000-0002-8557-8930;
   Zhang, Xiaoping/0000-0002-8891-0978; Zhang,
   Xiaoping/0000-0002-8891-0978; Tan, Xudong/0009-0007-6008-5789
FU National Natural Science Foundation of China
FX No Statement Available
CR ADAMS LP, 1990, ISPRS J PHOTOGRAMM, V45, P152, DOI 10.1016/0924-2716(90)90055-G
   Bouguet J.-Y., 2001, Intel Corporation, V5, P4, DOI DOI 10.1109/HPDC.2004.1323531
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Chen WX, 2018, LECT NOTES COMPUT SC, V11206, P356, DOI 10.1007/978-3-030-01216-8_22
   Chung HU, 2020, NAT MED, V26, P418, DOI 10.1038/s41591-020-0792-9
   Churpek MM, 2012, CRIT CARE MED, V40, P2102, DOI 10.1097/CCM.0b013e318250aa5a
   Cretikos MA, 2008, MED J AUSTRALIA, V188, P657
   Farah A., 2015, Int. J. Sci. Eng. Res., V6, P1748
   Guo Q. Lin, 2021, Electron. Imag., P2671
   Harris C, 1988, A combined corner and edge detector, P147, DOI [10.5244/C.2.23.23.1-23.6, DOI 10.5244/C.2.23]
   Hu MH, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190466
   Hu M, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3058983
   Inamori G, 2021, SCI ADV, V7, DOI 10.1126/sciadv.abe3793
   Jorge J., 2020, IEEE Sensors J., V20, P286
   Kempfle J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20143884
   Kusche R, 2018, IEEE T BIOMED CIRC S, V12, P614, DOI 10.1109/TBCAS.2018.2812222
   Liu X., 2020, Advances in Neural Information Processing Systems, V33, P19400
   Massaroni C, 2021, IEEE SENS J, V21, P12821, DOI 10.1109/JSEN.2020.3023486
   Mateu-Mateus M, 2020, IEEE ACCESS, V8, P154924, DOI 10.1109/ACCESS.2020.3018616
   Nam Y, 2016, IEEE J BIOMED HEALTH, V20, P1493, DOI 10.1109/JBHI.2015.2480838
   Nowara Ewa M, 2021, Adv. Neural Inform. Process. Syst. (NIPS), P4955
   Oh J, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3049248
   Pereira CB, 2017, J CLIN MONIT COMPUT, V31, P1241, DOI 10.1007/s10877-016-9949-y
   Poh MZ, 2011, IEEE T BIO-MED ENG, V58, P7, DOI 10.1109/TBME.2010.2086456
   Qiu Y, 2019, IEEE T MULTIMEDIA, V21, P1778, DOI 10.1109/TMM.2018.2883866
   Scalise Lorenzo., 2011, 2011 IEEE International Symposium on Medical Measurements and Applications, P657
   Shiota S, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P239
   Shyu KK, 2020, IEEE SENS J, V20, P10251, DOI 10.1109/JSEN.2020.2992687
   Tran QV, 2020, IEEE T SYST MAN CY-S, V50, P2659, DOI 10.1109/TSMC.2018.2825458
   Wang H., 2022, P IEEE EMBS INT C BI, P1
   Wang H, 2022, AAAI CONF ARTIF INTE, P2431
   Wang WJ, 2022, PHYSIOL MEAS, V43, DOI 10.1088/1361-6579/ac5b49
   Wang WJ, 2021, IEEE J BIOMED HEALTH, V25, P1358, DOI 10.1109/JBHI.2021.3072439
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P1479, DOI 10.1109/TBME.2016.2609282
   Wang YL, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401084
   Wang YL, 2020, IEEE INTERNET THINGS, V7, P8559, DOI 10.1109/JIOT.2020.2991456
   Yu ZT, 2022, PROC CVPR IEEE, P4176, DOI 10.1109/CVPR52688.2022.00415
   Zhang TY, 2023, IEEE T MULTIMEDIA, V25, P3773, DOI 10.1109/TMM.2022.3165715
   Zhao AT, 2023, IEEE T MULTIMEDIA, V25, P4464, DOI 10.1109/TMM.2022.3176751
NR 39
TC 4
Z9 4
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1761
EP 1775
DI 10.1109/TMM.2023.3286994
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800039
DA 2024-08-05
ER

PT J
AU Wang, ZZ
   Zhao, HT
   Yao, LJ
   Peng, JC
   Zhao, KJ
AF Wang, Zhongze
   Zhao, Haitao
   Yao, Lujian
   Peng, Jingchao
   Zhao, Kaijie
TI DFR-Net: Density Feature Refinement Network for Image Dehazing Utilizing
   Haze Density Difference
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Proposals; Image restoration; Atmospheric modeling;
   Task analysis; Robustness; Refining; Deep learning; density-aware; image
   dehazing; image enhancement; image restoration
AB In the image dehazing task, the haze density is a key feature that affects the performance of dehazing methods. The haze density difference, which has rarely been utilized in previous methods, can guide networks to perceive different global densities and focus on local areas with high density or that are difficult to dehaze. In this paper, we propose a density-aware dehazing method named the Density Feature Refinement Network (DFR-Net), which extracts haze density features from density differences and leverages density differences to refine density features. In DFR-Net, we first generate a proposal image that has a lower overall density than the hazy input, resulting in global density differences. Additionally, the dehazing residual of the proposal image reflects the level of dehazing performance and provides local density differences that indicate localized hard dehazing or high-density areas. Subsequently, we introduce a Global Branch (GB) and a Local Branch (LB) to achieve density awareness. In GB, we use Siamese networks for feature extraction of hazy inputs and proposal images, and we propose a Global Density Feature Refinement (GDFR) module that can refine features by pushing features with different global densities further away. In LB, we explore local density features from the dehazing residuals between hazy inputs and proposal images and introduce an Intermediate Dehazing Residual Feedforward (IDRF) module to update local features and pull them close to clear image features. Sufficient experiments demonstrate that the proposed method outperforms state-of-the-art methods on various datasets.
C1 [Wang, Zhongze; Yao, Lujian; Peng, Jingchao; Zhao, Kaijie] East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
   [Zhao, Haitao] East China Univ Sci & Technol, Shanghai 200237, Peoples R China.
C3 East China University of Science & Technology; East China University of
   Science & Technology
RP Zhao, HT (corresponding author), East China Univ Sci & Technol, Shanghai 200237, Peoples R China.
EM haitaozhao@ecust.edu.cn
OI Wang, Zhongze/0009-0002-6938-2724; Yao, Lujian/0000-0002-7571-1339
FU National Natural Science Foundation of China
FX No Statement Available
CR Ancuti CO, 2021, IEEE COMPUT SOC CONF, P627, DOI 10.1109/CVPRW53098.2021.00074
   Ancuti CO, 2019, IEEE COMPUT SOC CONF, P2241, DOI 10.1109/CVPRW.2019.00277
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Bai HR, 2022, IEEE T IMAGE PROCESS, V31, P1217, DOI 10.1109/TIP.2022.3140609
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen JH, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20216000
   Chen WT, 2022, IEEE T CIRC SYST VID, V32, P3346, DOI 10.1109/TCSVT.2021.3106198
   Cheng D., 2022, PROC 31 INT JOINT C, P848
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P2453, DOI 10.1109/ICCV.2019.00254
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   DRAKE RM, 1985, AM J PHYS, V53, P955, DOI 10.1119/1.14011
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Frants V, 2023, IEEE T CYBERNETICS, V53, P5448, DOI 10.1109/TCYB.2023.3238640
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hong M, 2022, AAAI CONF ARTIF INTE, P906
   Jin X, 2023, PROC CVPR IEEE, P18135, DOI 10.1109/CVPR52729.2023.01739
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Koch G, 2015, ICML DEEP LEARN WORK, V2
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Lin CY, 2023, IEEE T MULTIMEDIA, V25, P3089, DOI 10.1109/TMM.2022.3155937
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Liu Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P50, DOI 10.1145/3474085.3475331
   Loshchilov I., 2016, arXiv
   Lou WH, 2020, IEEE ACCESS, V8, P113318, DOI 10.1109/ACCESS.2020.3003444
   Lugmayr A, 2020, IEEE COMPUT SOC CONF, P2058, DOI 10.1109/CVPRW50498.2020.00255
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Qili Deng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P722, DOI 10.1007/978-3-030-58539-6_43
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Sun H, 2023, NEURAL NETWORKS, V163, P10, DOI 10.1016/j.neunet.2023.03.017
   Wang BF, 2023, INT J MACH LEARN CYB, V14, P2407, DOI 10.1007/s13042-022-01771-9
   Wang T, 2021, NEUROCOMPUTING, V439, P75, DOI 10.1016/j.neucom.2021.01.042
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Wu HR, 2017, 10 INT C IMAGE SIGNA
   Yang Y, 2022, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR52688.2022.00208
   Ye T, 2022, LECT NOTES COMPUT SC, V13679, P130, DOI 10.1007/978-3-031-19800-7_8
   Yi X, 2022, NEUROCOMPUTING, V485, P1, DOI 10.1016/j.neucom.2022.02.019
   Yin SB, 2021, NEUROCOMPUTING, V437, P143, DOI 10.1016/j.neucom.2020.12.081
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhang JA, 2022, IEEE T CYBERNETICS, V52, P11187, DOI 10.1109/TCYB.2021.3070310
   Zheng Y, 2023, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR52729.2023.00560
NR 54
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7673
EP 7686
DI 10.1109/TMM.2024.3369979
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000023
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yang, YH
   Pan, R
   Li, XY
   Yang, X
   Deng, C
AF Yang, Yanhua
   Pan, Rui
   Li, Xiangyu
   Yang, Xu
   Deng, Cheng
TI Dual-Stream Contrastive Learning for Compositional Zero-Shot Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compositional Zero-Shot learning; contrastive learning; transfer
   learning
AB Compositional Zero-Shot learning (CZSL) requires recognizing unseen attribute-object compositions using observed visual primitives attributes and objects in a training set, which is a critical capacity for learning systems because the long tail of new combinations dominates the distribution in the real world. However, CZSL is a challenging problem because learning systems tend to learn the dependencies between objects and attributes, which is not conducive to composition classification, and incorrect dependencies will mislead the classification of new combinations of known attributes and objects. This paper primarily introduces a novel yet effective dual-stream contrastive learning method with two main objectives: making the learned representations discriminative and transferring knowledge more efficiently from seen to unseen compositions. Specifically, we generate positive and negative pairs based on the similarity of different concepts (attributes and objects), independently capturing the discriminative representations of concepts. Meanwhile, unlike existing contrastive methods that select negative samples randomly, we construct confusable compositional representations as the negatives to explore the intrinsic relevance between attributes and objects, which can improve the generalization from seen to unseen compositions. Experimental results on two benchmarks show that the proposed method outperforms the state-of-the-arts.
C1 [Yang, Yanhua] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
   [Pan, Rui; Li, Xiangyu; Yang, Xu; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University
RP Yang, X; Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM yanhyang@xidian.edu.cn; ruipan0110@gmail.com;
   xdu_xyli@stu.xidian.edu.cn; xuyang.xidian@gmail.com; chdeng.xd@gmail.com
RI Li, Xiangyu/AGY-5200-2022
OI Li, Xiangyu/0000-0002-4226-1255; Yang, Xu/0000-0002-0405-6816
FU Key Research and Development Projects of Shaanxi Province
FX No Statement Available
CR Atzmon Y, 2016, Arxiv, DOI arXiv:1608.07639
   Atzmon Yuval, 2020, ADV NEURAL INFORM PR, P1462
   Bojanowski Piotr, 2017, T ASSOC COMPUT LING, V5, P135, DOI [10.48550/arXiv.1607.04606, DOI 10.1162/TACLA00051]
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen CY, 2014, PROC CVPR IEEE, P200, DOI 10.1109/CVPR.2014.33
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Deng C, 2020, IEEE T MULTIMEDIA, V22, P885, DOI 10.1109/TMM.2019.2934833
   Hjelm RD, 2019, Arxiv, DOI arXiv:1808.06670
   Frome A., 2013, Advances in neural information processing systems, V26
   Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Isola P, 2015, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR.2015.7298744
   Jaiswal A, 2021, TECHNOLOGIES, V9, DOI 10.3390/technologies9010002
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li XY, 2022, PROC CVPR IEEE, P9316, DOI 10.1109/CVPR52688.2022.00911
   Li Y.-L., 2020, P IEEECVF C COMPUTER, P11316
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P1600, DOI 10.1109/TMM.2021.3139211
   Liu K, 2019, WORLD WIDE WEB, V22, P807, DOI 10.1007/s11280-018-0642-6
   Mancini M, 2021, PROC CVPR IEEE, P5218, DOI 10.1109/CVPR46437.2021.00518
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P1, DOI DOI 10.48550/ARXIV.1310.4546
   Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129
   Naeem MF, 2021, PROC CVPR IEEE, P953, DOI 10.1109/CVPR46437.2021.00101
   Nagarajan T, 2018, LECT NOTES COMPUT SC, V11205, P172, DOI 10.1007/978-3-030-01246-5_11
   Nan ZX, 2019, AAAI CONF ARTIF INTE, P8811
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Norouzi M, 2014, Arxiv, DOI arXiv:1312.5650
   Purushwalkam S, 2019, IEEE I CONF COMP VIS, P3592, DOI 10.1109/ICCV.2019.00369
   Ruis Frank, 2021, ADV NEURAL INFORM PR, P10641
   Saini N, 2022, PROC CVPR IEEE, P13648, DOI 10.1109/CVPR52688.2022.01329
   Socher R, 2013, P 2013 C EMP METH NA, P935
   Song XH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3976, DOI 10.1145/3394171.3413568
   Wang CQ, 2021, AAAI CONF ARTIF INTE, V35, P2710
   Wei K, 2019, IEEE I CONF COMP VIS, P3740, DOI 10.1109/ICCV.2019.00384
   Xian YQ, 2019, PROC CVPR IEEE, P8248, DOI 10.1109/CVPR.2019.00845
   Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328
   Yang YH, 2023, IEEE T MULTIMEDIA, V25, P280, DOI 10.1109/TMM.2021.3125134
   Ye YL, 2023, IEEE T MULTIMEDIA, V25, P2252, DOI 10.1109/TMM.2022.3145237
   Ye YL, 2022, IEEE T MULTIMEDIA, V24, P1325, DOI 10.1109/TMM.2021.3063616
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
NR 42
TC 3
Z9 3
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1909
EP 1919
DI 10.1109/TMM.2023.3243674
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800022
DA 2024-08-05
ER

PT J
AU Zhang, L
   Chen, LT
   Zhou, C
   Li, X
   Yang, F
   Yi, Z
AF Zhang, Lei
   Chen, Leiting
   Zhou, Chuan
   Li, Xin
   Yang, Fan
   Yi, Zhang
TI Weighted Graph-Structured Semantics Constraint Network for Cross-Modal
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Training; Feature extraction; Representation learning; Data
   models; Correlation; Games; Cross-modal retrieval; adversarial learning;
   graph neural network
AB Cross-modal retrieval aims to retrieve relevant content of different modalities by giving a query of another modality. The biggest difficulty is how to bridge the heterogeneous gap between different modalities. The commonly-used methods tend to focus on exploiting individual image-text pair and mining the relations of cross-modality data thereof, but ignore the role of multi-sample correlation. Moreover, more global, structural inter-pair knowledge contained by the training dataset will be under-used. To fully exploit graph-structured semantics and mine the semantic information in the dataset for learning discriminative representations, we propose Weighted Graph-structured Semantics Constraint Network (WGSCN), a unified, graph-based, semantic-constrained learning framework, in which GCN is used to mine comprehensive relation information from cross modality data. Our main inspiration is to design a novel two-branch GCN-based Cross-modal Semantic Encoding (GCSE) module to produce semantic embeddings with the both modality-specific and modality-shared correlation. Moreover, a GAN-based dual learning approach is used to further improve the discriminability and model the joint distribution across different modalities. Our proposed GDL uses semantic embeddings as supervisory signal to make the common representation semantically discriminative while adversarial learning and dual learning are used to make the common representation modality-invariant. Through comparative experiments on five commonly used cross-modal datasets, we have shown the superior retrieval accuracy of our WGSCN.
C1 [Zhang, Lei; Chen, Leiting; Zhou, Chuan] Univ Elect Sci & Technol China, Digital Media Technol Key Lab Sichuan Prov, Chengdu 611731, Peoples R China.
   [Zhang, Lei; Chen, Leiting; Zhou, Chuan] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Chen, Leiting; Zhou, Chuan] Inst Elect & Informat Engn UESTC Guangdong, Dongguan 523429, Peoples R China.
   [Li, Xin; Yang, Fan] AIQ, Abu Dhabi 999041, U Arab Emirates.
   [Yi, Zhang] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Sichuan University
RP Zhou, C (corresponding author), Univ Elect Sci & Technol China, Digital Media Technol Key Lab Sichuan Prov, Chengdu 611731, Peoples R China.
EM lei_zhang@std.uestc.edu.cn; richardchen@uestc.edu.cn;
   zhouchuan@uestc.edu.cn; xinli_uestc@hotmail.com;
   fanyang_uestc@hotmail.com; zhangyi@scu.edu.cn
OI Zhou, Chuan/0000-0001-7700-7188; Zhang, Lei/0000-0002-4477-5532; Li,
   Xin/0000-0001-8047-9610; Yang, Fan/0000-0002-1157-8719
FU Key-Area Research and Development Program of Guangdong Province
FX No Statement Available
CR Andrew R., 2013, INT C MACHINE LEARNI, P1247, DOI DOI 10.5555/3042817.3043076
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Bruna J., 2014, ABS13126203 CORR, P1, DOI DOI 10.48550/ARXIV.1312.6203
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He D, 2016, ADV NEUR IN, V29
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Hu P, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P635, DOI 10.1145/3331184.3331213
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huang X, 2020, IEEE T CYBERNETICS, V50, P1047, DOI 10.1109/TCYB.2018.2879846
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kim Yoon, 2014, EMNLP, P1746, DOI 10.3115/v1/D14-1181
   Li C, 2019, AAAI CONF ARTIF INTE, P176
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li Dongge, 2003, P 11 ACM INT C MULT, P604, DOI DOI 10.1145/957013.957143
   Li X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P439, DOI 10.1145/3123266.3123290
   Li ZP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P880, DOI 10.1145/3394171.3413992
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luo A, 2022, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR52688.2022.00870
   Luo A, 2020, AAAI CONF ARTIF INTE, V34, P11693
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mandal D, 2020, IEEE T MULTIMEDIA, V22, P2345, DOI 10.1109/TMM.2019.2954741
   Ngiam A., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   Peng Yuxin, 2016, IJCAI, P3846
   Qian SS, 2021, IEEE T MULTIMEDIA, V24, P3520, DOI 10.1109/TMM.2021.3101642
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, P139
   Rasiwasia Nikhil, 2010, P 18 ACM INT C MULT, P251
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wu YL, 2021, IEEE T MULTIMEDIA, V23, P559, DOI 10.1109/TMM.2020.2985540
   Wu YL, 2020, IEEE T MULTIMEDIA, V22, P1310, DOI 10.1109/TMM.2019.2942494
   Xie Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1255, DOI 10.1145/3394171.3413822
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Yang F, 2021, IEEE T IMAGE PROCESS, V30, P2301, DOI 10.1109/TIP.2020.3038483
   Yang F, 2018, AAAI CONF ARTIF INTE, P7461
   Yang H, 2020, PROC CVPR IEEE, P3802, DOI 10.1109/CVPR42600.2020.00386
   Yang JK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P83, DOI 10.1145/3394171.3413952
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Yu J, 2018, LECT NOTES COMPUT SC, V11164, P223, DOI 10.1007/978-3-030-00776-8_21
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhang L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4277, DOI 10.1145/3474085.3475567
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang Y, 2017, P 8 IJCNLP TAIP TAIW, V1, P253, DOI DOI 10.48550/ARXIV.1510.03820
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zheng AH, 2022, IEEE T MULTIMEDIA, V24, P338, DOI 10.1109/TMM.2021.3050089
   Zhu YC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P646, DOI 10.1145/3394171.3413607
NR 71
TC 1
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1551
EP 1564
DI 10.1109/TMM.2023.3282894
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700021
DA 2024-08-05
ER

PT J
AU Chen, G
   Fu, HZ
   Zhou, T
   Xiao, GB
   Fu, KR
   Xia, Y
   Zhang, YN
AF Chen, Geng
   Fu, Huazhu
   Zhou, Tao
   Xiao, Guobao
   Fu, Keren
   Xia, Yong
   Zhang, Yanning
TI Fusion-Embedding Siamese Network for Light Field Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Light field; multi-modal learning; salient object detection; siamese
   network; transformer
AB Light field salient object detection (SOD) has shown remarkable success and gained considerable attention from the computer vision community. Existing methods usually employ a single-/two-stream network to detect saliency. However, these methods can only handle up to two different modalities at a time, preventing them from being able to fully explore the rich information in multi-modal light field derived data. To address this, we propose the first joint multi-modal learning framework, called FES-Net, for light field SOD, which can take rich inputs not limited to two modalities. Specifically, we propose an attention-aware adaptation module to first transform the multi-modal inputs for use in our joint learning framework. The transformed inputs are then fed to a Siamese network along with multiple embedded feature fusion modules to extract informative multi-modal features. Finally, we predict saliency maps from the high-level extracted features using a saliency decoder module. Our joint multi-modal learning framework effectively resolves the limitations of existing methods, providing efficient and effective multi-modal learning that can fully explore the valuable information in light field data for accurate saliency detection. Furthermore, we improve the performance by introducing the Transformer as our backbone network. To the best of our knowledge, the improved version of our model, called FES-Trans, is the first attempt to address the challenging light field SOD with the powerful Transformer technique. Extensive experiments on benchmark datasets demonstrate that our models are superior light field SOD approaches and outperform cutting-edge models remarkably.
C1 [Chen, Geng; Xia, Yong; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci & Engn, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, Data Applicat Technol, Xian 710072, Peoples R China.
   [Fu, Huazhu] ASTAR, Inst High Performance Comp IHPC, Singapore 138632, Singapore.
   [Zhou, Tao] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst & High Dimens I, PCA Lab,Minist Educ, Nanjing 210094, Peoples R China.
   [Xiao, Guobao] Minjiang Univ, Coll Comp & Control Engn, Fuzhou 350108, Peoples R China.
   [Fu, Keren] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
C3 Northwestern Polytechnical University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute of High Performance Computing
   (IHPC); Nanjing University of Science & Technology; Minjiang University;
   Sichuan University
RP Xia, Y (corresponding author), Northwestern Polytech Univ, Sch Comp Sci & Engn, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, Data Applicat Technol, Xian 710072, Peoples R China.; Fu, HZ (corresponding author), ASTAR, Inst High Performance Comp IHPC, Singapore 138632, Singapore.
EM geng.chen.cs@gmail.com; hzfu@ieee.org; taozhou.ai@gmail.com;
   x-gb@163.com; fkrsuper@scu.edu.cn; yxia@nwpu.edu.cn; ynzhang@nwpu.edu.cn
RI Fu, Huazhu/A-1411-2014; Chen, Geng/KMA-8119-2024
OI Fu, Huazhu/0000-0002-9702-5524; Zhou, Tao/0000-0002-3733-7286; Xiao,
   Guobao/0000-0003-2928-8100; Chen, Geng/0000-0001-8350-6581
FU STI-2030 Major Projects
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6981, DOI 10.1109/TCSVT.2022.3178173
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan D.-P., 2021, Sci Sin Inf, V51, P1475, DOI DOI 10.1360/SSI-2020-0370
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fu K, 2022, COMPUT VIS MEDIA, V8, P509, DOI 10.1007/s41095-021-0256-2
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762
   Khamis S, 2018, LECT NOTES COMPUT SC, V11219, P596, DOI 10.1007/978-3-030-01267-0_35
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Li NY, 2017, IEEE T PATTERN ANAL, V39, P1605, DOI 10.1109/TPAMI.2016.2610425
   Li NY, 2015, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2015.7299158
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Mao YX, 2022, Arxiv, DOI arXiv:2104.10127
   Marvasti-Zadeh SM, 2022, IEEE T INTELL TRANSP, V23, P3943, DOI 10.1109/TITS.2020.3046478
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Piao YR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P904
   Piao YR, 2020, AAAI CONF ARTIF INTE, V34, P11865
   Piao YR, 2020, IEEE T IMAGE PROCESS, V29, P1879, DOI 10.1109/TIP.2019.2942434
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Sheng H, 2016, INT CONF ACOUST SPEE, P1631, DOI 10.1109/ICASSP.2016.7471953
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tao MW, 2015, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2015.7298804
   Tu Zhengzheng, 2022, IEEE T MULTIMEDIA
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang AZ, 2017, NEURAL PROCESS LETT, V46, P1083, DOI 10.1007/s11063-017-9610-x
   Wang SZ, 2018, PROC CVPR IEEE, P2031, DOI 10.1109/CVPR.2018.00217
   Wang TT, 2019, IEEE I CONF COMP VIS, P8837, DOI 10.1109/ICCV.2019.00893
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xingping Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P378, DOI 10.1007/978-3-030-58565-5_23
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhang J., 2020, P IEEE CVF C COMP VI, P8582, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang J, 2022, IEEE T PATTERN ANAL, V44, P5761, DOI 10.1109/TPAMI.2021.3073564
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P4421, DOI 10.1109/TIP.2020.2970529
   Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956
   Zhang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2212
   Zhang M, 2019, ADV NEUR IN, V32
   Zhang M, 2020, IEEE T IMAGE PROCESS, V29, P6276, DOI 10.1109/TIP.2020.2990341
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
   Zhang QD, 2021, IEEE T CIRC SYST VID, V31, P1849, DOI 10.1109/TCSVT.2020.3013119
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhang XD, 2015, NEUROCOMPUTING, V166, P389, DOI 10.1016/j.neucom.2015.03.042
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
NR 84
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 984
EP 994
DI 10.1109/TMM.2023.3274933
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700048
DA 2024-08-05
ER

PT J
AU Cho, Y
   Yu, H
   Kang, SJ
AF Cho, Yubin
   Yu, Hyunwoo
   Kang, Suk-Ju
TI Cross-Aware Early Fusion With Stage-Divided Vision and Language
   Transformer Encoders for Referring Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Referring image segmentation; cross-aware early fusion; feature-based
   cross-modal alignment
ID NETWORK
AB Referring segmentation aims to segment a target object related to a natural language expression. Key challenges of this task are understanding the meaning of complex and ambiguous language expressions and determining the relevant regions in the image with multiple objects by referring to the expression. Recent models have focused on the early fusion with the language features at the intermediate stage of the vision encoder, but these approaches have a limitation that the language features cannot refer to the visual information. To address this issue, this paper proposes a novel architecture, Cross-aware early fusion with stage-divided Vision and Language Transformer encoders (CrossVLT), which allows both language and vision encoders to perform the early fusion for improving the ability of the cross-modal context modeling. Unlike previous methods, our method enables the vision and language features to refer to each other's information at each stage to mutually enhance the robustness of both encoders. Furthermore, unlike the conventional scheme that relies solely on the high-level features for the cross-modal alignment, we introduce a feature-based alignment scheme that enables the low-level to high-level features of the vision and language encoders to engage in the cross-modal alignment. By aligning the intermediate cross-modal features in all encoder stages, this scheme leads to effective cross-modal fusion. In this way, the proposed approach is simple but effective for referring image segmentation, and it outperforms the previous state-of-the-art methods on three public benchmarks.
C1 [Cho, Yubin] Sogang Univ, Sch Artificial Intelligence, Seoul 04017, South Korea.
   [Yu, Hyunwoo; Kang, Suk-Ju] Sogang Univ, Sch Elect Engn, Seoul 04017, South Korea.
C3 Sogang University; Sogang University
RP Kang, SJ (corresponding author), Sogang Univ, Sch Elect Engn, Seoul 04017, South Korea.
EM dbqls1219@sogang.ac.kr; hyunwoo137@sogang.ac.kr; sjkang@sogang.ac.kr
OI Cho, Yubin/0009-0001-8604-5431; Yu, Hyunwoo/0009-0009-4426-8272
FU Samsung Electronics
FX No Statement Available
CR Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding HH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16301, DOI 10.1109/ICCV48922.2021.01601
   Ding HH, 2020, IEEE T IMAGE PROCESS, V29, P3520, DOI 10.1109/TIP.2019.2962685
   Feng G, 2021, PROC CVPR IEEE, P15501, DOI 10.1109/CVPR46437.2021.01525
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   Guo MH, 2022, ADV NEUR IN
   Hu RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1419, DOI 10.1109/ICCV48922.2021.00147
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Hu ZW, 2020, PROC CVPR IEEE, P4423, DOI 10.1109/CVPR42600.2020.00448
   Hua GG, 2023, IEEE T MULTIMEDIA, V25, P8805, DOI 10.1109/TMM.2023.3241802
   Huang S., 2020, P IEEE CVF C COMP VI, P10485, DOI DOI 10.1109/CVPR42600.2020.01050
   Hui Tianrui, 2020, COMPUTER VISION ECCV, DOI DOI 10.1007/978-3-030-58607-2_4
   Jia C, 2021, PR MACH LEARN RES, V139
   Jing Y, 2021, PROC CVPR IEEE, P9853, DOI 10.1109/CVPR46437.2021.00973
   Kim N, 2022, PROC CVPR IEEE, P18124, DOI 10.1109/CVPR52688.2022.01761
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JH, 2021, ADV NEUR IN, V34
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Liao Y, 2022, IEEE T IMAGE PROCESS, V31, P4266, DOI 10.1109/TIP.2022.3181516
   Lin L, 2022, IEEE T MULTIMEDIA, V24, P1922, DOI 10.1109/TMM.2021.3074008
   Liu C, 2023, IEEE T MULTIMEDIA, V25, P3657, DOI 10.1109/TMM.2022.3163578
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Paszke A, 2019, ADV NEUR IN, V32
   Peters ME, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1499, DOI 10.5771/9783845286846
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Radford A, 2021, PR MACH LEARN RES, V139
   Sak H, 2014, INTERSPEECH, P338
   Shi HC, 2021, IEEE T MULTIMEDIA, V23, P995, DOI 10.1109/TMM.2020.2991504
   Shim JH, 2023, AAAI CONF ARTIF INTE, P2263
   Shuai B, 2019, IEEE T IMAGE PROCESS, V28, P1378, DOI 10.1109/TIP.2018.2878975
   Tang JJ, 2023, PROC CVPR IEEE, P23570, DOI 10.1109/CVPR52729.2023.02257
   Tenney I, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4593
   Vaswani A., 2017, C WORKSHOP NEURAL IN, P6000
   Wang ZQ, 2022, PROC CVPR IEEE, P11676, DOI 10.1109/CVPR52688.2022.01139
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Yang JY, 2022, PROC CVPR IEEE, P15650, DOI 10.1109/CVPR52688.2022.01522
   Yang SB, 2021, PROC CVPR IEEE, P11261, DOI 10.1109/CVPR46437.2021.01111
   Yang Z, 2022, PROC CVPR IEEE, P18134, DOI 10.1109/CVPR52688.2022.01762
   Ye LW, 2020, IEEE T MULTIMEDIA, V22, P3224, DOI 10.1109/TMM.2020.2971171
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhu CY, 2022, LECT NOTES COMPUT SC, V13695, P598, DOI 10.1007/978-3-031-19833-5_35
NR 45
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5823
EP 5833
DI 10.1109/TMM.2023.3340062
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100024
DA 2024-08-05
ER

PT J
AU Ding, ZW
   Lan, GL
   Song, YZ
   Yang, ZW
AF Ding, Zhiwei
   Lan, Guilin
   Song, Yanzhi
   Yang, Zhouwang
TI SGIR: Star Graph-Based Interaction for Efficient and Robust Multimodal
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stars; Feature extraction; Visualization; Transformers; Task analysis;
   Medical services; Noise measurement; Multimodal representation;
   multimodal sentiment analysis; healthcare; modal interaction
AB Multimodal representation aims to integrate information from multiple modalities to improve overall performance. Recent works utilizing pairwise interactions have been proposed to deal with the long-range inter-modal and intra-modal dependencies in modeling multimodal data. However, these works usually feature high model complexity, and they are not robust to noisy multimodal data. To address these problems, we propose a novel multimodal representation method that learns private and hub representations of modalities. These representations and their connections form a star graph, a basis for Star Graph-based Interaction (SGI). SGI not only captures the long-range dependencies in multimodal data but also has two natural properties. Firstly, the number of modal interactions increases linearly with the number of modalities, which is computationally efficient compared with the square increase rate of pairwise interactions in previous works. Secondly, the indirect modal interactions through the hub representation in SGI (rather than the direct pairwise interactions between modalities) ensure the model's robustness to noisy modalities. Experiments on five benchmark datasets demonstrate that our new SGI representation (SGIR) achieves state-of-the-art performance on various multimodal tasks, and our qualitative and quantitative analyses show the excellent generalization ability of SGIR. Further experiments reveal that SGIR still outperforms widely used baseline models when modalities are corrupted by low levels of noise.
C1 [Ding, Zhiwei; Lan, Guilin; Song, Yanzhi; Yang, Zhouwang] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Yang, ZW (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM zwding@mail.ustc.edu.cn; lgl09805012@mail.ustc.edu.cn;
   yanzhis@ustc.edu.cn; yangzw@ustc.edu.cn
FU Anhui Center for Applied Mathematics
FX No Statement Available
CR Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cai XY, 2023, IEEE T MULTIMEDIA, V25, P845, DOI 10.1109/TMM.2021.3132724
   Dancette C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1554, DOI 10.1109/ICCV48922.2021.00160
   Darabi S, 2020, IEEE J BIOMED HEALTH, V24, P3268, DOI 10.1109/JBHI.2020.2984931
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Eyben F, 2010, P 18 INT C MULT 2010, DOI DOI 10.1145/1873951.1874246
   Frantzidis CA, 2010, IEEE T INF TECHNOL B, V14, P309, DOI 10.1109/TITB.2009.2038481
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo XB, 2023, IEEE T MULTIMEDIA, V25, P2085, DOI 10.1109/TMM.2022.3142448
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Hao YC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P221, DOI 10.18653/v1/P17-1021
   Hasan M.K., 2019, arXiv
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hou M, 2019, ADV NEUR IN, V32
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZH, 2015, Arxiv, DOI arXiv:1508.01991
   Jayakumar S. M., 2019, P INT C LEARN REPR, P1
   Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35
   Le Q., 2014, INT C MACH LEARN
   Li RN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5060
   Liang PP, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1569
   Lianget P., 2021, P NEUR INF PROC SYST
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Mai Sijie, 2023, IEEE Transactions on Multimedia, P4121, DOI 10.1109/TMM.2022.3171679
   Mai SJ, 2022, IEEE T MULTIMEDIA, V24, P2488, DOI 10.1109/TMM.2021.3082398
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Parisot S, 2018, MED IMAGE ANAL, V48, P117, DOI 10.1016/j.media.2018.06.001
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tsai Y.-H. H., 2019, P INT C REPR LEARN
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Verma S, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3627
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P4909, DOI 10.1109/TMM.2022.3183830
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wei Han, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P6, DOI 10.1145/3462244.3479919
   Xue H., 2021, PROC ADV NEURAL INF, P4514
   Yang S., 2021, ADV NEUR IN, V34
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yuan JB, 2019, LECT NOTES COMPUT SC, V11769, P721, DOI 10.1007/978-3-030-32226-7_80
   Yuan ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4400, DOI 10.1145/3474085.3475585
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zhang DD, 2020, BMC MED INFORM DECIS, V20, DOI 10.1186/s12911-020-01297-6
   Zhang S, 2021, INFORM FUSION, V73, P1, DOI 10.1016/j.inffus.2021.02.022
   Zhao JM, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2608
   Zhu T, 2023, IEEE T MULTIMEDIA, V25, P3375, DOI 10.1109/TMM.2022.3160060
NR 51
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4217
EP 4229
DI 10.1109/TMM.2023.3321404
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100032
DA 2024-08-05
ER

PT J
AU Fang, SM
   Lin, ZW
   Yan, K
   Li, J
   Lin, XM
   Ji, RR
AF Fang, Shuman
   Lin, Zhiwen
   Yan, Ke
   Li, Jie
   Lin, Xianming
   Ji, Rongrong
TI HODN: Disentangling Human-Object Feature for HOI Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoding; Feature extraction; Transformers; Task analysis;
   Visualization; Object detection; Detectors; Disentangling features;
   Human-Object Interaction detection; transformer; visual attention
AB The task of Human-Object Interaction (HOI) detection is to detect humans and their interactions with surrounding objects, where transformer-based methods show dominant advances currently. However, these methods ignore the relationship among humans, objects, and interactions: 1) human features are more contributive than object ones to interaction prediction; 2) interactive information disturbs the detection of objects but helps human detection. In this article, we propose a Human and Object Disentangling Network (HODN) to model the HOI relationships explicitly, where humans and objects are first detected by two disentangling decoders independently and then processed by an interaction decoder. Considering that human features are more contributive to interaction, we propose a Human-Guide Linking method to make sure the interaction decoder focuses on the human-centric regions with human features as the positional embeddings. To handle the opposite influences of interactions on humans and objects, we propose a Stop-Gradient Mechanism to stop interaction gradients from optimizing the object detection but to allow them to optimize the human detection. Our proposed method achieves competitive performance on both the V-COCO and the HICO-Det datasets. It can be combined with existing methods easily for state-of-the-art results.
C1 [Fang, Shuman; Li, Jie; Lin, Xianming; Ji, Rongrong] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
   [Lin, Zhiwen; Yan, Ke] Tencent, Youtu Lab, Shanghai 200233, Peoples R China.
   [Ji, Rongrong] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China.
   [Ji, Rongrong] Xiamen Univ, Fujian Engn Res Ctr Trusted Artificial Intelligenc, Xiamen 361005, Peoples R China.
C3 Xiamen University; Tencent; Xiamen University; Xiamen University
RP Lin, XM (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
EM fangshuman@stu.xmu.edu.cn; xavier.lin@foxmail.com;
   kerwinyan@tencent.com; lijie.32@outlook.com; linxm@xmu.edu.cn;
   rrji@xmu.edu.cn
FU National Key Ramp;D Program of China
FX No Statement Available
CR Bansal A, 2020, AAAI CONF ARTIF INTE, V34, P10460
   Carion N., 2020, EUR C COMP VIS, P213
   Chao YW, 2018, IEEE WINT CONF APPL, P381, DOI 10.1109/WACV.2018.00048
   Chen Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P696, DOI 10.1007/978-3-030-58610-2_41
   Chen MF, 2021, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR46437.2021.00889
   Dong-Jin Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P718, DOI 10.1007/978-3-030-58589-1_43
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao Chen, 2018, BRIT MACH VIS C
   Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872
   Gupta S, 2015, Arxiv, DOI arXiv:1505.04474
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou Z, 2021, PROC CVPR IEEE, P14641, DOI 10.1109/CVPR46437.2021.01441
   Iftekhar ASM, 2022, PROC CVPR IEEE, P5343, DOI 10.1109/CVPR52688.2022.00528
   Kim B, 2021, PROC CVPR IEEE, P74, DOI 10.1109/CVPR46437.2021.00014
   Kim Bumsoo, 2020, ECCV, P498, DOI DOI 10.1007/978-3-030
   Li YL, 2020, PROC CVPR IEEE, P379, DOI [10.1109/ICEMME51517.2020.00080, 10.1109/CVPR42600.2020.00046]
   Li YL, 2019, PROC CVPR IEEE, P3580, DOI 10.1109/CVPR.2019.00370
   Li Yong-Lu, 2020, ADV NEURAL INFORM PR, V33, P5011
   Li ZM, 2022, AAAI CONF ARTIF INTE, P1509
   Liao Y, 2022, PROC CVPR IEEE, P20091, DOI 10.1109/CVPR52688.2022.01949
   Liao Y, 2020, PROC CVPR IEEE, P479, DOI 10.1109/CVPR42600.2020.00056
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1104
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4235, DOI 10.1145/3394171.3413600
   Loshchilov I., 2018, INT C LEARN REPR
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Tamura M, 2021, PROC CVPR IEEE, P10405, DOI 10.1109/CVPR46437.2021.01027
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tsai TJ, 2015, IEEE T MULTIMEDIA, V17, P1550, DOI 10.1109/TMM.2015.2454332
   Ulutan Oytun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13614, DOI 10.1109/CVPR42600.2020.01363
   Wan B, 2019, IEEE I CONF COMP VIS, P9468, DOI 10.1109/ICCV.2019.00956
   Wang H., 2020, P EUR C COMP VIS, P48
   Wang L, 2017, IEEE T MULTIMEDIA, V19, P646, DOI 10.1109/TMM.2016.2617079
   Wang TC, 2020, PROC CVPR IEEE, P4115, DOI 10.1109/CVPR42600.2020.00417
   Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Xubin Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P69, DOI 10.1007/978-3-030-58565-5_5
   Yang D., 2020, P INT JOINT C ART IN
   Yang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P248, DOI 10.1007/978-3-030-58568-6_15
   Yuan HJ, 2022, AAAI CONF ARTIF INTE, P3206
   Zhang AX, 2021, ADV NEUR IN, V34
   Zhang Y, 2022, PROC CVPR IEEE, P19526, DOI 10.1109/CVPR52688.2022.01894
   Zhi Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P584, DOI 10.1007/978-3-030-58555-6_35
   Zhong XB, 2021, PROC CVPR IEEE, P13229, DOI 10.1109/CVPR46437.2021.01303
   Zhou DS, 2022, PROC CVPR IEEE, P19546, DOI 10.1109/CVPR52688.2022.01896
   Zou C, 2021, PROC CVPR IEEE, P11820, DOI 10.1109/CVPR46437.2021.01165
NR 50
TC 0
Z9 0
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3125
EP 3136
DI 10.1109/TMM.2023.3307896
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700014
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Hu, ZJ
   Ma, X
   Liu, Y
   Chen, G
   Liu, YX
   Dannenberg, RB
AF Hu, Zhejing
   Ma, Xiao
   Liu, Yan
   Chen, Gong
   Liu, Yongxu
   Dannenberg, Roger B.
TI The Beauty of Repetition: An Algorithmic Composition Model With
   Motif-Level Repetition Generator and Outline-to-Music Generator in
   Symbolic Music Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Music; Transformers; Computational modeling; Machine
   learning algorithms; Rhythm; Multimedia computing; Algorithmic
   composition; symbolic music generation; repetition modeling
AB Most musical compositions utilize repetition as a fundamental element to create captivating aesthetic experiences. However, the potential of repetition in machine-learning-based algorithmic composition has not been thoroughly investigated. This article aims to make an initial attempt at repetition modeling by generating motif-level repetitions and integrating them into music through a combination of example-based and domain knowledge-based learning techniques. The article presents a new Motif-to-music Generation Model (MGM) that combines a motif-level repetition generator (MRG) and an outline-to-music generator (O2MG). To train this model, a new music repetition dataset (MRD) has been created, which includes 584,329 samples from various categories of motif repetition and 3,545 outline-music sequences from pop piano music. The MRG uses a Transformer encoder to learn the representation of music notes from MRD, while the repetition-aware learner in MRG takes advantage of the unique characteristics of repetitions based on music theory. The O2MG applies a novel outline-to-music learning strategy to learn the relationships among motif-level repetitions in the music and generate music based on these repetitions. The experiments show that MGM can generate a variety of beautiful repetitions with any given motif, improving the music quality and structure of machine-composed music.
C1 [Hu, Zhejing; Ma, Xiao; Liu, Yan; Chen, Gong; Liu, Yongxu] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Dannenberg, Roger B.] CarnegieMellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
C3 Hong Kong Polytechnic University
RP Liu, Y (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
EM zhejing.hu@connect.polyu.hk; edward-xiao.ma@polyu.edu.hk;
   csyliu@comp.polyu.edu.hk; gong-cg.chen@polyu.edu.hk;
   yongxu.liu@connect.polyu.hk; rbd@cs.cmu.edu
OI Liu, Yongxu/0000-0002-7267-7972; Dannenberg, Roger/0000-0003-1823-9856;
   MA, Xiao/0000-0002-5125-2510
FU DaSAIL-Music Generation via Machine Composition
FX No Statement Available
CR Huang CZA, 2018, Arxiv, DOI arXiv:1809.04281
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bao CH, 2023, IEEE T MULTIMEDIA, V25, P3602, DOI 10.1109/TMM.2022.3163543
   Chen G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1607, DOI 10.1145/3240508.3240604
   Collins T., 2017, J CREATIVE MUSIC SYS, V1
   Dai S., 2021, PROC 22 INT SOC MUSI
   Dai S., 2020, PROC JOINT C AI MUSI
   Dai SQ, 2022, J NEW MUSIC RES, V51, P69, DOI 10.1080/09298215.2023.2166848
   Dong HW, 2018, AAAI CONF ARTIF INTE, P34
   Ens Jeffrey, 2020, arXiv, DOI [10.48550/arXiv.2008.06048, DOI 10.48550/ARXIV.2008.06048]
   Fang L, 2021, Arxiv, DOI arXiv:2101.00822
   Fink R, 2005, REPEATING OURSELVES: AMERICAN MINIMAL MUSIC AS CULTURAL PRACTICE, P1
   Fu X, 2023, IEEE T MULTIMEDIA, V25, P3763, DOI 10.1109/TMM.2022.3165718
   Hernandez-Olivan Carlos., 2022, Advances in Speech and Music Technology (Signals and Communication Technology), P25, DOI DOI 10.1007/978-3-031-18444-4_2
   Hsiao WY, 2021, AAAI CONF ARTIF INTE, V35, P178
   Hsu JL, 2001, IEEE T MULTIMEDIA, V3, P311, DOI 10.1109/6046.944475
   Hu ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1223, DOI 10.1145/3503161.3548130
   Huang YSA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1180, DOI 10.1145/3394171.3413671
   Jhamtani H., 2019, MACH LEARN MUSIC DIS
   Jiang JY, 2020, INT CONF ACOUST SPEE, P516, DOI [10.1109/icassp40776.2020.9054554, 10.1109/ICASSP40776.2020.9054554]
   Kingma D. P., 2014, arXiv
   KOCH GR, 1984, TEMPO, V149, P44
   Kroher N, 2018, IEEE T MULTIMEDIA, V20, P1291, DOI 10.1109/TMM.2017.2771450
   Mailman J. B., 2007, Psychol. Music, V35, P363
   Mangal S, 2019, Arxiv, DOI arXiv:1908.01080
   Medeot Gabriele, 2018, P 19 INT SOC MUS INF, P725
   Muhamed A, 2021, AAAI CONF ARTIF INTE, V35, P408
   Nattiez J.-J., 1990, Music and Discourse-Toward a Semiology of Music
   Pareyon G., 2011, On Musical Self-Similarity: Intersemiosis as Synecdoche and Analogy
   Ren Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1198, DOI 10.1145/3394171.3413721
   Shih YJ, 2023, IEEE T MULTIMEDIA, V25, P3495, DOI 10.1109/TMM.2022.3161851
   Sun GF, 2021, IEEE T MULTIMEDIA, V23, P497, DOI 10.1109/TMM.2020.2981989
   Vaswani A., 2017, C WORKSHOP NEURAL IN, P6000
   Wang Z., 2020, PROC 21 INT SOC MUSI
   Wu J, 2020, ARTIF INTELL-AMST, V286, DOI 10.1016/j.artint.2020.103303
   Wu J, 2020, IEEE T CYBERNETICS, V50, P2749, DOI 10.1109/TCYB.2019.2953194
   Wu Y., 2020, PROC INT SOC MUSIC I, P142
   Yang L.-C., 2017, ISMIR, P324
   Zhang N, 2023, IEEE T NEUR NET LEAR, V34, P1754, DOI 10.1109/TNNLS.2020.2990746
   Zhang XY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1204, DOI 10.1145/3503161.3548084
   Zou Y, 2022, INT CONF ACOUST SPEE, P191, DOI 10.1109/ICASSP43922.2022.9747802
NR 41
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4320
EP 4333
DI 10.1109/TMM.2023.3321495
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100058
DA 2024-08-05
ER

PT J
AU Lan, SY
   Wang, ZL
   Wei, ER
   Roy-Chowdhury, AK
   Zhu, Q
AF Lan, Shuyue
   Wang, Zhilu
   Wei, Ermin
   Roy-Chowdhury, Amit K.
   Zhu, Qi
TI Collaborative Multi-Agent Video Fast-Forwarding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Streaming media; Robot vision systems; Reinforcement learning;
   Multi-agent systems; Collaboration; Task analysis; Video
   fast-forwarding; multi-agent systems; reinforcement learning
ID CONSENSUS; OPTIMIZATION
AB Multi-agent applications have recently gained significant popularity. In many computer vision tasks, a network of agents, such as a team of robots with cameras, could work collaboratively to perceive the environment for efficient and accurate situation awareness. However, these agents often have limited computation, communication, and storage resources. Thus, reducing resource consumption while still providing an accurate perception of the environment becomes an important goal when deploying multi-agent systems. To achieve this goal, we identify and leverage the overlap among different camera views in multi-agent systems for reducing the processing, transmission and storage of redundant/unimportant video frames. Specifically, we have developed two collaborative multi-agent video fast-forwarding frameworks in distributed and centralized settings, respectively. In these frameworks, each individual agent can selectively process or skip video frames at adjustable paces based on multiple strategies via reinforcement learning. Multiple agents then collaboratively sense the environment via either 1) a consensus-based distributed framework called DMVF that periodically updates the fast-forwarding strategies of agents by establishing communication and consensus among connected neighbors, or 2) a centralized framework called MFFNet that utilizes a central controller to decide the fast-forwarding strategies for agents based on collected data. We demonstrate the efficacy and efficiency of our proposed frameworks on a real-world surveillance video dataset VideoWeb and a new simulated driving dataset CarlaSim, through extensive simulations and deployment on an embedded platform with TCP communication. We show that compared with other approaches in the literature, our frameworks achieve better coverage of important frames, while significantly reducing the number of frames processed at each agent.
C1 [Lan, Shuyue; Wang, Zhilu; Wei, Ermin; Zhu, Qi] Northwestern Univ, Elect & Comp Engn, Evanston, IL 60208 USA.
   [Roy-Chowdhury, Amit K.] Univ Calif Riverside, Elect & Comp Engn, Riverside, CA 92521 USA.
C3 Northwestern University; University of California System; University of
   California Riverside
RP Zhu, Q (corresponding author), Northwestern Univ, Elect & Comp Engn, Evanston, IL 60208 USA.
EM shuyuelan2018@u.northwestern.edu; zhilu.wang@u.northwestern.edu;
   ermin.wei@northwestern.edu; amitrc@ece.ucr.edu; qzhu@northwestern.edu
RI Lan, Shuyue/GWQ-7150-2022; Wang, Zhilu/KHX-4982-2024
OI Wang, Zhilu/0000-0002-6645-262X
FU NSF
FX No Statement Available
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919
   Cheng KY, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P789
   Denina G, 2011, DISTRIBUTED VIDEO SENSOR NETWORKS, P335, DOI 10.1007/978-0-85729-127-1_23
   Dosovitskiy A., 2017, P 1 ANN C ROB LEARN, P1, DOI DOI 10.48550/ARXIV.1711.03938
   Elfeki M, 2022, IEEE WINT CONF APPL, P185, DOI 10.1109/WACV51458.2022.00026
   Elhamifar E, 2019, IEEE I CONF COMP VIS, P6350, DOI 10.1109/ICCV.2019.00644
   Elhamifar E, 2017, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2017.197
   Elhamifar E, 2012, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2012.6247852
   Foerster JN, 2016, ADV NEUR IN, V29
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Gong BQ, 2014, ADV NEUR IN, V27
   Guan GL, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2632267
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Halperin T, 2018, IEEE T CIRC SYST VID, V28, P1248, DOI 10.1109/TCSVT.2017.2651051
   Jiang J., 2011, P 19 ACM INT C MULT, P1061
   Jiang J., 2010, P IEEE COMP VIS PATT, P64
   Joshi N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766954
   Kamal AT, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2484339
   Khosla A, 2013, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR.2013.348
   Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538
   Kong XY, 2017, PROC CVPR IEEE, P7072, DOI 10.1109/CVPR.2017.748
   Krull A, 2017, PROC CVPR IEEE, P2566, DOI 10.1109/CVPR.2017.275
   Lan SY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1075, DOI 10.1145/3394171.3413767
   Lan SY, 2018, PROC CVPR IEEE, P6771, DOI 10.1109/CVPR.2018.00708
   Lifshitz I, 2016, LECT NOTES COMPUT SC, V9906, P246, DOI 10.1007/978-3-319-46475-6_16
   Matei I, 2011, IEEE J-STSP, V5, P754, DOI 10.1109/JSTSP.2011.2120593
   Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316
   Melo Silva Michel, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P557, DOI 10.1007/978-3-319-46604-0_40
   Nedic A, 2011, IEEE T AUTOMAT CONTR, V56, P1337, DOI 10.1109/TAC.2010.2079650
   Nedic A, 2010, IEEE T AUTOMAT CONTR, V55, P922, DOI 10.1109/TAC.2010.2041686
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Ou SH, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2331916
   Ou SH, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853799
   Panda R, 2017, IEEE I CONF COMP VIS, P3677, DOI 10.1109/ICCV.2017.395
   Panda R, 2017, PROC CVPR IEEE, P4274, DOI 10.1109/CVPR.2017.455
   Panda R, 2017, IEEE T MULTIMEDIA, V19, P2010, DOI 10.1109/TMM.2017.2708981
   Panda R, 2016, INT C PATT RECOG, P2971, DOI 10.1109/ICPR.2016.7900089
   Peker KA, 2003, PROC SPIE, V5242, P26, DOI 10.1117/12.514742
   Peker KA, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P414, DOI 10.1109/ICIP.2001.958139
   Petrovic N, 2005, MULTIMED TOOLS APPL, V26, P327, DOI 10.1007/s11042-005-0895-9
   Poleg Y, 2015, PROC CVPR IEEE, P4768, DOI 10.1109/CVPR.2015.7299109
   Ramos W., 2020, P IEEE CVF C COMP VI, P10931
   Ramos WLS, 2016, IEEE IMAGE PROC, P3334, DOI 10.1109/ICIP.2016.7532977
   Ren LL, 2018, LECT NOTES COMPUT SC, V11207, P605, DOI 10.1007/978-3-030-01219-9_36
   Ren Z, 2017, PROC CVPR IEEE, P1151, DOI 10.1109/CVPR.2017.128
   Rochan M, 2019, PROC CVPR IEEE, P7894, DOI 10.1109/CVPR.2019.00809
   Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9909, P71, DOI 10.1007/978-3-319-46454-1_5
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Sukhbaatar S, 2016, ADV NEUR IN, V29
   Szegedy C., 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594]
   Tsitsiklis JN, 1984, Problems in decentralized decision making and computation
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang HZ, 2006, INT C PATT RECOG, P223
   Wu WH, 2019, IEEE I CONF COMP VIS, P6231, DOI 10.1109/ICCV.2019.00632
   Wu ZX, 2019, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2019.00137
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang YJ, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9040750
   Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322
NR 63
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1041
EP 1054
DI 10.1109/TMM.2023.3275853
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700037
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, Z
   Wang, XY
   Liu, YL
   Jin, LW
   Huang, YC
   Ding, K
AF Li, Zhe
   Wang, Xinyu
   Liu, Yuliang
   Jin, Lianwen
   Huang, Yichao
   Ding, Kai
TI Improving Handwritten Mathematical Expression Recognition via Similar
   Symbol Distinguishing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Handwritten mathematical expression recognition; path signature;
   language model; dynamic time warping; ensemble
ID GENERATION; ATTENTION; CONTEXT; DECODER; NETWORK
AB Handwritten mathematical expression recognition (HMER) is an essential task in the OCR community, which consists of two sub-tasks, i.e., symbol recognition and structure parsing. Modern literature treats HMER as a LaTeX sequence predicting problem that simultaneously recognizes symbols and parses the structures of MEs. Although deep learning-based HMER methods have been achieving promising results on public benchmarks, it is admitted that the misclassification error between visually similar symbols still prevents these approaches from more generalized scenes. In this paper, we try to solve this issue from three aspects. 1) We enhanced the feature extraction progress by introducing path signature features, which incorporates local writing details and global spatial information. 2) We developed a language model that uses contextual information to correct the symbols misclassified by vision-only-based recognition models. 3) We solved the misalignment problem in existing ensemble method by designing a dynamic time warping (DTW) based algorithm. By combining the above improvements, our method achieved state-of-the-art results on three CROHME benchmarks, outperforming previous methods by a large margin.
C1 [Li, Zhe; Jin, Lianwen] South China Univ Technol, Sch Elect & Informat, Guangzhou 510640, Peoples R China.
   [Wang, Xinyu] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
   [Liu, Yuliang] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Hongshan, Peoples R China.
   [Huang, Yichao; Ding, Kai] IntSig Informat Co Ltd, Shanghai 200433, Peoples R China.
C3 South China University of Technology; University of Adelaide; Huazhong
   University of Science & Technology
RP Liu, YL (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Hongshan, Peoples R China.; Ding, K (corresponding author), IntSig Informat Co Ltd, Shanghai 200433, Peoples R China.
EM zheli0205@foxmail.com; xinyu.wang02@adelaide.edu.au; ylliu@hust.edu.cn;
   lianwen.jin@gmail.com; charlie_huang@intsig.net; danny_ding@intsig.net
RI Jin, Lianwen/AAJ-6536-2020
OI Li, Zhe/0000-0002-0372-8895; Jin, Lianwen/0000-0002-5456-0957
FU National Natural Science Foundation of China
FX No Statement Available
CR Alvaro F, 2016, PATTERN RECOGN, V51, P135, DOI 10.1016/j.patcog.2015.09.013
   Alvaro F, 2014, PATTERN RECOGN LETT, V35, P58, DOI 10.1016/j.patrec.2012.09.023
   Anderson R H, 1967, S INT SYST EXP APPL, P436, DOI 10.1145/2402536.2402585
   Le AD, 2020, IEEE COMPUT SOC CONF, P2413, DOI 10.1109/CVPRW50498.2020.00291
   Le AD, 2019, PATTERN RECOGN LETT, V128, P255, DOI 10.1016/j.patrec.2019.09.002
   Le AD, 2017, PROC INT CONF DOC, P1056, DOI 10.1109/ICDAR.2017.175
   [Anonymous], 2005, PROC 4 INT S INF COM
   Bahdanau D., 2015, P INT C LEARN REPR, P1
   Chan KF, 2001, PATTERN RECOGN, V34, P1671, DOI 10.1016/S0031-3203(00)00102-3
   Chen K.-T, 1958, T AM MATH SOC, V89, P395, DOI DOI 10.2307/1993193
   Chorowski J, 2015, ADV NEUR IN, V28
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Deng Yuntian, 2017, PR MACH LEARN RES, P980
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding HS, 2021, LECT NOTES COMPUT SC, V12822, P602, DOI 10.1007/978-3-030-86331-9_39
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Hambly B, 2010, ANN MATH, V171, P109, DOI 10.4007/annals.2010.171.109
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ung HQ, 2021, LECT NOTES COMPUT SC, V12917, P403, DOI 10.1007/978-3-030-86159-9_29
   Jiaming Wang, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1181, DOI 10.1109/ICDAR.2019.00191
   Julca-Aguilar F, 2020, INT J DOC ANAL RECOG, V23, P143, DOI 10.1007/s10032-019-00349-6
   Király FJ, 2019, J MACH LEARN RES, V20
   Lai SX, 2017, PROC INT CONF DOC, P400, DOI 10.1109/ICDAR.2017.73
   Lavirotte S, 1997, PROC INT CONF DOC, P357, DOI 10.1109/ICDAR.1997.619871
   Li Z, 2020, INT CONF FRONT HAND, P175, DOI 10.1109/ICFHR2020.2020.00041
   Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020
   Lyonset T., 2002, System Control and Rough Paths
   MacLean S, 2013, INT J DOC ANAL RECOG, V16, P139, DOI 10.1007/s10032-012-0184-x
   Mahdavi Mahshad, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1533, DOI 10.1109/ICDAR.2019.00247
   Mahdavi Mahshad, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P647, DOI 10.1109/ICDAR.2019.00109
   Mahdavi M, 2020, IEEE COMPUT SOC CONF, P2429, DOI 10.1109/CVPRW50498.2020.00293
   Mouchère H, 2016, INT CONF FRONT HAND, P607, DOI [10.1109/ICFHR.2016.0116, 10.1109/ICFHR.2016.108]
   Mouchère H, 2014, INT CONF FRONT HAND, P791, DOI 10.1109/ICFHR.2014.138
   Mouchère H, 2016, INT J DOC ANAL RECOG, V19, P173, DOI 10.1007/s10032-016-0263-5
   Nguyen CT, 2021, LECT NOTES COMPUT SC, V12822, P617, DOI 10.1007/978-3-030-86331-9_40
   Noya E, 2021, INT C PATT RECOG, P5696, DOI 10.1109/ICPR48806.2021.9412273
   Senin Pavel., 2007, Science, P1
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Truong TN, 2020, INT CONF FRONT HAND, P181, DOI 10.1109/ICFHR2020.2020.00042
   Tu ZP, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P76
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DH, 2020, INT CONF FRONT HAND, P211, DOI 10.1109/ICFHR2020.2020.00047
   Wang JM, 2021, LECT NOTES COMPUT SC, V12823, P39, DOI 10.1007/978-3-030-86334-0_3
   Wang JM, 2021, PATTERN RECOGN, V119, DOI 10.1016/j.patcog.2021.108047
   Wu JW, 2020, INT J COMPUT VISION, V128, P2386, DOI 10.1007/s11263-020-01291-5
   Wu JW, 2021, AAAI CONF ARTIF INTE, V35, P2925
   Wu JW, 2019, LECT NOTES ARTIF INT, V11051, P18, DOI 10.1007/978-3-030-10925-7_2
   Xie ZC, 2018, IEEE T PATTERN ANAL, V40, P1903, DOI 10.1109/TPAMI.2017.2732978
   Yang WX, 2016, PATTERN RECOGN, V58, P190, DOI 10.1016/j.patcog.2016.04.007
   Zanibbi R, 2012, INT J DOC ANAL RECOG, V15, P331, DOI 10.1007/s10032-011-0174-4
   Zhang JS, 2017, PROC INT CONF DOC, P902, DOI 10.1109/ICDAR.2017.152
   Zhang JS, 2021, IEEE T MULTIMEDIA, V23, P2471, DOI 10.1109/TMM.2020.3011316
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang JS, 2018, INT C PATT RECOG, P2245, DOI 10.1109/ICPR.2018.8546031
   Zhang JS, 2017, PATTERN RECOGN, V71, P196, DOI 10.1016/j.patcog.2017.06.017
   Zhang Jianshu, 2020, INT C MACHINE LEARNI, P11076
   Zhang T, 2020, NEURAL COMPUT APPL, V32, P4689, DOI 10.1007/s00521-018-3817-2
   Zhao WQ, 2021, LECT NOTES COMPUT SC, V12822, P570, DOI 10.1007/978-3-030-86331-9_37
NR 58
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 90
EP 102
DI 10.1109/TMM.2023.3260648
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500015
DA 2024-08-05
ER

PT J
AU Lin, FZ
   Ge, SM
   Bao, KX
   Yan, CG
   Zeng, D
AF Lin, Fanzhao
   Ge, Shiming
   Bao, Kexin
   Yan, Chenggang
   Zeng, Dan
TI Learning Shape-Biased Representations for Infrared Small Target
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Object detection; Feature extraction; Decoding; Kernel; Image
   reconstruction; Task analysis; Infrared small target detection;
   shape-biased representation; object segmentation; deep learning
ID FILTER; MODEL; DIM
AB Typically, infrared small target detection aims to accurately localize objects from complex backgrounds where the object textures are often dim and the object shapes are varying. A feasible solution is learning discriminative representations with deep convolutional neural networks (CNNs). However, the representations learned by traditional deep CNNs often suffer from low shape bias. In this work, we propose a unified framework to learn shape-biased representations for facilitating infrared small target detection by explicitly incorporating shape information into model learning. The framework cascades a large-kernel encoder and a shape-guided decoder to learn discriminative shape-biased representations in an end-to-end manner. The large-kernel encoder describes infrared images into shape-preserving representations by using a few convolutions whose kernel size is as large as $9\times 9$, in contrast to commonly used $3\times 3$. The shape-guided decoder simultaneously addresses two tasks: decodes the encoder representations via upsampling reconstruction to reconstruct the segmentation, and hierarchically fuses the decoder representations and edge information via cascaded gated ResNet blocks to reconstruct the contour. In this way, the learned shape-biased representations are effective for identifying infrared small targets. Extensive experiments show our approach outperforms 18 state-of-the-arts.
C1 [Lin, Fanzhao; Ge, Shiming; Bao, Kexin] Chinese Acad Sci, Inst Informat Engn, Beijing 100084, Peoples R China.
   [Lin, Fanzhao; Ge, Shiming; Bao, Kexin] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
   [Yan, Chenggang] Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou, Peoples R China.
   [Zeng, Dan] Hangzhou Dianzi Univ, Lishui Inst, Lishui 323000, Peoples R China.
   [Zeng, Dan] Shanghai Univ, Dept Commun Engn, Shanghai 200040, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Hangzhou Dianzi University; Hangzhou
   Dianzi University; Shanghai University
RP Ge, SM (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100084, Peoples R China.
EM linfanzhao@iie.ac.cn; geshiming@iie.ac.cn; baokexin@iie.ac.cn;
   cgyan@hdu.edu.cn; dzeng@shu.edu.cn
OI Lin, Fanzhao/0000-0003-0339-9400; Ge, Shiming/0000-0001-5293-310X; Bao,
   Kexin/0000-0003-4921-6112
FU National Key Research and Development Plan
FX No Statement Available
CR Asadi N, 2020, Arxiv, DOI arXiv:1909.08245
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bai XZ, 2010, PATTERN RECOGN, V43, P2145, DOI 10.1016/j.patcog.2009.12.023
   Barnett J., 1989, Proceedings of the SPIE - The International Society for Optical Engineering, V1050, P10
   Brochu F, 2019, Arxiv, DOI arXiv:1907.12892
   Cao YP, 2016, IEEE T CIRC SYST VID, V26, P2176, DOI 10.1109/TCSVT.2015.2493443
   Chen CLP, 2014, IEEE T GEOSCI REMOTE, V52, P574, DOI 10.1109/TGRS.2013.2242477
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen X, 2022, IEEE T MULTIMEDIA, V24, P288, DOI 10.1109/TMM.2021.3050069
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dai Y., 2021, PROC IEEE WINTER C A, P950
   Dai YM, 2021, IEEE T GEOSCI REMOTE, V59, P9813, DOI 10.1109/TGRS.2020.3044958
   Dai YM, 2017, IEEE J-STARS, V10, P3752, DOI 10.1109/JSTARS.2017.2700023
   Deshpande SD, 1999, P SOC PHOTO-OPT INS, V3809, P74, DOI 10.1117/12.364049
   Ding H, 2021, PROC CVPR IEEE, P8274, DOI 10.1109/CVPR46437.2021.00818
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Gao CQ, 2013, IEEE T IMAGE PROCESS, V22, P4996, DOI 10.1109/TIP.2013.2281420
   Ge SM, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3536426
   Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P2610, DOI 10.1109/TIP.2019.2950508
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Ge SM, 2017, PROC CVPR IEEE, P426, DOI 10.1109/CVPR.2017.53
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Geirhos R., 2020, Advances in Neural Information Processing Systems, V33, P13890
   Geirhos R, 2021, 35 C NEURAL INFORM P, V34
   Geirhos Robert, 2019, INT C LEARNING REPRE
   Han JH, 2021, IEEE GEOSCI REMOTE S, V18, P1670, DOI 10.1109/LGRS.2020.3004978
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YJ, 2015, INFRARED PHYS TECHN, V68, P98, DOI 10.1016/j.infrared.2014.10.022
   Hermann K., 2020, ADV NEURAL INFORM PR, V33, P19000
   Hou QY, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3141584
   Hua YY, 2023, IEEE T IMAGE PROCESS, V32, P1668, DOI 10.1109/TIP.2023.3246793
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Kim S, 2011, ELECTRON LETT, V47, P105, DOI 10.1049/el.2010.2066
   Kingma D. P., 2014, arXiv
   Lee S, 2022, IEEE COMPUT SOC CONF, P4322, DOI 10.1109/CVPRW56347.2022.00478
   Li BY, 2023, IEEE T IMAGE PROCESS, V32, P1745, DOI 10.1109/TIP.2022.3199107
   Li KQ, 2015, IEEE T MULTIMEDIA, V17, P994, DOI 10.1109/TMM.2015.2433795
   Li YS, 2018, ISPRS J PHOTOGRAMM, V146, P182, DOI 10.1016/j.isprsjprs.2018.09.014
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu Q, 2020, IEEE T MULTIMEDIA, V22, P666, DOI 10.1109/TMM.2019.2932615
   Min Li, 2015, Image and Graphics. 8th International Conference, ICIG 2015. Proceedings: LNCS 9219, P393, DOI 10.1007/978-3-319-21969-1_34
   Ng DW, 2022, INT CONF ACOUST SPEE, P3603, DOI 10.1109/ICASSP43922.2022.9747025
   Olivier R, 2012, INT J ADV COMPUT SC, V3, P25
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shao Z., 2008, Int. Arch. Photogrammetry, Remote Sens. Spatial Inf. Sci., V37, P1299
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stojanov S, 2021, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR46437.2021.00184
   Sun Y, 2021, IEEE T GEOSCI REMOTE, V59, P3737, DOI 10.1109/TGRS.2020.3022069
   Teutsch M., 2010, PROC INT WATER SIDE, P1
   Tuli S., 2021, P ANN M COGN SCI SOC, P1844, DOI DOI 10.48550/ARXIV.2105.07197
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2019, IEEE I CONF COMP VIS, P8508, DOI 10.1109/ICCV.2019.00860
   Wang J., 2018, J. Northwestern PolytechnicalUniv., V36, P258
   Wang KW, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3163410
   Wang X, 2012, INFRARED PHYS TECHN, V55, P513, DOI 10.1016/j.infrared.2012.08.004
   Wang ZY, 2023, 2023 2ND ASIA CONFERENCE ON ALGORITHMS, COMPUTING AND MACHINE LEARNING, CACML 2023, P547, DOI 10.1145/3590003.3590104
   Wang ZM, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2022.3230415
   Wu TH, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3235002
   Wu X, 2023, IEEE T IMAGE PROCESS, V32, P364, DOI 10.1109/TIP.2022.3228497
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Ying XY, 2023, PROC CVPR IEEE, P15528, DOI 10.1109/CVPR52729.2023.01490
   Zhang J, 2021, IEEE INTERNET THINGS, V8, P7789, DOI 10.1109/JIOT.2020.3039359
   Zhang LD, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11040382
   Zhang LD, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111821
   Zhang MJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1857, DOI 10.1145/3503161.3548264
   Zhang MJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1730, DOI 10.1145/3503161.3547817
   Zhang MJ, 2022, PROC CVPR IEEE, P867, DOI 10.1109/CVPR52688.2022.00095
   Zhang RH, 2022, IEEE T MULTIMEDIA, V24, P1735, DOI 10.1109/TMM.2021.3070138
   Zhang TF, 2023, IEEE T AERO ELEC SYS, V59, P4250, DOI [10.1109/TAES.2023.3238703, 10.1109/ICASSP49357.2023.10096500]
   Zhao Mingjing, 2023, Transactions of Beijing Institute of Technology, P755, DOI 10.15918/j.tbit1001-0645.2022.181
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou HB, 2023, IEEE T MULTIMEDIA, V25, P635, DOI 10.1109/TMM.2021.3129609
   Zhou XY, 2022, INT GEOSCI REMOTE SE, P867, DOI 10.1109/IGARSS46834.2022.9883812
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
NR 81
TC 2
Z9 2
U1 20
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4681
EP 4692
DI 10.1109/TMM.2023.3325743
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100031
DA 2024-08-05
ER

PT J
AU Liu, AA
   Huang, CX
   Xu, N
   Tian, HS
   Liu, J
   Zhang, YD
AF Liu, An-An
   Huang, Chenxi
   Xu, Ning
   Tian, Hongshuo
   Liu, Jing
   Zhang, Yongdong
TI Counterfactual Visual Dialog: Robust Commonsense Knowledge Learning From
   Unbiased Training
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Commonsense reasoning; History; Task analysis;
   Correlation; Knowledge based systems; Computational modeling; Visual
   dialog; commonsense; multi-modal; counterfactual
AB Visual Dialog (VD) requires an agent to answer the current question by engaging in a conversation with humans referring to an image. Despite the recent progress, it is beneficial to introduce external commonsense knowledge to fully understand the given image and dialog history. However, the existing knowledge-based VD models are inclined to rely on severe learning bias brought by commonsense, e.g., the retrieved < bus, capable of, transport people > , < bus ,is a ,public transport > , and < bus ,is a, car > can induce a spurious correlation between the question "What is the bus used for?" and the false answer "City bus". There are two challenges to make commonsense learning more robust against spurious correlations: 1) how to disentangle the true effect of "good" commonsense knowledge from the whole, and 2) how to estimate and remove the effect of "bad" commonsense bias on answers. In this article, we propose a novel CounterFactual Commonsense learning scheme for the Visual Dialog task (CFC-VD). First, comparing with the causal graph of existing VD models, we add one new commonsense node and one new link to multi-modal information from history, question, and image. Since the retrieved knowledge prior is subtle and uncontrollable, we consider it as an unobserved confounder in the commonsense node, which leads to spurious correlations for the answer inference. Then, to remove the effect of the confounder, we formulate it as the direct causal effect of commonsense on answers and remove the direct language effect by subtracting it from the total causal effect via counterfactual reasoning. Experimental results certify the effectiveness of our method on the prevailing Visdial v0.9 and Visdial v1.0 datasets.
C1 [Liu, An-An; Huang, Chenxi; Xu, Ning; Tian, Hongshuo; Liu, Jing] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Hefei 230026, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Xu, N (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM anan0422@gmail.com; 2021234291@tju.edu.cn; ningxu@tju.edu.cn;
   kellyeden@tju.edu.cn; jliu_tju@tju.edu.cn; zhyd73@ustc.edu.cn
RI Huang, Chenxi/ITV-4316-2023
OI Huang, Chenxi/0000-0002-5962-2993
FU National Natural Science Foundation of China
FX No Statement Available
CR Cadene Remi, 2019, ADV NEUR IN, P839
   Chen L, 2022, LECT NOTES COMPUT SC, V13696, P95, DOI 10.1007/978-3-031-20059-5_6
   Chen L, 2019, IEEE I CONF COMP VIS, P4612, DOI 10.1109/ICCV.2019.00471
   Chen S, 2022, PROC CVPR IEEE, P15565, DOI 10.1109/CVPR52688.2022.01514
   Chen T., 2022, PROC IEEE INT C MULT, P1
   Cho YS, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030931
   Clark C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4069
   Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Fan HH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3390891
   Fang ZY, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P840
   Feng FL, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P2226
   Fenton N, 2020, ARTIF INTELL, V284, DOI 10.1016/j.artint.2020.103286
   Gan Z, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6463
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   Guo DL, 2019, PROC CVPR IEEE, P10426, DOI 10.1109/CVPR.2019.01068
   Guo D, 2022, IEEE T PATTERN ANAL, V44, P6056, DOI 10.1109/TPAMI.2021.3085755
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4989
   Guo D, 2020, IEEE T IMAGE PROCESS, V29, P6655, DOI 10.1109/TIP.2020.2992888
   Han XJ, 2022, IEEE T CIRC SYST VID, V32, P8611, DOI 10.1109/TCSVT.2022.3193857
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Jiang XZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1265, DOI 10.1145/3394171.3413826
   Jiaxin Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10857, DOI 10.1109/CVPR42600.2020.01087
   Jin Y, 2022, IEEE T MULTIMEDIA, V24, P1896, DOI 10.1109/TMM.2021.3073624
   Kang GC, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2024
   Kingma D.P., 2014, Proc. of ICLR
   Kottur S, 2018, LECT NOTES COMPUT SC, V11219, P160, DOI 10.1007/978-3-030-01267-0_10
   Li J., 2022, PMLR, V162, P12843
   Lin BQ, 2021, NEUROCOMPUTING, V449, P399, DOI 10.1016/j.neucom.2021.03.104
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081
   Lu L., 2017, PROC INT C NEURAL IN, P314
   McAuley J, 2012, LECT NOTES COMPUT SC, V7575, P828, DOI 10.1007/978-3-642-33765-9_59
   Mihalcea R., 2004, P 2004 C EMP METH NA, P404
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Niu YL, 2019, PROC CVPR IEEE, P6672, DOI 10.1109/CVPR.2019.00684
   Park DH, 2018, PROC CVPR IEEE, P8779, DOI 10.1109/CVPR.2018.00915
   Pearl J, 2000, Causality: Models, reasoning and inference, V19, DOI DOI 10.1017/CBO9780511803161
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schwartz I, 2019, PROC CVPR IEEE, P2039, DOI 10.1109/CVPR.2019.00214
   Selvaraju RR, 2019, IEEE I CONF COMP VIS, P2591, DOI 10.1109/ICCV.2019.00268
   Seo PH, 2017, ADV NEUR IN, V30
   Song GL, 2021, IEEE T MULTIMEDIA, V23, P1882, DOI 10.1109/TMM.2020.3004963
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Sun KL, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2022.103008
   Sun T, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3548211
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P8483, DOI 10.1109/TIP.2021.3115672
   Van-Quang Nguyen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P223, DOI 10.1007/978-3-030-58586-0_14
   Wang JY, 2021, IEEE T MULTIMEDIA, V24, P3369, DOI 10.1109/TMM.2021.3097171
   Wu JL, 2022, AAAI CONF ARTIF INTE, P2712
   Wu Q, 2018, PROC CVPR IEEE, P6106, DOI 10.1109/CVPR.2018.00639
   Wu YL, 2021, IEEE T MULTIMEDIA, V23, P559, DOI 10.1109/TMM.2020.2985540
   Xiao S., 2022, PROC C EMPIR ICAL ME, P8188
   Yang TH, 2019, IEEE I CONF COMP VIS, P2561, DOI 10.1109/ICCV.2019.00265
   Yang X, 2023, IEEE T PATTERN ANAL, V45, P12996, DOI 10.1109/TPAMI.2021.3121705
   Yu J, 2021, IEEE T IMAGE PROCESS, V30, P220, DOI 10.1109/TIP.2020.3034494
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P642, DOI 10.1007/978-3-030-58592-1_38
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhang SY, 2022, IEEE COMPUT SOC CONF, P4599, DOI 10.1109/CVPRW56347.2022.00506
   Zhao L, 2023, IEEE T CIRC SYST VID, V33, P861, DOI 10.1109/TCSVT.2022.3207228
   Zhao L, 2022, NEUROCOMPUTING, V488, P54, DOI 10.1016/j.neucom.2021.10.121
   Zheng ZL, 2019, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2019.00683
   Zhu Y, 2022, IEEE T PATTERN ANAL, V44, P7190, DOI 10.1109/TPAMI.2021.3093360
NR 67
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1639
EP 1651
DI 10.1109/TMM.2023.3284594
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800034
DA 2024-08-05
ER

PT J
AU Lu, ZY
   Jin, L
   Li, ZC
   Tang, JH
AF Lu, Zhengyun
   Jin, Lu
   Li, Zechao
   Tang, Jinhui
TI Self-Paced Relational Contrastive Hashing for Large-Scale Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive learning; deep hashing; image retrieval; self-paced learning
AB Supervised deep hashing aims to learn hash functions using label information. Existing methods learn hash functions by employing either pairwise/triplet loss to explore the point-to-point relation or center loss to explore the point-to-class relation. However, these methods overlook the collaboration between the above two kinds of relations and the hardness of pairs. In this work, we propose a novel Self-Paced Relational Contrastive Hashing (SPRCH) method with a single learning objective to capture valuable discriminative information from hard pairs using both the point-to-point and point-to-class relations. To exploit the above two kinds of relations, the Relational Contrastive Hash (RCH) loss is proposed, which ensures that each data anchor is closer to all similar data points and corresponding class centers in the Hamming space compared to dissimilar ones. Moreover, the proposed RCH loss reduces the drastic imbalance between point-to-point pairs and point-to-class pairs by rebalancing their weights. To prioritize hard pairs, a self-paced learning schedule is proposed, assigning higher weights to these pairs in the RCH loss. The self-paced learning schedule assigns dynamic weights to pairs according to their similarities and the training process. In this way, deep hash model can initially learn universal patterns from the entire set of pairs and then gradually acquire more valuable discriminative information from hard pairs. Experimental results on four widely-used image retrieval datasets demonstrate that our proposed SPRCH method significantly outperforms the state-of-the-art supervised deep hash methods.
C1 [Lu, Zhengyun; Jin, Lu; Li, Zechao; Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
C3 Nanjing University of Science & Technology
RP Jin, L (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM zhengyunlu@njust.edu.cn; lu.jin@njust.edu.cn; zechao.li@njust.edu.cn;
   jinhuitang@njust.edu.cn
FU National Key Research and Development Program of China
FX No Statement Available
CR Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen T., 2020, Adv. Neural Inf. Process. Syst., V33, P22243, DOI DOI 10.48550/ARXIV.2006.10029
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen X, 2021, AUTOPHAGY, V17, P2054, DOI 10.1080/15548627.2020.1810918
   Chen XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9620, DOI 10.1109/ICCV48922.2021.00950
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Deng C, 2020, IEEE T NEUR NET LEAR, V31, P2189, DOI 10.1109/TNNLS.2019.2929068
   Doan KD, 2022, PROC CVPR IEEE, P9437, DOI 10.1109/CVPR52688.2022.00923
   Fan LX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P825
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Grill JB, 2020, P 34 INT C NEUR INF, V33, P21271
   Gui J, 2018, IEEE T NEUR NET LEAR, V29, P608, DOI 10.1109/TNNLS.2016.2636870
   Guo S, 2018, LECT NOTES COMPUT SC, V11214, P139, DOI 10.1007/978-3-030-01249-6_9
   Hoe JT, 2021, ADV NEUR IN, V34
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Jiang L, 2014, ADV NEUR IN, V27
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Jin L, 2019, IEEE T NEUR NET LEAR, V30, P1429, DOI 10.1109/TNNLS.2018.2869601
   Jin L, 2018, IEEE T IMAGE PROCESS, V27, P1405, DOI 10.1109/TIP.2017.2776745
   Jin S, 2020, AAAI CONF ARTIF INTE, V34, P11157
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang M., 2020, Advances in Neural Information Processing Systems, P21357
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kumar M, 2010, Advances in Neural Information Processing Systems, V23
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li C, 2019, ADV NEUR IN, V32
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li YQ, 2018, IEEE T PATTERN ANAL, V40, P1526, DOI 10.1109/TPAMI.2017.2710186
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin L, 2018, IEEE T PATTERN ANAL, V40, P7, DOI 10.1109/TPAMI.2017.2652459
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Lu J, 2018, PR MACH LEARN RES, V80
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Lu X, 2021, IEEE T MULTIMEDIA, V23, P4541, DOI 10.1109/TMM.2020.3044473
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mikriukov G, 2022, INT CONF ACOUST SPEE, P4463, DOI 10.1109/ICASSP43922.2022.9746251
   Norouzi M., 2012, P NIPS, P1070, DOI DOI 10.5555/2999134.2999253
   Norouzi M., 2011, P 28 INT C MACH LEAR, P353
   Qiu Zexuan, 2021, IJCAI, P959
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Su S., 2018, P ADV NEUR INF PROC, P806, DOI DOI 10.5555/3326943.3327018
   Tang JH, 2018, IEEE T NEUR NET LEAR, V29, P6154, DOI 10.1109/TNNLS.2018.2816743
   Tang JH, 2018, IEEE T CIRC SYST VID, V28, P2730, DOI 10.1109/TCSVT.2017.2715227
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   [王锦荟 Wang Jinhui], 2022, [中国科学. 技术科学, Scientia Sinica Technologica], V52, P713
   Wang JP, 2022, AAAI CONF ARTIF INTE, P2468
   Wang ZJ, 2021, IEEE T MULTIMEDIA, V23, P1274, DOI 10.1109/TMM.2020.2995267
   Weiss Y., 2008, Advances in Neural Information Processing Systems, V21, P1753
   Xiang XG, 2022, IEEE T IMAGE PROCESS, V31, P314, DOI 10.1109/TIP.2021.3131042
   Xie YZ, 2023, IEEE T MULTIMEDIA, V25, P9161, DOI 10.1109/TMM.2023.3248170
   Yang EK, 2019, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2019.00306
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zbontar J, 2021, PR MACH LEARN RES, V139
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
NR 68
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3392
EP 3404
DI 10.1109/TMM.2023.3310333
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200041
DA 2024-08-05
ER

PT J
AU Ma, YE
   Liu, Y
   Wang, LM
   Kang, WX
   Qiao, Y
   Wang, YL
AF Ma, Yuer
   Liu, Yi
   Wang, Limin
   Kang, Wenxiong
   Qiao, Yu
   Wang, Yali
TI Dual Masked Modeling for Weakly-Supervised Temporal Boundary Discovery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal grounding; temporal localization; temporal boundary discovery;
   weakly-supervised learning
ID NETWORK
AB Discovering temporal boundary is critical for untrimmed video tasks, such as temporal sentence grounding and action detection. Due to the labor-intensive boundary annotations, the recent studies focus on the weakly-supervised setting, with only sentences or action tags in the training videos. However, how to align temporal boundaries and textual descriptions is problematic in most weakly-supervised approaches. To alleviate this difficulty, we propose a novel Dual Masked Modeling (DM2) framework, which can effectively enhance clip-text alignment to boost temporal boundary discovery, by cross-modal masked modeling in the dual fashion. Specifically, we introduce two coupled reconstruction branches, i.e., Clip-Aware Masked Text Modeling (C-MTM), and Text-Aware Masked Clip Modeling (T-MCM), after generating a temporal proposal of the underlying clip. In C-MTM, we recover the masked text with visual assistance of the clip proposal. In T-MCM, we recover the masked clip proposal with lingual assistance of the text. Via such complementary reconstruction supervision, our DM2 can cooperatively exploit robust matching between the video clip and the referred text, allowing to unify grounding and localization in a concise manner. Finally, we perform extensive experiments on the popular temporal benchmarks, i.e., Charades-STA, ActivityNet Captions, ActivityNet-v1.3 and THUMOS-14. Our DM2 achieves state-of-the-art for both weakly-supervised temporal grounding and localization. Codes and models will be released afterward.
C1 [Ma, Yuer; Kang, Wenxiong] South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510641, Peoples R China.
   [Ma, Yuer] Guangzhou Natl Lab, Guangzhou Int Bio Isl, Guangzhou 510005, Peoples R China.
   [Liu, Yi; Qiao, Yu; Wang, Yali] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Liu, Yi; Qiao, Yu; Wang, Yali] Shanghai Artificial Intelligence Lab, Shanghai 202150, Peoples R China.
   [Wang, Limin] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
   [Kang, Wenxiong] Pazhou Lab, Young Scholar Project Ctr, Guangzhou 510335, Peoples R China.
C3 South China University of Technology; Chinese Academy of Sciences;
   Shenzhen Institute of Advanced Technology, CAS; Nanjing University;
   Pazhou Lab
RP Wang, YL (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM meyuerma@mail.scut.edu.cn; yi.liu1@siat.ac.cn; lmwang@nju.eud.cn;
   auwxkang@scut.edu.cn; yu.qiao@siat.ac.cn; yl.wang@siat.ac.cn
OI Kang, Wenxiong/0000-0001-9023-7252; Liu, Yi/0000-0002-4727-0572; Wang,
   Limin/0000-0002-3674-7718
FU National Key Ramp;D Program of China
FX No Statement Available
CR Baraka A, 2022, NEURAL COMPUT APPL, V34, P8479, DOI 10.1007/s00521-022-07102-x
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen G, 2022, AAAI CONF ARTIF INTE, P248
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Ghosh S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1984
   He B, 2022, PROC CVPR IEEE, P13915, DOI 10.1109/CVPR52688.2022.01355
   Huang LJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7982, DOI 10.1109/ICCV48922.2021.00790
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu Meng, 2023, ACM COMPUT SURV, V55, P1, DOI DOI 10.1145/3560815
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P6937, DOI 10.1109/TIP.2022.3217368
   Liu Y, 2019, PROC CVPR IEEE, P3599, DOI [10.1109/CVPR.2019.00372, 10.1109/CVPR.2019.00726]
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Luo W, 2021, PROC CVPR IEEE, P9964, DOI 10.1109/CVPR46437.2021.00984
   Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodriguez-Opazo C, 2021, IEEE WINT CONF APPL, P1078, DOI 10.1109/WACV48630.2021.00112
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Song YJ, 2020, Arxiv, DOI arXiv:2003.07048
   Tan J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13506, DOI 10.1109/ICCV48922.2021.01327
   Tong Z., 2022, NeurIPS, V35, P10078
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Wang YX, 2023, IEEE T MULTIMEDIA, V25, P3921, DOI 10.1109/TMM.2022.3168424
   Wu J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1283, DOI 10.1145/3394171.3413862
   Xu HJ, 2018, Arxiv, DOI arXiv:1804.05113
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yang M, 2023, COMPUT VIS IMAGE UND, V232, DOI 10.1016/j.cviu.2023.103692
   Yang WF, 2021, PROC CVPR IEEE, P53, DOI 10.1109/CVPR46437.2021.00012
   Yang WF, 2021, IEEE T IMAGE PROCESS, V30, P3252, DOI 10.1109/TIP.2021.3058614
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yuan YT, 2022, IEEE T PATTERN ANAL, V44, P2725, DOI 10.1109/TPAMI.2020.3038993
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Yueran Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P121, DOI 10.1007/978-3-030-58604-1_8
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang CL, 2022, LECT NOTES COMPUT SC, V13664, P492, DOI 10.1007/978-3-031-19772-7_29
   Zhang H, 2020, Arxiv, DOI arXiv:2004.13931
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4098, DOI 10.1145/3394171.3413967
   Zhang ZJ, 2021, IEEE T MULTIMEDIA, V23, P3306, DOI 10.1109/TMM.2020.3023339
   Zhao H, 2019, IEEE I CONF COMP VIS, P8667, DOI 10.1109/ICCV.2019.00876
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zheng MH, 2022, AAAI CONF ARTIF INTE, P3517
   Zheng MH, 2022, PROC CVPR IEEE, P15534, DOI 10.1109/CVPR52688.2022.01511
   Zheng Wang, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P1459, DOI 10.1145/3474085.3475278
   Zhou H, 2021, PROC CVPR IEEE, P8441, DOI 10.1109/CVPR46437.2021.00834
NR 63
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5694
EP 5704
DI 10.1109/TMM.2023.3338084
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100017
DA 2024-08-05
ER

PT J
AU Meng, LJ
   Jiang, XH
   Sun, TF
   Zhao, ZY
   Xu, Q
AF Meng, Laijin
   Jiang, Xinghao
   Sun, Tanfeng
   Zhao, Zeyu
   Xu, Qiang
TI A Robust Coverless Video Steganography Based on the Similarity of
   Inter-Frames
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coverless information hiding; inter-frames; robust steganography; video
   attacks; video steganography
ID STEGANALYSIS; CNN
AB With a deeper understanding of the security issues in steganography, coverless steganography has become a hotspot due to no modification to the carriers. However, the existing coverless video steganographic algorithms have considered a few types of video attacks. In this paper, a robust coverless video steganography based on the similarity of inter-frames is proposed. First, a public video database is selected and preprocessed to construct a Secret Communication Video Database (SCVD). The similarity score between the first and last frames is calculated for video sorting to utilize the temporal characteristics of videos. After that, the mapping table between the secret information and the SCVD is designed for both senders and receivers. Finally, each secret information segment can be represented by one video sequence in the SCVD according to the mapping table to accomplish the data hiding and extraction. Experimental results show that the proposed method performs much better in capacity, robustness, and security than the state-of-the-art methods. It is worth mentioning that the proposed method overcomes the security issue of transmitting a large amount of auxiliary information in coverless video steganographic algorithms.
C1 [Meng, Laijin; Jiang, Xinghao; Sun, Tanfeng; Zhao, Zeyu] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Natl Engn Lab Informat Content Anal Tech, Shanghai 200240, Peoples R China.
   [Xu, Qiang] City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
   [Xu, Qiang] City Univ Hong Kong, Ctr Intelligent Multidimens Data Anal, Kowloon, Hong Kong 518057, Peoples R China.
C3 Shanghai Jiao Tong University; City University of Hong Kong; City
   University of Hong Kong
RP Jiang, XH (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Natl Engn Lab Informat Content Anal Tech, Shanghai 200240, Peoples R China.
EM menglaijin@sjtu.edu.cn; xhjiang@sjtu.edu.cn; tfsun@sjtu.edu.cn;
   329161318zzy@sjtu.edu.cn; qiangxu027@gmail.com
OI Xu, Qiang/0000-0002-8750-7036; Zhao, Zeyu/0000-0002-3008-0457
FU National Natural Science Foundation of China
FX No Statement Available
CR Biswas Suman, 2023, 2023 7th International Conference on Trends in Electronics and Informatics (ICOEI), P1554, DOI 10.1109/ICOEI56765.2023.10125935
   Bojanowski P, 2014, LECT NOTES COMPUT SC, V8693, P628, DOI 10.1007/978-3-319-10602-1_41
   Bouchama S, 2012, LECT NOTES ENG COMP, P655
   Budhia U, 2006, IEEE T INF FOREN SEC, V1, P502, DOI 10.1109/TIFS.2006.885020
   Caelles S, 2019, Arxiv, DOI arXiv:1905.00737
   Delforouzi A, 2008, CIRC SYST SIGNAL PR, V27, P247, DOI 10.1007/s00034-008-9019-x
   Dong Y, 2023, IEEE T DEPEND SECURE, V20, P769, DOI 10.1109/TDSC.2022.3144139
   Dong Y, 2023, IEEE T MULTIMEDIA, V25, P2698, DOI 10.1109/TMM.2022.3150180
   EURASIP J, 2020, Image Video Process, P1
   Hao Bin, 2011, 2011 IEEE 3rd International Conference on Communication Software and Networks (ICCSN 2011), P406, DOI 10.1109/ICCSN.2011.6013622
   He SH, 2024, IEEE T MULTIMEDIA, V26, P687, DOI 10.1109/TMM.2023.3269663
   Jing HY, 2012, LECT NOTES ARTIF INT, V7197, P91, DOI 10.1007/978-3-642-28490-8_10
   Kapotas SK, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P277, DOI 10.1109/ICME.2008.4607425
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li MH, 2023, SYMMETRY-BASEL, V15, DOI 10.3390/sym15010116
   Li RZ, 2022, CMC-COMPUT MATER CON, V73, P1571, DOI 10.32604/cmc.2022.029378
   Li WJ, 2023, IEEE T MULTIMEDIA, V25, P8320, DOI 10.1109/TMM.2023.3234812
   Li ZH, 2023, IEEE T DEPEND SECURE, V20, P606, DOI 10.1109/TDSC.2022.3140899
   Liu JD, 2022, IEEE T MULTIMEDIA, V24, P2084, DOI 10.1109/TMM.2021.3075858
   Liu Q, 2022, IEEE T CIRC SYST VID, V32, P4038, DOI 10.1109/TCSVT.2021.3108772
   Liu Q, 2020, KNOWL-BASED SYST, V192, DOI 10.1016/j.knosys.2019.105375
   Luo YJ, 2021, IEEE T CIRC SYST VID, V31, P2779, DOI 10.1109/TCSVT.2020.3033945
   Meng LJ, 2024, MULTIMED TOOLS APPL, V83, P13427, DOI 10.1007/s11042-023-15697-z
   Meng LJ, 2023, IEEE T CIRC SYST VID, V33, P3542, DOI 10.1109/TCSVT.2022.3232790
   Pan N, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-00512-8
   Peng WL, 2023, IEEE SIGNAL PROC LET, V30, P299, DOI 10.1109/LSP.2023.3258862
   Rana S, 2020, MULTIMED TOOLS APPL, V79, P5881, DOI 10.1007/s11042-019-08525-w
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tan Y, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/5554058
   Tasdemir K, 2016, IEEE T IMAGE PROCESS, V25, P3316, DOI 10.1109/TIP.2016.2567073
   Wang H., 2016, 4 ACM WORKSHOPINF HI, P127
   Wang KR, 2014, IEEE T INF FOREN SEC, V9, P741, DOI 10.1109/TIFS.2014.2308633
   Yang YY, 2019, MULTIMED TOOLS APPL, V78, P8423, DOI 10.1007/s11042-018-6859-7
   Zhai LM, 2020, IEEE T INF FOREN SEC, V15, P1762, DOI 10.1109/TIFS.2019.2949428
   Zhang H, 2016, MULTIMED TOOLS APPL, V75, P13503, DOI 10.1007/s11042-015-2743-x
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zhang Y. Tan, 2022, Secur. Commun. Netw., V2022
   Zhang Z., 2021, DIGITAL FORENSICS CY, P265
   Zheng SL, 2017, LECT NOTES ARTIF INT, V10363, P536, DOI 10.1007/978-3-319-63315-2_47
   Zhili Zhou, 2015, Cloud Computing and Security. First International Conference, ICCCS 2015. Revised Selected Papers: LNCS 9483, P123, DOI 10.1007/978-3-319-27051-7_11
   Zhou ZL, 2019, SOFT COMPUT, V23, P4927, DOI 10.1007/s00500-018-3151-8
   Zou L., 2021, GEOMETRY VIS 1 INT S, P134
   Zou LM, 2023, IEEE T MULTIMEDIA, V25, P5552, DOI 10.1109/TMM.2022.3194990
NR 43
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5996
EP 6011
DI 10.1109/TMM.2023.3344357
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100027
DA 2024-08-05
ER

PT J
AU Qi, TH
   Xie, HT
   Li, PD
   Ge, JN
   Zhang, YD
AF Qi, Tianhao
   Xie, Hongtao
   Li, Pandeng
   Ge, Jiannan
   Zhang, Yongdong
TI Balanced Classification: A Unified Framework for Long-Tailed Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tail; Detectors; Training; Object detection; Feature extraction; Head;
   Task analysis; Long-tailed object detection; Long-short-term indicators
   pair; Foreground classification balance loss; Feature hallucination
   module
ID QUANTIZATION
AB Conventional detectors suffer from performance degradation when dealing with long-tailed data due to a classification bias towards the majority head categories. In this article, we contend that the learning bias originates from two factors: 1) the unequal competition arising from the imbalanced distribution of foreground categories, and 2) the lack of sample diversity in tail categories. To tackle these issues, we introduce a unified framework called BAlanced CLassification (BACL), which enables adaptive rectification of inequalities caused by disparities in category distribution and dynamic intensification of sample diversities in a synchronized manner. Specifically, a novel foreground classification balance loss (FCBL) is developed to ameliorate the domination of head categories and shift attention to difficult-to-differentiate categories by introducing pairwise class-aware margins and auto-adjusted weight terms, respectively. This loss prevents the over-suppression of tail categories in the context of unequal competition. Moreover, we propose a dynamic feature hallucination module (FHM), which enhances the representation of tail categories in the feature space by synthesizing hallucinated samples to introduce additional data variances. In this divide-and-conquer approach, BACL sets a new state-of-the-art on the challenging LVIS benchmark with a decoupled training pipeline, surpassing vanilla Faster R-CNN with ResNet-50-FPN by 5.8% AP and 16.1% AP for overall and tail categories. Extensive experiments demonstrate that BACL consistently achieves performance improvements across various datasets with different backbones and architectures.
C1 [Qi, Tianhao; Xie, Hongtao; Li, Pandeng; Ge, Jiannan; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xie, HT (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Peoples R China.
EM qth@mail.ustc.edu.cn; htxie@ustc.edu.cn; lpd@mail.ustc.edu.cn;
   gejn@mail.ustc.edu.cn; zhyd73@ustc.edu.cn
OI Qi, Tianhao/0000-0003-3649-5263; Ge, Jiannan/0000-0002-2580-9055
FU National Key Research and Development Program of China
FX No Statement Available
CR Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao KD, 2019, ADV NEUR IN, V32
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chang N, 2021, PR MACH LEARN RES, V139
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3397, DOI 10.1109/ICCV48922.2021.00340
   Ge JN, 2021, AAAI CONF ARTIF INTE, V35, P1406
   Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Hayat Nasir, 2020, P AS C COMP VIS, P155
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y.-Y., 2022, P IEEECVF C COMPUTER, P7000
   Hsieh TI, 2021, AAAI CONF ARTIF INTE, V35, P1549
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168
   Kang B., 2020, 8 INT C LEARN REPR I
   Kingma D. P., 2014, arXiv
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li PD, 2022, AAAI CONF ARTIF INTE, P1367
   Li PD, 2022, IEEE T IMAGE PROCESS, V31, P5909, DOI 10.1109/TIP.2022.3203612
   Li PD, 2022, IEEE T MULTIMEDIA, V24, P981, DOI 10.1109/TMM.2021.3062480
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2023, IEEE T MULTIMEDIA, V25, P4213, DOI 10.1109/TMM.2022.3172548
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Pengkai Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11690, DOI 10.1109/CVPR42600.2020.01171
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren J., 2020, ADV NEURAL INFORM PR, V33, P4175, DOI DOI 10.48550/ARXIV.2007.10740
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tan JR, 2021, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR46437.2021.00173
   Tang K., 2020, P NIPS, P1513
   Tao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P728, DOI 10.1007/978-3-030-58568-6_43
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957
   Wang T, 2021, PROC CVPR IEEE, P3102, DOI 10.1109/CVPR46437.2021.00312
   Wang ZY, 2021, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR46437.2021.00454
   Wu HW, 2022, IEEE T MULTIMEDIA, V24, P4016, DOI 10.1109/TMM.2021.3111491
   Wu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1570, DOI 10.1145/3394171.3413970
   Yang LY, 2021, IEEE T MULTIMEDIA, V23, P835, DOI 10.1109/TMM.2020.2990074
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10988, DOI 10.1109/CVPR42600.2020.01100
   Zang Y., 2021, P IEEECVF INT C COMP, P3457
   Zhan C, 2020, IEEE T MULTIMEDIA, V22, P795, DOI 10.1109/TMM.2019.2931441
   Zhang SY, 2021, PROC CVPR IEEE, P2361, DOI 10.1109/CVPR46437.2021.00239
   Zhao SZ, 2020, AAAI CONF ARTIF INTE, V34, P12967
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 66
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3088
EP 3101
DI 10.1109/TMM.2023.3306968
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700011
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shi, JS
   Gao, P
   Smolic, A
AF Shi, Jinsong
   Gao, Pan
   Smolic, Aljosa
TI Blind Image Quality Assessment via Transformer Predicted Error Map and
   Perceptual Quality Token
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Transformers; Predictive models; Image quality; Feature
   extraction; Visualization; Task analysis; NR-IQA; transformer; predicted
   error map; pceptual quality token
ID NATURAL SCENE STATISTICS
AB Image quality assessment is a fundamental problem in the field of image processing, and due to the lack of reference images in most practical scenarios, no-reference image quality assessment (NR-IQA), has gained increasing attention recently. With the development of deep learning technology, many deep neural network-based NR-IQA methods have been developed, which try to learn the image quality based on the understanding of database information. Currently, Transformer has achieved remarkable progress in various vision tasks. Since the characteristics of the attention mechanism in Transformer fit the global perceptual impact of artifacts perceived by a human, Transformer is thus well suited for image quality assessment tasks. In this paper, we propose a Transformer based NR-IQA model using a predicted objective error map and perceptual quality token. Specifically, we firstly generate the predicted error map by pre-training one model consisting of a Transformer encoder and decoder, in which the objective difference between the distorted and the reference images is used as supervision. Then, we freeze the parameters of the pre-trained model and design another branch using the vision Transformer to extract the perceptual quality token for feature fusion with the predicted error map. Finally, the fused features are regressed to the final image quality score. Extensive experiments have shown that our proposed method outperforms the state-of-the-art methods in both authentic and synthetic image datasets. Moreover, the attentional map extracted by the perceptual quality token also does conform to the characteristics of the human visual system.
C1 [Shi, Jinsong; Gao, Pan] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Shi, Jinsong; Gao, Pan] MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China.
   [Smolic, Aljosa] Lucerne Univ Appl Sci & Arts, Comp Sci Dept, Luzern, Switzerland.
C3 Nanjing University of Aeronautics & Astronautics
RP Gao, P (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM srachej@gmail.com; gaopan.1005@gmail.com; aljosa.smolic@hslu.ch
OI Smolic, Aljosa/0000-0001-7033-3335
FU Natural Science Foundation of China
FX No Statement Available
CR Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Cao YQ, 2023, IEEE T IMAGE PROCESS, V32, P3847, DOI 10.1109/TIP.2023.3290528
   Cao YQ, 2023, IEEE T IMAGE PROCESS, V32, P1882, DOI 10.1109/TIP.2023.3251695
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   Dosovitskiy A., 2021, PROC ICLR
   Gao XB, 2013, IEEE T NEUR NET LEAR, V24, P2013, DOI 10.1109/TNNLS.2013.2271356
   Gao Y., 2023, IEEE Trans. Circuits Syst. Video Technol.
   Gao YX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P997, DOI 10.1145/3503161.3547872
   Gao YX, 2023, IEEE T CIRC SYST VID, V33, P2656, DOI 10.1109/TCSVT.2022.3229839
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh S.A., 2022, P IEEECVF WINTER C A, P1220
   Golestaneh S, 2016, IEEE T IMAGE PROCESS, V25, P5293, DOI 10.1109/TIP.2016.2601821
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kingma D. P., 2014, arXiv
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Lin V., 2019, P IEEE 11 INT C QUAL, P1
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Mier JC, 2021, IEEE IMAGE PROC, P1484, DOI 10.1109/ICIP42928.2021.9506217
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh Hamid R, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun W, 2023, IEEE J-STSP, V17, P1178, DOI 10.1109/JSTSP.2023.3270621
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wu JJ, 2017, IEEE INT CONF COMP V, P510, DOI 10.1109/ICCVW.2017.67
   Xiongkuo Min, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457921
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yan QS, 2019, IEEE T IMAGE PROCESS, V28, P2200, DOI 10.1109/TIP.2018.2883741
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Zeng H, 2017, Arxiv, DOI arXiv:1708.08190
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang P, 2015, PROC CVPR IEEE, P2394, DOI 10.1109/CVPR.2015.7298853
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
NR 61
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4641
EP 4651
DI 10.1109/TMM.2023.3325719
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100047
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wan, B
   Zhou, XF
   Sun, YQ
   Wang, TY
   Lv, CT
   Wang, S
   Yin, HB
   Yan, CG
AF Wan, Bin
   Zhou, Xiaofei
   Sun, Yaoqi
   Wang, Tingyu
   Lv, Chengtao
   Wang, Shuai
   Yin, Haibing
   Yan, Chenggang
TI MFFNet: Multi-Modal Feature Fusion Network for V-D-T Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-modal feature fusion network; V-D-T salient object detection;
   triple-modal deep fusion encoder; progressive feature enhancement
   decoder
AB This article discusses the limitations of single- and two-modal salient object detection (SOD) methods and the emergence of multi-modal SOD techniques that integrate Visible, Depth, or Thermal information. However, current multi-modal methods often rely on simple fusion techniques such as addition, multiplication and concatenation, to combine the different modalities, which is ineffective for challenging scenes, such as low illumination and background messy. To address this issue, we propose a novel multi-modal feature fusion network (MFFNet) for V-D-T salient object detection, where the two key points are the triple-modal deep fusion encoder and the progressive feature enhancement decoder. The MFFNet's triple-modal deep fusion (TDF) module is designed to integrate the features of the three modalities and explore their complementarity by utilizing mutual optimization during the encoding phase. In addition, the progressive feature enhancement decoder consists of the weighted context-enhanced feature (WCF) module, region optimization (RO) module and boundary perception (BP) module to produce region-aware and contour-aware features. After that, a multi-scale fusion (MF) module is proposed to integrate these features and generate high-quality saliency maps. We conduct extensive experiments on the VDT-2048 dataset, and our results show that the proposed MFFNet outperforms 12 state-of-the-art multi-modal methods.
C1 [Wan, Bin; Zhou, Xiaofei; Wang, Tingyu; Lv, Chengtao; Yan, Chenggang] Hangzhou Dianzi Univ, Sch Automat, Hangzhou 310018, Peoples R China.
   [Sun, Yaoqi] Hangzhou Dianzi Univ, Lishui Inst, Sch Automat, Hangzhou 310018, Peoples R China.
   [Wang, Shuai] Hangzhou Dianzi Univ, Lishui Inst, Sch Cyberspace, Hangzhou 310018, Peoples R China.
   [Yin, Haibing] Hangzhou Dianzi Univ, Sch Commun Engn, Lishui Inst, Hangzhou 310018, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University; Hangzhou Dianzi
   University; Hangzhou Dianzi University
RP Zhou, XF; Yan, CG (corresponding author), Hangzhou Dianzi Univ, Sch Automat, Hangzhou 310018, Peoples R China.
EM wanbinxueshu@icloud.com; zxforchid@outlook.com; syq@hdu.edu.cn;
   wongtyu@foxmail.com; chengtaolv@foxmail.com; shuaiwang.tai@gmail.com;
   yhb@hdu.edu.cn; cgyan@hdu.edu.cn
RI lv, chengtao/KDO-8670-2024
OI lv, chengtao/0000-0003-4928-9493; Sun, Yaoqi/0000-0001-8874-241X; Wang,
   Shuai/0000-0003-3730-6401; Wan, Bin/0000-0001-7838-6669; Zhou,
   Xiaofei/0000-0002-7977-9728
FU National Key Research and Development Program of China
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bi HB, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109194
   Chen G, 2023, IEEE T CIRC SYST VID, V33, P1787, DOI 10.1109/TCSVT.2022.3215979
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6308, DOI 10.1109/TCSVT.2022.3166914
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6981, DOI 10.1109/TCSVT.2022.3178173
   Chen H, 2020, IEEE T IMAGE PROCESS, V29, P8407, DOI 10.1109/TIP.2020.3014734
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2024, IEEE T NEUR NET LEAR, V35, P4309, DOI 10.1109/TNNLS.2022.3202241
   Cong RM, 2023, IEEE T MULTIMEDIA, V25, P6971, DOI 10.1109/TMM.2022.3216476
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan RC, 2019, PROC CVPR IEEE, P6096, DOI 10.1109/CVPR.2019.00626
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Gao LA, 2023, NEUROCOMPUTING, V518, P507, DOI 10.1016/j.neucom.2022.11.031
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   He HY, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01312-y
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang HX, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-07654-x
   Huang K, 2022, PATTERN RECOGN LETT, V160, P122, DOI 10.1016/j.patrec.2022.06.006
   Huang NC, 2022, IEEE T IMAGE PROCESS, V31, P6621, DOI 10.1109/TIP.2022.3214092
   Huo FS, 2022, IEEE T CIRC SYST VID, V32, P3111, DOI 10.1109/TCSVT.2021.3102268
   Hussain T, 2022, IEEE COMPUT SOC CONF, P2877, DOI 10.1109/CVPRW56347.2022.00325
   Jian MW, 2021, INFORM SCIENCES, V576, P819, DOI 10.1016/j.ins.2021.08.069
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Jin X, 2022, IEEE T CIRC SYST VID, V32, P7632, DOI 10.1109/TCSVT.2022.3180274
   Liu JJ, 2023, IEEE T PATTERN ANAL, V45, P887, DOI 10.1109/TPAMI.2021.3140168
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Ma S, 2023, APPL INTELL, V53, P9038, DOI 10.1007/s10489-022-03950-1
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Nie J, 2023, IEEE T MULTIMEDIA, V25, P5601, DOI 10.1109/TMM.2022.3197369
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rahman Md Atiqur, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P234, DOI 10.1007/978-3-319-50835-1_22
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Rudolph M, 2022, IEEE WINT CONF APPL, P1829, DOI 10.1109/WACV51458.2022.00189
   Sharma P, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3511021
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song KC, 2023, IEEE-ASME T MECH, V28, P1558, DOI 10.1109/TMECH.2022.3215909
   Song MK, 2022, IEEE T IMAGE PROCESS, V31, P6124, DOI 10.1109/TIP.2022.3205747
   Tieleman T., 2012, COURSERA: Neural Networks for Machine Learning, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Tu ZZ, 2023, IEEE T MULTIMEDIA, V25, P4163, DOI 10.1109/TMM.2022.3171688
   Tu ZZ, 2022, IEEE T IMAGE PROCESS, V31, P3752, DOI 10.1109/TIP.2022.3176540
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Wan B, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3250302
   Wang H, 2023, ENG APPL ARTIF INTEL, V118, DOI 10.1016/j.engappai.2022.105640
   Wang J, 2022, IEEE T CIRC SYST VID, V32, P2949, DOI 10.1109/TCSVT.2021.3099120
   Wang YY, 2021, MEASUREMENT, V170, DOI 10.1016/j.measurement.2020.108698
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu J, 2023, DIGIT SIGNAL PROCESS, V133
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu ZW, 2022, INT CONF 3D VISION, P403, DOI 10.1109/3DV57658.2022.00052
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao K, 2019, IEEE I CONF COMP VIS, P8848, DOI 10.1109/ICCV.2019.00894
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhou WJ, 2023, IEEE T IMAGE PROCESS, V32, P1329, DOI 10.1109/TIP.2023.3242775
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhou X., 2021, IEEE Trans.Geosci. Remote Sens., V60, P1
   Zhou XF, 2023, IEEE T CYBERNETICS, V53, P539, DOI 10.1109/TCYB.2022.3163152
   Zhou XF, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3132082
   Zhuge MC, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108644
NR 71
TC 2
Z9 2
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2069
EP 2081
DI 10.1109/TMM.2023.3291823
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800024
DA 2024-08-05
ER

PT J
AU Wang, HR
   Chen, ZZ
   Chen, CW
AF Wang, Huairui
   Chen, Zhenzhong
   Chen, Chang Wen
TI Learned Video Compression via Heterogeneous Deformable Compensation
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Learned video compression; motion compensation; heterogeneous deformable
   convolution; divisive normalization; multi-frame enhancement
ID IMAGE
AB Learned video compression has recently emerged as an essential research topic in developing advanced video compression technologies, where motion compensation is considered one of the most challenging issues. In this article, we propose a learned video compression framework via heterogeneous deformable compensation strategy (HDCVC) to tackle the problems of unstable compression performance caused by single-size deformable kernels in downsampled feature domain. More specifically, instead of utilizing optical flow warping or single-size-kernel deformable alignment, the proposed algorithm extracts features from the two adjacent frames to estimate content-adaptive heterogeneous deformable (HetDeform) kernel offsets. Then we align the features extracted from the reference frames with the HetDeform convolution to accomplish motion compensation. Moreover, we design a Spatial-Neighborhood-Conditioned Divisive Normalization (SNCDN) to reduce spatial statistic dependencies and achieve more effective data Gaussianization combined with the Generalized Divisive Normalization. Furthermore, we propose a multi-frame enhanced reconstruction module for exploiting context and temporal information for final quality enhancement. Experimental results indicate that HDCVC achieves superior performance than the recent state-of-the-art learned video compression approaches.
C1 [Wang, Huairui; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
   [Chen, Chang Wen] Hong Kong Polytech Univ, Hong Kong, Peoples R China.
C3 Wuhan University; Hong Kong Polytechnic University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
EM wanghr827@whu.edu.cn; zzchen@ieee.org; changwen.chen@polyu.edu.hk
OI Wang, Huairui/0009-0004-2870-6117; Chen, Chang Wen/0000-0002-6720-234X
FU National Natural Science Foundation of China
FX No Statement Available
CR Agustsson Eirikur, 2020, P IEEE CVF C COMP VI, P8503, DOI DOI 10.1109/CVPR42600.2020.00853
   Akbari M, 2021, IEEE T MULTIMEDIA, V23, P3013, DOI 10.1109/TMM.2021.3068523
   Bakir N, 2021, IEEE T MULTIMEDIA, V23, P2972, DOI 10.1109/TMM.2021.3068563
   Balle J., 2018, ICLR
   Balle J., 2017, P INT C LEARN REPR
   Balle Johannes, 2017, 5 INT C LEARNING REP
   Begaint J., 2020, arXiv
   Bellard F., 2014, Bpg image format
   Bjontegaard G., 2001, Calculation of average PSNR differences between RD-Curves
   Bossen F., 2013, JCTVCL1100, V12, P7
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   Cheng ZX, 2020, IEEE T MULTIMEDIA, V22, P860, DOI 10.1109/TMM.2019.2938345
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Guo Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P456, DOI 10.1007/978-3-030-58536-5_27
   Habibian A, 2019, IEEE I CONF COMP VIS, P7032, DOI 10.1109/ICCV.2019.00713
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He PS, 2021, IEEE T MULTIMEDIA, V23, P3179, DOI 10.1109/TMM.2020.3021234
   Lee Jaehoon, 2018, INT C LEARN REPR
   Li J., 2021, P ADV NEUR INF PROC, V34, p18 114
   Liu HJ, 2022, IEEE T CIRC SYST VID, V32, P5650, DOI 10.1109/TCSVT.2022.3150014
   Liu HJ, 2020, AAAI CONF ARTIF INTE, V34, P11580
   Lu G, 2021, IEEE T PATTERN ANAL, V43, P3292, DOI 10.1109/TPAMI.2020.2988453
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Lyu SW, 2008, PROC CVPR IEEE, P3721
   Ma HC, 2020, IEEE T MULTIMEDIA, V22, P1667, DOI 10.1109/TMM.2019.2957990
   Mercat A, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P297, DOI 10.1145/3339825.3394937
   Minnen D, 2018, ADV NEUR IN, V31
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Rippel O, 2019, IEEE I CONF COMP VIS, P3453, DOI 10.1109/ICCV.2019.00355
   Singh P, 2019, PROC CVPR IEEE, P4830, DOI 10.1109/CVPR.2019.00497
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   videolan, x264, the free H.264/AVC encoder
   videolan, X265, the free H.265/HEVC encoder
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang R, 2021, IEEE J-STSP, V15, P388, DOI 10.1109/JSTSP.2020.3043590
   Yang R, 2020, PROC CVPR IEEE, P6627, DOI 10.1109/CVPR42600.2020.00666
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhihao Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P193, DOI 10.1007/978-3-030-58536-5_12
NR 47
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1855
EP 1866
DI 10.1109/TMM.2023.3289763
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800033
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xu, BW
   Liang, HR
   Liang, RH
   Chen, P
AF Xu, Binwei
   Liang, Haoran
   Liang, Ronghua
   Chen, Peng
TI Synthesize Boundaries: A Boundary-Aware Self-Consistent Framework for
   Weakly Supervised Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image edge detection; Annotations; Object recognition; Detectors; Object
   detection; Integral equations; Labeling; Salient object detection;
   scribble; weakly supervise; synthetic image; self-consistent framework
AB Fully supervised salient object detection (SOD) has made considerable progress based on expensive and time-consuming data with pixel-wise annotations. Recently, to relieve the labeling burden while maintaining performance, some scribble-based SOD methods have been proposed. However, learning precise boundary details from scribble annotations that lack edge information is still difficult. In this article, we propose to learn precise boundaries from our designed synthetic images and labels without introducing any extra auxiliary data. The synthetic image creates boundary information by inserting synthetic concave regions that simulate the real concave regions of salient objects. Furthermore, we propose a novel self-consistent framework that consists of a global integral branch (GIB) and a boundary-aware branch (BAB) to train a saliency detector. GIB aims to identify integral salient objects, whose input is the original image. BAB aims to help predict accurate boundaries, whose input is the synthetic image. These two branches are connected through a self-consistent loss to guide the saliency detector to predict precise boundaries while identifying salient objects. Experimental results on five benchmarks demonstrate that our method outperforms the state-of-the-art weakly supervised SOD methods and further narrows the gap with the fully supervised methods.
C1 [Xu, Binwei; Liang, Haoran; Liang, Ronghua; Chen, Peng] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China.
C3 Zhejiang University of Technology
RP Liang, HR (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China.
EM xubinwei@zjut.edu.cn; haoran@zjut.edu.cn; rhliang@zjut.edu.cn;
   chenpeng@zjut.edu.cn
RI Xu, Binwei/GPS-9140-2022
FU National Key Research and Development Program of China
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2017, J COMPUT SCI TECH-CH, V32, P110, DOI 10.1007/s11390-017-1681-7
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cong RM, 2023, IEEE T CIRC SYST VID, V33, P534, DOI 10.1109/TCSVT.2022.3205182
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Gao SY, 2022, AAAI CONF ARTIF INTE, P670
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He M., 2022, arXiv
   Hyeonsoo Lee, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P14, DOI 10.1007/978-3-030-59710-8_2
   Ji ZHX, 2019, LECT NOTES COMPUT SC, V11766, P175, DOI 10.1007/978-3-030-32248-9_20
   Li GB, 2018, AAAI CONF ARTIF INTE, P7024
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liang PP, 2016, IEEE SIGNAL PROC LET, V23, P949, DOI 10.1109/LSP.2016.2556706
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu XM, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108341
   Liu Xiaoming, 2022, ACM Trans. Multimedia Comput., Commun., Appl., V18, P1
   Obukhov A, 2019, Arxiv, DOI arXiv:1906.04651
   Pan ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7396, DOI 10.1109/ICCV48922.2021.00732
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2023, IEEE T MULTIMEDIA, V25, P2888, DOI 10.1109/TMM.2022.3152567
   Piao YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4116, DOI 10.1109/ICCV48922.2021.00410
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31
   Tang M, 2018, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2018.00195
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Xu BW, 2021, AAAI CONF ARTIF INTE, V35, P3004
   Xu JS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15334, DOI 10.1109/ICCV48922.2021.01507
   Xu YQ, 2022, IEEE T IMAGE PROCESS, V31, P2148, DOI 10.1109/TIP.2022.3151999
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu SY, 2021, AAAI CONF ARTIF INTE, V35, P3234
   Zeng Y, 2019, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR.2019.00623
   Zhang J., 2020, PROC IEEECVF C COMP, P12546
   Zhang K., 2022, P IEEE CVF C COMP VI, P11656
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhuge MC, 2023, IEEE T PATTERN ANAL, V45, P3738, DOI 10.1109/TPAMI.2022.3179526
NR 63
TC 0
Z9 0
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4194
EP 4205
DI 10.1109/TMM.2023.3321393
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100007
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yuan, MQ
   Jia, GY
   Bao, BK
AF Yuan, Mengqi
   Jia, Gengyun
   Bao, Bing-Kun
TI GPT-Based Knowledge Guiding Network for Commonsense Video Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Task analysis; Knowledge engineering; Feature
   extraction; Training; Decoding; Commonsense-based video captioning;
   external knowledge guiding; multi-step training
AB Video-based commonsense captioning aims to generate captions for the video content while providing multiple commonsense about the underlying event. Existing methods utilize video features to explore and generate commonsense containing latent semantics. However, this process needs to overcome the complex semantic gap between visible videos and invisible commonsense, which is not supported by the limited knowledge in existing video captioning datasets. To this end, we propose a novel GPT-based Two-stage Knowledge Guiding Network (TKG-Net), which uses GPT to augment datasets knowledge and introduces a cross-attention mechanism to fuse multimodal knowledge. Specifically, to augment knowledge, we set prompts and finetune GPT to imagine and reason based on the video content description at the first stage. At the second stage, to prevent over-reasoning caused by the loss of visual features in GPT, TKG-Net extracts high-level semantic representations of commonsense knowledge and fuses them with video features in a cross-attention mechanism for multimodal semantic interaction. Our experiments on the large-scale Video-to-Commonsense dataset manifest significant improvements over the previous state-of-the-art approach on all metrics.
C1 [Yuan, Mengqi; Jia, Gengyun; Bao, Bing-Kun] Nanjing Univ Posts & Telecommun, Nanjing 210003, Peoples R China.
C3 Nanjing University of Posts & Telecommunications
RP Bao, BK (corresponding author), Nanjing Univ Posts & Telecommun, Nanjing 210003, Peoples R China.
EM 2020010306@njupt.edu.cn; gengyun.jia@njupt.edu.cn;
   bingkunbao@njupt.edu.cn
OI Bao, Bingkun/0000-0001-5956-831X
FU National Key Research and Development
FX No Statement Available
CR Alexandr N., 2021, 5 COMPUT METHODSSYST, V2, P748, DOI [DOI 10.1007/978-3-030-90321-3_61, 10.1007/978-3-030-90321-3_61]
   Aytar Y, 2016, ADV NEUR IN, V29
   Bin Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P436, DOI 10.1145/2964284.2967258
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen David, 2011, ACL
   Chen SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1523, DOI 10.1109/ICCV48922.2021.00157
   Chen SH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4853, DOI 10.1145/3474085.3479216
   Dang X, 2021, LECT NOTES COMPUT SC, V12895, P665, DOI 10.1007/978-3-030-86383-8_53
   Deng CR, 2021, PROC CVPR IEEE, P234, DOI 10.1109/CVPR46437.2021.00030
   Deng JC, 2022, IEEE T CIRC SYST VID, V32, P880, DOI 10.1109/TCSVT.2021.3063423
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fang ZY, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P840
   Gao LL, 2022, IEEE T IMAGE PROCESS, V31, P202, DOI 10.1109/TIP.2021.3120867
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gui LK, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P956
   Han YH, 2021, FRONT INFORM TECH EL, V22, P625, DOI 10.1631/FITEE.2000722
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Y., 2023, P IEEECVF INT C COMP, P2963
   Hua X, 2022, IEEE T IMAGE PROCESS, V31, P2004, DOI 10.1109/TIP.2022.3148868
   Jeong D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071936
   Ji L, 2022, NEUROCOMPUTING, V488, P88, DOI 10.1016/j.neucom.2022.02.062
   Ji L, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P2004
   Jin W, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2763
   Jingzhou Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10897, DOI 10.1109/CVPR42600.2020.01091
   Kingma D. P., 2014, arXiv
   Li P, 2022, NEUROCOMPUTING, V472, P294, DOI 10.1016/j.neucom.2020.12.137
   Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin K, 2021, AAAI CONF ARTIF INTE, V35, P2047
   Lin Y., 2022, Advances in Neural Information Processing Systems, V35, P10560
   Liu R, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-021-1248-1
   Liu S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1425, DOI 10.1145/3240508.3240667
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Mukherjee S, 2019, INT CONF ACOUST SPEE, P2027, DOI [10.1109/ICASSP.2019.8682158, 10.1109/icassp.2019.8682158]
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Paszke A., 2017, P 31 C NEUR INF PROC, P1, DOI DOI 10.1017/CB09781107707221.009
   Qian TC, 2021, J INTELL FUZZY SYST, V40, P11085, DOI 10.3233/JIFS-202249
   Radford A., 2018, Improving language understanding by generative pre-training, P850
   Seo PH, 2022, PROC CVPR IEEE, P17938, DOI 10.1109/CVPR52688.2022.01743
   Shao HL, 2022, INT C PATT RECOG, P4095, DOI 10.1109/ICPR56361.2022.9956241
   Shao ZW, 2023, PROC CVPR IEEE, P14974, DOI 10.1109/CVPR52729.2023.01438
   Sievers B., 2020, P C LABS EV FOR, V2696
   Tang MK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4858, DOI 10.1145/3474085.3479207
   Vaswani A., 2017, ADV NEURAL INFORM PR, P5598
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang P, 2022, 39 INT C MACHINE LEA
   Wu AM, 2021, IEEE T CIRC SYST VID, V31, P2438, DOI 10.1109/TCSVT.2020.3020877
   Wu B., 2021, P INT JOINT C ART IN, P1157, DOI 10.24963
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yan YC, 2022, IEEE T PATTERN ANAL, V44, P666, DOI 10.1109/TPAMI.2019.2946823
   Yang ZY, 2022, AAAI CONF ARTIF INTE, P3081
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu WJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5213, DOI 10.1145/3474085.3475638
   Zhang CY, 2016, INT C PATT RECOG, P2924, DOI 10.1109/ICPR.2016.7900081
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhao H, 2022, PEERJ COMPUT SCI, V8, DOI 10.7717/peerj-cs.916
   Zheng Y, 2022, IEEE T CIRC SYST VID, V32, P31, DOI 10.1109/TCSVT.2021.3058626
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
NR 61
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5147
EP 5158
DI 10.1109/TMM.2023.3330070
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600037
DA 2024-08-05
ER

PT J
AU Zhou, Q
   Wang, LJ
   Gao, GW
   Kang, B
   Ou, WH
   Lu, HM
AF Zhou, Quan
   Wang, Linjie
   Gao, Guangwei
   Kang, Bin
   Ou, Weihua
   Lu, Huimin
TI Boundary-Guided Lightweight Semantic Segmentation With Multi-Scale
   Semantic Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Semantic segmentation; Electronic countermeasures; Head;
   Computer architecture; Computational modeling; Computational efficiency;
   Lightweight semantic segmentation; boundary detection; feature
   propagation; convolutional neural networks (CNNs)
ID NETWORK
AB Lightweight semantic segmentation plays an essential role in image signal processing that is beneficial to many multimedia applications, such as self-driving, robotic vision, and virtual reality. Due to the powerful capability to encode image details and semantics, many lightweight dual-resolution networks have been proposed in recent years for semantic segmentation. In spite of achieving remarkable progresses, they often ignore semantic context ranged from different scales. Furthermore, most of them always neglect the object boundaries, serving as a significant assistance for lightweight semantic segmentation. To alleviate these problems, this paper develops a Boundary-guide dual-resolution lightweight network with multi-scale Semantic Context, called BSCNet, for semantic segmentation. Specifically, to enhance the capability of feature representation, an Extremely Lightweight Pyramid Pooling Module (ELPPM) is designed to capture multi-scale semantic context at the top of low-resolution branch of BSCNet. In addition, to increase feature similarity of the same object while keeping feature discrimination of different objects, pixel information is propagated throughout the entire object area using a simple Boundary Auxiliary Fusion Module (BAFM), where the predicted object boundaries are served as high-level guidance to refine low-level convolutional features. The comprehensive experimental results have demonstrated that our BSCNet is simple and effective, achieving state-of-the-art trade-off in terms of segmentation accuracy and running efficiency on CityScapes, CamVid, and KITTI datasets.
C1 [Zhou, Quan] Nanjing Univ Posts & Telecommun, Natl Engn Res Ctr Commun & Networking, Nanjing 210003, Peoples R China.
   [Zhou, Quan] Nanjing Univ Sci & Technol, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
   [Zhou, Quan] Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens Inf, Minist Educ, Nanjing 210094, Peoples R China.
   [Wang, Linjie] New H3C Technol Co Ltd, Hangzhou 310052, Peoples R China.
   [Gao, Guangwei] Nanjing Univ Posts & Telecommun, Inst Adv Technol, Nanjing 210003, Peoples R China.
   [Kang, Bin] Nanjing Univ Posts & Telecommun, Dept Internet Things, Nanjing 210003, Peoples R China.
   [Ou, Weihua] Guizhou Normal Univ, Sch Big Data & Comp Sci, Guiyang 550001, Peoples R China.
   [Lu, Huimin] Southeast Univ, Sch Automat, Nanjing 210096, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Science & Technology; Nanjing University of Science & Technology;
   Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications; Guizhou Normal University; Southeast
   University - China
RP Zhou, Q (corresponding author), Nanjing Univ Posts & Telecommun, Natl Engn Res Ctr Commun & Networking, Nanjing 210003, Peoples R China.; Gao, GW (corresponding author), Nanjing Univ Posts & Telecommun, Inst Adv Technol, Nanjing 210003, Peoples R China.
EM quan.zhou@njupt.edu.cn; 1020010524@njupt.edu.cn; csgwgao@njupt.edu.cn;
   kangbin@njupt.edu.cn; ouweihuahust@gmail.com; dr.huimin.lu@ieee.org
OI Bin, Kang/0000-0002-6054-7556; Ou, Weihua/0000-0001-5241-7703; Lu,
   Huimin/0000-0001-9794-3221
FU National Natural Science Foundation of China
FX No Statement Available
CR An SM, 2022, IEEE T INTELL TRANSP, V23, P15256, DOI 10.1109/TITS.2021.3139001
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen W-Y, 2019, ARXIV
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng RX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P304, DOI 10.1145/3394171.3413750
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Fan JQ, 2023, IEEE T INTELL VEHICL, V8, P756, DOI 10.1109/TIV.2022.3176860
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Gao GW, 2022, IEEE T INTELL TRANSP, V23, P25489, DOI 10.1109/TITS.2021.3098355
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger P., Vision meets robotics: TheKITTI dataset
   Han H., 2016, 4 INT C LEARN REPR I, P51
   Hao SJ, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3154443
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Q, 2017, LECT NOTES COMPUT SC, V10111, P197, DOI 10.1007/978-3-319-54181-5_13
   Hyun J, 2022, IEEE T SYST MAN CY-S, V52, P5877, DOI 10.1109/TSMC.2021.3132026
   Lee HJ, 2020, PROC CVPR IEEE, P4816, DOI 10.1109/CVPR42600.2020.00487
   Li G., 2019, PROC BRIT MACH VIS C, P1
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Liu H, 2023, IEEE T MULTIMEDIA, V25, P1390, DOI 10.1109/TMM.2022.3141888
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loshchilov I., 2017, INT C LEARNING REPRE
   Lv QX, 2022, IEEE T INTELL TRANSP, V23, P4432, DOI 10.1109/TITS.2020.3044672
   Ma LF, 2023, IEEE T MULTIMEDIA, V25, P2774, DOI 10.1109/TMM.2022.3151145
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Nirkin Y, 2021, PROC CVPR IEEE, P4060, DOI 10.1109/CVPR46437.2021.00405
   Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Shen W, 2017, IEEE I CONF COMP VIS, P2410, DOI 10.1109/ICCV.2017.262
   Shi M, 2023, IEEE T NEUR NET LEAR, V34, P3205, DOI 10.1109/TNNLS.2022.3176493
   Shuai B, 2018, IEEE T PATTERN ANAL, V40, P1480, DOI 10.1109/TPAMI.2017.2712691
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2019, IEEE IMAGE PROC, P1860, DOI [10.1109/icip.2019.8803154, 10.1109/ICIP.2019.8803154]
   Weng X, 2022, IEEE T INTELL TRANSP, V23, P17224, DOI 10.1109/TITS.2022.3150350
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Wu XP, 2021, IEEE T MULTIMEDIA, V23, P3427, DOI 10.1109/TMM.2020.3025696
   Wu YH, 2023, IEEE T PATTERN ANAL, V45, P12760, DOI 10.1109/TPAMI.2022.3202765
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xu GA, 2023, IEEE T INTELL TRANSP, V24, P15897, DOI 10.1109/TITS.2023.3248089
   Xu JC, 2023, PROC CVPR IEEE, P19529, DOI 10.1109/CVPR52729.2023.01871
   Xuan Wenjie, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P1924, DOI 10.1145/3581783.3612136
   Xuan WJ, 2022, NEURAL NETWORKS, V145, P248, DOI 10.1016/j.neunet.2021.10.022
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yin CX, 2022, IEEE T MULTIMEDIA, V24, P4183, DOI 10.1109/TMM.2021.3114541
   Yin X, 2023, IEEE T MULTIMEDIA, V25, P6146, DOI 10.1109/TMM.2022.3205441
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhang KP, 2024, IEEE T MULTIMEDIA, V26, P737, DOI 10.1109/TMM.2023.3270637
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhen M., 2020, IEEE C COMPUT VIS PA, P13666, DOI DOI 10.48550/ARXIV.2004.07684
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 71
TC 1
Z9 1
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7887
EP 7900
DI 10.1109/TMM.2024.3372835
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000045
DA 2024-08-05
ER

PT J
AU Zhou, ZZ
   Zhu, YY
AF Zhou, Zhuangzhuang
   Zhu, Yingying
TI RaFPN: Relation-Aware Feature Pyramid Network for Dense Image Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Detectors; Task analysis; Transformers; Semantics;
   Object detection; Adaptation models; Dense image prediction;
   relation-aware; feature pyramid network
AB Intuitively, relations among objects assist a model in performing inference under constrained environments. However, the top-down information flow in the Feature pyramid network (FPN) dilutes the relation features contained in the non-adjacent layers. Such a defect reduces the accuracy of detectors, especially for small or obscured objects. To adequately exploit the relations among object instances, we propose the relation-aware feature pyramid network (RaFPN), a simple but effective balanced multi-scale feature module for dense image prediction. RaFPN models the relations among objects by computing the similarity between pixels located on cross-scale features. The result is then delivered to FPN to guide the detector in completing accurate inference. Specifically, we first generate a pair of cross-scale aggregated features based on the channel importance of the output features from FPN. After that, the relation among the cross-scale objects is extracted by a bi-directional interaction mechanism. Finally, relation features are injected directly into each layer of the feature pyramid to avoid dilution. In this way, the relation among instances can adequately guide the detector for dense prediction. Our RaFPN pushes the performance bound of Faster RCNN by 2.0 AP (average precision), outperforming the recent state-of-the-art FPN-based improvements. Notably, for dense prediction tasks such as instance, semantic, and panoptic segmentation, our method brings consistent boosts to them as well.
C1 [Zhou, Zhuangzhuang; Zhu, Yingying] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
C3 Shenzhen University
RP Zhu, YY (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM zhouzhuangzhuang2020@email.szu.edu.cn; zhuyy@szu.edu.cn
RI Zhou, Zhuangzhuang/KFR-9986-2024
FU National Natural Science Foundation of China
FX No Statement Available
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion N., 2020, EUR C COMP VIS, P213
   Chen GY, 2019, IEEE T IMAGE PROCESS, V28, P4192, DOI 10.1109/TIP.2019.2908062
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4939, DOI 10.1145/3474085.3475351
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chu X., 2023, PROC INT C LEAR REP
   Chu XX, 2021, ADV NEUR IN
   Contributors M., 2020, MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Dosovitskiy A, 2021, PROC INT C LEAR REP
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P844, DOI 10.1109/ICCV48922.2021.00090
   Kirillov A., 2020, P IEEE CVF C COMP VI, P9799, DOI DOI 10.1109/CVPR42600.2020.00982
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu ST, 2019, Arxiv, DOI arXiv:1911.09516
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xie J, 2023, IEEE T MULTIMEDIA, V25, P2153, DOI 10.1109/TMM.2022.3143707
   Xinjiang Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13356, DOI 10.1109/CVPR42600.2020.01337
   Xu M, 2018, IEEE T MULTIMEDIA, V20, P1335, DOI 10.1109/TMM.2017.2767784
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhao XC, 2018, IEEE T MULTIMEDIA, V20, P552, DOI 10.1109/TMM.2017.2750415
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
NR 61
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7787
EP 7800
DI 10.1109/TMM.2024.3371787
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000022
DA 2024-08-05
ER

PT J
AU Chen, JY
   Yang, GB
   Wang, SC
   Wang, DW
   Liao, X
AF Chen, Jiyou
   Yang, Gaobo
   Wang, Shengchun
   Wang, Dewang
   Liao, Xin
TI Image Dehazing Assessment: A Real-World Dataset and a Haze Density-Aware
   Criteria
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dehazing assessment metrics; transfer learning; benchmark dataset
ID QUALITY ASSESSMENT
AB Full-reference image dehazing quality assessment (FR-IDQA) evaluates the visual quality of a dehazed image by measuring its differences with a clear reference. The existing FR-IDQA methods are not convincing due to the lack of well-aligned datasets of hazy and clear image pairs and the limited hand-crafted features make it difficult to simulate the complicated perception by the human visual system (HVS). In this work, we build a real-world image dataset, namely RW-Haze, which comprises natural hazy images and their well-aligned clear references. Each clear image is paired with several hazy images with diverse haze levels from slight to heavy. Meanwhile, the existing FR-IDQA works evaluate the dehazed image quality in a global manner, without considering local haze distributions in the original hazy image. Actually, the perceived haze in a natural hazy image is not uniformly distributed, and the haze density varies with scene depth. Based on this priori observation, we design a haze density-aware convolutional neural network (CNN), namely DehIQA, for FR-IDQA. It adopts transfer learning to alleviate the issue of lacking sufficient labeled data. Specifically, we divide image dehazing assessment into two tasks. The source task is to classify unpaired clear and hazy images, which enforces the deep network to learn haze-related features. The target task is image quality assessment, which is achieved by transferring the trained model for the source task to the target task. Considering the fact that the perceived distortion in a dehazed image is also not uniform, we present a haze density-aware mechanism into DehIQA, which assigns different weights for different local regions in a dehazed image in terms of the dark channel of the original hazy image. Extensive experimental results show that DehIQA outperforms the state-of-the-art (SOTA) works on the benchmark dataset and achieves better consistency with human perceptions.
C1 [Chen, Jiyou; Yang, Gaobo; Wang, Dewang; Liao, Xin] Hunan Univ, Sch Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Chen, Jiyou] Hengyang Normal Univ, Sch Comp Sci & Technol, Hengyang 421008, Peoples R China.
   [Wang, Shengchun] Hunan Normal Univ, Coll Informat Sci & Elect Engn, Changsha, Peoples R China.
C3 Hunan University; Hengyang Normal University; Hunan Normal University
RP Yang, GB (corresponding author), Hunan Univ, Sch Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
EM jiyouchen@hnu.edu.cn; yanggaobo@hnu.edu.cn; scwang@hunnu.edu.cn;
   dewang_wang@126.com; xinliao@hnu.edu.cn
OI Yang, Gaobo/0000-0003-2734-659X
FU National Natural Science Foundation of China
FX No Statement Available
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Cao Y, 2022, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR52688.2022.00576
   Chen JY, 2022, IEEE IMAGE PROC, P11, DOI 10.1109/ICIP46576.2022.9897706
   Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ghildyal A, 2022, LECT NOTES COMPUT SC, V13678, P91, DOI 10.1007/978-3-031-19797-0_6
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Guan TX, 2023, IEEE T MULTIMEDIA, V25, P3934, DOI 10.1109/TMM.2022.3168438
   Guo F, 2014, J CENT SOUTH UNIV, V21, P272, DOI 10.1007/s11771-014-1938-z
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iandola S., 2017, arXiv
   Jacobs N., 2007, PROC IEEE C COMPUT V, P1
   Jacobs N, 2007, PROC CVPR IEEE, P2210
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Liao XR, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P970, DOI 10.1145/3503161.3548193
   Lin D, 2017, IEEE T IMAGE PROCESS, V26, P4154, DOI 10.1109/TIP.2017.2695883
   Liu W, 2021, IEEE T IMAGE PROCESS, V30, P176, DOI 10.1109/TIP.2020.3033402
   Lv X, 2023, IEEE T MULTIMEDIA, V25, P9410, DOI 10.1109/TMM.2023.3252267
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang T.-H., 2018, ARXIV
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Shuang, 2022, 2022 7th International Conference on Computer and Communication Systems (ICCCS), P331, DOI 10.1109/ICCCS55155.2022.9846820
   Yang Y, 2022, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR52688.2022.00208
   Yin GH, 2022, AAAI CONF ARTIF INTE, P3134
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
NR 52
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6037
EP 6049
DI 10.1109/TMM.2023.3345141
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100005
DA 2024-08-05
ER

PT J
AU Chen, SR
   Xu, QL
   Ma, Y
   Qiao, Y
   Wang, YL
AF Chen, Siran
   Xu, Qinglin
   Ma, Yue
   Qiao, Yu
   Wang, Yali
TI Attentive Snippet Prompting for Video Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video retrieval; prompting; attention; multi-modal learning
AB The recent advance of video retrieval has been driven by large-scale visual-language pretraining models. In particular, the state-of-the-art approaches are mainly based on temporal extension of the well-known CLIP model. However, they ignore a critical problem in video retrieval, i.e., the text often refers to a small snippet in the corresponding video. Blindly aggregating all the frames inevitably reduces the discriminative capacity of the final video token to match the text token. Hence, these approaches are limited to retrieve complex videos with diversified contents. To tackle this problem, we propose a concise and novel Attentive Snippet Prompting (ASP) framework, which can dynamically exploit the text-relevant video snippet to boost retrieval. Specifically, our ASP consists of two simple but effective modules, i.e., snippet prompting and video aggregating. Given a pair of text and video, snippet prompting can smartly use cross-modal attention to construct a text-driven visual prompt, namely attentive snippet token, which adaptively describes the relevant video snippet of the text query. Alternatively, video aggregating can summarize all the frame tokens as a video token, for providing the global context. With cooperation of attentive snippet token and global video token, our ASP can effectively learn a robust and text-relevant visual representation for video retrieval. Finally, we evaluate our ASP framework on the widely-used benchmarks, where it simply outperforms a number of recent approaches with a large margin.
C1 [Chen, Siran; Xu, Qinglin; Ma, Yue; Qiao, Yu; Wang, Yali] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Chen, Siran; Xu, Qinglin] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Ma, Yue] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Beijing 100190, Peoples R China.
   [Qiao, Yu; Wang, Yali] Shanghai AI Lab, Shanghai 202150, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Tsinghua University; Tsinghua Shenzhen International
   Graduate School; Shanghai Artificial Intelligence Laboratory
RP Wang, YL (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM sr.chen@siat.ac.cn; ql.xu@siat.ac.cn; y-ma21@mails.tsinghua.edu.cn;
   yu.qiao@siat.ac.cn; yl.wang@siat.ac.cn
OI Chen, Siran/0009-0009-6783-3270
FU National Key Ramp;D Program of China
FX No Statement Available
CR Akbari H, 2021, ADV NEUR IN
   Amrani E, 2021, AAAI CONF ARTIF INTE, V35, P6644
   [Anonymous], 2009, PROC 1STWORKSHOPWEB
   Araujo A, 2018, IEEE T CIRC SYST VID, V28, P1406, DOI 10.1109/TCSVT.2017.2667710
   Aytar Y, 2008, PROC CVPR IEEE, P3729
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Bogolin S.-V., 2021, PROC CVPR IEEE, P5194
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chen David, 2011, ACL
   Chen S., 2020, CVPR, P10638
   Cheng Xing, 2021, arXiv
   Croitoru I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11563, DOI 10.1109/ICCV48922.2021.01138
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dong JF, 2016, Arxiv, DOI arXiv:1604.06838
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dzabraev M, 2021, IEEE COMPUT SOC CONF, P3349, DOI 10.1109/CVPRW53098.2021.00374
   Fang H., 2021, arXiv
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao ZJ, 2022, Arxiv, DOI arXiv:2111.05610
   Ging S., 2020, PROCANNU C NEURAL IN, V33, P22605
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Jiang JJ, 2023, IEEE T MULTIMEDIA, V25, P5002, DOI 10.1109/TMM.2022.3185900
   Jin WK, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1114, DOI 10.1145/3404835.3462974
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim JH, 2018, ADV NEUR IN, V31
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lei CY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2567, DOI 10.1145/3474085.3475431
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Li KC, 2023, IEEE T PATTERN ANAL, V45, P12581, DOI 10.1109/TPAMI.2023.3282631
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Liu JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P665, DOI 10.1145/3343031.3350991
   Liu S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11895, DOI 10.1109/ICCV48922.2021.01170
   Liu Y, 2020, Arxiv, DOI arXiv:1907.13487
   Liu YQ, 2022, LECT NOTES COMPUT SC, V13674, P319, DOI 10.1007/978-3-031-19781-9_19
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Luo HS, 2020, Arxiv, DOI arXiv:2002.06353
   Ma Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4132, DOI 10.1145/3503161.3548257
   Miech A, 2020, Arxiv, DOI arXiv:1804.02516
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Portillo-Quintero JA, 2021, LECT NOTES COMPUT SC, V12725, P3, DOI 10.1007/978-3-030-77004-4_1
   Radford A, 2021, PR MACH LEARN RES, V139
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rossetto L, 2019, LECT NOTES COMPUT SC, V11295, P349, DOI 10.1007/978-3-030-05710-7_29
   Sabatier P.A., 1986, J PUBLIC POLICY, V6, P21, DOI [10.1017/S0143814X00003846, DOI 10.1017/S0143814X00003846]
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Souek T., 2020, arXiv
   Sun C, 2019, Arxiv, DOI [arXiv:1906.05743, 10.48550/arXiv.1906.05743]
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751
   Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu R., 2015, Proc. AAAI Conf. Artif. Intell., V29
   Zha Z.-J., 2019, ACM Trans. MultimediaComput., Commun., Appl., V15, P1
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhao S, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P970, DOI 10.1145/3477495.3531950
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu Linchao, 2020, P IEEECVF C COMPUTER, DOI 10.1109/CVPR42600.2020.00877
NR 68
TC 0
Z9 0
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4348
EP 4359
DI 10.1109/TMM.2023.3321503
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100024
DA 2024-08-05
ER

PT J
AU Cui, YW
   Yu, ZT
   Peng, W
   Tian, Q
   Liu, L
AF Cui, Yawen
   Yu, Zitong
   Peng, Wei
   Tian, Qi
   Liu, Li
TI Rethinking Few-Shot Class-Incremental Learning With Open-Set Hypothesis
   in Hyperbolic Geometry
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot learning; class-incremental learning; hyperbolic deep neural
   network; open-set recognition; knowledge distillation
AB By training first with a large base dataset, Few-Shot Class-Incremental Learning (FSCIL) aims at continually learning a sequence of few-shot learning tasks with novel classes. There are mainly two challenges in FSCIL: the overfitting issue of novel classes with limited labeled samples and the catastrophic forgetting of previously seen classes. The current protocol of FSCIL is built by mimicking the general class-incremental learning setting by building a unified framework, while the existing frameworks for FSCIL on this protocol always bias to the classes in the base dataset because the dominant performance of the deep model is decided by the size of the training dataset. Moreover, it is difficult to handle the stability-plasticity constraint in a unified FSCIL framework. To solve these issues, we rethink the configuration of FSCIL with the open-set hypothesis by reserving the possibility in the first session for incoming categories. To find a better decision boundary of close space and open space, Hyperbolic Reciprocal Point Learning module (Hyper-RPL) is built on Reciprocal Point Learning with hyperbolic neural networks. Besides, when learning novel categories from limited labeled data, we incorporate a hyperbolic metric learning (Hyper-Metric) module into the distillation-based framework to alleviate the overfitting issue and better handle the trade-off issue between the preservation of old knowledge and the acquisition of new knowledge. Finally, the comprehensive assessments of the proposed configuration and modules on three benchmark datasets are executed to validate the effectiveness, and state-of-the-art results are achieved.
C1 [Cui, Yawen] Univ Oulu, CMVS, Oulu 90570, Finland.
   [Yu, Zitong] Great Bay Univ, Dongguan 523000, Guangdong, Peoples R China.
   [Peng, Wei] Stanford Univ, CNSlab, Stanford, CA 94305 USA.
   [Tian, Qi] Xidian Univ, Xian 710071, Peoples R China.
   [Liu, Li] Natl Univ Def Technol NUDT, Coll Elect Sci, Changsha 410073, Peoples R China.
   [Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal CMVS, Oulu 90570, Finland.
C3 University of Oulu; Stanford University; Xidian University; National
   University of Defense Technology - China; University of Oulu
RP Liu, L (corresponding author), Natl Univ Def Technol NUDT, Coll Elect Sci, Changsha 410073, Peoples R China.
EM yawen.cui@oulu.fi; zitong.yu@ieee.org; wepeng@stanford.edu;
   wywqtian@gmail.com; dreamliu2010@gmail.com
OI Liu, li/0000-0002-2011-2873; Yu, Zitong/0000-0003-0422-6616; Peng,
   Wei/0000-0002-2892-5764
FU National Key Research and Development Program of China
FX No Statement Available
NR 0
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5897
EP 5910
DI 10.1109/TMM.2023.3340550
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Du, KL
   Lyu, F
   Li, LY
   Hu, FY
   Feng, W
   Xu, FL
   Xi, XF
   Cheng, HJ
AF Du, Kaile
   Lyu, Fan
   Li, Linyan
   Hu, Fuyuan
   Feng, Wei
   Xu, Fenglei
   Xi, Xuefeng
   Cheng, Hanjing
TI Multi-Label Continual Learning Using Augmented Graph Convolutional
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Correlation; Training; Image recognition; Convolutional
   neural networks; Dogs; Recurrent neural networks; Continual learning;
   multi-label recognition; partial label encoder; augmented correlation
   matrix
ID CLASSIFICATION
AB Multi-Label Continual Learning (MLCL) is a framework designed for class-incremental multi-label image recognition. However, MLCL faces two critical challenges: the construction of label relationships on past-missing and future-missing partial labels of training data, and the problem of catastrophic forgetting, which leads to poor generalization. To address these challenges, this study proposes an enhanced version of the Augmented Graph Convolutional Network (AGCN++), capable of constructing cross-task label relationships and mitigating catastrophic forgetting. First, an Augmented Correlation Matrix (ACM) is constructed across all observed classes, incorporating intra-task relationships derived from hard label statistics. Additionally, inter-task relationships are established by leveraging both hard and soft labels obtained from the data, as well as a constructed expert network. Next, a novel partial label encoder (PLE) is introduced for MLCL, enabling the extraction of dynamic class representations for each partial label image as graph nodes. This PLE also facilitates the generation of soft labels, which contribute to the creation of a more persuasive ACM and effectively mitigate forgetting. Lastly, a relationship-preserving constrainter is proposed to address the issue of forgetting label dependencies across old tasks. In the AGCN++, the label relationships topology can be augmented automatically, thereby generating efficient class representations. The effectiveness of the proposed method is evaluated using two multi-label image benchmarks. The experimental results demonstrate that the proposed approach is highly effective in the context of MLCL image recognition. It can establish compelling correlations across tasks, even in scenarios where the old task labels are missing.
C1 [Du, Kaile; Hu, Fuyuan; Xu, Fenglei; Xi, Xuefeng; Cheng, Hanjing] Suzhou Univ Sci & Technol, Suzhou 215009, Peoples R China.
   [Du, Kaile; Hu, Fuyuan; Xu, Fenglei; Xi, Xuefeng; Cheng, Hanjing] Southeast Univ, Coll Automat, Nanjing 210000, Peoples R China.
   [Lyu, Fan; Li, Linyan; Feng, Wei] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Cheng, Hanjing] Suzhou Inst Trade & Commerce, Suzhou, Peoples R China.
C3 Suzhou University of Science & Technology; Southeast University - China;
   Tianjin University
RP Hu, FY (corresponding author), Suzhou Univ Sci & Technol, Suzhou 215009, Peoples R China.
EM kailedu@post.usts.edu.cn; fanlyu@tju.edu.cn; lilinyan@szjm.edu.cn;
   fuyuanhu@mail.usts.edu.cn; wfeng@tju.edu.cn; xufl@mail.usts.edu.cn;
   xfxi@mail.usts.edu.cn; chj@mail.usts.eud.cn
OI Hu, Fuyuan/0000-0002-6818-2221
FU Natural Science Foundation of China
FX No Statement Available
CR Aljundi R, 2019, ADV NEUR IN, V32
   Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753
   Bang J, 2021, PROC CVPR IEEE, P8214, DOI 10.1109/CVPR46437.2021.00812
   Chaudhry A., 2019, ICLR
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen ZM, 2023, IEEE T PATTERN ANAL, V45, P6969, DOI 10.1109/TPAMI.2021.3063496
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chen Zhiyuan, 2018, Lifelong machine learning, V12, P1, DOI DOI 10.2200/S00737ED1V01Y201610AIM033
   Chua TS, 2009, P ACM INT C IM VID R, P1
   De Lange M, 2022, IEEE T PATTERN ANAL, V44, P3366, DOI 10.1109/TPAMI.2021.3057446
   De Lange M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8230, DOI 10.1109/ICCV48922.2021.00814
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Douillard A, 2022, PROC CVPR IEEE, P9275, DOI 10.1109/CVPR52688.2022.00907
   Du K., 2022, P IEEE INT C MULT EX, P01
   Gong XW, 2022, IEEE T MULTIMEDIA, V24, P1055, DOI 10.1109/TMM.2021.3109438
   Gong XW, 2022, IEEE T NEUR NET LEAR, V33, P6775, DOI 10.1109/TNNLS.2021.3083397
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kim Chris Dongjoo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P411, DOI 10.1007/978-3-030-58601-0_25
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Li HY, 2021, IEEE T NEUR NET LEAR, V32, P3206, DOI 10.1109/TNNLS.2020.3010581
   Li YC, 2017, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2017.199
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lyu F, 2021, AAAI CONF ARTIF INTE, V35, P8819
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Mai Z, 2021, IEEE COMPUT SOC CONF, P3584, DOI 10.1109/CVPRW53098.2021.00398
   Mallya A, 2018, LECT NOTES COMPUT SC, V11208, P72, DOI 10.1007/978-3-030-01225-0_5
   Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810
   Nguyen G, 2020, Arxiv, DOI arXiv:1909.08745
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Rolnick D, 2019, 33 C NEURAL INFORM P, V32
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Serrà J, 2018, PR MACH LEARN RES, V80
   Shin H, 2017, ADV NEUR IN, V30
   Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368
   Silver DL, 2020, IEEE COMPUT SOC CONF, P1035, DOI 10.1109/CVPRW50498.2020.00136
   Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404
   Sun LJ, 2022, IEEE T MULTIMEDIA, V24, P581, DOI 10.1109/TMM.2021.3055959
   Sun Q., 2022, ADV NEURAL INFORM PR
   Thuseethan S, 2022, IEEE T MULTIMEDIA, V24, P4367, DOI 10.1109/TMM.2021.3116434
   van deVen G. M., 2018, P NEURIPS CONT LEARN, P1
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang Y., 2022, NeurIPS, V35, P5682
   Wang ZF, 2022, LECT NOTES COMPUT SC, V13686, P631, DOI 10.1007/978-3-031-19809-0_36
   Yang GL, 2023, IEEE T MULTIMEDIA, V25, P3841, DOI 10.1109/TMM.2022.3167555
   Yang H, 2016, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2016.37
   Ye F, 2022, IEEE T PATTERN ANAL, V44, P6280, DOI 10.1109/TPAMI.2021.3092677
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou DW, 2023, IEEE T PATTERN ANAL, V45, P12816, DOI 10.1109/TPAMI.2022.3200865
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
NR 54
TC 0
Z9 0
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2978
EP 2992
DI 10.1109/TMM.2023.3305871
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700003
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Han, G
   Lin, M
   Li, ZY
   Zhao, HT
   Kwong, S
AF Han, Guang
   Lin, Min
   Li, Ziyang
   Zhao, Haitao
   Kwong, Sam
TI Text-to-Image Person Re-Identification Based on Multimodal Graph
   Convolutional Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; person re-identification; person search;
   image-text retrieval; graph convolutional network
AB Text-to-image person re-identification (ReID) is a common subproblem in the field of person re-identification and image-text retrieval. Recent approaches generally follow the structure of a dual-stream network, extracting image and text features. There is no deep interaction between images and text in this approach, making it difficult for the network to learn a highly semantic feature representation. In addition, for both image data and text data, the feature extraction process is modeled in a regular way, such as using Transformer to extract sequence embeddings. However, this type of modeling disregards the inherent relationships among multimodal input embeddings. A more flexible approach to mining multimodal data, which uniformly treats the data as graphs, is proposed. In this way, the extraction and interaction of multimodal information are accomplished by means of messages passing between graph nodes. First, a unified multimodal feature extraction and fusion network is proposed based on the graph convolutional network, which enables the progression of multimodal information from 'local' to 'global'. Second, an asymmetric multilevel alignment module, which focuses on more accurate 'local' information from a 'global' perspective, is proposed to progressively divide the multimodal information at each level. Last, a cross-modal representation matching strategy based on similarity distribution and mutual information is proposed to achieve cross-modal alignment. The proposed algorithm in this paper is simple and efficient, and the testing results on three public datasets (CUHK-PEDES, ICFG-PEDES and RSTPReID) show that it can achieve SOTA-level performance.
C1 [Han, Guang; Lin, Min; Li, Ziyang; Zhao, Haitao] Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210003, Peoples R China.
   [Kwong, Sam] Lingnan Univ, Dept Comp & Decis Sci, Hong Kong, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Lingnan University
RP Han, G (corresponding author), Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210003, Peoples R China.
EM hanguang8848@njupt.edu.cn; 1221014328@njupt.edu.cn;
   1222014523@njupt.edu.cn; zhaoht@njupt.edu.cn; samkwong@ln.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261; Li, Ziyang/0009-0001-3084-1308
FU Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805
   Chen YC, 2021, IEEE T IMAGE PROCESS, V30, P4057, DOI 10.1109/TIP.2021.3068825
   Chen YH, 2022, NEUROCOMPUTING, V494, P171, DOI 10.1016/j.neucom.2022.04.081
   Devlin J, 2018, ARXIV
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gao CY, 2021, Arxiv, DOI arXiv:2101.03036
   Gao HY, 2019, PR MACH LEARN RES, V97
   Han Kai, 2022, ADV NEURAL INF PROCE, V35, P8291, DOI DOI 10.48550/ARXIV.2206.00272
   Han X., 2021, P BRIT MACH VIS C
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y, 2019, IEEE I CONF COMP VIS, P5773, DOI 10.1109/ICCV.2019.00587
   Belghazi MI, 2021, Arxiv, DOI arXiv:1801.04062
   Ji Z, 2023, IEEE T MULTIMEDIA, V25, P7699, DOI 10.1109/TMM.2022.3225754
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee J, 2019, PR MACH LEARN RES, V97
   Li F, 2022, Arxiv, DOI arXiv:2211.08657
   Li SP, 2022, INT CONF ACOUST SPEE, P2724, DOI 10.1109/ICASSP43922.2022.9746846
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Ma Y, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P723, DOI 10.1145/3292500.3330982
   Marneffe De, 2008, P COL P WORKSH CROSS, P1
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Shao ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5566, DOI 10.1145/3503161.3548028
   Shu X., 2022, P ECCV, P624
   Suo W, 2022, LECT NOTES COMPUT SC, V13695, P726, DOI 10.1007/978-3-031-19833-5_42
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Tian XD, 2021, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR46437.2021.00157
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu ZQ, 2023, IEEE T MULTIMEDIA, V25, P9315, DOI 10.1109/TMM.2023.3251104
   Yan SY, 2021, PROC CVPR IEEE, P8092, DOI 10.1109/CVPR46437.2021.00800
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Ying R, 2018, ADV NEUR IN, V31
   Zeng C, 2022, INFORM SCIENCES, V609, P913, DOI 10.1016/j.ins.2022.07.142
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang S., 2015, P 29 PAC AS C LANG I, P73
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
NR 48
TC 0
Z9 0
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6025
EP 6036
DI 10.1109/TMM.2023.3344354
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100023
DA 2024-08-05
ER

PT J
AU Han, HC
   Zheng, QH
   Luo, MN
   Miao, KY
   Tian, F
   Chen, Y
AF Han, Haochen
   Zheng, Qinghua
   Luo, Minnan
   Miao, Kaiyao
   Tian, Feng
   Chen, Yan
TI Noise-Tolerant Learning for Audio-Visual Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; audio-visual learning; noisy labels; noisy
   correspondence
ID NETWORKS
AB Recently, video recognition is emerging with the help of multi-modal learning, which focuses on integrating distinct modalities to improve the performance or robustness of the model. Although various multi-modal learning methods have been proposed and offer remarkable recognition results, almost all of these methods rely on high-quality manual annotations and assume that modalities among multi-modal data provide semantically relevant information. Unfortunately, the widely used video datasets are usually coarse-annotated or collected from the Internet. Thus, it inevitably contains a portion of noisy labels and noisy correspondence. To address this challenge, we use the audio-visual action recognition task as a proxy and propose a noise-tolerant learning framework to find anti-interference model parameters against both noisy labels and noisy correspondence. Specifically, our method consists of two phases that aim to rectify noise by the inherent correlation between modalities. First, a noise-tolerant contrastive training phase is performed to make the model immune to the possible noisy-labeled data. Despite the benefits brought by contrastive training, it would overfit the noisy correspondence and thus provide false supervision. To alleviate the influence of noisy correspondence, we propose a cross-modal noise estimation component to adjust the consistency between different modalities. As the noisy correspondence existed at the instance level, we further propose a category-level contrastive loss to reduce its interference. Second, in the hybrid-supervised training phase, we calculate the distance metric among features to obtain corrected labels, which are used as complementary supervision to guide the training. Furthermore, due to the lack of suitable datasets, we establish a benchmark of real-world noisy correspondence in audio-visual data by relabeling the Kinetics dataset. Extensive experiments on a wide range of noisy levels demonstrate that our method significantly improves the robustness of the action recognition model and surpasses the baselines by a clear margin.
C1 [Han, Haochen; Zheng, Qinghua; Luo, Minnan; Tian, Feng; Chen, Yan] Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian 710049, Peoples R China.
   [Miao, Kaiyao] Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
   [Han, Haochen; Zheng, Qinghua; Luo, Minnan; Miao, Kaiyao; Tian, Feng; Chen, Yan] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China.
   [Miao, Kaiyao] Xi An Jiao Tong Univ, Sch Cyber Sci & Engn, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University; Xi'an Jiaotong University
RP Zheng, QH (corresponding author), Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian 710049, Peoples R China.
EM hhc2077@outlook.com; qhzheng@xjtu.edu.cn; minnluo@xjtu.edu.cn;
   miaoky814@stu.xjtu.edu.cn; fengtian@mail.xjtu.edu.cn;
   chenyan@mail.xjtu.edu.cn
OI tian, feng/0000-0001-7888-0587
FU National Key Research and Development Program of China
FX No Statement Available
CR Abu-El-Haija S., 2016, arXiv
   Alwassel H., 2020, NEURIPS, V33, P9758
   Arandjelovic R., 2018, P EUROPEAN C COMPUTE, P435
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Arpit D, 2017, PR MACH LEARN RES, V70
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cheng H., 2021, PROC INT C LEARN REP
   Cuturi M., 2013, Ad-vances in Neural Information Processing Systems, V26, P1
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Han B, 2018, ADV NEUR IN, V31
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P9595, DOI 10.1109/TPAMI.2023.3247939
   Hu P, 2021, PROC CVPR IEEE, P5399, DOI 10.1109/CVPR46437.2021.00536
   Huang JC, 2019, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2019.00342
   Huang Z., 2021, Advances in Neural Informa- tion Processing Systems (NeurIPS-21), P29406
   Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572
   Karim N, 2022, PROC CVPR IEEE, P9666, DOI 10.1109/CVPR52688.2022.00945
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kinga D., 2015, PROC INT CONFLEARN R, V5
   Kong QQ, 2020, IEEE-ACM T AUDIO SPE, V28, P2880, DOI 10.1109/TASLP.2020.3030497
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li J., 2019, PROC INT C LEARN REP
   Li JC, 2022, LECT NOTES COMPUT SC, V13684, P128, DOI 10.1007/978-3-031-20053-3_8
   Li L, 2022, PROC CVPR IEEE, P18847, DOI 10.1109/CVPR52688.2022.01830
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li SK, 2022, PROC CVPR IEEE, P316, DOI 10.1109/CVPR52688.2022.00041
   Li SK, 2020, AAAI CONF ARTIF INTE, V34, P4667
   Li YF, 2021, AAAI CONF ARTIF INTE, V35, P8547
   Liu JY, 2019, IEEE T MULTIMEDIA, V21, P887, DOI 10.1109/TMM.2018.2871418
   Ma F, 2022, IEEE T NEUR NET LEAR, V33, P6275, DOI 10.1109/TNNLS.2021.3073248
   Ma X., 2020, INT C MACHINE LEARNI, P6543
   Morgado P, 2021, PROC CVPR IEEE, P12470, DOI 10.1109/CVPR46437.2021.01229
   Panda R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7556, DOI 10.1109/ICCV48922.2021.00748
   Petridis S, 2016, IEEE T AFFECT COMPUT, V7, P45, DOI 10.1109/TAFFC.2015.2446462
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ren MY, 2018, PR MACH LEARN RES, V80
   Rouditchenko A, 2019, INT CONF ACOUST SPEE, P2357, DOI 10.1109/icassp.2019.8682467
   Shu J, 2019, ADV NEUR IN, V32
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun ZR, 2022, PROC CVPR IEEE, P5301, DOI 10.1109/CVPR52688.2022.00524
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian YP, 2021, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR46437.2021.00555
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Wang W., 2020, P IEEE CVF C COMP VI, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie SN, 2018, Arxiv, DOI [arXiv:1712.04851, DOI 10.48550/ARXIV.1712.04851]
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang MX, 2021, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR46437.2021.00119
   Yao JC, 2019, IEEE T IMAGE PROCESS, V28, P1909, DOI 10.1109/TIP.2018.2877939
   Yuan X, 2021, PROC CVPR IEEE, P6991, DOI 10.1109/CVPR46437.2021.00692
   Zhang CY, 2022, IEEE T MULTIMEDIA, V24, P1198, DOI 10.1109/TMM.2021.3134156
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang S., 2021, INT CONFLEARN REPRES
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zheng GQ, 2021, AAAI CONF ARTIF INTE, V35, P11053
NR 63
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7761
EP 7774
DI 10.1109/TMM.2024.3371220
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000021
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Hu, YS
   Luo, C
   Chen, ZZ
AF Hu, Yaosi
   Luo, Chong
   Chen, Zhenzhong
TI A Benchmark for Controllable Text -Image-to-Video Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video generation; text-image-to-video; multimodal-conditioned generation
AB Automatic video generation is a challenging research topic, attracting interests from different perspectives, including Image-to-Video generation (I2V), Video-to-Video generation (V2V), and Text-to-Video generation (T2V). To pursue more controllable and fine-grained video generation, a novel video generation task, named Text-Image-to-Video generation (TI2V), and a corresponding baseline solution, named Motion Anchor-based video Generator (MAGE), were proposed. However, two other factors, namely clean datasets and reliable evaluation metrics, also play important roles in the success of the TI2V task. In this article, we present a complete benchmark for the TI2V task which includes synthetic video-text paired datasets, a baseline method, and two evaluation metrics. More specifically: (1) Two versions of synthetic datasets are built based on CATER containing rich combinations of objects and actions, as well as the resulting changes of brightness and shadow. We also provide both explicit and ambiguous text descriptions to support deterministic and diverse video generation, respectively. (2) A refined version of MAGE, dubbed MAGE+, is proposed with an innovative motion anchor structure to store appearance-motion aligned representation, which can be further injected with explicit condition and implicit randomness to model the uncertainty in data distribution. (3) To evaluate the quality of generated video especially given ambiguous description, we introduce action precision and referring expression precision to assess the quality of motion based on captioning-and-matching method. Experiments conducted on proposed datasets, as well as relevant datasets, verify the effectiveness of our baseline and show appealing potentials of TI2V task.
C1 [Hu, Yaosi; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
   [Luo, Chong] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Wuhan University; Microsoft Research Asia; Microsoft
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.; Luo, C (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM ys_hu@whu.edu.cn; cluo@microsoft.com; zzchen@ieee.org
RI Hu, Yaosi/GRY-6324-2022
OI Hu, Yaosi/0000-0003-2784-6738
FU Microsoft and National Natural Science Foundation of China
FX No Statement Available
CR B. Series, 2012, RECOMMENDATION ITU R, P13
   Babaeizadeh M., 2018, PROC INT C LEARN REP
   Balaji Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1995
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Blattmann A, 2021, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR46437.2021.00513
   Castrejon L, 2019, IEEE I CONF COMP VIS, P7607, DOI 10.1109/ICCV.2019.00770
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Cui RP, 2020, IEEE T MULTIMEDIA, V22, P2551, DOI 10.1109/TMM.2019.2960700
   Deng KL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2216
   Dorkenwald M, 2021, PROC CVPR IEEE, P3741, DOI 10.1109/CVPR46437.2021.00374
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Eskimez SE, 2021, IEEE T MULTIMEDIA, V24, P3480, DOI 10.1109/TMM.2021.3099900
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Gafni O., 2020, PROC INT C LEARN REP
   Girdhar R., 2020, PROC INT C LEARN REP
   Han LG, 2022, PROC CVPR IEEE, P3605, DOI 10.1109/CVPR52688.2022.00360
   Hao ZK, 2018, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2018.00819
   Hensel M, 2017, ADV NEUR IN, V30
   Higgins I., 2017, INT C LEARN REPR
   Ho JAT, 2019, Arxiv, DOI arXiv:1912.12180
   Hu YS, 2022, PROC CVPR IEEE, P18198, DOI 10.1109/CVPR52688.2022.01768
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiangning Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P300, DOI 10.1007/978-3-030-58558-7_18
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Li YJ, 2018, LECT NOTES COMPUT SC, V11213, P609, DOI 10.1007/978-3-030-01240-3_37
   Li YT, 2018, AAAI CONF ARTIF INTE, P7065
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Lin Chin-Yew, 2004, P 42 ANN M ASS COMP, P605, DOI DOI 10.3115/1218955.1219032
   Lin K, 2022, PROC CVPR IEEE, P17928, DOI 10.1109/CVPR52688.2022.01742
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Marwah T, 2017, IEEE I CONF COMP VIS, P1435, DOI 10.1109/ICCV.2017.159
   Menapace W, 2021, PROC CVPR IEEE, P10056, DOI 10.1109/CVPR46437.2021.00993
   Mittal G, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1096, DOI 10.1145/3123266.3123309
   Pan JT, 2019, PROC CVPR IEEE, P3728, DOI 10.1109/CVPR.2019.00385
   Pan YW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1789, DOI 10.1145/3123266.3127905
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shao H., 2020, PMLR, P8655
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Unterthiner T, 2019, Arxiv, DOI [arXiv:1812.01717, 10.48550/arXiv.1812.01717]
   van den Oord A, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Wang T.-C., 2019, P ADV NEUR INF PROC, P5013
   Wang T.-H., 2018, ARXIV
   Wang W, 2020, IEEE T MULTIMEDIA, V22, P2808, DOI 10.1109/TMM.2019.2963621
   Wang YH, 2020, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR42600.2020.00531
   Wichers N, 2018, PR MACH LEARN RES, V80
   Wu CF, 2021, Arxiv, DOI arXiv:2104.14806
   Wu Y, 2020, PROC CVPR IEEE, P5538, DOI 10.1109/CVPR42600.2020.00558
   Xiong W, 2018, PROC CVPR IEEE, P2364, DOI 10.1109/CVPR.2018.00251
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang CH, 2018, PROCEEDINGS OF 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS), P201, DOI 10.1109/CCOMS.2018.8463302
   Ye ZP, 2023, IEEE T MULTIMEDIA, V25, P2033, DOI 10.1109/TMM.2022.3142387
   Zhang HK, 2019, IEEE I CONF COMP VIS, P1725, DOI 10.1109/ICCV.2019.00181
   Zhao L, 2018, LECT NOTES COMPUT SC, V11219, P403, DOI 10.1007/978-3-030-01267-0_24
NR 65
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1706
EP 1719
DI 10.1109/TMM.2023.3284989
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800006
DA 2024-08-05
ER

PT J
AU Jiang, B
   Luo, SX
   Wang, X
   Li, CF
   Tang, J
AF Jiang, Bo
   Luo, Shuxian
   Wang, Xiao
   Li, Chuanfu
   Tang, Jin
TI AMatFormer: Efficient Feature Matching via Anchor Matching Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature matching; anchor selection; self-attention; cross-attention;
   transformer
AB Learning based feature matching methods have been commonly studied in recent years. The core issue for learning feature matching is to how to learn (1) discriminative representations for feature points (or regions) within each intra-image and (2) consensus representations for feature points across inter-images. Recently, self- and cross-attention models have been exploited to address this issue. However, in many scenes, features are coming with large-scale, redundant and outliers contaminated. Previous self-/cross-attention models generally conduct message passing on all primal features which thus lead to redundant learning and high computational cost. To mitigate limitations, inspired by recent seed matching methods, in this article, we propose a novel efficient Anchor Matching Transformer (AMatFormer) for the feature matching problem. AMatFormer has two main aspects: First, it mainly conducts self-/cross-attention on some anchor features and leverages these anchor features as message bottleneck to learn the representations for all primal features. Thus, it can be implemented efficiently and compactly. Second, AMatFormer adopts a shared FFN module to further embed the features of two images into the common domain and thus learn the consensus feature representations for the matching problem. Experiments on several benchmarks demonstrate the effectiveness and efficiency of the proposed AMatFormer matching approach.
C1 [Jiang, Bo] Anhui Univ, Sch Comp Sci & Technol, Informat Mat & Intelligent Sensing Lab Anhui Prov, Hefei 230601, Peoples R China.
   [Luo, Shuxian; Wang, Xiao; Tang, Jin] Anhui Univ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
   [Li, Chuanfu] Anhui Univ Chinese Med, Affiliated Hosp 1, Hefei 230022, Peoples R China.
C3 Anhui University; Anhui University; Anhui University of Chinese Medicine
RP Wang, X (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
EM jiangbo@ahu.edu.cn; 17305693483@163.com; wangxiaocvpr@foxmail.com;
   licf@ahtcm.edu.cn; tangjin@ahu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bian J.-W., 2019, P 30 BRIT MACH VIS C, V2, P25
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Cao MD, 2023, IEEE T CIRC SYST VID, V33, P160, DOI 10.1109/TCSVT.2022.3201045
   Cao S, 2022, IEEE T CIRC SYST VID, V32, P7005, DOI 10.1109/TCSVT.2022.3178844
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Cavalli Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P770, DOI 10.1007/978-3-030-58529-7_45
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen HK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6281, DOI 10.1109/ICCV48922.2021.00624
   Chen J, 2023, IEEE T MULTIMEDIA, V25, P7113, DOI 10.1109/TMM.2022.3217410
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Darmon F, 2020, INT CONF 3D VISION, P1127, DOI 10.1109/3DV50981.2020.00123
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fey M., 2020, PROC INT C LEARN REP
   Fishkind DE, 2019, PATTERN RECOGN, V87, P203, DOI 10.1016/j.patcog.2018.09.014
   Fu YJ, 2023, IEEE T CIRC SYST VID, V33, P1335, DOI 10.1109/TCSVT.2022.3210602
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Hou J, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109035
   Jiang B, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5419, DOI 10.1145/3474085.3475669
   Jiang B, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108167
   Jiang E., 2021, P IEEE CVF INT C COM, P6207
   Jiang Z., 2022, P CVPR, P2343
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Lee J, 2019, PR MACH LEARN RES, V97
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu H, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109059
   Liu L., 2021, INT C MACHINE LEARNI, P6815
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263
   Ma JY, 2022, INT J COMPUT VISION, V130, P2249, DOI 10.1007/s11263-022-01644-2
   Ma JY, 2022, ISPRS J PHOTOGRAMM, V183, P196, DOI 10.1016/j.isprsjprs.2021.11.004
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Pan HH, 2022, IEEE T CIRC SYST VID, V32, P2845, DOI 10.1109/TCSVT.2021.3099846
   Paszke A, 2019, ADV NEUR IN, V32
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ren Q., 2022, PROCIEEECVF C COMPUT, P15263
   Rolinek Michal, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P407, DOI 10.1007/978-3-030-58604-1_25
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sattler T, 2009, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2009.5459459
   Shen TW, 2019, LECT NOTES COMPUT SC, V11361, P415, DOI 10.1007/978-3-030-20887-5_26
   SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Sun K, 2020, IEEE T MULTIMEDIA, V22, P2246, DOI 10.1109/TMM.2019.2957984
   Tang CM, 2023, IEEE T CIRC SYST VID, V33, P5102, DOI 10.1109/TCSVT.2023.3249468
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang RZ, 2023, IEEE T PATTERN ANAL, V45, P6984, DOI 10.1109/TPAMI.2020.3005590
   Wang RZ, 2019, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2019.00315
   Wang X., 2022, ARXIV
   Wang X, 2024, Arxiv, DOI arXiv:2302.10035
   Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zanfir A, 2018, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2018.00284
   Zhai X., 2022, P IEEECVF C COMPUTER, P12104, DOI [10.1109/CVPR52688.2022.01179, DOI 10.1109/CVPR52688.2022.01179]
   Zhang JH, 2019, IEEE I CONF COMP VIS, P5844, DOI 10.1109/ICCV.2019.00594
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561
   Zhao HJ, 2023, PATTERN RECOGN LETT, V168, P10, DOI 10.1016/j.patrec.2023.02.023
   Zhao XM, 2022, IEEE T CIRC SYST VID, V32, P1313, DOI 10.1109/TCSVT.2021.3068761
NR 72
TC 2
Z9 2
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1504
EP 1515
DI 10.1109/TMM.2023.3282546
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700026
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Jiang, NF
   Chen, WL
   Lin, JL
   Zhao, TS
   Lin, CW
AF Jiang, Nanfeng
   Chen, Weiling
   Lin, Jielian
   Zhao, Tiesong
   Lin, Chia-Wen
TI Video Compression Artifacts Removal With Spatial-Temporal
   Attention-Guided Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Bit rate; Image coding; Video coding;
   Computational complexity; Video compression; video compression artifacts
   removal (VCAR); video enhancement; video quality
ID REDUCTION
AB Recently, many compression algorithms are applied to decrease the cost of video storage and transmission. This will introduce undesirable artifacts, which severely degrade visual quality. Therefore, Video Compression Artifacts Removal (VCAR) aims at reconstructing a high-quality video from its corrupted version of compression. Generally, this task is considered as a vision-related instead of media-related problem. In vision-related research, the visual quality has been significantly improved while the computational complexity and bitrate issues are less considered. In this work, we review the performance constraints of video coding and transfer to evaluate the VCAR outputs. Based on the analyses, we propose a Spatial-Temporal Attention-Guided Enhancement Network (STAGE-Net). First, we employ dynamic filter processing, instead of conventional optical flow method, to reduce the computational cost of VCAR. Second, we introduce self-attention mechanism to design Sequential Residual Attention Blocks (SRABs) to improve visual quality of enhanced video frames with bitrate constraints. Both quantitative and qualitative experimental results have demonstrated the superiority of our proposed method, which achieves high visual qualities and low computational costs.
C1 [Jiang, Nanfeng; Chen, Weiling; Lin, Jielian; Zhao, Tiesong] Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transmi, Fuzhou 350108, Peoples R China.
   [Jiang, Nanfeng] Xiamen Univ Technol, Fujian Key Lab Pattern Recognit & Image Understand, Xiamen 361024, Peoples R China.
   [Chen, Weiling; Zhao, Tiesong] Fujian Sci & Technol Innovat Lab Optoelect Informa, Fuzhou 350116, Peoples R China.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
C3 Fuzhou University; Xiamen University of Technology; Fujian Science &
   Technology Innovation Laboratory for Optoelectronic Information of
   China; National Tsing Hua University; National Tsing Hua University
RP Zhao, TS (corresponding author), Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transmi, Fuzhou 350108, Peoples R China.
EM jnfrock@gmail.com; weiling.chen@fzu.edu.cn; n191110005@fzu.edu.cn;
   t.zhao@fzu.edu.cn; cwlin@ee.nthu.edu.tw
RI Lin, Chia-Wen/ABH-6075-2020
OI Lin, JieLian/0000-0002-7957-2858; Jiang, Nanfeng/0000-0003-1810-8311
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2017, Standard 2042-1:2017
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Chen T, 2001, IEEE T CIRC SYST VID, V11, P594, DOI 10.1109/76.920189
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Deng JN, 2020, AAAI CONF ARTIF INTE, V34, P10696
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Fu XY, 2019, IEEE I CONF COMP VIS, P2501, DOI 10.1109/ICCV.2019.00259
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Guo J, 2016, LECT NOTES COMPUT SC, V9905, P628, DOI 10.1007/978-3-319-46448-0_38
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Huang B, 2020, IEEE T CIRC SYST VID, V30, P795, DOI 10.1109/TCSVT.2019.2893396
   Jia X., 2016, P 28 INT C SCI STAT, P12
   Jiang NF, 2022, IEEE T MULTIMEDIA, V24, DOI 10.1109/TMM.2021.3115442
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim S, 2019, IEEE INT CONF COMP V, P3609, DOI 10.1109/ICCVW.2019.00446
   Liew AWC, 2004, IEEE T CIRC SYST VID, V14, P450, DOI 10.1109/TCSVT.2004.825555
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin LQ, 2023, IEEE SIGNAL PROC LET, V30, P693, DOI 10.1109/LSP.2023.3283541
   Lin LQ, 2020, IEEE T CIRC SYST VID, V30, P3898, DOI 10.1109/TCSVT.2020.2980571
   Liu XM, 2015, IEEE IMAGE PROC, P1628, DOI 10.1109/ICIP.2015.7351076
   Liu XM, 2015, PROC CVPR IEEE, P5171, DOI 10.1109/CVPR.2015.7299153
   Lu G, 2018, LECT NOTES COMPUT SC, V11218, P591, DOI 10.1007/978-3-030-01264-9_35
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2011, CONF REC ASILOMAR C, P723, DOI 10.1109/ACSSC.2011.6190099
   Paliwal A, 2021, IEEE INT CONF COMPUT, DOI 10.1109/ICCP51581.2021.9466268
   Qi ZY, 2022, IEEE I C VI COM I PR, DOI 10.1109/VCIP56404.2022.10008797
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Rassool R, 2017, IEEE INT SYM BROADB, P351
   Ren J, 2013, IEEE DATA COMPR CONF, P516, DOI 10.1109/DCC.2013.95
   Rota C, 2023, ARTIF INTELL REV, V56, P5317, DOI 10.1007/s10462-022-10302-5
   Samuelsson J., 2020, SMPTE Mot. Imag. J., V129, P10
   Santamaria M, 2022, IEEE IMAGE PROC, P2251, DOI 10.1109/ICIP46576.2022.9897757
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shen W, 2021, IEEE T IMAGE PROCESS, V30, P277, DOI 10.1109/TIP.2020.3033617
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang C, 2013, SIGNAL PROCESS-IMAGE, V28, P522, DOI 10.1016/j.image.2013.01.006
   Wang J., 2020, COMPUTER VISION ECCV, P405
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu Y, 2021, IEEE COMPUT SOC CONF, P213, DOI 10.1109/CVPRW53098.2021.00030
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1426, DOI 10.1109/TMM.2020.2997126
   Yin CX, 2022, IEEE T MULTIMEDIA, V24, P4183, DOI 10.1109/TMM.2021.3114541
   Zhang J, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954503
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
   Zhang Y., 2019, PROC INT C LEARN REP
   Zhao TS, 2013, CONF REC ASILOMAR C, P1107, DOI 10.1109/ACSSC.2013.6810465
   Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257
NR 61
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5657
EP 5669
DI 10.1109/TMM.2023.3338087
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600022
DA 2024-08-05
ER

PT J
AU Lu, AD
   Zhang, Z
   Huang, Y
   Zhang, YF
   Li, CL
   Tang, J
   Wang, L
AF Lu, Andong
   Zhang, Zhang
   Huang, Yan
   Zhang, Yifan
   Li, Chenglong
   Tang, Jin
   Wang, Liang
TI Illumination Distillation Framework for Nighttime Person
   Re-Identification and a New Benchmark
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bottleneck fusion; computer vision; illumination distillation; image
   enhancement; person re-identification
ID NETWORK
AB Nighttime person Re-ID (person re-identification in the nighttime) is a very important and challenging task for visual surveillance but it has not been thoroughly investigated. Under the low illumination condition, the performance of person Re-ID methods usually sharply deteriorates. To address the low illumination challenge in nighttime person Re-ID, this article proposes an Illumination Distillation Framework (IDF), which utilizes illumination enhancement and illumination distillation schemes to promote the learning of Re-ID models. Specifically, IDF consists of a master branch, an illumination enhancement branch, and an illumination distillation module. The master branch is used to extract the features from a nighttime image. The illumination enhancement branch first estimates an enhanced image from the nighttime image using a nonlinear curve mapping method and then extracts the enhanced features. However, nighttime and enhanced features usually contain data noise due to unstable lighting conditions and enhancement failures. To fully exploit the complementary benefits of nighttime and enhanced features while suppressing data noise, we propose an illumination distillation module. In particular, the illumination distillation module fuses the features from two branches through a bottleneck fusion model and then uses the fused features to guide the learning of both branches in a distillation manner. In addition, we build a real-world nighttime person Re-ID dataset, named Night600, which contains 600 identities captured from different viewpoints and nighttime illumination conditions under complex outdoor environments. Experimental results demonstrate that our IDF can achieve state-of-the-art performance on two nighttime person Re-ID datasets (i.e., Night600 and Knight).
C1 [Lu, Andong] Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
   [Lu, Andong; Zhang, Zhang; Huang, Yan; Zhang, Yifan; Tang, Jin; Wang, Liang] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Li, Chenglong] Anhui Univ, Sch Artificial Intelligence, Informat Mat & Intelligent Sensing Lab Anhui Prov, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
C3 Anhui University; Chinese Academy of Sciences; Institute of Automation,
   CAS; Anhui University
RP Li, CL (corresponding author), Anhui Univ, Sch Artificial Intelligence, Informat Mat & Intelligent Sensing Lab Anhui Prov, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
EM adlu_ah@foxmail.com; zzhang@nlpr.ia.ac.cn; huangyan.750@outlook.com;
   yifanzhang.cs@gmail.com; lcl1314@foxmail.com; tangjin@ahu.edu.cn;
   wangliang@nlpr.ia.ac.cn
RI Li, Chenglong/AAH-4234-2019; Yang, han/KFS-2671-2024; zhang,
   zheng/HCH-9684-2022; Lu, Andong/AFY-8491-2022; Huang, Yan/N-3447-2018
OI zhang, zhang/0000-0001-9425-3065; Huang, Yan/0000-0002-1363-5318
FU Natural Science Foundation of Anhui Province
FX No Statement Available
CR An SM, 2022, IEEE T INTELL TRANSP, V23, P15256, DOI 10.1109/TITS.2021.3139001
   Bai JW, 2022, LECT NOTES COMPUT SC, V13684, P1, DOI 10.1007/978-3-031-20053-3_1
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chung I., 2020, International Conference on Machine Learning, P2006
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao Z, 2021, IEEE T MULTIMEDIA, V23, P3332, DOI 10.1109/TMM.2020.3023784
   Gray D., 2007, PROC IEEE INT WORKS, P1
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   Hassaballah M, 2021, IEEE T INTELL TRANSP, V22, P4230, DOI 10.1109/TITS.2020.3014013
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Heo B, 2019, AAAI CONF ARTIF INTE, P3771, DOI 10.1609/aaai.v33i01.33013771
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang Y, 2019, INT JOINT C NEURAL N
   Huang YK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P365, DOI 10.1145/3343031.3350994
   Huang Z., 2017, P INT C LEARN REPR, P1
   Hwang S, 2022, IEEE T INTELL TRANSP, V23, P14482, DOI 10.1109/TITS.2021.3129901
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jiang QP, 2022, IEEE T INTELL TRANSP, V23, P19440, DOI 10.1109/TITS.2022.3165176
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Karanam S, 2019, IEEE T PATTERN ANAL, V41, P523, DOI 10.1109/TPAMI.2018.2807450
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11720, DOI 10.1109/ICCV48922.2021.01153
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Qiushan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11017, DOI 10.1109/CVPR42600.2020.01103
   Rahman S, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0138-1
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   T. E. Team, 2021, Crimes that happen while you sleep
   Tianhong Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14627, DOI 10.1109/CVPR42600.2020.01465
   Triantafyllidou Danai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P103, DOI 10.1007/978-3-030-58601-0_7
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H, 2022, IEEE T INTELL TRANSP, V23, P21405, DOI 10.1109/TITS.2022.3177615
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu DM, 2022, IEEE T INF FOREN SEC, V17, P115, DOI 10.1109/TIFS.2021.3075894
   Wu Y., 2019, Detectron2
   Xu BQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P673, DOI 10.1145/3394171.3414056
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P924, DOI 10.1109/TPAMI.2020.3013379
   Ye M, 2022, IEEE T INF FOREN SEC, V17, P386, DOI 10.1109/TIFS.2021.3139224
   Ye M, 2022, IEEE T IMAGE PROCESS, V31, P379, DOI 10.1109/TIP.2021.3131937
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Yukun Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14072, DOI 10.1109/CVPR42600.2020.01409
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang GW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P516, DOI 10.1145/3474085.3475202
   Zhang J, 2019, IEEE ACCESS, V7, P95496, DOI 10.1109/ACCESS.2019.2929854
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZY, 2020, IEEE IMAGE PROC, P2321, DOI 10.1109/ICIP40778.2020.9190796
   Zhao ZJ, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108120
   Zheng AH, 2021, AAAI CONF ARTIF INTE, V35, P3529
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2018, IEEE COMPUT SOC CONF, P1974, DOI 10.1109/CVPRW.2018.00251
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
NR 69
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 406
EP 419
DI 10.1109/TMM.2023.3266066
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000026
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Lv, Y
   Liu, Z
   Li, GY
AF Lv, Ying
   Liu, Zhi
   Li, Gongyang
TI Context-Aware Interaction Network for RGB-T Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context-aware complementation; detail aggregation; global context; RGB-T
   semantic segmentation
AB RGB-T semantic segmentation is a key technique for autonomous driving scenes understanding. For the existing RGB-T semantic segmentation methods, however, the effective exploration of the complementary relationship between different modalities is not implemented in the information interaction between multiple levels. To address such an issue, the Context-Aware Interaction Network (CAINet) is proposed for RGB-T semantic segmentation, which constructs interaction space to exploit auxiliary tasks and global context for explicitly guided learning. Specifically, we propose a Context-Aware Complementary Reasoning (CACR) module aimed at establishing the complementary relationship between multimodal features with the long-term context in both spatial and channel dimensions. Further, considering the importance of global contextual and detailed information, we propose the Global Context Modeling (GCM) module and Detail Aggregation (DA) module, and we introduce specific auxiliary supervision to explicitly guide the context interaction and refine the segmentation map. Extensive experiments on two benchmark datasets of MFNet and PST900 demonstrate that the proposed CAINet achieves state-of-the-art performance.
C1 [Lv, Ying; Liu, Zhi; Li, Gongyang] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Sch Commun & Informat Engn,Key Lab Specialty Fiber, Shanghai 200444, Peoples R China.
   [Liu, Zhi; Li, Gongyang] Shanghai Univ, Wenzhou Inst, Wenzhou 325000, Peoples R China.
C3 Shanghai University; Shanghai University
RP Liu, Z (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Sch Commun & Informat Engn,Key Lab Specialty Fiber, Shanghai 200444, Peoples R China.
EM yinglv@shu.edu.cn; liuzhisjtu@163.com; ligongyang@shu.edu.cn
RI Li, Gongyang/IXD-9078-2023; LIU, Zhi/D-4518-2012
OI LIU, Zhi/0000-0002-8428-1131; , Gongyang Li/0000-0001-7324-1196; lv,
   ying/0000-0003-0227-9909
FU National Natural Science Foundation of China
FX No Statement Available
CR Ainetter S, 2021, IEEE INT CONF ROBOT, P13452, DOI 10.1109/ICRA48506.2021.9561398
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464
   Cai YQ, 2024, VISUAL COMPUT, V40, P169, DOI 10.1007/s00371-023-02773-6
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen L.C., 2014, ARXIV PREPRINT ARXIV, V6, P357, DOI DOI 10.48550/ARXIV.1412.7062
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P968, DOI 10.1109/TMM.2021.3061816
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Cheng B, 2021, ADV NEUR IN, V34
   Deng FQ, 2021, IEEE INT C INT ROBOT, P4467, DOI 10.1109/IROS51168.2021.9636084
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feng Z, 2023, IEEE ROBOT AUTOM LET, V8, P2205, DOI 10.1109/LRA.2023.3247175
   Frigo O, 2022, IEEE COMPUT SOC CONF, P3020, DOI 10.1109/CVPRW56347.2022.00341
   Fu YP, 2022, VISUAL COMPUT, V38, P3243, DOI 10.1007/s00371-022-02559-2
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Gong TT, 2023, ENG APPL ARTIF INTEL, V117, DOI 10.1016/j.engappai.2022.105510
   Guo ZF, 2021, MEASUREMENT, V186, DOI 10.1016/j.measurement.2021.110176
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kim D., 2023, PROC INT CONFLEARN R
   Lan X, 2022, APPL INTELL, V52, P5817, DOI 10.1007/s10489-021-02687-7
   Li GY, 2023, IEEE T IMAGE PROCESS, V32, P5257, DOI 10.1109/TIP.2023.3314285
   Li GY, 2023, IEEE T CIRC SYST VID, V33, P1223, DOI 10.1109/TCSVT.2022.3208833
   Li W., 2022, PROC IEEE INT CONFIN, P1
   Li Z, 2016, LECT NOTES COMPUT SC, V9906, P541, DOI 10.1007/978-3-319-46475-6_34
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu JF, 2022, NEUROCOMPUTING, V506, P60, DOI 10.1016/j.neucom.2022.07.041
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma LF, 2023, IEEE T MULTIMEDIA, V25, P2774, DOI 10.1109/TMM.2022.3151145
   Ma X., 2023, PROC INT C LEARN REP
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Paszke A, 2019, ADV NEUR IN, V32
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Saxe A. M., 2018, PROC INTCONF LEARNRE
   Shivakumar SS, 2020, IEEE INT CONF ROBOT, P9441, DOI [10.1109/icra40945.2020.9196831, 10.1109/ICRA40945.2020.9196831]
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song ZJ, 2022, IEEE T CIRC SYST VID, V32, P4599, DOI 10.1109/TCSVT.2021.3132047
   Sun YX, 2021, IEEE T AUTOM SCI ENG, V18, P1000, DOI 10.1109/TASE.2020.2993143
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Tian YJ, 2022, IEEE T CIRC SYST VID, V32, P3798, DOI 10.1109/TCSVT.2021.3116210
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2023, PROC INT C ADV NEURA
   Wang Y., 2023, P IEEECVF INT C COMP, P22025
   Wang YZ, 2024, IEEE T MULTIMEDIA, V26, P2314, DOI 10.1109/TMM.2023.3294808
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu W, 2022, PATTERN RECOGN, V131, DOI 10.1016/j.patcog.2022.108881
   Xiaokang Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P561, DOI 10.1007/978-3-030-58621-8_33
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xu C, 2023, IEEE T CIRC SYST VID, V33, P1577, DOI 10.1109/TCSVT.2022.3216313
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yi S, 2022, NEUROCOMPUTING, V482, P236, DOI 10.1016/j.neucom.2021.11.056
   Zhang B., 2022, P C NEUR INF PROC SY, V35, P4971
   Zhang JM, 2023, IEEE T INTELL TRANSP, V24, P14679, DOI 10.1109/TITS.2023.3300537
   Zhang M, 2023, IEEE T MULTIMEDIA, V25, P7980, DOI 10.1109/TMM.2022.3232037
   Zhang Q, 2021, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR46437.2021.00266
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao L, 2024, IEEE T MULTIMEDIA, V26, P1158, DOI 10.1109/TMM.2023.3277281
   Zhao SL, 2023, IEEE T CIRC SYST VID, V33, P2892, DOI 10.1109/TCSVT.2022.3229359
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou H, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3179721
   Zhou WJ, 2023, IEEE T INTELL VEHICL, V8, P48, DOI 10.1109/TIV.2022.3164899
   Zhou WJ, 2023, IEEE T INTELL TRANSP, V24, P4794, DOI 10.1109/TITS.2023.3242651
   Zhou WJ, 2022, AAAI CONF ARTIF INTE, P3571
   Zhou WJ, 2021, IEEE T IMAGE PROCESS, V30, P7790, DOI 10.1109/TIP.2021.3109518
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2526, DOI 10.1109/TMM.2021.3086618
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
NR 75
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6348
EP 6360
DI 10.1109/TMM.2023.3349072
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600016
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Palash, M
   Bhargava, B
AF Palash, Mijanur
   Bhargava, Bharat
TI EMERSK -Explainable Multimodal Emotion Recognition With Situational
   Knowledge
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Face recognition; Visualization; Feature
   extraction; Convolutional neural networks; Reliability; Deep learning;
   Emotion Recognition; Deep Learning; Multimodal; Convolutional neural
   network (CNN); LSTM
ID EXPRESSION
AB Automatic emotion recognition has recently gained significant attention due to the growing popularity of deep learning algorithms. One of the primary challenges in emotion recognition is effectively utilizing the various cues (modalities) available in the data. Another challenge is providing a proper explanation of the outcome of the learning. To address these challenges, we present Explainable Multimodal Emotion Recognition with Situational Knowledge (EMERSK), a generalized and modular system for human emotion recognition and explanation using visual information. Our system can handle multiple modalities, including facial expressions, posture, and gait, in a flexible and modular manner. The network consists of different modules that can be added or removed depending on the available data. We utilize a two-stream network architecture with convolutional neural networks (CNNs) and encoder-decoder style attention mechanisms to extract deep features from face images. Similarly, CNNs and recurrent neural networks (RNNs) with Long Short-term Memory (LSTM) are employed to extract features from posture and gait data. We also incorporate deep features from the background as contextual information for the learning process. The deep features from each module are fused using an early fusion network. Furthermore, we leverage situational knowledge derived from the location type and adjective-noun pair (ANP) extracted from the scene, as well as the spatio-temporal average distribution of emotions, to generate explanations. Ablation studies demonstrate that each sub-network can independently perform emotion recognition, and combining them in a multimodal approach significantly improves overall recognition performance. Extensive experiments conducted on various benchmark datasets, including GroupWalk, validate the superior performance of our approach compared to other state-of-the-art methods.
C1 [Palash, Mijanur; Bhargava, Bharat] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.
C3 Purdue University System; Purdue University
RP Palash, M (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.
EM bbshail@purdue.edu
OI Bhargava, Bharat/0000-0003-3803-8672
CR Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052
   [Anonymous], 2013, Fer-2013 learn facial expressions from an image
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Banziger T., 2010, Blueprint for affective computing: A sourcebook, V2010, P271, DOI DOI 10.1037/A0025827
   Bazarevsky V, 2020, Arxiv, DOI [arXiv:2006.10204, DOI 10.48550/ARXIV.2006.10204]
   Bhatia Yajurv, 2021, 2021 IEEE 20th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)., P214, DOI 10.1109/ICCICC53683.2021.9811330
   Bhattacharya U, 2020, AAAI CONF ARTIF INTE, V34, P1342
   Borth D., 2013, P 21 ACM INT C MULT, P223, DOI 10.1145/2502081.2502282
   Castellano G, 2008, MUSIC PERCEPT, V26, P103, DOI 10.1525/MP.2008.26.2.103
   Chen LF, 2024, IEEE T NEUR NET LEAR, V35, P9663, DOI 10.1109/TNNLS.2023.3236320
   Conway M.A., 1987, COGNITION EMOTION, V1, P145, DOI [10.1080/02699938708408044, DOI 10.1080/02699938708408044]
   Crenn A, 2016, INT CONF 3D IMAG
   D'Mello SK, 2010, USER MODEL USER-ADAP, V20, P147, DOI 10.1007/s11257-010-9074-4
   Daoudi M, 2017, LECT NOTES COMPUT SC, V10484, P550, DOI 10.1007/978-3-319-68560-1_49
   Deramgozin M, 2021, IEEE CONF IMAGING SY, DOI 10.1109/IST50367.2021.9651357
   Dhankhar P., 2019, Int. J. Innov. Eng. Technol., V13, P126
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fard AP, 2022, IEEE ACCESS, V10, P26756, DOI 10.1109/ACCESS.2022.3156598
   Gan YL, 2019, PATTERN RECOGN LETT, V125, P105, DOI 10.1016/j.patrec.2019.04.002
   Gedeon T., 2021, PROC 19 ACM INT C MU, P524
   Gunes H, 2007, J NETW COMPUT APPL, V30, P1334, DOI 10.1016/j.jnca.2006.09.007
   Joseph J. L., 2021, PROC IEEE 4 INT C CO, P1
   Kandeel Amany A., 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12666), P699, DOI 10.1007/978-3-030-68780-9_53
   Knobloch-Westerwick S, 2020, COMMUN SPORT, V8, P236, DOI 10.1177/2167479519830359
   Kosti R, 2020, IEEE T PATTERN ANAL, V42, P2755, DOI 10.1109/TPAMI.2019.2916866
   Lee J, 2019, IEEE I CONF COMP VIS, P10142, DOI 10.1109/ICCV.2019.01024
   Li BB, 2018, IEEE T AFFECT COMPUT, V9, P585, DOI 10.1109/TAFFC.2016.2637343
   Li M, 2021, CHIN CONTR CONF, P7410, DOI 10.23919/CCC52363.2021.9549897
   Li WX, 2023, IEEE T AFFECT COMPUT, V14, P650, DOI 10.1109/TAFFC.2021.3064918
   Liu HW, 2022, IEEE T CIRC SYST VID, V32, P6253, DOI 10.1109/TCSVT.2022.3165321
   Lundberg SM, 2017, ADV NEUR IN, V30
   Fernandez PDM, 2019, IEEE COMPUT SOC CONF, P837, DOI 10.1109/CVPRW.2019.00112
   Mittal Trisha, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14222, DOI 10.1109/CVPR42600.2020.01424
   Mittal T, 2021, IEEE MULTIMEDIA, V28, P67, DOI 10.1109/MMUL.2021.3068387
   Mittal T, 2020, AAAI CONF ARTIF INTE, V34, P1359
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.0055, 10.1109/ICDM.2016.178]
   Randhavane T, 2022, 15TH ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION AND GAMES, MIG 2022, DOI 10.1145/3561975.3562957
   Renda A, 2019, EXPERT SYST APPL, V136, P1, DOI 10.1016/j.eswa.2019.06.025
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Santhoshkumar R., 2020, Proceedings of First International Conference on Computing, Communications, and Cyber-Security (IC4S 2019). Lecture Notes in Networks and Systems (LNNS 121), P261, DOI 10.1007/978-981-15-3369-3_20
   Scherer KR, 2007, EMOTION, V7, P158, DOI 10.1037/1528-3542.7.1.158
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sikka K, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P517, DOI 10.1145/2522848.2531741
   Simonyan K, 2014, Arxiv, DOI [arXiv:1312.6034, DOI 10.48550/ARXIV.1312.6034]
   Sun B, 2018, NEURAL NETWORKS, V105, P36, DOI 10.1016/j.neunet.2017.11.021
   Tahghighi Peyman, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12663), P741, DOI 10.1007/978-3-030-68796-0_54
   Veltmeijer EA, 2023, IEEE T AFFECT COMPUT, V14, P89, DOI 10.1109/TAFFC.2021.3065726
   Wang K, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P640, DOI 10.1145/3242969.3264991
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 53
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2785
EP 2794
DI 10.1109/TMM.2023.3304015
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400026
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Peng, F
   Yang, XS
   Xiao, LH
   Wang, YW
   Xu, CS
AF Peng, Fang
   Yang, Xiaoshan
   Xiao, Linhui
   Wang, Yaowei
   Xu, Changsheng
TI SgVA-CLIP: Semantic-Guided Visual Adapting of Vision-Language Models for
   Few-Shot Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot; image classification; vision-language models
AB Although significant progress has been made in few-shot learning, most of existing few-shot image classification methods require supervised pre-training on a large amount of samples of base classes, which limits their generalization ability in real world application. Recently, large-scale Vision-Language Pre-trained models (VLPs) have been gaining increasing attention in few-shot learning because they can provide a new paradigm for transferable visual representation learning with easily available text on the Web. However, the VLPs may neglect detailed visual information that is difficult to describe by language sentences, but important for learning an effective classifier to distinguish different images. To address the above problem, we propose a new framework, named Semantic-guided Visual Adapting (SgVA), which can effectively extend vision-language pre-trained models to produce discriminative adapted visual features by comprehensively using an implicit knowledge distillation, a vision-specific contrastive loss, and a cross-modal contrastive loss. The implicit knowledge distillation is designed to transfer the fine-grained cross-modal knowledge to guide the updating of the vision adapter. State-of-the-art results on 13 datasets demonstrate that the adapted visual features can well complement the cross-modal features to improve few-shot image classification.
C1 [Peng, Fang; Xiao, Linhui] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Peng, Fang; Yang, Xiaoshan; Xiao, Linhui; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Yang, Xiaoshan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Peng, Fang; Yang, Xiaoshan; Xiao, Linhui; Wang, Yaowei; Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; Institute of Automation, CAS; Peng Cheng
   Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
EM pengfang21@mails.ucas.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn;
   xiaolinhui16@mails.ucas.ac.cn; wangyw@pcl.ac.cn; csxu@nlpr.ia.ac.cn
RI Xiao, Linhui/HJO-8602-2023; xu, cj/HJZ-3488-2023; yang,
   xiaoshan/HSE-6093-2023
OI Xiao, Linhui/0000-0003-2592-5264; xu, chang sheng/0000-0001-8343-9665;
   Peng, Fang/0000-0002-3948-7413; Yang, Xiaoshan/0000-0001-5453-9755
FU National Natural Science Foundation of China
FX No Statement Available
CR Bendou Y, 2022, J IMAGING, V8, DOI 10.3390/jimaging8070179
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen YB, 2021, Arxiv, DOI [arXiv:2003.04390, 10.48550/arXiv.2003.04390]
   Cho WI, 2020, INTERSPEECH, P896, DOI 10.21437/Interspeech.2020-1246
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan Q, 2022, LECT NOTES COMPUT SC, V13679, P701, DOI 10.1007/978-3-031-19800-7_41
   Fang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1408, DOI 10.1109/ICCV48922.2021.00146
   Gao P., 2021, arXiv
   Hafner Frank M., 2022, Computer Vision and Image Understanding, V216, DOI 10.1016/j.cviu.2021.103352
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He YJ, 2022, PROC CVPR IEEE, P9109, DOI 10.1109/CVPR52688.2022.00891
   Helber P, 2019, IEEE J-STARS, V12, P2217, DOI 10.1109/JSTARS.2019.2918242
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hospedales T, 2022, IEEE T PATTERN ANAL, V44, P5149, DOI 10.1109/TPAMI.2021.3079209
   Hu SX, 2022, PROC CVPR IEEE, P9058, DOI 10.1109/CVPR52688.2022.00886
   Hu YQ, 2022, ALGORITHMS, V15, DOI 10.3390/a15050147
   Hu YQ, 2021, LECT NOTES COMPUT SC, V12892, P487, DOI 10.1007/978-3-030-86340-1_39
   Huang T., 2022, arXiv
   Jia C, 2021, PR MACH LEARN RES, V139
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiao XQ, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P4163
   Kim Konwoo, 2022, How to adapt your large-scale visionand-language model
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kumar V, 2019, PROCEEDINGS OF THE TENTH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES AND DEVELOPMENT (ICTD), DOI 10.1145/3287098.3287099
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li J., 2023, P 11 INT C LEARN REP
   Li XF, 2023, IEEE T MULTIMEDIA, V25, P8358, DOI 10.1109/TMM.2023.3236212
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P1600, DOI 10.1109/TMM.2021.3139211
   Ling J, 2022, PROC CVPR IEEE, P14544, DOI 10.1109/CVPR52688.2022.01416
   Liu YX, 2023, IEEE T MULTIMEDIA, V25, P2851, DOI 10.1109/TMM.2022.3152086
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Pahde F, 2021, IEEE WINT CONF APPL, P2643, DOI 10.1109/WACV48630.2021.00269
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Qiu LT, 2022, Arxiv, DOI arXiv:2112.02399
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren M., 2018, 6 INT C LEARN REPR
   Rodriguez Pau, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P121, DOI 10.1007/978-3-030-58574-7_8
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sanh V, 2020, Arxiv, DOI [arXiv:1910.01108, 10.48550/arXiv.1910.01108]
   Shen ZQ, 2021, AAAI CONF ARTIF INTE, V35, P9594
   Snell J, 2017, ADV NEUR IN, V30
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Tao ZL, 2023, IEEE T MULTIMEDIA, V25, P5107, DOI 10.1109/TMM.2022.3187556
   Vinyals O., 2016, P ADV NEUR INF PROC, P3637
   Wortsman M, 2022, PROC CVPR IEEE, P7949, DOI 10.1109/CVPR52688.2022.00780
   Xia W, 2023, IEEE T MULTIMEDIA, V25, P6665, DOI 10.1109/TMM.2022.3213208
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xing C., 2019, ADV NEURAL INFORM PR, P4847
   Yoon SW, 2019, PR MACH LEARN RES, V97
   Yuan L., 2021, Florence: A new foundation model for computer vision
   Zhang RR, 2022, LECT NOTES COMPUT SC, V13695, P493, DOI 10.1007/978-3-031-19833-5_29
   Zhang Y, 2022, Arxiv, DOI [arXiv:2210.10841, DOI 10.48550/ARXIV.2210.10841, 10.48550/arXiv.2210.10841]
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhou KY, 2022, PROC CVPR IEEE, P16795, DOI 10.1109/CVPR52688.2022.01631
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
   Zhu BE, 2024, Arxiv, DOI [arXiv:2205.14865, 10.48550/arXiv.2205.14865]
NR 61
TC 2
Z9 2
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3469
EP 3480
DI 10.1109/TMM.2023.3311646
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200021
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, J
   Yin, P
   Wang, YY
   Yang, WH
AF Wang, Jun
   Yin, Peng
   Wang, Yuanyun
   Yang, Wenhui
TI CMAT: Integrating Convolution Mixer and Self-Attention for Visual
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution mixer; dropout; self-attention; visual tracking
AB Convolutional Neural Networks (CNNs) and Transformer are two powerful representation learning techniques for visual tracking. Although CNNs can effectively reduce local redundancy via small-neighborhood convolution operations, their limited receptive fields make it difficult to capture global dependency. Self-attention in Transformer uses patches as the input representation, which can effectively capture long-range dependency. However, blind similarity comparisons between all patches can lead to high redundancy. Is there then a technique that combines well the advantages of both paradigms for visual tracking? In this work, we design a novel backbone network for feature extraction. First, we choose Depthwise Convolution and Pointwise Convolution to build a Convolution Mixer, which effectively separates spatial mixing from channel-wise mixing of information. The Convolution Mixer reduces redundancy in spatial and channel features while increasing receptive field. Then, to exploit the global modeling ability of self-attention, we construct a module by aggregating Convolution Mixer and self-attention. The module shares dominant computational complexity (the square of the channel size) in the first stage. In the second stage, the shift and summation operations are lightweight. Finally, to alleviate the overfitting of the backbone network during training, a dropout layer is added at the end of the module to improve the generalization ability of the network model. Stronger image features are provided for subsequent feature fusion and prediction. The proposed tracker (named CMAT) achieves satisfying tracking performance on ten challenging datasets. In particular, CMAT achieves a 64.1% AUC on LaSOT and a 68.9% AUC on UAV123 while running at 23 FPS.
C1 [Wang, Jun; Yin, Peng; Wang, Yuanyun; Yang, Wenhui] Nanchang Inst Technol, Sch Informat Engn, Nanchang 330029, Peoples R China.
C3 Nanchang Institute Technology
RP Wang, YY (corresponding author), Nanchang Inst Technol, Sch Informat Engn, Nanchang 330029, Peoples R China.
EM wangjun012778@126.com; yinpeng2020@yeah.net; wangyy_abc@163.com;
   ywh837720@163.com
RI Yang, Wenhui/K-4599-2019
OI Yang, Wenhui/0000-0002-0566-3810
FU National Natural Science Foundation of China
FX No Statement Available
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Blatter P, 2023, IEEE WINT CONF APPL, P1571, DOI 10.1109/WACV56688.2023.00162
   Cao ZA, 2022, PROC CVPR IEEE, P14778, DOI 10.1109/CVPR52688.2022.01438
   Cao ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15437, DOI 10.1109/ICCV48922.2021.01517
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Dosovitskiy A., 2021, INT C LEARN REPRESEN, P1
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fu CH, 2022, IEEE INT C INT ROBOT, P12122, DOI 10.1109/IROS47612.2022.9981248
   Fu ZH, 2022, Arxiv, DOI arXiv:2205.03776
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kristan M., 2018, PROC EUR C COMPUT VI, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin L., 2021, P ADV NEUR INF PROC, P16743
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Ma F, 2022, PROC CVPR IEEE, P8771, DOI 10.1109/CVPR52688.2022.00858
   Mayer C, 2022, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR52688.2022.00853
   Mayer C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13424, DOI 10.1109/ICCV48922.2021.01319
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Pan XR, 2022, PROC CVPR IEEE, P805, DOI 10.1109/CVPR52688.2022.00089
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Shen QH, 2022, PROC CVPR IEEE, P8091, DOI 10.1109/CVPR52688.2022.00793
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Trockman A, 2022, Arxiv, DOI arXiv:2201.09792
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang GT, 2020, PROC CVPR IEEE, P6287, DOI 10.1109/CVPR42600.2020.00632
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang Y., 2023, Adaptive temporal feature modeling for visual tracking via cross-channel learning, knowledge-based systems, V265
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie F, 2022, PROC CVPR IEEE, P8741, DOI 10.1109/CVPR52688.2022.00855
   Xie F, 2021, IEEE INT CONF COMP V, P2688, DOI 10.1109/ICCVW54120.2021.00303
   Xingping Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P378, DOI 10.1007/978-3-030-58565-5_23
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Yang Y., 2022, arXiv
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Zhao MJ, 2021, Arxiv, DOI [arXiv:2105.03817, DOI 10.48550/ARXIV.2105.03817]
   Zheng JL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13526, DOI 10.1109/ICCV48922.2021.01329
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 63
TC 5
Z9 5
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 326
EP 338
DI 10.1109/TMM.2023.3264851
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500006
DA 2024-08-05
ER

PT J
AU Xiang, YX
   Tang, DJ
   Huang, R
   Yao, Y
   Xie, C
   Shi, QM
   Xu, RY
   Haghighat, MR
   Bao, CTY
   Gu, YC
   Qi, ZW
   Guan, HB
AF Xiang, Yuxin
   Tang, Dongjie
   Huang, Rui
   Yao, Yong
   Xie, Chao
   Shi, Qiming
   Xu, Randy
   Haghighat, Mohammad Reza
   Bao, Cathy
   Gu, Yicheng
   Qi, Zhengwei
   Guan, Haibing
TI CARE: Cloudified Android With Optimized Rendering Platform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloudified Android; Cloud gaming; GPU utilization; Kubernetes
ID SYSTEM
AB Due to the excellent rendering capabilities, GPUs are mainstream accelerators in the Cloud-rendering industry. However, current Cloud-rendering systems suffer from a CPU-GPU workload imbalance that not only degrades application performance but also causes a significant waste of GPU resources. Recent proposals (such as API-forwarding and c-GPU) for improving CPU-GPU balance are promising but fail to solve system-resource redundancy issues (i.e., each instance tends to occupy all resources, exceeding its requirements). Such behavior will increase CPU load and lower effective GPU utilization. To demonstrate the severity of the issue, we evaluated real-world applications and results show that in most cases, nearly 50% of resources are useless. To solve this problem, we present CARE, the first framework intended to reduce the system-level redundancy by cloudifying the system from monolithic to Cloud-native. To allow users to configure required services, CARE puts forward a functional unit called Configurable Android (CA). To allow multiple instances to share certain types of resources, CARE innovates Sharing Resource (SR). To reduce the unused services, CARE introduces Pruning Resources (PR). To further alleviate the CPU pressure and achieve CPU-GPU balance, we propose rShare, a system aiming at enhancing CPU effective utilization and increasing Android instance density of the Cloud-rendering platform. Based on Kubernetes, rShare divides all the CPUs into non-overlapping shared CPU pools, allocates instances to pools within milliseconds, and dynamically migrates them by tracking their QoS status. So far, CARE primarily focuses on Android systems and can handle 60 heavyweight instances (e.g., KOG (King of Glory)) on Intel SG1. rShare can apply instance allocation within milliseconds and increase the platform density by 39.4%.
C1 [Xiang, Yuxin; Tang, Dongjie; Gu, Yicheng; Qi, Zhengwei; Guan, Haibing] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
   [Huang, Rui; Xie, Chao; Shi, Qiming; Xu, Randy] Intel Corp, Shanghai 200241, Peoples R China.
   [Yao, Yong; Haghighat, Mohammad Reza; Bao, Cathy] Intel Corp, Santa Clara, CA 95054 USA.
C3 Shanghai Jiao Tong University; Intel Corporation; Intel Corporation
RP Qi, ZW (corresponding author), Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
EM xiangyuxin@sjtu.edu.cn; 018033210001@sjtu.edu.cn; rui1.huang@intel.com;
   yong.yao@intel.com; chao.xie@intel.com; qiming.shi@intel.com;
   randy.xu@intel.com; mohammad.r.haghighat@intel.com; cathy.bao@intel.com;
   guyicheng98@sjtu.edu.cn; qizhwei@sjtu.edu.cn; hbguan@sjtu.edu.cn
OI Haghighat, Mohammad Reza/0009-0009-9051-303X
FU National NSF of China
FX No Statement Available
CR [Anonymous], 2019, CRI resource manager for kubernetes
   [Anonymous], 1992, ABOUT US
   [Anonymous], 2007, Linux: The completely fair scheduler
   [Anonymous], 2011, Red HAT OpenShift
   [Anonymous], 2017, Android emulator
   [Anonymous], 2020, Cuttlefish
   [Anonymous], 2016, NVIDIA virtual GPU (vGPU)
   [Anonymous], 2018, Amazon elastic kubernetes services
   [Anonymous], 2017, Amazon elastic graphics
   Aycock J, 2003, ACM COMPUT SURV, V35, P97, DOI 10.1145/857076.857077
   Baratto L. N., P 20 ACM S OP SYST P
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Chung A, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P121, DOI 10.1145/3267809.3267819
   Du J, 2016, IEEE T MULTIMEDIA, V18, P820, DOI 10.1109/TMM.2016.2537781
   Giunta R., P EUR C PAR PROC
   Gripsgard M., 2017, Anbox
   Guo F, 2019, PROCEEDINGS OF THE 2019 TENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '19), P114, DOI 10.1445/3357223.3362714
   Gupta V., 2009, P 3 ACM WORKSH SYST, P17, DOI DOI 10.1145/1519138.1519141
   Huang Chun-Ying, 2013, P 4 ACM MULT SYST C, P36, DOI [10.1145/2483977.2483981, DOI 10.1145/2483977.2483981]
   Humphreys G, 2001, COMP GRAPH, P129, DOI 10.1145/383259.383272
   Hunt T, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P817
   Li YS, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P491, DOI 10.1145/3343031.3350941
   Li YS, 2019, HPDC'19: PROCEEDINGS OF THE 28TH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE PARALLEL AND DISTRIBUTED COMPUTING, P231, DOI 10.1145/3307681.3325409
   Liao XF, 2016, IEEE ACM T NETWORK, V24, P2128, DOI 10.1109/TNET.2015.2450254
   Linsheng Li, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P3348, DOI 10.1145/3394171.3413675
   Prades J, 2019, IEEE T PARALL DISTR, V30, P2718, DOI 10.1109/TPDS.2019.2924433
   Silberstein M, 2016, ACM T COMPUT SYST, V34, DOI 10.1145/2963098
   Silberstein M, 2013, ACM SIGPLAN NOTICES, V48, P485, DOI 10.1145/2499368.2451169
   Su Z, 2016, IEEE T MULTIMEDIA, V18, P1650, DOI 10.1109/TMM.2016.2566584
   Suzuki Y, 2016, IEEE T COMPUT, V65, P2752, DOI 10.1109/TC.2015.2506582
   Tang D, P 29 INT S HIGH PERF, P197
   Tang DJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4582, DOI 10.1145/3474085.3475617
   Vesely J, 2018, CONF PROC INT SYMP C, P843, DOI 10.1109/ISCA.2018.00075
   Wojciechowski L, 2021, IEEE INFOCOM SER, DOI 10.1109/INFOCOM42981.2021.9488670
   Yu HC, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P807, DOI 10.1145/3373376.3378466
   Zhang W, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P324, DOI 10.1145/3123266.3123306
NR 36
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 958
EP 971
DI 10.1109/TMM.2023.3274303
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700018
DA 2024-08-05
ER

PT J
AU Yang, ZG
   Yang, ZP
   Guo, ZW
   Lin, ZH
   Zhu, HZ
   Li, Q
   Liu, WY
AF Yang, Zhenguo
   Yang, Zhuopan
   Guo, Zhiwei
   Lin, Zehang
   Zhu, Haizhong
   Li, Qing
   Liu, Wenyin
TI Towards Temporal Event Detection: A Dataset, Benchmarks and Challenges
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social networking (online); Event detection; Blogs; Training; Task
   analysis; Media; Data models; News event detection; multimodal data;
   social media
AB The availability of datasets annotated with verified events by the public is a necessary prerequisite for unleashing the potential of multimodal deep learning for news event detection. Publicly available datasets are either incompletely annotated due to expensive cost, or ignore the verifiability of event labels, which are susceptible to bias and errors introduced by a limited number of annotators. In this article, we provide a YouTube dataset labelled by real-world news events that can be verified by Wikipedia-like crowd sourcing platforms, with the target of advancing temporal event detection. The events in our dataset cover a wide range of event topics including public security, natural disasters, elections, sports, and entertainment events, etc. In the dataset, each sample is labelled with real-world event that is verifiable by the public. We extensively evaluate the performance of 13 state-of-the-art algorithms on our dataset in a temporal manner, involving the multiple relationships between training and testing event labels, and provide a thorough analysis of the findings.
C1 [Yang, Zhenguo; Yang, Zhuopan; Guo, Zhiwei; Zhu, Haizhong; Liu, Wenyin] Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510006, Peoples R China.
   [Lin, Zehang; Li, Qing] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong 999077, Peoples R China.
C3 Guangdong University of Technology; Hong Kong Polytechnic University
RP Yang, ZG (corresponding author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510006, Peoples R China.
EM yzg@gdut.edu.cn; zhuopanyang@gmail.com; joeguogzw@163.com;
   cszlin@comp.polyu.edu.hk; seamiddle@163.com; csqli@comp.polyu.edu.hk;
   liuwy@gdut.edu.cn
RI Li, Qing/JMH-1365-2023; lin, zehang/ABD-1029-2020
OI Li, Qing/0000-0003-3370-471X; Liu, Wenyin/0000-0002-6237-6607
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdelhaq F, 2013, PROC VLDB ENDOW, V6, P1326, DOI 10.14778/2536274.2536307
   [Anonymous], 2014, P 5 ACM MULT SYST C
   [Anonymous], 2012, P 18 ACM SIGKDD INT
   [Anonymous], 2010, P 19 ACM INT C INF K
   [Anonymous], 1998, DARPA BROADCAST NEWS
   Boididou C., 2015, MEDIAEVAL, V3, P7
   Cambria E, 2018, LECT NOTES COMPUT SC, V10762, P166, DOI 10.1007/978-3-319-77116-8_13
   Carta S, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.438
   Chae J, 2012, IEEE CONF VIS ANAL, P143, DOI 10.1109/VAST.2012.6400557
   Chen L., 2009, Proceedings of the 18th ACM conference on Information and knowledge management, P523
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Choi J, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P193, DOI 10.1145/2733373.2809934
   Chu LY, 2016, IEEE T CIRC SYST VID, V26, P556, DOI 10.1109/TCSVT.2014.2347551
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Erra U, 2015, INFORM SCIENCES, V292, P143, DOI 10.1016/j.ins.2014.08.062
   Fansen A, 2017, INT CONF ACOUST SPEE, P786, DOI 10.1109/ICASSP.2017.7952263
   Gao Y, 2016, IEEE T MULTIMEDIA, V18, P2115, DOI 10.1109/TMM.2016.2581483
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Hansu Gu, 2011, 2011 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies, P300, DOI 10.1109/WI-IAT.2011.126
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hua T, 2016, GEOINFORMATICA, V20, P765, DOI 10.1007/s10707-016-0263-0
   Phan H, 2015, IEEE INT CON MULTI
   Jelodar H, 2021, MULTIMED TOOLS APPL, V80, P4155, DOI 10.1007/s11042-020-09755-z
   Jiang Y.-G., 2011, P 1 ACM INT C MULT R, P1
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Katragadda S, 2017, PROCEEDINGS OF THE 50TH ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, P1716
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Khan Hikmat Ullah, 2021, Expert Systems with Applications, V164, P443, DOI 10.1016/j.eswa.2020.113990
   Li C., 2012, P 21 ACM INT C INF K, P155, DOI [10.1145/2396761.2396785, DOI 10.1145/2396761.2396785]
   Li R, 2012, PROC INT CONF DATA, P1273, DOI 10.1109/ICDE.2012.125
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Mai S., 2022, IEEE Trans, Multimedia, early access, DOI [10.1109/TMM.20223171679, DOI 10.1109/TMM.20223171679]
   Müller-Budack E, 2021, IEEE WINT CONF APPL, P2927, DOI 10.1109/WACV48630.2021.00297
   Peng H, 2023, IEEE T PATTERN ANAL, V45, P980, DOI 10.1109/TPAMI.2022.3144993
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Phuvipadawat S., 2010, Proceedings of the 2010 IEEE/ACM International Conference on Web Intelligence-Intelligent Agent Technology - Workshops (WI-IAT 2010), P120, DOI 10.1109/WI-IAT.2010.205
   Qi P, 2019, IEEE DATA MINING, P518, DOI 10.1109/ICDM.2019.00062
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Reuter T., 2013, P MED EV MULT BENCHM, P18
   Sakaki T, 2013, IEEE T KNOWL DATA EN, V25, P919, DOI 10.1109/TKDE.2012.29
   Shah Z, 2022, IEEE T BIG DATA, V8, P508, DOI 10.1109/TBDATA.2019.2948594
   Shao J, 2012, PATTERN RECOGN LETT, V33, P410, DOI 10.1016/j.patrec.2011.07.026
   Sivaraman NK, 2021, LECT NOTES BUS INF P, V415, P637, DOI 10.1007/978-3-030-75018-3_47
   Steiner T., 2011, P 10 INT SEM WEB C 1, P58
   Tang TT, 2021, ASIAPAC SIGN INFO PR, P939
   Tartir S, 2017, J KING SAUD UNIV-COM, V29, P229, DOI 10.1016/j.jksuci.2016.11.011
   Tsai Y.-H.H., 2019, P 7 INT C LEARN REPR, P1
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Weng J., 2011, P 5 INT AAAI C WEBL, V5, P401, DOI 10.1609/icwsm.v5i1.14102
   Williams J, 2018, FIRST GRAND CHALLENGE AND WORKSHOP ON HUMAN MULTIMODAL LANGUAGE (CHALLENGE-HML), P64
   Wu Y, 2021, PROC CVPR IEEE, P1326, DOI 10.1109/CVPR46437.2021.00138
   Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639
   Xue F, 2020, IEEE T MULTIMEDIA, V22, P2098, DOI 10.1109/TMM.2019.2951194
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang J, 2022, APPL INTELL, V52, P6503, DOI 10.1007/s10489-021-02729-0
   Yang Z., 2022, P IEEE INT C MULT EX, P1
   Yang ZG, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3374754
   Yang ZG, 2020, IEEE T PATTERN ANAL, V42, P1243, DOI 10.1109/TPAMI.2019.2893953
   Yang ZG, 2017, ACM T INTERNET TECHN, V17, DOI 10.1145/3015463
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhang HW, 2021, IEEE T MULTIMEDIA, V23, P4441, DOI 10.1109/TMM.2020.3042055
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang Y., 2013, P IEEE INT C MULT EX, P1
   Zhao SC, 2016, MULTIMED TOOLS APPL, V75, P8921, DOI 10.1007/s11042-014-2342-2
NR 69
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1102
EP 1113
DI 10.1109/TMM.2023.3276523
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700035
DA 2024-08-05
ER

PT J
AU Zeng, YW
   Han, N
   Pan, KY
   Jin, Q
AF Zeng, Yawen
   Han, Ning
   Pan, Keyu
   Jin, Qin
TI Temporally Language Grounding With Multi-Modal Multi-Prompt Tuning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporally language grounding; prompt learning; multi-modal
   understanding
AB The task of temporally language grounding (TLG), aiming to locate a video moment within an untrimmed video that matches a given textual query, has attracted considerable research attention in recent years. Typical retrieval-based TLG methods are inefficient due to their reliance on a large number of pre-segmented candidate moments, while localization-based TLG solutions adopt reinforcement learning, resulting in unstable convergence. Meanwhile, the cutting-edge capabilities of multi-modal architecture, especially pre-training paradigm, have not been fully exploited. Therefore, how to perform TLG task efficiently and stably is a non-trivial task. In this work, we propose a novel TLG solution named Multi-modal Multi-Prompt Tuning (MMPT), which formulates the TLG task as a prompt-based multi-modal problem and integrates multiple sub-tasks to tune the performance. In this way, off-the-shelf pre-trained models can be directly leveraged to achieve more stable performance. Specifically, a flexible multi-prompt strategy is contributed to rewrite the query firstly, which contains the query, the start and end timestamps. Among them, various prompt templates are integrated to enhance robustness. Thereafter, a multi-modal Transformer is adopted to fully learn the multi-modal context. Moreover, we design various sub-tasks to optimize this novel framework including the matching task, localization task and joint learning task. Extensive experiments on two real-world datasets validate the effectiveness and rationality of our proposed solution.
C1 [Zeng, Yawen; Pan, Keyu] Bytedance AI Lab, Beijing 100728, Peoples R China.
   [Han, Ning] Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.
   [Jin, Qin] Renmin Univ China, Beijing 100728, Peoples R China.
C3 Xiangtan University; Renmin University of China
RP Pan, KY (corresponding author), Bytedance AI Lab, Beijing 100728, Peoples R China.; Jin, Q (corresponding author), Renmin Univ China, Beijing 100728, Peoples R China.
EM yawenzeng11@gmail.com; ninghan@hnu.edu.cn; pankeyu96@gmail.com
OI zeng, yawen/0000-0003-1908-1157
CR Cai D., 2021, P 59 ANN M ASS COMPU, V1, P7307, DOI [DOI 10.18653/V1/2021.ACL-LONG.567, 10.18653/V1/2021.ACL-LONG.567]
   Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P898, DOI 10.1145/3394171.3413841
   Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4162, DOI 10.1145/3394171.3413840
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Chen SY, 2022, KNOWL-BASED SYST, V245, DOI 10.1016/j.knosys.2022.108598
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P177, DOI 10.1109/TMM.2020.3047546
   Chen ZF, 2020, Arxiv, DOI arXiv:2001.09308
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Han N, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3826, DOI 10.1145/3474085.3475241
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Jiang B, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P217, DOI 10.1145/3323873.3325019
   Kim W, 2021, PR MACH LEARN RES, V139
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Lan X., 2021, arXiv
   Li W, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2592
   Li XLS, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P4582
   Liao DL, 2021, AAAI CONF ARTIF INTE, V35, P13353
   Lin JY, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3251, DOI 10.1145/3447548.3467206
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu PF, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3560815
   Liu TL, 2020, IEEE T MULTIMEDIA, V22, P1098, DOI 10.1109/TMM.2019.2936805
   Liu X, 2022, Arxiv, DOI [arXiv:2110.07602, 10.48550/arXiv.2110.07602]
   Liu X, 2023, Arxiv, DOI [arXiv:2103.10385, 10.1016/j.aiopen.2023.08.012, 10.48550/arXiv.2103.10385]
   Liu Y., 2022, arXiv
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Ni MH, 2021, PROC CVPR IEEE, P3976, DOI 10.1109/CVPR46437.2021.00397
   Radford A, 2018, Improving language understanding by generative Pre-Training
   Radford A, 2021, PR MACH LEARN RES, V139
   Regneri M., 2013, T ASSOC COMPUT LING, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Rendle S., 2009, UAI 2009, P452
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Teng JY, 2022, IEEE T MULTIMEDIA, V24, P1141, DOI 10.1109/TMM.2021.3120545
   Wang H, 2021, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR46437.2021.00695
   Wang M, 2021, arXiv
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wang YR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P843, DOI 10.1145/3394171.3413747
   Yang XJ, 2021, J FRANKLIN I, V358, P6462, DOI 10.1016/j.jfranklin.2021.06.009
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yao Y, 2022, Arxiv, DOI arXiv:2109.11797
   Yawen Zeng, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3478025
   Yi J, 2022, IEEE T MULTIMEDIA, V24, P1067, DOI 10.1109/TMM.2021.3111487
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P3208
   Yu X., 2021, PROC SIGIR, P1860
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, P IEEECVF C COMPUTER, P10287
   Zeng Y., 2022, TNNLS
   Zeng YW, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P2003, DOI 10.1145/3477495.3531795
   Zeng YW, 2021, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR46437.2021.00225
   Zhang H., 2020, P 58 ANN M ASS COMPU, P6543
   Zhang H, 2022, Arxiv, DOI arXiv:2201.08071
   Zhang H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P917, DOI 10.1145/3474085.3475272
   Zhang H, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P685, DOI 10.1145/3404835.3462874
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang ZM, 2021, IEEE T IMAGE PROCESS, V30, P8265, DOI 10.1109/TIP.2021.3113791
   Zhao Y, 2021, PROC CVPR IEEE, P4195, DOI 10.1109/CVPR46437.2021.00418
   Zheng Wang, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P1459, DOI 10.1145/3474085.3475278
   Zhou H, 2021, PROC CVPR IEEE, P8441, DOI 10.1109/CVPR46437.2021.00834
NR 66
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3366
EP 3377
DI 10.1109/TMM.2023.3310282
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200016
DA 2024-08-05
ER

PT J
AU Zhou, Z
   Sun, YH
   Sun, QS
   Li, CB
   Ren, ZW
AF Zhou, Ze
   Sun, Yinghui
   Sun, Quansen
   Li, Chaobo
   Ren, Zhenwen
TI Unit Correlation With Interactive Feature for Robust and Effective
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Correlation; Feature extraction; Robustness;
   Transformers; Visualization; Finite element analysis; Visual object
   tracking; feature enhancement; feature interaction; convolution
   sampling; unit correlation
AB For robust and effective tracking, most efforts strive to design a powerful representation target model, while we are inspired by the idea of "knowing oneself and knowing others" to major in both the target and non-target features. In this work, we propose a unit correlation with interactive feature tracker (UCIF), which utilizes feature interaction and independent correlation operation to improve robustness and effectiveness. Specifically, we first propose a feature integration network, in which the feature enhancement module concentrates on enhancing the tracker's representation ability for both target and non-target. The feature interaction module is in charge of completing the interactive learning between target and non-target features. Then, considering the potential risk of blurring spatial information in regular correlation operation, a unit correlation network is presented, where the convolution sampling strategy can integrate the target features as well as reduce the computation costs. The unit kernel for correlation operation can protect the target spatial information. The channel ranking module suppresses background interference via weight assignment. Extensive experiments are conducted on both the short-term and long-term challenging benchmarks, including OTB2015, NFS, UAV123, TrackingNet, GOT-10 k, TLP, LaSOT and VOT-LT2019. Our tracker achieves remarkable performance in robustness and effectiveness.
C1 [Zhou, Ze; Sun, Yinghui; Sun, Quansen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Li, Chaobo] Nantong Univ, Sch Informat Sci & Technol, Nantong 226019, Peoples R China.
   [Ren, Zhenwen] Minist Educ, Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.
   [Ren, Zhenwen] Southwest Univ Sci & Technol, Sch Natl Def Sci & Technol, Mianyang 621010, Peoples R China.
C3 Nanjing University of Science & Technology; Nantong University;
   Southwest University of Science & Technology - China
RP Sun, QS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.; Ren, ZW (corresponding author), Minist Educ, Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.; Ren, ZW (corresponding author), Southwest Univ Sci & Technol, Sch Natl Def Sci & Technol, Mianyang 621010, Peoples R China.
EM zhoumze@163.com; yinghuisun@njust.edu.cn; sunquansen@njust.edu.cn;
   1811310007@yjs.ntu.edu.cn; rzw@njust.edu.cn
OI Sun, Yinghui/0000-0003-1456-2859; REN, ZHEN WEN/0000-0003-3791-9750
FU National Natural Science Foundation of China
FX No Statement Available
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Blatter P, 2023, IEEE WINT CONF APPL, P1571, DOI 10.1109/WACV56688.2023.00162
   Carion N., 2020, EUR C COMP VIS, P213
   Chen X., 2022, PROC EUR C COMPUT V, P461
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng SY, 2021, PROC CVPR IEEE, P4419, DOI 10.1109/CVPR46437.2021.00440
   Cheng ZQ, 2022, PROC CVPR IEEE, P19606, DOI 10.1109/CVPR52688.2022.01902
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Cheng ZQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1897, DOI 10.1145/3343031.3350898
   Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Dong XP, 2023, IEEE T PATTERN ANAL, V45, P8049, DOI 10.1109/TPAMI.2022.3230064
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jung I., 2018, P ECCV, P83
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   Lan J.-P., 2023, P IEEE INT C AC SPEE, P1
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li XF, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109083
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Lukezic A, 2019, LECT NOTES COMPUT SC, V11362, P595, DOI 10.1007/978-3-030-20890-5_38
   Moudgil A, 2019, LECT NOTES COMPUT SC, V11362, P629, DOI 10.1007/978-3-030-20890-5_40
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Seokeon Choi, 2020, Proceedings of the 16th European Conference on Computer Vision (ECCV 2020) Workshops. Lecture Notes in Computer Science (LNCS 12539), P602, DOI 10.1007/978-3-030-68238-5_40
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR46437.2021.00525
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yu QQ, 2023, IEEE T MULTIMEDIA, V25, P1452, DOI 10.1109/TMM.2023.3234372
   Zhang JP, 2023, NEUROCOMPUTING, V522, P73, DOI 10.1016/j.neucom.2022.11.093
   Zhang YH, 2021, INT J COMPUT VISION, V129, P2536, DOI 10.1007/s11263-021-01487-3
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao MJ, 2021, Arxiv, DOI [arXiv:2105.03817, DOI 10.48550/ARXIV.2105.03817]
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhou Z, 2023, IEEE T MULTIMEDIA, V25, P5444, DOI 10.1109/TMM.2022.3192775
   Zhu XF, 2022, IEEE T MULTIMEDIA, V24, P301, DOI 10.1109/TMM.2021.3050073
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 62
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4242
EP 4254
DI 10.1109/TMM.2023.3321476
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100014
DA 2024-08-05
ER

PT J
AU Zhu, WJ
   Peng, B
   Yan, WQ
AF Zhu, Wenjie
   Peng, Bo
   Yan, Wei Qi
TI Dual Knowledge Distillation on Multiview Pseudo Labels for Unsupervised
   Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Task analysis; Reliability; Cameras; Training data; Noise
   measurement; Multiprotocol label switching; Unsupervised person
   re-identification; knowledge distillation; multiview pseudo labels;
   self-knowledge distillation
AB Unsupervised person re-identification (Re-ID) has made significant progress by leveraging valuable pseudo labels from completely unlabeled data. However, the predominant use of pseudo labels heavily relies on clustering results, which may lead to the accumulation of supervision deviation due to inevitable noise. In this paper, we propose a novel framework, namely Dual Knowledge Distillation on Multiview Pseudo Labels (DKD-MPL), to address this challenge. Specifically, the proposed DKD-MPL framework consists of two modules: Global Knowledge Distillation (GKD) and Self-Knowledge Distillation (SKD). In the GKD module, the pseudo labels obtained from the epoch-wise clustering procedure serve as the logits for the teacher model, while the mini-batch query images' pseudo labels act as the logits for the student model. Within the SKD module, we facilitate self-knowledge distillation by considering the pseudo labels generated by positive anchors and query images as two augmentations of the mini-batch data. As a result, DKD-MPL facilitates the exploitation of both global and local complementary knowledge across different views of pseudo labels, thereby mitigating supervision deviation. To demonstrate the effectiveness of DKD-MPL, we provide a theoretical analysis of the proposed loss and conduct extensive experiments on four popular datasets, e.g., Market-1501, DukeMTMC-reID, MSMT17, and VeRi-776. The results indicate that our method surpasses unsupervised approaches and achieves comparable performance to supervised person Re-ID methods.
C1 [Zhu, Wenjie] China Jiliang Univ, Coll Informat Engn, Hangzhou 310018, Peoples R China.
   [Zhu, Wenjie] Auckland Univ Technol, Auckland 1010, New Zealand.
   [Peng, Bo] Univ Queensland, St Lucia, Qld 4072, Australia.
   [Yan, Wei Qi] Auckland Univ Technol, Sch Engn Comp & Math Sci, Auckland 1010, New Zealand.
C3 China Jiliang University; Auckland University of Technology; University
   of Queensland; Auckland University of Technology
RP Zhu, WJ (corresponding author), Auckland Univ Technol, Auckland 1010, New Zealand.
EM zhwj@cjlu.edu.cn; weiqi.yan@aut.ac.nz
FU Natural Science Foundation of Zhejiang Province of China
FX No Statement Available
CR Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P4285, DOI 10.1109/TMM.2021.3114539
   Chen YC, 2023, IEEE T MULTIMEDIA, V25, P9479, DOI 10.1109/TMM.2023.3253391
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Dai G., 2022, P AS C COMP VIS ACCV, P1142
   Dai YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11844, DOI 10.1109/ICCV48922.2021.01165
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Feng H, 2021, IEEE T IMAGE PROCESS, V30, P2898, DOI 10.1109/TIP.2021.3056212
   Feng YJ, 2023, IEEE T MULTIMEDIA, V25, P7647, DOI 10.1109/TMM.2022.3224663
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, ICLR
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   Han XM, 2023, IEEE T IMAGE PROCESS, V32, P29, DOI 10.1109/TIP.2022.3224325
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hou RB, 2021, PROC CVPR IEEE, P2014, DOI 10.1109/CVPR46437.2021.00205
   Ji M, 2021, PROC CVPR IEEE, P10659, DOI 10.1109/CVPR46437.2021.01052
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330
   Lee H, 2020, PR MACH LEARN RES, V119
   Li HF, 2022, IEEE T CIRC SYST VID, V32, P2814, DOI 10.1109/TCSVT.2021.3099943
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Mao JZ, 2023, IEEE T MULTIMEDIA, V25, P1592, DOI 10.1109/TMM.2023.3265159
   Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Peng JJ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P913
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Shi YX, 2021, IEEE T MULTIMEDIA, V23, P4376, DOI 10.1109/TMM.2020.3042068
   Shi YX, 2021, IEEE T MULTIMEDIA, V23, P3264, DOI 10.1109/TMM.2020.3023272
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Sukmin Yun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13873, DOI 10.1109/CVPR42600.2020.01389
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Verma A, 2023, IEEE T MULTIMEDIA, V25, P364, DOI 10.1109/TMM.2021.3126404
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu L, 2022, IEEE T IMAGE PROCESS, V31, P4803, DOI 10.1109/TIP.2022.3186746
   Wu YH, 2022, AAAI CONF ARTIF INTE, P2750
   Yao YM, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108708
   Ye M, 2022, IEEE T IMAGE PROCESS, V31, P379, DOI 10.1109/TIP.2021.3131937
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang CY, 2021, COMMUN ACM, V64, P107, DOI 10.1145/3446776
   Zhang LF, 2019, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2019.00381
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
NR 64
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7359
EP 7371
DI 10.1109/TMM.2024.3366395
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000026
DA 2024-08-05
ER

PT J
AU Chen, YN
   Li, A
   Wu, D
   Zhou, L
AF Chen, Yanan
   Li, Ang
   Wu, Dan
   Zhou, Liang
TI Toward General Cross-Modal Signal Reconstruction for Robotic
   Teleoperation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Robotic teleoperation; cross-modal signal reconstruction; masked
   auto-encoder; scalable architecture
ID IMAGE
AB The multi-modal robotic teleoperation, as an important application in human-computer interaction (HCI), is playing a significant role in various domains such as industry, healthcare, and education. However, existing robotic teleoperation systems face significant challenges with multi-modal signals, primarily in designing a cross-modal communication architecture that caters to diverse modal requirements and ensuring high-quality cross-modal signal reconstruction even in poor network conditions. To this end, this work proposes a general cross-modal signal reconstruction scheme by taking full advantage of the correlation among different modality signals. Specifically, we first propose a scalable cross-modal communication architecture that meets the diverse needs of various modality signals using multi-modal encoding and multi-directional decoding, eliminating the need for a specialized feature extraction model. Next, we design a masked auto-encoder with discriminator assistance (MAE-D) cross-modal signal reconstruction method, which leverages the idea of generative confrontation by combining the codec for signal reconstruction with the discriminator responsible for assessing the authenticity of the reconstructed signal to achieve accurate and efficient cross-modal signal reconstruction. Finally, numerical experiments conducted on our self-built multi-modal dataset, a public dataset, and a teleoperation simulation platform demonstrate that the proposed scheme offers significant advantages in cross-modal signal reconstruction.
C1 [Chen, Yanan; Li, Ang; Zhou, Liang] Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Network, Minist Educ, Nanjing 210003, Peoples R China.
   [Wu, Dan] Army Engn Univ PLA, Inst Commun Engn, Nanjing 210007, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Army Engineering
   University of PLA
RP Zhou, L (corresponding author), Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Network, Minist Educ, Nanjing 210003, Peoples R China.
EM yananchen2021@163.com; liangnjupt@163.com; wujing1958725@126.com;
   liang.zhou@njupt.edu.cn
OI Li, Ang/0000-0001-7149-3250
FU National Natural Science Foundation of China
FX No Statement Available
CR Cai SY, 2021, IEEE ROBOT AUTOM LET, V6, P7525, DOI 10.1109/LRA.2021.3095925
   Chen MZ, 2021, IEEE J SEL AREA COMM, V39, P3579, DOI 10.1109/JSAC.2021.3118346
   Cizmeci B, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063594
   Covaci A, 2020, IEEE T MULTIMEDIA, V22, P1249, DOI 10.1109/TMM.2019.2941274
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Darvish K, 2023, IEEE T ROBOT, V39, P1706, DOI 10.1109/TRO.2023.3236952
   de Melo CM, 2020, IEEE INT C INT ROBOT, P10278, DOI 10.1109/IROS45743.2020.9340728
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eid M, 2012, IEEE ACM DIS SIM, P118, DOI 10.1109/DS-RT.2012.23
   Gao Y, 2022, IEEE J SEL AREA COMM, V40, P3139, DOI 10.1109/JSAC.2022.3211539
   Gui J, 2023, IEEE T KNOWL DATA EN, V35, P3313, DOI 10.1109/TKDE.2021.3130191
   Guo SX, 2019, IEEE T ROBOT, V35, P685, DOI 10.1109/TRO.2019.2896763
   Han Z, 2021, IEEE INT CONF ROBOT, P2473, DOI 10.1109/ICRA48506.2021.9560847
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Jia C, 2021, PR MACH LEARN RES, V139
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim W., 2021, PROC INT C MACH LEAR, P5583
   Lambeta M, 2020, IEEE ROBOT AUTOM LET, V5, P3838, DOI 10.1109/LRA.2020.2977257
   Li R, 2022, IEEE T CIRC SYST VID, V32, P2647, DOI 10.1109/TCSVT.2021.3057992
   Long XM, 2021, IEEE INT CONF ROBOT, P11169, DOI 10.1109/ICRA48506.2021.9561092
   Luo S, 2018, IEEE INT CONF ROBOT, P2722, DOI 10.1109/ICRA.2018.8460494
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Nie DY, 2023, IEEE T MULTIMEDIA, V25, P6436, DOI 10.1109/TMM.2022.3208740
   Niu ST, 2022, ACM TRANS MANAG INF, V13, DOI 10.1145/3464324
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Panzirsch M, 2022, SCI ROBOT, V7, DOI 10.1126/scirobotics.abl6307
   Ruo A, 2023, SPR PROC ADV ROBOT, V26, P233, DOI 10.1007/978-3-031-22731-8_17
   Shuteng Niu, 2020, IEEE Transactions on Artificial Intelligence, V1, P151, DOI 10.1109/TAI.2021.3054609
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei X, 2023, IEEE T MULTIMEDIA, V25, P5527, DOI 10.1109/TMM.2022.3194309
   Wei X, 2022, IEEE T MULTIMEDIA, V24, P4514, DOI 10.1109/TMM.2021.3119860
   Wei X, 2021, IEEE WIREL COMMUN, V28, P182, DOI 10.1109/MWC.001.2000448
   Xing HJ, 2022, IEEE T SYST MAN CY-S, V52, P6283, DOI 10.1109/TSMC.2022.3144009
   Yan SH, 2019, IEEE WIREL COMMUN, V26, P19, DOI 10.1109/MWC.001.1900057
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P746, DOI 10.1109/TMM.2018.2865828
   Yu H, 2022, IEEE ROBOT AUTOM LET, V7, P2162, DOI 10.1109/LRA.2022.3142439
   Yu LY, 2022, IEEE T MULTIMEDIA, V24, P2950, DOI 10.1109/TMM.2021.3091863
   Zambelli M., 2021, P MACHINE LEARNING R, P1415
   Zhou L, 2021, IEEE J SEL AREA COMM, V39, P426, DOI 10.1109/JSAC.2020.3021543
   Zhou L, 2020, IEEE WIREL COMMUN, V27, P112, DOI 10.1109/MWC.001.1900201
   Zhou ZL, 2022, IEEE T HAPTICS, V15, P464, DOI 10.1109/TOH.2022.3170723
NR 43
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3541
EP 3553
DI 10.1109/TMM.2023.3312944
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200001
DA 2024-08-05
ER

PT J
AU Groenen, I
   Rudinac, S
   Worring, M
AF Groenen, Inske
   Rudinac, Stevan
   Worring, Marcel
TI PanorAMS: Automatic Annotation for Detecting Objects in Urban Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Annotations; Geospatial analysis; Noise measurement; Urban areas;
   Protocols; Object detection; Labeling; noisy labeling; crowdsourcing;
   urban computing; panoramic image datasets
ID DATABASE; TOOL
AB Large collections of geo-referenced panoramic images are freely available for cities across the globe, as well as detailed maps with location and meta-data on a great variety of urban objects. They provide a potentially rich source of information on urban objects, but manual annotation for object detection is expensive, laborious and challenging. Can we utilize such multimedia sources to automatically annotate street level images as an inexpensive alternative to manual labeling? With the PanorAMS framework we introduce a method to automatically generate bounding box annotations for panoramic images based on urban context information. Following this method, we acquire large-scale, albeit noisy, annotations for an urban dataset solely from open data sources in a fast and automatic manner. The dataset covers the City of Amsterdam and includes over 14 million noisy bounding box annotations of 22 object categories present in 771,299 panoramic images. For many objects further fine-grained information is available, obtained from geospatial meta-data, such as building value, function and average surface area. Such information would have been difficult, if not impossible, to acquire via manual labeling based on the image alone. For detailed evaluation, we introduce an efficient crowdsourcing protocol for bounding box annotations in panoramic images, which we deploy to acquire 147,075 ground-truth object annotations for a subset of 7,348 images, the PanorAMS-clean dataset. For our PanorAMS-noisy dataset, we provide an extensive analysis of the noise and how different types of noise affect image classification and object detection performance.
C1 [Groenen, Inske] Univ Amsterdam, Informat Inst, NL-1098 XH Amsterdam, Netherlands.
   [Rudinac, Stevan] Univ Amsterdam, Amsterdam Business Sch, NL-1018 TV Amsterdam, Netherlands.
   [Worring, Marcel] Univ Amsterdam, Informat Inst, Fac Natuurwetenschappen Wiskunde Informat, Amsterdam, Netherlands.
C3 University of Amsterdam; University of Amsterdam; University of
   Amsterdam
RP Groenen, I (corresponding author), Univ Amsterdam, Informat Inst, NL-1098 XH Amsterdam, Netherlands.
EM research@inske.groenen.me; s.rudinac@uva.nl; m.worring@uva.nl
OI Worring, Marcel/0000-0003-4097-4136
FU European Regional Development Fund
FX No Statement Available
CR Ahn, 2019, About us
   [Anonymous], 2019, Amazon Mechanical Turk
   [Anonymous], 2020, Kerncijfers Amsterdam.
   [Anonymous], 2019, Data en informatie
   Ardeshir S, 2015, PROC CVPR IEEE, P2792, DOI 10.1109/CVPR.2015.7298896
   Ardeshir S, 2014, LECT NOTES COMPUT SC, V8694, P602, DOI 10.1007/978-3-319-10599-4_39
   Arpit D, 2017, PR MACH LEARN RES, V70
   Bianco S, 2015, COMPUT VIS IMAGE UND, V131, P88, DOI 10.1016/j.cviu.2014.06.015
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   D'Orazio T, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P559, DOI 10.1109/AVSS.2009.69
   Dong XY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P279, DOI 10.1145/3123266.3123455
   Dutta A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2276, DOI 10.1145/3343031.3350535
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Ghosh A., 2017, AAAI CONF ARTIF INTE
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Gygli M, 2020, INT J COMPUT VISION, V128, P1061, DOI 10.1007/s11263-019-01255-4
   He K., 2017, P IEEE INT C COMP VI, P2961
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hongwei Yong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P635, DOI 10.1007/978-3-030-58452-8_37
   Karasawa T, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P35, DOI 10.1145/3126686.3126727
   Kletz S, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P133, DOI 10.1145/3304109.3306223
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Laddha A, 2016, IEEE INT VEH SYM, P118, DOI 10.1109/IVS.2016.7535374
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li W, 2017, Arxiv, DOI arXiv:1708.02862
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Máttyus G, 2016, PROC CVPR IEEE, P3611, DOI 10.1109/CVPR.2016.393
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Openstreetmap, 2019, About us
   Papadopoulos DP, 2017, IEEE I CONF COMP VIS, pCP38, DOI 10.1109/ICCV.2017.528
   Papadopoulos DP, 2017, PROC CVPR IEEE, P180, DOI 10.1109/CVPR.2017.27
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sergieh H. M., 2012, P 2 ACM INT C MULT R, P1
   Smith LN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2520589
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su H., 2012, WORKSH 26 AAAI C ART
   Sukel M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4518, DOI 10.1145/3394171.3414427
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Tao QY, 2019, IEEE T MULTIMEDIA, V21, P1135, DOI 10.1109/TMM.2018.2875597
   Venkatesh G. M., 2021, UrbanMM'21: Proceedings of the 1st International Workshop on Multimedia Computing for Urban Data, P23, DOI 10.1145/3475721.3484313
   Wang SL, 2017, IEEE I CONF COMP VIS, P3028, DOI 10.1109/ICCV.2017.327
   Wang SL, 2015, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2015.7299022
   Wegner JD, 2016, PROC CVPR IEEE, P6014, DOI 10.1109/CVPR.2016.647
   Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y
   Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yang KL, 2020, IEEE INT VEH SYM, P457, DOI [10.1109/IV47402.2020.9304706, 10.1109/iv47402.2020.9304706]
   Yang KL, 2021, IEEE T IMAGE PROCESS, V30, P1866, DOI 10.1109/TIP.2020.3048682
   Yang KL, 2020, IEEE T INTELL TRANSP, V21, P4171, DOI 10.1109/TITS.2019.2938965
   Yang K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1688, DOI 10.1145/3394171.3413835
   Yao YZ, 2020, IEEE T KNOWL DATA EN, V32, P1199, DOI 10.1109/TKDE.2019.2903036
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Ying Z, 2016, P 24 ACM INT C MULT, P611, DOI DOI 10.1145/2964284.2967294
   Yogamani S, 2019, IEEE I CONF COMP VIS, P9307, DOI 10.1109/ICCV.2019.00940
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang J., 2020, PROC IEEECVF C COMP, P12546
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 65
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1281
EP 1294
DI 10.1109/TMM.2023.3279696
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700022
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Han, Z
   Wang, YM
   Zhang, SJ
   Fan, HJ
   Tang, YD
   Wang, Y
AF Han, Zhi
   Wang, Yanmei
   Zhang, Shaojie
   Fan, Huijie
   Tang, Yandong
   Wang, Yao
TI Online Video Sparse Noise Removing via Nonlocal Robust PCA
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Robust PCA; online scheme; nonlocal similarity; video denoising
ID SUBSPACE-TRACKING
AB Online schemes and nonlocal similarity are two effective approaches for strengthening robust principal component analysis (RPCA) techniques in video denoising. However, their limitations are also evident. The online scheme is usually highly efficient but lacks consideration of regional appearance information, thus it cannot effectively handle videos with complex dynamics such as object movements. On the other hand, nonlocal similarity is used to better utilize regional information but incurs a heavy computational cost. Moreover, these two techniques are incompatible and challenging to work together. To overcome this barrier and harness the advantages of both approaches, this paper proposes a novel online nonlocal RPCA method. 1) A clustering based nonlocal strategy (ClusNonlocal) is adopted, which not only greatly reduces the computation cost, but also forms low-dimensional subspaces for online processing. 2) A new weighted RPCA model is proposed, which regards samples with different importances and improves the performance of subspace pursuit and video recovery. 3) A multi-level subspace updating scheme and weighted projection method is proposed, which keeps the performance of online video data processing at a high level at all time. A series of video denoising experiments are carried out to demonstrate the overall advantages of our procedure over several other ones, in terms of both visual quality and running speed.
C1 [Han, Zhi; Wang, Yanmei; Zhang, Shaojie; Fan, Huijie; Tang, Yandong] Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang 110016, Peoples R China.
   [Han, Zhi; Zhang, Shaojie; Fan, Huijie; Tang, Yandong] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110169, Peoples R China.
   [Wang, Yanmei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Wang, Yao] Xi An Jiao Tong Univ, Ctr Intelligent Decis Making & Machine Learning, Sch Management, Xian 710049, Peoples R China.
C3 Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Xi'an Jiaotong University
RP Wang, Y (corresponding author), Xi An Jiao Tong Univ, Ctr Intelligent Decis Making & Machine Learning, Sch Management, Xian 710049, Peoples R China.
EM hanzhi@sia.cn; wangyanmei@sia.cn; zhangshaojie9509@gmail.com;
   fanhuijie@sia.cn; ytang@sia.cn; yao.s.wang@gmail.com
OI Han, Zhi/0000-0002-8039-6679; Tang, Yandong/0000-0003-3805-7654
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2012, PROC INT S COMMUN
   [Anonymous], 2013, Advances in Neural Information Processing Systems
   Arabi H, 2021, ANN NUCL MED, V35, P176, DOI 10.1007/s12149-020-01550-y
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1014, DOI 10.1109/TCYB.2015.2419737
   Chan RH, 2005, IEEE T IMAGE PROCESS, V14, P1479, DOI 10.1109/TIP.2005.852196
   Chen HA, 2023, IEEE T MULTIMEDIA, V25, P5704, DOI 10.1109/TMM.2022.3198317
   Chen HA, 2022, IEEE T MULTIMEDIA, V24, P2164, DOI 10.1109/TMM.2021.3077140
   Chen K, 2013, BIOMETRIKA, V100, P901, DOI 10.1093/biomet/ast036
   Chenlu Qiu, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P752
   Chenlu Qiu, 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P591, DOI 10.1109/ALLERTON.2010.5706961
   Chouvardas Symeon, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5497, DOI 10.1109/ICASSP.2014.6854654
   Chouvardas S, 2015, IEEE T SIGNAL PROCES, V63, P5060, DOI 10.1109/TSP.2015.2449254
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5
   Guo H, 2018, IEEE SIGNAL PROC LET, V25, P1009, DOI 10.1109/LSP.2018.2833429
   Guo H, 2014, IEEE T SIGNAL PROCES, V62, P4284, DOI 10.1109/TSP.2014.2331612
   Guo LQ, 2022, IEEE T IMAGE PROCESS, V31, P1311, DOI 10.1109/TIP.2022.3140918
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   Hong B, 2016, NEUROCOMPUTING, V175, P216, DOI 10.1016/j.neucom.2015.10.052
   Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250
   Ji H, 2011, SIAM J IMAGING SCI, V4, P1122, DOI 10.1137/100817206
   Jia ZG, 2022, IEEE T IMAGE PROCESS, V31, P3868, DOI 10.1109/TIP.2022.3176133
   Jin LH, 2013, IEEE T CIRC SYST VID, V23, P741, DOI 10.1109/TCSVT.2012.2207272
   Kong ZM, 2019, IEEE T IMAGE PROCESS, V28, P4247, DOI 10.1109/TIP.2019.2907478
   Liang JY, 2022, Arxiv, DOI [arXiv:2201.12288, DOI 10.48550/ARXIV.2201.12288]
   Liang Jingyun, 2022, ADV NEURAL INF PROCE, V35, P378
   Liu HS, 2022, IEEE T IMAGE PROCESS, V31, P5677, DOI 10.1109/TIP.2022.3193754
   Narayanamurthy P, 2018, PR MACH LEARN RES, V80
   Ou Y, 2022, MULTIMED TOOLS APPL, V81, P20357, DOI 10.1007/s11042-022-12083-z
   Papyan V, 2016, IEEE T IMAGE PROCESS, V25, P249, DOI 10.1109/TIP.2015.2499698
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Rehman A, 2011, IEEE IMAGE PROC, P217, DOI 10.1109/ICIP.2011.6116065
   Rui Lai, 2010, Proceedings of the 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P720, DOI 10.1109/CISP.2010.5646842
   Seidel F, 2014, MACH VISION APPL, V25, P1227, DOI 10.1007/s00138-013-0555-4
   Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143
   Tassano M, 2019, IEEE IMAGE PROC, P1805, DOI [10.1109/ICIP.2019.8803136, 10.1109/icip.2019.8803136]
   Vaksman G, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2137, DOI 10.1109/ICCV48922.2021.00216
   Van De Ville D, 2009, IEEE SIGNAL PROC LET, V16, P973, DOI 10.1109/LSP.2009.2027669
   Vaswani N, 2018, IEEE SIGNAL PROC MAG, V35, P32, DOI 10.1109/MSP.2018.2826566
   Wang GH, 2010, SIGNAL PROCESS, V90, P3213, DOI 10.1016/j.sigpro.2010.05.026
   Wang Y, 2022, SIGNAL IMAGE VIDEO P, V16, P403, DOI 10.1007/s11760-021-01948-9
   Wen BH, 2019, IEEE T IMAGE PROCESS, V28, P1691, DOI 10.1109/TIP.2018.2865684
   Wen BH, 2017, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2017.35
   Wright A., 2009, Neural Inf. Process. Syst., V58, P289
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P15, DOI 10.1109/TMM.2018.2849605
   Yi XY, 2016, 30 C NEURAL INFORM P, V29
   Zhao YP, 2022, IEEE IMAGE PROC, P2036, DOI 10.1109/ICIP46576.2022.9898028
   Zhou ZH, 2010, IEEE INT SYMP INFO, P1518, DOI 10.1109/ISIT.2010.5513535
NR 53
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7130
EP 7145
DI 10.1109/TMM.2024.3360713
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000062
DA 2024-08-05
ER

PT J
AU Lei, PC
   Fang, FM
   Zeng, TY
   Zhang, GX
AF Lei, Pengcheng
   Fang, Faming
   Zeng, Tieyong
   Zhang, Guixu
TI Flow Guidance Deformable Compensation Network for Video Frame
   Interpolation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Estimation; Deformation; Interpolation; Convolution;
   Kernel; Optical imaging; Video frame interpolation; motion estimation;
   motion compensation; deformable convolution; distillation learning
AB Flow-based and deformable convolution (DConv)-based methods are two mainstream approaches for solving the video frame interpolation (VFI) problem, which have made remarkable progress with the development of deep convolutional networks over the past years. However, flow-based VFI methods often suffer from the inaccuracy of flow map estimation, especially in dealing with complex and irregular real-world motions. DConv-based VFI methods have advantages in handling complex motions, while the increased degree of freedom makes the training of the DConv model difficult. To address these problems, in this article, we propose a flow guidance deformable compensation network (FGDCN) for the VFI task. FGDCN decomposes the frame sampling process into two steps: a flow step and a deformation step. Specifically, the flow step utilizes a coarse-to-fine flow estimation network to directly estimate the intermediate flows and synthesizes an anchor frame simultaneously. To ensure the accuracy of the estimated flow, a distillation loss and a task-oriented loss are jointly employed in this step. Under the guidance of the flow priors learned in step one, the deformation step designs a new pyramid deformable compensation network to compensate for the missing details of the flow step. In addition, a pyramid loss is proposed to supervise the model in both the image and frequency domains. Experimental results show that the proposed algorithm achieves excellent performance on various datasets with fewer parameters.
C1 [Lei, Pengcheng; Fang, Faming; Zhang, Guixu] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
   [Lei, Pengcheng; Fang, Faming; Zhang, Guixu] East China Normal Univ, KLATASDS MOE, Shanghai 200062, Peoples R China.
   [Zeng, Tieyong] Chinese Univ Hong Kong, Dept Math, Shenzhen 518172, Peoples R China.
C3 East China Normal University; East China Normal University; The Chinese
   University of Hong Kong, Shenzhen
RP Fang, FM (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.; Fang, FM (corresponding author), East China Normal Univ, KLATASDS MOE, Shanghai 200062, Peoples R China.
EM pengchenglei1995@163.com; fmfang@cs.ecnu.edu.cn; zeng@math.cuhk.edu.hk;
   gxzhang@cs.ecnu.edu.cn
OI ZENG, Tieyong/0000-0002-0688-202X
FU National Key Ramp;D Program of China
FX No Statement Available
CR Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Bertasius G, 2018, LECT NOTES COMPUT SC, V11216, P342, DOI 10.1007/978-3-030-01258-8_21
   Chan K.C., 2022, P IEEE CVF C COMP VI, P5972
   Chan KCK, 2021, AAAI CONF ARTIF INTE, V35, P973
   Chen ZQ, 2021, IEEE OPEN J SIGNAL P, V2, P413, DOI 10.1109/OJSP.2021.3075879
   Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Danier D, 2022, PROC CVPR IEEE, P3511, DOI 10.1109/CVPR52688.2022.00351
   Deng LY, 2020, IEEE T INTELL TRANSP, V21, P4350, DOI 10.1109/TITS.2019.2939832
   Fuoli D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2340, DOI 10.1109/ICCV48922.2021.00236
   Hu P., 2022, PROC IEEECVF C COMPU, P3553
   Huang Z., 2020, arXiv
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Kong LT, 2022, PROC CVPR IEEE, P1968, DOI 10.1109/CVPR52688.2022.00201
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Li Z, 2022, PROC CVPR IEEE, P17541, DOI 10.1109/CVPR52688.2022.01704
   Lin J., 2021, arXiv
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCV.2017.478, 10.1109/ICCVW.2017.361]
   Lu L., 2022, P IEEE C COMP VIS PA, P3532
   Luo ZW, 2022, IEEE COMPUT SOC CONF, P997, DOI 10.1109/CVPRW56347.2022.00113
   Mac KNC, 2019, IEEE I CONF COMP VIS, P6291, DOI 10.1109/ICCV.2019.00638
   Meister S, 2018, AAAI CONF ARTIF INTE, P7251
   Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Park J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14519, DOI 10.1109/ICCV48922.2021.01427
   Reda FA, 2019, IEEE I CONF COMP VIS, P892, DOI 10.1109/ICCV.2019.00098
   Shi ZH, 2022, PROC CVPR IEEE, P17461, DOI 10.1109/CVPR52688.2022.01696
   Shi ZH, 2022, IEEE T MULTIMEDIA, V24, P426, DOI 10.1109/TMM.2021.3052419
   Shurui Gui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14001, DOI 10.1109/CVPR42600.2020.01402
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Usman M, 2016, IEEE T MULTIMEDIA, V18, P831, DOI 10.1109/TMM.2016.2537200
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Xiang XY, 2020, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR42600.2020.00343
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yihao Liu, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P41, DOI 10.1007/978-3-030-66823-5_3
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 43
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1801
EP 1812
DI 10.1109/TMM.2023.3289702
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800018
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, ZY
   Zhong, P
   Huang, JW
   Gao, F
   Wang, JX
AF Li, Zhaoyi
   Zhong, Ping
   Huang, Jiawei
   Gao, Feng
   Wang, Jianxin
TI Achieving QoE Fairness in Bitrate Allocation of 360° Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360(degrees) video; bitrate allocation; multi-users; video streaming
AB In tile-based 360 degrees video streaming, the users employ the tile rate allocation algorithm to select appropriate bitrate to maximize the quality of experience (QoE). The preferences and viewports, however, can vary significantly across the different users. Since the users independently choose their bitrate according to their own preferences and viewports, it is hard to ensure QoE fairness for users under the constraint of available bandwidth. In this article, we propose a QoE-fairness aware bitrate allocation algorithm for multi-users (QBAM) to reduce difference of user QoE. According to the trajectory of the user viewpoint and user preferences for video quality, rebuffer time and quality switching, we leverage multi-agent reinforcement learning to train the bitrate allocation strategy. The experimental results show, compared with the current tile rate allocation algorithm, QBAM effectively improves the QoE fairness.
C1 [Li, Zhaoyi; Zhong, Ping; Huang, Jiawei; Gao, Feng; Wang, Jianxin] Cent South Univ, Sch Comp Sci & Engn, Changsha 410017, Peoples R China.
C3 Central South University
RP Huang, JW (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410017, Peoples R China.
EM lizhaoyi@csu.edu.cn; ping.zhong@csu.edu.cn; jiaweihuang@csu.edu.cn;
   gaofengcse@csu.edu.cn; jxwang@csu.edu.cn
OI Li, Zhaoyi/0000-0002-9677-2368
FU High Performance Computing Center of Central South University
FX No Statement Available
CR Alface PR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1105, DOI 10.1145/3123266.3123307
   Altamimi S, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3397227
   [Anonymous], 2014, Internet connection speed recommendations
   [Anonymous], 2022, Kvazar
   [Anonymous], 2017, Q1 2017 state of the internet-connectivity report
   [Anonymous], 2018, Cisco visual networking index: Forecast and trends, 2017-2022
   Ban YX, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102836
   Concolato C, 2018, IEEE T CIRC SYST VID, V28, P1981, DOI 10.1109/TCSVT.2017.2688491
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   F. C.. Commission, 2016, Fcc datase
   Guan Y, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P394, DOI 10.1145/3341302.3342063
   Halvorsen P., 2013, Hsdpa datase
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jiang Junchen, 2012, P 8 INT C EM NETW EX, V12, P97, DOI [DOI 10.1145/2413176.2413189, 10.1145/2413176.2413189]
   Jin HL, 2020, IEEE INT CONF CON AU, P1360, DOI [10.1109/ICCA51439.2020.9264312, 10.1109/icca51439.2020.9264312]
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P2488, DOI 10.1109/TMM.2022.3147667
   Mahajan A, 2019, ADV NEUR IN, V32
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Nathan V, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P408, DOI 10.1145/3341302.3342077
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Qian F., 2016, P 5 WORKSH ALL THING, P1, DOI DOI 10.1145/2980055.2980056
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Rashid T., 2018, 35 INT C MACH LEARN, P6846
   Rui-Xiao Zhang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P3678, DOI 10.1145/3394171.3413918
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2020, IEEE ACM T NETWORK, V28, P1698, DOI 10.1109/TNET.2020.2996964
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   van der Hooft J., 2016, 4G/LTE bandwidth logs
   Wang Z, 2017, P INT C LEARN REPR
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Xie L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P564, DOI 10.1145/3240508.3240556
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Yadav PK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3724, DOI 10.1145/3394171.3413550
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang X, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P289, DOI 10.1145/3341302.3342089
   Zhang YX, 2021, IEEE T MOBILE COMPUT, V20, P2338, DOI 10.1109/TMC.2020.2978187
   Zhou C, 2018, IEEE INFOCOM SER, P962, DOI 10.1109/INFOCOM.2018.8486282
   Zou JN, 2020, IEEE J-STSP, V14, P161, DOI 10.1109/JSTSP.2019.2956716
NR 39
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1169
EP 1178
DI 10.1109/TMM.2023.3277286
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700023
DA 2024-08-05
ER

PT J
AU Liu, Y
   Zhang, XM
   Kauttonen, J
   Zhao, GY
AF Liu, Yang
   Zhang, Xingming
   Kauttonen, Janne
   Zhao, Guoying
TI Uncertain Facial Expression Recognition via Multi-Task Assisted
   Correction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial expression recognition; uncertainty; action unit; valence;
   arousal; multi-task learning
ID FEATURES; WILD
AB Deep models for facial expression recognition achieve high performance by training on large-scale labeled data. However, publicly available datasets contain uncertain facial expressions caused by ambiguous annotations or confusing emotions, which could severely decline the robustness. Previous studies usually follow the bias elimination method in general tasks without considering the uncertainty problem from the perspective of different corresponding sources. This article proposes a novel method of multi-task assisted correction in addressing uncertain facial expression recognition called MTAC. Specifically, a confidence estimation block and a weighted regularization module are applied to highlight solid samples and suppress uncertain samples in every batch. In addition, two auxiliary tasks, i.e., action unit detection and valence-arousal measurement, are introduced to learn semantic distributions from a data-driven AU graph and mitigate category imbalance based on latent dependencies between discrete and continuous emotions, respectively. Moreover, a re-labeling strategy guided by feature-level similarity constraint further generates new labels for identified uncertain samples to promote model learning. The proposed method can flexibly combine with existing frameworks in a fully-supervised or weakly-supervised manner. Experiments on five popular benchmarks demonstrate that the MTAC substantially improves over baselines when facing synthetic and real uncertainties and outperforms the state-of-the-art methods.
C1 [Liu, Yang; Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, FI-90014 Oulu, Finland.
   [Zhang, Xingming] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Kauttonen, Janne] Haaga Hel Univ Appl Sci, FI-00520 Helsinki, Finland.
C3 University of Oulu; South China University of Technology
RP Zhao, GY (corresponding author), Univ Oulu, Ctr Machine Vis & Signal Anal, FI-90014 Oulu, Finland.
EM yang.liu@oulu.fi; cxzxm@scut.edu.cn; Janne.Kauttonen@haaga-helia.fi;
   guoying.zhao@oulu.fi
OI Zhao, Guoying/0000-0003-3694-206X; Kauttonen, Janne/0000-0003-2432-5072;
   Liu, Yang/0000-0003-2157-0080
FU Finnish Cultural Foundation for North Ostrobothnia Regional Fund Towards
   Crowdsensing Facial Affect Encoder for Trustworthy Mental Wellbeing
FX No Statement Available
CR Aila T., 2017, P INT C LEARN REPR
   Antoniadis P., 2021, P IEEE 16 INT C AUT, P1, DOI 10.1109/FG52635.2021.9667014
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Ben XY, 2023, IEEE T MULTIMEDIA, V25, P5429, DOI 10.1109/TMM.2022.3192727
   Ben XY, 2022, IEEE T PATTERN ANAL, V44, P5826, DOI 10.1109/TPAMI.2021.3067464
   Chen YC, 2023, IEEE T MULTIMEDIA, V25, P9479, DOI 10.1109/TMM.2023.3253391
   Chen YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14960, DOI 10.1109/ICCV48922.2021.01471
   Cheng Y, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108718
   Cui ZJ, 2020, AAAI CONF ARTIF INTE, V34, P3693
   Dhall A., 2014, P 16 INT C MULT INT, P461, DOI 10.1145/2663204.2666275
   Ekman P., 1978, Facial action coding system: A technique for the measurement of facial movement
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Gu Y, 2023, IEEE T CIRC SYST VID, V33, P2033, DOI 10.1109/TCSVT.2022.3220669
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu W, 2019, PROC CVPR IEEE, P11879, DOI 10.1109/CVPR.2019.01216
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang W, 2021, IEEE T MULTIMEDIA, V24, P3327, DOI 10.1109/TMM.2021.3096068
   Jin X, 2021, IEEE T IMAGE PROCESS, V30, P7143, DOI 10.1109/TIP.2021.3101820
   Kollias D, 2022, IEEE COMPUT SOC CONF, P2327, DOI 10.1109/CVPRW56347.2022.00259
   Kollias D, 2019, INT J COMPUT VISION, V127, P907, DOI 10.1007/s11263-019-01158-4
   Le N, 2023, IEEE WINT CONF APPL, P6077, DOI 10.1109/WACV56688.2023.00603
   Lei L, 2021, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW53098.2021.00173
   Li J., 2021, P INT C COMP VIS, P9485
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li YT, 2022, IEEE T AFFECT COMPUT, V13, P2028, DOI 10.1109/TAFFC.2022.3205170
   Li YJ, 2023, IEEE T AFFECT COMPUT, V14, P451, DOI 10.1109/TAFFC.2020.3031602
   Li YJ, 2023, IEEE T MULTIMEDIA, V25, P1359, DOI 10.1109/TMM.2022.3141604
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3178, DOI 10.1109/TCSVT.2021.3103760
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu Y, 2023, IEEE T AFFECT COMPUT, V14, P2657, DOI 10.1109/TAFFC.2022.3215918
   Liu Y, 2022, INT C PATT RECOG, P777, DOI 10.1109/ICPR56361.2022.9956650
   Liu Y, 2021, NEUROCOMPUTING, V462, P320, DOI 10.1016/j.neucom.2021.07.017
   Liu Y, 2020, IEEE T COGN DEV SYST, V12, P311, DOI 10.1109/TCDS.2019.2917711
   Liu Y, 2018, AEU-INT J ELECTRON C, V87, P134, DOI 10.1016/j.aeue.2018.02.019
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lo L, 2022, IEEE T MULTIMEDIA, V24, P4275, DOI 10.1109/TMM.2022.3197365
   Lucey P., 2010, 2010 IEEE COMP SOC C, P94, DOI DOI 10.1109/CVPRW.2010.5543262
   Luo Cheng, 2022, P 31 INT JOINT C ART, P1239, DOI [DOI 10.24963/IJCAI.2022/173, 10.24963/ijcai.2022/173]
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   RUSSELL JA, 1978, J PERS SOC PSYCHOL, V36, P1152, DOI 10.1037/0022-3514.36.10.1152
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Song H, 2023, IEEE T NEUR NET LEAR, V34, P8135, DOI 10.1109/TNNLS.2022.3152527
   Song TF, 2021, PROC CVPR IEEE, P6263, DOI 10.1109/CVPR46437.2021.00620
   Sun ZR, 2022, IEEE T MULTIMEDIA, V24, P1093, DOI 10.1109/TMM.2021.3116430
   Toisoul A, 2021, NAT MACH INTELL, V3, P42, DOI 10.1038/s42256-020-00280-0
   Tzirakis P, 2017, IEEE J-STSP, V11, P1301, DOI 10.1109/JSTSP.2017.2764438
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xie Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1255, DOI 10.1145/3394171.3413822
   Yoo B, 2018, IEEE SIGNAL PROC LET, V25, P808, DOI 10.1109/LSP.2018.2822241
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zeng JB, 2018, LECT NOTES COMPUT SC, V11217, P227, DOI 10.1007/978-3-030-01261-8_14
   Zhang FF, 2022, IEEE T MULTIMEDIA, V24, P1800, DOI 10.1109/TMM.2021.3072786
   Zhang J., 2021, P BRIT MACH VIS C, P1
   Zhang SW, 2021, IEEE WINT CONF APPL, P21, DOI 10.1109/WACV48630.2021.00007
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao R, 2023, IEEE T AFFECT COMPUT, V14, P1159, DOI 10.1109/TAFFC.2021.3088895
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
NR 61
TC 1
Z9 1
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2531
EP 2543
DI 10.1109/TMM.2023.3301209
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100032
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, YT
   Zhang, BC
   Hu, RZ
   Gu, K
   Zhai, GT
   Dong, JY
AF Liu, Yutao
   Zhang, Baochao
   Hu, Runze
   Gu, Ke
   Zhai, Guangtao
   Dong, Junyu
TI Underwater Image Quality Assessment: Benchmark Database and Objective
   Method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality; Databases; Imaging; Image color analysis; Transformers;
   Measurement; Degradation; Attention mechanism; image database; image
   quality assessment (IQA); transformer; underwater image
AB Underwater image quality assessment (UIQA) plays a crucial role in monitoring and detecting the quality of acquired underwater images in underwater imaging systems. Currently, the investigation of UIQA encounters two major challenges. First, a lack of large-scale UIQA databases for benchmarking UIQA algorithms remains, which greatly restricts the development of UIQA research. The other limitation is that there is a shortage of effective UIQA methods that can faithfully predict underwater image quality. To alleviate these two challenges, in this paper, we first construct a large-scale UIQA database (UIQD). Specifically, UIQD contains a total of 5369 authentic underwater images that span abundant underwater scenes and typical quality degradation conditions. Extensive subjective experiments are executed to annotate the perceived quality of the underwater images in UIQD. Based on an in-depth analysis of underwater image characteristics, we further establish a novel baseline UIQA metric that integrates channel and spatial attention mechanisms and a transformer. Channel- and spatial attention modules are used to capture the image channel and local quality degradations, while the transformer module characterizes the image quality from a global perspective. Multilayer perception is employed to fuse the local and global feature representations and yield the image quality score. Extensive experiments conducted on UIQD demonstrate that the proposed UIQA model achieves superior prediction performance compared with the state-of-the-art UIQA and IQA methods.
C1 [Liu, Yutao; Zhang, Baochao; Dong, Junyu] Ocean Univ China, Sch Comp Sci & Technol, Qingdao 266100, Peoples R China.
   [Hu, Runze] Beijing Inst Technol, Sch Informat & Elect, Beijing 100080, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
C3 Ocean University of China; Beijing Institute of Technology; Beijing
   University of Technology; Shanghai Jiao Tong University
RP Hu, RZ (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100080, Peoples R China.
EM liuyutao@ouc.edu.cn; zhangbaochao@stu.ouc.edu.cn; hrzlpk2015@gmail.com;
   guke@bjut.edu.cn; zhaiguangtao@gmail.com; dongjunyu@ouc.edu.cn
OI Dong, Junyu/0000-0001-7012-2087; Zhai, Guangtao/0000-0001-8165-9322
FU National Science Foundation of China
FX No Statement Available
CR B. Series, 2012, RECOMMENDATION ITU R, P13
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Clough JR, 2022, IEEE T PATTERN ANAL, V44, P8766, DOI 10.1109/TPAMI.2020.3013679
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Guo PF, 2022, IEEE T MULTIMEDIA, V24, P1980, DOI 10.1109/TMM.2021.3074825
   Hu RZ, 2023, IEEE T CYBERNETICS, V53, P3651, DOI 10.1109/TCYB.2021.3128023
   Huang T, 2022, Arxiv, DOI arXiv:2207.05557
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   kaggle, 2021, Kaggle starfish
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kumawat S, 2023, IEEE T PATTERN ANAL, V45, P4109, DOI 10.1109/TPAMI.2022.3196350
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Liu CW, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455997
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Liu YT, 2024, IEEE T MULTIMEDIA, V26, P2560, DOI 10.1109/TMM.2023.3301226
   Liu YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414837
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu YT, 2017, J VIS COMMUN IMAGE R, V46, P70, DOI 10.1016/j.jvcir.2017.03.007
   Lu HM, 2016, IEEE IMAGE PROC, P1998, DOI 10.1109/ICIP.2016.7532708
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Masana M, 2023, IEEE T PATTERN ANAL, V45, P5513, DOI 10.1109/TPAMI.2022.3213473
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Oh H, 2017, IEEE T IMAGE PROCESS, V26, P4923, DOI 10.1109/TIP.2017.2725584
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Pedersen M., 2019, P IEEE CVF C COMP VI, P18
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Rohaly A.M., 2000, ITU T STANDARDS CONT, P9
   Rongfu Lin, 2021, 2021 IEEE/CIC International Conference on Communications in China (ICCC Workshops), P205, DOI 10.1109/ICCCWorkshops52231.2021.9538855
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shnayderman A, 2006, IEEE T IMAGE PROCESS, V15, P422, DOI 10.1109/TIP.2005.860605
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tang SQ, 2020, 2020 13TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2020), P378, DOI 10.1109/CISP-BMEI51763.2020.9263539
   Thatipelli A, 2022, PROC CVPR IEEE, P19926, DOI 10.1109/CVPR52688.2022.01933
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2023, IEEE T CIRC SYST VID, V33, P1123, DOI 10.1109/TCSVT.2022.3212788
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu HB, 2021, MATER CORROS, V72, P720, DOI 10.1002/maco.202011886
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang N, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116218
   Zhai GT, 2021, IEEE T MULTIMEDIA, V23, P3700, DOI 10.1109/TMM.2020.3029891
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zheng YN, 2022, IEEE T IMAGE PROCESS, V31, P5456, DOI 10.1109/TIP.2022.3196815
   Zhou W, 2022, IEEE T CIRC SYST VID, V32, P1778, DOI 10.1109/TCSVT.2021.3081182
   Zhou WJ, 2018, IEEE T IMAGE PROCESS, V27, P2086, DOI 10.1109/TIP.2018.2794207
NR 57
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7734
EP 7747
DI 10.1109/TMM.2024.3371218
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000002
DA 2024-08-05
ER

PT J
AU Ruan, SL
   Zhang, K
   Wu, L
   Xu, T
   Liu, Q
   Chen, EH
AF Ruan, Shulan
   Zhang, Kun
   Wu, Le
   Xu, Tong
   Liu, Qi
   Chen, Enhong
TI Color Enhanced Cross Correlation Net for Image Sentiment Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Sentiment analysis; Feature extraction;
   Correlation; Visualization; Brain modeling; Analytical models; Image
   sentiment analysis; neural network; cross correlation; feature
   representation
ID PSYCHOLOGICAL PRIMARY COLORS; ANXIETY; FUSION
AB Automatic analysis of image sentiment has gained considerable attention with the increasing throughput of user-generated visual contents online. Recently, researchers generally tend to design different Convolutional Neural Networks (CNNs) to extract image content features for sentiment analysis. However, they underestimated the importance of image color, which has been proved very crucial for image sentiment expressing by psychology and art theory. Moreover, we further observe that the coordination of content and color is the main form of image sentiment expressing. Different combinations of content and color could express extremely different sentiments. To that end, in this paper, we propose a Color Enhanced Cross Correlation Net (CECCN), a novel architecture for image sentiment analysis that not only leverages contents and colors simultaneously, but also takes their correlations into consideration. Specifically, we first use a pre-trained CNN to extract content features and color moment to collect color features from multiple color spaces. Then, we propose a novel Cross Correlation (CC) method to model the correlations between content features and color features with attention mechanism and sequence convolution, in which sentiment expressing of content and color can be enhanced by each other. Finally, we integrate these two types of information for better image sentiment analysis. Extensive experiments on two popular and well-studied benchmark datasets demonstrate the superiority and rationality of our proposed CECCN.
C1 [Ruan, Shulan; Xu, Tong; Liu, Qi; Chen, Enhong] Univ Sci & Technol China, Sch Comp Sci & Technol, Anhui Prov Key Lab Big Data Anal & Applicat, Hefei 230026, Peoples R China.
   [Zhang, Kun; Wu, Le] Hefei Univ Technol, Sch Comp & Informat, Hefei 230029, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Hefei University of Technology
RP Chen, EH (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Anhui Prov Key Lab Big Data Anal & Applicat, Hefei 230026, Peoples R China.
EM slruan@mail.ustc.edu.cn; zhang1028kun@gmail.com; lewu.ustc@gmail.com;
   tongxu@ustc.edu.cn; qiliuql@ustc.edu.cn; cheneh@ustc.edu.cn
OI Chen, Enhong/0000-0002-4835-4102; Zhang, Kun/0000-0002-0743-9003; Ruan,
   Shulan/0000-0001-7466-8909
FU National Natural Science Foundation of China
FX No Statement Available
CR Bo Pang, 2008, Foundations and Trends in Information Retrieval, V2, P1, DOI 10.1561/1500000001
   Borth D., 2013, P 21 ACM INT C MULT, P223, DOI 10.1145/2502081.2502282
   Cai HY, 2018, IEEE T KNOWL DATA EN, V30, P1616, DOI 10.1109/TKDE.2018.2807452
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Cascianelli S, 2020, IEEE T MULTIMEDIA, V22, P271, DOI 10.1109/TMM.2019.2924598
   Corchs S, 2019, INT J MACH LEARN CYB, V10, P2057, DOI 10.1007/s13042-017-0734-0
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Du XY, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3357154
   Felicetti A, 2019, LECT NOTES COMPUT SC, V11751, P477, DOI 10.1007/978-3-030-30642-7_43
   Fortin MP, 2019, ICPRAM: PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P368, DOI 10.5220/0007313503680376
   Gong M, 2017, COMPUT VIS IMAGE UND, V162, P46, DOI 10.1016/j.cviu.2017.07.003
   GUILFORD JP, 1959, AM J PSYCHOL, V72, P487, DOI 10.2307/1419491
   Guo LT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P765, DOI 10.1145/3343031.3350943
   Hanbury A., 2010, P 18 ACM INT C MULTI, V18, P83, DOI DOI 10.1145/1873951.1873965
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Hyun D, 2018, ACM/SIGIR PROCEEDINGS 2018, P965, DOI 10.1145/3209978.3210111
   Indriani O. R., 2017, PROC INT C INNOV CRE, P1
   JACOBS KW, 1974, PERCEPT MOTOR SKILL, V38, P763, DOI 10.2466/pms.1974.38.3.763
   JACOBS KW, 1975, PERCEPT MOTOR SKILL, V41, P207, DOI 10.2466/pms.1975.41.1.207
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Katsurai M, 2016, INT CONF ACOUST SPEE, P2837, DOI 10.1109/ICASSP.2016.7472195
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1175, DOI 10.1145/3343031.3350993
   Liu PZ, 2017, INFORM SCIENCES, V390, P95, DOI 10.1016/j.ins.2017.01.025
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Nie F., 2016, IJCAI, P1881
   Ortis A, 2021, MULTIMED TOOLS APPL, V80, P22323, DOI 10.1007/s11042-019-08312-7
   Ortis A, 2020, IET IMAGE PROCESS, V14, P1440, DOI 10.1049/iet-ipr.2019.1270
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Peng KC, 2016, IEEE IMAGE PROC, P614, DOI 10.1109/ICIP.2016.7532430
   Peng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1202, DOI 10.1145/3343031.3350925
   PROFUSEK PJ, 1987, PERCEPT MOTOR SKILL, V65, P941, DOI 10.2466/pms.1987.65.3.941
   Ruan SL, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102855
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Siersdorfer S, 2010, P 18 INT C MULT FIR
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh S. M., 2012, Int. J. Comput. Appl., V58, P27
   Song KK, 2018, NEUROCOMPUTING, V312, P218, DOI 10.1016/j.neucom.2018.05.104
   Song YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P784, DOI 10.1145/3343031.3350996
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tay Y, 2018, AAAI CONF ARTIF INTE, P5956
   Vadicamo L, 2017, IEEE INT CONF COMP V, P308, DOI 10.1109/ICCVW.2017.45
   VALDEZ P, 1994, J EXP PSYCHOL GEN, V123, P394, DOI 10.1037/0096-3445.123.4.394
   Vaswani A, 2017, ADV NEUR IN, V30
   WILSON GD, 1966, PERCEPT MOTOR SKILL, V23, P947, DOI 10.2466/pms.1966.23.3.947
   Wu LF, 2020, NEURAL PROCESS LETT, V51, P2063, DOI 10.1007/s11063-019-10027-7
   Yadav A, 2020, MULTIMEDIA SYST, V26, P431, DOI 10.1007/s00530-020-00656-7
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zhang J, 2020, KNOWL-BASED SYST, V191, DOI 10.1016/j.knosys.2019.105245
   Zhang TY, 2018, IEEE ACCESS, V6, P10644, DOI 10.1109/ACCESS.2018.2806372
   Zhang W, 2020, IEEE T MULTIMEDIA, V22, P515, DOI 10.1109/TMM.2019.2928998
   Zhao SC, 2018, IEEE T CYBERNETICS, V48, P3218, DOI 10.1109/TCYB.2017.2762344
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zheng RL, 2020, LECT NOTES COMPUT SC, V11961, P303, DOI 10.1007/978-3-030-37731-1_25
   Zhi-Chun Huang, 2010, 2010 International Conference on Machine Learning and Cybernetics (ICMLC 2010), P719, DOI 10.1109/ICMLC.2010.5580566
   Zhu XL, 2019, LECT NOTES COMPUT SC, V11295, P264, DOI 10.1007/978-3-030-05710-7_22
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 63
TC 7
Z9 7
U1 26
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4097
EP 4109
DI 10.1109/TMM.2021.3118208
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100029
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Tang, S
   Shi, YJ
   Song, ZH
   Ye, M
   Zhang, CS
   Zhang, JW
AF Tang, Song
   Shi, Yuji
   Song, Zihao
   Ye, Mao
   Zhang, Changshui
   Zhang, Jianwei
TI Progressive Source-Aware Transformer for Generalized Source-Free Domain
   Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Adaptation models; Trajectory; Task analysis; Semantics;
   Self-supervised learning; Data models; Domain adaptation; mitigating
   forgetting; historical global attention; denoising; object cognition
AB Source-free domain adaptation (SFDA) tends to forget the source domain, suffering from limitations in real-world scenarios. Recently, generalized source-free domain adaptation (GSFDA) problem naturally emerges, aiming for good performance on both target and source domains. The existing methods attempt to retain model parameters associated with the source domain to prevent such forgetting. However, this strategy is not conducive to improving cross-domain performance on the target domain, prioritizing mitigating forgetting on the source domain. This article introduces a Progressive Source-Aware Transformer approach for GSFDA, dubbed PSAT-GDA. Our core idea is to enforce the domain adaptation process to remember the source domain by imposing source guidance, offering a target domain-centric anti-forgetting mechanism. Specifically, for each epoch, a Transformer-based deep network is adapted to do domain alignment like the traditional SFDA method, because the transformer working on the image patch sequence helps to reduce image noise caused by domain shift. Meanwhile, another Transformer is designed to generate source guidance supervising domain alignment. By augmenting target sample and mining the source information from the historical models before current epoch, source injected feature group is constructed. Based on the Transformer mechanism, the attention block can select useful source information for each target sample. From it, we devise neighbour-based and augmentation-based regularizations to shape the source guidance. Experiments on three challenging datasets show that our method can achieve evident cross-domain improvement on the target domains. Also, it can mitigate forgetting on all domains after adapting to single or multiple target domains.
C1 [Tang, Song] Univ Shanghai Sci & Technol, Inst Machine Intelligence IMI, Shanghai 200093, Peoples R China.
   [Tang, Song] Univ Hamburg, Dept Informat, Tech Aspects Multimodal Syst TAMS Grp, D-21073 Hamburg, Germany.
   [Shi, Yuji; Song, Zihao] Univ Shanghai Sci & Technol, IMI, Shanghai 200093, Peoples R China.
   [Ye, Mao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Peoples R China.
   [Zhang, Changshui] Tsinghua Univ THUAI, Inst Artificial Intelligence, Beijing 100190, Peoples R China.
   [Zhang, Changshui] Beijing Natl Res Ctr Informat Sci & Technol BNRist, State Key Lab Intelligent Technol & Syst, Beijing 100190, Peoples R China.
   [Zhang, Changshui] Tsinghua Univ, Dept Automat, Beijing 100190, Peoples R China.
   [Zhang, Jianwei] Univ Hamburg, Dept Informat, TAMS Grp, D-21073 Hamburg, Germany.
C3 University of Shanghai for Science & Technology; University of Hamburg;
   University of Shanghai for Science & Technology; University of
   Electronic Science & Technology of China; Tsinghua University;
   University of Hamburg
RP Ye, M (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Peoples R China.; Zhang, JW (corresponding author), Univ Hamburg, Dept Informat, TAMS Grp, D-21073 Hamburg, Germany.
EM steventangsong@gmail.com; shiyj@st.usst.edu.cn; songzh711@gmail.com;
   cvlab.uestc@gmail.com; zcs@mail.tsinghua.edu.cn; tntechlab@hotmail.com
OI TANG, Song/0000-0003-2635-1872
FU German Research Foundation
FX No Statement Available
CR Aljundi R, 2019, PROC CVPR IEEE, P11246, DOI 10.1109/CVPR.2019.01151
   Arulkumaran K, 2017, IEEE SIGNAL PROC MAG, V34, P26, DOI 10.1109/MSP.2017.2743240
   Bobu Andreea, 2018, Adapting to continuously shifting domains
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen XY, 2019, PR MACH LEARN RES, V97
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Cui X., 2019, PROC INT C RECENT AD, P213
   Das D, 2018, IEEE IMAGE PROC, P3758, DOI 10.1109/ICIP.2018.8451152
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258
   Guo X., 2018, P ACML, P550
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu W., 2017, P 34 INT C MACHINE L, P1558
   Jiang X, 2020, PR MACH LEARN RES, V119
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li Rui, 2020, P IEEE CVF C COMP VI, P9641
   Liang J., 2020, International Conference on Machine Learning, P6028
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Long M., 2019, PROC NEURIPS, P1951
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Mancini M, 2019, PROC CVPR IEEE, P6561, DOI 10.1109/CVPR.2019.00673
   Müller R, 2019, ADV NEUR IN, V32
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Tang SX, 2021, AAAI CONF ARTIF INTE, V35, P2665
   Tang S, 2021, IEEE INT C INT ROBOT, P5679, DOI 10.1109/IROS51168.2021.9636206
   Tang S, 2019, NEURAL COMPUT APPL, V31, P1189, DOI 10.1007/s00521-017-3152-z
   Tanwisuth K, 2021, ADV NEUR IN, V34
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang F, 2022, PROC CVPR IEEE, P7141, DOI 10.1109/CVPR52688.2022.00701
   Xia HF, 2023, IEEE T PATTERN ANAL, V45, P3434, DOI 10.1109/TPAMI.2022.3174526
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Xu T., 2022, PROC INT C LEARN REP, DOI [10.48550/arXiv.2109.06165, DOI 10.48550/ARXIV.2109.06165]
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang G., 2022, APPL INTELL, V53, P16560
   Yang JY, 2023, IEEE WINT CONF APPL, P520, DOI 10.1109/WACV56688.2023.00059
   Yang SQ, 2023, COMPUT VIS IMAGE UND, V234, DOI 10.1016/j.cviu.2023.103747
   Yang SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8958, DOI 10.1109/ICCV48922.2021.00885
   Ying Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P464, DOI 10.1007/978-3-030-58589-1_28
   Youngeun Kim, 2021, IEEE Transactions on Artificial Intelligence, V2, P508, DOI 10.1109/TAI.2021.3110179
   Yuan Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P540, DOI 10.1007/978-3-030-58526-6_32
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhao JW, 2022, INT CONF ACOUST SPEE, P4673, DOI 10.1109/ICASSP43922.2022.9747832
NR 50
TC 0
Z9 0
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4138
EP 4152
DI 10.1109/TMM.2023.3321421
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100048
DA 2024-08-05
ER

PT J
AU Tang, YP
   Wang, WN
   Zhang, CJ
   Liu, J
   Zhao, Y
AF Tang, Yepeng
   Wang, Weining
   Zhang, Chunjie
   Liu, Jing
   Zhao, Yao
TI Temporal Action Proposal Generation With Action Frequency Adaptive
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Task analysis; Data models; Time-frequency analysis;
   Representation learning; Predictive models; Information science;
   Temporal action proposal generation; expert learning; fine-gained
   detection; action frequency
ID IMBALANCED DATA; SMOTE
AB As the cornerstone of human-behavior analysis in video understanding, temporal action proposal generation aims to predict the starting and ending time of human action instances in untrimmed videos. Although large achievements in temporal action proposal generation have been achieved, most previous studies ignore the variability of action frequency in raw videos, leading to unsatisfying performances on high-action-frequency videos. In fact, there exists two main issues which should be well addressed: data imbalance between high and low action-frequency videos, and inferior detection of short actions in high-action-frequency videos. To address the above issues, we propose an effective framework by adapting to the variability of action frequency, namely Action Frequency Adaptive Network (AFAN), which can be flexibly built upon any temporal action proposal generation method. AFAN consists of two modules: Learning From Experts (LFE) and Fine-Grained Processing (FGP). The LFE first trains a series of action proposal generators on different subsets of imbalanced data as experts and then teaches a unified student model via knowledge distillation. To better detect short actions, FGP first finds out high-action-frequency videos and then performs fine-grained detection. Extensive experimental results on four benchmark datasets (ActivityNet-1.3, HACS, THUMOS14 and FineAction) demonstrate the effectiveness and generalizability of the proposed AFAN, especially for high-action-frequency videos.
C1 [Tang, Yepeng; Zhang, Chunjie] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Tang, Yepeng; Zhang, Chunjie] Anhui Univ, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230093, Peoples R China.
   [Tang, Yepeng; Zhang, Chunjie] Beijing Jiaotong Univ, Inst Informat Sci, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Wang, Weining; Liu, Jing] Chinese Acad Sci, Inst Automat, Lab Cognit & Decis Intelligence Complex Syst, Beijing 100190, Peoples R China.
   [Liu, Jing] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Zhao, Yao] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; Anhui University; Beijing Jiaotong
   University; Chinese Academy of Sciences; Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Beijing Jiaotong University; Beijing Jiaotong University
RP Zhang, CJ (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
EM yepengtang@bjtu.edu.cn; weining.wang@nlpr.ia.ac.cn; cjzhang@bjtu.edu.cn;
   jliu@nlpr.ia.ac.cn; yzhao@bjtu.edu.cn
OI zhang, chunjie/0000-0002-1161-8995; Tang, Yepeng/0000-0002-4314-7550;
   Zhao, Yao/0000-0002-8581-9554; liu, jing/0000-0003-0903-9131; Wang,
   Weining/0000-0001-7299-6431
FU Institute of Automation, Chinese Academy of Sciences
FX No Statement Available
CR Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Buch S., 2017, BMVC
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang Shuning, 2022, HCMA '22: Proceedings of the 3rd International Workshop on Human-Centric Multimedia Analysis, P41, DOI 10.1145/3552458.3556443
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen G, 2022, AAAI CONF ARTIF INTE, P248
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Cui JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P695, DOI 10.1109/ICCV48922.2021.00075
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Dong Q, 2017, IEEE I CONF COMP VIS, P1869, DOI 10.1109/ICCV.2017.205
   Drumnond C., 2003, P ICML KDD WORKSH LE, P1
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eun H, 2020, IEEE T CIRC SYST VID, V30, P4232, DOI 10.1109/TCSVT.2019.2953187
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feng YA, 2019, IEEE T MULTIMEDIA, V21, P1762, DOI 10.1109/TMM.2018.2885237
   Gao J., 2017, BMVC, P1
   Gao JL, 2020, AAAI CONF ARTIF INTE, V34, P10810
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Ghanem B, 2017, Arxiv, DOI arXiv:1710.08011
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   Hilman D, 2022, KNOWL-BASED SYST, V241, DOI 10.1016/j.knosys.2022.108228
   Huang C, 2020, IEEE T PATTERN ANAL, V42, P2781, DOI 10.1109/TPAMI.2019.2914680
   Jeatrakul P, 2010, LECT NOTES COMPUT SC, V6444, P152, DOI 10.1007/978-3-642-17534-3_19
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kahatapitiya K, 2021, PROC CVPR IEEE, P8381, DOI 10.1109/CVPR46437.2021.00828
   Kang B., 2020, 8 INT C LEARN REPR I
   Khan SH, 2018, IEEE T NEUR NET LEAR, V29, P3573, DOI 10.1109/TNNLS.2017.2732482
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu S., 2020, Proceedings of the Asian Conference on Computer Vision, V12626, P530
   Liu XL, 2022, IEEE T IMAGE PROCESS, V31, P5427, DOI 10.1109/TIP.2022.3195321
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P6937, DOI 10.1109/TIP.2022.3217368
   Liu Y, 2019, PROC CVPR IEEE, P3599, DOI [10.1109/CVPR.2019.00372, 10.1109/CVPR.2019.00726]
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Liuyu Xiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P247, DOI 10.1007/978-3-030-58558-7_15
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   mindspore, ABOUT US
   Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Simonyan K, 2014, ADV NEUR IN, V27
   Su HS, 2021, AAAI CONF ARTIF INTE, V35, P2602
   Tahir MA, 2012, PATTERN RECOGN, V45, P3738, DOI 10.1016/j.patcog.2012.03.014
   Tan J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13506, DOI 10.1109/ICCV48922.2021.01327
   Tao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P728, DOI 10.1007/978-3-030-58568-6_43
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang L., 2021, Temporal action proposal generation with transformers
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang T, 2021, PROC CVPR IEEE, P3102, DOI 10.1109/CVPR46437.2021.00312
   Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37
   Wang Yu-Xiong, 2017, ADV NEURAL INFORM PR, V30, P7032, DOI DOI 10.5555/3295222.3295446
   Wang ZW, 2021, PROC CVPR IEEE, P13209, DOI 10.1109/CVPR46437.2021.01301
   Wu HB, 2020, IEEE T MULTIMEDIA, V22, P2293, DOI 10.1109/TMM.2019.2953814
   Xinting Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14042, DOI 10.1109/CVPR42600.2020.01406
   Xu M., 2020, P IEEE CVF C COMP VI, P10156
   Yu WP, 2021, IEEE WINT CONF APPL, P3257, DOI 10.1109/WACV48630.2021.00330
   Zeng RH, 2022, IEEE T PATTERN ANAL, V44, P6209, DOI 10.1109/TPAMI.2021.3090167
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhang YF, 2023, IEEE T PATTERN ANAL, V45, P10795, DOI 10.1109/TPAMI.2023.3268118
   Zhao H, 2019, IEEE I CONF COMP VIS, P8667, DOI 10.1109/ICCV.2019.00876
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhong ZS, 2021, PROC CVPR IEEE, P16484, DOI 10.1109/CVPR46437.2021.01622
NR 67
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2340
EP 2353
DI 10.1109/TMM.2023.3295090
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100010
DA 2024-08-05
ER

PT J
AU Wang, JG
   Qian, SS
   Hu, J
   Hong, RC
AF Wang, Jinguang
   Qian, Shengsheng
   Hu, Jun
   Hong, Richang
TI Positive Unlabeled Fake News Detection via Multi-Modal Masked
   Transformer Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fake news detection; multi-modal learning; social media
AB Fake news detection has gotten continuous attention during these years with more and more people have been posting and reading news online. To enable fake news detection, existing researchers usually assume labeled posts are provided for two classes (true or false) so that the model can learn a discriminative classifier from the labeled data. However, this supposition may not hold true in reality, as most users may only label a small number of posts in a single category that they are interested in. Furthermore, most existing methods fail to mask the noise or irrelevant context (i.e., regions or words) between different modalities to assist in strengthening the correlations between relevant contexts. To tackle these issues, we present a curriculum-based multi-modal masked transformer network (CMMTN) for positive unlabeled multi-modal fake news detection by jointly modeling the inter-modality and intra-modality relationships of multi-modal information and masking the irrelevant context between modalities. In particular, we adopt BERT and ResNet to obtain better representations for texts and images, separately. Then, the extracted features of images and texts are fed into a multi-modal masked transformer network to fuse the multi-modal content and mask the irrelevant context between modalities by calculating the similarity between inter-modal contexts. Finally, we design a curriculum-based PU learning method to handle the positive and unlabeled data. Massive experiments on three public real datasets prove the effectiveness of the CMMTN.
C1 [Wang, Jinguang; Hong, Richang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
   [Wang, Jinguang; Hong, Richang] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230009, Peoples R China.
   [Qian, Shengsheng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100049, Peoples R China.
   [Hu, Jun] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 Hefei University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; National University of Singapore
RP Hong, RC (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.; Hong, RC (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230009, Peoples R China.
EM wangjinguang502@gmail.com; shengsheng.qian@nlpr.ia.ac.cn;
   jun.hu@nus.edu.sg; hongrc.hfut@gmail.com
OI Wang, Jinguang/0000-0003-1938-4692
FU National Key Research and Development Program of China
FX No Statement Available
CR Bing L, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P179, DOI 10.1109/icdm.2003.1250918
   Boididou C., 2015, MediaEval
   Castillo C., 2011, Proceedings of the 20th international conference on World wide web, P675, DOI [DOI 10.1145/1963405.1963500, 10.1145/1963405.1963500]
   Chen Y, 2015, CONVOLUTIONAL NEURAL
   Cui LM, 2020, Arxiv, DOI arXiv:2006.00885
   de Souza MC, 2022, MACH LEARN, V111, P3549, DOI 10.1007/s10994-021-06111-6
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   du Plessis MC, 2014, ADV NEUR IN, V27
   du Plessis MC, 2015, PR MACH LEARN RES, V37, P1386
   Giachanou A, 2021, J ASSOC INF SCI TECH, V72, P1117, DOI 10.1002/asi.24480
   Giachanou A, 2020, PR INT CONF DATA SC, P647, DOI 10.1109/DSAA49011.2020.00091
   Gupta A, 2014, LECT NOTES COMPUT SC, V8851, P228, DOI 10.1007/978-3-319-13734-6_16
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Fusilier DH, 2015, INFORM PROCESS MANAG, V51, P433, DOI 10.1016/j.ipm.2014.11.001
   Jain S., 2016, Advances in Neural Information Processing Systems, P2693, DOI DOI 10.1101/GR.160325.113
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Kai Shu, 2017, ACM SIGKDD Explorations Newsletter, V19, P22, DOI 10.1145/3137597.3137600
   Kalamaras I, 2014, IEEE T MULTIMEDIA, V16, P1460, DOI 10.1109/TMM.2014.2316473
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kingma D.P., 2014, Proc. of ICLR
   Kiryo R, 2017, ADV NEUR IN, V30
   Kocmi T., 2017, P RANLP, P379
   Kwon S, 2013, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2013.61
   Li X., 2003, IJCAI, P587
   Liu Xiaomo, 2015, P 24 ACM INT C INF K, P1867, DOI [DOI 10.1145/2806416.2806651, 10.1145/2806416.2806651]
   Liu Y, 2020, ACM T INFORM SYST, V38, DOI 10.1145/3386253
   Ma J., 2016, 25 INT JOINT C ART I
   Ma J, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3049, DOI 10.1145/3308558.3313741
   Ma Jing, 2015, P 24 ACM INT C INF K, P1751
   Paszke A., 2017, P INT C ADV NEUR INF
   Platanios EA, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1162
   Qian SS, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P153, DOI 10.1145/3404835.3462871
   Ruffo G, 2023, COMPUT SCI REV, V47, DOI 10.1016/j.cosrev.2022.100531
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schölkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Singhal S, 2020, AAAI CONF ARTIF INTE, V34, P13915
   Singhal S, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P39, DOI [10.1109/BigMM.2019.00018, 10.1109/BigMM.2019.00-44]
   Souza M. C. d., 2021, BRACIS, P3
   Vaswani A, 2017, ADV NEUR IN, V30
   Vosoughi S, 2018, SCIENCE, V359, P1146, DOI 10.1126/science.aap9559
   Wang JG, 2020, NEUROCOMPUTING, V412, P42, DOI 10.1016/j.neucom.2020.04.145
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Wu M., 2021, ACM Transactions on Knowledge Discovery from Data (TKDD), V15, P1
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xu M., 2019, arXiv
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Ying L, 2021, IEEE ACCESS, V9, P132363, DOI 10.1109/ACCESS.2021.3114093
   Yu F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3901
   Zhang GB, 2024, J INF SCI, V50, P355, DOI 10.1177/01655515221087683
   Zhang HW, 2022, IEEE T MULTIMEDIA, V24, P1449, DOI 10.1109/TMM.2021.3065498
   Zhang JQ, 2019, IEEE T MULTIMEDIA, V21, P1332, DOI 10.1109/TMM.2018.2871421
   Zhou XY, 2020, LECT NOTES ARTIF INT, V12085, P354, DOI 10.1007/978-3-030-47436-2_27
   Zubiaga A., 2017, SOCIAL INFORM 1, P109
NR 56
TC 4
Z9 4
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 234
EP 244
DI 10.1109/TMM.2023.3263552
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500016
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Xing, YH
   Wu, QR
   Cheng, D
   Zhang, SZ
   Liang, GQ
   Wang, P
   Zhang, YN
AF Xing, Yinghui
   Wu, Qirui
   Cheng, De
   Zhang, Shizhou
   Liang, Guoqiang
   Wang, Peng
   Zhang, Yanning
TI Dual Modality Prompt Tuning for Vision-Language Pre-Trained Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Tuning; Task analysis; Adaptation models; Computational
   modeling; Feature extraction; Training; Few-shot learning; transfer
   learning; image classification; prompt tuning; vision-language model
AB With the emergence of large pretrained vison-language models such as CLIP, transferable representations can be adapted to a wide range of downstream tasks via prompt tuning. Prompt tuning probes for beneficial information for downstream tasks from the general knowledge stored in the pretrained model. A recently proposed method named Context Optimization (CoOp) introduces a set of learnable vectors as text prompts from the language side. However, tuning the text prompt alone can only adjust the synthesized "classifier", while the computed visual features of the image encoder cannot be affected, thus leading to suboptimal solutions. In this article, we propose a novel dual-modality prompt tuning (DPT) paradigm through learning text and visual prompts simultaneously. To make the final image feature concentrate more on the target visual concept, a class-aware visual prompt tuning (CAVPT) scheme is further proposed in our DPT. In this scheme, the class-aware visual prompt is generated dynamically by performing the cross attention between text prompt features and image patch token embeddings to encode both the downstream task-related information and visual instance information. Extensive experimental results on 11 datasets demonstrate the effectiveness and generalization ability of the proposed method.
C1 [Xing, Yinghui; Wu, Qirui; Zhang, Shizhou; Liang, Guoqiang; Wang, Peng; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Xing, Yinghui] Northwestern Polytech Univ Shenzhen, Res Dev Inst, Shenzhen 518057, Peoples R China.
   [Cheng, De] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Xidian University
RP Cheng, D (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
EM xyh_7491@163.com; wuqirui@mail.nwpu.edu.cn; dcheng@xidian.edu.cn;
   szzhang@nwpu.edu.cn; gqliang@nwpu.edu.cn; peng.wang@nwpu.edu.cn;
   ynzhang@nwpu.edu.cn
RI Wu, Qirui/KYQ-9921-2024; Liang, Guoqiang/JUF-0287-2023; 南欧,
   以南/KDM-8983-2024; Xing, Yinghui/HOA-8917-2023
OI Wu, Qirui/0009-0001-0763-5274; Liang, Guoqiang/0000-0002-8710-5520;
   Xing, Yinghui/0000-0001-6021-8261
FU National Natural Science Foundation of China
FX No Statement Available
CR Bahng H, 2022, Arxiv, DOI arXiv:2203.17274
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Cai H., 2020, Advances in Neural Information Processing Systems, P11285
   Cheng H, 2023, IEEE T MULTIMEDIA, V25, P8225, DOI 10.1109/TMM.2022.3233442
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Desai K, 2021, PROC CVPR IEEE, P11157, DOI 10.1109/CVPR46437.2021.01101
   Dosovitskiy A., 2021, PROC ICLR
   Fang P., 2022, IEEE Trans. Multimedia, earlyaccess, DOI [10.1109/TMM.2022.3227416.[17]J., DOI 10.1109/TMM.2022.3227416.[17]J]
   Gao P., 2021, CLIP ADAPTER BETTER
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Grill JB, 2020, P 34 INT C NEUR INF, V33, P21271
   Guo C., 2022, IEEE Trans. Multimedia,early access, DOI [10.1109/TMM.2022.3168146.[43]H., DOI 10.1109/TMM.2022.3168146.[43]H]
   He H., 2020, P IEEE CVF C COMP VI, P9729
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Helber P, 2019, IEEE J-STARS, V12, P2217, DOI 10.1109/JSTARS.2019.2918242
   Hendrycks D, 2021, PROC CVPR IEEE, P15257, DOI 10.1109/CVPR46437.2021.01501
   Hendrycks Dan, 2020, ARXIV200616241
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jia C, 2021, PR MACH LEARN RES, V139
   Jia ML, 2022, LECT NOTES COMPUT SC, V13693, P709, DOI 10.1007/978-3-031-19827-4_41
   Jiang ZB, 2020, T ASSOC COMPUT LING, V8, P423, DOI 10.1162/tacl_a_00324
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Jinyu Yang, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P3492, DOI 10.1145/3503161.3547851
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lester B, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3045
   Li DX, 2022, PROC CVPR IEEE, P4943, DOI 10.1109/CVPR52688.2022.00490
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li Y., 2021, P INT C LEARN REPR, P1
   Liu PF, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3560815
   Lu YN, 2022, PROC CVPR IEEE, P5196, DOI 10.1109/CVPR52688.2022.00514
   Lu YW, 2021, IEEE T MULTIMEDIA, V23, P2056, DOI 10.1109/TMM.2020.3007340
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Mokady R., 2021, Clipcap: Clip prefix for image captioning
   Ni B, 2022, LECT NOTES COMPUT SC, V13664, P1, DOI 10.1007/978-3-031-19772-7_1
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Radford A, 2021, PR MACH LEARN RES, V139
   Rao YM, 2022, PROC CVPR IEEE, P18061, DOI 10.1109/CVPR52688.2022.01755
   Rebuffi SA, 2017, ADV NEUR IN, V30
   Recht B, 2019, PR MACH LEARN RES, V97
   Sahoo A, 2022, Frustratingly simple contrastive prompt tuning for vision-language models, P1
   Shin T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4222
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tian H., 2022, IEEE Trans. Multimedia, early access, DOI [10.1109/TMM.2022.3215310.[45]X, DOI 10.1109/TMM.2022.3215310.[45]X]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HH, 2019, ADV NEUR IN, V32
   Wang ZF, 2022, PROC CVPR IEEE, P139, DOI 10.1109/CVPR52688.2022.00024
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yang H, 2022, Prompt tuning for generative multimodal pretrained models
   Yao Y, 2021, CPT: Colorful prompt tuning for pre-trained vision-language models
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Zhang HG, 2023, IEEE T MULTIMEDIA, V25, P2111, DOI 10.1109/TMM.2022.3142955
   Zhang Jeffrey O., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P698, DOI 10.1007/978-3-030-58580-8_41
   Zhang L, 2023, IEEE T MULTIMEDIA, V25, P3319, DOI 10.1109/TMM.2022.3158072
   Zhang RR, 2022, LECT NOTES COMPUT SC, V13695, P493, DOI 10.1007/978-3-031-19833-5_29
   Zhang RR, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6868, DOI 10.1145/3503161.3549201
   Zhang RR, 2022, PROC CVPR IEEE, P8542, DOI 10.1109/CVPR52688.2022.00836
   Zhang YH, 2022, Arxiv, DOI arXiv:2206.04673
   Zhang Yuhao, 2020, P 7 MACH LEARN HEALT, V182, P2
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhou KY, 2022, PROC CVPR IEEE, P16795, DOI 10.1109/CVPR52688.2022.01631
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
   Zhu BE, 2024, Arxiv, DOI [arXiv:2205.14865, 10.48550/arXiv.2205.14865]
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
NR 66
TC 4
Z9 4
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2056
EP 2068
DI 10.1109/TMM.2023.3291588
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800032
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, HY
   Liu, GX
   Zhang, Y
   Hao, ZH
AF Zhang, Haoyang
   Liu, Guixi
   Zhang, Yi
   Hao, Zhaohui
TI Robust Multi-Model Visual Tracking With Distractor-Aware
   Template-Coupled Correlation Filters Joint Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distractor-aware; multi-template; correlation filter; joint learning;
   multi-model visual tracking
ID OBJECT TRACKING; LONG-TERM
AB Existing correlation filter (CF) tracking methods are fragile for boundary effects, vague target information, and heuristic model updating, as these limitations degrade the detection ability of the learned filter. In response to that, this article embarks on basic CF learning and presents a novel distractor-aware template-coupled correlation filter (DATC-CF) by exploiting the spatial-temporal appearance context of the target, which aims at improving the discriminative ability of the learned filter against distractive background and the descriptive ability in adapting unexpected scenes. Specifically, the power of spatial context comes from a distractor-aware regularizer weighted by background distractors. By adaptively optimizing the weight of each distractor, our filter training can focus more on the critical distractors. The temporal context is represented by a dynamic template set, and we formulate a template-coupled regularizer that can make use of the commonality over all templates while maintaining a passive filter update under a multi-template learning scheme. DATC-CF integrates the two regularizers and is summarized as a multi-variable joint optimization problem where a filter ensemble can be learned. With DATC-CF, a multi-model tracking framework DATC_MM is developed by maximizing the posterior distribution over the learned filters. For robust tracking, we further apply high-confidence updating and establish a complementary distractor-aware color detector to restore the CF tracking failures. Finally, experiments on several large-scale benchmark datasets demonstrate the effectiveness of the proposed tracking methods against state-of-the-art trackers.
C1 [Zhang, Haoyang; Liu, Guixi; Zhang, Yi; Hao, Zhaohui] Xidian Univ, Sch Mechanoelect Engn, Xian 710071, Peoples R China.
C3 Xidian University
RP Liu, GX (corresponding author), Xidian Univ, Sch Mechanoelect Engn, Xian 710071, Peoples R China.
EM zhanghy_xd@163.com; gxliu@xidian.edu.cn; yizzhang@stu.xidian.edu.cn;
   haozhaohui1981@163.com
RI Zhang, Haoyang/ABF-4117-2022
OI Zhang, Haoyang/0000-0003-3392-320X; Zhang, Yi/0000-0003-3986-8272
FU National Natural Science Foundation of China
FX No Statement Available
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bibby C, 2008, LECT NOTES COMPUT SC, V5303, P831, DOI 10.1007/978-3-540-88688-4_61
   Bibi A, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P613, DOI 10.1109/ICCVW.2015.83
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chen GC, 2020, IEEE T CIRC SYST VID, V30, P4810, DOI 10.1109/TCSVT.2019.2961999
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   Guan MY, 2021, IEEE T MULTIMEDIA, V23, P3841, DOI 10.1109/TMM.2020.3032043
   Guo Q, 2020, IEEE T IMAGE PROCESS, V29, P2999, DOI 10.1109/TIP.2019.2955292
   Han RZ, 2018, IEEE INT CON MULTI
   Han RZ, 2020, IEEE T IMAGE PROCESS, V29, P7128, DOI 10.1109/TIP.2020.2998978
   Han YM, 2020, IEEE T MULTIMEDIA, V22, P2698, DOI 10.1109/TMM.2019.2958759
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P2162, DOI 10.1109/TMM.2020.3008035
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liao JW, 2021, IEEE T MULTIMEDIA, V23, P3346, DOI 10.1109/TMM.2020.3023794
   Lu XH, 2019, NEUROCOMPUTING, V348, P134, DOI 10.1016/j.neucom.2018.06.090
   Lukezic A, 2018, IEEE T CYBERNETICS, V48, P1849, DOI 10.1109/TCYB.2017.2716101
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/INTMAG.2018.8508195, 10.1109/TPAMI.2018.2865311]
   Ma C, 2018, INT J COMPUT VISION, V126, P771, DOI 10.1007/s11263-018-1076-4
   Marvasti-Zadeh SM, 2022, IEEE T INTELL TRANSP, V23, P3943, DOI 10.1109/TITS.2020.3046478
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Sui Y, 2018, IEEE T IMAGE PROCESS, V27, P1282, DOI 10.1109/TIP.2017.2779275
   Sun YX, 2019, PROC CVPR IEEE, P5776, DOI 10.1109/CVPR.2019.00593
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang Q., 2017, ARXIV PREPRINT ARXIV
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiao JJ, 2016, LECT NOTES COMPUT SC, V9908, P121, DOI 10.1007/978-3-319-46493-0_8
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Yang YH, 2017, IEEE T CYBERNETICS, V47, P485, DOI 10.1109/TCYB.2016.2519532
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang L, 2017, PATTERN RECOGN, V69, P82, DOI 10.1016/j.patcog.2017.04.004
   Zhang P, 2019, IEEE T CIRC SYST VID, V29, P3673, DOI 10.1109/TCSVT.2018.2882339
   Zhang YP, 2021, IEEE T MULTIMEDIA, V23, P4232, DOI 10.1109/TMM.2020.3038310
   Zheng YH, 2020, IEEE T NEUR NET LEAR, V31, P2336, DOI 10.1109/TNNLS.2019.2929407
   Zhou ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9846, DOI 10.1109/ICCV48922.2021.00972
   Zhu H, 2022, IEEE T MULTIMEDIA, V24, P2098, DOI 10.1109/TMM.2021.3075876
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 71
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1813
EP 1828
DI 10.1109/TMM.2023.3289700
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800035
DA 2024-08-05
ER

PT J
AU Zhao, WH
   Li, Q
   Xu, HF
   Gao, QX
   Wang, QQ
   Gao, XB
AF Zhao, Wenhui
   Li, Qin
   Xu, Huafu
   Gao, Quanxue
   Wang, Qianqian
   Gao, Xinbo
TI Anchor Graph-Based Feature Selection for One-Step Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-view clustering; feature selection; sparse representation
ID RANK
AB Recently, multi-view clustering methods have been widely used in handling multi-media data and have achieved impressive performances. Among the many multi-view clustering methods, anchor graph-based multi-view clustering has been proven to be highly efficient for large-scale data processing. However, most existing anchor graph-based clustering methods necessitate post-processing to obtain clustering labels and are unable to effectively utilize the information within anchor graphs. To address this issue, we draw inspiration from regression and feature selection to propose Anchor Graph-Based Feature Selection for One-Step Multi-View Clustering (AGFS-OMVC). Our method combines embedding learning and sparse constraint to perform feature selection, allowing us to remove noisy anchor points and redundant connections in the anchor graph. This results in a clean anchor graph that can be projected into the label space, enabling us to obtain clustering labels in a single step without post-processing. Lastly, we employ the tensor Schatten $p$-norm as a tensor rank approximation function to capture the complementary information between different views, ensuring similarity between cluster assignment matrices. Experimental results on five real-world datasets demonstrate that our proposed method outperforms state-of-the-art approaches.
C1 [Zhao, Wenhui; Gao, Quanxue; Wang, Qianqian] Xidian Univ, Sch Telecommun Engn, Xian, 710071, Peoples R China.
   [Li, Qin] Shenzhen Inst Informat Technol, Sch Software Engn, Shenzhen 518172, Peoples R China.
   [Xu, Huafu] Informat Ctr Guangxi Zhuang Autonomous Reg, Guangxi Key Lab Digital Infrastruct, Nanning 530000, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; Shenzhen Institute of Information Technology;
   Chongqing University of Posts & Telecommunications
RP Gao, QX (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian, 710071, Peoples R China.; Li, Q (corresponding author), Shenzhen Inst Informat Technol, Sch Software Engn, Shenzhen 518172, Peoples R China.
EM whzhao@stu.xidian.edu.cn; liqin@sziit.edu.cn; 836356065@qq.com;
   qxgao@xidian.edu.cn; qqwang@xidian.edu.cn; xd_gxb_pr@163.com
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2011, INT C NEURAL INF PRO
   APTE C, 1994, ACM T INFORM SYST, V12, P233, DOI 10.1145/183422.183423
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Cai X, 2013, IEEE I CONF COMP VIS, P1737, DOI 10.1109/ICCV.2013.218
   Chen Z, 2023, IEEE T IMAGE PROCESS, V32, P6514, DOI 10.1109/TIP.2023.3261746
   Chen Z, 2023, IEEE T NEUR NET LEAR, V34, P10225, DOI 10.1109/TNNLS.2022.3165217
   Chen Z, 2022, IEEE T NEUR NET LEAR, V33, P3645, DOI 10.1109/TNNLS.2021.3053941
   Deng L., 2012, IEEE Signal Process. Mag., V29, P141
   Dua D., 2017, UCI Machine Learning Repository
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Gao QX, 2021, IEEE T PATTERN ANAL, V43, P2133, DOI 10.1109/TPAMI.2020.3017672
   Hu ZX, 2020, INFORM FUSION, V55, P251, DOI 10.1016/j.inffus.2019.09.005
   Huang J, 2013, ACM T KNOWL DISCOV D, V7, DOI 10.1145/2541268.2541270
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Kumar A, 2011, P 28 INT C MACH LEAR, P393
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Li XL, 2017, IEEE T CYBERNETICS, V47, P3840, DOI 10.1109/TCYB.2016.2585355
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Liao SL, 2018, AAAI CONF ARTIF INTE, P3604
   Liu H, 2008, CH CRC DATA MIN KNOW, P3
   Liu J., 2013, PROC 13 SIAM INT C, P252, DOI DOI 10.1137/1.9781611972832.28
   Luo SR, 2018, AAAI CONF ARTIF INTE, P3730
   Ming D, 2019, AAAI CONF ARTIF INTE, P4586
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Nie H., 2010, NIPS, P1813
   Shu XC, 2023, IEEE T MULTIMEDIA, V25, P5485, DOI 10.1109/TMM.2022.3193855
   Signoretto M., 2010, Linear Algebra Appl., V43, P1
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Wang HB, 2021, IEEE T MULTIMEDIA, V23, P3828, DOI 10.1109/TMM.2020.3032023
   Wang Q, 2022, IEEE T CYBERNETICS, V52, P10228, DOI 10.1109/TCYB.2021.3067137
   Wang QQ, 2021, IEEE T MULTIMEDIA, V23, P3483, DOI 10.1109/TMM.2020.3025666
   Wang QQ, 2018, IEEE T IMAGE PROCESS, V27, P1336, DOI 10.1109/TIP.2017.2777184
   Wang SW, 2022, IEEE T IMAGE PROCESS, V31, P556, DOI 10.1109/TIP.2021.3131941
   Wang Y., 2016, P 25 INT JOINT C ART, P2153
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3408317
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang YY, 2016, NEUROCOMPUTING, V204, P172, DOI 10.1016/j.neucom.2015.08.126
   Winn J, 2005, IEEE I CONF COMP VIS, P756
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xia W, 2023, IEEE T PATTERN ANAL, V45, P5187, DOI 10.1109/TPAMI.2022.3187976
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xia W, 2022, IEEE T CYBERNETICS, V52, P8962, DOI 10.1109/TCYB.2021.3052352
   Yang B, 2023, IEEE T KNOWL DATA EN, V35, P6887, DOI 10.1109/TKDE.2022.3185683
   Yang B, 2021, IEEE T IMAGE PROCESS, V30, P2575, DOI 10.1109/TIP.2020.3045631
   Yang HZ, 2022, IEEE T IMAGE PROCESS, V31, P3591, DOI 10.1109/TIP.2022.3171411
   Yang M, 2020, SIAM J IMAGING SCI, V13, P2361, DOI 10.1137/20M1318006
   Zhang XC, 2014, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2014.19
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
NR 52
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7413
EP 7425
DI 10.1109/TMM.2024.3367605
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000001
DA 2024-08-05
ER

PT J
AU Baniya, AA
   Lee, TK
   Eklund, PW
   Aryal, S
AF Baniya, Arbind Agrahari
   Lee, Tsz-Kwan
   Eklund, Peter W.
   Aryal, Sunil
TI Omnidirectional Video Super-Resolution Using Deep Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Omnidirectional videos; 360(degrees) videos; super-resolution; quality
   enhancement; 360(degrees) video dataset; deep learning; weighted
   spherically smooth L1 loss function
AB Omnidirectional Videos (or 360(degrees) videos) are widely used in Virtual Reality (VR) to facilitate immersive and interactive viewing experiences. However, the limited spatial resolution in 360(degrees) videos does not allow for each degree of view to be represented with adequate pixels, limiting the visual quality offered in the immersive experience. Deep learning Video Super-Resolution (VSR) techniques used for conventional videos could provide a promising software-based solution; however, these techniques do not tackle the distortion present in equirectangular projections of 360(degrees )video signals. An additional obstacle is the limited 360(degrees) video datasets to study. To address these issues, this paper creates a novel 360(degrees) Video Dataset (360VDS) with a study of the extensibility of conventional VSR models to 360(degrees) videos. This paper further proposes a novel deep learning model for 360(degrees) Video Super-Resolution (360(degrees) VSR), called Spherical Signal Super-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent modelling with an attention mechanism, unbound from conventional VSR techniques like alignment. With a purpose-built feature extractor and a novel loss-function addressing spherical distortion, S3PO outperforms most state-of-the-art conventional VSR models and 360(degrees) specific super-resolution models on 360(degrees) video datasets. A step-wise ablation study is presented to understand and demonstrate the impact of the chosen architectural sub-components, targeted training and optimisation.
C1 [Baniya, Arbind Agrahari; Lee, Tsz-Kwan; Eklund, Peter W.; Aryal, Sunil] Deakin Univ, Sch Informat Technol, Geelong, Vic 3217, Australia.
C3 Deakin University
RP Baniya, AA (corresponding author), Deakin Univ, Sch Informat Technol, Geelong, Vic 3217, Australia.
EM aagraharibaniya@deakin.edu.au; glory.lee@deakin.edu.au;
   peter.eklund@deakin.edu.au; sunil.aryal@deakin.edu.au
RI Aryal, Sunil/ABB-2228-2020; Agrahari Baniya, Arbind/HSF-1478-2023
OI Aryal, Sunil/0000-0002-6639-6824; Lee, Tsz-Kwan/0000-0003-4176-2215;
   Agrahari Baniya, PhD, Arbind/0000-0002-9359-6506
FU Faculty of Science, Engineering and Built Environment
FX No Statement Available
CR Afzal S, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P1, DOI 10.1145/3097895.3097896
   Ai H, 2022, arXiv
   Nguyen A, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P279, DOI 10.1145/3304109.3325820
   Balaji P, 2021, IEEE INT SM C CONF, DOI 10.1109/ISC253183.2021.9562830
   Baniya A. A., 2022, Onlinevideo super-resolution using unidirectional recurrent model
   Bhandari K, 2021, INT C PATT RECOG, P8196, DOI 10.1109/ICPR48806.2021.9412035
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Castellano B., 2022, Pyscenedetect
   Chan K.C., 2022, P IEEE CVF C COMP VI, P5972
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Chiariotti F, 2021, COMPUT COMMUN, V177, P133, DOI 10.1016/j.comcom.2021.06.029
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dasari M, 2020, IEEE INFOCOM SER, P1977, DOI [10.1109/infocom41043.2020.9155477, 10.1109/INFOCOM41043.2020.9155477]
   Deng X, 2021, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR46437.2021.00907
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   Fakour-Sevom V, 2018, VISAPP: PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL 4: VISAPP, P159, DOI 10.5220/0006618901590165
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Isobe Takashi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8005, DOI 10.1109/CVPR42600.2020.00803
   Isobe T, 2020, Arxiv, DOI arXiv:2008.05765
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Kingma D. P., 2014, arXiv
   Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581
   Lin YC, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2535, DOI 10.1145/3025453.3025757
   Liu HY, 2020, Arxiv, DOI arXiv:2008.10320
   Liu HY, 2022, ARTIF INTELL REV, V55, P5981, DOI 10.1007/s10462-022-10147-y
   Lucas A, 2019, IEEE T IMAGE PROCESS, V28, P3312, DOI 10.1109/TIP.2019.2895768
   Maniotis P, 2020, IEEE T MULTIMEDIA, V22, P2382, DOI 10.1109/TMM.2019.2957993
   Million M, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-64834-3
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Nasrabadi AT, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P273, DOI 10.1145/3304109.3325812
   Nishiyama A, 2021, IEEE IMAGE PROC, P1829, DOI 10.1109/ICIP42928.2021.9506233
   Ozcinar C, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901764
   Paszke A, 2019, ADV NEUR IN, V32
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Shi H, 2022, Arxiv, DOI arXiv:2202.13388
   Startsev M, 2018, SIGNAL PROCESS-IMAGE, V69, P43, DOI 10.1016/j.image.2018.03.013
   Su YC, 2018, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2018.00816
   Sun Y., 2016, P JOINT VID EXPL TEA, P15
   Telephone Installations and Local Line, 1999, Networks, V910, P5
   Thompson B., 2018, P 3 C MACH TRANSL RE, P124, DOI 10.18653/v1/W18-6313
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yaqoob A, 2020, IEEE COMMUN SURV TUT, V22, P2801, DOI 10.1109/COMST.2020.3006999
   Yi P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4409, DOI 10.1109/ICCV48922.2021.00439
   Yi P, 2020, IEEE T CIRC SYST VID, V30, P2503, DOI 10.1109/TCSVT.2019.2925844
   Zhou YM, 2020, IEEE J-STSP, V14, P118, DOI 10.1109/JSTSP.2019.2957952
   Zhou YF, 2018, INT CONF SIGN PROCES, P54, DOI 10.1109/ICSP.2018.8652269
NR 56
TC 3
Z9 3
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 540
EP 554
DI 10.1109/TMM.2023.3267294
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000017
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Cai, JY
   Zhang, YH
   Wang, SP
   Fan, JC
   Guo, WZ
AF Cai, Jinyu
   Zhang, Yunhe
   Wang, Shiping
   Fan, Jicong
   Guo, Wenzhong
TI Wasserstein Embedding Learning for Deep Clustering: A Generative
   Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Data models; Generative adversarial networks; Clustering
   methods; Task analysis; Deep learning; Decoding; Unsupervised learning;
   clustering analysis; Wasserstein embedding; generative models;
   auto-encoder
ID ADVERSARIAL NETWORKS; IDENTIFICATION; SELECTION
AB Deep learning-based clustering methods, especially those incorporating deep generative models, have recently shown noticeable improvement on many multimedia benchmark datasets. However, existing generative models still suffer from unstable training, and the gradient vanishes, which results in the inability to learn desirable embedded features for clustering. In this paper, we aim to tackle this problem by exploring the capability of Wasserstein embedding in learning representative embedded features and introducing a new clustering module for jointly optimizing embedding learning and clustering. To this end, we propose Wasserstein embedding clustering (WEC), which integrates robust generative models with clustering. By directly minimizing the discrepancy between the prior and marginal distribution, we transform the optimization problem of Wasserstein distance from the original data space into embedding space, which differs from other generative approaches that optimize in the original data space. Consequently, it naturally allows us to construct a joint optimization framework with the designed clustering module in the embedding layer. Due to the substitutability of the penalty term in Wasserstein embedding, we further propose two types of deep clustering models by selecting different penalty terms. Comparative experiments conducted on nine publicly available multimedia datasets with several state-of-the-art methods demonstrate the effectiveness of our method.
C1 [Cai, Jinyu; Zhang, Yunhe; Wang, Shiping; Guo, Wenzhong] Fuzhou Univ, Coll Comp & Data Sci, Fujian 350108, Peoples R China.
   [Fan, Jicong] Chinese Univ Hong Kong, Shenzhen 518172, Peoples R China.
   [Fan, Jicong] Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China.
C3 Fuzhou University; The Chinese University of Hong Kong, Shenzhen;
   Shenzhen Research Institute of Big Data
RP Guo, WZ (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, Fujian 350108, Peoples R China.
EM guowenzhong@fzu.edu.cn
RI Cai, Jinyu/ABA-8415-2021
OI Cai, Jinyu/0000-0003-2241-2754
FU National Natural Science Foundation of China
FX No Statement Available
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bharti KK, 2015, EXPERT SYST APPL, V42, P3105, DOI 10.1016/j.eswa.2014.11.038
   Borghuis V, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5225
   Cai JY, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108386
   Chang JL, 2020, IEEE T PATTERN ANAL, V42, P809, DOI 10.1109/TPAMI.2018.2889949
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P4054, DOI 10.1109/TMM.2021.3112230
   Chuang Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P735, DOI 10.1007/978-3-030-58595-2_44
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dilokthanakul N, 2017, PROC INT C LEARN REP, P1
   Dizaji KG, 2019, PROC CVPR IEEE, P4386, DOI 10.1109/CVPR.2019.00452
   Dizaji KG, 2017, IEEE I CONF COMP VIS, P5747, DOI 10.1109/ICCV.2017.612
   Fan JC, 2021, IEEE T SIGNAL PROCES, V69, P1755, DOI 10.1109/TSP.2021.3062988
   Fan WT, 2013, IEEE T KNOWL DATA EN, V25, P1670, DOI 10.1109/TKDE.2012.101
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grill JB, 2020, P 34 INT C NEUR INF, V33, P21271
   Guo XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1753
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hsu CC, 2018, IEEE T MULTIMEDIA, V20, P421, DOI 10.1109/TMM.2017.2745702
   Huang D, 2018, IEEE T CYBERNETICS, V48, P1460, DOI 10.1109/TCYB.2017.2702343
   Huang J., 2020, P IEEE CVF C COMP VI, P8846, DOI DOI 10.1109/CVPR42600.2020.00887
   Huang JB, 2020, AAAI CONF ARTIF INTE, V34, P11029
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiang X, 2021, IEEE T MULTIMEDIA, V23, P2602, DOI 10.1109/TMM.2020.3013449
   Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D. P., 2014, arXiv
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Li XL, 2017, IEEE T CYBERNETICS, V47, P3840, DOI 10.1109/TCYB.2016.2585355
   Lin JC, 2021, NEURAL NETWORKS, V133, P132, DOI 10.1016/j.neunet.2020.09.001
   Liu XW, 2021, PR MACH LEARN RES, V139
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Ma ZH, 2021, AAAI CONF ARTIF INTE, V35, P2319
   Macqueen J., 1967, P 5 BERKELEY S MATH, V1
   Mi L., 2018, P EUROPEAN C COMPUTE, P322
   Miyato T, 2018, CoRR
   Nowozin S, 2016, ADV NEUR IN, V29
   Patel VM, 2013, IEEE I CONF COMP VIS, P225, DOI 10.1109/ICCV.2013.35
   Peng X., 2016, INT JOINT C ARTIFICI, P1925
   Peng X, 2020, IEEE T NEUR NET LEAR, V31, P4857, DOI 10.1109/TNNLS.2019.2958324
   Peng X, 2017, IEEE T CYBERNETICS, V47, P1053, DOI 10.1109/TCYB.2016.2536752
   Peyré G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073
   Ren JF, 2019, AAAI CONF ARTIF INTE, P10011
   Sarkar A, 2020, AAAI CONF ARTIF INTE, V34, P5604
   Suh S, 2021, NEURAL NETWORKS, V133, P69, DOI 10.1016/j.neunet.2020.10.004
   Taherkhani F, 2021, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR46437.2021.01209
   Springenberg JT, 2016, Arxiv, DOI arXiv:1511.06390
   Tolstikhin I., 2018, PROC INT C LEARN REP, P1
   Tsai T. W., 2021, PROC INTCONF LEARNRE
   Van Der Maaten L., 2009, P MACHINE LEARNING R, P384
   Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670
   Wang QQ, 2021, IEEE T MULTIMEDIA, V23, P3483, DOI 10.1109/TMM.2020.3025666
   Wu JQ, 2018, LECT NOTES COMPUT SC, V11209, P673, DOI 10.1007/978-3-030-01228-1_40
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xu CY, 2022, KNOWL-BASED SYST, V238, DOI 10.1016/j.knosys.2021.107967
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Yang L, 2022, IEEE T NEUR NET LEAR, V33, P340, DOI 10.1109/TNNLS.2020.3027761
   Yang XJ, 2023, IEEE T NEUR NET LEAR, V34, P6263, DOI 10.1109/TNNLS.2021.3135375
   Yang Zhihan., 2020, PROC AAAI C ARTIF IN, V16, P137
   Yin M, 2020, AAAI CONF ARTIF INTE, V34, P6688
   Zhang TJ, 2023, IEEE T MULTIMEDIA, V25, P993, DOI 10.1109/TMM.2021.3136094
   Zhang Z, 2020, IEEE T PATTERN ANAL, V42, P1741, DOI 10.1109/TPAMI.2019.2903050
   Zhong ZL, 2018, AAAI CONF ARTIF INTE, P8191
   Zhou P, 2018, PROC CVPR IEEE, P1596, DOI 10.1109/CVPR.2018.00172
NR 65
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7567
EP 7580
DI 10.1109/TMM.2024.3369862
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000042
DA 2024-08-05
ER

PT J
AU Chai, XL
   Shao, F
   Jiang, QP
   Wang, XJ
   Xu, L
   Ho, YS
AF Chai, Xiongli
   Shao, Feng
   Jiang, Qiuping
   Wang, Xuejin
   Xu, Long
   Ho, Yo-Sung
TI Blind Quality Evaluator of Light Field Images by Group-Based
   Representations and Multiple Plane-Oriented Perceptual Characteristics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Light field images; blind image quality assessment; angular consistency;
   texture statistic measurement; 3D Log-Gabor filters
ID SUPERRESOLUTION; VIDEO
AB Due to the emergency of multi-view cameras and commercial Light Field (LF) cameras, the demand of high-performance LF quality evaluator is of great significance for guiding LF acquisition, processing and application and further promoting the visual perceived quality of LF visualizations. However, LF Images (LFIs), as high-dimensional data, suffer from various quality degradations not only in the spatial domain but also in the angular domain. Therefore, it is of great challenge to predict LF quality accurately. An effective LF evaluator should be able to represent these heterogeneous artifacts. In this paper, we provide a novel No-Reference LF Quality Assessment Evaluator (NR LF-QAE) to tackle this problem. Firstly, to measure angular consistency among viewports, we utilize group-based representations to character information similarity of aligned view stacks. Secondly, to better describe the texture information of LFIs, unifying spatial-angular texture statistic measurement is performed via Local Binary Patterns from Three Orthogonal Planes (LBP-TOP). Thirdly, we design 3D Log-Gabor filters to extract LF global structure information in Sub-Aperture Images (SAIs) as spatial feature characterizations and 2D Log-Gabor filters are adopted to characterize ray direction/depth information in Epipolar Plane Images (EPIs) as angular feature characterizations. By comprehensive LF information analyses in angular consistency and spatial-angular feature extraction with texture and structure descriptors, experimental results demonstrate the superiority of the proposed NR LF-QAE over the state-of-the-art comparative models in predicting the quality of LFIs on three available benchmark databases.
C1 [Chai, Xiongli; Shao, Feng; Jiang, Qiuping] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Wang, Xuejin] Fujian Univ Technol, Sch Comp Sci & Math, Fuzhou 350118, Peoples R China.
   [Xu, Long] Natl Astron Observ CAS, Key Lab Solar Act, Beijing 100101, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Fujian University of Technology; Chinese Academy of
   Sciences; National Astronomical Observatory, CAS; Gwangju Institute of
   Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 747866472@qq.com; shaofeng@nbu.edu.cn; jiangqiuping@nbu.edu.cn;
   1020468620@qq.com; lxu@nao.cas.cn; hoyo@gist.ac.kr
RI Xu, Long/AAH-9908-2019
OI Xu, Long/0000-0002-9286-2876; Qiuping, Jiang/0000-0002-6025-9343; HO,
   YO-SUNG/0000-0002-7220-1034; Chai, Xiongli/0000-0002-4245-5391
FU National Natural Science Foundation of China
FX No Statement Available
CR Aabed MA, 2017, IEEE INT CON MULTI, P1476, DOI 10.1109/ICME.2017.8019333
   Adelson E. H., 1991, PLENOPTIC FUNCTION E, V2
   Adhikarla VK, 2017, PROC CVPR IEEE, P3720, DOI 10.1109/CVPR.2017.396
   Bakir N, 2021, IEEE T MULTIMEDIA, V23, P2972, DOI 10.1109/TMM.2021.3068563
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P3722, DOI 10.1109/TMM.2021.3106775
   Cui YL, 2021, DIGIT SIGNAL PROCESS, V117, DOI 10.1016/j.dsp.2021.103138
   Dendi SVR, 2020, IEEE T IMAGE PROCESS, V29, P5612, DOI 10.1109/TIP.2020.2984879
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Ding YY, 2022, IEEE T CIRC SYST VID, V32, P467, DOI 10.1109/TCSVT.2021.3063853
   Dosil R, 2005, IEEE T BIO-MED ENG, V52, P2115, DOI 10.1109/TBME.2005.857635
   Fang YM, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Hacihaliloglu I, 2008, LECT NOTES COMPUT SC, V5241, P287, DOI 10.1007/978-3-540-85988-8_35
   Henriksson L, 2009, J NEUROSCI, V29, P14342, DOI 10.1523/JNEUROSCI.3136-09.2009
   Huang HL, 2022, IEEE T IMAGE PROCESS, V31, P3765, DOI 10.1109/TIP.2022.3175619
   Lei JJ, 2021, IEEE T CIRC SYST VID, V31, P3051, DOI 10.1109/TCSVT.2020.3037068
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy M, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.270
   Li TT, 2019, IEEE T IMAGE PROCESS, V28, P1798, DOI 10.1109/TIP.2018.2880510
   Li Z, 2012, IEEE T IMAGE PROCESS, V21, P2130, DOI 10.1109/TIP.2011.2173697
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Meng CL, 2021, IEEE T MULTIMEDIA, V24, P3193, DOI 10.1109/TMM.2021.3096071
   Meng CL, 2020, IEEE SIGNAL PROC LET, V27, P525, DOI 10.1109/LSP.2020.2982060
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Pan ZY, 2021, IEEE J-STSP, V15, P672, DOI 10.1109/JSTSP.2021.3056959
   Paudyal P, 2021, IEEE T BROADCAST, V67, P395, DOI 10.1109/TBC.2020.3034445
   Paudyal P, 2019, IEEE T BROADCAST, V65, P152, DOI 10.1109/TBC.2019.2892092
   Shan L, 2019, IEEE ACCESS, V7, P127217, DOI 10.1109/ACCESS.2019.2940093
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi LK, 2020, IEEE T CIRC SYST VID, V30, P4114, DOI 10.1109/TCSVT.2019.2955011
   Shi LK, 2018, IEEE IMAGE PROC, P41, DOI 10.1109/ICIP.2018.8451077
   Tian Y, 2021, IEEE T CIRC SYST VID, V31, P2046, DOI 10.1109/TCSVT.2020.2971256
   Tian Y, 2020, IEEE T IMAGE PROCESS, V29, P7945, DOI 10.1109/TIP.2020.3008856
   Tian Y, 2018, J VIS COMMUN IMAGE R, V57, P212, DOI 10.1016/j.jvcir.2018.11.005
   Viola I, 2018, INT WORK QUAL MULTIM, P189
   Wang YQ, 2021, IEEE T IMAGE PROCESS, V30, P1057, DOI 10.1109/TIP.2020.3042059
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiang JJ, 2023, IEEE T MULTIMEDIA, V25, P457, DOI 10.1109/TMM.2021.3127398
   Xiang JJ, 2021, IEEE T CIRC SYST VID, V31, P2575, DOI 10.1109/TCSVT.2020.3030049
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang JC, 2020, IEEE T MULTIMEDIA, V22, P2635, DOI 10.1109/TMM.2019.2961209
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
NR 55
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 607
EP 622
DI 10.1109/TMM.2023.3268370
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000029
DA 2024-08-05
ER

PT J
AU Fan, FG
   Su, YT
   Nie, LQ
   Jing, PG
   Hong, DZ
   Liu, Y
AF Fan, Fugui
   Su, Yuting
   Nie, Liqiang
   Jing, Peiguang
   Hong, Daozheng
   Liu, Yu
TI Dual-Domain Aligned Deep Hierarchical Matrix Factorization Method for
   Micro-Video Multi-Label Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Correlation; Visualization; Task analysis; Matrix
   decomposition; Estimation; Training; Micro-video; multi-label
   classification; semantic alignment; deep matrix factorization
AB Recently, with the growing popularity of micro-videos, multi-label learning has attracted increasing attention due to its potential commercial value in different scenarios. However, existing methods place more emphasis on the alignment between explicit semantics and visual features, while neglecting the exploration of interactions at fine-grained semantic levels. To address this problem, we propose a novel dual-domain aligned deep hierarchical matrix factorization (DADHMF) method for micro-video multi-label classification. Specifically, we construct a dual-stream deep matrix factorization framework to explore implicit hierarchical semantics and corresponding intrinsic feature representations in top-down and bottom-up ways, respectively. On this basis, we leverage the intralayer alignment strategy to narrow the semantic gap between label and instance domains by introducing adaptive semantic-aware embeddings. Moreover, we further utilize the inverse covariance estimation module to automatically capture latent semantic correlations, and project the structural information into the semantic-aware embeddings to ensure the stability of the intralayer alignment. Extensive experiments on two available micro-video multi-label datasets demonstrate that our proposed method outperforms the state-of-the-art methods.
C1 [Fan, Fugui; Su, Yuting; Jing, Peiguang; Hong, Daozheng] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Liqiang] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Liu, Yu] Tianjin Univ, Sch Microelect, Tianjin 300072, Peoples R China.
C3 Tianjin University; Harbin Institute of Technology; Tianjin University
RP Jing, PG (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM tjufan@tju.edu.cn; ytsu@tju.edu.cn; nieliqiang@gmail.com;
   pgjing@tju.edu.cn; 2573086477@qq.com; liuyu@tju.edu.cn
OI Liu, Yu/0000-0002-5949-6587; Fan, Fugui/0000-0002-7429-0245; Jing,
   Peiguang/0000-0003-2648-7358
FU National Natural Science Foundation of China
FX No Statement Available
CR Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Cai DS, 2022, IEEE T MULTIMEDIA, V24, P805, DOI 10.1109/TMM.2021.3059508
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P898, DOI 10.1145/2964284.2964314
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen WS, 2022, NEUROCOMPUTING, V491, P305, DOI 10.1016/j.neucom.2021.08.152
   Chen XS, 2021, IEEE T MULTIMEDIA, V23, P484, DOI 10.1109/TMM.2020.2978618
   Chen ZM, 2022, IEEE T IMAGE PROCESS, V31, P2570, DOI 10.1109/TIP.2022.3148867
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   De Handschutter P, 2021, COMPUT SCI REV, V42, DOI 10.1016/j.cosrev.2021.100423
   El Guennouni A, 2002, NUMER ALGORITHMS, V29, P75, DOI 10.1023/A:1014807923223
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Guo ZX, 2020, BIG DATA MIN ANAL, V3, P13, DOI 10.26599/BDMA.2019.9020020
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jing PG, 2018, IEEE T KNOWL DATA EN, V30, P1519, DOI 10.1109/TKDE.2017.2785784
   Li HC, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3107151
   Li S, 2016, IEEE T NEUR NET LEAR, V27, P2160, DOI 10.1109/TNNLS.2015.2464090
   Lin ZC, 2011, PROG MOL BIOL TRANSL, V98, P1, DOI 10.1016/B978-0-12-385506-0.00001-6
   Liu M, 2019, IEEE T IMAGE PROCESS, V28, P1235, DOI 10.1109/TIP.2018.2875363
   Liu S, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3020, DOI 10.1145/3308558.3313513
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Lu W, 2023, IEEE T MULTIMEDIA, V25, P77, DOI 10.1109/TMM.2021.3121567
   Lyu BS, 2017, LECT NOTES COMPUT SC, V10639, P443, DOI 10.1007/978-3-319-70136-3_47
   Ma JH, 2022, IEEE T CYBERNETICS, V52, P4596, DOI 10.1109/TCYB.2020.3031832
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Ou NR, 2022, INT CONF ACOUST SPEE, P4543, DOI 10.1109/ICASSP43922.2022.9746567
   Qiu YN, 2017, Arxiv, DOI arXiv:1711.07437
   Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015
   Shang XD, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P279, DOI 10.1145/3323873.3325056
   Song GL, 2021, IEEE T MULTIMEDIA, V23, P1882, DOI 10.1109/TMM.2020.3004963
   Su YT, 2021, INFORM SCIENCES, V575, P587, DOI 10.1016/j.ins.2021.07.021
   Su YT, 2020, IEEE SIGNAL PROC LET, V27, P740, DOI 10.1109/LSP.2020.2983831
   Sun LJ, 2022, IEEE T MULTIMEDIA, V24, P581, DOI 10.1109/TMM.2021.3055959
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan ZH, 2020, MACH LEARN, V109, P623, DOI 10.1007/s10994-019-05837-8
   Tarekegn AN, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.107965
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Trigeorgis G, 2017, IEEE T PATTERN ANAL, V39, P417, DOI 10.1109/TPAMI.2016.2554555
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Xie JY, 2023, IEEE T MULTIMEDIA, V25, P24, DOI 10.1109/TMM.2021.3120537
   Xu M, 2020, IEEE T KNOWL DATA EN, V32, P1610, DOI 10.1109/TKDE.2019.2908898
   Ye FH, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1393, DOI 10.1145/3269206.3271697
   Yu JS, 2018, IEEE ACCESS, V6, P58096, DOI 10.1109/ACCESS.2018.2873385
   Zhang J., 2016, P ACM INT C MULT, P1415
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang ML, 2009, INFORM SCIENCES, V179, P3218, DOI 10.1016/j.ins.2009.06.010
   Zheng HT, 2021, IEEE INT CONF BIG DA, P5014, DOI 10.1109/BigData52589.2021.9671563
   Zheng HL, 2020, IEEE T IMAGE PROCESS, V29, P476, DOI 10.1109/TIP.2019.2921876
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
NR 51
TC 1
Z9 1
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2598
EP 2607
DI 10.1109/TMM.2023.3301224
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400006
DA 2024-08-05
ER

PT J
AU Feng, WT
   Bai, L
   Yao, YQ
   Gan, WH
   Wu, W
   Ouyang, WL
AF Feng, Weitao
   Bai, Lei
   Yao, Yongqiang
   Gan, Weihao
   Wu, Wei
   Ouyang, Wanli
TI Similarity- and Quality-Guided Relation Learning for Joint Detection and
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Feature extraction; Videos; Correlation; Semantics;
   Target tracking; Object detection; Multi-object tracking; joint
   detection and tracking; similarity- and quality-guided attention;
   relation learning; instance-level spatial-temporal aggregation
ID OBJECT TRACKING
AB Joint detection and tracking, which solves two fundamental vision challenges in a unified manner, is a challenging topic in computer vision. In this area, the proper use of spatial-temporal information in videos can help reduce local defects and improve the quality of feature representations. Although modeling low-level (usually pixel-wise) spatial-temporal information has been studied, instance-level spatial-temporal correlations (i.e., relations between semantic regions in which instances have occurred) have not been fully exploited. In comparison, modeling instance-level correlation is a more flexible and reasonable way to enhance feature representations. However, we have found that conventional instance-level relation learning that works for the separate tasks of detection or tracking is not effective in joint tasks in which a variety of scenarios may be presented. To try to resolve this problem, in this study, we effectively exploited instance-level spatial-temporal semantic information for joint detection and tracking via a joint relation learning pipeline with a novel relation learning mechanism called Similarity- and Quality-Guided Attention (SQGA). Specifically, we added task-specific SQGA relation modules before the corresponding task prediction heads to refine the instance feature representation using features of other reference instances in the neighboring frames; these features are aggregated on the basis of relational affinities. In particular, in SQGA, relational affinities were factorized to similarity and quality terms so that fine-grained supervision rules could be applied. Then we added task-specific attention losses for each SQGA relation module, resulting in a better feature aggregation for the corresponding task. Quantitative experiments based on several challenging multi-object tracking benchmarks showed that our approach was more effective than the baselines and provided competitive results compared with recent state-of-the-art methods.
C1 [Feng, Weitao] Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia.
   [Bai, Lei] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
   [Yao, Yongqiang; Gan, Weihao; Wu, Wei] SenseTime Grp Ltd, Hong Kong, Peoples R China.
   [Ouyang, Wanli] Univ Sydney, Sch Elect & Informat Engn, Shanghai Artificial Intelligence Lab, Sydney, NSW 2006, Australia.
C3 University of Sydney; University of Sydney
RP Bai, L (corresponding author), Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
EM weitao.feng@sydney.edu.au; baisanshi@gmail.com; soundbupt@gmail.com;
   ganweihao@sensetime.com; wuwei@sensetime.com; wanli.ouyang@sydney.edu.au
OI BAI, LEI/0000-0003-3378-7201
FU Australian Research Council
FX No Statement Available
CR Babaee M, 2019, NEUROCOMPUTING, V368, P69, DOI 10.1016/j.neucom.2019.08.008
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bertasius G, 2018, LECT NOTES COMPUT SC, V11216, P342, DOI 10.1007/978-3-030-01258-8_21
   Brasó G, 2020, PROC CVPR IEEE, P6246, DOI 10.1109/CVPR42600.2020.00628
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen Y., 2020, CVPR, P10337
   Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347
   Chu Q, 2017, IEEE I CONF COMP VIS, P4846, DOI 10.1109/ICCV.2017.518
   Dendorfer P., 2020, arXiv
   Deng JJ, 2019, IEEE I CONF COMP VIS, P7022, DOI 10.1109/ICCV.2019.00712
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Fang JW, 2022, IEEE T INTELL TRANSP, V23, P9601, DOI 10.1109/TITS.2022.3157254
   Fang K, 2018, IEEE WINT CONF APPL, P466, DOI 10.1109/WACV.2018.00057
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Hornakova A, 2020, PR MACH LEARN RES, V119
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P2162, DOI 10.1109/TMM.2020.3008035
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Kieritz H, 2018, IEEE COMPUT SOC CONF, P1540, DOI 10.1109/CVPRW.2018.00195
   Leal-Taix‚ L, 2015, Arxiv, DOI [arXiv:1504.01942, DOI 10.48550/ARXIV.1504.01942]
   Lee B, 2016, LECT NOTES COMPUT SC, V9914, P68, DOI 10.1007/978-3-319-48881-3_6
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin WY, 2021, Arxiv, DOI arXiv:2005.04490
   Liu QK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P530
   Mahmoudi N, 2019, MULTIMED TOOLS APPL, V78, P7077, DOI 10.1007/s11042-018-6467-6
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Nair V., 2010, ICML, P807
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shao S, 2018, Arxiv, DOI arXiv:1805.00123
   Shijie Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P626, DOI 10.1007/978-3-030-58586-0_37
   Shvets M, 2019, IEEE I CONF COMP VIS, P9755, DOI 10.1109/ICCV.2019.00985
   Stadler D., 2021, P IEEE 17 INT C ADV, P1
   Stadler D, 2021, PROC CVPR IEEE, P10953, DOI 10.1109/CVPR46437.2021.01081
   Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394
   Tokmakov P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10840, DOI 10.1109/ICCV48922.2021.01068
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan XY, 2018, IEEE IMAGE PROC, P788, DOI 10.1109/ICIP.2018.8451174
   Wang GA, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P482, DOI 10.1145/3343031.3350853
   Wang Q, 2021, PROC CVPR IEEE, P3875, DOI 10.1109/CVPR46437.2021.00387
   Wang SY, 2018, LECT NOTES COMPUT SC, V11217, P557, DOI 10.1007/978-3-030-01261-8_33
   Wang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13199, DOI 10.1109/ICCV48922.2021.01297
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YX, 2021, IEEE INT CONF ROBOT, P13708, DOI 10.1109/ICRA48506.2021.9561110
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Xiao FY, 2018, LECT NOTES COMPUT SC, V11212, P494, DOI 10.1007/978-3-030-01237-3_30
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409
   Yao SY, 2021, IEEE T IMAGE PROCESS, V30, P4814, DOI 10.1109/TIP.2021.3076272
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Yu E, 2023, IEEE T MULTIMEDIA, V25, P2686, DOI 10.1109/TMM.2022.3150169
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhou ZW, 2018, INT C PATT RECOG, P1809, DOI 10.1109/ICPR.2018.8545450
   Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
NR 62
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1267
EP 1280
DI 10.1109/TMM.2023.3279670
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700039
DA 2024-08-05
ER

PT J
AU Gong, HH
   Dong, MJ
   Ma, SQ
   Camtepe, S
   Nepal, S
   Xu, C
AF Gong, Huihui
   Dong, Minjing
   Ma, Siqi
   Camtepe, Seyit
   Nepal, Surya
   Xu, Chang
TI Stealthy Physical Masked Face Recognition Attack via Adversarial Style
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Perturbation methods; Generators; Computational
   modeling; COVID-19; Task analysis; Optimization; Adversarial attack;
   computer vision; deep learning; face recognition; physical setting
AB Deep neural networks (DNNs) have achieved state-of-the-art performance on face recognition (FR) tasks in the last decade. In real scenarios, the deployment of DNNs requires taking various face accessories into consideration, like glasses, hats, and masks. In the COVID-19 pandemic era, wearing face masks is one of the most effective ways to defend against the novel coronavirus. However, DNNs are known to be vulnerable to adversarial examples with a small but elaborated perturbation. Thus, a facial mask with adversarial perturbations may pose a great threat to the widely used deep learning-based FR models. In this paper, we consider a challenging adversarial setting: targeted attack against FR models. We propose a new stealthy physical masked FR attack via adversarial style optimization. Specifically, we train an adversarial style mask generator that hides adversarial perturbations inside style masks. Moreover, to ameliorate the phenomenon of sub-optimization with one fixed style, we propose to discover the optimal style given a target through style optimization in a continuous relaxation manner. We simultaneously optimize the generator and the style selection for generating strong and stealthy adversarial style masks. We evaluated the effectiveness and transferability of our proposed method via extensive white-box and black-box digital experiments. Furthermore, we also conducted physical attack experiments against local FR models and online platforms.
C1 [Gong, Huihui; Dong, Minjing; Xu, Chang] Univ Sydney, Fac Engn, Sch Comp Sci, Sydney, NSW 2008, Australia.
   [Ma, Siqi] Univ New South Wales Canberra, Sch Engn & Informat Technol, Canberra, ACT 2612, Australia.
   [Camtepe, Seyit; Nepal, Surya] CSIRO, Data61, Sydney, NSW 1466, Australia.
C3 University of Sydney; University of New South Wales Sydney; Commonwealth
   Scientific & Industrial Research Organisation (CSIRO)
RP Xu, C (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Sydney, NSW 2008, Australia.
EM hgon9611@uni.sydney.edu.au; mdon0736@uni.sydney.edu.au;
   siqi.ma@adfa.edu.au; sayit.camtepe@data61.csiro.au;
   surya.nepal@data61.csiro.au; c.xu@sydney.edu.au
RI Nepal, Surya/B-7523-2011; Camtepe, Seyit/E-6113-2013
OI Nepal, Surya/0000-0002-3289-6599; DONG, Minjing/0009-0003-1717-818X;
   Camtepe, Seyit/0000-0001-6353-8359
FU Australian Research Council
FX No Statement Available
CR Amini S, 2020, IEEE T MULTIMEDIA, V22, P1889, DOI 10.1109/TMM.2020.2969784
   Athalye A, 2018, PR MACH LEARN RES, V80
   Baidu, 2023, Online face verification platform
   Brown T.B, 2017, 2017 WORKSHOP MACH L, P1
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Cao YX, 2023, P IEEE S SECUR PRIV, P1631, DOI 10.1109/SP46215.2023.10179383
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Cheng YP, 2022, IEEE T MULTIMEDIA, V24, P3807, DOI 10.1109/TMM.2021.3108009
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dong YP, 2019, PROC CVPR IEEE, P7706, DOI 10.1109/CVPR.2019.00790
   Duan R., 2021, PROC IEEE C COMPUT V, P16062
   Duan RJ, 2020, PROC CVPR IEEE, P997, DOI 10.1109/CVPR42600.2020.00108
   Dumoulin J., 2017, 5THINTCONFLEARNREPRE, P1
   Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P1879, DOI 10.1109/TMM.2022.3192769
   Gao LL, 2022, IEEE T MULTIMEDIA, V24, P2329, DOI 10.1109/TMM.2021.3079723
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Guetta N, 2021, Arxiv, DOI arXiv:2109.06467
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosseini H, 2018, IEEE COMPUT SOC CONF, P1695, DOI 10.1109/CVPRW.2018.00212
   Hu YCT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7828, DOI 10.1109/ICCV48922.2021.00775
   Hu Z., 2022, P IEEE CVF C COMP VI, P13307
   Huang G. B., 2007, Technical report, DOI 10.1.1. 122.8268
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   iFLYTEK, 2023, Online face verification platform
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2015, ICLR, P1
   Komkov S, 2021, INT C PATT RECOG, P819, DOI 10.1109/ICPR48806.2021.9412236
   Kurakin A., 2016, 4 INT C LEARN REPRES, P1
   Laidlaw C, 2019, ADV NEUR IN, V32
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H.T.D., 2019, 7 INT C LEARN REPRES, P1
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Madry A., 2018, 6 INT CONFLEARN REPR, P1
   Meng Q, 2021, PROC CVPR IEEE, P14220, DOI 10.1109/CVPR46437.2021.01400
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Qin Y, 2019, International conference on machine learning. PMLR, V97, P5231
   Rice L., 2020, PMLR, P8093
   Ronneberger P., 2015, MEDICAL IMAGE COMPUT, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sengupta S, 2016, IEEE WINT CONF APPL
   Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392
   Simonyan K., 2014, C TRACK P
   Sun JN, 2020, IEEE T MULTIMEDIA, V22, P2833, DOI 10.1109/TMM.2020.2966863
   Sun Y, 2014, ADV NEUR IN, V27
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Szegedy C., 2014, P 2 INT C LEARN REPR, P1
   Tencent, 2023, Online face verification platform
   Wan F. Huang, 2023, IEEE Trans. Multimedia, P1
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xiao ZH, 2021, PROC CVPR IEEE, P11840, DOI 10.1109/CVPR46437.2021.01167
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Yi D, 2014, Arxiv, DOI [arXiv:1411.7923, DOI 10.48550/ARXIV.1411.7923]
   Yin B., 2021, P INT JOINT C ART IN, P1252
   Zeng XH, 2019, PROC CVPR IEEE, P4297, DOI 10.1109/CVPR.2019.00443
   Zhang HZ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5564
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhong YY, 2022, IEEE T MULTIMEDIA, V24, P1186, DOI 10.1109/TMM.2021.3123478
   Zhong YY, 2021, IEEE T INF FOREN SEC, V16, P1452, DOI 10.1109/TIFS.2020.3036801
   Zhong YQ, 2022, PROC CVPR IEEE, P15324, DOI 10.1109/CVPR52688.2022.01491
   Zolfi A, 2023, LECT NOTES ARTIF INT, V13715, P304, DOI 10.1007/978-3-031-26409-2_19
NR 71
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5014
EP 5025
DI 10.1109/TMM.2023.3330089
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600046
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Gou, JP
   Xie, NN
   Yuan, YH
   Du, L
   Ou, WH
   Yi, Z
AF Gou, Jianping
   Xie, Nannan
   Yuan, Yunhao
   Du, Lan
   Ou, Weihua
   Yi, Zhang
TI Reconstructed Graph Constrained Auto-Encoders for Multi-View
   Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Representation learning; Laplace equations; Neural networks; Deep
   learning; Data models; Manifold learning; Linear programming;
   Auto-encoders; Reconstructed graph regularization; Multi-view
   representation learning
ID DIMENSIONALITY REDUCTION; RECOGNITION
AB The application of Auto-Encoder (AE) to multi-view representation learning has gained traction due to advancements in deep learning. While some current AE-based multi-view representation learning algorithms incorporate the geometric structure of the input data into their feature representation learning process, their use of a shallow structured graph regularization term can be restrictive when used in conjunction with deep models. Furthermore, current multi-view representation learning algorithms do not fully utilize the diversity and consistency presented in different views, leading to a reduction in the efficacy of feature learning. This paper introduces a novel approach, reconstructed graph constrained auto-encoders (RGCAE), for multi-view representation learning. Unlike existing methods, our approach incorporates deep adaptive graph regularization based on multi-layer perceptron to ensure the preservation of the geometric similarity graph, which is constructed based on the local invariance principle. By decoupling the feature representation learning from the preservation of the geometric structure among different views, our approach can better leverage the diversity presented in multi-view data. We obtain view-specific representations that preserve the geometric structure and then combine them by averaging to obtain a common representation. To ensure the consistency of the multi-view data, we minimize the loss between the view-specific and common representations. Consequently, our RGCAE approach can maintain the geometric structure of multi-view data and is better suited for integration with deep models. Extensive experiments on six datasets demonstrate that RGCAE obtained promising performance, compared with the state-of-the-art methods.
C1 [Gou, Jianping] Southwest Univ, Coll Comp & Informat Sci, Coll Software, Chongqing 400715, Peoples R China.
   [Xie, Nannan] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Peoples R China.
   [Yuan, Yunhao] Yangzhou Univ, Sch Comp Sci & Technol, Yangzhou 225012, Peoples R China.
   [Du, Lan] Monash Univ, Fac Informat Technol, Clayton, Vic 3800, Australia.
   [Ou, Weihua] Guizhou Normal Univ, Sch Big Data & Comp Sci, Guiyang 550003, Peoples R China.
   [Yi, Zhang] Sichuan Univ, Sch Comp Sci, Chengdu 610017, Peoples R China.
C3 Southwest University - China; Jiangsu University; Yangzhou University;
   Monash University; Guizhou Normal University; Sichuan University
RP Gou, JP (corresponding author), Southwest Univ, Coll Comp & Informat Sci, Coll Software, Chongqing 400715, Peoples R China.; Du, L (corresponding author), Monash Univ, Fac Informat Technol, Clayton, Vic 3800, Australia.
EM cherish.gjp@gmail.com; 2222008075@stmail.ujs.edu.cn; yhyuan@yzu.edu.cn;
   lan.du@monash.edu; ouweihuahust@gmail.com; zhangyi@scu.edu.cn
RI Ou, Weihua/T-9156-2019
OI Ou, Weihua/0000-0001-5241-7703; Gou, Jianping/0000-0003-1413-0693
FU National Natural Science Foundation of China
FX No Statement Available
CR Baldi P., 2012, P WORKSH UNS TRANSF, P37
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bengio Y., 2006, Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Cai X, 2012, BIOINFORMATICS, V28, pI16, DOI 10.1093/bioinformatics/bts220
   Chai H., 2021, Mach.Learn., V6
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Du GW, 2021, DATA SCI ENG, V6, P323, DOI 10.1007/s41019-021-00159-z
   Fayek HM, 2017, NEURAL NETWORKS, V92, P60, DOI 10.1016/j.neunet.2017.02.013
   Gou JP, 2018, IEEE ACCESS, V6, P75748, DOI 10.1109/ACCESS.2018.2884027
   Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022
   Han Z., 2021, PROC INT C LEARN REP, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XF, 2004, ADV NEUR IN, V16, P153
   Jia K, 2015, NEUROCOMPUTING, V160, P250, DOI 10.1016/j.neucom.2015.02.023
   Kaloga Y, 2021, SIGNAL PROCESS, V188, DOI 10.1016/j.sigpro.2021.108182
   Kang HY, 2020, IEEE T MED IMAGING, V39, P2606, DOI 10.1109/TMI.2020.2992546
   Khalil RA, 2019, IEEE ACCESS, V7, P117327, DOI 10.1109/ACCESS.2019.2936124
   Li NJ, 2021, IEEE T MULTIMEDIA, V23, P203, DOI 10.1109/TMM.2020.2984093
   Li R, 2020, IEEE ACCESS, V8, P28614, DOI 10.1109/ACCESS.2020.2972132
   Liao YH, 2002, COMPUT SECUR, V21, P439, DOI 10.1016/S0167-4048(02)00514-X
   Liao YY, 2017, IEEE T IMAGE PROCESS, V26, P2839, DOI 10.1109/TIP.2016.2605010
   Liu AA, 2021, INFORM SCIENCES, V547, P984, DOI 10.1016/j.ins.2020.09.057
   Liu CH, 2022, AAAI CONF ARTIF INTE, P7542
   Liu CL, 2021, ENG APPL ARTIF INTEL, V104, DOI 10.1016/j.engappai.2021.104341
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Olson CC, 2018, EXPERT SYST APPL, V91, P374, DOI 10.1016/j.eswa.2017.08.005
   Qi SH, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102053
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Salahian N, 2023, EXPERT SYST APPL, V214, DOI 10.1016/j.eswa.2022.119051
   Shahamiri SR, 2021, IEEE T NEUR SYS REH, V29, P852, DOI 10.1109/TNSRE.2021.3076778
   Shi C, 2020, IEEE T MULTIMEDIA, V22, P487, DOI 10.1109/TMM.2019.2928491
   Song ZX, 2023, IEEE T NEUR NET LEAR, V34, P8174, DOI 10.1109/TNNLS.2022.3155478
   Su XR, 2022, BRIEF BIOINFORM, V23, DOI 10.1093/bib/bbab526
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang C, 2023, SCI CHINA INFORM SCI, V66, DOI 10.1007/s11432-022-3579-1
   Tang C, 2023, IEEE T KNOWL DATA EN, V35, P6449, DOI 10.1109/TKDE.2022.3172687
   Tian F, 2014, AAAI CONF ARTIF INTE, P1293
   Wang HX, 2012, NEURAL NETWORKS, V27, P38, DOI 10.1016/j.neunet.2011.11.003
   Wang R, 2017, IEEE T IMAGE PROCESS, V26, P5019, DOI 10.1109/TIP.2017.2726188
   Wang SP, 2021, INFORM SCIENCES, V562, P438, DOI 10.1016/j.ins.2021.03.040
   Weston J., 2008, P 25 INT C MACH LEAR, P1168
   Wu J.-S., 2021, Pattern Recognit., V112
   Xu J, 2023, IEEE T KNOWL DATA EN, V35, P7470, DOI 10.1109/TKDE.2022.3193569
   Xu XZ, 2022, IEEE T MULTIMEDIA, V24, P2752, DOI 10.1109/TMM.2021.3087098
   Yang Shijie, 2017, IEEE C COMPUT VIS PA, P1203
   Zhang CQ, 2020, INT J COMPUT VISION, V128, P2344, DOI 10.1007/s11263-020-01307-0
   Zhang CQ, 2022, IEEE T PATTERN ANAL, V44, P2402, DOI 10.1109/TPAMI.2020.3037734
   Zhang C, 2019, PROC CVPR IEEE, P9444, DOI 10.1109/CVPR.2019.00968
   Zheng QH, 2022, IEEE T CIRC SYST VID, V32, P4202, DOI 10.1109/TCSVT.2021.3127007
NR 51
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1319
EP 1332
DI 10.1109/TMM.2023.3279988
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700043
DA 2024-08-05
ER

PT J
AU He, MZ
   Wang, HX
   Zhang, F
   Xiang, YY
AF He, Mingze
   Wang, Hongxia
   Zhang, Fei
   Xiang, Yuyuan
TI Exploring Accurate Invariants on Polar Harmonic Fourier Moments in Polar
   Coordinates for Robust Image Watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Watermarking; Robustness; Distortion; Harmonic analysis; Transforms;
   Digital images; Resists; Image watermarking; polar harmonic Fourier
   moments; robustness; polar pixel tiling; geometric attacks
ID RESILIENT WATERMARKING; ZERNIKE MOMENTS
AB In moment-based watermarking schemes, the accuracy of the moments is crucial for constructing robust watermarking schemes. The robustness of the watermarking scheme relies heavily on the proper representation of the moments. Despite the importance, current theoretical research on accuracy is very limited in watermarking techniques. To this end, we propose a novel robust image watermarking scheme based on accurate polar harmonic Fourier moments (PHFMs). Specifically, the accurate PHFMs computation based on polar pixel tiling with nearest neighbor interpolation (PPTN) is designed. This computation is general and used for embedder and extractor. This ingenious design eliminates geometric and numerical integration errors and also avoids the distortion interaction caused by watermarks. Also, an improved quantization strategy is applied to the embedding process, and satisfactory imperceptibility is obtained. The watermark is extracted without the host image. The experimental results show the excellent robustness of the proposed watermarking scheme to common image processing attacks, geometric attacks, and some kinds of compound attacks. The proposed scheme is superior to the state-of-the-art image watermarking schemes.
C1 [He, Mingze; Wang, Hongxia; Zhang, Fei; Xiang, Yuyuan] Sichuan Univ, Sch Cyber Sci & Engn, Chengdu 610065, Peoples R China.
   [He, Mingze; Wang, Hongxia; Zhang, Fei; Xiang, Yuyuan] Sichuan Univ, Key Lab Data Protect & Intelligent Management, Minist Educ, Chengdu 610065, Peoples R China.
C3 Sichuan University; Sichuan University
RP Wang, HX (corresponding author), Sichuan Univ, Sch Cyber Sci & Engn, Chengdu 610065, Peoples R China.
EM hemingze@stu.scu.edu.cn; hxwang@scu.edu.cn; zhangfei@stu.scu.edu.cn;
   xiangyuyuan@stu.scu.edu.cn
OI Zhang, Fei/0000-0002-2591-906X; He, Mingze/0000-0002-0611-0767
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahmadi M, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113157
   Alghoniemy M, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1291, DOI 10.1109/ICME.2000.871003
   [Anonymous], 2016, The USC-SIPI image database
   Delmotte A, 2021, IEEE T MULTIMEDIA, V23, P3467, DOI 10.1109/TMM.2020.3025660
   Fang H, 2023, IEEE T MULTIMEDIA, V25, P2648, DOI 10.1109/TMM.2022.3149641
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Farzam M, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P529, DOI 10.1109/MMSP.2001.962787
   Filler T., 2022, The BOSSBase image database
   Hosny KM, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.103007
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu RW, 2021, IEEE T IMAGE PROCESS, V30, P318, DOI 10.1109/TIP.2020.3036727
   Huan WN, 2022, IEEE T CIRC SYST VID, V32, P1955, DOI 10.1109/TCSVT.2021.3092004
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Jia ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P41, DOI 10.1145/3474085.3475324
   Kang XG, 2010, IEEE T INF FOREN SEC, V5, P1, DOI 10.1109/TIFS.2009.2039604
   Kim HS, 2003, IEEE T CIRC SYST VID, V13, P766, DOI 10.1109/TCSVT.2003.815955
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Liao SX, 1998, IEEE T PATTERN ANAL, V20, P1358, DOI 10.1109/34.735809
   Lin CY, 2001, IEEE T IMAGE PROCESS, V10, P767, DOI 10.1109/83.918569
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lutovac B, 2017, MULTIMED TOOLS APPL, V76, P23333, DOI 10.1007/s11042-016-4127-2
   Ma B, 2020, SIGNAL PROCESS, V172, DOI 10.1016/j.sigpro.2020.107544
   Ma ZH, 2021, IEEE T CIRC SYST VID, V31, P4826, DOI 10.1109/TCSVT.2021.3055255
   Nezhadarya E, 2011, IEEE T INF FOREN SEC, V6, P1200, DOI 10.1109/TIFS.2011.2163627
   Niu PP, 2022, J MATH IMAGING VIS, V64, P537, DOI 10.1007/s10851-022-01084-0
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Qi SR, 2023, IEEE T PATTERN ANAL, V45, P5337, DOI 10.1109/TPAMI.2022.3204971
   Qi SR, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3479428
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Ren HP, 2003, J OPT SOC AM A, V20, P631, DOI 10.1364/JOSAA.20.000631
   Tan J, 2022, COMPUT MATH METHOD M, V2022, DOI 10.1155/2022/9880038
   Tang YC, 2023, IEEE T CIRC SYST VID, V33, P1593, DOI 10.1109/TCSVT.2022.3216849
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920
   Wang CP, 2016, SIGNAL PROCESS-IMAGE, V45, P10, DOI 10.1016/j.image.2016.03.007
   Wang CP, 2021, ENG APPL ARTIF INTEL, V106, DOI 10.1016/j.engappai.2021.104450
   Wang CP, 2022, IEEE T CIRC SYST VID, V32, P1998, DOI 10.1109/TCSVT.2021.3094882
   Wang CP, 2020, IEEE T CIRC SYST VID, V30, P4440, DOI 10.1109/TCSVT.2019.2960507
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia ZQ, 2022, APPL INTELL, V52, P607, DOI 10.1007/s10489-021-02476-2
   Xia ZQ, 2021, KNOWL-BASED SYST, V216, DOI 10.1016/j.knosys.2020.106568
   Xiao B, 2010, PATTERN RECOGN, V43, P2620, DOI 10.1016/j.patcog.2010.03.013
   Xin YQ, 2007, PATTERN RECOGN, V40, P3740, DOI 10.1016/j.patcog.2007.05.004
   Xin YQ, 2007, IEEE T IMAGE PROCESS, V16, P581, DOI 10.1109/TIP.2006.888346
   Yamni M, 2022, EXPERT SYST APPL, V203, DOI 10.1016/j.eswa.2022.117325
   Yang HY, 2015, AEU-INT J ELECTRON C, V69, P389, DOI 10.1016/j.aeue.2014.10.012
   Yap PT, 2010, IEEE T PATTERN ANAL, V32, P1259, DOI 10.1109/TPAMI.2009.119
   Ye B, 2002, J OPT A-PURE APPL OP, V4, P606, DOI 10.1088/1464-4258/4/6/304
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Zhang XY, 2020, P IEEE, V108, P894, DOI 10.1109/JPROC.2020.2989782
   Zhang Y, 2021, IEEE T EM TOP COMP I, V5, P726, DOI 10.1109/TETCI.2021.3100641
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 52
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5435
EP 5449
DI 10.1109/TMM.2023.3333659
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600035
DA 2024-08-05
ER

PT J
AU Jiang, QP
   Kang, YZ
   Wang, ZH
   Ren, WQ
   Li, CY
AF Jiang, Qiuping
   Kang, Yaozu
   Wang, Zhihua
   Ren, Wenqi
   Li, Chongyi
TI Perception-Driven Deep Underwater Image Enhancement Without Paired
   Supervision
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Training; Atmospheric modeling; Generative
   adversarial networks; Image enhancement; Measurement; Image quality;
   Underwater image enhancement; perception-driven; human perception;
   generative adversarial network
ID ADAPTIVE HISTOGRAM EQUALIZATION; MODEL
AB Underwater image enhancement (UIE) aims to improve the visual quality of raw underwater images. Current UIE algorithms primarily train a deep neural network (DNN) on synthetic datasets or datasets with pseudo labels by minimizing the reconstruction loss between enhanced images and ground truth images. However, there is a domain gap between synthetic and real-world underwater images, and the widely used l(2) loss tends to overlook the importance of human perception, resulting in unsatisfactory perceptual quality of the final enhanced results. In this paper, we propose an unsupervised perception-driven DNN called PDD-Net for generalizable UIE. Instead of relying on paired images for training, we resort to an unsupervised generative adversarial network (GAN) with a large-scale set of easily available natural images as the target domain. This enables training on larger image sets collected from various domains while avoiding over-fitted to any specific data generation protocol. Additionally, to make the visual quality of enhanced underwater images more in line with human perception, we pre-train a DNN-based pairwise quality ranking (PQR) model based on which a PQR loss is formulated to progressively guides the enhancement of raw underwater image toward the higher quality direction. In addition, we introduce a global attention module (GAM) that integrates modulation and attention mechanisms to enable capturing rich global and local information, leading to improvements in both brightness and contrast. Extensive experiments demonstrate that our proposed PDD-Net exhibits excellent generalization capabilities and outperforms existing methods in terms of both visual perception quality and quantitative indicators across different datasets.
C1 [Jiang, Qiuping; Kang, Yaozu] Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Wang, Zhihua] Shenzhen MSU BIT Univ, Guangdong Lab Machine Percept & Intelligent Comp, Shenzhen 518115, Peoples R China.
   [Ren, Wenqi] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen 510006, Peoples R China.
   [Li, Chongyi] Nankai Univ, Sch Comp Sci, Tianjin 300071, Peoples R China.
C3 Ningbo University; Shenzhen MSU-BIT University; Sun Yat Sen University;
   Nankai University
RP Wang, ZH (corresponding author), Shenzhen MSU BIT Univ, Guangdong Lab Machine Percept & Intelligent Comp, Shenzhen 518115, Peoples R China.
EM jiangqiuping@nbu.edu.cn; 2571059190@qq.com; zhihua.wang@my.cityu.edu.hk;
   renwq3@mail.sysu.edu.cn; lichongyi25@gmail.com
RI Ren, Wenqi/L-8724-2019
OI Zhihua, WANG/0000-0002-4398-536X; Qiuping, Jiang/0000-0002-6025-9343
FU National Natural Science Foundation of China
FX No Statement Available
CR Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Anwar S, 2018, Arxiv, DOI arXiv:1807.03528
   Bae Y, 2010, IEEE IMAGE PROC, P3597, DOI 10.1109/ICIP.2010.5652000
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Cai XW, 2024, IEEE J OCEANIC ENG, V49, P226, DOI 10.1109/JOE.2023.3245760
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Fu ZQ, 2022, SIGNAL PROCESS-IMAGE, V102, DOI 10.1016/j.image.2021.116622
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Gao FR, 2021, J MAR SCI ENG, V9, DOI 10.3390/jmse9020225
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu JK, 2021, IEEE SIGNAL PROC LET, V28, P2152, DOI 10.1109/LSP.2021.3099746
   Hummel R.A., 1975, COMPUT VISION GRAPH, V4, P209
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jiang NF, 2022, IEEE T MULTIMEDIA, V24, DOI 10.1109/TMM.2021.3115442
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang Q, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108324
   Jiang ZY, 2022, IEEE T CIRC SYST VID, V32, P6584, DOI 10.1109/TCSVT.2022.3174817
   Jolicoeur-Martineau A, 2018, Arxiv, DOI arXiv:1807.00734
   Kang YZ, 2023, IEEE T CIRC SYST VID, V33, P988, DOI 10.1109/TCSVT.2022.3208100
   Khan A, 2016, 2016 IEEE 6TH INTERNATIONAL CONFERENCE ON UNDERWATER SYSTEM TECHNOLOGY: THEORY AND APPLICATIONS, P83, DOI 10.1109/USYS.2016.7893927
   Kingma D. P., 2014, arXiv
   Lebart K, 2003, IEEE J OCEANIC ENG, V28, P673, DOI 10.1109/JOE.2003.819314
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li JC, 2021, IEEE T MULTIMEDIA, V23, P2986, DOI 10.1109/TMM.2021.3068561
   Liang Z, 2022, IEEE T CIRC SYST VID, V32, P4879, DOI 10.1109/TCSVT.2021.3114230
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu Z, 2022, J MAR SCI ENG, V10, DOI 10.3390/jmse10091204
   Ma CX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2420, DOI 10.1145/3474085.3475408
   Matkovic K., 2005, Computational Aesthetics, P159
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Qi Q, 2022, IEEE T CIRC SYST VID, V32, P1133, DOI 10.1109/TCSVT.2021.3074197
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Skarlatos D., 2016, Digital Heritage. Progress in Cultural Heritage: Documentation, Preservation, and Protection: 6th International Conference, EuroMed 2016, Nicosia, Cyprus, VVolume 10058, DOI [10.1007/978-3-319-48496-964, DOI 10.1007/978-3-319-48496-964]
   Song W, 2020, IEEE T BROADCAST, V66, P153, DOI 10.1109/TBC.2019.2960942
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wang Y, 2019, IEEE ACCESS, V7, P140233, DOI 10.1109/ACCESS.2019.2932130
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wu JJ, 2022, SIGNAL PROCESS-IMAGE, V109, DOI 10.1016/j.image.2022.116855
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang N, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116218
   Yang SD, 2019, IEEE ACCESS, V7, P165318, DOI 10.1109/ACCESS.2019.2953463
   Yuh J, 2001, ADV ROBOTICS, V15, P609, DOI 10.1163/156855301317033595
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P1304, DOI 10.1109/TPAMI.2020.3024207
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zheng YN, 2022, IEEE T IMAGE PROCESS, V31, P5456, DOI 10.1109/TIP.2022.3196815
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
   Zhuang PX, 2021, ENG APPL ARTIF INTEL, V101, DOI 10.1016/j.engappai.2021.104171
NR 66
TC 3
Z9 3
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4884
EP 4897
DI 10.1109/TMM.2023.3327613
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600003
DA 2024-08-05
ER

PT J
AU Lin, DX
   Peng, YX
   Meng, JK
   Zheng, WS
AF Lin, Dixuan
   Peng, Yi-Xing
   Meng, Jingke
   Zheng, Wei-Shi
TI Cross-Modal Adaptive Dual Association for Text-to-Image Person Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoding; Feature extraction; Adaptation models; Visualization; Task
   analysis; Footwear; Feeds; Person re-identification; text-to-image
AB Text-to-image person re-identification (ReID) aims to retrieve images of a person based on a given textual description. The key challenge is to learn the relations between detailed information from visual and textual modalities. Existing work focuses on learning a latent space to narrow the modality gap and further build local correspondences between two modalities. However, these methods assume that image-to-text and text-to-image associations are modality-agnostic, resulting in suboptimal associations. In this work, we demonstrate the discrepancy between image-to-text association and text-to-image association and proposecross-modal adaptive dual association (CADA) to build fine bidirectional image-text detailed associations. Our approach features a decoder-based adaptive dual association module that enables full interaction between visual and textual modalities, enabling bidirectional and adaptive cross-modal correspondence associations. Specifically, this paper proposes a bidirectional association mechanism: Association of text Tokens to image Patches (ATP) and Association of image Regions to text Attributes (ARA). We adaptively model the ATP based on the fact that aggregating cross-modal features based on mistaken associations will lead to feature distortion. For modeling the ARA, since attributes are typically the first distinguishing cues of a person, we explore attribute-level associations by predicting the masked text phrase using the related image region. Finally, we learn the dual associations between texts and images, and the experimental results demonstrate the superiority of our dual formulation.
C1 [Lin, Dixuan; Peng, Yi-Xing; Meng, Jingke; Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
C3 Sun Yat Sen University
RP Meng, JK (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
EM asxlwsl@gmail.com; pengyx23@mail2.sysu.edu.cn; mengjke@gmail.com;
   wszheng@ieee.org
FU National Science Foundation for Young Scientists of China
FX No Statement Available
CR Chang H, 2023, MACH LEARN, V112, P1847, DOI 10.1007/s10994-022-06208-6
   Chen TL, 2018, IEEE WINT CONF APPL, P1879, DOI 10.1109/WACV.2018.00208
   Chen YH, 2022, NEUROCOMPUTING, V494, P171, DOI 10.1016/j.neucom.2022.04.081
   Devlin J., 2019, PROC C N AM CHAPTER
   Ding ZF, 2021, Arxiv, DOI arXiv:2107.12666
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan HN, 2022, IEEE T MULTIMEDIA, V24, P49, DOI 10.1109/TMM.2020.3045286
   Farooq A, 2022, AAAI CONF ARTIF INTE, P4477
   Gao CY, 2021, Arxiv, DOI arXiv:2101.03036
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Han X, 2021, Arxiv, DOI arXiv:2110.10807
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ji Z, 2023, IEEE T MULTIMEDIA, V25, P7699, DOI 10.1109/TMM.2022.3225754
   Jia C, 2021, PR MACH LEARN RES, V139
   Jiang D, 2023, PROC CVPR IEEE, P2787, DOI 10.1109/CVPR52729.2023.00273
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Kingma D.P., 2014, Proc. of ICLR
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li JH, 2021, ADV NEUR IN, V34
   Li JN, 2022, PR MACH LEARN RES
   Li SP, 2022, INT CONF ACOUST SPEE, P2724, DOI 10.1109/ICASSP43922.2022.9746846
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P665, DOI 10.1145/3343031.3350991
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Ning X, 2021, IEEE T CIRC SYST VID, V31, P3391, DOI 10.1109/TCSVT.2020.3043026
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Peng YX, 2023, IEEE T MULTIMEDIA, V25, P2393, DOI 10.1109/TMM.2022.3146775
   Radford A, 2021, PR MACH LEARN RES, V139
   Shao ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5566, DOI 10.1145/3503161.3548028
   Shi YX, 2021, IEEE T MULTIMEDIA, V23, P4376, DOI 10.1109/TMM.2020.3042068
   Shu Xiujun, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13805), P624, DOI 10.1007/978-3-031-25072-9_42
   Wang YY, 2019, INT CONF ACOUST SPEE, P2057, DOI 10.1109/ICASSP.2019.8682456
   Wang ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5314, DOI 10.1145/3503161.3548057
   Wang ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1984, DOI 10.1145/3503161.3548166
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu YS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1604, DOI 10.1109/ICCV48922.2021.00165
   Yan SL, 2023, IEEE T IMAGE PROCESS, V32, P6032, DOI 10.1109/TIP.2023.3327924
   Yang JY, 2022, PROC CVPR IEEE, P15650, DOI 10.1109/CVPR52688.2022.01522
   Zhang Y., 2018, P EUR C COMP VIS ECC, P686
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
NR 44
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6609
EP 6620
DI 10.1109/TMM.2024.3355644
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600048
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, PF
   Xu, TF
   Chen, H
   Zhou, SY
   Qin, HL
   Li, JA
AF Liu, Peifu
   Xu, Tingfa
   Chen, Huan
   Zhou, Shiyun
   Qin, Haolin
   Li, Jianan
TI Spectrum-Driven Mixed-Frequency Network for Hyperspectral Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image edge detection; Feature extraction; Object detection;
   Hyperspectral imaging; Head; Neural networks; Image color analysis;
   Hyperspectral salient object detection (HSOD); mixed-frequency
   attention; spectrum
ID DATASET; MODEL
AB Hyperspectral salient object detection (HSOD) aims to detect spectrally salient objects in hyperspectral images (HSIs). However, existing methods inadequately utilize spectral information by either converting HSIs into false-color images or converging neural networks with clustering. We propose a novel approach that fully leverages the spectral characteristics by extracting two distinct frequency components from the spectrum: low-frequency Spectral Saliency and high-frequency Spectral Edge. The Spectral Saliency approximates the region of salient objects, while the Spectral Edge captures edge information of salient objects. These two complementary components, crucial for HSOD, are derived by computing from the inter-layer spectral angular distance of the Gaussian pyramid and the intra-neighborhood spectral angular gradients, respectively. To effectively utilize this dual-frequency information, we introduce a novel lightweight Spectrum-driven Mixed-frequency Network (SMN). SMN incorporates two parameter-free plug-and-play operators, namely Spectral Saliency Generator and Spectral Edge Operator, to extract the Spectral Saliency and Spectral Edge components from the input HSI independently. Subsequently, the Mixed-frequency Attention module, comprised of two frequency-dependent heads, intelligently combines the embedded features of edge and saliency information, resulting in a mixed-frequency feature representation. Furthermore, a saliency-edge-aware decoder progressively scales up the mixed-frequency feature while preserving rich detail and saliency information for accurate salient object prediction. Extensive experiments conducted on the HS-SOD benchmark and our custom dataset HSOD-BIT demonstrate that our SMN outperforms state-of-the-art methods regarding HSOD performance.
C1 [Liu, Peifu; Xu, Tingfa; Chen, Huan; Zhou, Shiyun; Qin, Haolin; Li, Jianan] Beijing Inst Technol, Beijing 10081, Peoples R China.
   [Xu, Tingfa] Minist Educ China, Key Lab Photoelect Imaging Technol & Syst, Beijing 100081, Peoples R China.
   [Xu, Tingfa] Beijing Inst Technol, Chongqing Innovat Ctr, Big Data & Artificial Intelligence Lab, Chongqing 401151, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology
RP Xu, TF; Li, JA (corresponding author), Beijing Inst Technol, Beijing 10081, Peoples R China.; Xu, TF (corresponding author), Minist Educ China, Key Lab Photoelect Imaging Technol & Syst, Beijing 100081, Peoples R China.; Xu, TF (corresponding author), Beijing Inst Technol, Chongqing Innovat Ctr, Big Data & Artificial Intelligence Lab, Chongqing 401151, Peoples R China.
EM laprf@bit.edu.cn; ciom_xtf1@bit.edu.cn; huanchen@bit.edu.cn;
   zhoushiyun@bit.edu.cn; 3120225333@bit.edu.cn; lijianan@bit.edu.cn
OI Chen, Huan/0000-0003-1965-9107; Liu, Peifu/0000-0003-4018-1478; xu,
   tingfa/0000-0001-5452-2662; qin, hao lin/0000-0001-8569-7430; Zhou,
   Shiyun/0000-0002-5795-1566
FU National Key Scientific Instrument and Equipment Development Project of
   China
FX No Statement Available
CR Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   Arad B, 2016, LECT NOTES COMPUT SC, V9911, P19, DOI 10.1007/978-3-319-46478-7_2
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P742, DOI 10.1109/TIP.2014.2383320
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Cao Y, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P1086, DOI 10.1109/ChinaSIP.2015.7230572
   Cong RM, 2023, IEEE T MULTIMEDIA, V25, P6971, DOI 10.1109/TMM.2022.3216476
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Hassani A, 2023, PROC CVPR IEEE, P6185, DOI 10.1109/CVPR52729.2023.00599
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang C, 2021, AD HOC NETW, V112, DOI 10.1016/j.adhoc.2020.102369
   Imamoglu N, 2019, INT CONF ACOUST SPEE, P2192, DOI 10.1109/ICASSP.2019.8682522
   Imamoglu NR, 2018, INT WORK QUAL MULTIM, P165
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jänicke C, 2020, REMOTE SENS LETT, V11, P1, DOI 10.1080/2150704X.2019.1670518
   Le Moan S, 2013, IEEE J-STARS, V6, P2472, DOI 10.1109/JSTARS.2013.2257989
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2022, DIGIT SIGNAL PROCESS, V126, DOI 10.1016/j.dsp.2022.103461
   Liang J, 2018, PATTERN RECOGN, V76, P476, DOI 10.1016/j.patcog.2017.11.024
   Liang J, 2013, IEEE IMAGE PROC, P2393, DOI 10.1109/ICIP.2013.6738493
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Mnih V, 2014, ADV NEUR IN, V27
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Rosin PL, 2009, PATTERN RECOGN, V42, P2363, DOI 10.1016/j.patcog.2009.04.021
   Su Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5097, DOI 10.1109/ICCV48922.2021.00507
   Sun SY, 2023, IEEE T NEUR NET LEAR, V34, P8679, DOI 10.1109/TNNLS.2022.3152252
   Tu ZZ, 2023, IEEE T MULTIMEDIA, V25, P4163, DOI 10.1109/TMM.2022.3171688
   Vaswani A., 2017, C WORKSHOP NEURAL IN, P6000
   Wang G., 2018, P 13 C IM GRAPH TECH, P359
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wilson TA, 1997, IEEE T GEOSCI REMOTE, V35, P1007, DOI 10.1109/36.602543
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P5346, DOI 10.1109/TCSVT.2022.3144852
   Yao ZJ, 2022, IEEE T MULTIMEDIA, V24, P4236, DOI 10.1109/TMM.2021.3115344
   Yin W., 2016, Trans. of the Assoc. for Computational Linguistics, V4, P259, DOI 10.1162/tacl_a_00097
   Zhang HQ, 2008, PROC SPIE, V6806, DOI 10.1117/12.766703
   Zhang J, 2021, Advances in Neural Information Processing Systems, P15448
   Zhang J., 2020, PROC IEEECVF C COMP, P12546
   Zhang J, 2017, IEEE WINT CONF APPL, P1, DOI 10.1109/WACV.2017.8
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao TY, 2020, 2020 13TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2020), P181, DOI [10.1109/CISP-BMEI51763.2020.9263563, 10.1109/cisp-bmei51763.2020.9263563]
   Zhu H., 2022, arXiv
   Zhuge MC, 2023, IEEE T PATTERN ANAL, V45, P3738, DOI 10.1109/TPAMI.2022.3179526
NR 49
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5296
EP 5310
DI 10.1109/TMM.2023.3331196
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600028
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Lu, YW
   Wong, WK
   Yuan, C
   Lai, ZH
   Li, XL
AF Lu, Yuwu
   Wong, Wai Keung
   Yuan, Chun
   Lai, Zhihui
   Li, Xuelong
TI Low-Rank Correlation Learning for Unsupervised Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Correlation; Training data;
   Electronic mail; Noise measurement; Image color analysis; domain
   adaptation; image classification; low-rank; transfer learning
ID REGRESSION; NETWORKS
AB In unsupervised domain adaptation (UDA), negative transfer is one of the most challenging problems. Due to complex environments, the used domain data are always corrupted by noise or outliers in many applications. If the noisy data are directly used for domain adaptation, the disturbances and negative influence of the noise are also shifted for the target tasks. Thus, preventing disturbances and negative effects caused by noise are key problems in UDA that need to be addressed. In this article, a low-rank correlation learning (LRCL) method is proposed for UDA. In LRCL, the noisy domain data are recovered by low-rank learning; then both domain data are cleaned. Hence, the disturbances and negative effects of the noise are prevented. The maximized correlated features of the clean data from the source and target domains are learned by a novel correlation regularization term in a latent common space. LRCL also reduces the distribution difference of the learned clean source and target data by constructing a reconstruction term, in which the clean target data are linearly represented by the clean source data. To explore the temporal and structural information of the data, we further extend LRCL into a graph case and propose graph LRCL (GLRCL). Extensive experiments have been conducted on several public data benchmarks, and the experimental results demonstrate that our methods can effectively prevent negative transfer and obtain better classification outcomes than other compared approaches.
C1 [Lu, Yuwu] South China Normal Univ, Sch Software, Foshan 528225, Peoples R China.
   [Wong, Wai Keung] Hong Kong Polytech Univ, Sch Fash & Text, Kowloon, Hong Kong, Peoples R China.
   [Wong, Wai Keung] Lab Artificial Intelligence Design, Sci Pk, Hong Kong, Peoples R China.
   [Yuan, Chun] Tsinghua Shenzhen Int Grad Sch, Shenzhen, Peoples R China.
   [Yuan, Chun] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Lai, Zhihui] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518055, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
C3 South China Normal University; Hong Kong Polytechnic University;
   Tsinghua Shenzhen International Graduate School; Peng Cheng Laboratory;
   Shenzhen University; Northwestern Polytechnical University
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Sch Fash & Text, Kowloon, Hong Kong, Peoples R China.
EM luyuwu2008@163.com; calvin.wong@polyu.edu.hk; yuanc@sz.tsinghua.edu.cn;
   lai_zhi_hui@163.com; xuelong_li@nwpu.edu.cn
OI Wong, Wai Keung/0000-0002-5214-7114; Lai, Zhihui/0000-0002-4388-3080
FU National Natural Science Foundation of China
FX No Statement Available
CR Asuncion A, UCI machine learning repository
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen JH, 2022, IEEE T CYBERNETICS, V52, P11491, DOI 10.1109/TCYB.2021.3107292
   Chen ST, 2020, IEEE T NEUR NET LEAR, V31, P5204, DOI 10.1109/TNNLS.2020.2964790
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Deng WX, 2022, IEEE T CYBERNETICS, V52, P10735, DOI 10.1109/TCYB.2021.3065247
   Ding ZM, 2019, IEEE T NEUR NET LEAR, V30, P1768, DOI 10.1109/TNNLS.2018.2874567
   Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Hedegaard L, 2021, IEEE T IMAGE PROCESS, V30, P8619, DOI 10.1109/TIP.2021.3118978
   Hendrycks D., 2018, PROC INT C MACH LEAR, P1
   Jiang WH, 2019, IEEE T KNOWL DATA EN, V31, P561, DOI 10.1109/TKDE.2018.2837085
   Jing MM, 2021, IEEE T CYBERNETICS, V51, P3390, DOI 10.1109/TCYB.2020.2974106
   Krizhevsky A, 2009, Technical Report
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li LM, 2019, IEEE T PATTERN ANAL, V41, P2724, DOI 10.1109/TPAMI.2018.2866846
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167
   Lin Z, 2009, Technical Report (No. UILU-ENG-09-2215
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu HF, 2019, IEEE T KNOWL DATA EN, V31, P799, DOI 10.1109/TKDE.2018.2843342
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Lu YW, 2021, IEEE T MULTIMEDIA, V23, P2056, DOI 10.1109/TMM.2020.3007340
   Lu YW, 2019, IEEE T CYBERNETICS, V49, P1859, DOI 10.1109/TCYB.2018.2815559
   Ma A, 2022, IEEE T NEUR NET LEAR, V33, P6263, DOI 10.1109/TNNLS.2021.3073119
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Makantasis K, 2018, IEEE T GEOSCI REMOTE, V56, P6884, DOI 10.1109/TGRS.2018.2845450
   Mancini M, 2021, IEEE T PATTERN ANAL, V43, P485, DOI 10.1109/TPAMI.2019.2933829
   Meng M, 2022, IEEE T CIRC SYST VID, V32, P5481, DOI 10.1109/TCSVT.2022.3151387
   MERRIS R, 1994, LINEAR ALGEBRA APPL, V198, P143
   Mo DM, 2020, IEEE T MULTIMEDIA, V22, P2873, DOI 10.1109/TMM.2019.2961508
   Nene S.A., 1996, Columbia object image library (COIL-100)
   Nie FP, 2019, IEEE T IMAGE PROCESS, V28, P2378, DOI 10.1109/TIP.2018.2886712
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Ren CX, 2020, IEEE T CYBERNETICS, V50, P821, DOI 10.1109/TCYB.2018.2874219
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Tao JW, 2019, IEEE ACCESS, V7, P145406, DOI 10.1109/ACCESS.2019.2944211
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang W, 2022, IEEE T CIRC SYST VID, V32, P3319, DOI 10.1109/TCSVT.2021.3104835
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yang SJ, 2021, IEEE T MULTIMEDIA, V23, P3124, DOI 10.1109/TMM.2020.3020697
   Yang S, 2022, IEEE T CYBERNETICS, V52, P7464, DOI 10.1109/TCYB.2020.3040763
   Yao T, 2015, PROC CVPR IEEE, P2142, DOI 10.1109/CVPR.2015.7298826
   Youngeun Kim, 2021, IEEE Transactions on Artificial Intelligence, V2, P508, DOI 10.1109/TAI.2021.3110179
   Zhang L, 2019, IEEE T CIRC SYST VID, V29, P1339, DOI 10.1109/TCSVT.2018.2842206
   Zhang L, 2019, IEEE T NEUR NET LEAR, V30, P3759, DOI 10.1109/TNNLS.2019.2899037
   Zhang L, 2016, IEEE T IMAGE PROCESS, V25, P1177, DOI 10.1109/TIP.2016.2516952
   Zhang WC, 2021, IEEE T PATTERN ANAL, V43, P2047, DOI 10.1109/TPAMI.2019.2962476
   Zhang XQ, 2022, IEEE T MULTIMEDIA, V24, P3882, DOI 10.1109/TMM.2021.3109442
   Zhou SJ, 2022, IEEE T MULTIMEDIA, V24, P1210, DOI 10.1109/TMM.2022.3142524
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 60
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4153
EP 4167
DI 10.1109/TMM.2023.3321430
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100035
DA 2024-08-05
ER

PT J
AU Shao, YX
   Zhang, FF
   Xu, CS
AF Shao, Yuxiang
   Zhang, Feifei
   Xu, Changsheng
TI Snippet-to-Prototype Contrastive Consensus Network for Weakly Supervised
   Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive learning; knowledge distillation; weakly-supervised temporal
   action localization
AB Weakly-supervised temporal action localization aims to localize action instances from untrimmed videos with only video-level labels. Due to the lack of frame-wise annotations, most methods embrace a localization-by-classification paradigm. However, the large supervision gap between classification and localization hinders models from obtaining accurate snippet-wise classification sequences and action proposals. We propose a snippet-to-prototype contrastive consensus network (SPCC-Net) to simultaneously generate feature-level and label-level supervision information to narrow the supervision gap between classification and localization. Specifically, the network adopts a two-stream framework incorporating the optical flow and fusion streams to fully leverage the motion and complementary information from multiple modalities. Firstly, the snippet-to-prototype contrast module is executed within each stream to learn prototypes for all categories and contrast them with action snippets to guarantee intra-class compactness and inter-class separability of snippet features. Secondly, for generating accurate label-level supervision information through complementary information of multimodal features, the multi-modality consensus module ensures not only category consistency through knowledge distillation but also semantic consistency through contrastive learning. Finally, we introduce the auxiliary multiple instance learning (MIL) loss to alleviate the issue that existing MIL-based methods only localize sparse discriminative snippets. Extensive experiments are conducted on two public datasets, THUMOS-14 and ActivityNet-1.3, to demonstrate the superior performance of our method over state-of-the-art methods.
C1 [Shao, Yuxiang] Tianjin Univ Technol, Tianjin 300382, Peoples R China.
   [Zhang, Feifei] Tianjin Univ Technol, Sch Comp Sci & Engn, Tianjin 300384, Peoples R China.
   [Zhang, Feifei] Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Tianjin University of Technology; Tianjin University of Technology;
   Tianjin University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
EM yxshao@stud.tjut.edu.cn; feifeizhang@email.tjut.edu.cn;
   csxu@nlpr.ia.ac.cn
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Plan of China
FX No Statement Available
CR Buch S., 2017, BMVC
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao M, 2022, IEEE T IMAGE PROCESS, V31, P5203, DOI 10.1109/TIP.2022.3193752
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen FY, 2021, IEEE T MULTIMEDIA, V23, P3073, DOI 10.1109/TMM.2020.3019710
   Chen M, 2022, LECT NOTES COMPUT SC, V13664, P192, DOI 10.1007/978-3-031-19772-7_12
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen ZY, 2023, IEEE T MULTIMEDIA, V25, P4349, DOI 10.1109/TMM.2022.3174344
   Dou P, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3567828
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao JY, 2022, PROC CVPR IEEE, P19967, DOI 10.1109/CVPR52688.2022.01937
   Gong G., 2020, P IEEECVF C COMPUTER, P9816
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Gutmann A., 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   He B, 2022, PROC CVPR IEEE, P13915, DOI 10.1109/CVPR52688.2022.01355
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hong FT, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1591, DOI 10.1145/3474085.3475298
   Huang LJ, 2022, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR52688.2022.00327
   Huang LJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7982, DOI 10.1109/ICCV48922.2021.00790
   Huang LJ, 2022, IEEE T PATTERN ANAL, V44, P5729, DOI 10.1109/TPAMI.2021.3076172
   Huang YP, 2019, IEEE INT CON MULTI, P1288, DOI 10.1109/ICME.2019.00224
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Islam A, 2021, AAAI CONF ARTIF INTE, V35, P1637
   Ju C, 2023, IEEE T MULTIMEDIA, V25, P6688, DOI 10.1109/TMM.2022.3213478
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kingma D. P., 2014, arXiv
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Li Jingjing, 2022, P IEEE CVF C COMP VI, P19914
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu Y, 2019, PROC CVPR IEEE, P3599, DOI [10.1109/CVPR.2019.00372, 10.1109/CVPR.2019.00726]
   Liu ZY, 2021, AAAI CONF ARTIF INTE, V35, P2233
   Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Luo W, 2021, PROC CVPR IEEE, P9964, DOI 10.1109/CVPR46437.2021.00984
   Ma JW, 2021, PROC CVPR IEEE, P7583, DOI 10.1109/CVPR46437.2021.00750
   Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17
   Moniruzzaman M, 2023, IEEE T CIRC SYST VID, V33, P6939, DOI 10.1109/TCSVT.2023.3272891
   Narayan S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13588, DOI 10.1109/ICCV48922.2021.01335
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Qu SQ, 2021, Arxiv, DOI arXiv:2104.02967
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Snell J, 2017, ADV NEUR IN, V30
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Wang HB, 2016, IEEE T MULTIMEDIA, V18, P1579, DOI 10.1109/TMM.2016.2569412
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang Y, 2023, PROC CVPR IEEE, P18878, DOI 10.1109/CVPR52729.2023.01810
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu YL, 2019, AAAI CONF ARTIF INTE, P9070
   Yang WF, 2023, IEEE T PATTERN ANAL, V45, P5252, DOI 10.1109/TPAMI.2022.3200399
   Yang WF, 2021, PROC CVPR IEEE, P53, DOI 10.1109/CVPR46437.2021.00012
   Yang ZC, 2022, AAAI CONF ARTIF INTE, P3090
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yuan Y, 2019, Arxiv, DOI arXiv:1905.08586
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zhai YH, 2022, IEEE T MULTIMEDIA, V24, P1857, DOI 10.1109/TMM.2021.3073235
   Zhang C, 2021, PROC CVPR IEEE, P16005, DOI 10.1109/CVPR46437.2021.01575
   Zhang SW, 2020, IEEE T MULTIMEDIA, V22, P2610, DOI 10.1109/TMM.2019.2959425
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhekun Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P729, DOI 10.1007/978-3-030-58526-6_43
   Zhou Y, 2021, IEEE T MULTIMEDIA, V23, P4363, DOI 10.1109/TMM.2020.3042077
NR 74
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6717
EP 6729
DI 10.1109/TMM.2024.3355628
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600003
DA 2024-08-05
ER

PT J
AU Shen, QQ
   Xu, TT
   Liang, YS
   Chen, YY
   He, ZY
AF Shen, Qiangqiang
   Xu, Tingting
   Liang, Yongsheng
   Chen, Yongyong
   He, Zhenyu
TI Robust Tensor Recovery for Incomplete Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensors; Clustering methods; Noise reduction; Transforms; Kernel;
   Robustness; Security; Denoising; incomplete multi-view clustering;
   low-rank tensor recovery; tensor completion
ID REPRESENTATION; GRAPH
AB Incomplete multi-view clustering is gaining increased attention owing to its great success in mining underlying information from the missing views. However, the existing approaches still encounter two issues: 1) They generally do not give sufficient consideration to the robustness of incomplete multi-view data with noise; 2) They only exploit the low-rank structures in the intra-view graphs, while the low-rank priors embedded in inter-view graphs are ignored. To this end, we propose a Robust Tensor Recovery for Incomplete Multi-view Clustering (RIMC) method, which transforms the view-missing problem into the tensor graph recovery problem by manipulating the comprehensive low-rank priors. Specifically, RIMC first employs a marginalized denoising operation to construct robust graphs and further builds a tensor graph by stacking these robust graphs. Then, we develop a novel tensor completion to recover the tensor graph by performing comprehensive low-rank priors: low-rank structures in the inter-view graphs (i.e., horizontal and lateral slices); low-rank structures in the intra-view graphs (i.e., frontal slices). Meanwhile, we integrate the tensor completion and spectral clustering to learn a unified indicator matrix. Extensive experiments show the promising performance of our method.
C1 [Shen, Qiangqiang; Liang, Yongsheng] Harbin Inst Technol Shenzhen, Sch Elect & Informat Engn, Shenzhen 518055, Peoples R China.
   [Xu, Tingting; Chen, Yongyong; He, Zhenyu] Harbin Inst Technol Shenzhen, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Chen, Yongyong] Harbin Inst Technol Shenzhen, Guangdong Prov Key Lab Novel Secur Intelligence Te, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology; Harbin
   Institute of Technology
RP Liang, YS (corresponding author), Harbin Inst Technol Shenzhen, Sch Elect & Informat Engn, Shenzhen 518055, Peoples R China.; Chen, YY (corresponding author), Harbin Inst Technol Shenzhen, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM 1120810623@hit.edu.cn; 21S151168@stu.hit.edu.cn; liangys@hit.edu.cn;
   YongyongChen.cn@gmail.com; zhenyuhe@hit.edu.cn
RI ; Chen, yongyong/P-3801-2016
OI Shen, Qiangqiang/0000-0002-3564-6042; Chen, yongyong/0000-0003-1970-1993
FU National Natural Science Foundation of China
FX No Statement Available
CR Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Campello RJGB, 2012, IEEE ACM T COMPUT BI, V9, P1850, DOI 10.1109/TCBB.2012.115
   Chen M., 2012, P 29 INT C MACHINE L, P1627
   Chen MS, 2020, AAAI CONF ARTIF INTE, V34, P3513
   Chen MM, 2015, J MACH LEARN RES, V16, P3849
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P4054, DOI 10.1109/TMM.2021.3112230
   Chen YY, 2021, IEEE T IMAGE PROCESS, V30, P4022, DOI 10.1109/TIP.2021.3068646
   Chung F.R.K., 1997, Spectral Graph Theory, V92
   Guo DY, 2014, INT C PATT RECOG, P3774, DOI 10.1109/ICPR.2014.648
   Guo JP, 2022, IEEE T NEUR NET LEAR, V33, P3157, DOI 10.1109/TNNLS.2021.3071797
   Guo J, 2019, AAAI CONF ARTIF INTE, P118
   Hu ML, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2262
   Huang ZY, 2021, IEEE T IMAGE PROCESS, V30, P5352, DOI 10.1109/TIP.2021.3083072
   Jiang TX, 2020, IEEE T IMAGE PROCESS, V29, P7233, DOI 10.1109/TIP.2020.3000349
   Kang Z, 2022, IEEE T CYBERNETICS, V52, P8976, DOI 10.1109/TCYB.2021.3061660
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Li K, 2022, IEEE T CYBERNETICS, V52, P12734, DOI 10.1109/TCYB.2021.3087746
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Li ZL, 2022, IEEE T IMAGE PROCESS, V31, P2067, DOI 10.1109/TIP.2022.3147046
   Lin YJ, 2023, IEEE T PATTERN ANAL, V45, P4447, DOI 10.1109/TPAMI.2022.3197238
   Lin Z., 2011, ADV NEURAL INFORM PR, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu CH, 2022, AAAI CONF ARTIF INTE, P7542
   Liu JY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2726, DOI 10.1145/3474085.3475379
   Liu XW, 2024, IEEE T PATTERN ANAL, V46, P1412, DOI 10.1109/TPAMI.2021.3116948
   Lu CY, 2019, PROC CVPR IEEE, P5989, DOI 10.1109/CVPR.2019.00615
   Lu CY, 2020, IEEE T PATTERN ANAL, V42, P925, DOI 10.1109/TPAMI.2019.2891760
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Peng X, 2019, PR MACH LEARN RES, V97
   Rübel O, 2010, IEEE ACM T COMPUT BI, V7, P64, DOI 10.1109/TCBB.2008.49
   Shao WX, 2015, LECT NOTES ARTIF INT, V9284, P318, DOI 10.1007/978-3-319-23528-8_20
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tang YQ, 2022, IEEE T MULTIMEDIA, V24, P3920, DOI 10.1109/TMM.2021.3110098
   Tao ZQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2843
   Tao ZQ, 2020, IEEE T NEUR NET LEAR, V31, P600, DOI 10.1109/TNNLS.2019.2906867
   Trivedi P., 2010, NEURAL INF PROCESS S, P1
   Wang H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3677
   Wang LA, 2010, IEEE T KNOWL DATA EN, V22, P1401, DOI 10.1109/TKDE.2009.192
   Wang SW, 2022, PROC CVPR IEEE, P9766, DOI 10.1109/CVPR52688.2022.00955
   Wen J, 2023, IEEE T SYST MAN CY-S, V53, P1136, DOI 10.1109/TSMC.2022.3192635
   Wen J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3753, DOI 10.1145/3394171.3413807
   Wen J, 2021, AAAI CONF ARTIF INTE, V35, P10273
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wen J, 2021, IEEE T CYBERNETICS, V51, P101, DOI 10.1109/TCYB.2020.2987164
   Wen J, 2020, IEEE T CYBERNETICS, V50, P1418, DOI 10.1109/TCYB.2018.2884715
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xia W, 2022, IEEE T CYBERNETICS, V52, P13635, DOI 10.1109/TCYB.2021.3140068
   Yang MX, 2023, IEEE T PATTERN ANAL, V45, P1055, DOI 10.1109/TPAMI.2022.3155499
   Yang MX, 2021, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR46437.2021.00119
   Ye QL, 2022, IEEE T CYBERNETICS, V52, P12745, DOI 10.1109/TCYB.2021.3088519
   Zhan K, 2019, IEEE T KNOWL DATA EN, V31, P1984, DOI 10.1109/TKDE.2018.2872061
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CH, 2022, IEEE T KNOWL DATA EN, V34, P3701, DOI 10.1109/TKDE.2020.3029582
   Zhou P, 2021, IEEE T PATTERN ANAL, V43, P1718, DOI 10.1109/TPAMI.2019.2954874
   Zhu XZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3271
   Zou Q, 2020, BRIEF BIOINFORM, V21, P1, DOI 10.1093/bib/bby090
NR 59
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3856
EP 3870
DI 10.1109/TMM.2023.3321499
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300010
DA 2024-08-05
ER

PT J
AU Tian, C
   Zhou, ZK
   Huang, YQ
   Li, GJ
   He, ZY
AF Tian, Chao
   Zhou, Zikun
   Huang, Yuqing
   Li, Gaojun
   He, Zhenyu
TI Cross-Modality Proposal-Guided Feature Mining for Unregistered
   RGB-Thermal Pedestrian Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modality proposal; feature mining; RGB-thermal (RGB-T);
   unregistered pedestrian detection
ID NETWORK; FUSION
AB RGB-Thermal (RGB-T) pedestrian detection aims to locate pedestrians in RGB-T image pairs to exploit the complementation between the two modalities for improving detection robustness in extreme conditions. Most existing algorithms assume that the RGB-T image pairs are well registered, while in the real world, they are not ideally aligned due to parallax or different field-of-view of the cameras. The pedestrians in misaligned image pairs may be located at different positions in two images, which results in two challenges: 1) how to achieve inter-modality complementation using spatially misaligned RGB-T pedestrian patches and 2) how to recognize unpaired pedestrians at the boundary. To address these issues, we propose a new paradigm for unregistered RGB-T pedestrian detection, which predicts two separate pedestrian locations in RGB and thermal images. Specifically, we propose a cross-modality proposal-guided feature mining (CPFM) mechanism to extract two precise fusion features for representing a pedestrian in the two modalities, even if the given RGB-T image pair is unaligned. It enables us to effectively exploit the complementation between the two modalities. With the CPFM mechanism, we build a two-stream dense detector that predicts two pedestrian locations in the two modalities based on the corresponding fusion features mined by the CPFM mechanism. In addition, we design a data augmentation method, named Homography, to simulate the discrepancy in scales and views between images. We also investigate two non-maximum suppression (NMS) methods for post-processing purposes. Favorable experimental results demonstrate the effectiveness and robustness of our method in addressing unregistered pedestrians with different shifts.
C1 [Tian, Chao; Huang, Yuqing; Li, Gaojun; He, Zhenyu] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Zhou, Zikun; Huang, Yuqing] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory
RP He, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM tianchao@stu.hit.edu.cn; zhouzikunhit@gmail.com;
   22b951033@stu.hit.edu.cn; lee_gaojun@163.com; zhenyuhe@hit.edu.cn
RI Zhou, Zikun/HNI-2670-2023
OI Zhou, Zikun/0000-0002-2687-7762; Tian, Chao/0000-0002-7029-5065; Huang,
   Yuqing/0009-0002-3883-0010
FU National Natural Science Foundation of China
FX No Statement Available
CR Benenson R, 2015, LECT NOTES COMPUT SC, V8926, P613, DOI 10.1007/978-3-319-16181-5_47
   Bilal M, 2017, IEEE T CIRC SYST VID, V27, P2260, DOI 10.1109/TCSVT.2016.2581660
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen L, 2021, IEEE T INTELL TRANSP, V22, P3234, DOI 10.1109/TITS.2020.2993926
   Chen YT, 2022, LECT NOTES COMPUT SC, V13669, P139, DOI 10.1007/978-3-031-20077-9_9
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Führ G, 2017, IEEE T CIRC SYST VID, V27, P1132, DOI 10.1109/TCSVT.2015.2511812
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   González A, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060820
   Guan DY, 2019, INFORM FUSION, V50, P148, DOI 10.1016/j.inffus.2018.11.017
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Herrmann C., 2018, Autonomous Systems: Sensors, Vehicles, Security, and the Internet of Everything, P38
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Kailai Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P787, DOI 10.1007/978-3-030-58523-5_46
   Kim J, 2021, IEEE ROBOT AUTOM LET, V6, P7846, DOI 10.1109/LRA.2021.3099870
   Kim JU, 2022, IEEE T CIRC SYST VID, V32, P1510, DOI 10.1109/TCSVT.2021.3076466
   Koenig D, 2017, IEEE COMPUT SOC CONF, P243, DOI 10.1109/CVPRW.2017.36
   Li CY, 2019, PATTERN RECOGN, V85, P161, DOI 10.1016/j.patcog.2018.08.005
   Li D., 2018, BRIT MACH VISCONF, P1
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li Q, 2023, IEEE T MULTIMEDIA, V25, P3420, DOI 10.1109/TMM.2022.3160589
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S., 2016, 27 BRIT MACH VIS C, P1
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu JS, 2019, ADV NEUR IN, V32
   Mao JY, 2017, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2017.639
   My Kieu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P546, DOI 10.1007/978-3-030-58542-6_33
   Ni Han, 2022, 2022 7th International Conference on Image, Vision and Computing (ICIVC), P75, DOI 10.1109/ICIVC55077.2022.9887331
   Park K, 2018, PATTERN RECOGN, V80, P143, DOI 10.1016/j.patcog.2018.03.007
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren S., 2015, Advances in neural information processing systems, P91
   Wagner J., 2016, ESANN, V587, P509
   Wanchaitanawong N, 2021, PROCEEDINGS OF 17TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA 2021), DOI 10.23919/MVA51890.2021.9511366
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Yang ZH, 2018, IEEE INT VEH SYM, P179, DOI 10.1109/IVS.2018.8500642
   Yuan MX, 2022, LECT NOTES COMPUT SC, V13669, P509, DOI 10.1007/978-3-031-20077-9_30
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang H, 2021, IEEE WINT CONF APPL, P72, DOI 10.1109/WACV48630.2021.00012
   Zhang L, 2019, IEEE I CONF COMP VIS, P5126, DOI 10.1109/ICCV.2019.00523
   Zhang L, 2019, INFORM FUSION, V50, P20, DOI 10.1016/j.inffus.2018.09.015
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zuo FY, 2022, INT J WAVELETS MULTI, V20, DOI 10.1142/S0219691322500199
NR 52
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6449
EP 6461
DI 10.1109/TMM.2024.3350926
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600008
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, JC
   Liu, P
   Liu, JE
   Xu, W
AF Wang, Jiacheng
   Liu, Ping
   Liu, Jingen
   Xu, Wei
TI Text-Guided Eyeglasses Manipulation With Spatial Constraints
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Glass; Faces; Shape; Three-dimensional displays; Training; Modulation;
   Solid modeling; Eyeglasses virtual try-on; Text-guided face attributes
   manipulation; Generative adversarial network
AB Virtual try-on of eyeglasses involves placing eyeglasses of different shapes and styles onto a face image without physically trying them on. While existing methods have shown impressive results, the variety of eyeglasses styles is limited and the interactions are not always intuitive or efficient. To address these limitations, we propose GlassesCLIP, a text-guided eyeglasses manipulation method with spatial constraints, which allows for control of the eyeglasses shape and style based on a binary mask and text, respectively. Specifically, we introduce a mask encoder to extract mask conditions and a modulation module that enables simultaneous injection of text and mask conditions. This design allows for fine-grained control of the eyeglasses' appearance based on both textual descriptions and spatial constraints. Our approach includes a disentangled mapper and a decoupling strategy that preserves irrelevant areas, resulting in better local editing. We employ a two-stage training scheme to handle the different convergence speeds of the various modality conditions, successfully controlling both the shape and style of eyeglasses. Extensive comparison experiments and ablation analyses demonstrate the effectiveness of our approach in achieving diverse eyeglasses styles while preserving irrelevant areas.
C1 [Wang, Jiacheng; Xu, Wei] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Hubei Key Lab Smart Internet Technol, Wuhan 430074, Peoples R China.
   [Liu, Ping] Res Agcy Sci Technol & Res A STAR, Ctr Frontier AI Res CFAR, Singapore 138634, Singapore.
   [Liu, Jingen] Disney Streaming Adv Res, New York, NY 10011 USA.
C3 Huazhong University of Science & Technology; Agency for Science
   Technology & Research (A*STAR)
RP Xu, W (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Hubei Key Lab Smart Internet Technol, Wuhan 430074, Peoples R China.; Liu, P (corresponding author), Res Agcy Sci Technol & Res A STAR, Ctr Frontier AI Res CFAR, Singapore 138634, Singapore.
EM jiacheng@hust.edu.cn; pino.pingliu@gmail.com; jingen.liu@gmail.com;
   xuwei@hust.edu.cn
OI Xu, Wei/0000-0003-4705-7189
FU A*STAR Career Development Funding Award
FX No Statement Available
CR Avrahami O, 2022, PROC CVPR IEEE, P18187, DOI 10.1109/CVPR52688.2022.01767
   Brack M, 2023, Arxiv, DOI arXiv:2301.12247
   Chandramouli P., 2022, PROC 33 BRIT MACH VI
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Gal R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530164
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Harkonen E, 2020, C NEUR INF PROC SYST
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hertz A., 2023, PROC 11 INT C LEARN
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hou XX, 2023, IEEE T MULTIMEDIA, V25, P3409, DOI 10.1109/TMM.2022.3160360
   Hou Xianxu, 2022, arXiv
   Hu BW, 2021, IEEE T CYBERNETICS, V51, P4373, DOI 10.1109/TCYB.2020.2995496
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P4002, DOI 10.1109/TMM.2021.3111501
   Huberman-Spiegelglas I, 2024, Arxiv, DOI arXiv:2304.06140
   Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI [10.1109/cvpr42600.2020.00813, DOI 10.1109/CVPR42600.2020.00813]
   Karras T., 2018, INT C LEARNING REPRE
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Korhonen J, 2012, INT WORK QUAL MULTIM, P37, DOI 10.1109/QoMEX.2012.6263880
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li XY, 2021, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR46437.2021.00853
   Liang LY, 2019, IEEE T MULTIMEDIA, V21, P3068, DOI 10.1109/TMM.2019.2918717
   Liu S, 2023, IEEE T MULTIMEDIA, V25, P4213, DOI 10.1109/TMM.2022.3172548
   Liu Xingchao, 2021, arXiv
   Liu YH, 2023, IEEE T MULTIMEDIA, V25, P3343, DOI 10.1109/TMM.2022.3159115
   Lyu YM, 2023, PROC CVPR IEEE, P6894, DOI 10.1109/CVPR52729.2023.00666
   Marelli Davide, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12662), P460, DOI 10.1007/978-3-030-68790-8_36
   Marelli D, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103832
   Marelli D, 2019, 2019 IEEE 23RD INTERNATIONAL SYMPOSIUM ON CONSUMER TECHNOLOGIES (ISCT), P299, DOI [10.1109/isce.2019.8900979, 10.1109/ISCE.2019.8900979]
   Milanova Mariofanna, 2021, New Approaches for Multidimensional Signal Processing. Proceedings of International Workshop, NAMSP 2020. Smart Innovation, Systems and Technologies (SIST 216), P99, DOI 10.1007/978-981-33-4676-5_7
   Mokady R, 2023, PROC CVPR IEEE, P6038, DOI 10.1109/CVPR52729.2023.00585
   Paiss R, 2022, LECT NOTES COMPUT SC, V13672, P334, DOI 10.1007/978-3-031-19775-8_20
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Plesh R, 2023, PROC CVPR IEEE, P16847, DOI 10.1109/CVPR52729.2023.01616
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Shen Y., 2020, P IEEECVF C COMPUTER, P9243
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Singh K., 2022, PROC 33 BRIT MACH VI
   Smith T., 1931, Trans. of the Opt. Soc, V33, P73, DOI [DOI 10.1088/1475-4878/33/3/301, 10.1088/1475-4878/33/3/301]
   Song J., 2021, PROC 9 INT C LEARN R
   Sun JX, 2022, PROC CVPR IEEE, P18666, DOI 10.1109/CVPR52688.2022.01813
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Tsaban L, 2023, Arxiv, DOI arXiv:2307.00522
   Wang H, 2023, Arxiv, DOI arXiv:2210.00445
   Wang Y., 2021, P IEEE CVF INT C COM, p13 749
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei TY, 2022, PROC CVPR IEEE, P18051, DOI 10.1109/CVPR52688.2022.01754
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xu ZP, 2022, PROC CVPR IEEE, P18208, DOI 10.1109/CVPR52688.2022.01769
   Yu YC, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3637, DOI 10.1145/3503161.3547935
   Yu-Hui Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P243, DOI 10.1007/978-3-030-58526-6_15
   Zhao TC, 2023, Arxiv, DOI arXiv:2207.00221
   Zheng W., 2022, arXiv
   Zhu MR, 2021, INT J COMPUT VISION, V129, P1820, DOI 10.1007/s11263-021-01442-2
   Zhu MR, 2022, IEEE T NEUR NET LEAR, V33, P893, DOI 10.1109/TNNLS.2020.3030536
   Zhu MR, 2019, IEEE T NEUR NET LEAR, V30, P3096, DOI 10.1109/TNNLS.2018.2890018
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhuming Feng, 2018, Procedia Computer Science, V131, P226, DOI 10.1016/j.procs.2018.04.207
NR 63
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4375
EP 4388
DI 10.1109/TMM.2023.3322326
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100019
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, XP
   Liu, M
   Wang, F
   Dai, JH
   Liu, AA
   Wang, YN
AF Wang, Xueping
   Liu, Min
   Wang, Fei
   Dai, Jianhua
   Liu, An-An
   Wang, Yaonan
TI Relation-Preserving Feature Embedding for Unsupervised Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unsupervised learning; relation-preserving feature embedding; person
   re-identification
AB Some unsupervised approaches have been proposed recently for the person re-identification (ReID) problem since annotations of samples across cameras are time-consuming. However, most of these methods focus on the appearance content of the sample itself, and thus seldom take the structure relations among samples into account when learning the feature representation, which would provide a valuable guide for learning the representations of the samples. Thus hard samples may not be well solved due to the limited or even misleading information of the sample itself. To address this issue, in this article, we propose a Relation-Preserving Feature Embedding (RPE) model that leverages structure relations among samples to boost the performance of the unsupervised person ReID methods without requiring any sample annotations. RPE aims at integrating the sample content and the neighborhood structure relations among samples into the learning of feature embeddings by combining the advantages of the autoencoder and graph autoencoder. Specifically, a relation and content information fusion (RCIF) module is proposed to dynamically merge the information from both perspectives of content and relation levels for feature embedding learning. Also, due to the lack of the identity labels of samples, we adopt an adaptive optimization strategy to update the affinity relations among samples instead of the reconstruction of the whole affinity matrix for optimizing the RPE model, which is more suitable for the unsupervised ReID task. Rigorous experiments on three widely-used large-scale benchmarks for person ReID demonstrate the superiority of the proposed method over current state-of-the-art unsupervised methods.
C1 [Wang, Xueping; Dai, Jianhua] Hunan Normal Univ, Coll Informat Sci & Engn, Changsha 410081, Peoples R China.
   [Wang, Xueping; Dai, Jianhua] Hunan Normal Univ, Prov Key Lab Intelligent Comp & Language Informat, Changsha 410081, Peoples R China.
   [Liu, Min; Wang, Fei; Wang, Yaonan] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
   [Liu, Min; Wang, Fei; Wang, Yaonan] Natl Engn Res Ctr RVC, Changsha 410082, Peoples R China.
   [Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Hunan Normal University; Hunan Normal University; Hunan University;
   Tianjin University
RP Liu, M (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.; Liu, M (corresponding author), Natl Engn Res Ctr RVC, Changsha 410082, Peoples R China.
EM wang_xueping@hnu.edu.cn; liu_min@hnu.edu.cn; wang_fei@hnu.edu.cn;
   jhdai@hunnu.edu.cn; anan0422@gmail.com; yaonan@hnu.edu.cn
OI Wang, Xueping/0000-0003-4862-8975; Dai, Jianhua/0000-0003-1459-0833
FU National Natural Science Foundation of China
FX No Statement Available
CR Bo DY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1400, DOI 10.1145/3366423.3380214
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen DP, 2018, LECT NOTES COMPUT SC, V11220, P56, DOI 10.1007/978-3-030-01270-0_4
   Chen GY, 2019, IEEE T IMAGE PROCESS, V28, P4192, DOI 10.1109/TIP.2019.2908062
   Chen Y., 2018, PROC BRIT MACH VIS C, P1
   Cheng JF, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2973
   Cui GQ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P976, DOI 10.1145/3394486.3403140
   Dai Z, 2021, Cluster contrast for unsupervised person re-identification
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Ge Y., 2020, ICLR
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ji Deyi, 2021, P AAAI, V35, P1646
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kipf T., 2016, ARXIV
   Kipf T.N., 2017, INT C LEARN REPR, P1
   LeCun Y., 1990, P 10 INT C PATT REC, V2, P35, DOI [10.1109/icpr.1990.119325, DOI 10.1109/ICPR.1990.119325, 10.1109/ICPR.1990.119325]
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Li M., 2018, EUR C COMPUT VIS, P737
   Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058
   Li YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2115, DOI 10.1145/3343031.3350982
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2020, IEEE T IMAGE PROCESS, V29, P5481, DOI 10.1109/TIP.2020.2982826
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Pan SR, 2020, IEEE T CYBERNETICS, V50, P2475, DOI 10.1109/TCYB.2019.2932096
   Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817
   Rao YM, 2019, INT J COMPUT VISION, V127, P701, DOI 10.1007/s11263-018-1135-x
   Ren LL, 2019, IEEE T IMAGE PROCESS, V28, P4970, DOI 10.1109/TIP.2019.2915655
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sabokrou M, 2019, IEEE I CONF COMP VIS, P8009, DOI 10.1109/ICCV.2019.00810
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Stisen A, 2015, SENSYS'15: PROCEEDINGS OF THE 13TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P127, DOI 10.1145/2809695.2809718
   Sun K, 2020, AAAI CONF ARTIF INTE, V34, P5892
   Tu WX, 2021, AAAI CONF ARTIF INTE, V35, P9978
   Wang C, 2020, IEEE DATA MINING, P571, DOI 10.1109/ICDM50108.2020.00066
   Wang XP, 2021, IEEE T CIRC SYST VID, V31, P4020, DOI 10.1109/TCSVT.2020.3043444
   Wu GL, 2020, AAAI CONF ARTIF INTE, V34, P12362
   Wu JL, 2019, IEEE I CONF COMP VIS, P8320, DOI 10.1109/ICCV.2019.00841
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhou Q, 2021, IEEE T IMAGE PROCESS, V30, P1623, DOI 10.1109/TIP.2019.2914575
NR 54
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 714
EP 723
DI 10.1109/TMM.2023.3270636
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000015
DA 2024-08-05
ER

PT J
AU Wang, Y
   Zhu, L
   Liu, YY
AF Wang, Yun
   Zhu, Lu
   Liu, Yuanyuan
TI CFENet: Boosting Few-Shot Semantic Segmentation With Complementary
   Feature-Enhanced Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot learning; semantic segmentation; few-shot semantic segmentation
ID AGGREGATION
AB Few-shot semantic segmentation aims to extract information from few annotated support images to segment unknown class objects in the query image. Traditional algorithms may produce errors and insufficient feature extraction using multi-layer cosine similarity to extract correlation information, due to the large differences in appearance and posture between novel class objects, as well as the similarity in texture and shape among different categories. To address the above issue, we propose a Complementary Feature-Enhanced Network (CFENet). Specifically, we propose a correlation complementary extraction module (CCEM) to facilitate long-range information interaction between query features and support features in the intermediate layer, which contains detailed information. The generated multi-channel correlation information complements the prior information obtained through cosine similarity comparison. In addition, we propose a multi-branch feature enhancement module to capture long-range dependencies in aggregated features which are composed of prior correlation information and query features. The module effectively suppresses noise in the aggregated features and enhances the query target object feature from both global and local perspectives in a complementary way. Experiments of the network on PASCAL-5(i )and COCO-20(i) datasets validate the effectiveness of our proposed method.
C1 [Wang, Yun; Zhu, Lu; Liu, Yuanyuan] East China Jiaotong Univ, Sch Informat Engn, Nanchang 330013, Peoples R China.
C3 East China Jiaotong University
RP Zhu, L; Liu, YY (corresponding author), East China Jiaotong Univ, Sch Informat Engn, Nanchang 330013, Peoples R China.
EM 3291734351@qq.com; lzhu@ecjtu.edu.cn; luyuanwanwan@163.com
OI Zhu, Lu/0000-0002-9735-3974
FU National Natural Science Foundation of China
FX No Statement Available
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen JC, 2023, IEEE T MULTIMEDIA, V25, P4361, DOI 10.1109/TMM.2022.3174405
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZL, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3271623
   Dosovitskiy A., 2021, INT C LEARN REPRESEN, P1
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan Q, 2022, LECT NOTES COMPUT SC, V13679, P701, DOI 10.1007/978-3-031-19800-7_41
   Farhangi A, 2022, LECT NOTES ARTIF INT, V13280, P447, DOI 10.1007/978-3-031-05933-9_35
   Fei-Fei L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1134, DOI 10.1109/ICCV.2003.1238476
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Iqbal E., 2022, arXiv
   Jankowski Norbert, 2011, Studies in Computational Intelligence., V358
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Koch G., 2015, ICML DEEP LEARN WORK, V2, P1
   Lake B. M., 2013, Advances in neural information processing systems, P2526, DOI DOI 10.5555/2999792.2999894
   Lang CB, 2022, PROC CVPR IEEE, P8047, DOI 10.1109/CVPR52688.2022.00789
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Li YH, 2022, PROC CVPR IEEE, P4794, DOI 10.1109/CVPR52688.2022.00476
   Liang Chen, 2022, P ADV NEUR INF PROC, V35, P31360
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HF, 2023, IEEE T MULTIMEDIA, V25, P8580, DOI 10.1109/TMM.2023.3238521
   Liu J, 2022, PROC CVPR IEEE, P11543, DOI 10.1109/CVPR52688.2022.01126
   Liu N., 2022, Advances in neural information processing systems, V35, P38020
   Liu YW, 2022, PROC CVPR IEEE, P11563, DOI 10.1109/CVPR52688.2022.01128
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P7885, DOI 10.1109/TPAMI.2021.3115815
   Lu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8721, DOI 10.1109/ICCV48922.2021.00862
   Min J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6921, DOI 10.1109/ICCV48922.2021.00686
   Okazawa A, 2022, LECT NOTES COMPUT SC, V13689, P362, DOI 10.1007/978-3-031-19818-2_21
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shaban Amirreza, 2017, BMVC, DOI 10.5244/C.31.167
   Shi XW, 2023, IET IMAGE PROCESS, V17, P204, DOI 10.1049/ipr2.12628
   Shi XY, 2022, LECT NOTES COMPUT SC, V13680, P151, DOI 10.1007/978-3-031-20044-1_9
   Snell J, 2017, ADV NEUR IN, V30
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Vinyals Oriol, 2016, NEURIPS
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xiong ZT, 2022, LECT NOTES COMPUT SC, V13680, P133, DOI 10.1007/978-3-031-20044-1_8
   Yang BY, 2022, IEEE T NEUR NET LEAR, V33, P7141, DOI 10.1109/TNNLS.2021.3084252
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zhang BF, 2021, PROC CVPR IEEE, P8308, DOI 10.1109/CVPR46437.2021.00821
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang GW, 2021, ADV NEUR IN, V34
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou TF, 2022, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR52688.2022.00261
NR 61
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5630
EP 5640
DI 10.1109/TMM.2023.3338088
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600040
DA 2024-08-05
ER

PT J
AU Wang, ZY
   Zhang, YZ
   Liu, Y
   Qin, C
   Coleman, SA
   Kerr, D
AF Wang, Zhenyu
   Zhang, Yunzhou
   Liu, Yan
   Qin, Cao
   Coleman, Sonya A.
   Kerr, Dermot
TI LARNet: Towards Lightweight, Accurate and Real-Time Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Real-time systems; Neurons; Feature extraction;
   Visualization; Computational modeling; Performance evaluation; Context
   gating module; feature fusion; lightweight; saliency backbone network;
   salient object detection
ID VISUAL-ATTENTION; NETWORK; MODEL
AB Salient object detection (SOD) has rapidly developed in recent years, and detection performance has greatly improved. However, the price of these improvements is increasingly complex networks that require more computing resources and sacrifice real-time performance. This makes it difficult to deploy these approaches on devices with limited computing resources (such as mobile phones, embedded platforms, etc.). Considering recently developed lightweight SOD models, their detection and real-time performance are always compromised in demanding practical application scenarios. To solve these problems, we propose a novel lightweight SOD method called LARNet and its corresponding extremely lightweight method LARNet* according to application requirements. These methods balance the relationship between lightweight requirements, detection accuracy and real-time performance. First, we propose a saliency backbone network tailored for SOD, which removes the need for pre-training with ImageNet and effectively reduces feature redundancy. Subsequently, we propose a novel context gating module (CGM), which simulates the physiological mechanism of human brain neurons and visual information processing, and realizes the deep fusion of multi-level features at the global level. Finally, the saliency map is output after fusion of multi-level features. Extensive experiments on popular benchmark datasets demonstrate that the proposed LARNet (LARNet*) achieves 98 (113) FPS on a GPU and 3 (6) FPS on a CPU. With approximately 680 K (90 K) parameters, the model has significant performance advantages over (extremely) lightweight methods, even surpassing some heavyweight models.
C1 [Wang, Zhenyu] Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110819, Peoples R China.
   [Wang, Zhenyu] Tech Univ Munich, D-80333 Munich, Germany.
   [Zhang, Yunzhou; Liu, Yan; Qin, Cao] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
   [Coleman, Sonya A.; Kerr, Dermot] Ulster Univ, Intelligent Syst Res Ctr, Londonderry BT48 7JL, England.
C3 Northeastern University - China; Technical University of Munich;
   Northeastern University - China
RP Zhang, YZ (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
EM 1910652@stu.neu.edu.cn; zhangyunzhou@mail.neu.edu.cn;
   1810630@stu.neu.edu.cn; qincao1994@gmail.com; sa.coleman@ulster.ac.uk;
   d.kerr@ulster.ac.uk
FU National Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Aksac A, 2017, PATTERN RECOGN, V66, P268, DOI 10.1016/j.patcog.2017.01.010
   Chang YF, 2022, IEEE T COMPUT AID D, V41, P2598, DOI 10.1109/TCAD.2021.3108706
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2022, IEEE T PATTERN ANAL, V44, P8006, DOI 10.1109/TPAMI.2021.3107956
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Craye C, 2016, IEEE INT CONF ROBOT, P2303, DOI 10.1109/ICRA.2016.7487379
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fang CW, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3384-y
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Gu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4914, DOI 10.1109/ICCV48922.2021.00489
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang MK, 2023, IEEE T CIRC SYST VID, V33, P6191, DOI 10.1109/TCSVT.2023.3253685
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Huang XM, 2020, IEEE T IMAGE PROCESS, V29, P1384, DOI 10.1109/TIP.2019.2941663
   Huo LN, 2016, PATTERN RECOGN, V49, P162, DOI 10.1016/j.patcog.2015.07.005
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2020, IEEE T IMAGE PROCESS, V29, P2903, DOI 10.1109/TIP.2019.2954209
   Jiang ZL, 2013, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2013.266
   Jin X, 2022, IEEE T CIRC SYST VID, V32, P7632, DOI 10.1109/TCSVT.2022.3180274
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kong YQ, 2022, IEEE T MULTIMEDIA, V24, P1515, DOI 10.1109/TMM.2021.3066775
   Lai QX, 2022, IEEE T IMAGE PROCESS, V31, P3111, DOI 10.1109/TIP.2022.3158064
   Lai QX, 2021, IEEE T MULTIMEDIA, V23, P2086, DOI 10.1109/TMM.2020.3007321
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee WJ, 2005, IEEE T NEUR SYS REH, V13, P186, DOI 10.1109/TNSRE.2005.848686
   Li GY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3145483
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li JX, 2022, IEEE T COGN DEV SYST, V14, P1532, DOI 10.1109/TCDS.2022.3152910
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Li TP, 2022, IEEE T MULTIMEDIA, V24, P492, DOI 10.1109/TMM.2021.3054526
   Li WS, 2022, PROC CVPR IEEE, P773, DOI 10.1109/CVPR52688.2022.00086
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu JJ, 2020, IEEE T IMAGE PROCESS, V29, P8652, DOI 10.1109/TIP.2020.3017352
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu Y, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.106961
   Liu Y, 2022, IMAGE VISION COMPUT, V126, DOI 10.1016/j.imavis.2022.104536
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P6719, DOI 10.1109/TIP.2022.3215887
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P3804, DOI 10.1109/TIP.2021.3065239
   Liu Y, 2021, IEEE T CYBERNETICS, V51, P4439, DOI 10.1109/TCYB.2020.3035613
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Ma MC, 2021, AAAI CONF ARTIF INTE, V35, P2311
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qiu F., 2022, PROC IEEE INT C MULT, P1
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SY, 2022, IEEE T MULTIMEDIA, V24, P128, DOI 10.1109/TMM.2020.3046868
   Wang K, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3105243
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LY, 2022, IEEE T NEUR NET LEAR, V33, P1925, DOI 10.1109/TNNLS.2021.3111019
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang ZC, 2018, IEEE T GEOSCI REMOTE, V56, P1855, DOI 10.1109/TGRS.2017.2769045
   Wang ZY, 2022, NEURAL COMPUT APPL, V34, P11789, DOI 10.1007/s00521-022-07069-9
   Wang ZY, 2021, IMAGE VISION COMPUT, V113, DOI 10.1016/j.imavis.2021.104243
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Wu YH, 2022, IEEE T IMAGE PROCESS, V31, P3125, DOI 10.1109/TIP.2022.3164550
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xu BW, 2021, AAAI CONF ARTIF INTE, V35, P3004
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang GR, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12815
   Yang S, 2021, IEEE T IMAGE PROCESS, V30, P8426, DOI 10.1109/TIP.2021.3113794
   Yao ZJ, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118973
   Yao ZJ, 2022, IEEE T MULTIMEDIA, V24, P4236, DOI 10.1109/TMM.2021.3115344
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang M, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P667, DOI 10.1145/3474085.3475231
   Zhang YY, 2020, IEEE T IMAGE PROCESS, V29, P1536, DOI 10.1109/TIP.2019.2942796
   Zhang YF, 2022, IEEE T MULTIMEDIA, V24, P755, DOI 10.1109/TMM.2021.3058788
   Zhang ZY, 2019, PROC CVPR IEEE, P4101, DOI 10.1109/CVPR.2019.00423
   Zhao FF, 2021, NEURAL COMPUT, V34, P172, DOI 10.1162/neco_a_01448
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhao ZJ, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108120
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou WJ, 2023, IEEE T IMAGE PROCESS, V32, P1329, DOI 10.1109/TIP.2023.3242775
   Zhou WJ, 2021, IEEE T MULTIMEDIA, V23, P3388, DOI 10.1109/TMM.2020.3025166
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhuge MC, 2023, IEEE T PATTERN ANAL, V45, P3738, DOI 10.1109/TPAMI.2022.3179526
NR 99
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5207
EP 5222
DI 10.1109/TMM.2023.3330082
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600032
OA Green Published
DA 2024-08-05
ER

PT J
AU Yang, ZC
   Li, LD
   Yang, YZ
   Li, YQ
   Lin, WS
AF Yang, Zhichao
   Li, Leida
   Yang, Yuzhe
   Li, Yaqian
   Lin, Weisi
TI Multi-Level Transitional Contrast Learning for Personalized Image
   Aesthetics Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Personalized image aesthetics assessment; contrast learning; aesthetic
   preferences
AB Personalized image aesthetics assessment (PIAA) is aimed at modeling the unique aesthetic preferences of individuals, based on which personalized aesthetic scores are predicted. People have different standards for image aesthetics, and accordingly, images rated at the same aesthetic level by different users explicitly reveal their aesthetic preferences. However, previous PIAA models treat each individual as an isolated optimization target, failing to take full advantage of the contrastive information among users. Further, although people's aesthetic preferences are unique, they still share some commonalities, meaning that PIAA models could be built on the basis of generic aesthetics. Motivated by the above facts, this article presents a Multi-level Transitional Contrast Learning (MTCL) framework for PIAA by transiting features from generic aesthetics to personalized aesthetics via contrastive learning. First, a generic image aesthetics assessment network is pre-trained to learn the common aesthetic features. Then, image sets rated to have the same aesthetic levels by different users are employed to learn the differentiated aesthetic features through multiple level-wise contrast learning based on the generic aesthetic features. Finally, a target user's PIAA model is built by integrating generic and differentiated aesthetic features. Extensive experiments on four benchmark PIAA databases demonstrate that the proposed MTCL model outperforms the state-of-the-arts.
C1 [Yang, Zhichao; Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Yang, Yuzhe; Li, Yaqian] OPPO Res Inst, Shanghai 200032, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Xidian University; Nanyang Technological University
RP Li, LD (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
EM yangzhichao@stu.xidian.edu.cn; ldli@xidian.edu.cn; ippllewis@gmail.com;
   liyaqian@oppo.com; wslin@ntu.edu.sg
RI Lin, Wei/D-3353-2012; Lin, Weisi/A-3696-2011
OI Yang, Yuzhe/0000-0001-9098-2105; Lin, Weisi/0000-0001-9866-1947; Yang,
   Zhichao/0009-0008-3398-1286
FU National Natural Science Foundation of China
FX No Statement Available
CR Celona L, 2022, IEEE T IMAGE PROCESS, V31, P5009, DOI 10.1109/TIP.2022.3191853
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen PF, 2023, IEEE T CIRC SYST VID, V33, P2577, DOI 10.1109/TCSVT.2022.3225552
   Chen PF, 2022, IEEE SIGNAL PROC LET, V29, P513, DOI 10.1109/LSP.2022.3145326
   Chen PF, 2022, IEEE T IMAGE PROCESS, V31, P458, DOI 10.1109/TIP.2021.3130536
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cui CR, 2020, INFORM SCIENCES, V512, P780, DOI 10.1016/j.ins.2019.10.011
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Hamzah M., 2012, PROC INT C INNOV MAN, P247
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou JW, 2023, IEEE T MULTIMEDIA, V25, P5263, DOI 10.1109/TMM.2022.3189276
   Hou JW, 2022, IEEE T CIRC SYST VID, V32, P7386, DOI 10.1109/TCSVT.2022.3186307
   Hu K., 2021, P IEEE CVF INT C COM, P7939
   Jin X., 2016, PROC IEEE 8 INT C WI, P1
   Ke JJ, 2023, PROC CVPR IEEE, P10041, DOI 10.1109/CVPR52729.2023.00968
   Kim JH, 2016, ASIAPAC SIGN INFO PR
   Kim WH, 2020, IEEE T AFFECT COMPUT, V11, P493, DOI 10.1109/TAFFC.2018.2809752
   Kingma D. P., 2014, arXiv
   Komodakis N., 2017, PROC INT C LEARN REP, P1
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Le-Khac PH, 2020, IEEE ACCESS, V8, P193907, DOI 10.1109/ACCESS.2020.3031549
   Lee JT, 2019, IEEE ACCESS, V7, P114349, DOI 10.1109/ACCESS.2019.2936289
   Lee JT, 2019, IEEE I CONF COMP VIS, P1191, DOI 10.1109/ICCV.2019.00128
   Li LD, 2023, IEEE T CIRC SYST VID, V33, P4798, DOI 10.1109/TCSVT.2023.3249185
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li Yaohui, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P896, DOI 10.1145/3503161.3548244
   Loui AC, 2003, IEEE T MULTIMEDIA, V5, P390, DOI 10.1109/TMM.2003.814723
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Lv P, 2023, IEEE T MULTIMEDIA, V25, P736, DOI 10.1109/TMM.2021.3130752
   Lv P, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1328, DOI 10.1145/3240508.3240635
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Myers J.L., 2013, Research Design and Statistical Analysis, V3rd ed.
   Ni SJ, 2023, IEEE T MULTIMEDIA, V25, P6836, DOI 10.1109/TMM.2022.3215003
   Paszke A., 2017, PROC INT C NEURAL I
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Sun WT, 2017, IEEE T MULTIMEDIA, V19, P1870, DOI 10.1109/TMM.2017.2688929
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tao L, 2022, IEEE T CIRC SYST VID, V32, P5266, DOI 10.1109/TCSVT.2022.3141051
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang F, 2021, PROC CVPR IEEE, P2495, DOI 10.1109/CVPR46437.2021.00252
   Wang GL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P957
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao T., 2021, PROC INT C LEARN REP, P1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang YZ, 2022, PROC CVPR IEEE, P19829, DOI 10.1109/CVPR52688.2022.01924
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang MR, 2023, IEEE T MULTIMEDIA, V25, P4653, DOI 10.1109/TMM.2022.3180217
   Zhao K, 2023, PROC CVPR IEEE, P22302, DOI 10.1109/CVPR52729.2023.02136
   Zhu HC, 2022, IEEE T CYBERNETICS, V52, P1798, DOI 10.1109/TCYB.2020.2984670
   Zhu HC, 2023, IEEE T MULTIMEDIA, V25, P179, DOI 10.1109/TMM.2021.3123468
NR 60
TC 3
Z9 3
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1944
EP 1956
DI 10.1109/TMM.2023.3290479
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800005
DA 2024-08-05
ER

PT J
AU Zhang, SX
   Yang, C
   Zhu, XB
   Yin, XC
AF Zhang, Shi-Xue
   Yang, Chun
   Zhu, Xiaobin
   Yin, Xu-Cheng
TI Arbitrary Shape Text Detection via Boundary Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Arbitrary shape text detection; boundary proposal; boundary transformer;
   boundary energy
AB In arbitrary shape text detection, locating accurate text boundaries is challenging and non-trivial. Existing methods often suffer from indirect text boundary modeling or complex post-processing. In this article, we systematically present a unified coarse-to-fine framework via boundary learning for arbitrary shape text detection, which can accurately and efficiently locate text boundaries without post-processing. In our method, we explicitly model the text boundary via an innovative iterative boundary transformer in a coarse-to-fine manner. In this way, our method can directly gain accurate text boundaries and abandon complex post-processing to improve efficiency. Specifically, our method mainly consists of a feature extraction backbone, a boundary proposal module, and an iteratively optimized boundary transformer module. The boundary proposal module consisting of multi-layer dilated convolutions will predict important prior information (including classification map, distance field, and direction field) for generating coarse boundary proposals while guiding the boundary transformer's optimization. The boundary transformer module adopts an encoder-decoder structure, in which the encoder is constructed by multi-layer transformer blocks with residual connection while the decoder is a simple multi-layer perceptron network (MLP). Under the guidance of prior information, the boundary transformer module will gradually refine the coarse boundary proposals via iterative boundary deformation. Furthermore, we propose a novel boundary energy loss (BEL) that introduces an energy minimization constraint and an energy monotonically decreasing constraint to further optimize and stabilize the learning of boundary refinement. Extensive experiments on publicly available and challenging datasets demonstrate the state-of-the-art performance and promising efficiency of our method.
C1 [Zhang, Shi-Xue; Yang, Chun; Zhu, Xiaobin; Yin, Xu-Cheng] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
   [Yin, Xu-Cheng] Univ Sci & Technol Beijing, Inst Artificial Intelligence, Beijing 100083, Peoples R China.
   [Yin, Xu-Cheng] USTB EEasyTech Joint Lab Artificial Intelligence, Beijing 100083, Peoples R China.
C3 University of Science & Technology Beijing; University of Science &
   Technology Beijing
RP Zhu, XB (corresponding author), Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
EM zhangshixue111@163.com; chunyang@ustb.edu.cn; zhuxiaobin@ustb.edu.cn;
   xuchengyin@ustb.edu.cn
OI Yin, Xucheng/0000-0003-0023-0220; Zhang, Shi-Xue/0000-0001-7030-1974
FU National Key Research and Development Program of China
FX No Statement Available
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Dai PW, 2021, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR46437.2021.00731
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   He K., 2015, ICCV, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He MH, 2021, PROC CVPR IEEE, P8809, DOI 10.1109/CVPR46437.2021.00870
   He WH, 2018, IEEE T IMAGE PROCESS, V27, P5406, DOI 10.1109/TIP.2018.2855399
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hou JB, 2021, IEEE T INTELL TRANSP, V22, P6890, DOI 10.1109/TITS.2020.2996027
   Hou JB, 2020, IEEE T IMAGE PROCESS, V29, P7904, DOI 10.1109/TIP.2020.3008863
   Jianqiang Wan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9250, DOI 10.1109/CVPR42600.2020.00927
   Kingma D. P., 2014, arXiv
   Liao MH, 2023, IEEE T PATTERN ANAL, V45, P919, DOI 10.1109/TPAMI.2022.3155612
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ling H, 2019, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR.2019.00540
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744
   Liu ZC, 2018, PROC CVPR IEEE, P6936, DOI 10.1109/CVPR.2018.00725
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Minghui Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P706, DOI 10.1007/978-3-030-58621-8_41
   Qiao L, 2020, AAAI CONF ARTIF INTE, V34, P11899
   Sheng Tao, 2021, ADV NEURAL INF PROCE, P335
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Sida Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8530, DOI 10.1109/CVPR42600.2020.00856
   Tang JQ, 2022, PROC CVPR IEEE, P4553, DOI 10.1109/CVPR52688.2022.00452
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   Wang FF, 2023, IEEE T IMAGE PROCESS, V32, P1, DOI 10.1109/TIP.2022.3201467
   Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819
   Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12160
   Wang JY, 2015, J MATH IMAGING VIS, V51, P229, DOI 10.1007/s10851-014-0519-y
   Wang PF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1277, DOI 10.1145/3343031.3350988
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Xue CH, 2018, LECT NOTES COMPUT SC, V11220, P370, DOI 10.1007/978-3-030-01270-0_22
   Ye J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P516
   Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang S.X., 2020, P IEEE CVF C COMP VI, P9699, DOI DOI 10.1109/CVPR42600.2020.00972
   Zhang SX, 2023, IEEE T PATTERN ANAL, V45, P2736, DOI 10.1109/TPAMI.2022.3176122
   Zhang SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1285, DOI 10.1109/ICCV48922.2021.00134
   Zhang SX, 2023, APPL INTELL, V53, P2280, DOI 10.1007/s10489-022-03396-5
   Zhang SX, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3152596
   Zhao MB, 2022, IEEE T IMAGE PROCESS, V31, P5513, DOI 10.1109/TIP.2022.3197987
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu XB, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107619
   Zhu XB, 2019, AAAI CONF ARTIF INTE, P5981, DOI 10.1609/aaai.v33i01.33015981
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
   Zhu YX, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107336
NR 63
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1747
EP 1760
DI 10.1109/TMM.2023.3286657
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800027
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhao, MD
   Qi, XQ
   Hu, ZP
   Li, LC
   Zhang, YQ
   Huang, Z
   Yu, X
AF Zhao, Minda
   Qi, Xingqun
   Hu, Zhipeng
   Li, Lincheng
   Zhang, Yongqiang
   Huang, Zi
   Yu, Xin
TI Calligraphy Font Generation via Explicitly Modeling Location-Aware Glyph
   Component Deformations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative adversarial networks; image processing; image synthesis;
   image transformation
AB Automatic font generation is a challenging and time-consuming task, particularly in languages that consist of large amounts of characters with complicated structures. Typical component-wise font generation methods decompose the source character into components and search for them from the reference glyph set as candidate components. These candidate components are then utilized to learn the local styles of the target glyph. However, these methods overlook that the same component at different locations may have different profiles. When the candidate components locate differently from their corresponding components in the target glyph, the style of a generated glyph will look inconsistent. It is observed that for arbitrary components at two specific locations, the deformation patterns are similar. Driven by this, we present a location-aware component-deformable font generation method. Specifically, we search for candidate components and their corresponding deformative component pairs from the reference glyph set. Each deformative component pair can accurately depict how to deform the candidate component to the desired profile in the target glyph. Hence, we introduce a location-dependent deformation module to perform component warping. In this way, we significantly improve the component deformation ability. Lastly, we integrate deformed components into target glyphs while enforcing their styles to be consistent with the reference ones. Extensive experiments demonstrate that our method produces target-font consistent glyphs and outperforms the state-of-the-art on both seen and unseen fonts.
C1 [Zhao, Minda; Qi, Xingqun; Hu, Zhipeng; Li, Lincheng; Zhang, Yongqiang] Netease Inc, Fuxi AI Lab, Hangzhou 310052, Peoples R China.
   [Huang, Zi; Yu, Xin] Univ Queensland, Brisbane, QLD 4072, Australia.
C3 University of Queensland
RP Li, LC (corresponding author), Netease Inc, Fuxi AI Lab, Hangzhou 310052, Peoples R China.
EM zhaominda01@corp.netease.com; xingqunqi@gmail.com;
   zphu@corp.netease.com; lilincheng@corp.netease.com;
   zhangyongqiang02@corp.netease.com; huang@itee.uq.edu.au;
   xin.yu@uq.edu.au
RI li, lincheng/AAO-4355-2020
OI li, lincheng/0000-0003-3774-492X; HUANG, ZI/0000-0002-9738-4949; Yu,
   Xin/0000-0002-0269-5649; Qi, Xingqun/0000-0002-9772-5707
CR Cha J., 2020, P ECCV, P735, DOI DOI 10.1007/978-3-030-58529-7_43
   Chefer H, 2021, PROC CVPR IEEE, P782, DOI 10.1109/CVPR46437.2021.00084
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Fu B, 2023, PROC CVPR IEEE, P22438, DOI 10.1109/CVPR52729.2023.02149
   Gao YM, 2020, AAAI CONF ARTIF INTE, V34, P646
   Gao Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356574
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HuANG Y., 2020, EUROPEAN C COMPUTER, V12351, P156, DOI [10.1007/, DOI 10.1007/978-3-030-58539-6_10]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang Y., PROC SIGGRAPH ASIA20, P1
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim Taeksoo, 2017, P 34 INT C MACHINE L, P1857, DOI DOI 10.1109/WPT.2017.7953894
   Kong YX, 2022, PROC CVPR IEEE, P13472, DOI 10.1109/CVPR52688.2022.01312
   Li S, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3866, DOI 10.1145/3394171.3413995
   Lian ZH, 2022, COMPUT GRAPH FORUM, V41, P212, DOI 10.1111/cgf.14580
   Linardatos P, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23010018
   Liu AH, 2018, ADV NEUR IN, V31
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu W, 2022, PROC CVPR IEEE, P7895, DOI 10.1109/CVPR52688.2022.00775
   Liu YT, 2023, PATTERN RECOGN, V141, DOI 10.1016/j.patcog.2023.109593
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mok TCW, 2022, PROC CVPR IEEE, P20803, DOI 10.1109/CVPR52688.2022.02017
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Park S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13880, DOI 10.1109/ICCV48922.2021.01364
   Park S, 2021, AAAI CONF ARTIF INTE, V35, P2393
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Chen TQ, 2016, Arxiv, DOI arXiv:1612.04337
   Saharia C., 2022, PROC ACM SIGGRAPH C, P1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun DH, 2018, INT C PATT RECOG, P976, DOI 10.1109/ICPR.2018.8545701
   Tang LC, 2022, PROC CVPR IEEE, P7885, DOI 10.1109/CVPR52688.2022.00774
   TIAN Y., 2017, ZI2ZI MASTER CHINESE
   Wang C, 2023, PROC CVPR IEEE, P1858, DOI 10.1109/CVPR52729.2023.00185
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen C, 2021, IEEE WINT CONF APPL, P3881, DOI 10.1109/WACV48630.2021.00393
   Wen Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P621, DOI 10.1145/3474085.3475225
   Wolleb J., 2022, arXiv
   Wu SJ, 2020, Arxiv, DOI arXiv:2005.12500
   Xie YC, 2021, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR46437.2021.00509
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu X., 2019, Adv. Neural Inf. Process. Syst, V32, P2994
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YX, 2018, PROC CVPR IEEE, P8447, DOI 10.1109/CVPR.2018.00881
   Zhao M., 2022, Advances in Neural Information Processing Systems, V35, P3609
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 51
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5939
EP 5950
DI 10.1109/TMM.2023.3342690
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100021
DA 2024-08-05
ER

PT J
AU Zhao, YQ
   Teng, QZ
   Chen, HG
   Zhang, SJ
   He, XH
   Li, Y
   Sheriff, RE
AF Zhao, Yaoqian
   Teng, Qizhi
   Chen, Honggang
   Zhang, Shujiang
   He, Xiaohai
   Li, Yi
   Sheriff, Ray E.
TI Activating More Information in Arbitrary-Scale Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Super-resolution; arbitrary-scale; scale-aware; local feature
   adaptation; dynamic convolution; deformable convolution
ID INTERPOLATION; NETWORK
AB Single-image super-resolution (SISR) has experienced vigorous growth with the rapid development of deep learning. However, handling arbitrary scales (e.g., integers, non-integers, or asymmetric) using a single model remains a challenging task. Existing super-resolution (SR) networks commonly employ static convolutions during feature extraction, which cannot effectively perceive changes in scales. Moreover, these continuous-scale upsampling modules only utilize the scale factors, without considering the diversity of local features. To activate more information for better reconstruction, two plug-in and compatible modules for fixed-scale networks are designed to perform arbitrary-scale SR tasks. Firstly, we design a Scale-aware Local Feature Adaptation Module (SLFAM), which adaptively adjusts the attention weights of dynamic filters based on the local features and scales. It enables the network to possess stronger representation capabilities. Then we propose a Local Feature Adaptation Upsampling Module (LFAUM), which combines scales and local features to perform arbitrary-scale reconstruction. It allows the upsampling to adapt to local structures. Besides, deformable convolution is utilized letting more information to be activated in the reconstruction, enabling the network to better adapt to the texture features. Extensive experiments on various benchmark datasets demonstrate that integrating the proposed modules into a fixed-scale SR network enables it to achieve satisfactory results with non-integer or asymmetric scales while maintaining advanced performance with integer scales.
C1 [Zhao, Yaoqian; Teng, Qizhi; Chen, Honggang; He, Xiaohai] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
   [Zhao, Yaoqian] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
   [Chen, Honggang] Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.
   [Chen, Honggang] Yunan Univ, Yunnan Key Lab Software Engn, Kunming 650600, Peoples R China.
   [Zhang, Shujiang] Enjoyor Technol Co Ltd, Zhejiang Intelligent Transportat Engn Technol Res, Hangzhou 311400, Peoples R China.
   [Li, Yi] DI Sinma Sichuan Machinery Co Ltd, Suining 629200, Peoples R China.
   [Sheriff, Ray E.] Edge Hill Univ, Dept Comp Sci, Ormskirk L39 4QP, England.
C3 Sichuan University; Guangxi Normal University; Tianjin University of
   Technology; Yunnan University; Edge Hill University
RP Chen, HG (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
EM 244716386@qq.com; qzteng@scu.edu.cn; honggang_chen@scu.edu.cn;
   34209310@qq.com; hxh@scu.edu.cn; 943787101@qq.com;
   sheriffr@edgehill.ac.uk
OI Sheriff, Ray/0000-0003-4143-692X; teng, qizhi/0000-0002-5462-683X
FU National Natural Science Foundation of China
FX No Statement Available
CR Agarwal A., 2021, PROC IEEECVF INT WOR, P1
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Anwar S, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3390462
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Chantas G, 2021, IEEE T IMAGE PROCESS, V30, P838, DOI 10.1109/TIP.2020.3038521
   Chen HW, 2023, PROC CVPR IEEE, P18257, DOI 10.1109/CVPR52729.2023.01751
   Chen HG, 2022, INFORM FUSION, V79, P124, DOI 10.1016/j.inffus.2021.09.005
   Chen HG, 2017, IEEE T MULTIMEDIA, V19, P1702, DOI 10.1109/TMM.2017.2688920
   Chen KY, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3272473
   Chen XY, 2023, PROC CVPR IEEE, P22367, DOI 10.1109/CVPR52729.2023.02142
   Chen YB, 2021, PROC CVPR IEEE, P8624, DOI 10.1109/CVPR46437.2021.00852
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   Fu Y, 2021, NEUROCOMPUTING, V427, P201, DOI 10.1016/j.neucom.2020.11.010
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P1505, DOI 10.1109/TMM.2023.3240880
   Georgescu MI, 2023, IEEE WINT CONF APPL, P2194, DOI 10.1109/WACV56688.2023.00223
   Gu JJ, 2021, PROC CVPR IEEE, P9195, DOI 10.1109/CVPR46437.2021.00908
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D., 2015, P INT C LEARN REPR S, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Lee J, 2022, PROC CVPR IEEE, P1928, DOI 10.1109/CVPR52688.2022.00197
   Lemke C, 2015, ARTIF INTELL REV, V44, P117, DOI 10.1007/s10462-013-9406-y
   Li F, 2023, IEEE T MULTIMEDIA, V25, P2825, DOI 10.1109/TMM.2022.3152090
   Li WJ, 2024, IEEE T MULTIMEDIA, V26, P864, DOI 10.1109/TMM.2023.3272474
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu YQ, 2022, IEEE T MULTIMEDIA, V24, P2259, DOI 10.1109/TMM.2021.3078615
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Moser BB, 2023, IEEE T PATTERN ANAL, V45, P9862, DOI 10.1109/TPAMI.2023.3243794
   Romano Y, 2014, IEEE T IMAGE PROCESS, V23, P3085, DOI 10.1109/TIP.2014.2325774
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Wang H, 2023, PROC CVPR IEEE, P22378, DOI 10.1109/CVPR52729.2023.02143
   Wang LG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4781, DOI 10.1109/ICCV48922.2021.00476
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HL, 2023, IEEE T GEOSCI REMOTE, V61, DOI [10.1109/TGRS.2023.3304297, 10.1109/TGRS.2023.3240254]
   Yang HJ, 2023, APPL INTELL, V53, P20891, DOI 10.1007/s10489-023-04566-9
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang DY, 2021, IEEE T MULTIMEDIA, V23, P2172, DOI 10.1109/TMM.2020.3008041
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang YF, 2022, KNOWL-BASED SYST, V249, DOI 10.1016/j.knosys.2022.108984
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu SY, 2017, IEEE SIGNAL PROC LET, V24, P1178, DOI 10.1109/LSP.2017.2711609
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 56
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7946
EP 7961
DI 10.1109/TMM.2024.3373257
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000067
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhu, SM
   Zou, XX
   Qian, JJ
   Wong, WK
AF Zhu, Shumin
   Zou, Xingxing
   Qian, Jianjun
   Wong, Wai Keung
TI Learning Structured Relation Embeddings for Fine-Grained Fashion
   Attribute Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Image recognition; Feature extraction; Task analysis;
   Semantics; Transformers; Image segmentation; Fashion attribute
   recognition; multi-relational embeddings; structured-aware embedding;
   attribute attention; spatial attention; channel attention
ID STYLE
AB Fashion attribute recognition is a not-new topic, but rather a core task in understanding fashion from the perspective of computer vision. This article proposes a structured relation-aware network (sRA-Net), which exploits multiple hidden relations in fashion images to enrich and achieve accurate attribute representations to boost the performance of fashion attribute recognition. Specifically, it deconstructs the features of a clothing fashion item into three levels, including low-level attribute-related image region information, mid-level attribute dependency information, and high-level clothing look information. To learn these multi-relational embeddings, we present three relation-aware attention mechanisms. The attribute attention mechanism describes the relationship among different attribute vectors through self-attention and uses the attention map to update the attribute embedding. Then, the spatial attention mechanism associates the attribute with the image features and enhances the attribute embedding by leveraging the attribute-related image region. Finally, the channel attention mechanism selects attribute-related image feature channels to obtain a more fine-grained attribute embedding. Furthermore, we introduce structure-aware embedding to constrain attribute recognition in images from a global perspective by identifying the inner structure of the clothing. Without bells and whistles, sRA-Net outperforms all state-of-the-art attribute recognition methods on two mainstream fashion attribute datasets, namely the DeepFashion-C dataset and iFashion-Attribute dataset, with over 1%-3% improvement.
C1 [Zhu, Shumin; Zou, Xingxing; Wong, Wai Keung] Hong Kong Polytech Univ, Sch Fash & Text, Kowloon, Hong Kong, Peoples R China.
   [Zou, Xingxing; Wong, Wai Keung] Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
   [Qian, Jianjun] Nanjing Univ Sci & Technol, PCA Lab, Key Lab Intelligent Percept & Syst High Dimens Inf, Minist Educ, Nanjing 210094, Peoples R China.
C3 Hong Kong Polytechnic University; Nanjing University of Science &
   Technology
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Sch Fash & Text, Kowloon, Hong Kong, Peoples R China.; Wong, WK (corresponding author), Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
EM shumin.zhu@connect.polyu.hk; aemika.zou@connect.polyu.hk;
   csjqian@njust.edu.cn; calvin.wong@polyu.edu.hk
OI zou, xingxing/0000-0002-0336-3734; Zhu, Shumin/0000-0003-4050-9592; Lai,
   Zhihui/0000-0002-4388-3080; Wong, Wai Keung/0000-0002-5214-7114; Qian,
   Jianjun/0000-0002-0968-8556
FU Laboratory for Artificial Intelligence in Design
FX No Statement Available
CR Ak KE, 2018, PROC CVPR IEEE, P7708, DOI 10.1109/CVPR.2018.00804
   Ak KE, 2018, IEEE WINT CONF APPL, P1671, DOI 10.1109/WACV.2018.00186
   Al-Halah Z, 2021, IEEE T MULTIMEDIA, V23, P4143, DOI 10.1109/TMM.2020.3037459
   Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Di W, 2013, IEEE COMPUT SOC CONF, P8, DOI 10.1109/CVPRW.2013.6
   Dong JF, 2021, IEEE T IMAGE PROCESS, V30, P8410, DOI 10.1109/TIP.2021.3115658
   Elisseeff A, 2002, ADV NEUR IN, V14, P681
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   Guo S, 2019, IEEE INT CONF COMP V, P3113, DOI 10.1109/ICCVW.2019.00377
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hidayati SC, 2021, IEEE T MULTIMEDIA, V23, P365, DOI 10.1109/TMM.2020.2980195
   Hou YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12127, DOI 10.1109/ICCV48922.2021.01193
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Jing PG, 2022, IEEE T MULTIMEDIA, V24, P1277, DOI 10.1109/TMM.2021.3062736
   Jing PG, 2020, IEEE T MULTIMEDIA, V22, P1555, DOI 10.1109/TMM.2019.2944749
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Laenen K., 2022, Ph.D. dissertation
   Laenen K., 2017, PROC ACM KDD WORKSHO, P1
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lanchantin J, 2021, PROC CVPR IEEE, P16473, DOI 10.1109/CVPR46437.2021.01621
   Lang YN, 2020, PROC CVPR IEEE, P2592, DOI 10.1109/CVPR42600.2020.00267
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu X, 2021, IEEE T MULTIMEDIA, V23, P2894, DOI 10.1109/TMM.2020.3018021
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mall U, 2019, IEEE I CONF COMP VIS, P411, DOI 10.1109/ICCV.2019.00050
   Menglin Jia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P316, DOI 10.1007/978-3-030-58452-8_19
   Murtaza M, 2022, MATH METHOD APPL SCI, DOI 10.1002/mma.8197
   Parekh V, 2021, IEEE COMPUT SOC CONF, P3968, DOI 10.1109/CVPRW53098.2021.00447
   Qi G.-J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tsoumakas G., 2007, Int. J. Data Warehousing Mining, V3, P681
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang Xianwang., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM'11, P1353
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Xu SL, 2022, LECT NOTES COMPUT SC, V13697, P545, DOI 10.1007/978-3-031-19836-6_31
   Yang M., 2021, PROC INT C NEURAL IN, P186
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Yuan YF, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P839, DOI 10.1145/3404835.3462881
   Yuwei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13535, DOI 10.1109/CVPR42600.2020.01355
   Zhan HJ, 2022, IEEE T MULTIMEDIA, V24, P819, DOI 10.1109/TMM.2021.3059514
   Zhang SY, 2018, NEUROCOMPUTING, V282, P98, DOI 10.1016/j.neucom.2017.12.027
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
   Zhao XJ, 2022, ADV ENG INFORM, V51, DOI 10.1016/j.aei.2021.101447
   Zhou W, 2019, J VIS COMMUN IMAGE R, V61, P112, DOI 10.1016/j.jvcir.2019.03.003
   Zhu D., 2022, Human-Intell. Syst. Integration, V4, P1
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zou XX, 2019, IEEE COMPUT SOC CONF, P296, DOI 10.1109/CVPRW.2019.00039
NR 65
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1652
EP 1664
DI 10.1109/TMM.2023.3284593
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800007
DA 2024-08-05
ER

PT J
AU Zhuang, K
   Li, Q
   Yuan, Y
   Wang, Q
AF Zhuang, Kai
   Li, Qiang
   Yuan, Yuan
   Wang, Qi
TI Multi-Domain Adaptation for Motion Deblurring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion deblurring; domain transfer strategy; multi-domain dataset; meta
   deblurring
AB Motion deblurring is an important topic in the field of image enhancement, which has widespread applications including video surveillance, object detection, etc. Many algorithms are designed for motion deblurring and achieve remarkable performance. However, mainstream motion blur datasets are collected under normal weather and illuminance conditions, i.e., normal domain, ignoring their variations. As a result, current methods perform poorly in dynamic real-world scenes. To address these issues, we study the work in two aspects. First, we collect the real-world motion blur dataset with a well-designed collection device from various angles, focal lengths, and street scenes. Considering its domain is single, it is augmented via a Domain Transfer Strategy (DTS) to construct a Multi-Domain dataset (MD dataset), expanding the domains of the collected dataset. Second, we propose a Multi-Domain Adaptive Deblur Network (MDADNet) with two modules. The one is the Domain Adaptation (DA) module that exploits domain invariant features to stabilize the performance of the MDADNet in multiple domains. The other is the Meta Deblurring (MDB) module that employs the auxiliary branch to enhance the deblurring ability. It also enables the MDADNet to update parameters during the testing stage, improving the generalizations of the MDADNet. Extensive experimental results demonstrate that the MD-trained methods significantly strengthen the motion deblurring ability in multiple domains. Particularly, the proposed MDADNet achieves state-of-the-art performance on the MD dataset and public motion blur datasets.
C1 [Zhuang, Kai] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Li, Qiang; Yuan, Yuan; Wang, Qi] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
EM zhuangkai@mail.nwpu.edu.cn; liqmges@gmail.com; y.yuan1.ieee@gmail.com;
   crabwq@gmail.com
RI Li, Qiang/AAR-9738-2021
OI Li, Qiang/0000-0002-6736-3389; Wang, Qi/0000-0002-7028-4956
FU National Natural Science Foundation of China
FX No Statement Available
CR Belghazi MI, 2018, PR MACH LEARN RES, V80
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chi ZX, 2021, PROC CVPR IEEE, P9133, DOI 10.1109/CVPR46437.2021.00902
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gong D, 2017, PROC CVPR IEEE, P3806, DOI 10.1109/CVPR.2017.405
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jaesung Rim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P184, DOI 10.1007/978-3-030-58595-2_12
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Li JC, 2021, IEEE T MULTIMEDIA, V23, P2986, DOI 10.1109/TMM.2021.3068561
   Li Q, 2022, IEEE T IMAGE PROCESS, V31, P7252, DOI 10.1109/TIP.2022.3221287
   Li Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3203749
   Liang CH, 2022, IEEE T MULTIMEDIA, V24, P61, DOI 10.1109/TMM.2020.3045303
   Liu S, 2019, ADV NEUR IN, V32
   Liu Y, 2022, IEEE T MULTIMEDIA, V24, P2890, DOI 10.1109/TMM.2021.3090206
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao XT, 2022, Arxiv, DOI [arXiv:2111.11745, DOI 10.48550/ARXIV.2111.11745]
   Masoumian A, 2023, NEUROCOMPUTING, V517, P81, DOI 10.1016/j.neucom.2022.10.073
   Min C, 2018, IEEE ACCESS, V6, P69242, DOI 10.1109/ACCESS.2018.2880279
   Mukherjee S, 2023, IEEE T MED IMAGING, V42, P42, DOI 10.1109/TMI.2022.3203022
   Mukherjee S, 2021, I S BIOMED IMAGING, P1869, DOI 10.1109/ISBI48211.2021.9433864
   Nah S, 2021, IEEE COMPUT SOC CONF, P149, DOI 10.1109/CVPRW53098.2021.00025
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Tran P, 2021, PROC CVPR IEEE, P11951, DOI 10.1109/CVPR46437.2021.01178
   Pizzati F, 2021, PROC CVPR IEEE, P14283, DOI 10.1109/CVPR46437.2021.01406
   Ruder S, 2017, Arxiv, DOI arXiv:1609.04747
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887
   Shen ZY, 2019, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2019.00567
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wu A, 2022, IEEE T PATTERN ANAL, V44, P4178, DOI 10.1109/TPAMI.2021.3060446
   Xu L., 2014, Advances in NIPS 27, V27, P1790
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734
   Yuan Y, 2020, PROC CVPR IEEE, P3552, DOI 10.1109/CVPR42600.2020.00361
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang XX, 2022, PROC CVPR IEEE, P4900, DOI 10.1109/CVPR52688.2022.00486
   Zhang XY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1448, DOI 10.1109/ICASSP.2018.8462601
   Zhang Y, 2013, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2013.145
   Zhong ZH, 2021, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR46437.2021.00910
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou WB, 2021, IEEE INT CONF COMP V, P1895, DOI 10.1109/ICCVW54120.2021.00216
NR 58
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3676
EP 3688
DI 10.1109/TMM.2023.3314154
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200044
DA 2024-08-05
ER

PT J
AU Agnolucci, L
   Galteri, L
   Bertini, M
   Bimbo, AD
AF Agnolucci, Lorenzo
   Galteri, Leonardo
   Bertini, Marco
   Bimbo, Alberto Del
TI Perceptual Quality Improvement in Videoconferencing Using
   Keyframes-Based GAN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video conferencing; Video restoration; generative adversarial networks;
   videoconferencing
ID IMAGE
AB In the latest years, videoconferencing has taken a fundamental role in interpersonal relations, both for personal and business purposes. Lossy video compression algorithms are the enabling technology for videoconferencing, as they reduce the bandwidth required for real-time video streaming. However, lossy video compression decreases the perceived visual quality. Thus, many techniques for reducing compression artifacts and improving video visual quality have been proposed in recent years. In this work, we propose a novel GAN-based method for compression artifacts reduction in videoconferencing. Given that, in this context, the speaker is typically in front of the camera and remains the same for the entire duration of the transmission, we can maintain a set of reference keyframes of the person from the higher-quality I-frames that are transmitted within the video stream and exploit them to guide the visual quality improvement; a novel aspect of this approach is the update policy that maintains and updates a compact and effective set of reference keyframes. First, we extract multi-scale features from the compressed and reference frames. Then, our architecture combines these features in a progressive manner according to facial landmarks. This allows the restoration of the high-frequency details lost after the video compression. Experiments show that the proposed approach improves visual quality and generates photo-realistic results even with high compression rates.
C1 [Agnolucci, Lorenzo; Bertini, Marco; Bimbo, Alberto Del] Univ Florence, Dept Informat Engn MICC, I-50139 Florence, Italy.
   [Galteri, Leonardo] Univ Firenze, MICC, I-50134 Florence, Italy.
C3 University of Florence; University of Florence
RP Agnolucci, L (corresponding author), Univ Florence, Dept Informat Engn MICC, I-50139 Florence, Italy.
EM lorenzo.agnolucci@unifi.it; leonardo.galteri@unifi.it;
   marco.bertini@unifi.it; alberto.delbimbo@unifi.it
RI Bertini, Marco/ABF-2564-2020; Galteri, Leonardo/HSI-0092-2023;
   Agnolucci, Lorenzo/KFA-0244-2024
OI Bertini, Marco/0000-0003-3010-855X; Galteri,
   Leonardo/0000-0002-7247-9407; DEL BIMBO, ALBERTO/0000-0002-1052-8322;
   Agnolucci, Lorenzo/0000-0002-9558-1287; Bertini,
   Marco/0000-0002-1364-218X
FU European Commission
FX No Statement Available
CR [Anonymous], 2012, ITU-R BT.500-13
   [Anonymous], 2016, Journal of WSCG
   Bellard F., FFMPEG
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chen CF, 2021, PROC CVPR IEEE, P11891, DOI 10.1109/CVPR46437.2021.01172
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Dogan B, 2019, IEEE COMPUT SOC CONF, P1814, DOI 10.1109/CVPRW.2019.00232
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Doukas MC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14378, DOI 10.1109/ICCV48922.2021.01413
   Fischer Asja, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P14, DOI 10.1007/978-3-642-33275-3_2
   Galteri L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P862, DOI 10.1145/3394171.3413508
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2017, Arxiv, DOI [arXiv:1701.00160, 10.48550/arXiv.1701.00160, DOI 10.48550/ARXIV.1701.00160]
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Goring S., 2021, PROC IEEE 23 INT WOR, P1
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Guo YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3947, DOI 10.1145/3394171.3413709
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Lim JH, 2017, Arxiv, DOI [arXiv:1705.02894, 10.48550/arXiv.1705.02894, DOI 10.48550/ARXIV.1705.02894]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kingma D.P., 2014, Proc. of ICLR
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Ko H, 2020, IEEE T IMAGE PROCESS, V29, P5964, DOI 10.1109/TIP.2020.2987180
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li J., 2021, Universal face restoration with memorized modulation
   Li XM, 2020, PROC CVPR IEEE, P2703, DOI 10.1109/CVPR42600.2020.00278
   Li XM, 2018, LECT NOTES COMPUT SC, V11217, P278, DOI 10.1007/978-3-030-01261-8_17
   Li X, 2019, PROC INT C TOOLS ART, P524, DOI 10.1109/ICTAI.2019.00079
   Li Zhi, 2020, Toward a better quality metric for the video community
   Li Z, 2016, SCI REP-UK, V6, DOI 10.1038/srep30338
   Liu MY, 2021, P IEEE, V109, P839, DOI 10.1109/JPROC.2021.3049196
   Liu T, 2022, IEEE T CIRC SYST VID, V32, P2881, DOI 10.1109/TCSVT.2021.3103519
   Madhusudana PC, 2022, IEEE T IMAGE PROCESS, V31, P4149, DOI 10.1109/TIP.2022.3181496
   Maleki D., 2018, P IEEE C COMPUTER VI, P2555
   Mao XJ, 2016, ADV NEUR IN, V29
   Mittal A, 2011, CONF REC ASILOMAR C, P723, DOI 10.1109/ACSSC.2011.6190099
   Miyato T, 2018, CoRR
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Parkhi O. M., 2015, P BRIT MACH VIS C, p41.1
   Phillips J. B., 2017, Camera Image Quality Benchmarking
   Quan HT, 2012, TELECOMMUN SYST, V49, P35, DOI 10.1007/s11235-010-9351-x
   Rao RRR, 2021, INT WORK QUAL MULTIM, P1, DOI 10.1109/QoMEX51781.2021.9465425
   Rippel O., 2017, P 34 INT C MACH LEAR, P2922
   Rippel O, 2019, IEEE I CONF COMP VIS, P3453, DOI 10.1109/ICCV.2019.00355
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Seidenari L., 2022, PROC ACM MULTIMEDIA, P1, DOI [10.1145/3469877.3490605, DOI 10.1145/3469877.3490605]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vaccaro F, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1221, DOI 10.1145/3474085.3475683
   Wang J., 2022, P IEEE CVF C COMP VI, P17512
   Wang T.-C., 2019, ADV NEUR IN
   Wang TC, 2021, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR46437.2021.00991
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Wijnants M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2449, DOI 10.1145/3343031.3351045
   Winkler S, 2009, INT WORK QUAL MULTIM, P139, DOI 10.1109/QOMEX.2009.5246961
   Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37
   Xiaoming Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P399, DOI 10.1007/978-3-030-58545-7_23
   Xikun Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14321, DOI 10.1109/CVPR42600.2020.01434
   Yang LB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1551, DOI 10.1145/3394171.3413965
   Yang T, 2021, PROC CVPR IEEE, P672, DOI 10.1109/CVPR46437.2021.00073
   Yasarla R, 2021, Arxiv, DOI arXiv:2105.06528
   Yoo J, 2018, PROC CVPR IEEE, P6684, DOI 10.1109/CVPR.2018.00699
   Yu L, 2018, IEEE T MULTIMEDIA, V20, P15, DOI 10.1109/TMM.2017.2726900
   Yu Liu, 2021, 2021 IEEE International Conference on Computer Science, Artificial Intelligence and Electronic Engineering (CSAIEE), P222, DOI 10.1109/CSAIEE54046.2021.9543344
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X, 2023, IEEE T PATTERN ANAL, V45, P2024, DOI 10.1109/TPAMI.2022.3157388
   Zhang ZM, 2021, PROC CVPR IEEE, P3660, DOI 10.1109/CVPR46437.2021.00366
NR 83
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 339
EP 352
DI 10.1109/TMM.2023.3264882
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500014
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Cui, WX
   Fan, XP
   Zhang, J
   Zhao, DB
AF Cui, Wenxue
   Fan, Xiaopeng
   Zhang, Jian
   Zhao, Debin
TI Deep Unfolding Network for Image Compressed Sensing by Content-Adaptive
   Gradient Updating and Deformation-Invariant Non-Local Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Compressed sensing; Adaptation models; Deformable
   models; Image coding; Adaptive systems; Limiting; Convolutional neural
   networks (CNNs); deep unfolding network (DUN); image compressed sensing;
   non-local neural network; proximal gradient descent (PGD)
ID RECONSTRUCTION NETWORK; ALGORITHMS
AB Inspired by certain optimization solvers, the deep unfolding network (DUN) has attracted much attention in recent years for image compressed sensing (CS). However, there still exist the following two issues: 1) In existing DUNs, most hyperparameters are usually content independent, which greatly limits their adaptability for different input contents. 2) In each iteration, a plain convolutional neural network is usually adopted, which weakens the perception of wider context prior and therefore depresses the expressive ability. In this article, inspired by the traditional Proximal Gradient Descent (PGD) algorithm, a novel DUN for image compressed sensing (dubbed DUN-CSNet) is proposed to solve the above two issues. Specifically, for the first issue, a novel content adaptive gradient descent network is proposed, in which a well-designed step size generation sub-network is developed to dynamically allocate the corresponding step sizes for different textures of input image by generating a content-aware step size map, realizing a content-adaptive gradient updating. For the second issue, considering the fact that many similar patches exist in an image but have undergone a deformation, a novel deformation-invariant non-local proximal mapping network is developed, which can adaptively build the long-range dependencies between the nonlocal patches by deformation-invariant non-local modeling, leading to a wider perception on context priors. Extensive experiments manifest that the proposed DUN-CSNet outperforms existing state-of-the-art CS methods by large margins.
C1 [Cui, Wenxue; Fan, Xiaopeng; Zhao, Debin] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Cui, Wenxue; Zhang, Jian; Zhao, Debin] Peng Cheng Lab, Shenzhen 518052, Peoples R China.
   [Zhang, Jian] Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Peking University
RP Zhao, DB (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.; Zhao, DB (corresponding author), Peng Cheng Lab, Shenzhen 518052, Peoples R China.
EM wxcui@hit.edu.cn; fxp@hit.edu.cn; zhangjian.sz@pku.edu.cn;
   dbzhao@hit.edu.cn
OI Cui, Wenxue/0000-0001-8656-0954
FU National Key Ramp;D Program of China
FX No Statement Available
CR Anselmi N, 2015, IEEE T ANTENN PROPAG, V63, P4889, DOI 10.1109/TAP.2015.2444423
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen C, 2011, CONF REC ASILOMAR C, P1193, DOI 10.1109/ACSSC.2011.6190204
   Cui WX, 2023, IEEE T MULTIMEDIA, V25, P816, DOI 10.1109/TMM.2021.3132489
   Cui WX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1748, DOI 10.1109/ICASSP.2018.8461766
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dong WS, 2019, IEEE T PATTERN ANAL, V41, P2305, DOI 10.1109/TPAMI.2018.2873610
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Dosovitskiy A., 2021, PROC ICLR
   Gao XW, 2015, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2015.47
   Gilton D, 2020, IEEE T COMPUT IMAG, V6, P328, DOI 10.1109/TCI.2019.2948732
   Golbabaee M, 2012, INT CONF ACOUST SPEE, P2741, DOI 10.1109/ICASSP.2012.6288484
   Huang Y, 2021, KNOWL-BASED SYST, V231, DOI 10.1016/j.knosys.2021.107384
   Huang ZL, 2023, IEEE T PATTERN ANAL, V45, P6896, DOI 10.1109/TPAMI.2020.3007032
   Jiwei Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P513, DOI 10.1007/978-3-030-58542-6_31
   Jung MY, 2011, IEEE T IMAGE PROCESS, V20, P1583, DOI 10.1109/TIP.2010.2092433
   Kim YK, 2010, IEEE IMAGE PROC, P3365, DOI 10.1109/ICIP.2010.5652744
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Li Chengbo., 2009, Tval3: Tv minimization by augmented lagrangian and alternating direction algorithm
   Li SC, 2013, IEEE T IND INFORM, V9, P2177, DOI 10.1109/TII.2012.2189222
   Li W, 2019, IEEE ACCESS, V7, P70910, DOI 10.1109/ACCESS.2019.2918593
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lohit S, 2018, IEEE T COMPUT IMAG, V4, P326, DOI 10.1109/TCI.2018.2846413
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Metzler CA, 2017, ADV NEUR IN, V30
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Shi WZ, 2021, IEEE T CIRC SYST VID, V31, P425, DOI 10.1109/TCSVT.2020.2978703
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Song JC, 2023, IEEE T IMAGE PROCESS, V32, P2202, DOI 10.1109/TIP.2023.3263100
   Song JC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4249, DOI 10.1145/3474085.3475562
   Su YM, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.115989
   Sun YB, 2022, IEEE T CIRC SYST VID, V32, P5931, DOI 10.1109/TCSVT.2022.3164241
   Sun YB, 2020, IEEE T MULTIMEDIA, V22, P3236, DOI 10.1109/TMM.2020.2973862
   Sun YB, 2020, IEEE T IMAGE PROCESS, V29, P9482, DOI 10.1109/TIP.2020.3023629
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Valsesia D, 2020, IEEE T IMAGE PROCESS, V29, P8226, DOI 10.1109/TIP.2020.3013166
   Valsesia D, 2019, IEEE IMAGE PROC, P2399, DOI [10.1109/ICIP.2019.8803367, 10.1109/icip.2019.8803367]
   Wang HK, 2023, IEEE T IMAGE PROCESS, V32, P2761, DOI 10.1109/TIP.2023.3274967
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xu K, 2018, LECT NOTES COMPUT SC, V11214, P491, DOI 10.1007/978-3-030-01249-6_30
   Yang Y, 2020, IEEE T PATTERN ANAL, V42, P521, DOI 10.1109/TPAMI.2018.2883941
   Yao HT, 2019, NEUROCOMPUTING, V359, P483, DOI 10.1016/j.neucom.2019.05.006
   You D, 2021, IEEE T IMAGE PROCESS, V30, P6066, DOI 10.1109/TIP.2021.3091834
   Yuan X, 2021, IEEE SIGNAL PROC MAG, V38, P65, DOI 10.1109/MSP.2020.3023869
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8960, DOI 10.1109/TIP.2020.3021291
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2012, IEEE J EM SEL TOP C, V2, P380, DOI 10.1109/JETCAS.2012.2220391
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
   Zhao C, 2014, IEEE INT CON MULTI
   Zhao C, 2016, IEEE DATA COMPR CONF, P161, DOI 10.1109/DCC.2016.104
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P1182, DOI 10.1109/TCSVT.2016.2527181
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 67
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4012
EP 4027
DI 10.1109/TMM.2023.3321424
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300008
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ding, Q
   Shen, LQ
   Yu, LW
   Yang, H
   Xu, M
AF Ding, Qing
   Shen, Liquan
   Yu, Liangwei
   Yang, Hao
   Xu, Mai
TI Blind Quality Enhancement for Compressed Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind quality enhancement; compression artifacts; convolutional neural
   network (CNN); video coding
ID DCT
AB Deep convolutional neural networks (CNNs) have achieved impressive success in enhancing the quality of compressed images/videos. These approaches mostly obtain the noise level in advance and train multiple architecture-identical models for enhancement on images/videos of known levels of noise. It largely hinders their practical applications where the noise level is unknown and resource is limited. To practically perform quality enhancement, we propose a novel blind quality enhancement framework for compressed video (BQEV), which utilizes a single network to conduct enhancement on videos compressed at various and unknown quality parameters (QPs). Since there exists feature similarity and difference among videos compressed at multiple QPs, BQEV utilizes this prior to efficiently handle enhancement on videos compressed at blind QPs, which consists of progressive feature extraction and QP-adaptive feature fusion subnets. They utilize temporal information and feature similarity to progressively extract valuable features and further employ the feature difference to conduct reasonable QP-adaptive feature fusion and quality enhancement, respectively. In the progressive feature extraction subnet, we first design a quality rank module to assign more attention to higher-quality frames for efficient utilization of temporal information, then propose a progressive extraction module to further extract features from different QPs. In the QP-adaptive feature fusion subnet, we develop a quality estimation module to guide reasonable feature fusion of these extracted progressive features for stable and promising enhancement results on multiple QPs. Experimental results demonstrate that BQEV achieves 0.31-0.69 dB PSNR improvement compared with videos compressed at various QPs, outperforming state-of-the-art approaches.
C1 [Ding, Qing; Yu, Liangwei; Yang, Hao] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Shen, Liquan] Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai 200444, Peoples R China.
   [Xu, Mai] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
C3 Shanghai University; Shanghai University; Beihang University
RP Shen, LQ (corresponding author), Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai 200444, Peoples R China.
EM dingqing@shu.edu.cn; jsslq@163.com; leoyu800@shu.edu.cn;
   aidoneus@shu.edu.cn; maixu@buaa.edu.cn
RI Shen, Liquan/D-4832-2012
OI Shen, Liquan/0000-0002-2148-6279
FU National Natural Science Foundation of China
FX No Statement Available
CR Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Budagavi M., 2014, HEVC transform and quantization, P141
   Chen T, 2001, IEEE T CIRC SYST VID, V11, P594, DOI 10.1109/76.920189
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Ding Q, 2021, IEEE T IMAGE PROCESS, V30, P6459, DOI 10.1109/TIP.2021.3092949
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Guo J, 2016, LECT NOTES COMPUT SC, V9905, P628, DOI 10.1007/978-3-319-46448-0_38
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He XY, 2018, IEEE IMAGE PROC, P216, DOI 10.1109/ICIP.2018.8451086
   Kim Y, 2020, IEEE T CIRC SYST VID, V30, P1121, DOI 10.1109/TCSVT.2019.2901919
   Kingma D. P., 2014, arXiv
   Kong Hao-Song, 2004, PROC IEEE INT S CIRC, pIII
   Lebrun M, 2015, IEEE T IMAGE PROCESS, V24, P3149, DOI 10.1109/TIP.2015.2439041
   Lee JY, 2005, IEEE T CIRC SYST VID, V15, P1077, DOI 10.1109/TCSVT.2005.852628
   Li K, 2017, IEEE INT CON MULTI, P1320, DOI 10.1109/ICME.2017.8019416
   Li LRH, 2018, PROC CVPR IEEE, P6616, DOI 10.1109/CVPR.2018.00692
   Liu C, 2008, IEEE T PATTERN ANAL, V30, P299, DOI [10.1109/TPAMI.2007.1176, 10.1109/TPAMI.20071176]
   Lu G, 2018, LECT NOTES COMPUT SC, V11218, P591, DOI 10.1007/978-3-030-01264-9_35
   Qunliang Xing, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P275, DOI 10.1007/978-3-030-58517-4_17
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi XJ, 2015, ADV NEUR IN, V28
   Shiwen Shen, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P656, DOI 10.1109/CISP.2011.6100331
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Wenlong Z., 2022, IEEE Trans. Pattern Anal. Mach. Intell., V44, P7149
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu Y, 2019, IEEE I CONF COMP VIS, P7042, DOI 10.1109/ICCV.2019.00714
   Yang R, 2019, IEEE INT CON MULTI, P532, DOI 10.1109/ICME.2019.00098
   Yang R, 2019, IEEE T CIRC SYST VID, V29, P2039, DOI 10.1109/TCSVT.2018.2867568
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Yang R, 2017, IEEE INT CON MULTI, P817, DOI 10.1109/ICME.2017.8019299
   Yu LW, 2021, IEEE T CIRC SYST VID, V31, P2824, DOI 10.1109/TCSVT.2020.3028330
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XF, 2015, IEEE T IMAGE PROCESS, V24, P6048, DOI 10.1109/TIP.2015.2485780
   Zhang Y., 2019, PROC INT C LEARN REP
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou SP, 2021, IEEE T IMAGE PROCESS, V30, P1, DOI 10.1109/TIP.2020.3027992
NR 44
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5782
EP 5794
DI 10.1109/TMM.2023.3339599
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100029
DA 2024-08-05
ER

PT J
AU Huang, Y
   Huang, JC
   Liu, JZ
   Yan, MF
   Dong, Y
   Lv, JX
   Chen, CQ
   Chen, SF
AF Huang, Yi
   Huang, Jiancheng
   Liu, Jianzhuang
   Yan, Mingfu
   Dong, Yu
   Lv, Jiaxi
   Chen, Chaoqi
   Chen, Shifeng
TI WaveDM: Wavelet-Based Diffusion Models for Image Restoration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Diffusion models; image restoration; wavelet transform
ID NETWORK
AB Latest diffusion-based methods for many image restoration tasks outperform traditional models, but they encounter the long-time inference problem. To tackle it, this paper proposes a Wavelet-Based Diffusion Model (WaveDM). WaveDM learns the distribution of clean images in the wavelet domain conditioned on the wavelet spectrum of degraded images after wavelet transform, which is more time-saving in each step of sampling than modeling in the spatial domain. To ensure restoration performance, a unique training strategy is proposed where the low-frequency and high-frequency spectrums are learned using distinct modules. In addition, an Efficient Conditional Sampling (ECS) strategy is developed from experiments, which reduces the number of total sampling steps to around 5. Evaluations on twelve benchmark datasets including image raindrop removal, rain steaks removal, dehazing, defocus deblurring, demoir & eacute;ing, and denoising demonstrate that WaveDM achieves state-of-the-art performance with the efficiency that is comparable to traditional one-pass methods and over 100x faster than existing image restoration methods using vanilla diffusion models.
C1 [Huang, Yi; Huang, Jiancheng; Liu, Jianzhuang; Yan, Mingfu; Dong, Yu; Lv, Jiaxi; Chen, Shifeng] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Comp Vis & Pattern Recognit, Shenzhen 518055, Peoples R China.
   [Huang, Yi; Huang, Jiancheng; Liu, Jianzhuang; Yan, Mingfu; Dong, Yu; Lv, Jiaxi; Chen, Shifeng] Univ Chinese Acad Sci, Beijing 100039, Peoples R China.
   [Chen, Chaoqi] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; University of Hong Kong
RP Chen, SF (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Comp Vis & Pattern Recognit, Shenzhen 518055, Peoples R China.
EM yi.huang@siat.ac.cn; jc.huang@siat.ac.cn; jz.liu@siat.ac.cn;
   mf.yan@siat.ac.cn; yu.dong@siat.ac.cn; jx.lv1@siat.ac.cn;
   cqchen1994@gmail.com; shifeng.chen@siat.ac.cn
OI Huang, Jiancheng/0000-0003-2826-9231; Yan, Mingfu/0009-0000-4820-0704
FU Shenzhen Science and Technology Innovation Commission
FX No Statement Available
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Abuolaim Abdullah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P111, DOI 10.1007/978-3-030-58607-2_7
   Abuolaim Abdullah, 2021, P IEEE INT C COMP VI, P2289
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Bin He, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P713, DOI 10.1007/978-3-030-58542-6_43
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Choi J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14347, DOI 10.1109/ICCV48922.2021.01410
   Chung H., 2023, PROC INT C LEARN REP
   Chung H, 2023, PROC CVPR IEEE, P6059, DOI 10.1109/CVPR52729.2023.00587
   Chung H, 2022, PROC CVPR IEEE, P12403, DOI 10.1109/CVPR52688.2022.01209
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dudhane A, 2022, PROC CVPR IEEE, P5749, DOI 10.1109/CVPR52688.2022.00567
   Fang T., 2006, ACM SIGGRAPH 2006 SK, P156
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P1505, DOI 10.1109/TMM.2023.3240880
   Guth F., 2022, Advances in Neural Information Processing Systems, P478
   Han KW, 2020, INT CONF ACOUST SPEE, P1828, DOI [10.1109/ICASSP40776.2020.9053123, 10.1109/icassp40776.2020.9053123]
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Ho J., 2021, PROC NEURIPS WORKSHO
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Ho J, 2022, J MACH LEARN RES, V23, P1
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang Y., IEEE Trans. Consum. Electron, early access, DOI [10.1109/TCH2023,3320662, DOI 10.1109/TCH2023,3320662]
   Hui KH, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555394
   Hyungjin S., 2022, Advances in Neural Information Processing Systems, V35, P25683
   Hyvarinen A., 2005, Mach Learn. Res, V6, P419
   Jiang K, 2021, IEEE IMAGE PROC, P1759, DOI 10.1109/ICIP42928.2021.9506318
   Jin X, 2019, IEEE IMAGE PROC, P2761, DOI [10.1109/icip.2019.8803238, 10.1109/ICIP.2019.8803238]
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kawar B., 2023, Trans. Mach. Learn. Res.
   Kawar B., 2022, Advances in Neural Information Processing Systems, P23593
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Kumar M., 2021, PROC INT C LEARN REP
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kwon G., 2023, PROC INT C LEARN REP
   Lec S., 2022, PROC NEURIPS WORKSHO
   Lee J, 2021, PROC CVPR IEEE, P2034, DOI 10.1109/CVPR46437.2021.00207
   Lee J, 2019, PROC CVPR IEEE, P12214, DOI 10.1109/CVPR.2019.01250
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li HY, 2022, NEUROCOMPUTING, V479, P47, DOI 10.1016/j.neucom.2022.01.029
   LI J., 2023, ARXIV
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li RT, 2020, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR42600.2020.00324
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin JX, 2023, IEEE T MULTIMEDIA, V25, P8396, DOI 10.1109/TMM.2023.3236845
   Lin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P86, DOI 10.1007/978-3-030-58601-0_6
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu XH, 2023, IEEE WINT CONF APPL, P289, DOI [10.1109/WACV56688.2023.00037, 10.1007/978-3-031-33545-7_20]
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu C., 2022, P ADV NEUR INF PROC, V35, P5775
   Lu C, 2022, Arxiv, DOI arXiv:2211.01095
   Lugmayr Andreas, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P715, DOI 10.1007/978-3-030-58558-7_42
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Lyu Z, 2022, Arxiv, DOI arXiv:2205.12524
   Ma HY, 2022, LECT NOTES COMPUT SC, V13683, P1, DOI 10.1007/978-3-031-20050-2_1
   Ma I., 2022, PROC IEEECVF C COMPU, P5637
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meng C., 2022, PROC INT C LEARN REP
   Meng CL, 2023, PROC CVPR IEEE, P14297, DOI 10.1109/CVPR52729.2023.01374
   Michaeli T, 2013, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2013.121
   Mou C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4308, DOI 10.1109/ICCV48922.2021.00429
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Murata N., 2023, PROC ICML, P25501
   Nair N. G., 2022, PROC IEEE WINTER C A, P3434
   Nichol P., 2021, Improved denoising diffusion probabilistic models, P8162, DOI DOI 10.48550/ARXIV.2102.09672
   ÖOzdenizci O, 2023, IEEE T PATTERN ANAL, V45, P10346, DOI 10.1109/TPAMI.2023.3238179
   Phung H, 2023, PROC CVPR IEEE, P10199, DOI 10.1109/CVPR52729.2023.00983
   Preechakul K, 2022, PROC CVPR IEEE, P10609, DOI 10.1109/CVPR52688.2022.01036
   Puthussery Densen, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P519, DOI 10.1007/978-3-030-67070-2_31
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Quan RJ, 2021, PROC CVPR IEEE, P9143, DOI 10.1109/CVPR46437.2021.00903
   Quan YH, 2019, IEEE I CONF COMP VIS, P2463, DOI 10.1109/ICCV.2019.00255
   Ren C, 2021, PROC CVPR IEEE, P8592, DOI 10.1109/CVPR46437.2021.00849
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saharia C., 2022, PROC ACM SIGGRAPH C, P1
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Salimans T., 2022, INT C LEARNING REPRE
   Sasaki H, 2021, Arxiv, DOI arXiv:2104.05358
   Shi J., 2022, arXiv
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Son H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2622, DOI 10.1109/ICCV48922.2021.00264
   Song J., 2021, PROC INT C LEARN REP
   Song Y., 2020, Adv Neural Inf Process Syst, V33, P12438
   Song Y., 2021, INT C LEARNING REPRE
   Song Yang, 2019, Advances in Neural Information Processing Systems, V32
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Sun YJ, 2018, IEEE T IMAGE PROCESS, V27, P4160, DOI 10.1109/TIP.2018.2834737
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Vahdat J. Song. A., 2023, PROC INT C LEARN REP
   Valanarasu J. M. J., 2022, P IEEECVF C COMPUTER, P2353
   Vaswani A, 2017, ADV NEUR IN, V30
   Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142
   Wang TF, 2022, Arxiv, DOI arXiv:2205.12952
   Wang Y., 2023, PROC INT C LEARA REP
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Whang J., 2021, PMLR, P11158
   Whang J, 2022, PROC CVPR IEEE, P16272, DOI 10.1109/CVPR52688.2022.01581
   Xiao J, 2023, IEEE T PATTERN ANAL, V45, P12978, DOI 10.1109/TPAMI.2022.3183612
   Xin JW, 2022, IEEE T NEUR NET LEAR, V33, P707, DOI 10.1109/TNNLS.2020.3028688
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yi QS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4218, DOI 10.1109/ICCV48922.2021.00420
   Yi QS, 2022, IEEE T MULTIMEDIA, V24, P3114, DOI 10.1109/TMM.2021.3093724
   Yu X, 2022, LECT NOTES COMPUT SC, V13678, P646, DOI 10.1007/978-3-031-19797-0_37
   Yue ZS, 2024, Arxiv, DOI arXiv:2212.06512
   Zagoruyko S., 2016, Wide residual networks, DOI DOI 10.5244/C.30.87
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KH, 2021, IEEE T IMAGE PROCESS, V30, P7608, DOI 10.1109/TIP.2021.3108019
   Zhang L, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3600632
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YX, 2023, PROC CVPR IEEE, P10146, DOI 10.1109/CVPR52729.2023.00978
   Zhao M., 2022, Advances in Neural Information Processing Systems, V35, P3609
   Zheng BL, 2022, IEEE T PATTERN ANAL, V44, P7705, DOI 10.1109/TPAMI.2021.3115139
NR 131
TC 0
Z9 0
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7058
EP 7073
DI 10.1109/TMM.2024.3359769
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000003
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Jing, SQ
   Zhang, HN
   Zeng, PP
   Gao, LL
   Song, JK
   Shen, HT
AF Jing, Shuaiqi
   Zhang, Haonan
   Zeng, Pengpeng
   Gao, Lianli
   Song, Jingkuan
   Shen, Heng Tao
TI Memory-Based Augmentation Network for Video Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Decoding; Task analysis; Semantics; Transformers; Context
   modeling; Linguistics; Attention mechanism; image captioning; LSTM;
   memory network; video captioning
AB Video captioning focuses on generating natural language descriptions according to the video content. Existing works mainly explore this multimodal learning with the paired source video and corresponding sentence, which have achieved competitive performances. Nonetheless, learning from video-description pair cannot capture implicit external knowledge, i.e., multiple visual context information and linguistic clues existing in the video-language dataset, which may limit the cognitive capability of the model to generate diverse descriptions. To this end, we propose a Memory-based Augmentation Network (MAN), in which a memory structure is designed to augment the current encoder-decoder framework by incorporating implicit external knowledge with a neural memory. Specifically, we first propose a visual memory for the encoder to store multiple visual contexts across videos in the dataset, which is utilized to obtain memory-augmented contextual features for the source video. In addition, a textual memory is introduced for the decoder to capture the external language clues across sentences in the dataset. It is adapted to capture memory-augmented language features in each time step. The proposed approach is able to capture comprehensive contextual understanding compared to the basic encoder-decoder framework, which is more compatible with the human cognitive process. Extensive experiments on three video captioning datasets including MSVD, MSR-VTT, and VATEX demonstrate the effectiveness of the proposed method.
C1 [Jing, Shuaiqi; Zhang, Haonan; Zeng, Pengpeng; Gao, Lianli; Song, Jingkuan; Shen, Heng Tao] Univ Elect Sci & Technol China, Future Media Ctr, Chengdu 611731, Peoples R China.
   [Jing, Shuaiqi; Zhang, Haonan; Zeng, Pengpeng; Gao, Lianli; Song, Jingkuan; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China
RP Song, JK (corresponding author), Univ Elect Sci & Technol China, Future Media Ctr, Chengdu 611731, Peoples R China.; Song, JK (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM jingshuaiqi@uestc.edu.cn; zchiowal@gmail.com; is.pengpengzeng@gmail.com;
   lianli.gao@uestc.edu.cn; jingkuan.song@gmail.com;
   shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021; Zhang, Haonan/JHT-4941-2023
OI Zhang, Haonan/0000-0003-1015-7338; Zeng, Pengpeng/0000-0002-0672-3790;
   song, jingkuan/0000-0002-2549-8322
FU National Key Ramp;D Program of China
FX No Statement Available
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Ba J, 2016, ADV NEUR IN, V29
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Bordes A, 2015, Arxiv, DOI arXiv:1506.02075
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Chen David, 2011, ACL
   Chen JW, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3539225
   Chen SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1523, DOI 10.1109/ICCV48922.2021.00157
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Das A, 2017, IEEE I CONF COMP VIS, P2970, DOI 10.1109/ICCV.2017.321
   Denkowski M., 2014, P WMT ACL, P376
   Dix Alan, 2003, HUM FAC ER
   Vo DM, 2022, PROC CVPR IEEE, P17979, DOI 10.1109/CVPR52688.2022.01747
   Fan Z., 2021, PROC INT JOINT C ART, P657
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Fei ZC, 2021, AAAI CONF ARTIF INTE, V35, P1317
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Gao LL, 2022, IEEE T IMAGE PROCESS, V31, P202, DOI 10.1109/TIP.2021.3120867
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Graves A, 2014, Arxiv, DOI arXiv:1410.5401
   Grefenstette E, 2015, ADV NEUR IN, V28
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Gupta A, 2012, P 26 AAAI C ART INT, P606
   Han N, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3826, DOI 10.1145/3474085.3475241
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herdade S, 2019, ADV NEUR IN, V32
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Ji WT, 2022, APPL SOFT COMPUT, V117, DOI 10.1016/j.asoc.2021.108332
   Jiang C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1618, DOI 10.1145/3474085.3475301
   Jiang Huaizu, 2020, P IEEE CVF C COMP VI, P10267, DOI DOI 10.1109/CVPR42600.2020.01028
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Joulin A., 2015, P ADV NEUR INF PROC, P190
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kingma D., 3 INT C LEARNING REP
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishnamoorthy Niveda, 2013, AAAI
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lee J. Y. F., 2021, PROC 27 ACM INT C MU, P1
   Lei J, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2603
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin K, 2022, PROC CVPR IEEE, P17928, DOI 10.1109/CVPR52688.2022.01742
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y, 2017, AAAI CONF ARTIF INTE, P4197
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao J., 2015, PROC INT C LEARN REP
   Maruf S, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1275
   McInnes L., 2021, J. Open Source Softw., V3, P861
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Rane C., 2021, PROC IEEE INT C COMM, P1
   Reese S, 2010, LREC 2010 - SEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1418
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Seo PH, 2022, PROC CVPR IEEE, P17938, DOI 10.1109/CVPR52688.2022.01743
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shetty Rakshith, 2016, P 24 ACM INT C MULTI, P1073
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Tan GC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P745
   Ushiku Y, 2015, IEEE I CONF COMP VIS, P2668, DOI 10.1109/ICCV.2015.306
   Nguyen VQ, 2022, LECT NOTES COMPUT SC, V13696, P167, DOI 10.1007/978-3-031-20059-5_10
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2015, P 2015 C N AM CHAPT, P1494
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Voykinska V, 2016, ACM CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW 2016), P1584, DOI 10.1145/2818048.2820013
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wang W, 2011, PROC CVPR IEEE, P441, DOI 10.1109/CVPR.2011.5995423
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Wang X, 2019, AAAI CONF ARTIF INTE, P8965
   Weston J., 2015, PROC INT C LEARN REP
   Wu X, 2018, PROC CVPR IEEE, P6829, DOI 10.1109/CVPR.2018.00714
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu HJ, 2019, IEEE WINT CONF APPL, P396, DOI 10.1109/WACV.2019.00048
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K., 2015, PROC INT C MACH LEAR, P2048
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang B, 2021, AAAI CONF ARTIF INTE, V35, P3119
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P146, DOI 10.1145/3123266.3123327
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Ye HH, 2022, PROC CVPR IEEE, P17918, DOI 10.1109/CVPR52688.2022.01741
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zeng PP, 2022, Arxiv, DOI arXiv:2211.09469
   Zeng PP, 2022, IEEE T IMAGE PROCESS, V31, P5936, DOI 10.1109/TIP.2022.3205212
   Zeng Pengpeng, 2022, P 31 INT JOINT C ART, P1608, DOI [DOI 10.24963/IJCAI.20"2F224, 10.24963/ijcai.2022/224, 10.24963/ijcai.2022/]
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Zhao WT, 2021, ADV NEUR IN, V34
   Zheng Q., 2020, 2020 CVPR, P13093, DOI 10.1109/CVPR42600.2020.01311
   Zhu JK, 2023, IEEE T CIRC SYST VID, V33, P4362, DOI 10.1109/TCSVT.2023.3235523
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 105
TC 3
Z9 3
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2367
EP 2379
DI 10.1109/TMM.2023.3295098
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100020
DA 2024-08-05
ER

PT J
AU Li, XC
   Fan, BY
   Zhang, RZ
   Zhao, K
   Guo, ZH
   Zhao, YQ
   Li, RA
AF Li, Xiaochuan
   Fan, Baoyu
   Zhang, Runze
   Zhao, Kun
   Guo, Zhenhua
   Zhao, Yaqian
   Li, Rengang
TI Inexactly Matched Referring Expression Comprehension With Rationale
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Grounding; Visualization; Pipelines; Transformers;
   Training; Annotations; Referring expression comprehension; computational
   linguistics; multimodal learning
ID LANGUAGE
AB Referring Expression Comprehension (REC) is a multimodal comprehension task that aims to locate an object in an image, given a text description. Traditionally, during the existing REC tasks, there has been a basic assumption that the given text expression and the image are usually exactly matched to each other. However, in real-world scenarios, there is uncertainty in how well the image and text match each other exactly. Illegible objects in the image or ambiguous phrases in the text have the potential to significantly degrade the performance of conventional REC tasks. To overcome these limitations, we consider a more practical and comprehensive REC task, where the given image and its referring text expression can be inexactly matched. Our models aim to correct such inexact matching and supply corresponding interpretations. We refer to this task as Further REC (FREC). This task is divided into three subtasks: 1) correcting the erroneous text expression using visual information, 2) generating the rationale for this input expression, and 3) localizing the proper object based on the corrected expression. We introduce three new datasets for FREC: Further-RefCOCOs, Further-Copsref and Further-Talk2Car. These datasets are based on the existing REC datasets, including RefCOCO and Talk2Car. We developed a novel pipeline architecture to execute the three subtasks simultaneously in an end-to-end fashion. Next, we developed an elastic masked language modeling (EMLM) training head to rectify text errors with uncertain lengths. Our experimental results demonstrate the validity of our proposed pipeline. We hope this work sparks more research focused on inexactly matched REC.
C1 [Li, Xiaochuan; Fan, Baoyu; Zhang, Runze; Zhao, Kun; Guo, Zhenhua; Zhao, Yaqian; Li, Rengang] Inspur Elect Informat Ind Co Ltd, Jinan 250014, Peoples R China.
   [Fan, Baoyu] Nankai Univ, Tianjin 300071, Peoples R China.
   [Li, Rengang] Tsinghua Univ, Beijing 100190, Peoples R China.
C3 Inspur; Nankai University; Tsinghua University
RP Fan, BY (corresponding author), Nankai Univ, Tianjin 300071, Peoples R China.
EM lixiaochuan2088@gmail.com; fanbaoyu@inspur.com; zhangrunze@inspur.com;
   zhaokunbj@inspur.com; guozhenhua@inspur.com; zhaoyaqian@inspur.com;
   lirengang.hsslab@gmail.com
OI Li, Xiaochuan/0000-0001-6143-0854
FU National Key Research and Development Program of China
FX No Statement Available
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Arumugam D, 2019, AUTON ROBOT, V43, P449, DOI 10.1007/s10514-018-9792-8
   Bert C.W., 1996, Appl. mech. Rev, V49, P1, DOI 10.1115/1.3101882
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Carion N., 2020, EUR C COMP VIS, P213
   Chandu KR, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P4283
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen Zhenfang, 2020, P IEEE CVF C COMP VI, P10086
   Cho Jaemin, 2021, INT C MACH LEARN, P1931
   Cho K., 2014, ARXIV14061078, V1406, P1078, DOI DOI 10.3115/V1/D14-1179
   Cornia M, 2019, PROC CVPR IEEE, P8299, DOI 10.1109/CVPR.2019.00850
   de Vries H, 2017, PROC CVPR IEEE, P4466, DOI 10.1109/CVPR.2017.475
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Deruyttere T, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2088
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding N, 2016, Arxiv, DOI arXiv:1612.07833
   Fan BY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P655, DOI 10.1145/3394171.3414038
   Hong RC, 2022, IEEE T PATTERN ANAL, V44, P684, DOI 10.1109/TPAMI.2019.2911066
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Huang BB, 2021, PROC CVPR IEEE, P16883, DOI 10.1109/CVPR46437.2021.01661
   Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Kintsch W, 2005, CEN IM E R, P71
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li CS, 2022, IEEE T IMAGE PROCESS, V31, P2767, DOI 10.1109/TIP.2022.3161076
   Li CS, 2021, IEEE T IMAGE PROCESS, V30, P9280, DOI 10.1109/TIP.2021.3124317
   Li CS, 2019, IEEE T PATTERN ANAL, V41, P1382, DOI 10.1109/TPAMI.2018.2840980
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li Wang, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P1866, DOI 10.1145/3474085.3475340
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520
   Liu RT, 2019, PROC CVPR IEEE, P4180, DOI 10.1109/CVPR.2019.00431
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Radford A., 2019, OpenAI blog, V1, P9
   Shekhar R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P255, DOI 10.18653/v1/P17-1024
   Spranger M., 2012, LANGUAGE GROUNDING R, P173
   Steels L, 2003, TRENDS COGN SCI, V7, P308, DOI 10.1016/S1364-6613(03)00129-3
   Steels L, 1997, FROM ANIM ANIMAT, P474
   Su W, 2020, PROC 8 INT C LEARN R
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Taylor WL, 1953, JOURNALISM QUART, V30, P415, DOI 10.1177/107769905303000401
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang P, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P28, DOI 10.1145/3394171.3413905
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhou YY, 2023, IEEE T NEUR NET LEAR, V34, P134, DOI 10.1109/TNNLS.2021.3090426
NR 56
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3937
EP 3950
DI 10.1109/TMM.2023.3318289
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300016
DA 2024-08-05
ER

PT J
AU Li, Z
   Guo, CL
   Feng, ZR
   Hwang, JN
   Du, ZT
AF Li, Zheng
   Guo, Caili
   Feng, Zerun
   Hwang, Jenq-Neng
   Du, Zhongtian
TI Integrating Language Guidance Into Image-Text Matching for Correcting
   False Negatives
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correcting false negatives; image-text matching; language guidance
ID RELEVANCE FEEDBACK; REPRESENTATION
AB Image-Text Matching (ITM) aims to establish the correspondence between images and sentences. ITM is fundamental to various vision and language understanding tasks. However, there are limitations in the way existing ITM benchmarks are constructed. The ITM benchmark collects pairs of images and sentences during construction. Therefore, only samples that are paired at collection are annotated as positive. All other samples are annotated as negative. Many correlations are missed in these samples that are annotated as negative. For example, a sentence matches only one image at the time of collection. Only this image is annotated as positive for the sentence. All other images are annotated as negative. However, these negative images may contain images that correspond to the sentences. These mislabeled samples are called false negatives. Existing ITM models are optimized based on annotations containing mislabels, which can introduce noise during training. In this paper, we propose an ITM framework integrating Language Guidance (LG) for correcting false negatives. A language pre-training model is introduced into the ITM framework to identify false negatives. To correct false negatives, we propose language guidance loss, which adaptively corrects the locations of false negatives in the visual-semantic embedding space. Extensive experiments on two ITM benchmarks show that our method can improve the performance of existing ITM models. To verify the performance of correcting false negatives, we conduct further experiments on ECCV Caption. ECCV Caption is a verified dataset where false negatives in annotations have been corrected. The experimental results show that our method can recall more relevant false negatives.
C1 [Li, Zheng; Guo, Caili; Feng, Zerun] Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing 100876, Peoples R China.
   [Hwang, Jenq-Neng] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
   [Du, Zhongtian] China Telecom Digital Intelligence Technol Co Ltd, Beijing 100035, Peoples R China.
C3 Beijing University of Posts & Telecommunications; University of
   Washington; University of Washington Seattle
RP Guo, CL (corresponding author), Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing 100876, Peoples R China.
EM lizhengzachary@bupt.edu.cn; guocaili@bupt.edu.cn; fengzerun@bupt.edu.cn;
   hwang@uw.edu; duzt@chinatelecom.cn
OI Feng, Zerun/0000-0003-3987-0591; Li, Zheng/0000-0003-2535-2523; Hwang,
   Jenq-Neng/0000-0002-8877-2421
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Cascante-Bonilla P, 2022, PROC CVPR IEEE, P5046, DOI 10.1109/CVPR52688.2022.00500
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Chun S, 2022, LECT NOTES COMPUT SC, V13668, P1, DOI 10.1007/978-3-031-20074-8_1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Dong XF, 2022, IEEE T CIRC SYST VID, V32, P6437, DOI 10.1109/TCSVT.2022.3164230
   Dosovitskiy A., 2021, INT C LEARN REPRESEN, P1
   Doulamis AD, 2000, SIGNAL PROCESS, V80, P1049, DOI 10.1016/S0165-1684(00)00019-0
   Doulamis AD, 2004, IEEE T CIRC SYST VID, V14, P656, DOI 10.1109/TCSVT.2004.826752
   Doulamis ND, 2016, IEEE T CYBERNETICS, V46, P2810, DOI 10.1109/TCYB.2015.2489841
   Faghri Fartash, 2018, BMVC
   Frome A., 2013, P ADV NEUR INF PROC
   Ge YY, 2022, PROC CVPR IEEE, P16146, DOI 10.1109/CVPR52688.2022.01569
   Gupta V, 2022, PROC CVPR IEEE, P5068, DOI 10.1109/CVPR52688.2022.00502
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang Z., 2021, Advances in Neural Informa- tion Processing Systems (NeurIPS-21), P29406
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Jia C, 2021, PR MACH LEARN RES, V139
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D., 3 INT C LEARNING REP
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li KP, 2023, IEEE T PATTERN ANAL, V45, P641, DOI 10.1109/TPAMI.2022.3148470
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li YA, 2022, PROC CVPR IEEE, P17969, DOI 10.1109/CVPR52688.2022.01746
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu Chunxiao, 2020, CVPR, P10918, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu X, 2024, IEEE T CYBERNETICS, V54, P948, DOI 10.1109/TCYB.2022.3179020
   Musgrave Kevin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P681, DOI 10.1007/978-3-030-58595-2_41
   Ordonez V., 2011, NeurIPS, V24
   Parekh Z, 2021, 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2021), P2855
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Qin Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4948, DOI 10.1145/3503161.3547922
   Radford A, 2021, PR MACH LEARN RES, V139
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Seo PH, 2022, PROC CVPR IEEE, P17938, DOI 10.1109/CVPR52688.2022.01743
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Song K., 2020, 33 ADV NEUR INF PROC, V33, P16857, DOI DOI 10.48550/ARXIV.2004.09297
   Tang JH, 2019, IEEE T PATTERN ANAL, V41, P2027, DOI 10.1109/TPAMI.2019.2906603
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P3362, DOI 10.1109/TMM.2020.3024822
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Wu J, 2022, IEEE T CIRC SYST VID, V32, P388, DOI 10.1109/TCSVT.2021.3060713
   Yang S, 2022, IEEE T CIRC SYST VID, V32, P8037, DOI 10.1109/TCSVT.2022.3182426
   Yang ZL, 2019, ADV NEUR IN, V32
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Zhang HT, 2022, AAAI CONF ARTIF INTE, P3262
   Zhang JC, 2022, IEEE COMPUT SOC CONF, P3215, DOI 10.1109/CVPRW56347.2022.00363
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
NR 61
TC 2
Z9 2
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 103
EP 116
DI 10.1109/TMM.2023.3261443
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500003
DA 2024-08-05
ER

PT J
AU Ma, X
   Liu, C
   Xie, CY
   Ye, L
   Deng, YF
   Ji, XY
AF Ma, Xin
   Liu, Chang
   Xie, Chunyu
   Ye, Long
   Deng, Yafeng
   Ji, Xiangyang
TI Disjoint Masking With Joint Distillation for Efficient Masked Image
   Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Disjoint masking; joint distillation; masked image modeling;
   self-supervised learning; and training efficiency
AB Masked image modeling (MIM) has shown great promise for self-supervised learning (SSL) yet been criticized for learning inefficiency. We believe the insufficient utilization of training signals should be responsible. To alleviate this issue, we introduce a conceptually simple yet learning-efficient MIM training scheme, termed Disjoint Masking with Joint Distillation (DMJD). For disjoint masking (DM), we sequentially sample multiple masked views per image in a mini-batch with the disjoint regulation to raise the usage of tokens for reconstruction in each image while keeping the masking rate of each view. For joint distillation (JD), we adopt a dual branch architecture to respectively predict invisible (masked) and visible (unmasked) tokens with superior learning targets. Rooting in orthogonal perspectives for training efficiency improvement, DM and JD cooperatively accelerate the training convergence yet not sacrificing the model generalization ability. Concretely, DM can train ViT with less effective training epochs (at most 3.7 chi less time-consuming) to report competitive performance. With JD, our DMJD clearly improves the linear probing classification accuracy, up to 3.4$%. On fine-grained downstream tasks like semantic segmentation, object detection, etc., our DMJD also presents superior generalization compared with state-of-the-art SSL methods.
C1 [Ma, Xin; Ye, Long] Commun Univ China, Sch Informat & Commun Engn, Beijing 100024, Peoples R China.
   [Liu, Chang; Ji, Xiangyang] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Xie, Chunyu; Deng, Yafeng] 360 AI Res, Beijing 100015, Peoples R China.
C3 Communication University of China; Tsinghua University
RP Ji, XY (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
EM mx_mark@cuc.edu.cn; liuchang2022@tsinghua.edu.cn; yuxie@buaa.edu.cn;
   yelong@cuc.edu.cn; dengyafeng@gmail.com; xyji@tsinghua.edu.cn
OI Liu, Chang/0000-0001-6747-0646; Deng, Yafeng/0000-0003-2416-6913
FU National Natural Science Foundation of China
FX No Statement Available
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Baevski A, 2022, PR MACH LEARN RES
   Bao H., 2022, P INT C LEARN REPR
   Berman M, 2019, Arxiv, DOI arXiv:1902.05509
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chen J, 2022, Arxiv, DOI arXiv:2206.00790
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XK, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01852-4
   Chen XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9620, DOI 10.1109/ICCV48922.2021.00950
   Chen YB, 2022, LECT NOTES COMPUT SC, V13690, P108, DOI 10.1007/978-3-031-20056-4_7
   Choi D, 2020, Arxiv, DOI arXiv:1907.05550
   Clark K, 2020, INFORM SYST RES, DOI 10.48550/arXiv.2003.10555
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong X.Y., 2023, P AAAI C ARTIFICIAL, VVolume 37, P552
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   El-Nouby A, 2021, Are large-scale datasets necessary for self-supervised pretraining? arXiv:2112.10740
   Fort S., 2021, arXiv
   Gao P., 2022, PROC ADV NEURAL IN, V35, P35632
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   Guo J., 2022, arXiv
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hoffer Elad, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8126, DOI 10.1109/CVPR42600.2020.00815
   Hoffer E, 2017, ADV NEUR IN, V30
   Hou ZJ, 2022, Arxiv, DOI arXiv:2208.06049
   Huang L., 2022, Adv. Neural Inf. Process. Syst., V35, P19997
   Huo XY, 2022, IEEE T MULTIMEDIA, V24, P4224, DOI 10.1109/TMM.2021.3115335
   Jia XY, 2018, Arxiv, DOI arXiv:1807.11205
   Kakogeorgiou I, 2022, LECT NOTES COMPUT SC, V13690, P300, DOI 10.1007/978-3-031-20056-4_18
   Keskar N. S., 2016, P INT C LEARN REPR
   Li C., 2022, P INT C LEARN REPR
   Li Gang, 2022, Advances in Neural Information Processing Systems
   Li X, 2022, Arxiv, DOI [arXiv:2205.10063, 10.48550/arXiv.2205.10063]
   Li XT, 2022, LECT NOTES COMPUT SC, V13690, P231, DOI 10.1007/978-3-031-20056-4_14
   Li Z., 2021, NEURIPS, P13165
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JH, 2022, Arxiv, DOI [arXiv:2205.13137, 10.48550/arXiv.2205.13137]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2017, PROC INT C LEARNING
   Loshchilov I., 2018, INT C LEARN REPR
   Peng ZL, 2022, Arxiv, DOI arXiv:2208.06366
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Shi Y., 2022, INT C MACHINE LEARNI, P20026
   Song GL, 2021, IEEE T MULTIMEDIA, V23, P1882, DOI 10.1109/TMM.2020.3004963
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2022, Arxiv, DOI arXiv:2208.10442
   Wei C, 2022, PROC CVPR IEEE, P14648, DOI 10.1109/CVPR52688.2022.01426
   Wei LH, 2022, LECT NOTES COMPUT SC, V13690, P337, DOI 10.1007/978-3-031-20056-4_20
   Wei Yixuan, 2022, arXiv
   Wettig A., 2023, P 17 C EUROPEAN CHAP, P2977
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xie ZD, 2022, PROC CVPR IEEE, P9643, DOI 10.1109/CVPR52688.2022.00943
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Ying CS, 2018, Arxiv, DOI arXiv:1811.06992
   You Y, 2017, Arxiv, DOI arXiv:1708.03888
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang CJ, 2018, IEEE T MULTIMEDIA, V20, P903, DOI 10.1109/TMM.2017.2759500
   Zhang X., 2022, arXiv
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou J., 2022, P INT C LEARN REPR
NR 64
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3077
EP 3087
DI 10.1109/TMM.2023.3306840
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700010
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, JG
   Qian, SS
   Hu, J
   Hong, RC
AF Wang, Jinguang
   Qian, Shengsheng
   Hu, Jun
   Hong, Richang
TI Comment-Context Dual Collaborative Masked Transformer Network for Fake
   News Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fake news detection; multi-modal learning; social media
AB The rapid proliferation of social media data has led to the widespread dissemination of multi-modal fake news, prompting researchers to develop novel detection methods. Most fake news detection approaches mine the rich context information, including the text and image content of news and associated comments. However, existing methods are often insufficient to filter out irrelevant contexts, such as noisy words, redundant image regions, and spam comments, which may introduce noise into the model. Particularly, these approaches struggle to handle comments, which often contain the most severe noise. In many cases, only a minuscule portion of comments is relevant to the news. To overcome these limitations, our research introduces a novel Comment-Context Dual Collaborative Masked Transformer Network ((CDCMTN)-D-2). To handle the irrelevant contexts, we propose a Multi-modal Masked Transformer Network. This network extends the traditional Transformer with a mask mechanism capable of dynamically obscuring irrelevant multi-modal context information. To effectively deal with comments, which can suffer from more severe noise issues, we have designed a Comment-Context Encoder that focuses solely on the most crucial comments. Comprehensive experiments on two publicly available real-world datasets confirm that (CDCMTN)-D-2 outperforms state-of-the-art methods.
C1 [Wang, Jinguang; Hong, Richang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
   [Wang, Jinguang] Chinaso Inc, Res Inst, Beijing 100077, Peoples R China.
   [Qian, Shengsheng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Qian, Shengsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Hu, Jun] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Hong, Richang] Hefei Comprehens Natl ScienceCenter, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
C3 Hefei University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; National University of Singapore
RP Hong, RC (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
EM wangjinguang502@gmail.com; shengsheng.qian@nlpr.ia.ac.cn;
   jun.hu@nus.edu.sg; hongrc.hfut@gmail.com
OI Wang, Jinguang/0000-0003-1938-4692
FU National Key Research and Development Program of China
FX No Statement Available
CR Baevski M., 2019, INT C LEARN REPRESEN, P1
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Castillo C., 2011, Proceedings of the 20th international conference on World wide web, P675, DOI [DOI 10.1145/1963405.1963500, 10.1145/1963405.1963500]
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen T, 2018, LECT NOTES ARTIF INT, V11154, P40, DOI 10.1007/978-3-030-04503-6_4
   Chen YX, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P2897, DOI 10.1145/3485447.3511968
   Cheng WH, 2021, ACM COMPUT SURV, V54, DOI [10.1145/3447239, 10.1145/3552468.3554360]
   Cui LM, 2020, Arxiv, DOI arXiv:2006.00885
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dou YT, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P2051, DOI 10.1145/3404835.3462990
   Guo H, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P943, DOI 10.1145/3269206.3271709
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Kalamaras I, 2014, IEEE T MULTIMEDIA, V16, P1460, DOI 10.1109/TMM.2014.2316473
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Kou ZY, 2020, IEEE INT CONF BIG DA, P631, DOI 10.1109/BigData50022.2020.9378019
   Kwon S, 2013, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2013.61
   Li HY, 2015, P ANN HICSS, P2003, DOI 10.1109/HICSS.2015.240
   Lin HZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P10035
   Liu Xiaomo, 2015, P 24 ACM INT C INF K, P1867, DOI [DOI 10.1145/2806416.2806651, 10.1145/2806416.2806651]
   Lo H. X., 2021, IEEE INT C MULTIMEDI, P1
   Ma J., 2016, 25 INT JOINT C ART I
   Ma Jing, 2015, P 24 ACM INT C INF K, P1751
   Paszke A, 2017, NIPS W
   Procter R, 2013, POLIC SOC, V23, P413, DOI 10.1080/10439463.2013.780223
   Qian SS, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P153, DOI 10.1145/3404835.3462871
   Radford A, 2018, Improving language understanding by generative Pre-Training
   Rae Jack W., 2020, INT C LEARN REPR ICL
   Shang LY, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P3623, DOI 10.1145/3485447.3512257
   Shu K, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P395, DOI 10.1145/3292500.3330935
   Singhal S, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P39, DOI [10.1109/BigMM.2019.00018, 10.1109/BigMM.2019.00-44]
   Subbalakshmi K. P., 2021, C ADV SOCIALNETW ANA, P31, DOI 10.1145/3487351
   Sun Y. Rao, 2023, INPROC AAAI C ARTIF
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2023, ACM T SENSOR NETWORK, V19, DOI 10.1145/3563776
   Wang S., 2023, IEEE Trans. Mul-timedia, early access, DOI [10.1109/TMM.2023.3263552,2023, DOI 10.1109/TMM.2023.3263552,2023]
   Wang SJ, 2018, AAAI CONF ARTIF INTE, P2532
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Ying L, 2021, IEEE ACCESS, V9, P132363, DOI 10.1109/ACCESS.2021.3114093
   Ying Q, 2023, PROCAAAI C ARTIF INT
   Yu F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3901
   Zhang HW, 2022, IEEE T MULTIMEDIA, V24, P1449, DOI 10.1109/TMM.2021.3065498
   Zhang WJ, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P3637, DOI 10.1145/3459637.3482196
   Zhou XY, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P3205, DOI 10.1145/3340531.3412880
   Zhou XY, 2020, LECT NOTES ARTIF INT, V12085, P354, DOI 10.1007/978-3-030-47436-2_27
NR 51
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5170
EP 5180
DI 10.1109/TMM.2023.3330074
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800005
DA 2024-08-05
ER

PT J
AU Wang, Q
   Li, S
   Wang, ZC
   Zhang, XP
   Feng, GR
AF Wang, Quan
   Li, Sheng
   Wang, Zichi
   Zhang, Xinpeng
   Feng, Guorui
TI Multi-Source Style Transfer via Style Disentanglement Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Style transfer; style disentanglement; style swap; content component;
   style component
AB Despite the great success of deep neural networks for style transfer tasks, the entanglement of content and style in images leads to more style information not being captured. To tackle this problem, a novel style disentanglement network is proposed to transfer multi-source style elements. Specifically, we specialize in designing a learnable content style separation module, which can efficiently extract content and style components from images in the latent space. This method differs from the previous approaches by predefining content and style layers in the network. Under the condition of content and style separation, we continue to propose the multi-style swap module, which allows the content image to match more style elements. Additionally, by introducing alternate training strategies for the main and auxiliary decoders as well as style disentanglement loss, the stylized results look very similar to the original artworks. Experimental results demonstrate the superiority of our proposed method compared with existing schemes.
C1 [Wang, Quan; Wang, Zichi; Zhang, Xinpeng; Feng, Guorui] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Li, Sheng] Fudan Univ, Sch Comp Sci, Shanghai 200438, Peoples R China.
C3 Shanghai University; Fudan University
RP Feng, GR (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM quan_w@shu.edu.cn; lisheng@fudan.edu.cn; wangzichi@shu.edu.cn;
   xzhang@shu.edu.cn; grfeng@shu.edu.cn
FU Science and Technology Planning Project of Zhejiang Province
FX No Statement Available
CR [Anonymous], 2016, ICML
   [Anonymous], 2016, Fast patch-based style transfer of arbitrary style
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen HB, 2021, PROC CVPR IEEE, P872, DOI 10.1109/CVPR46437.2021.00093
   Chen XH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3246, DOI 10.1145/3394171.3413770
   Chu WT, 2018, IEEE T MULTIMEDIA, V20, P2491, DOI 10.1109/TMM.2018.2801718
   Deng YY, 2021, IEEE T MULTIMEDIA, V23, P2794, DOI 10.1109/TMM.2020.3016887
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D.P., 2014, Proc. of ICLR
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452
   Kotovenko D, 2019, PROC CVPR IEEE, P10024, DOI 10.1109/CVPR.2019.01027
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee H., 2010, P 8 INT S NONPH AN R, P43, DOI DOI 10.1145/1809939.1809945
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li SH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1716, DOI 10.1145/3123266.3123425
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Li YJ, 2017, ADV NEUR IN, V30
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu X., 2017, P 15 INT S NONPHOTOR
   Mao XJ, 2016, ADV NEUR IN, V29
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Phillips F, 2011, ISS ACCOUNT EDUC, V26, P593, DOI 10.2308/iace-50038
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Semmo A., 2017, NPAR 2017, DOI 10.1145/3092919.3092920
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Svoboda Jan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13813, DOI 10.1109/CVPR42600.2020.01383
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2245, DOI 10.1109/TMM.2021.3087026
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759
   Yang LC, 2018, COMPUT GRAPH FORUM, V37, P97, DOI 10.1111/cgf.13551
   Zhang YL, 2019, IEEE I CONF COMP VIS, P5942, DOI 10.1109/ICCV.2019.00604
   Zhizhong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7786, DOI 10.1109/CVPR42600.2020.00781
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 48
TC 2
Z9 2
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1373
EP 1383
DI 10.1109/TMM.2023.3281087
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700040
DA 2024-08-05
ER

PT J
AU Wang, SW
   Shen, LQ
   Liu, JY
AF Wang, Shiwei
   Shen, Liquan
   Liu, Jingyue
TI Spatial-Temporal Inter-Layer Reference Frame Generation Network for
   Spatial SHVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reference frame reconstruction; inter prediction; SHVC
ID SCALABLE EXTENSIONS; NEURAL-NETWORK; VIDEO; HEVC
AB In the current spatial Scalable High Efficiency Video Coding (SHVC) standard, the main techniques involve exploiting the correlation between pixel values of different layers to achieve inter-layer prediction samples, allowing the enhancement layer (EL) to predict samples from the upsampled base layer (BL) frame and remove temporal redundancy. However, existing network-based methods cannot effectively handle multi-layer compressed images with different resolutions to generate reference frame in spatial SHVC. Meanwhile, spatial SHVC only uses traditional interpolation filters to upsample the BL frame for EL frame sample prediction, which cannot handle different structures and contents. Therefore, considering the high correlation of multi-scale distortion characteristics across different layers, this article proposes a spatial-temporal inter-layer reference frame generation network (ST-ILR) for spatial SHVC, which can generate a high-fidelity reference frame for efficient inter-prediction and insert it into the EL reference picture list. The proposed method consists of two modules: a multi-scale motion restoration (MMR) module and a guided multi-scale feature reconstruction (GMFR) module. The MMR model is designed to accurately predict the motion trend of the EL based on the BL motion information, while implicitly compensating for previous EL frames. This is achieved by dynamically modeling the current EL motion information from the BL, capturing compression downsampling differences of prior motion vectors across different layers. The GMFR module adaptively super-resolves compressed BL frames and selectively aggregates high-frequency information from aligned EL features to preserve precise spatial detail, fusing abundant features from different layers to achieve better ILR frame quality performance. Extensive experiments show that our network achieves a 13.6% BD-rate (Bjontegaard Delta Rate) reduction in random access configuration compared to the SHVC baseline, which offers state-of-the-art coding performance.
C1 [Wang, Shiwei; Liu, Jingyue] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Shen, Liquan] Shanghai Univ, Key Lab Adv Display & Syst Applicat, Minist Educ, Shanghai 200072, Peoples R China.
C3 Shanghai University; Shanghai University
RP Shen, LQ (corresponding author), Shanghai Univ, Key Lab Adv Display & Syst Applicat, Minist Educ, Shanghai 200072, Peoples R China.
EM ieemia@shu.edu.cn; jsslq@163.com; m15387311031@163.com
RI Shen, Liquan/D-4832-2012
OI Shen, Liquan/0000-0002-2148-6279; Liu, Jingyue/0009-0000-7637-893X
FU National Natural Science Foundation of China
FX No Statement Available
CR Aminlou A, 2014, IEEE T CIRC SYST VID, V24, P1945, DOI 10.1109/TCSVT.2014.2317884
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Chakraborty S., 2014, PROC IEEE 16 INT WOR, P1
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Ding DD, 2022, IEEE T IMAGE PROCESS, V31, P773, DOI 10.1109/TIP.2021.3134465
   Ding Q, 2021, IEEE T IMAGE PROCESS, V30, P6459, DOI 10.1109/TIP.2021.3092949
   Ding Q, 2020, IEEE SIGNAL PROC LET, V27, P2049, DOI 10.1109/LSP.2020.3037683
   Dong CH, 2022, IEEE INT SYMP CIRC S, P3190, DOI 10.1109/ISCAS48785.2022.9937596
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   He G, 2022, IEEE T CIRC SYST VID, V32, P3217, DOI 10.1109/TCSVT.2021.3096072
   HoangVan X, 2019, IEEE T BROADCAST, V65, P504, DOI 10.1109/TBC.2018.2881355
   Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kim BJ, 1997, IEEE DATA COMPR CONF, P251, DOI 10.1109/DCC.1997.582048
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim SY, 2019, IEEE IMAGE PROC, P2831, DOI [10.1109/ICIP.2019.8803297, 10.1109/icip.2019.8803297]
   Kingma D. P., 2015, ICLR, P1
   Lai PL, 2013, PICT COD SYMP, P117, DOI 10.1109/PCS.2013.6737697
   Lee H, 2015, IEEE T BROADCAST, V61, P388, DOI 10.1109/TBC.2015.2419172
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Liu JY, 2020, IEEE T MULTIMEDIA, V22, P2497, DOI 10.1109/TMM.2019.2961504
   Liu Xiaoyong, 2022, Digital TV and Wireless Multimedia Communications: 18th International Forum, IFTC 2021, Revised Selected Papers. Communications in Computer and Information Science (1560), P412, DOI 10.1007/978-981-19-2266-4_32
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Paul M, 2010, INT CONF ACOUST SPEE, P734, DOI 10.1109/ICASSP.2010.5495033
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Secker A, 2003, IEEE T IMAGE PROCESS, V12, P1530, DOI 10.1109/TIP.2003.819433
   Seregin V., 2014, PROC DOCUMENT JCTVCQ, P1
   Seregin Vadim, 2014, JCTVCQ1009, P1
   Song Q, 2020, IEEE T CIRC SYST VID, V30, P2575, DOI 10.1109/TCSVT.2019.2928270
   Suehring K., 2013, Common HM test conditions and software reference configurations (ICTVC-L1100)
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Wang X., 2018, PROC EUR C COMPUT VI, P1
   Wu Y., 2008, IEEE Trans. Circuits Syst., V8, P697
   HoangVan X, 2018, PROC INT WORKSH ADV
   Yan N, 2017, IEEE INT SYMP CIRC S, P822
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Ye Y, 2014, IEEE MULTIMEDIA, V21, P58, DOI 10.1109/MMUL.2014.47
   Yu SW, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102764
   Zhang H., 2017, PROC IEEE VIS COMMUN, P1
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao L, 2019, IEEE T IMAGE PROCESS, V28, P4832, DOI 10.1109/TIP.2019.2913545
NR 44
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3235
EP 3250
DI 10.1109/TMM.2023.3308444
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200030
DA 2024-08-05
ER

PT J
AU Wang, TY
   Li, Z
   Liu, RX
   Wang, YL
   Nie, LQ
AF Wang, Tianyi
   Li, Zian
   Liu, Ruixia
   Wang, Yinglong
   Nie, Liqiang
TI An Efficient Attribute-Preserving Framework for Face Swapping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face swapping; attribute preservation; facial landmarks; generative
   adversarial network
ID FEATURES
AB By leveraging deep neural networks, recent face swapping techniques have performed admirably in generating faces that maintain consistent identities. Nevertheless, while these methods accurately transfer source identities, they often struggle to preserve important attributes (such as head poses, expressions, and gaze directions) in the target faces. As a consequence, the current research in this domain has not resulted in satisfactory performance. In this article, we propose an efficient attribute-preserving framework, called AP-Swap, for short, for face swapping. Our approach incorporates two innovative modules designed specifically to preserve critical facial attributes. First, we propose a global residual attribute-preserving encoder (GRAPE), which adaptively extracts globally complete attribute features from target faces. Second, in addition to the regular network streams for the source and target facial images, we introduce a network stream that takes into account the facial landmarks of the target faces. This additional stream enables our landmark-guided feature entanglement module (LFEM), which efficiently preserves fine-grained facial attributes by conducting a landmark-based attribute-preserving (LBAP) operation. Through extensive quantitative and qualitative experiments, we demonstrate the superiority of AP-Swap over other state-of-the-art methods in terms of facial attribute preservation and model efficiency, along with satisfactory identity consistency performance.
C1 [Wang, Tianyi] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Li, Zian; Liu, Ruixia] Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China.
   [Wang, Yinglong] Qilu Univ Technol, Shandong Acad Sci, Key Lab Comp Power Network & Informat Secur, Minist Educ, Jinan 250014, Peoples R China.
   [Nie, Liqiang] Harbin Inst Technol Shenzhen, Dept Comp Sci & Technol, Shenzhen 518055, Peoples R China.
C3 University of Hong Kong; Qilu University of Technology; Qilu University
   of Technology; Harbin Institute of Technology
RP Wang, YL (corresponding author), Qilu Univ Technol, Shandong Acad Sci, Key Lab Comp Power Network & Informat Secur, Minist Educ, Jinan 250014, Peoples R China.
EM tywang@cs.hku.hk; lza1611@163.com; liurx@sdas.org;
   wangyinglong@qlu.edu.cn; nieliqiang@gmail.com
OI Wang, Yinglong/0000-0002-8350-7186; Wang, Tianyi/0000-0003-2920-6099
CR Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702
   Bitouk D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360638
   Blanz V, 2004, COMPUT GRAPH FORUM, V23, P669, DOI 10.1111/j.1467-8659.2004.00799.x
   Chang JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9660, DOI 10.1109/ICCV48922.2021.00954
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Chen Y, 2021, IEEE T MULTIMEDIA, V23, P3098, DOI 10.1109/TMM.2020.3020693
   Cheng Y.-T., 2009, PROC SIGGRAPH 09 POS
   Choi KH, 2005, IEEE T MULTIMEDIA, V7, P628, DOI 10.1109/TMM.2005.850964
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Ding F, 2021, IEEE T MULTIMEDIA, V24, P3429, DOI 10.1109/TMM.2021.3098422
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Dolhansky B, 2019, Arxiv, DOI arXiv:1910.08854
   DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X
   Gao GG, 2021, PROC CVPR IEEE, P3403, DOI 10.1109/CVPR46437.2021.00341
   github, 2017, deepfakes
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hsu GS, 2022, PROC CVPR IEEE, P632, DOI 10.1109/CVPR52688.2022.00072
   Huang PH, 2020, PROC CVPR IEEE, P7082, DOI 10.1109/CVPR42600.2020.00711
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, INT C LEARNING REPRE
   Kim J, 2022, PROC CVPR IEEE, P10769, DOI 10.1109/CVPR52688.2022.01051
   Kingma D. P., 2014, P 2 INT C LEARNING R, P1
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HL, 2011, IEEE T MULTIMEDIA, V13, P1230, DOI 10.1109/TMM.2011.2168814
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Li Q., 2022, arXiv
   Liu J., 2021, PROC IEEE INT C MULT, P1
   Liu SG, 2019, IEEE T MULTIMEDIA, V21, P2461, DOI 10.1109/TMM.2019.2903413
   Liuet K., 2023, Pattern Recognit., V141
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Natsume R., 2018, PROC ACM SIGGRAPH PO, P1
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Rosberg F, 2023, IEEE WINT CONF APPL, P3443, DOI 10.1109/WACV56688.2023.00345
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Thompson NC, 2021, IEEE SPECTRUM, V58, P50, DOI 10.1109/MSPEC.2021.9563954
   Tripathy S, 2022, IEEE WINT CONF APPL, P2121, DOI 10.1109/WACV51458.2022.00218
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang TY, 2023, AAAI CONF ARTIF INTE, P14548
   Wang TY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3588574
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37
   Xu C, 2022, LECT NOTES COMPUT SC, V13675, P54, DOI 10.1007/978-3-031-19784-0_4
   Xu C, 2022, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR52688.2022.00748
   Xu YY, 2022, PROC CVPR IEEE, P7632, DOI 10.1109/CVPR52688.2022.00749
   Xu ZL, 2022, AAAI CONF ARTIF INTE, P2973
   Xu ZL, 2022, LECT NOTES COMPUT SC, V13674, P661, DOI 10.1007/978-3-031-19781-9_38
   Yang KW, 2022, LECT NOTES COMPUT SC, V13673, P55, DOI 10.1007/978-3-031-19778-9_4
   Yuan Lin, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P333, DOI 10.1109/ICME.2012.26
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhang LH, 2022, IEEE T CIRC SYST VID, V32, P2226, DOI 10.1109/TCSVT.2021.3089724
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
NR 58
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6554
EP 6565
DI 10.1109/TMM.2024.3354573
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600054
DA 2024-08-05
ER

PT J
AU Wu, Y
   Liu, JM
   Gong, MG
   Liu, ZX
   Miao, QG
   Ma, WP
AF Wu, Yue
   Liu, Jiaming
   Gong, Maoguo
   Liu, Zhixiao
   Miao, Qiguang
   Ma, Wenping
TI MPCT: Multiscale Point Cloud Transformer With a Residual Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geometric and semantic features; multiscale generation; point cloud
   transformer; residual network
AB The self-attention (SA) network revisits the essence of data and has achieved remarkable results in text processing and image analysis. SA is conceptualized as a set operator that is insensitive to the order and number of data, making it suitable for point sets embedded in 3D space. However, working with point clouds still poses challenges. To tackle the issue of exponential growth in complexity and singularity induced by the original SA network without position encoding, we modify the attention mechanism by incorporating position encoding to make it linear, thus reducing its computational cost and memory usage and making it more feasible for point clouds. This article presents a new framework called multiscale point cloud transformer (MPCT), which improves upon prior methods in cross-domain applications. The utilization of multiple embeddings enables the complete capture of the remote and local contextual connections within point clouds, as determined by our proposed attention mechanism. Additionally, we use a residual network to facilitate the fusion of multiscale features, allowing MPCT to better comprehend the representations of point clouds at each stage of attention. Experiments conducted on several datasets demonstrate that MPCT outperforms the existing methods, such as achieving accuracies of 94.2% and 84.9% in classification tasks implemented on ModelNet40 and ScanObjectNN, respectively.
C1 [Wu, Yue; Liu, Jiaming; Miao, Qiguang] Xidian Univ, Sch Comp Sci & Technol, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
   [Gong, Maoguo] Xidian Univ, Sch Elect Engn, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
   [Liu, Zhixiao] Harbin Engn Univ, Yantai Res Inst, Yantai 264006, Peoples R China.
   [Ma, Wenping] Xidian Univ, Sch Artificial Intelligence, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University; Harbin Engineering University;
   Xidian University
RP Gong, MG (corresponding author), Xidian Univ, Sch Elect Engn, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
EM ywu@xidian.edu.cn; ljm@stu.xidian.edu.cn; gong@ieee.org;
   robinliu@hrbeu.edu.cn; qgmiao@mail.xidian.edu.cn;
   wpma@mail.xidian.edu.cn
OI Liu, Jiaming/0009-0003-9699-1987
FU National Natural Science Foundation of China
FX No Statement Available
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Ben-Shabat Y, 2018, IEEE ROBOT AUTOM LET, V3, P3145, DOI 10.1109/LRA.2018.2850061
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han XF, 2023, IEEE T MULTIMEDIA, V25, P5638, DOI 10.1109/TMM.2022.3198318
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jing WP, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14041036
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Li YZ, 2018, ADV NEUR IN, V31
   Lin XX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P807, DOI 10.1145/3474085.3475252
   Liu J., IEEE T INSTRUM MEAS
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Loshchilov I., 2016, arXiv
   Loshchilov I., 2018, INT C LEARN REPR
   Lv CL, 2022, IEEE T MULTIMEDIA, V24, P1815, DOI 10.1109/TMM.2021.3073265
   Lyu YC, 2022, IEEE WINT CONF APPL, P256, DOI 10.1109/WACV51458.2022.00033
   Ma Xu, 2022, P INT C LEARN REPR
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mérigot Q, 2011, IEEE T VIS COMPUT GR, V17, P743, DOI 10.1109/TVCG.2010.261
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qian G. C., 2022, ADV NEURAL INFORM PR, V35, P23192, DOI [DOI 10.48550/ARXIV.2206.04670, https://doi.org/10.48550/arXiv.2206.04670]
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Qiu S, 2021, IEEE WINT CONF APPL, P3812, DOI 10.1109/WACV48630.2021.00386
   Shan JY, 2021, IEEE INT C INT ROBOT, P1310, DOI 10.1109/IROS51168.2021.9636821
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun Chao, 2022, IEEE T MULTIMEDIA
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tao WX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5266, DOI 10.1145/3474085.3475645
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CS, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3170493
   Wang GH, 2022, KNOWL-BASED SYST, V247, DOI 10.1016/j.knosys.2022.108769
   Wang HQ, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-020-9521-2
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Weng T., 2022, IEEE Trans. Multimedia
   Wu B., 2020, Visual transformers: Tokenbased image representation and processing for computer vision
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu Y, 2024, IEEE T EVOLUT COMPUT, V28, P62, DOI 10.1109/TEVC.2022.3215743
   Wu Y, 2024, IEEE T MULTIMEDIA, V26, P1626, DOI 10.1109/TMM.2023.3284591
   Wu Y, 2024, IEEE T EM TOP COMP I, V8, P110, DOI 10.1109/TETCI.2023.3290009
   Wu Y, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3286943
   Wu Y, 2023, IEEE T CIRC SYST VID, V33, P3585, DOI 10.1109/TCSVT.2023.3237328
   Wu Y, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3271757
   Wu Y, 2023, IEEE T CIRC SYST VID, V33, P1413, DOI 10.1109/TCSVT.2022.3213592
   Wu Y, 2023, IEEE T EM TOP COMP I, V7, P357, DOI 10.1109/TETCI.2022.3205384
   Wu Y, 2022, IEEE T NEUR NET LEAR, V33, P4257, DOI 10.1109/TNNLS.2021.3056238
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   You Y, 2022, IEEE T PATTERN ANAL, V44, P9489, DOI 10.1109/TPAMI.2021.3130590
   Zhang C, 2022, PROC CVPR IEEE, P11789, DOI 10.1109/CVPR52688.2022.01150
   Zhang YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1230, DOI 10.1145/3474085.3475294
   Zhang ZC, 2023, CAAI T INTELL TECHNO, V8, P987, DOI 10.1049/cit2.12141
   Zhao H., 2020, P IEEECVF C COMPUTER, P10076, DOI [10.1109/CVPR42600.2020.01009, DOI 10.1109/CVPR42600.2020.01009]
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhen Q, 2022, P INT C LEARN REPR
   Zhou CQ, 2022, PROC CVPR IEEE, P8521, DOI 10.1109/CVPR52688.2022.00834
   Zhou HR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4945, DOI 10.1109/ICCV48922.2021.00492
NR 70
TC 4
Z9 4
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3505
EP 3516
DI 10.1109/TMM.2023.3312855
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200005
DA 2024-08-05
ER

PT J
AU Xiao, YP
   Li, XH
   Zhang, QQ
   Lv, R
   Li, Q
   Wang, R
AF Xiao, Yunpeng
   Li, Xuehong
   Zhang, Qunqing
   Lv, Rui
   Li, Qian
   Wang, Rong
TI Spreading Mosaic: An Image Restoration-Inspired Social Rumor Propagation
   Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Behavioral sciences; Image restoration; Predictive models; Image
   quality; Uncertainty; Prediction algorithms; Task analysis;
   representation learning; rumor propagation; social networks; topic
   pixelation
ID INFORMATION DIFFUSION; FAKE NEWS; TWITTER
AB This article draws inspiration from deep learning image restoration technology. If users involved in the rumor topic are regarded as pixels in an image, the uncertainty of user behavior is similar to the ambiguity of pixels in a mosaic image. The prediction of user behavior is influenced by the user and neighboring friends. Similarly, the recovery of mosaic image pixels is also influenced by these pixels and neighboring pixels. Thus, during rumor propagation, the prediction of user behavior is equivalent to the restoration of pixels in the mosaic image. Based on this inspiration, this study proposes a rumor propagation prediction model based on image restoration technology. First, we propose the concept of topic images and design the rumor2pixel algorithm to pixelate the topic of rumor propagation. Second, through the Generative Adversarial Network model, fuzzy pixels in the "rumor topic image" are compensated to learn more realistic rumor propagation trends. Finally, a dynamic approach for predicting the propagation of rumor and countering it based on evolutionary game theory is proposed, named Rumor-DPM (rumor dynamic propagation model). This approach is focused on reconstructing rumor images while taking into account the conflict between rumors and anti-rumors as well as its timeliness. The experimental findings demonstrate that this strategy can more accurately depict the internal dynamics between rumors and anti-rumors and effectively and successfully improve the ability to forecast user behavior throughout the rumor-propagation process.
C1 [Xiao, Yunpeng; Li, Xuehong; Zhang, Qunqing; Lv, Rui; Li, Qian; Wang, Rong] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Xiao, YP (corresponding author), Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
EM xiaoyp@cqupt.edu.cn; lixuehongcqupt@gmail.com;
   s210231258@stu.cqupt.edu.cn; llvrui@163.com; liqian@cqupt.edu.cn;
   wangrong1@cqupt.edu.cn
RI Li, Xuehong/JEO-7945-2023
OI Li, Xuehong/0000-0002-7024-9296; Li, Qian/0000-0003-3905-8173; Xiao,
   Yunpeng/0000-0002-2846-3571; Wang, Rong/0000-0002-7963-1766
FU National Natural Science Foundation of China
FX No Statement Available
CR Alkhodair SA, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.02.016
   Bao YY, 2013, 2013 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM), P1472
   Cai KC, 2018, IEEE ACM T NETWORK, V26, P478, DOI 10.1109/TNET.2018.2791412
   Chen XQ, 2019, PROC INT CONF DATA, P770, DOI 10.1109/ICDE.2019.00074
   Choi J, 2020, IEEE ACM T NETWORK, V28, P2271, DOI 10.1109/TNET.2020.3009946
   Dutta HS, 2018, 2018 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM), P242, DOI 10.1109/ASONAM.2018.8508801
   Fang LT, 2023, IEEE T KNOWL DATA EN, V35, P10309, DOI 10.1109/TKDE.2023.3267821
   Fu TY, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1797, DOI 10.1145/3132847.3132953
   Gao S, 2017, IEEE T IND INFORM, V13, P2097, DOI 10.1109/TII.2017.2684160
   Grinberg N, 2019, SCIENCE, V363, P374, DOI 10.1126/science.aau2706
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Guo JX, 2021, IEEE ACM T NETWORK, V29, P386, DOI 10.1109/TNET.2020.3032893
   Guo Z, 2022, IEEE T NETW SCI ENG, V9, P3775, DOI 10.1109/TNSE.2022.3181130
   Han QY, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P1347, DOI 10.1109/ICCT.2017.8359853
   Hu B, 2019, IEEE T MULTIMEDIA, V21, P2042, DOI 10.1109/TMM.2019.2894958
   Jiang B, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P977, DOI 10.1145/2911451.2914713
   Jiang JJ, 2018, IEEE T DEPEND SECURE, V15, P166, DOI 10.1109/TDSC.2016.2522436
   Kandhway K, 2017, IEEE T SYST MAN CY-S, V47, P1099, DOI 10.1109/TSMC.2016.2531690
   Lazer DMJ, 2018, SCIENCE, V359, P1094, DOI 10.1126/science.aao2998
   Li JS, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1531, DOI 10.1145/3123266.3123438
   Li YW, 2023, PROC CVPR IEEE, P18278, DOI 10.1109/CVPR52729.2023.01753
   Liu B, 2024, IEEE T NEUR NET LEAR, V35, P4887, DOI 10.1109/TNNLS.2022.3161697
   Lu FH, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3524137
   Luo ZC, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P869
   Ma RF, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P525, DOI 10.1145/3331184.3331236
   Myers SA, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P493, DOI 10.1145/2567948.2576939
   Najar A., 2012, PROC 21 INT C WORLD, P1197
   Shu K, 2020, BIG DATA, V8, P171, DOI 10.1089/big.2020.0062
   Song CH, 2021, IEEE T KNOWL DATA EN, V33, P3035, DOI 10.1109/TKDE.2019.2961675
   Tang J, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1165, DOI 10.1145/2783258.2783307
   Tu XG, 2022, IEEE T CIRC SYST VID, V32, P1285, DOI 10.1109/TCSVT.2021.3078517
   Vosoughi S, 2018, SCIENCE, V359, P1146, DOI 10.1126/science.aap9559
   Wei LW, 2024, IEEE T NEUR NET LEAR, V35, P2522, DOI 10.1109/TNNLS.2022.3190348
   Weng J., 2010, P 3 ACM INT C WEB SE, P261, DOI DOI 10.1145/1718487.1718520
   Xiao YP, 2023, IEEE T KNOWL DATA EN, V35, P4682, DOI 10.1109/TKDE.2022.3144310
   Xiao YP, 2020, IEEE T NETW SERV MAN, V17, P1910, DOI 10.1109/TNSM.2020.2994141
   Yamaguchi Y, 2010, LECT NOTES COMPUT SC, V6488, P240, DOI 10.1007/978-3-642-17616-6_22
   Yang Z., 2010, P 19 ACM INT C INF K, P1633, DOI DOI 10.1145/1871437.1871691
   Yu ZW, 2015, ACM T KNOWL DISCOV D, V10, DOI 10.1145/2742801
   Zhang HW, 2022, IEEE T MULTIMEDIA, V24, P1449, DOI 10.1109/TMM.2021.3065498
   Zhang Q, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P75, DOI 10.1145/2983323.2983809
   Zhang XX, 2022, IEEE T NETW SCI ENG, V9, P2353, DOI 10.1109/TNSE.2022.3163203
   Zhang ZY, 2015, IEEE T EMERG TOP COM, V3, P410, DOI 10.1109/TETC.2015.2398353
NR 43
TC 1
Z9 1
U1 13
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2906
EP 2917
DI 10.1109/TMM.2023.3305095
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400025
DA 2024-08-05
ER

PT J
AU Xu, J
   Zhang, XQ
   Zhao, CM
   Geng, ZL
   Feng, YR
   Miao, K
   Li, YJ
AF Xu, Jie
   Zhang, Xiaoqian
   Zhao, Changming
   Geng, Zili
   Feng, Yuren
   Miao, Ke
   Li, Yunji
TI Improving Fine-Grained Image Classification With Multimodal Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image classification; Visualization; Data mining;
   Birds; Spatiotemporal phenomena; Fuses; Multimodal information;
   fine-grained image classification; multi-temporal feature fusion;
   self-attention; dynamic MLP
ID NETWORK
AB Fine-grained image datasets have small inter-class differences and large intra-class differences, which is a difficulty of the fine-grained image classification. Traditional fine-grained image classification methods only focus on the visual features of images. However, this limitation can be eliminated when these methods are improved with multimodal information. This paper proposes an improved fine-grained image classification method with multimodal information that includes multimodal data preprocessing, multimodal feature extraction, multi-temporal feature fusion and decision correction. The preprocessing method proposed solves the problems of scattered distribution, difficult processing and uneven contribution to prediction of multimodal data through normalization, packing phrases and weighted concatenating methods. When extracting multimodal features, the SAMLP (Self-Attention MLP) module proposed combines self-attention with MLP to capture the internal correlation of multimodal information. The multi-temporal feature fusion proposed is divided into early feature fusion and late feature fusion. The former refers to adding multimodal information markers to the original image, and the latter refers to designing a multi-cascade dynamic MLP structure to fuse visual features and multimodal features. In view of the limitation of feature fusion, a decision strategy is proposed to revise the prediction results of fused features according to the prediction results of multimodal features. Ablation experiment on INAT18-1K and INAT21-1K datasets shows that our method is effective in improving classification with multimodal information. Experiments on the INAT2021_mini large dataset show that the comprehensive method in this article has higher accuracy and negligible efficiency loss compared with the state-of-the-art method.
C1 [Xu, Jie; Zhang, Xiaoqian; Geng, Zili; Feng, Yuren; Miao, Ke] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Zhao, Changming; Li, Yunji] Chengdu Univ Informat Technol, Sch Comp Sci, Chengdu 610225, Peoples R China.
C3 University of Electronic Science & Technology of China; Chengdu
   University of Information Technology
RP Zhao, CM (corresponding author), Chengdu Univ Informat Technol, Sch Comp Sci, Chengdu 610225, Peoples R China.
EM xuj@uestc.edu.cn; 202122010730@std.uestc.edu.cn; zcm84@cuit.edu.cn;
   2019010801025@std.uestc.edu.cn; fengyr@std.uestc.edu.cn;
   202221011326@std.uestc.edu.cn; liyunji@cuit.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Agrawal Aishwarya, 2016, C EMPIRICAL METHODS, P1955, DOI [10. 18653/v1/D16-1203, DOI 10.18653/V1/D16-1203]
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Behera A, 2021, AAAI CONF ARTIF INTE, V35, P929
   Branson G., 2014, BRITMACH VIS C, P1
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chu G, 2019, IEEE INT CONF COMP V, P247, DOI 10.1109/ICCVW.2019.00033
   Fan Zhang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P136, DOI 10.1007/978-3-030-67832-6_12
   Fu JM, 2022, IEEE IMAGE PROC, P2846, DOI 10.1109/ICIP46576.2022.9897323
   Fu W., 2017, CORR, P1
   Gao Y, 2020, AAAI CONF ARTIF INTE, V34, P10818
   He J, 2022, AAAI CONF ARTIF INTE, P852
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang SL, 2021, AAAI CONF ARTIF INTE, V35, P1628
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Li H, 2020, IEEE I C VI COM I PR, P243, DOI [10.1109/VCIP49819.2020.9301763, 10.1109/vcip49819.2020.9301763]
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P543, DOI 10.1109/TMM.2021.3128744
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Liu HB, 2022, IEEE T MULTIMEDIA, V24, P2902, DOI 10.1109/TMM.2021.3090274
   Liu XD, 2022, NEUROCOMPUTING, V492, P137, DOI 10.1016/j.neucom.2022.04.037
   Mac Aodha O, 2019, IEEE I CONF COMP VIS, P9595, DOI 10.1109/ICCV.2019.00969
   Minetto R, 2019, IEEE T GEOSCI REMOTE, V57, P6530, DOI 10.1109/TGRS.2019.2906883
   Nitta N, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9060354
   Prakash A, 2021, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR46437.2021.00700
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Salem Tawfiq, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12432, DOI 10.1109/CVPR42600.2020.01245
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shih A., 2015, BRITISHMACH VIS C
   Tang K, 2015, IEEE I CONF COMP VIS, P1008, DOI 10.1109/ICCV.2015.121
   Terry JCD, 2020, METHODS ECOL EVOL, V11, P303, DOI 10.1111/2041-210X.13335
   Touvron H, 2019, PROC ADV NEURAL INF, P1549
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P854, DOI 10.1109/ICCV48922.2021.00091
   Van Horn G, 2021, PROC CVPR IEEE, P12879, DOI 10.1109/CVPR46437.2021.01269
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Wittich HC, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2201-7
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xu J, 2021, NEUROCOMPUTING, V441, P350, DOI 10.1016/j.neucom.2020.04.150
   Xu J, 2020, PATTERN RECOGN, V99, DOI 10.1016/j.patcog.2019.107098
   Xu Q, 2023, IEEE T MULTIMEDIA, V25, P9015, DOI 10.1109/TMM.2023.3244340
   Xu Q, 2022, IEEE T MULTIMEDIA, V24, P567, DOI 10.1109/TMM.2021.3055362
   Yang LF, 2022, PROC CVPR IEEE, P10935, DOI 10.1109/CVPR52688.2022.01067
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Zhang CJ, 2023, IEEE T MULTIMEDIA, V25, P6756, DOI 10.1109/TMM.2022.3214431
   Zhang LB, 2022, IEEE T MULTIMEDIA, V24, P4409, DOI 10.1109/TMM.2021.3117064
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zheng J., 2019, ADV NEURAL INFPROCES, P1427
   Zheng XT, 2021, IEEE T MULTIMEDIA, V23, P1187, DOI 10.1109/TMM.2020.2993960
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 47
TC 1
Z9 1
U1 20
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2082
EP 2095
DI 10.1109/TMM.2023.3291819
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800031
DA 2024-08-05
ER

PT J
AU Xu, LF
   Wu, QB
   Pan, LL
   Meng, FM
   Li, HL
   He, CY
   Wang, HX
   Cheng, SX
   Dai, Y
AF Xu, Linfeng
   Wu, Qingbo
   Pan, Lili
   Meng, Fanman
   Li, Hongliang
   He, Chiyuan
   Wang, Hanxin
   Cheng, Shaoxu
   Dai, Yu
TI Towards Continual Egocentric Activity Recognition: A Multi-Modal
   Egocentric Activity Dataset for Continual Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-modal dataset; egocentric activity recognition; continual
   learning; wearable device
ID REAL-WORLD DATASET; GAZE
AB With the rapid development of wearable cameras, it is now feasible to considerably increase the collection of egocentric video for first-person visual perception. However, the development is hindered by a shortage of multi-modal egocentric activity datasets. Furthermore, the catastrophic forgetting problem of multimodal continual activity learning, as a branch of continual learning, has not been thoroughly explored, which makes accumulating a larger collection of multi-modal activity data more urgent. To address this shortage, we propose a multi-modal egocentric activity dataset for continual activity learning named UESTC-MMEA-CL in this paper. The dataset is collected using our self-developed glasses with a first-person camera and wearable sensors, and it contains synchronized data of video, accelerometers, and gyroscopes for 32 types of daily activities performed by 10 participants who wore our glasses. Statistical analysis of the sensor data is given to show the auxiliary effects of activity recognition. We report the results of egocentric activity recognition of three modalities (RGB, acceleration, and gyroscope) separately and jointly on a base network architecture. We thoroughly evaluated four baseline methods with different multimodal combinations to explore the catastrophic forgetting in continual learning on UESTC-MMEA-CL. We hope that the UESTC-MMEA-CL dataset can act as a facilitator for future studies on continual learning for first-person activity recognition in wearable applications. You can download preliminary data from https://ivipclab.github.io/publication_uestc-mmea-cl/mmea-cl. The data is currently used to solve the problems of multimodal continual learning of activities.
C1 [Xu, Linfeng; Wu, Qingbo; Pan, Lili; Meng, Fanman; Li, Hongliang; He, Chiyuan; Wang, Hanxin; Cheng, Shaoxu; Dai, Yu] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Xu, LF; Wu, QB; Pan, LL; Meng, FM (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM lfxu@uestc.edu.cn; qbwu@uestc.edu.cn; lilipan@uestc.edu.cn;
   fmmeng@uestc.edu.cn; hlli@uestc.edu.cn; 202121011634@std.uestc.edu.cn;
   hxwang09@std.uestc.edu.cn; 202222011833@std.uestc.edu.cn;
   ydai@std.uestc.edu.cn
RI ; Wu, Qingbo/M-5065-2015; Xu, Linfeng/HME-1913-2023
OI Pan, Lili/0000-0003-3903-071X; Wu, Qingbo/0000-0003-2936-6340; Wang,
   Hanxin/0009-0007-4405-2709; Cheng, Shaoxu/0009-0001-9968-6780; Xu,
   Linfeng/0000-0002-9934-0958; Dai, Yu/0009-0002-3695-4159; He,
   Chiyuan/0000-0002-6626-432X
FU National Key Ramp;D Program of China
FX No Statement Available
CR Abati D, 2020, PROC CVPR IEEE, P3930, DOI 10.1109/CVPR42600.2020.00399
   Ahn D, 2023, IEEE WINT CONF APPL, P3319, DOI 10.1109/WACV56688.2023.00333
   Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753
   Bambach S, 2015, IEEE I CONF COMP VIS, P1949, DOI 10.1109/ICCV.2015.226
   Bernal EA, 2018, IEEE T MULTIMEDIA, V20, P107, DOI 10.1109/TMM.2017.2726187
   Beyan C, 2021, IEEE T MULTIMEDIA, V23, P2071, DOI 10.1109/TMM.2020.3007350
   Brousmiche M., 2021, arXiv
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Chen JW, 2022, IEEE WINT CONF APPL, P786, DOI 10.1109/WACV51458.2022.00086
   Damen D, 2022, INT J COMPUT VISION, V130, P33, DOI 10.1007/s11263-021-01531-2
   Dave IR, 2023, PROC CVPR IEEE, P2341, DOI 10.1109/CVPR52729.2023.00232
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Douillard Arthur, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P86, DOI 10.1007/978-3-030-58565-5_6
   Douillard A, 2021, PROC CVPR IEEE, P4039, DOI 10.1109/CVPR46437.2021.00403
   Everingham M., 2012, PASCAL VISUAL OBJECT
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fathi A, 2012, LECT NOTES COMPUT SC, V7572, P314, DOI 10.1007/978-3-642-33718-5_23
   Gao Z, 2022, IEEE T NEUR NET LEAR, V33, P1147, DOI 10.1109/TNNLS.2020.3041018
   Goertzel B., 2007, P ADV ART GEN INT CO, V6
   Grauman K, 2022, PROC CVPR IEEE, P18973, DOI 10.1109/CVPR52688.2022.01842
   Hinton Geoffrey E, 2012, LECT 6A OVERVIEW MIN, DOI DOI 10.1002/DA.20889
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Hu MH, 2023, MULTIMEDIA SYST, V29, P1, DOI 10.1007/s00530-021-00875-6
   Hu XT, 2021, PROC CVPR IEEE, P3956, DOI 10.1109/CVPR46437.2021.00395
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10986, DOI 10.1109/ICCV48922.2021.01082
   Joseph KJ, 2021, PROC CVPR IEEE, P5826, DOI 10.1109/CVPR46437.2021.00577
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Kitani K. M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3241, DOI 10.1109/CVPR.2011.5995406
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kumano S, 2017, IEEE T MULTIMEDIA, V19, P107, DOI 10.1109/TMM.2016.2608002
   Li JC, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022), P2913, DOI 10.1109/ICRA46639.2022.9812234
   Li YH, 2021, PROC CVPR IEEE, P6939, DOI 10.1109/CVPR46437.2021.00687
   Li Y, 2013, IEEE I CONF COMP VIS, P3216, DOI 10.1109/ICCV.2013.399
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XL, 2018, INT C PATT RECOG, P2262, DOI 10.1109/ICPR.2018.8545895
   Ma R, 2023, IEEE T MULTIMEDIA, V25, P8817, DOI 10.1109/TMM.2023.3242143
   Martínez-Villaseñor L, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19091988
   McCloskey M., 1989, Psychology of learning and motivation, V24, P165
   Myung IJ, 2003, J MATH PSYCHOL, V47, P90, DOI 10.1016/S0022-2496(02)00028-7
   Nagarajan T, 2019, IEEE I CONF COMP VIS, P8687, DOI 10.1109/ICCV.2019.00878
   Nakamura K, 2017, PROC CVPR IEEE, P6817, DOI 10.1109/CVPR.2017.721
   Ng Evonne, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9887, DOI 10.1109/CVPR42600.2020.00991
   Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999
   Ordóñez FJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010115
   Park J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13678, DOI 10.1109/ICCV48922.2021.01344
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Reiss A, 2012, IEEE INT SYM WRBL CO, P108, DOI 10.1109/ISWC.2012.13
   Rezaie H, 2015, HEALTHC TECHNOL LETT, V2, P95, DOI 10.1049/htl.2015.0017
   Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318
   Ryoo MS, 2013, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2013.352
   Shi ZS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13658, DOI 10.1109/ICCV48922.2021.01342
   Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368
   Smith JS, 2020, IEEE INT CONF ROBOT, P2703, DOI [10.1109/ICRA40945.2020.9196721, 10.1109/icra40945.2020.9196721]
   Song SB, 2016, IEEE COMPUT SOC CONF, P378, DOI 10.1109/CVPRW.2016.54
   Spriggs EH, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2009.5204354
   Su YC, 2016, LECT NOTES COMPUT SC, V9909, P454, DOI 10.1007/978-3-319-46454-1_28
   Thuseethan S, 2022, IEEE T MULTIMEDIA, V24, P4367, DOI 10.1109/TMM.2021.3116434
   Tu ZZ, 2023, IEEE T MULTIMEDIA, V25, P4163, DOI 10.1109/TMM.2022.3171688
   Wang FY, 2022, LECT NOTES COMPUT SC, V13685, P398, DOI 10.1007/978-3-031-19806-9_23
   Wang LX, 2023, IEEE T MULTIMEDIA, V25, P5400, DOI 10.1109/TMM.2022.3192729
   Yan SP, 2021, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR46437.2021.00303
   Yang GL, 2023, IEEE T MULTIMEDIA, V25, P3841, DOI 10.1109/TMM.2022.3167555
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhang SF, 2020, IEEE T MULTIMEDIA, V22, P380, DOI 10.1109/TMM.2019.2929005
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhao YB, 2023, IEEE T MULTIMEDIA, V25, P8253, DOI 10.1109/TMM.2023.3234362
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou DW, 2021, Arxiv, DOI arXiv:2112.12533
   Zhu K, 2022, PROC CVPR IEEE, P9286, DOI 10.1109/CVPR52688.2022.00908
NR 76
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2430
EP 2443
DI 10.1109/TMM.2023.3295899
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100028
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xu, P
   Liu, L
   Zheng, HF
   Yuan, X
   Xu, C
   Xue, LY
AF Xu, Ping
   Liu, Lei
   Zheng, Haifeng
   Yuan, Xin
   Xu, Chen
   Xue, Lingyun
TI Degradation-Aware Dynamic Fourier-Based Network for Spectral Compressive
   Imaging
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Degradation; Feature extraction; Imaging;
   Mathematical models; Heuristic algorithms; Convolution; Deep learning;
   fourier transform; hyperspectral images; snapshot compressive imaging
ID APERTURE; ALGORITHMS; DESIGN
AB We consider the problem of hyperspectral image (HSI) reconstruction, which aims to recover 3D hyperspectral data from 2D compressive HSI measurements acquired by a coded aperture snapshot spectral imaging (CASSI) system. Existing deep learning methods have achieved acceptable results in HSI reconstruction. However, these methods did not consider the imaging system degradation pattern. In this article, based on observing the initialized HSIs obtained by shifting and splitting the measurements, we propose a dynamic Fourier network based on degradation learning, called the degradation-aware dynamic Fourier-based network (DADF-Net). We estimate the degradation feature maps from the degraded hyperspectral images to realize the linear transformation and dynamic processing of the features. In particular, we use the Fourier transform to extract the HSI non-local features. Extensive experimental results show that the proposed model outperforms state-of-the-art algorithms on simulation and real-world HSI datasets.
C1 [Xu, Ping; Liu, Lei; Xu, Chen; Xue, Lingyun] Hangzhou Dianzi Univ, Sch Automat, Hangzhou 310018, Peoples R China.
   [Zheng, Haifeng] Fuzhou Univ, Coll Phys & Informat Engn, Fujian Key Lab Intelligent Proc & Wireless Transmi, Fuzhou 350108, Peoples R China.
   [Yuan, Xin] Westlake Univ, Res Ctr Ind Future, Hangzhou 310030, Peoples R China.
   [Yuan, Xin] Westlake Univ, Sch Engn, Hangzhou 310030, Peoples R China.
C3 Hangzhou Dianzi University; Fuzhou University; Westlake University;
   Westlake University
RP Xu, P; Xue, LY (corresponding author), Hangzhou Dianzi Univ, Sch Automat, Hangzhou 310018, Peoples R China.
EM xuping@hdu.edu.cn; liu_lei@hdu.edu.cn; zhenghf@fzu.edu.cn;
   xyuan@westlake.edu.cn; xuchen@hdu.edu.cn; xly@hdu.edu.cn
RI liu, lei/JFA-4177-2023; YUAN, XIN/AGH-6764-2022
OI liu, lei/0000-0001-9271-3304; YUAN, XIN/0000-0002-8311-7524
FU National Natural Science Foundation of China
FX No Statement Available
CR Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319
   Bjorgan A, 2015, PROC SPIE, V9537, DOI 10.1117/12.2184155
   Borengasser M., 2007, Hyperspectral remote sensing: principles and applications
   Cai YH, 2022, LECT NOTES COMPUT SC, V13677, P686, DOI 10.1007/978-3-031-19790-1_41
   Cai YH, 2022, PROC CVPR IEEE, P17481, DOI 10.1109/CVPR52688.2022.01698
   Cai Yuanhao, 2022, P ADV NEUR INF PROC, V35, P37749
   Cao X, 2016, IEEE SIGNAL PROC MAG, V33, P95, DOI 10.1109/MSP.2016.2582378
   Chi L., 2020, ADV NEURAL INF PROCE, V33, P4479, DOI DOI 10.5555/3495724.3496100
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Choi I, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130810
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Du H, 2009, IEEE I CONF COMP VIS, P175, DOI 10.1109/ICCV.2009.5459162
   Hu XW, 2022, PROC CVPR IEEE, P17521, DOI 10.1109/CVPR52688.2022.01702
   Huang T, 2021, PROC CVPR IEEE, P16211, DOI 10.1109/CVPR46437.2021.01595
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Kim MH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185534
   Kingma D. P., 2015, PROC INT C LEARN, P1, DOI DOI 10.1063/1.4902458
   Kittle D, 2010, APPL OPTICS, V49, P6824, DOI 10.1364/AO.49.006824
   Liu GS, 2023, IEEE T MULTIMEDIA, V25, P256, DOI 10.1109/TMM.2021.3124385
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P2990, DOI 10.1109/TPAMI.2018.2873587
   Liu YQ, 2022, IEEE T MULTIMEDIA, V24, P2259, DOI 10.1109/TMM.2021.3078615
   Llull P, 2013, OPT EXPRESS, V21, P10526, DOI 10.1364/OE.21.010526
   Loshchilov I., 2017, INT C LEARNING REPRE
   Luo Z., 2022, CVPR, P6063
   Ma JW, 2019, IEEE I CONF COMP VIS, P10222, DOI 10.1109/ICCV.2019.01032
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Meng ZY, 2020, Arxiv, DOI arXiv:2012.08364
   Meng ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2602, DOI 10.1109/ICCV48922.2021.00262
   Meng ZY, 2020, OPT LETT, V45, P3893, DOI 10.1364/OL.393213
   Miao X, 2019, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2019.00416
   Pan ZH, 2003, IEEE T PATTERN ANAL, V25, P1552, DOI 10.1109/TPAMI.2003.1251148
   Park JI, 2007, IEEE I CONF COMP VIS, P2049
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Rao Yongming, 2021, Advances in neural information processing systems, V34
   Rippel O, 2015, ADV NEUR IN, V28
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Vaswani A, 2017, ADV NEUR IN, V30
   Wagadarikar A, 2008, APPL OPTICS, V47, pB44, DOI 10.1364/AO.47.000B44
   Wagadarikar AA, 2009, OPT EXPRESS, V17, P6368, DOI 10.1364/OE.17.006368
   Wang LS, 2022, PHOTONICS RES, V10, P1848, DOI 10.1364/PRJ.458231
   Wang LZ, 2020, PROC CVPR IEEE, P1658, DOI 10.1109/CVPR42600.2020.00173
   Wang LZ, 2019, PROC CVPR IEEE, P8024, DOI 10.1109/CVPR.2019.00822
   Wang LZ, 2017, IEEE T PATTERN ANAL, V39, P2104, DOI 10.1109/TPAMI.2016.2621050
   Wang LZ, 2015, APPL OPTICS, V54, P848, DOI 10.1364/AO.54.000848
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yuan X, 2021, IEEE SIGNAL PROC MAG, V38, P65, DOI 10.1109/MSP.2020.3023869
   Yuan X, 2020, PROC CVPR IEEE, P1444, DOI 10.1109/CVPR42600.2020.00152
   Yuan X, 2016, IEEE IMAGE PROC, P2539, DOI 10.1109/ICIP.2016.7532817
   Yuan Y, 2017, IEEE J-STARS, V10, P1963, DOI 10.1109/JSTARS.2017.2655112
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang SP, 2019, IEEE I CONF COMP VIS, P10182, DOI 10.1109/ICCV.2019.01028
   Zhang Xuanyu, 2022, P IEEE CVF C COMP VI, P17532
   Ziyi Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P187, DOI 10.1007/978-3-030-58592-1_12
NR 62
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2838
EP 2850
DI 10.1109/TMM.2023.3304450
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400014
DA 2024-08-05
ER

PT J
AU Ye, DJ
   Ni, ZK
   Yang, WH
   Wang, HL
   Wang, SQ
   Kwong, S
AF Ye, Dongjie
   Ni, Zhangkai
   Yang, Wenhan
   Wang, Hanli
   Wang, Shiqi
   Kwong, Sam
TI Glow in the Dark: Low-Light Image Enhancement With External Memory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Testing; Image enhancement; Transformers; Training data; Training;
   Histograms; Lighting; Low-light image enhancement; memory module;
   plug-and-play; vision transformer
ID NETWORK
AB Deep learning-based methods have achieved remarkable success with powerful modeling capabilities. However, the weights of these models are learned over the entire training dataset, which inevitably leads to the ignorance of sample specific properties in the learned enhancement mapping. This situation causes ineffective enhancement in the testing phase for the samples that differ significantly from the training distribution. In this paper, we introduce external memory to form an external memory-augmented network (EMNet) for low-light image enhancement. The external memory aims to capture the sample specific properties of the training dataset to guide the enhancement in the testing phase. Benefiting from the learned memory, more complex distributions of reference images in the entire dataset can be "remembered" to facilitate the adjustment of the testing samples more adaptively. To further augment the capacity of the model, we take the transformer as our baseline network, which specializes in capturing long-range spatial redundancy. Experimental results demonstrate that our proposed method has a promising performance and outperforms state-of-the-art methods. It is noted that, the proposed external memory is a plug-and-play mechanism that can be integrated with any existing method to further improve the enhancement quality. More practices of integrating external memory with other image enhancement methods are qualitatively and quantitatively analyzed. The results further confirm that the effectiveness of our proposed memory mechanism when combing with existing enhancement methods.
C1 [Ye, Dongjie; Wang, Shiqi; Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Ni, Zhangkai; Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 201804, Peoples R China.
   [Ni, Zhangkai; Wang, Hanli] Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 200092, Peoples R China.
   [Yang, Wenhan] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 City University of Hong Kong; Tongji University; Tongji University; Peng
   Cheng Laboratory
RP Kwong, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
EM dj.ye@my.cityu.edu.hk; zkni@tongji.edu.cn; yangwh@pcl.ac.cn;
   hanliwang@tongji.edu.cn; shiqwang@cityu.edu.hk; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012; Wang, Hanli/G-5111-2014; Ni,
   Zhangkai/F-3232-2019
OI Kwong, Sam/0000-0001-7484-7261; Wang, Hanli/0000-0002-9999-4871; Ni,
   Zhangkai/0000-0003-3682-6288; YE, Dongjie/0000-0002-4118-5625
FU Hong Kong Innovation and Technology Commission
FX No Statement Available
CR Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen T, 2021, IEEE T IMAGE PROCESS, V30, P3179, DOI 10.1109/TIP.2021.3058615
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong X, 2011, IEEE INT CON MULTI
   Dosovitskiy A., 2021, PROC ICLR
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Huang HF, 2022, IEEE T IMAGE PROCESS, V31, P1391, DOI 10.1109/TIP.2022.3140610
   Huang HB, 2023, IEEE T PATTERN ANAL, V45, P3446, DOI 10.1109/TPAMI.2022.3180560
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A, 2016, PR MACH LEARN RES, V48
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li JJ, 2021, IEEE T CIRC SYST VID, V31, P4227, DOI 10.1109/TCSVT.2021.3049940
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Lv F., 2018, Proc. BMVC, V220, P4
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Miller A. H., 2016, EMNLP, P1400, DOI [10.18653/v1/D16-1147, DOI 10.18653/V1/D16-1147]
   Nakai K, 2013, I S INTELL SIG PROC, P445, DOI 10.1109/ISPACS.2013.6704591
   Ni ZK, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1484, DOI 10.1145/3503161.3548006
   Ni ZK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1697, DOI 10.1145/3394171.3413839
   Ni ZK, 2020, IEEE T IMAGE PROCESS, V29, P9140, DOI 10.1109/TIP.2020.3023615
   Park CC, 2017, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2017.681
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2015, DOI 10.1145/3343031.3350983
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Weston J., 2015, PROC INT C LEARN REP
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Ye DJ, 2023, IEEE T IMAGE PROCESS, V32, P2827, DOI 10.1109/TIP.2023.3274988
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang Y, 2022, IEEE T IMAGE PROCESS, V31, P759, DOI 10.1109/TIP.2021.3135473
   Zhang Z, 2022, PROC CVPR IEEE, P1889, DOI 10.1109/CVPR52688.2022.00194
   Zhao ZJ, 2022, IEEE T CIRC SYST VID, V32, P1076, DOI 10.1109/TCSVT.2021.3073371
   Zheng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4419, DOI 10.1109/ICCV48922.2021.00440
   Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106
NR 64
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2148
EP 2163
DI 10.1109/TMM.2023.3293736
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100031
DA 2024-08-05
ER

PT J
AU Zhang, CR
   Chen, JX
   Chen, DM
   Wang, W
   Zhang, YS
   Zhou, YC
AF Zhang, Chengrui
   Chen, Junxin
   Chen, Dongming
   Wang, Wei
   Zhang, Yushu
   Zhou, Yicong
TI Exploiting Substitution Box for Cryptanalyzing Image Encryption Schemes
   With DNA Coding and Nonlinear Dynamics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Chosen-plaintext attack; DNA coding; image encryption; S-box matrix
ID CHAOS-BASED IMAGE; ALGORITHM; SECURITY; DIFFUSION
AB In recent years, a number of image encryption schemes based on DNA coding and nonlinear dynamics have been proposed. Generally, these DNA-based schemes first encode plaintext images into DNA sequences and then encrypt them with pseudorandom elements produced by chaotic systems or other nonlinear dynamics. Although ciphertexts can pass some security tests, many image encryption schemes are being shown to have intrinsic flaws and that they cannot guarantee a high level of security. In this article, we cryptanalyze a family of image encryption schemes for which the encryption kernel is DNA coding or its variant. The complex DNA operation can be simplified as a substitution box (S-box). The whole cryptosystem's security level is thus significantly decreased and is vulnerable to the chosen-plaintext attack. Applications of this concept to break five ciphers are theoretically presented and experimentally verified. In addition, some suggestions for resisting similar attacks are also given in this article.
C1 [Zhang, Chengrui; Chen, Dongming] Northeastern Univ, Software Coll, Shenyang 110169, Peoples R China.
   [Chen, Junxin] Dalian Univ Technol, Sch Software, Dalian 116621, Peoples R China.
   [Wang, Wei] Beijing Inst Technol, Sch Med Technol, Beijing 100081, Peoples R China.
   [Wang, Wei] Shenzhen MSU BIT Univ, Dept Engn, Shenzhen 518172, Peoples R China.
   [Zhang, Yushu] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 210016, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 Northeastern University - China; Dalian University of Technology;
   Beijing Institute of Technology; Shenzhen MSU-BIT University; Nanjing
   University of Aeronautics & Astronautics; University of Macau
RP Chen, DM (corresponding author), Northeastern Univ, Software Coll, Shenyang 110169, Peoples R China.
EM 2010496@stu.neu.edu.cn; junxinchen@ieee.org; chendm@mail.neu.edu.cn;
   ehomewang@ieee.org; yushu@nuaa.edu.cn; yicongzhou@um.edu.mo
RI Chen, Junxin/ABC-1747-2020; Wei, Wang/GYU-4649-2022; Zhou,
   Yicong/A-8017-2009
OI Chen, Junxin/0000-0003-4745-8361; Wei, Wang/0000-0002-1717-5785; zhang,
   yushu/0000-0001-8183-8435; Zhou, Yicong/0000-0002-4487-6384; Chen,
   Dongming/0000-0001-7863-1230
FU National Natural Science Foundation of China
FX No Statement Available
CR ADLEMAN LM, 1994, SCIENCE, V266, P1021, DOI 10.1126/science.7973651
   Akhavan A, 2017, OPT LASER TECHNOL, V95, P94, DOI 10.1016/j.optlastec.2017.04.022
   Amani HR, 2019, MULTIMED TOOLS APPL, V78, P21537, DOI 10.1007/s11042-018-6989-y
   Arora A, 2022, MULTIMED TOOLS APPL, V81, P16679, DOI 10.1007/s11042-022-11973-6
   Azimi Z, 2020, MULTIMED TOOLS APPL, V79, P1727, DOI 10.1007/s11042-019-08375-6
   Bard G. V., 2006, A challenging but feasible blockwise-adaptive chosenplaintext attack on SSL
   Ben Farah MA, 2020, OPT LASER TECHNOL, V121, DOI 10.1016/j.optlastec.2019.105777
   Boussif M, 2019, MULTIMED TOOLS APPL, V78, P35493, DOI 10.1007/s11042-019-08108-9
   Budiansky Stephen., 2000, BATTLE WITS COMPLETE
   Chai XL, 2022, INFORM SCIENCES, V604, P115, DOI 10.1016/j.ins.2022.05.008
   Chai XL, 2022, IEEE SIGNAL PROC LET, V29, P972, DOI 10.1109/LSP.2022.3163685
   Chai XL, 2022, NONLINEAR DYNAM, V108, P2671, DOI 10.1007/s11071-022-07328-3
   Chen JX, 2022, IEEE T IND INFORM, V18, P2000, DOI 10.1109/TII.2021.3088465
   Chen JX, 2021, IEEE T CIRC SYST VID, V31, P4747, DOI 10.1109/TCSVT.2021.3054508
   Chen JX, 2021, IEEE T MULTIMEDIA, V23, P2372, DOI 10.1109/TMM.2020.3011315
   Chen JX, 2020, INFORM SCIENCES, V520, P130, DOI 10.1016/j.ins.2020.02.024
   Chen JX, 2018, SIGNAL PROCESS, V142, P340, DOI 10.1016/j.sigpro.2017.07.034
   Chen RJ, 2023, MULTIMED TOOLS APPL, V82, P4289, DOI 10.1007/s11042-022-12515-w
   Deb S, 2019, MULTIMED TOOLS APPL, V78, P34901, DOI 10.1007/s11042-019-08086-y
   Dhall S, 2018, SIGNAL PROCESS, V146, P22, DOI 10.1016/j.sigpro.2017.12.021
   Elsaid SA, 2023, MULTIMED TOOLS APPL, V82, P1995, DOI 10.1007/s11042-022-12641-5
   Feng W, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2880590
   Fridrich J, 1998, INT J BIFURCAT CHAOS, V8, P1259, DOI 10.1142/S021812749800098X
   Gan ZH, 2018, MULTIMED TOOLS APPL, V77, P27919, DOI 10.1007/s11042-018-5974-9
   Heys HM, 2002, CRYPTOLOGIA, V26, P189, DOI 10.1080/0161-110291890885
   Hua ZY, 2021, NONLINEAR DYNAM, V104, P4505, DOI 10.1007/s11071-021-06472-6
   Hua ZY, 2018, SIGNAL PROCESS, V144, P134, DOI 10.1016/j.sigpro.2017.10.004
   Jithin KC, 2020, J INF SECUR APPL, V50, DOI 10.1016/j.jisa.2019.102428
   Jolfaei A, 2016, IEEE T INF FOREN SEC, V11, P235, DOI 10.1109/TIFS.2015.2489178
   Katz J, 2020, Introduction to Modern Cryptography
   Khan M, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0225031
   Kumar M, 2016, SIGNAL PROCESS, V125, P187, DOI 10.1016/j.sigpro.2016.01.017
   Kumar S, 2018, MEASUREMENT, V116, P1, DOI 10.1016/j.measurement.2017.10.064
   Liu Q, 2020, IEEE ACCESS, V8, P83596, DOI 10.1109/ACCESS.2020.2991420
   Lone PN, 2022, MULTIMED TOOLS APPL, V81, P5669, DOI 10.1007/s11042-021-11802-2
   Lv ZH, 2021, IEEE T IND INFORM, V17, P1496, DOI 10.1109/TII.2020.2994747
   Mondal B, 2017, J KING SAUD UNIV-COM, V29, P499, DOI 10.1016/j.jksuci.2016.02.003
   Munir N, 2022, MULTIMED TOOLS APPL, V81, P6571, DOI 10.1007/s11042-021-11810-2
   Naskar PK, 2020, NONLINEAR DYNAM, V100, P2877, DOI 10.1007/s11071-020-05625-3
   Panwar K, 2019, INT J BIFURCAT CHAOS, V29, DOI 10.1142/S0218127419501037
   Paul LSJ, 2022, MULTIMED TOOLS APPL, V81, P37873, DOI 10.1007/s11042-022-13095-5
   Preishuber M, 2018, IEEE T INF FOREN SEC, V13, P2137, DOI 10.1109/TIFS.2018.2812080
   Qian ZX, 2018, IEEE T DEPEND SECURE, V15, P1055, DOI 10.1109/TDSC.2016.2634161
   Saarinen M.-J.O., 2011, International workshop on selected areas in cryptography, P118
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Singh AK, 2018, FUTURE GENER COMP SY, V86, P926, DOI 10.1016/j.future.2016.11.023
   Singh AK, 2017, MULTIMED TOOLS APPL, V76, P8881, DOI 10.1007/s11042-016-3514-z
   Solak E, 2010, INT J BIFURCAT CHAOS, V20, P1405, DOI 10.1142/S0218127410026563
   Song CY, 2015, ENTROPY-SWITZ, V17, P6954, DOI 10.3390/e17106954
   Su X, 2017, MULTIMED TOOLS APPL, V76, P14021, DOI 10.1007/s11042-016-3800-9
   Thakur S, 2019, MULTIMED TOOLS APPL, V78, P3457, DOI 10.1007/s11042-018-6263-3
   Wang QY, 2022, J INF SECUR APPL, V70, DOI 10.1016/j.jisa.2022.103340
   Wen HP, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21030246
   Wu JH, 2018, SIGNAL PROCESS, V153, P11, DOI 10.1016/j.sigpro.2018.06.008
   Wu XJ, 2015, APPL SOFT COMPUT, V37, P24, DOI 10.1016/j.asoc.2015.08.008
   Wu YR, 2023, IEEE T IND INFORM, V19, P2089, DOI 10.1109/TII.2022.3194590
   Xie GL, 2023, SIGNAL PROCESS, V203, DOI 10.1016/j.sigpro.2022.108813
   Ye GD, 2018, NONLINEAR DYNAM, V94, P745, DOI 10.1007/s11071-018-4391-y
   Ye GD, 2010, PATTERN RECOGN LETT, V31, P347, DOI 10.1016/j.patrec.2009.11.008
   Zhang LY, 2018, INFORM SCIENCES, V430, P228, DOI 10.1016/j.ins.2017.11.021
   Zhang X, 2021, MULTIMED TOOLS APPL, V80, P8809, DOI 10.1007/s11042-020-09465-6
   Zhang YS, 2014, INFORM SCIENCES, V289, P254, DOI 10.1016/j.ins.2014.08.005
   Zhang YS, 2013, NONLINEAR DYNAM, V72, P751, DOI 10.1007/s11071-013-0750-x
   Zhen P, 2016, MULTIMED TOOLS APPL, V75, P6303, DOI 10.1007/s11042-015-2573-x
   Zhou YC, 2014, SIGNAL PROCESS, V97, P172, DOI 10.1016/j.sigpro.2013.10.034
NR 65
TC 1
Z9 1
U1 27
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1114
EP 1128
DI 10.1109/TMM.2023.3276504
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700042
DA 2024-08-05
ER

PT J
AU Zheng, GH
   Sang, JT
   Xu, CS
AF Zheng, Guanhua
   Sang, Jitao
   Xu, Changsheng
TI TIF: Threshold Interception and Fusion for Compact and Fine-Grained
   Visual Attribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interpretation; explanation; backpropagation-based attribution;
   compactness; fine-grained attribution; TIF
AB The blackbox nature of deep models has prompted a growing interest in explaining their inner workings and decision-making processes. Although backpropagation (BP)-based attribution methods are popular for visual interpretation, existing methods frequently yield implausible outcomes. For example, gradient-based attribution methods tend to highlight irrelevant regions and generate noise, while CAM-based attributions suffer from low resolution and blurry results. These limitations undermine their ability to correctly identify the target objects and fail to provide the desired justification to guarantee the credibility of the model's decision-making. In this article, we analyze plausibility issues in the frequency domain and point out that the plausibility issues correspond to frequency-domain incompleteness, i.e., the frequency-domain representation of explanations lacks low- or high-frequency components. Then, we propose a straightforward yet effective approach, threshold interception and fusion (TIF), to address this issue by fusing multilayer attributions. Our strategy involves collecting attribution results for all neurons and dividing the attribution map into a concept region that represents the current neuron and background regions based on a given threshold value $\alpha$. We then fuse these concept regions with the neuron weights in each layer and upsample the layer attributions to match the input size. Finally, we obtain the overall attribution by summing the layer attributions pixelwise. Our experiments demonstrate TIF efficacy by consistently enhancing visual performance across a variety of gradient-based attributions. To further demonstrate the ability to provide compact and fine-grained target objects, we directly employ TIF for the weakly supervised semantic segmentation task. Our results illustrate that TIF significantly outperforms existing methods without additional supervision or architectural modifications. We also observe an overall TIF improvement in the fidelity metric, suggesting that compactness and fine-graininess are not only plausibility issues but also fidelity issues.
C1 [Zheng, Guanhua] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Sang, Jitao] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Sang, Jitao] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Xu, Changsheng] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Beijing Jiaotong University; Beijing Jiaotong University;
   Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM zhenggh@mail.ustc.edu.cn; jtsang@bjtu.edu.cn; csxu@nlpr.ia.ac.cn
OI Zheng, Guanhua/0000-0002-6878-971X; xu, chang sheng/0000-0001-8343-9665
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Benitez JM, 1997, IEEE T NEURAL NETWOR, V8, P1156, DOI 10.1109/72.623216
   Castiglioni I, 2021, PHYS MEDICA, V83, P9, DOI 10.1016/j.ejmp.2021.02.006
   Chefer H, 2021, PROC CVPR IEEE, P782, DOI 10.1109/CVPR46437.2021.00084
   Desai S, 2020, IEEE WINT CONF APPL, P972, DOI [10.1109/wacv45572.2020.9093360, 10.1109/WACV45572.2020.9093360]
   Everingham M., 2012, PASCAL VISUAL OBJECT
   Fang SC, 2023, IEEE T PATTERN ANAL, V45, P7123, DOI 10.1109/TPAMI.2022.3223908
   Feiyu Xu, 2019, Natural Language Processing and Chinese Computing. 8th CCF International Conference, NLPCC 2019. Proceedings. Lecture Notes in Artificial Intelligence, Subseries of Lecture Notes in Computer Science (LNAI 11839), P563, DOI 10.1007/978-3-030-32236-6_51
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Gade K, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P3203, DOI 10.1145/3292500.3332281
   Guillaumin M, 2014, INT J COMPUT VISION, V110, P328, DOI 10.1007/s11263-014-0713-9
   Gunning D, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aay7120
   Gunning D, 2019, AI MAG, V40, P44, DOI 10.1609/aimag.v40i2.2850
   Haohan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8681, DOI 10.1109/CVPR42600.2020.00871
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang KX, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P182, DOI 10.1109/dicta47822.2019.8946068
   Iwana BK, 2019, IEEE INT CONF COMP V, P4176, DOI 10.1109/ICCVW.2019.00513
   Jalwana Mohammad A. A. K., 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P16322, DOI 10.1109/CVPR46437.2021.01606
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kapishnikov A, 2021, PROC CVPR IEEE, P5048, DOI 10.1109/CVPR46437.2021.00501
   Lapuschkin S, 2016, PROC CVPR IEEE, P2912, DOI 10.1109/CVPR.2016.318
   Lec S., 2019, PROC IEEE C COMPUT V, P32
   Li R, 2023, IEEE T MULTIMEDIA, V25, P5626, DOI 10.1109/TMM.2022.3197367
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Lindeberg T., 1994, Comput, Vix., V01, P349
   Lu Z., 2022, Adv. Neural Inf. Process. Syst, V35, P14663
   Lundberg SM, 2017, ADV NEUR IN, V30
   Nair V., 2010, ICML, P807
   Nam W.-J., 2020, PROC AAAI C ARTIF IN, P11604
   Nam WJ, 2020, AAAI CONF ARTIF INTE, V34, P2501
   Omeiza Daniel, 2019, ARXIV
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pinheiro PO, 2015, ADV NEUR IN, V28
   Rebuffi S.A., 2020, P IEEECVF C COMPUTER, P8839, DOI [10.1109/CVPR42600.2020.00886, DOI 10.1109/CVPR42600.2020.00886]
   Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820
   Sato T., 2022, PROC INI C ARTIF INT, P231
   Sattarzadeh s, 2021, AAAI C ARTIF INTELL, P11639
   Schulz K., 2019, INT C LEARN REPRE SE
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sheng Z., 2022, IEEE Trans. Multimedia, carly access, DOI [10.1109/TMM.2022.32143, DOI 10.1109/TMM.2022.32143]
   Shrikumar A., PROC 34 INT C MACH L, V70, P3145
   Shrikumar A, 2017, PR MACH LEARN RES, V70
   Shu YC, 2023, IEEE T MULTIMEDIA, V25, P1700, DOI 10.1109/TMM.2022.3154159
   Simonyan K., 2014, 13126034 ARXIV
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smilkov D., 2017, PROC INT C MACH LEAR
   Springenberg Jost Tobias, 2014, PROC 3 INT C LEARN R
   Srinivas S, 2019, ADV NEUR IN, V32
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Temenos A, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14133074
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Wang YL, 2020, IEEE T MULTIMEDIA, V22, P1796, DOI 10.1109/TMM.2019.2949872
   Xie L, 2020, IEEE T MULTIMEDIA, V22, P1182, DOI 10.1109/TMM.2019.2942478
   Xiong B, 2019, IEEE T PATTERN ANAL, V41, P2677, DOI 10.1109/TPAMI.2018.2865794
   Yang Y, 2023, IEEE T MULTIMEDIA, V25, P167, DOI 10.1109/TMM.2021.3122542
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zemouri R, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9081526
   Zhang K., 2021, IEEE Trans.Multimedia, V25, P352
   Zhang YM, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12020237
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 61
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4575
EP 4589
DI 10.1109/TMM.2023.3324808
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100022
DA 2024-08-05
ER

PT J
AU Zhou, ML
   Wu, XT
   Wei, XK
   Xiang, T
   Fang, B
   Kwong, S
AF Zhou, Mingliang
   Wu, Xingtai
   Wei, Xuekai
   Xiang, Tao
   Fang, Bin
   Kwong, Sam
TI Low-Light Enhancement Method Based on a Retinex Model for Structure
   Preservation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coefficient of variation; retinex model; structure-preserving
ID ILLUMINATION; FRAMEWORK; IMAGES
AB Enhancing low-light image visibility is a critical task in computer vision since it helps to improve input for high-level algorithms. High-quality images typically have clear structural information. In previous studies, due to the lack of proper structural guidance, restored images had some problems, such as unclear structural areas and overexposed or underexposed local areas. To address the above problems, in this paper, we introduce a coefficient of variation (COV) with excellent performance in maintaining structural information, and then we propose a low-light image enhancement method that utilizes the COV to extract structural information from images. First, we apply a traditional retinex model to estimate both reflectance and illumination. Second, we use the COV to indicate the degree of dispersion of the input sample, which enables us to obtain a robust structure-distinguishing weight map for low-light images. The weight map is adaptively divided to obtain a structural weight map, which is then used to enhance the gradient image. This process is applied before the reflectance layer of the retinex model. Finally, the result is obtained by using the block coordinate descent method. According to extensive experiments, outstanding results can be achieved by our proposed method in terms of both subjective and objective evaluation metrics in comparison with other state-of-the-art methods. The source code is available at our website.
C1 [Zhou, Mingliang; Wu, Xingtai; Wei, Xuekai; Xiang, Tao; Fang, Bin] Chongqing Univ, Sch Comp Sci, Chongqing 400044, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong 999077, Peoples R China.
C3 Chongqing University; City University of Hong Kong
RP Zhou, ML (corresponding author), Chongqing Univ, Sch Comp Sci, Chongqing 400044, Peoples R China.
EM mingliangzhou@cqu.edu.cn; 450924536@qq.com;
   xuekaiwei2-c@my.cityu.edu.hk; txiang@cqu.edu.cn; fb@cqu.edu.cn;
   cssamk@cityu.edu.hk
RI Xiang, Tao/N-3706-2016; Kwong, Sam/C-9319-2012; Zhou,
   Mingliang/HPC-0298-2023
OI Xiang, Tao/0000-0002-9439-4623; Kwong, Sam/0000-0001-7484-7261; WEI,
   Xuekai/0000-0002-3761-1759; Zhou, Mingliang/0000-0002-1874-3641
FU National Natural Science Foundation of China
FX No Statement Available
CR Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chang M, 2022, IEEE T MULTIMEDIA, V24, P702, DOI 10.1109/TMM.2021.3058586
   Cheng HD, 2004, DIGIT SIGNAL PROCESS, V14, P158, DOI 10.1016/j.dsp.2003.07.002
   Donget X., 2011, P IEEE INT C MULT EX, P1
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fu G, 2019, IEEE IMAGE PROC, P1925, DOI 10.1109/ICIP.2019.8803197
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Jiang NF, 2022, IEEE T MULTIMEDIA, V24, DOI 10.1109/TMM.2021.3115442
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998
   Krishnan D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461992
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liang JW, 2015, J MATH IMAGING VIS, V52, P345, DOI 10.1007/s10851-015-0568-x
   Liang ZT, 2018, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2018.00500
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shen L, 2017, Arxiv, DOI arXiv:1711.02488
   Su HN, 2022, IEEE T MULTIMEDIA, V24, P17, DOI 10.1109/TMM.2020.3043106
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang YF, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2922106
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang J, 2016, INT C PATT RECOG, P751, DOI 10.1109/ICPR.2016.7899725
   Yin JL, 2021, IEEE T MULTIMEDIA, V23, P1049, DOI 10.1109/TMM.2020.2992962
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
NR 50
TC 2
Z9 2
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 650
EP 662
DI 10.1109/TMM.2023.3268867
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000013
DA 2024-08-05
ER

PT J
AU Chen, S
   Meng, FM
   Zhang, RT
   Qiu, HQ
   Li, HL
   Wu, QB
   Xu, LF
AF Chen, Shuai
   Meng, Fanman
   Zhang, Runtong
   Qiu, Heqian
   Li, Hongliang
   Wu, Qingbo
   Xu, Linfeng
TI Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation
   and Beyond
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Image segmentation; Annotations;
   Prototypes; Adaptation models; Training; Few-shot segmentation;
   zero-shot; any-shot; class-agnostic; CLIP
ID AGGREGATION
AB Few-shot segmentation (FSS) aims to segment the novel class with a few annotated images. Due to CLIP's advantages of aligning visual and textual information, the integration of CLIP can enhance the generalization ability of FSS model. However, even with the CLIP model, the existing CLIP-based FSS methods are still subject to the biased prediction towards base class, which is caused by the class-specific feature level interactions. To solve this issue, we propose a visual and textual Prior Guided Mask Assemble Network (PGMA-Net). It employs a class-agnostic mask assembly process to alleviate the bias, and formulates diverse tasks into a unified manner by assembling the prior through affinity. Specifically, the class-relevant textual and visual features are first transformed to class-agnostic prior in the form of probability map. Then, a Prior-Guided Mask Assemble Module (PGMAM) including multiple General Assemble Units (GAUs) is introduced. It considers diverse and plug-and-play interactions, such as visual-textual, inter- and intra-image, training-free, and high-order ones. Lastly, to ensure the class-agnostic ability, a Hierarchical Decoder with Channel-Drop Mechanism (HDCDM) is proposed to flexibly exploit the assembled masks and low-level features, without relying on any class-specific information. It achieves new state-of-the-art results in the FSS task, with mIoU of 77.6 on PASCAL-5(i) and 59.4 on COCO-20(i) in 1-shot scenario. Beyond this, we show that without extra re-training, the proposed PGMA-Net can solve bbox-level and cross-domain FSS, co-segmentation, zero-shot segmentation (ZSS) tasks, leading an any-shot segmentation framework capable of accommodating diverse weak or pixel annotations.
C1 [Chen, Shuai; Meng, Fanman; Zhang, Runtong; Qiu, Heqian; Li, Hongliang; Wu, Qingbo; Xu, Linfeng] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Meng, FM; Wu, QB (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM schen@std.uestc.edu.cn; fmmeng@uestc.edu.cn;
   202211012322@std.uestc.edu.cn; hqqiu@std.uestc.edu.cn;
   hlli@uestc.edu.cn; qbwu@uestc.edu.cn; lfxu@uestc.edu.cn
RI ; Xu, Linfeng/HME-1913-2023
OI Zhang, Runtong/0000-0002-8198-8457; Qiu, Heqian/0000-0002-0963-0311; Xu,
   Linfeng/0000-0002-9934-0958; Chen, Shuai/0009-0007-0434-2197
FU National Key R&D Program of China
FX No Statement Available
CR Botach A, 2022, PROC CVPR IEEE, P4975, DOI 10.1109/CVPR52688.2022.00493
   Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Bucher M, 2019, ADV NEUR IN, V32
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng B, 2021, ADV NEUR IN, V34
   Cheng G, 2023, IEEE T PATTERN ANAL, V45, P4650, DOI 10.1109/TPAMI.2022.3193587
   Chenyang Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8540, DOI 10.1109/CVPR42600.2020.00857
   Du Y, 2022, PROC CVPR IEEE, P14064, DOI 10.1109/CVPR52688.2022.01369
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan Q, 2022, LECT NOTES COMPUT SC, V13679, P701, DOI 10.1007/978-3-031-19800-7_41
   Finn C, 2017, PR MACH LEARN RES, V70
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He WB, 2023, PROC CVPR IEEE, P11207, DOI 10.1109/CVPR52729.2023.01078
   Hong S, 2022, LECT NOTES COMPUT SC, V13689, P108, DOI 10.1007/978-3-031-19818-2_7
   Hu SX, 2022, PROC CVPR IEEE, P9058, DOI 10.1109/CVPR52688.2022.00886
   Hu YQ, 2022, ALGORITHMS, V15, DOI 10.3390/a15050147
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Lei S., 2021, PROC IEEE INT C MULT, P1
   Li B., 2022, PROC INT C LEARN REP
   Li B, 2019, IEEE I CONF COMP VIS, P8518, DOI 10.1109/ICCV.2019.00861
   Li DZ, 2022, AAAI CONF ARTIF INTE, P1297
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2023, IEEE T MULTIMEDIA, V25, P3657, DOI 10.1109/TMM.2022.3163578
   Liu HF, 2023, IEEE T MULTIMEDIA, V25, P8580, DOI 10.1109/TMM.2023.3238521
   Liu J., 2023, PROC IEEECVF INT C C, P3324
   Liu J, 2022, PROC CVPR IEEE, P11543, DOI 10.1109/CVPR52688.2022.01126
   Liu XY, 2023, PROC CVPR IEEE, P2999, DOI 10.1109/CVPR52729.2023.00293
   Liu Y., 2020, COMPUTER VISION ECCV, P142, DOI DOI 10.1007/978-3-030-58545-79
   Liu YW, 2022, PROC CVPR IEEE, P11563, DOI 10.1109/CVPR52688.2022.01128
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8721, DOI 10.1109/ICCV48922.2021.00862
   Lüddecke T, 2022, PROC CVPR IEEE, P7076, DOI 10.1109/CVPR52688.2022.00695
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Min J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6921, DOI 10.1109/ICCV48922.2021.00686
   Perez Ethan, 2018, Distill, DOI [DOI 10.23915/DISTILL.00011, 10.23915/distill.00011]
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Radford A, 2021, PR MACH LEARN RES, V139
   Rajeswaran A, 2019, ADV NEUR IN, V32
   Shaban A., 2017, PROC BRIT MACH VIS C
   Shi HC, 2022, PROC CVPR IEEE, P9601, DOI 10.1109/CVPR52688.2022.00939
   Shi XY, 2022, LECT NOTES COMPUT SC, V13680, P151, DOI 10.1007/978-3-031-20044-1_9
   Shin Gyungin, 2022, Advances in Neural Information Processing Systems, V35, P33754
   Siam M, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P860
   Snell J, 2017, ADV NEUR IN, V30
   Sun Y., 2022, Advances in Neural Information Processing Systems, V35, P37484
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang H., 2022, PROC 31 INT JOINT C, P1385
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang WJ, 2022, PROC CVPR IEEE, P7055, DOI 10.1109/CVPR52688.2022.00693
   Wang Y, 2023, PROC CVPR IEEE, P7183, DOI 10.1109/CVPR52729.2023.00694
   Wu DM, 2023, IEEE I CONF COMP VIS, P2749, DOI 10.1109/ICCV51070.2023.00259
   Wu DM, 2023, PROC CVPR IEEE, P14633, DOI 10.1109/CVPR52729.2023.01406
   Wu DM, 2022, PROC CVPR IEEE, P4986, DOI 10.1109/CVPR52688.2022.00494
   Wu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P497, DOI 10.1109/ICCV48922.2021.00056
   Xian YQ, 2019, PROC CVPR IEEE, P8248, DOI 10.1109/CVPR.2019.00845
   Yang LH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8701, DOI 10.1109/ICCV48922.2021.00860
   Yang XY, 2023, IEEE T MULTIMEDIA, V25, P1555, DOI 10.1109/TMM.2023.3253054
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang GW, 2021, ADV NEUR IN, V34
   Zhang HG, 2023, IEEE T MULTIMEDIA, V25, P2111, DOI 10.1109/TMM.2022.3142955
   Zhang HL, 2024, IEEE T MULTIMEDIA, V26, P1720, DOI 10.1109/TMM.2023.3285441
   Zhang KP, 2024, IEEE T MULTIMEDIA, V26, P737, DOI 10.1109/TMM.2023.3270637
   Zhang L., 2023, P IEEE CVF C COMP VI, P4344
   Zhang LL, 2023, IEEE T CIRC SYST VID, V33, P6609, DOI 10.1109/TCSVT.2023.3265075
   Zhang M, 2023, IEEE T MULTIMEDIA, V25, P7980, DOI 10.1109/TMM.2022.3232037
   Zhang RR, 2022, LECT NOTES COMPUT SC, V13695, P493, DOI 10.1007/978-3-031-19833-5_29
   Zhang RT, 2023, IEEE IMAGE PROC, P1550, DOI 10.1109/ICIP49359.2023.10222652
   Zhang Y., 2022, P 31 INT JOINT C ART, P1658
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhou C, 2022, LECT NOTES COMPUT SC, V13688, P696, DOI 10.1007/978-3-031-19815-1_40
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
NR 81
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7197
EP 7209
DI 10.1109/TMM.2024.3361181
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000054
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Cheng, H
   Guo, YY
   Yin, JH
   Chen, HN
   Wang, JF
   Nie, LQ
AF Cheng, Harry
   Guo, Yangyang
   Yin, Jianhua
   Chen, Haonan
   Wang, Jiafang
   Nie, Liqiang
TI Audio-Driven Talking Video Frame Restoration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Faces; Lips; Task analysis; Image restoration;
   Visualization; Synchronization; Frame Restoration; Frame-Dropped Video;
   Cross-Modal Learning; Dynamic Programming; Generative Adversial Network
ID EMBEDDINGS
AB Talking video frames occasionally drop while streaming for reasons like network errors, which greatly hurts the online team collaboration and user experiences. Directly generating the dropped frames from the remaining ones is unfavorable since a person's lip motion is usually non-linear and thus hard to be restored when consecutive frames are missing. Nevertheless, the audio content provides strong signals for lip motion and is less likely to drop during transmitting. Inspired by this, as an initial attempt, we present the task of audio-driven talking video frame restoration in this paper, i.e., restoring dropped video frames by jointly leveraging the audio and remaining video frames. Towards the high-quality frame generation, we devise a cross-modal frame restoration network. This network aligns the complete audio content with video frames, precisely identifies and sequentially generates the dropped frames. To justify our model, we construct a new dataset, Talking Video Frames Drop, TVFD for short, consisting of 2.5K video and 144K frames in total. We conduct extensive experiments over TVFD and another publicly accessible dataset - Voxceleb2. Our model obtains significantly improved performance as compared to other state-of-the-art competitors.
C1 [Cheng, Harry; Guo, Yangyang; Yin, Jianhua; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266000, Peoples R China.
   [Chen, Haonan; Wang, Jiafang] Alibaba Grp, Alibaba Cloud Intelligence Business Grp, Tmall Genie AI, Hangzhou 310000, Peoples R China.
C3 Shandong University; Alibaba Group
RP Nie, LQ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266000, Peoples R China.
EM xacheng1996@gmail.com; guoyang.eric@gmail.com; jhyin@sdu.edu.cn;
   haolan.chn@alibaba-inc.com; jiafang.wjf@alibaba-inc.com;
   nieliqiang@gmail.com
RI Yin, Jianhua/HMD-6684-2023
OI Yin, Jianhua/0000-0002-4611-2986; Cheng, Harry/0000-0001-7436-0162
FU Shandong Provincial Natural Science Foundation
FX No Statement Available
CR Afouras T., 2020, Lecture Notes in Computer Science, P208
   [Anonymous], 2019, INT CONF ACOUST SPEE, DOI DOI 10.1109/icassp.2019.8682524
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Chatfield K., 2014, Proc. Brit. Mach. Vis. Conf, p6:1
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Chung JS, 2018, INTERSPEECH, P1086
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Chung SW, 2020, IEEE J-STSP, V14, P568, DOI 10.1109/JSTSP.2020.2987720
   Feng TT, 2020, IEEE T MULTIMEDIA, V22, P2963, DOI 10.1109/TMM.2019.2962313
   Gao PC, 2021, IEEE T MULTIMEDIA, V23, P926, DOI 10.1109/TMM.2020.2991507
   GUO Y, 2021, P INT JOINT C ART IN, P708
   Guo YY, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P75, DOI 10.1145/3331184.3331186
   Gupta A, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P256, DOI 10.1145/3394171.3413686
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Huang Z., 2020, arXiv
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Kaur A, 2008, PROCEEDINGS OF THE INDICON 2008 IEEE CONFERENCE & EXHIBITION ON CONTROL, COMMUNICATIONS AND AUTOMATION, VOL I, P40
   Kingma D.P., 2014, Proc. of ICLR
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCV.2017.478, 10.1109/ICCVW.2017.361]
   Luo CW, 2019, IEEE T MULTIMEDIA, V21, P2473, DOI 10.1109/TMM.2019.2903724
   Nagrani A, 2020, INT CONF ACOUST SPEE, P6829, DOI [10.1109/ICASSP40776.2020.9054057, 10.1109/icassp40776.2020.9054057]
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Prajwal KR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1428, DOI 10.1145/3343031.3351066
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Siarohin A, 2019, ADV NEUR IN, V32
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Wang Q, 2021, IEEE T MULTIMEDIA, V23, P429, DOI 10.1109/TMM.2020.2978633
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Xie L, 2007, IEEE T MULTIMEDIA, V9, P500, DOI 10.1109/TMM.2006.888009
   Xu XY, 2019, ADV NEUR IN, V32
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhu H, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2362
NR 43
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4110
EP 4122
DI 10.1109/TMM.2021.3118287
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100044
DA 2024-08-05
ER

PT J
AU Cui, YL
   Jiang, GY
   Yu, M
   Chen, YY
   Ho, YS
AF Cui, Yueli
   Jiang, Gangyi
   Yu, Mei
   Chen, Yeyao
   Ho, Yo-Sung
TI Stitched Wide Field of View Light Field Image Quality Assessment:
   Benchmark Database and Objective Metric
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Databases; Visualization; Degradation; Image stitching;
   Distortion; Benchmark testing; Benchmark database; image quality
   assessment; light field stitching; pairwise comparison; stitched wide
   field of view light field image (WLFI)
ID STATISTICS
AB Due to the limitation of commercial light field camera hardware devices, the imaging field of view is quite narrow. Numerous Light Field Image (LFI) stitching algorithms have been developed to expand the field of view. However, it is highly challenging to compare the performance of LFI stitching algorithms in a fair manner. Currently, due to the absence of a comprehensive benchmark database for subjective rating and a reliable objective quality metric, it is fairly difficult to comprehensively and accurately compare the actual performance of existing LFI stitching algorithms. In this study, we dedicate our efforts to the development of quality metrics for stitched Wide field of view LFI (WLFI) from subjective and objective assessment aspects. Specifically, we build the first stitched WLFI database, which provides the stitched WLFIs generated by eight representative LFI stitching algorithms, along with their corresponding subjective rating scores. Secondly, an effective blind stitched WLFI quality metric is developed to accurately assess the visual quality degradation. Extensive experiments conducted over our established WLFI database demonstrate that the proposed metric achieves higher consistency with subjective ratings than the competing quality metrics.
C1 [Cui, Yueli; Jiang, Gangyi; Yu, Mei; Chen, Yeyao] Ningbov Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Cui, Yueli] Sch Elect & Informat Engn, Taizhou Univ, Taizhou 318000, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol, Sch Elect Engn & Comp Sci, Gwangju 61005, South Korea.
C3 Taizhou University; Gwangju Institute of Science & Technology (GIST)
RP Jiang, GY; Yu, M (corresponding author), Ningbov Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM cuiyueli@tzc.edu.cn; jianggangyi@nbu.edu.cn; yumei@nbu.edu.cn;
   cyy941027@126.com; hoyo@gist.ac.kr
OI HO, YO-SUNG/0000-0002-7220-1034; Cui, Yueli/0000-0002-9837-6705
FU National Natural Science Foundation of China
FX No Statement Available
CR Bader B. W., 2015, MATLAB tensor toolbox
   Birklbauer C, 2014, COMPUT GRAPH FORUM, V33, P43, DOI 10.1111/cgf.12289
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chandramouli P, 2022, IEEE T PATTERN ANAL, V44, P1712, DOI 10.1109/TPAMI.2020.3039841
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen XQ, 2019, EURASIP J IMAGE VIDE, V2019, DOI 10.1186/s13640-019-0479-7
   Chen ZB, 2018, IEEE T IMAGE PROCESS, V27, P721, DOI 10.1109/TIP.2017.2766780
   Cui YL, 2022, IEEE T EM TOP COMP I, V6, P1222, DOI 10.1109/TETCI.2022.3165935
   Cui YL, 2021, DIGIT SIGNAL PROCESS, V117, DOI 10.1016/j.dsp.2021.103138
   Duan F., 2020, PROC 8 INT S TEST AU, P234
   Fang ZW, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3306527
   Golestaneh S, 2016, IEEE T IMAGE PROCESS, V25, P5293, DOI 10.1109/TIP.2016.2601821
   Guo XQ, 2016, IEEE T VIS COMPUT GR, V22, P1852, DOI 10.1109/TVCG.2015.2476805
   Hou JW, 2020, IEEE IMAGE PROC, P3463, DOI 10.1109/ICIP40778.2020.9191241
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Huang XP, 2022, IEEE T MULTIMEDIA, V24, P152, DOI 10.1109/TMM.2020.3046860
   Jia Q, 2021, PROC CVPR IEEE, P12181, DOI 10.1109/CVPR46437.2021.01201
   Jin J, 2022, IEEE T IMAGE PROCESS, V31, P2216, DOI 10.1109/TIP.2022.3154288
   Jin X, 2020, IEEE T IMAGE PROCESS, V29, P1929, DOI 10.1109/TIP.2019.2945687
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Liao TL, 2019, SIGNAL IMAGE VIDEO P, V13, P1199, DOI 10.1007/s11760-019-01466-9
   Liao TL, 2020, IEEE T IMAGE PROCESS, V29, P724, DOI 10.1109/TIP.2019.2934344
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Ling SY, 2018, IEEE INT CON MULTI
   Liu LX, 2017, SIGNAL PROCESS-IMAGE, V58, P287, DOI 10.1016/j.image.2017.08.011
   Liu LX, 2016, SIGNAL PROCESS-IMAGE, V40, P1, DOI 10.1016/j.image.2015.10.005
   Lv J, 2018, IEEE INT CON MULTI
   Madhusudana PC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921858
   Meng CL, 2021, IEEE T MULTIMEDIA, V24, P3193, DOI 10.1109/TMM.2021.3096071
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   MORAN PAP, 1947, BIOMETRIKA, V34, P363, DOI 10.2307/2332449
   Nie L, 2021, IEEE T IMAGE PROCESS, V30, P6184, DOI 10.1109/TIP.2021.3092828
   Oliveira A, 2018, EUR SIGNAL PR CONF, P236, DOI 10.23919/EUSIPCO.2018.8552948
   Ou FZ, 2019, IEEE IMAGE PROC, P1004, DOI [10.1109/ICIP.2019.8803047, 10.1109/icip.2019.8803047]
   Pan ZY, 2021, IEEE J-STSP, V15, P672, DOI 10.1109/JSTSP.2021.3056959
   Paudyal P, 2019, IEEE T BROADCAST, V65, P152, DOI 10.1109/TBC.2019.2892092
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi LK, 2020, IEEE T CIRC SYST VID, V30, P4114, DOI 10.1109/TCSVT.2019.2955011
   Shi LK, 2019, IEEE IMAGE PROC, P3781, DOI [10.1109/icip.2019.8803559, 10.1109/ICIP.2019.8803559]
   Tian Y, 2021, IEEE T CIRC SYST VID, V31, P2046, DOI 10.1109/TCSVT.2020.2971256
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
   Wu GC, 2022, IEEE T PATTERN ANAL, V44, P5430, DOI 10.1109/TPAMI.2021.3073739
   Xiang JJ, 2023, IEEE T MULTIMEDIA, V25, P457, DOI 10.1109/TMM.2021.3127398
   Xiang JJ, 2021, IEEE T CIRC SYST VID, V31, P2575, DOI 10.1109/TCSVT.2020.3030049
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yan WQ, 2020, SIGNAL PROCESS, V172, DOI 10.1016/j.sigpro.2020.107541
   Yang LY, 2017, COMM COM INF SC, V772, P256, DOI 10.1007/978-981-10-7302-1_22
   Yang LY, 2017, IEEE INT CONF COMP V, P2487, DOI 10.1109/ICCVW.2017.293
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zhao P, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3106113
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
NR 59
TC 12
Z9 12
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5092
EP 5107
DI 10.1109/TMM.2023.3330096
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600021
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Gao, N
   Yao, RY
   Liang, RH
   Chen, P
   Liu, TS
   Dang, YJ
AF Gao, Nan
   Yao, Renyuan
   Liang, Ronghua
   Chen, Peng
   Liu, Tianshuang
   Dang, Yuanjie
TI Multi-Level Objective Alignment Transformer for Fine-Grained Oral
   Panoramic X-Ray Report Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bridge-tower connection; fine-grained image report captioning; oral
   panoramic X-ray; positional alignment graph; radiology report generation
ID DISEASE
AB Automatically generated oral panoramic X-ray report is highly beneficial for improving the efficiency of dental diagnosis. However, recent solutions adopt holistic methods, resulting in a cursory description of the oral condition. This may lead to reports lacking details, such as specific sites or lesion contours. Therefore, we propose a Multi-Level objective Alignment Transformer(MLAT) network, which integrates all tooth and disease objects into a positional alignment graph to extract fine-grained object-level features. Specifically, we introduce a novel Object-Level Collaborative Encoder (OLCE) module, which uses a positional alignment graph to construct object relationships. OLCE enhances object-level feature extraction by eliminating interference information between pathologically unrelated objects. In addition, we build a high-quality panoramic X-ray image-report dataset consisting of 562 sets of images and reports labeled by 13 experienced dental specialists. Experiments on the collected dataset show that the proposed MLAT significantly outperforms the state-of-the-art baselines by more than 5% in 4 different metrics, including BLEUs, Meteor, Rouge, and BERTScore.
C1 [Gao, Nan; Yao, Renyuan; Liang, Ronghua; Chen, Peng; Dang, Yuanjie] Zhejiang Univ Technol, Coll Comp Sci & Technol, Coll Software, Hangzhou 310000, Peoples R China.
   [Liu, Tianshuang] Hangzhou Stomatol Hosp, Dept Stomatol, Hangzhou 310000, Peoples R China.
C3 Zhejiang University of Technology
RP Gao, N (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Coll Software, Hangzhou 310000, Peoples R China.; Liu, TS (corresponding author), Hangzhou Stomatol Hosp, Dept Stomatol, Hangzhou 310000, Peoples R China.
EM gaonan@zjut.edu.cn; 2112112033@zjut.edu.cn; liangrh@zjut.edu.cn;
   chenpeng@zjut.edu.cn; lts3k@163.com; dangyj@zjut.edu.cn
OI Dang, Yuanjie/0000-0002-8302-1338; , Renyuan/0000-0001-7938-6724; gao,
   nan/0000-0003-4545-7197
FU Zhejiang Provincial Natural Science Foundation of China
FX No Statement Available
CR Alfarghaly O., 2021, INFORM MED UNLOCKED, V24, P100557, DOI [10.1016/j.imu.2021.100557, DOI 10.1016/J.IMU.2021.100557]
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Chen J, 2022, PROC CVPR IEEE, P18009, DOI 10.1109/CVPR52688.2022.01750
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Devlin J, 2018, ARXIV
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dou ZY, 2022, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52688.2022.01763
   Du M., 2017, China Med. Herald, V14, P152
   Johnson AEW, 2019, Arxiv, DOI arXiv:1901.07042
   Herdade S, 2019, ADV NEUR IN, V32
   Hou B, 2021, LECT NOTES COMPUT SC, V12907, P293, DOI 10.1007/978-3-030-87234-2_28
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang ZZ, 2023, PROC CVPR IEEE, P19809, DOI 10.1109/CVPR52729.2023.01897
   James SL, 2018, LANCET, V392, P1789, DOI [10.1016/s0140-6736(18)32335-3, 10.1016/S0140-6736(18)32335-3]
   Ji JY, 2023, IEEE T MULTIMEDIA, V25, P3962, DOI 10.1109/TMM.2022.3169061
   Jing BY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6570
   Jing BY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2577
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Li C, 2018, DES AUT CON, DOI 10.1145/3195970.3196091
   Li JH, 2021, ADV NEUR IN, V34
   Li JN, 2022, PR MACH LEARN RES
   Li W, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P3187
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Liu FL, 2021, PROC CVPR IEEE, P13748, DOI 10.1109/CVPR46437.2021.01354
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Radford A., 2019, OpenAI blog, V1, P9
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang CZ, 2023, EXPERT SYST APPL, V211, DOI 10.1016/j.eswa.2022.118474
   Wang J, 2022, LECT NOTES COMPUT SC, V13695, P563, DOI 10.1007/978-3-031-19833-5_33
   Wang W., 2022, P ADV NEUR INF PROC, V35, P32897
   Wang XS, 2018, PROC CVPR IEEE, P9049, DOI 10.1109/CVPR.2018.00943
   Xu X., 2023, AAAI, V37, P10637
   Xue Y, 2018, LECT NOTES COMPUT SC, V11070, P457, DOI 10.1007/978-3-030-00928-1_52
   Yang SX, 2022, MED IMAGE ANAL, V80, DOI 10.1016/j.media.2022.102510
   Yang XY, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P5000
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You D, 2021, LECT NOTES COMPUT SC, V12903, P72, DOI 10.1007/978-3-030-87199-4_7
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Yuan JB, 2019, LECT NOTES COMPUT SC, V11769, P721, DOI 10.1007/978-3-030-32226-7_80
   Zeng Yan, 2022, P MACHINE LEARNING R
   Zhang V. K, 2020, PROC INT C LEARN REP, P1
   Zhang YX, 2020, AAAI CONF ARTIF INTE, V34, P12910
   Zhang ZJ, 2022, IEEE T MULTIMEDIA, V24, P3101, DOI 10.1109/TMM.2021.3093725
   Zhu HH, 2023, NEURAL COMPUT APPL, V35, P16051, DOI 10.1007/s00521-021-06684-2
   Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312
NR 55
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7462
EP 7474
DI 10.1109/TMM.2024.3368922
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000036
DA 2024-08-05
ER

PT J
AU Gou, JP
   Chen, Y
   Yu, BS
   Liu, JH
   Du, L
   Wan, SH
   Yi, Z
AF Gou, Jianping
   Chen, Yu
   Yu, Baosheng
   Liu, Jinhua
   Du, Lan
   Wan, Shaohua
   Yi, Zhang
TI Reciprocal Teacher-Student Learning via Forward and Feedback Knowledge
   Distillation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge engineering; Training; Visualization; Computational modeling;
   Reviews; Knowledge transfer; Correlation; Model compression; knowledge
   distillation; feedback knowledge; visual recognition
AB Knowledge distillation (KD) is a prevalent model compression technique in deep learning, aiming to leverage knowledge from a large teacher model to enhance the training of a smaller student model. It has found success in deploying compact deep models in intelligent applications like intelligent transportation, smart health, and distributed intelligence. Current knowledge distillation methods primarily fall into two categories: offline and online knowledge distillation. Offline methods involve a one-way distillation process, transferring unvaried knowledge from teacher to student, while online methods enable the simultaneous training of multiple peer students. However, existing knowledge distillation methods often face challenges where the student may not fully comprehend the teacher's knowledge due to model capacity gaps, and there might be knowledge incongruence among outputs of multiple students without teacher guidance. To address these issues, we propose a novel reciprocal teacher-student learning inspired by human teaching and examining through forward and feedback knowledge distillation (FFKD). Forward knowledge distillation operates offline, while feedback knowledge distillation follows an online scheme. The rationale is that feedback knowledge distillation enables the pre-trained teacher model to receive feedback from students, allowing the teacher to refine its teaching strategies accordingly. To achieve this, we introduce a new weighting constraint to gauge the extent of students' understanding of the teacher's knowledge, which is then utilized to enhance teaching strategies. Experimental results on five visual recognition datasets demonstrate that the proposed FFKD outperforms current state-of-the-art knowledge distillation methods.
C1 [Gou, Jianping] Southwest Univ, Coll Comp & Informat Sci, Coll Software, Chongqing 400715, Peoples R China.
   [Chen, Yu] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Peoples R China.
   [Yu, Baosheng] Univ Sydney, Sch Comp Sci, Darlington, NSW 2008, Australia.
   [Liu, Jinhua] Shangrao Normal Univ, Sch Math & Comp Sci, Shangrao 334001, Peoples R China.
   [Du, Lan] Monash Univ, Fac Informat Technol, Clayton, Vic 3800, Australia.
   [Wan, Shaohua] Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Shenzhen 611731, Peoples R China.
   [Yi, Zhang] Sichuan Univ, Sch Comp Sci, Chengdu 610065, Peoples R China.
C3 Southwest University - China; Jiangsu University; University of Sydney;
   Shangrao Normal University; Monash University; University of Electronic
   Science & Technology of China; Shenzhen Institute for Advanced Study,
   UESTC; Sichuan University
RP Liu, JH (corresponding author), Shangrao Normal Univ, Sch Math & Comp Sci, Shangrao 334001, Peoples R China.
EM pjgzy61@swu.edu.cn; yuchen@stmail.ujs.edu.cn; baosheng.yu@sydney.edu.au;
   liujinhua_uestc@126.com; lan.du@monash.edu; shaohua.wan@uestc.edu.cn;
   zhangyi@scu.edu.cn
RI Wan, Shaohua/B-9243-2014
OI Wan, Shaohua/0000-0001-7013-9081
FU National Natural Science Foundation of China
FX No Statement Available
CR An SM, 2022, IEEE T INTELL TRANSP, V23, P15256, DOI 10.1109/TITS.2021.3139001
   Anbang Yao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P294, DOI 10.1007/978-3-030-58555-6_18
   Bharadhwaj M, 2022, IEEE COMPUT SOC CONF, P3191, DOI 10.1109/CVPRW56347.2022.00360
   Borza DL, 2023, COMPUT VIS IMAGE UND, V228, DOI 10.1016/j.cviu.2023.103632
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong XP, 2023, IEEE T PATTERN ANAL, V45, P8049, DOI 10.1109/TPAMI.2022.3230064
   Garcia NC, 2021, IEEE WINT CONF APPL, P2754, DOI 10.1109/WACV48630.2021.00280
   Gou J., 2022, IEEE T NEUR NET LEAR, DOI [10.1109/TNNLS.2022.3212733, DOI 10.1109/TNNLS.2022.3212733]
   Gou JP, 2023, INT J COMPUT VISION, V131, P1857, DOI 10.1007/s11263-023-01792-z
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gu P., 2023, PROC IEEE INT C COMP, P1
   Guo ZY, 2023, PROC CVPR IEEE, P11868, DOI 10.1109/CVPR52729.2023.01142
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang ZH, 2022, IEEE T IMAGE PROCESS, V31, P1364, DOI 10.1109/TIP.2022.3141255
   Jacob GM, 2023, IEEE WINT CONF APPL, P2358, DOI 10.1109/WACV56688.2023.00239
   Jin H, 2023, IEEE T PARALL DISTR, V34, P567, DOI 10.1109/TPDS.2022.3225185
   Kag A., 2023, PROC INT C LEARN REP, P1
   Kornblith S, 2019, PR MACH LEARN RES, V97
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2009, CIFAR-10 dataset
   Li B, 2021, LECT NOTES ARTIF INT, V12815, P357, DOI 10.1007/978-3-030-82136-4_29
   Li J., 2023, P AAAI, V37, P1323
   Li JZ, 2023, PROC CVPR IEEE, P20156, DOI 10.1109/CVPR52729.2023.01930
   Li WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9506, DOI 10.1109/ICCV48922.2021.00939
   Li WJ, 2022, IEEE T INTELL TRANSP, V23, P17922, DOI 10.1109/TITS.2022.3161986
   Li Z, 2023, AAAI CONF ARTIF INTE, P1504
   Liu L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8251, DOI 10.1109/ICCV48922.2021.00816
   Liu Y., 2021, arXiv
   Liu Z., 2021, PROC BR MACH VIS C, P1
   Long ZX, 2023, INFORM FUSION, V90, P12, DOI 10.1016/j.inffus.2022.09.007
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Martinel N, 2022, IEEE T IND INFORM, V18, P87, DOI 10.1109/TII.2021.3068927
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Qiu Z., 2023, PROC 11 INT C LEARN, P1
   Qiushan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11017, DOI 10.1109/CVPR42600.2020.01103
   Romero A., 2015, ICLR, P1
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saha A, 2023, BLOOD PURIFICAT, V51, P51, DOI 10.1159/000526923
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shu CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5291, DOI 10.1109/ICCV48922.2021.00526
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su MY, 2023, IEEE T MULTIMEDIA, V25, P662, DOI 10.1109/TMM.2021.3129623
   Sukmin Yun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13873, DOI 10.1109/CVPR42600.2020.01389
   Tan WT, 2023, IEEE T MULTIMEDIA, V25, P4520, DOI 10.1109/TMM.2022.3177901
   Tang RN, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2023.109320
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang L, 2022, IEEE T PATTERN ANAL, V44, P3048, DOI 10.1109/TPAMI.2021.3055564
   Wang LT, 2023, PROC CVPR IEEE, P11186, DOI 10.1109/CVPR52729.2023.01076
   Wu DM, 2022, PROC CVPR IEEE, P4986, DOI 10.1109/CVPR52688.2022.00494
   Yang CG, 2022, PROC CVPR IEEE, P12309, DOI 10.1109/CVPR52688.2022.01200
   Yang GL, 2023, IEEE T MULTIMEDIA, V25, P3841, DOI 10.1109/TMM.2022.3167555
   Yang J, 2022, IEEE T VEH TECHNOL, V71, P11006, DOI 10.1109/TVT.2022.3184994
   Yoon D, 2020, IEEE SIGNAL PROC LET, V27, P2139, DOI 10.1109/LSP.2020.3039952
   Zagoruyko S., 2017, ICLR, DOI DOI 10.1016/J.CVIU.2019.07.006.ARXIV:1612.0
   Zhang SW, 2020, AAAI CONF ARTIF INTE, V34, P12862
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao W., 2023, PROC AAAI C ARTIF IN, P3615
   Zhao ZJ, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108120
   Zhou ZD, 2020, Arxiv, DOI arXiv:2006.01683
NR 65
TC 2
Z9 2
U1 21
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7901
EP 7916
DI 10.1109/TMM.2024.3372833
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000028
DA 2024-08-05
ER

PT J
AU Hadikhani, P
   Lai, DTC
   Ong, WH
AF Hadikhani, Parham
   Lai, Daphne Teck Ching
   Ong, Wee-Hong
TI Human Activity Discovery With Automatic Multi-Objective Particle Swarm
   Optimization Clustering With Gaussian Mutation and Game Theory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human activity discovery; unsupervised learning; clustering;
   multi-objectives; evolutionary algorithm; game theory; dimension
   reduction; skeleton sequence
ID ACTION RECOGNITION; ALGORITHM
AB Despite many advances in Human Activity Recognition (HAR), most existing works are conducted with supervision. Supervised methods rely on labeled training data. However, obtaining labeled data is difficult, costly, and time-consuming. In this paper, we introduce an automatic multi-objective particle swarm optimization clustering based on Gaussian mutation and game theory (MOPGMGT) to provide fully unsupervised human activity discovery. Furthermore, we map the multi-objective clustering problem to game theory to get the best optimal solution. The proposed algorithm can accurately find the number of activities without any prior knowledge. Multi-objective optimization problems typically cannot have a single optimal solution. We solve this problem by applying, Nash Equilibrium (NE) to the pareto front as the decision-making for choosing the best solution. NE does not just look for the best solution but tries to optimize the final solution by considering the effect of choosing each of the solutions as the best solution on the other solutions and one with the best impact is chosen. Moreover, a Gaussian mutation is applied on the pareto front to avoid premature convergence. As far as we know, this is the first time that human activity discovery is performed fully unsupervised, and a multi-objective PSO is mapped to the game theory space for finding the best solution. Experiments on six challenging human activity datasets demonstrate the capability of the proposed approach in achieving the best accuracy in human activity discovery and determining the optimal number of clusters. In comparison to well-known multi-objective algorithms, the MOPGMGT significantly improves the clustering outcomes on six benchmark clustering datasets.
C1 [Hadikhani, Parham; Lai, Daphne Teck Ching; Ong, Wee-Hong] Univ Brunei Darussalam, Sch Digital Sci, BE-1410 Bandar Seri Begawan, Brunei.
   [Lai, Daphne Teck Ching; Ong, Wee-Hong] Univ Brunei Darussalam, Inst Appl Data Analyt, BE-1410 Bandar Seri Begawan, Brunei.
C3 University Brunei Darussalam; University Brunei Darussalam
RP Hadikhani, P (corresponding author), Univ Brunei Darussalam, Sch Digital Sci, BE-1410 Bandar Seri Begawan, Brunei.
EM 20h8561@ubd.edu.bn; daphne.lai@ubd.edu.bn; weehong.ong@ubd.edu.bn
RI /T-5917-2017
OI /0000-0001-8290-8941; Hadikhani, Parham/0000-0002-4832-4368; Ong, Wee
   Hong/0000-0003-2667-1980
FU Universiti Brunei Darussalam
FX No Statement Available
CR Abubaker A, 2015, PLOS ONE, V10, DOI [10.1371/journal.pone.0130995, 10.1371/journal.pone.0135641]
   Adama DA, 2018, SOFT COMPUT, V22, P7027, DOI 10.1007/s00500-018-3364-x
   Agarwal P., 2021, Artif. Intell. Sustain. Ind, V4, P169
   Agarwal P, 2021, SOFT COMPUT, V25, P10237, DOI 10.1007/s00500-021-05973-1
   Akbari R, 2012, SWARM EVOL COMPUT, V2, P39, DOI 10.1016/j.swevo.2011.08.001
   Alsarhan T, 2022, COMPUT VIS IMAGE UND, V216, DOI 10.1016/j.cviu.2021.103348
   Arzani MM, 2021, IEEE T CYBERNETICS, V51, P5859, DOI 10.1109/TCYB.2019.2960481
   Babu SS, 2022, INT J PATTERN RECOGN, V36, DOI 10.1142/S0218001422590273
   Bhattacharjee KS, 2017, IEEE T EVOLUT COMPUT, V21, P813, DOI 10.1109/TEVC.2017.2687320
   Cai JH, 2020, IEEE ACCESS, V8, P88200, DOI 10.1109/ACCESS.2020.2992903
   Cao WM, 2019, IEEE ACCESS, V7, P132049, DOI 10.1109/ACCESS.2019.2940291
   Chandrashekhar H. V, 2006, Human activity representation, analysis, and recognition
   Cheng Y.-B., 2021, P IEEE INT C MULT EX, P1
   Choachaicharoenkul S, 2020, EXPERT SYST APPL, V158, DOI 10.1016/j.eswa.2020.113446
   Coello CAC, 2004, IEEE T EVOLUT COMPUT, V8, P256, DOI 10.1109/tevc.2004.826067
   Das I, 1999, STRUCT OPTIMIZATION, V18, P107, DOI 10.1007/s001580050111
   Deb K., 2000, Parallel Problem Solving from Nature PPSN VI. 6th International Conference. Proceedings (Lecture Notes in Computer Science Vol.1917), P849
   Friedrich T., 2008, P 10 ANN C GEN EV CO, P945, DOI DOI 10.1145/1389095.1389276
   Gaglio S, 2015, IEEE T HUM-MACH SYST, V45, P586, DOI 10.1109/THMS.2014.2377111
   Gupta P, 2020, NEURAL COMPUT APPL, V32, P12351, DOI 10.1007/s00521-020-04737-6
   Hadikhani P., 2023, Automatic deep sparse multi-trial vector-based differential evolution clustering with manifold learning and incremental technique
   Hadikhani P, 2022, PROCEEDINGS OF THE 2022 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE COMPANION, GECCO 2022, P487, DOI 10.1145/3520304.3528885
   Hadikhani P, 2022, Arxiv, DOI arXiv:2201.05314
   Hadikhani P, 2020, EVOL INTELL, V13, P695, DOI 10.1007/s12065-020-00384-x
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Heloulou I, 2017, EXPERT SYST APPL, V67, P32, DOI 10.1016/j.eswa.2016.09.008
   Jana B, 2019, APPL SOFT COMPUT, V74, P330, DOI 10.1016/j.asoc.2018.09.027
   Kaur A, 2022, PATTERN ANAL APPL, V25, P209, DOI 10.1007/s10044-021-01052-1
   Kukleva A, 2019, PROC CVPR IEEE, P12058, DOI 10.1109/CVPR.2019.01234
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Lee I, 2021, IEEE T MULTIMEDIA, V23, P415, DOI 10.1109/TMM.2020.2978637
   Li C., 2008, An adaptive mutation operator for particle swarm optimization
   Li C, 2022, IEEE T NEUR NET LEAR, V33, P4800, DOI 10.1109/TNNLS.2021.3061115
   Li JN, 2018, ADV NEUR IN, V31
   Li XY, 2012, EXPERT SYST APPL, V39, P288, DOI 10.1016/j.eswa.2011.07.019
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2022, IEEE SIGNAL PROC LET, V29, P632, DOI 10.1109/LSP.2022.3144898
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu X, 2020, SIGNAL IMAGE VIDEO P, V14, P1227, DOI 10.1007/s11760-020-01644-0
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Lu YP, 2011, MACH LEARN, V82, P43, DOI 10.1007/s10994-009-5154-2
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Mohammadzade H, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102691
   Mukhopadhyay A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2742642
   Ong W.-H., 2015, IEEJ Trans. Elect. Inform. Syst., V135, P1136
   Orouskhani M, 2021, EXPERT SYST APPL, V165, DOI 10.1016/j.eswa.2020.113916
   Paoletti G., 2021, P BRIT MACH VIS C
   Paoletti G, 2021, INT C PATT RECOG, P6035, DOI 10.1109/ICPR48806.2021.9412060
   Peng B, 2020, IEEE T IND INFORM, V16, P555, DOI 10.1109/TII.2019.2937514
   Petchrompo S, 2022, EUR J OPER RES, V297, P203, DOI 10.1016/j.ejor.2021.04.053
   Prakash J, 2015, MEMET COMPUT, V7, P93, DOI 10.1007/s12293-014-0147-5
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Ray T, 2022, APPL SOFT COMPUT, V119, DOI 10.1016/j.asoc.2022.108505
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Sempena S., 2011, P IEEE INT C EL ENG, P1
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shan JJ, 2014, 2014 IEEE WORKSHOP ON ADVANCED ROBOTICS AND ITS SOCIAL IMPACTS (ARSO), P69, DOI 10.1109/ARSO.2014.7020983
   Shen XP, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103386
   Shi YH, 1998, IEEE C EVOL COMPUTAT, P69, DOI 10.1109/ICEC.1998.699146
   Srinivas N., 1994, Evolutionary Computation, V2, P221, DOI 10.1162/evco.1994.2.3.221
   Sugar CA, 2003, J AM STAT ASSOC, V98, P750, DOI 10.1198/016214503000000666
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Ursem R. K., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P462
   van der Merwe D, 2003, IEEE C EVOL COMPUTAT, P215, DOI 10.1109/CEC.2003.1299577
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wen YH, 2019, AAAI CONF ARTIF INTE, P8989
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Xu SH, 2023, IEEE T MULTIMEDIA, V25, P624, DOI 10.1109/TMM.2021.3129616
   Yang L, 2022, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR52688.2022.00316
   Yang L, 2022, IEEE T PATTERN ANAL, V44, P9814, DOI 10.1109/TPAMI.2021.3132058
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13403, DOI 10.1109/ICCV48922.2021.01317
   Yao H., 2021, P IEEE INT C MULT EX, P1
   Zhan ZH, 2009, IEEE C EVOL COMPUTAT, P1383, DOI 10.1109/CEC.2009.4983105
   Zhan ZH, 2009, IEEE T SYST MAN CY B, V39, P1362, DOI 10.1109/TSMCB.2009.2015956
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang YB, 2016, IEEE T MULTIMEDIA, V18, P405, DOI 10.1109/TMM.2015.2512046
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
NR 81
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 420
EP 435
DI 10.1109/TMM.2023.3266603
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000002
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Song, YG
   Yang, XS
   Wang, YW
   Xu, CS
AF Song, Yaguang
   Yang, Xiaoshan
   Wang, Yaowei
   Xu, Changsheng
TI Recovering Generalization via Pre-Training-Like Knowledge Distillation
   for Out-of-Distribution Visual Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-modal Foundation Model; Out-of-Distribution Generalization; Visual
   Question Answering; Knowledge Distillation
AB With the emergence of large-scale multi-modal foundation models, significant improvements have been made towards Visual Question Answering (VQA) in recent years via the "Pre-training and Fine-tuning" paradigm. However, the fine-tuned VQA model, which is more specialized for the downstream training data, may fail to generalize well when there is a distribution shift between the training and test data, which is defined as the Out-of-Distribution (OOD) problem. An intuitive way to solve this problem is to transfer the common knowledge from the foundation model to the fine-tuned VQA model via knowledge distillation for better generalization. However, the generality of distilled knowledge based on the task-specific training data is questionable due to the bias between the training and test data. An ideal way is to adopt the pre-training data to distill the common knowledge shared by the training and OOD test samples, which however is impracticable due to the huge size of pre-training data. Based on the above considerations, in this article, we propose a method, named Pre-training-like Knowledge Distillation (PKD), to imitate the pre-training feature distribution and leverage it to distill the common knowledge, which can improve the generalization performance of the fine-tuned model for OOD VQA. Specifically, we first leverage the in-domain VQA data as guidance and adopt two cross-modal feature prediction networks, which are learned under the supervision of image-text matching loss and feature divergence loss, to estimate pre-training-like vision and text features. Next, we conduct feature-level distillation by explicitly integrating the downstream VQA input features with the predicted pre-training-like features through a memory mechanism. In the meantime, we also conduct model-level distillation by constraining the image-text matching output of the downstream VQA model and the output of the foundation model for the pre-training-like image and text features. Extensive experiments on the VQA-CP v2 and VQA v2 datasets demonstrate the effectiveness of our method.
C1 [Song, Yaguang; Yang, Xiaoshan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Song, Yaguang; Yang, Xiaoshan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
   [Song, Yaguang; Yang, Xiaoshan; Wang, Yaowei; Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
EM songyaguang2019@ia.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn; wangyw@pcl.ac.cn;
   csxu@nlpr.ia.ac.cn
OI xu, chang sheng/0000-0001-8343-9665
FU National Natural Science Foundation of China
FX No Statement Available
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreassen A., 2021, The evolution of out-of-distribution robustness throughout fine-tuning
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bommasani R., 2021, arXiv, DOI DOI 10.48550/ARXIV.2108.07258
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Cadene Remi, 2019, ADV NEUR IN, P839
   Chen FL, 2023, MACH INTELL RES, V20, P38, DOI 10.1007/s11633-022-1369-5
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Chung I., 2020, International Conference on Machine Learning, P2006
   Fang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1408, DOI 10.1109/ICCV48922.2021.00146
   Gokhale T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P878
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Han X, 2021, AI OPEN, V2, P225, DOI 10.1016/j.aiopen.2021.08.002
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hudson Drew A, 2019, P IEEE CVF C COMP VI, P6700, DOI DOI 10.1109/CVPR.2019.00686
   Jiang JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P199, DOI 10.1145/3474085.3475350
   Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217
   Kervadec C, 2021, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR46437.2021.00280
   Kim JH, 2018, ADV NEUR IN, V31
   Kong X, 2022, PROC CVPR IEEE, P9296, DOI 10.1109/CVPR52688.2022.00909
   Li JH, 2021, ADV NEUR IN, V34
   Li Linjie, 2020, arXiv
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081
   Lu JS, 2019, ADV NEUR IN, V32
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Niu Yulei, 2021, ADV NEURAL INFORM PR, V34, P2
   Ouyang NL, 2021, IEEE T MULTIMEDIA, V24, P3405, DOI 10.1109/TMM.2021.3097502
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramakrishnan S., 2018, INT C NEURAL INF PRO, P1548
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Selvaraju RR, 2019, IEEE I CONF COMP VIS, P2591, DOI 10.1109/ICCV.2019.00268
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Teney D, 2020, INT C NEURAL INF PRO, P407
   Teney D, 2021, P IEEECVF INT C COMP, P1417
   van den Oord A, 2017, ADV NEUR IN, V30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wang Z., 2022, arXiv
   Wei C, 2021, Adv Neural Inf Process Syst, P16158
   Wen ZQ, 2021, ADV NEUR IN, V34
   Wortsman M, 2022, PROC CVPR IEEE, P7949, DOI 10.1109/CVPR52688.2022.00780
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Y, 2021, P NEURIPS WORKSH DIS
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang DL, 2020, IEEE T CYBERNETICS, V50, P3033, DOI 10.1109/TCYB.2019.2905157
   Zhi X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1083
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zhou BL, 2015, Arxiv, DOI arXiv:1512.02167
NR 58
TC 3
Z9 3
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 837
EP 851
DI 10.1109/TMM.2023.3272224
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700047
DA 2024-08-05
ER

PT J
AU Tong, KD
   Jin, X
   Yang, YQ
   Wang, C
   Kang, JS
   Jiang, F
AF Tong, Kedeng
   Jin, Xin
   Yang, Yuqing
   Wang, Chen
   Kang, Jinshi
   Jiang, Fan
TI Learned Focused Plenoptic Image Compression With Microimage
   Preprocessing and Global Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Focused plenoptic camera; global attention; learned image compression;
   plenoptic image coding
AB Focused plenoptic cameras can record spatial and angular information of the light field (LF) simultaneously with higher spatial resolution relative to traditional plenoptic cameras, which facilitate various applications in computer vision. However, the existing plenoptic image compression methods present ineffectiveness to the captured images due to the complex micro-textures generated by the microlens relay imaging and long-distance correlations among the microimages. In this article, a lossy end-to-end learning architecture is proposed to compress the focused plenoptic images efficiently. First, a data preprocessing scheme is designed according to the imaging principle to remove the sub-aperture image ineffective pixels in the recorded light field and align the microimages to the rectangular grid. Then, the global attention module with large receptive field is proposed to capture the global correlation among the feature maps using pixel-wise vector attention computed in the resampling process. Also, a new image dataset consisting of 1910 focused plenoptic images with content and depth diversity is built to benefit training and testing. Extensive experimental evaluations demonstrate the effectiveness of the proposed approach. It outperforms intra coding of HEVC and VVC by an average of 62.57% and 51.67% bitrate reduction on the 20 preprocessed focused plenoptic images, respectively. Also, it achieves 18.73% bitrate saving and generates perceptually pleasant reconstructions compared to the state-of-the-art end-to-end image compression methods, which benefits the applications of focused plenoptic cameras greatly. The dataset and code are publicly available at https://github.com/VincentChandelier/GACN.
C1 [Tong, Kedeng; Jin, Xin; Yang, Yuqing; Wang, Chen; Kang, Jinshi; Jiang, Fan] Tsinghua Univ, Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
C3 Tsinghua University
RP Jin, X (corresponding author), Tsinghua Univ, Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM tkd20@mails.tsinghua.edu.cn; jin.xin@sz.tsinghua.edu.cn;
   yangyq22@mails.tsinghua.edu.cn; wang-che20@mails.tsinghua.edu.cn;
   kjs20@mails.tsinghua.edu.cn; jf19@mails.tsinghua.edu.cn
RI Yuqing, Yang/ADJ-2720-2022; jiang, fan/KHV-8640-2024
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahmad W, 2020, IEEE T IMAGE PROCESS, V29, P4269, DOI 10.1109/TIP.2020.2969087
   [Anonymous], HEVC SCC official test model HM+SCM
   [Anonymous], 2022, Standard ISO/IEC JTC 1/SC 29/WG 04
   Astola P, 2018, EUR W VIS INF PROCES
   Bakir N, 2018, IEEE IMAGE PROC, P1128, DOI 10.1109/ICIP.2018.8451597
   Balle J., 2018, ICLR
   Balle J., 2017, P INT C LEARN REPR
   Bjontegaard G., 2001, Calculation of Average PSNR Differences between RDcurves
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Conti C, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574667
   Conti C, 2018, SIGNAL PROCESS-IMAGE, V60, P144, DOI 10.1016/j.image.2017.10.006
   Dai F, 2015, IEEE IMAGE PROC, P4733, DOI 10.1109/ICIP.2015.7351705
   Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137
   Alves GD, 2020, IEEE ACCESS, V8, P170807, DOI 10.1109/ACCESS.2020.3024844
   Georgiev T, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3442712
   Han HX, 2017, IEEE IMAGE PROC, P4008, DOI 10.1109/ICIP.2017.8297035
   Han HX, 2017, IEEE INT CON MULTI, P1177, DOI 10.1109/ICME.2017.8019424
   He DL, 2021, PROC CVPR IEEE, P14766, DOI 10.1109/CVPR46437.2021.01453
   Huang XP, 2022, IEEE T MULTIMEDIA, V24, P152, DOI 10.1109/TMM.2020.3046860
   Jiang F, 2021, IEEE IMAGE PROC, P2029, DOI 10.1109/ICIP42928.2021.9506363
   Jiang F, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675380
   Jiang G., 2022, Standard ISO/IEC JTCI/SC29/WG04 M58758
   Jiang X, 2017, INT CONF ACOUST SPEE, P1313, DOI 10.1109/ICASSP.2017.7952369
   Jin X., 2021, Standard ISOJEC JTC1/SC29/WG11 M58377
   Jin X, 2022, IEEE T BROADCAST, V68, P110, DOI 10.1109/TBC.2021.3108058
   Jin X, 2018, IEEE T IMAGE PROCESS, V27, P3954, DOI 10.1109/TIP.2018.2832449
   Li L, 2017, IEEE J-STSP, V11, P1107, DOI 10.1109/JSTSP.2017.2725198
   Li LJ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102725
   Li Y, 2016, IEEE T CIRC SYST VID, V26, P1308, DOI 10.1109/TCSVT.2015.2450333
   Liu D., 2016, P IEEE INT C MULT EX, P1, DOI [10.1109/ICMEW.2016.7574674, DOI 10.1109/ICMEW.2016.7574674]
   Liu DY, 2023, IEEE T MULTIMEDIA, V25, P4400, DOI 10.1109/TMM.2022.3175023
   Liu DY, 2021, INFORM SCIENCES, V545, P118, DOI 10.1016/j.ins.2020.07.073
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Lucas LER, 2014, EUR SIGNAL PR CONF, P11
   Lumsdaine A., 2009, P IEEE INT C COMP PH, P1, DOI DOI 10.1109/ICCPHOT.2009.5559008
   Minnen D, 2020, IEEE IMAGE PROC, P3339, DOI [10.1109/icip40778.2020.9190935, 10.1109/ICIP40778.2020.9190935]
   Minnen D, 2018, ADV NEUR IN, V31
   Monteiro RJS, 2017, IEEE J-STSP, V11, P1120, DOI 10.1109/JSTSP.2017.2721358
   Mukati MU, 2020, IEEE DATA COMPR CONF, P43, DOI 10.1109/DCC47342.2020.00012
   Ng R., 2005, Ph.D. dissertation
   Perra C, 2016, IEEE INT CONF MULTI
   Qian Y., 2021, P INT C LEARN REPR
   Schelkens P, 2019, PROC SPIE, V11137, DOI 10.1117/12.2532049
   Senoh T., 2021, Standard ISO/IEC JTC 1/SC 29/WG 04 M57813
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun X., 2020, Standard ISO/IEC JTC1/SC29/WG11 MPEG2020 M52250
   Sun X., 2019, Standard ISO/IEC JTCI/SC29/WG11 MPEG2019 M49008
   Sun X., 2019, Standard ISO/IEC JTC1/SC29/WG11 MPEG2019 M46259
   Taubman David., 2012, JPEG2000 Image Compression Fundamentals, Standards and Practice
   Teniou G., 2022, Standard ISO/IEC JTCI/SC29/WG02 M59163
   Teratani M., 2022, Standard ISO/IEC JTC 1/SC 29/WG 04 M58466
   Tong KD, 2022, INT CONF ACOUST SPEE, P1870, DOI 10.1109/ICASSP43922.2022.9747377
   Vaswani A, 2017, ADV NEUR IN, V30
   vegit, HEVC official test model
   vegit.hhi.fraunhofer.de, VVC official test model
   Vieira A, 2015, INT CONF IMAG PROC, P494, DOI 10.1109/IPTA.2015.7367195
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Zhang XY, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhong T., 2019, Standard ISO/IEC JTC1/SC29/WG11 MPEG2019 M49007
   Zhong TT, 2020, IEEE I C VI COM I PR, P209, DOI 10.1109/vcip49819.2020.9301793
   Zhong TT, 2019, INT CONF ACOUST SPEE, P8563, DOI 10.1109/icassp.2019.8682820
   Zhu X., 2022, P 2022 IEEE CVF C CO, P17612
   Zou RJ, 2022, PROC CVPR IEEE, P17471, DOI 10.1109/CVPR52688.2022.01697
NR 66
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 890
EP 903
DI 10.1109/TMM.2023.3272747
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700008
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, BL
   Bu, TC
   Hu, ZY
   Yang, L
   Zhao, YQ
   Li, XL
AF Wang, Binglu
   Bu, Tianci
   Hu, Zaiyi
   Yang, Le
   Zhao, Yongqiang
   Li, Xuelong
TI Coarse-to-Fine Nutrition Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Nutrition prediction; coarse-to-fine; multi-task learning; label
   smoothing
ID DIETARY ASSESSMENT; FOOD
AB Healthy dietary intake has a broad influence on the quality of life, and nutrition prediction plays a great role in the auxiliary decision-making of diet. Given a food image, existing nutrition prediction methods directly regress the nutrition content. However, due to the complex variations in food images, such as differences in viewpoint and lighting conditions, directly regressing the nutrition content faces significant challenges. The complexity of the food image data results in a high-dimensional and feature-rich input space, which poses difficulties for traditional regression models to efficiently navigate and optimize. Consequently, the direct regression paradigm usually generates inaccurate nutrition predictions. To alleviate the ambiguity challenge in the prediction progress, we propose to narrow the searchable space for the model's predictions by decomposing the direct regression into two steps: first coarsely selecting the nutrition scope and then finely refining the prediction value, forming a coarse-to-fine nutrition prediction paradigm. Although the process of coarse prediction which selects a bin from a series of scope bins can be formulated as a standard classification problem, it exhibits a distinguishable characteristic, i.e. the closer to the ground truth bin, the less punishment in the training phase. However, most of the current methods have ignored this phenomenon, thus, we specially design the linearly smoothed label in the nutrition prediction task to reveal the relative distance to the ground truth bin, leading to extraordinary improvements. Furthermore, we conduct a pair-wise comparison among all bins by extending the 1D label into 2D space and propose the structure loss to guide the bin selection process effectively. Due to the narrowed decision space, the nutrition prediction problem can be effectively optimized, and the proposed method achieves promising results on three benchmarks ECUSTFD, VFD and Nutrition5K, demonstrating the efficiency of the coarse-to-fine paradigm equipped with the linear-smoothed structure loss.
C1 [Wang, Binglu; Bu, Tianci] Xian Univ Architecture & Technol, Coll Informat & Control Engn, Xian 710055, Peoples R China.
   [Wang, Binglu] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Hu, Zaiyi; Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence OPt & Elect iOPEN, Xian 710072, Peoples R China.
   [Yang, Le] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230039, Peoples R China.
   [Yang, Le] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Peoples R China.
   [Zhao, Yongqiang] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Key Lab Intelligent Interact & Applicat, Minist Ind & Informat Technol, Xian 710072, Peoples R China.
C3 Xi'an University of Architecture & Technology; Beijing Institute of
   Technology; Northwestern Polytechnical University; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Northwestern
   Polytechnical University; Northwestern Polytechnical University
RP Yang, L (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230039, Peoples R China.
EM wbl921129@gmail.com; btc010001@gmail.com; zaiyihu@gmail.com;
   nwpuyangle@gmail.com; zhaoyq@nwpu.edu.cn; li@nwpu.edu.cn
OI Wang, Binglu/0000-0002-9266-4685; Hu, Zaiyi/0009-0004-4515-9688
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdar M, 2021, INFORM FUSION, V76, P243, DOI 10.1016/j.inffus.2021.05.008
   Aizawa K, 2013, IEEE T MULTIMEDIA, V15, P2176, DOI 10.1109/TMM.2013.2271474
   Ando Y, 2019, MADIMA'19: PROCEEDINGS OF THE 5TH INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P76, DOI 10.1145/3347448.3357172
   Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Chae J, 2011, PROC SPIE, V7873, DOI 10.1117/12.876669
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen M.-Y., 2012, P SIGGRAPH AS TECH B, P1
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Chorowski J, 2017, INTERSPEECH, P523, DOI 10.21437/Interspeech.2017-343
   Daugherty BL, 2012, J MED INTERNET RES, V14, DOI 10.2196/jmir.1967
   Dehais J, 2015, LECT NOTES COMPUT SC, V9281, P433, DOI 10.1007/978-3-319-23222-5_53
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Dou Z.-Y., 2022, Advances in neural information processing systems, V35, P32942
   Ege T., 2018, P JOINT WORKSHOP MUL, P53
   Fang SB, 2018, IEEE IMAGE PROC, P251, DOI 10.1109/ICIP.2018.8451461
   Fang SB, 2015, IEEE INT SYM MULTIM, P385, DOI 10.1109/ISM.2015.67
   Fleuret F, 2001, INT J COMPUT VISION, V41, P85, DOI 10.1023/A:1011113216584
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gawlikowski J., 2022, arXiv
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Hassannejad H, 2017, ALGORITHMS, V10, DOI 10.3390/a10020066
   He J., 2021, P IS T INT S EL IM I
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Hou SH, 2017, IEEE I CONF COMP VIS, P541, DOI 10.1109/ICCV.2017.66
   Kingma D. P., 2014, arXiv
   Kong FY, 2012, PERVASIVE MOB COMPUT, V8, P147, DOI 10.1016/j.pmcj.2011.07.003
   Li H, 2018, ADV NEUR IN, V31
   Liang YC, 2017, Arxiv, DOI [arXiv:1705.07632, DOI 10.48550/ARXIV.1705.07632]
   Liao HC, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P198, DOI 10.1109/SIPROCESS.2016.7888252
   Lo FPW, 2018, NUTRIENTS, V10, DOI 10.3390/nu10122005
   Lu Y, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20154283
   Lukasik M., 2020, INT C MACHINE LEARNI, P6448
   Marin J, 2021, IEEE T PATTERN ANAL, V43, P187, DOI 10.1109/TPAMI.2019.2927476
   Mezgec S, 2017, NUTRIENTS, V9, DOI 10.3390/nu9070657
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Müller R, 2019, ADV NEUR IN, V32
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nguyen HT, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108470
   Paszke A, 2019, ADV NEUR IN, V32
   Peng KY, 2022, IEEE COMPUT SOC CONF, P2074, DOI 10.1109/CVPRW56347.2022.00225
   Pouladzadeh P, 2014, IEEE T INSTRUM MEAS, V63, P1947, DOI 10.1109/TIM.2014.2303533
   Puri M., 2009, 2009 WORKSH APPL COM, P1, DOI [10.1109/WACV.2009.5403087, DOI 10.1109/WACV.2009.5403087]
   Quader Niamul, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P35, DOI 10.1007/978-3-030-58577-8_3
   Ruede R, 2021, INT C PATT RECOG, P4001, DOI 10.1109/ICPR48806.2021.9412839
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300
   Shroff G, 2008, TWELFTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P119, DOI 10.1109/ISWC.2008.4911602
   Situju SF, 2019, APPL ARTIF INTELL, V33, P732, DOI 10.1080/08839514.2019.1602318
   Su H, 2022, PATTERN RECOGN, V131, DOI 10.1016/j.patcog.2022.108868
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Thames Q, 2021, PROC CVPR IEEE, P8899, DOI 10.1109/CVPR46437.2021.00879
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Wang Q, 2022, IEEE T MULTIMEDIA, V24, P1031, DOI 10.1109/TMM.2021.3104141
   Wang Y, 2023, NEURAL NETWORKS, V160, P50, DOI 10.1016/j.neunet.2022.12.021
   Wei XK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5785, DOI 10.1109/ICCV48922.2021.00575
   Xu C, 2013, IEEE IMAGE PROC, P2534, DOI 10.1109/ICIP.2013.6738522
   Yang ZG, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10131556
   Yansong Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9836, DOI 10.1109/CVPR42600.2020.00986
   Yao Chun-Han, 2021, P IEEE INT C COMP VI, P12981
   Zhang KB, 2017, IEEE T NEUR NET LEAR, V28, P1109, DOI 10.1109/TNNLS.2015.2511069
   Zhang Weiyu, 2015, J Diabetes Sci Technol, V9, P525, DOI 10.1177/1932296815582222
   Zheng Yangtao, 2020, P IEEE CVF C COMP VI, P13766
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu FQ, 2010, IEEE J-STSP, V4, P756, DOI 10.1109/JSTSP.2010.2051471
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
NR 67
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3651
EP 3662
DI 10.1109/TMM.2023.3313638
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200020
DA 2024-08-05
ER

PT J
AU Wei, SC
   Luo, CB
   Luo, Y
   Xu, JL
AF Wei, Shicai
   Luo, Chunbo
   Luo, Yang
   Xu, Jialang
TI Privileged Modality Learning via Multimodal Hallucination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Privileged modality; incomplete modality; multimodal hallucination;
   knowledge distillation
ID REPRESENTATION; FRAMEWORK
AB Learning based on multimodal data has attracted increasing interest recently. While a variety of sensory modalities can be collected for training, not all of them are always available in practical scenarios, which raises the challenge to infer with incomplete modality. This article presents a general framework termed multimodal hallucination (MMH) to bridge the gap between ideal training scenarios and real-world deployment scenarios with incomplete modality data by transferring the complete multimodal knowledge to the hallucination network with incomplete modality input. Compared with the modality hallucination methods that restore privileged modalities information for late fusion, the proposed framework not only helps to preserve the crucial cross-modal cues but relates the study in complete modalities and in incomplete modalities. Then, we introduce two strategies called region-aware distillation and discrepancy-aware distillation to transfer the response-based and joint-representation-based knowledge of pre-trained multimodal networks, respectively. Region-aware distillation establishes and weights knowledge transferring pipelines between the response of multimodal and hallucination networks at multiple regions, which guides the hallucination network to focus on discriminative regions and avoid wasted gradients. Discrepancy-aware distillation guides the hallucination network to mimic the local inter-sample distance of multimodal representations, which enables the hallucination network to acquire the inter-class discrimination refined by multimodal cues. Extensive experiments on multimodal action recognition and face anti-spoofing demonstrate the proposed multimodal hallucination framework can overcome the problem of incomplete modality input in various scenes and achieve state-of-the-art performance.
C1 [Wei, Shicai; Luo, Chunbo; Luo, Yang; Xu, Jialang] Univ Elect Sci & Technol China, Sch Informat & Commun, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Luo, CB (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun, Chengdu 611731, Peoples R China.
EM shicaiwei@std.uestc.edu.cn; c.luo@ieee.org; luoyang@uestc.edu.cn;
   wzjialang@gmail.com
RI Xu, Jialang/ABE-9627-2020; Wei, Shicai/KFQ-4302-2024
OI Xu, Jialang/0000-0003-2324-7033; Wei, Shicai/0000-0001-5744-2035
FU National Natural Science Foundation of China
FX No Statement Available
CR Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bernal EA, 2018, IEEE T MULTIMEDIA, V20, P107, DOI 10.1109/TMM.2017.2726187
   Cai L, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1158, DOI 10.1145/3219819.3219963
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen LQ, 2021, PROC CVPR IEEE, P16291, DOI 10.1109/CVPR46437.2021.01603
   Chen Z, 2018, PR MACH LEARN RES, V80
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Dai X, 2021, PROC CVPR IEEE, P7838, DOI 10.1109/CVPR46437.2021.00775
   Day Oscar, 2017, Journal of Big Data, V4, DOI 10.1186/s40537-017-0089-0
   Ebrahimighahnavieh MA, 2020, COMPUT METH PROG BIO, V187, DOI 10.1016/j.cmpb.2019.105242
   Fan K, 2018, IEEE T IND INFORM, V14, P1656, DOI 10.1109/TII.2018.2794996
   Feyereisl J, 2014, P NIPS, V1, P208
   Gao ZF, 2020, IEEE T MED IMAGING, V39, P1524, DOI 10.1109/TMI.2019.2952939
   Garcia NC, 2018, LECT NOTES COMPUT SC, V11212, P106, DOI 10.1007/978-3-030-01237-3_7
   Garcia NC, 2020, IEEE T PATTERN ANAL, V42, P2581, DOI 10.1109/TPAMI.2019.2929038
   Garcia NC, 2021, IEEE WINT CONF APPL, P2754, DOI 10.1109/WACV48630.2021.00280
   Gat Itai, 2021, PROC 35 C NEURAL INF, V34
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   Gu ZX, 2021, IEEE T MULTIMEDIA, V23, P3738, DOI 10.1109/TMM.2020.3035231
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hessel J, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P861
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96
   Hong DF, 2020, IEEE GEOSCI REMOTE S, V17, P1470, DOI 10.1109/LGRS.2019.2944599
   Hong Y, 2022, LECT NOTES COMPUT SC, V13670, P87, DOI 10.1007/978-3-031-20080-9_6
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Y., 2021, ADV NEURAL INFORM PR
   Huang ZH, 2017, Arxiv, DOI arXiv:1707.01219
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Jaiswal M, 2020, AAAI CONF ARTIF INTE, V34, P7985
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Jue J, 2019, LECT NOTES COMPUT SC, V11769, P221, DOI 10.1007/978-3-030-32226-7_25
   Kay W., 2017, The Kinetics Human Action Video Dataset
   Komodakis S., 2017, INT C LEARN REPRESEN, P1
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lee SHY, 2019, Arxiv, DOI arXiv:1907.02226
   Li X, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P827, DOI 10.1145/3366423.3380163
   Li X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2020.3048332
   Li Z, 2023, IEEE T MULTIMEDIA, V25, P62, DOI 10.1109/TMM.2021.3121140
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Liu AJ, 2021, IEEE WINT CONF APPL, P1178, DOI 10.1109/WACV48630.2021.00122
   Liu AJ, 2021, IEEE T INF FOREN SEC, V16, P2759, DOI 10.1109/TIFS.2021.3065495
   Liu H, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3174352
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu YF, 2019, PROC CVPR IEEE, P2599, DOI 10.1109/CVPR.2019.00271
   Luo ZL, 2018, LECT NOTES COMPUT SC, V11218, P174, DOI 10.1007/978-3-030-01264-9_11
   Müller H, 2017, IEEE T MULTIMEDIA, V19, P2093, DOI 10.1109/TMM.2017.2729400
   Pan YS, 2020, IEEE T MED IMAGING, V39, P2965, DOI 10.1109/TMI.2020.2983085
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Passban Y., 2020, AAAI CONFARTIF INTEL, P13657
   Peng X, 2019, PR MACH LEARN RES, V97
   Romero A., 2015, ICLR, P1
   Sengupta N, 2018, IEEE T NEUR NET LEAR, V29, P5249, DOI 10.1109/TNNLS.2018.2796023
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sharma R., 2016, INT C LEARN REPRESEN, P1
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Stroud JC, 2020, IEEE WINT CONF APPL, P614, DOI 10.1109/wacv45572.2020.9093274
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Vaswani A, 2017, ADV NEUR IN, V30
   Vu TH, 2019, IEEE I CONF COMP VIS, P7363, DOI 10.1109/ICCV.2019.00746
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang QQ, 2018, IEEE DATA MINING, P1290, DOI 10.1109/ICDM.2018.00174
   Wonkyung Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P465, DOI 10.1007/978-3-030-58586-0_28
   Wu ZN, 2022, IEEE T MULTIMEDIA, V24, P1080, DOI 10.1109/TMM.2021.3116417
   Xu C, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2590, DOI 10.1145/3394486.3403309
   Yang MX, 2023, IEEE T PATTERN ANAL, V45, P1055, DOI 10.1109/TPAMI.2022.3155499
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Zhang SF, 2019, PROC CVPR IEEE, P919, DOI 10.1109/CVPR.2019.00101
   Zhang Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2717, DOI 10.1145/3474085.3475204
   Zhao H., 2016, PROC INT JOINT C AR, P2392
   Zhao PS, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108741
   Zhou JT, 2019, ARTIF INTELL, V275, P310, DOI 10.1016/j.artint.2019.06.001
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 78
TC 1
Z9 1
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1516
EP 1527
DI 10.1109/TMM.2023.3282874
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700041
DA 2024-08-05
ER

PT J
AU Yang, J
   Wei, P
   Ren, ZY
   Zheng, NN
AF Yang, Jin
   Wei, Ping
   Ren, Ziyang
   Zheng, Nanning
TI Gated Multi-Scale Transformer for Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal action localization; multi-scale transformer; gated control
   mechanism; video understanding
ID NETWORK
AB Temporal action localization (TAL) is a critical task in video understanding. Effectively utilizing multi-scale information and handling interactions across various scales have consistently posed challenging issues within the realm of TAL. In this article, we propose a novel gated multi-scale Transformer model (TransGMC) for temporal action localization. A gated control mechanism is designed to filter and aggregate the information at different scales, by which the contributions of contexts at different temporal scales are well characterized. To enhance the feature representation at each temporal scale, the rich global-local contexts are extracted at each temporal scale. A cascade attention module that contains two seamlessly integrated channel attention and moment attention is proposed for capturing global temporal contexts. We utilize a new regression loss function for locating the time boundaries. We conducted experiments on four challenging benchmark datasets, including two third-person view datasets and two first-person view datasets. Our method achieves an average mAP of 67.5% on THUMOS14, 36.1% on ActivityNet v1.3, 24.9% on EPIC-Kitchens 100, and 23.2% on Ego4D, which all outperform the previous state-of-the-arts methods. Extensive ablation studies also validate the effectiveness of the proposed method.
C1 [Yang, Jin; Wei, Ping; Ren, Ziyang; Zheng, Nanning] Xi An Jiao Tong Univ, Natl Engn Res Ctr Visual Informat & Applicat, Natl Key Lab Human Machine Hybrid Augmented Intell, Xian 710049, Shaanxi, Peoples R China.
   [Yang, Jin; Wei, Ping; Ren, Ziyang; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Wei, P (corresponding author), Xi An Jiao Tong Univ, Natl Engn Res Ctr Visual Informat & Applicat, Natl Key Lab Human Machine Hybrid Augmented Intell, Xian 710049, Shaanxi, Peoples R China.
EM jin.yang@stu.xjtu.edu.cn; pingwei@xjtu.edu.cn; rzyrzy@stu.xjtu.edu.cn;
   nnzheng@mail.xjtu.edu.cn
OI yang, jin/0000-0001-9625-7449
FU National Natural Science Foundation of China
FX No Statement Available
CR Beltagy I, 2020, Arxiv, DOI arXiv:2004.05150
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carion N., 2020, EUR C COMP VIS, P213
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Damen D, 2022, INT J COMPUT VISION, V130, P33, DOI 10.1007/s11263-021-01531-2
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Girdhar R, 2022, PROC CVPR IEEE, P16081, DOI 10.1109/CVPR52688.2022.01563
   Gong GQ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102850
   Grauman K, 2022, PROC CVPR IEEE, P18973, DOI 10.1109/CVPR52688.2022.01842
   Guo ZC, 2023, IEEE T MULTIMEDIA, V25, P38, DOI 10.1109/TMM.2021.3120544
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Huang ZL, 2021, Arxiv, DOI arXiv:2106.03650
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Kingma D. P., 2014, arXiv
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin Kevin Qinghong, 2022, ADV NEURAL INFORM PR, V35, P7575
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu QY, 2020, AAAI CONF ARTIF INTE, V34, P11612
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu ST, 2019, Arxiv, DOI arXiv:1911.09516
   Liu XL, 2022, IEEE T IMAGE PROCESS, V31, P5427, DOI 10.1109/TIP.2022.3195321
   Liu XL, 2021, PROC CVPR IEEE, P12591, DOI 10.1109/CVPR46437.2021.01241
   Liu Y, 2019, PROC CVPR IEEE, P3599, DOI [10.1109/CVPR.2019.00372, 10.1109/CVPR.2019.00726]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Mettes P, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P427, DOI 10.1145/2671188.2749404
   Nie D, 2022, PROC CVPR IEEE, P17263, DOI 10.1109/CVPR52688.2022.01677
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Pirsiavash H, 2014, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2014.85
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Qiu ZF, 2019, PROC CVPR IEEE, P12048, DOI 10.1109/CVPR.2019.01233
   Sang L, 2021, IEEE T MULTIMEDIA, V23, P2019, DOI 10.1109/TMM.2020.3007330
   Saquil Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1698, DOI 10.1109/ICCV48922.2021.00174
   Shang JH, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104589
   Sheng XH, 2023, IEEE T MULTIMEDIA, V25, P7311, DOI 10.1109/TMM.2022.3220421
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Tan M., 2020, PROC IEEECVF C COMPU, p10 781
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang W., 2022, PROC INT C LEARN REP
   Wei P, 2019, IEEE T MULTIMEDIA, V21, P2195, DOI 10.1109/TMM.2019.2897902
   Wei P, 2017, IEEE T PATTERN ANAL, V39, P1165, DOI 10.1109/TPAMI.2016.2574712
   Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054
   Xia K, 2023, IEEE T MULTIMEDIA, V25, P9425, DOI 10.1109/TMM.2023.3252176
   Xu M., 2020, P IEEE CVF C COMP VI, P10156
   Yan S, 2022, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR52688.2022.00333
   Yang Jin, 2024, IEEE Transactions on Circuits and Systems for Video Technology, V34, P4625, DOI 10.1109/TCSVT.2023.3326692
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yuan J, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2019.107131
   Yueran Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P121, DOI 10.1007/978-3-030-58604-1_8
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang CL, 2022, LECT NOTES COMPUT SC, V13664, P492, DOI 10.1007/978-3-031-19772-7_29
   Zhao C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13638, DOI 10.1109/ICCV48922.2021.01340
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou Y, 2021, IEEE T MULTIMEDIA, V23, P4363, DOI 10.1109/TMM.2020.3042077
   Zhu WC, 2021, IEEE T IMAGE PROCESS, V30, P948, DOI 10.1109/TIP.2020.3039886
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
   Zhu ZX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13496, DOI 10.1109/ICCV48922.2021.01326
NR 68
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5705
EP 5717
DI 10.1109/TMM.2023.3338082
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100010
DA 2024-08-05
ER

PT J
AU Yang, XF
   Liu, FY
   Lin, GS
AF Yang, Xiaofeng
   Liu, Fayao
   Lin, Guosheng
TI Neural Logic Vision Language Explainer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Explainable artificial intelligence; vision language pretraining
ID PREDICTION; RULES
AB If we compare how humans reason and how deep models reason, humans reason in a symbolic manner with a formal language called logic, while most deep models reason in black-box. A natural question to ask is "Do the trained deep models reason similar as humans?" or "Can we explain the reasoning of deep models in the language of logic?". In this work, we present NeurLogX to explain the reasoning process of deep vision language models in the language of logic. Given a trained vision language model, our method starts by generating reasoning facts through augmenting the input data. We then develop a differentiable inductive logic programming framework to learn interpretable logic rules from the facts. We show our results on various popular vision language models. Interestingly, we observe that almost all of the tested models can reason logically.
C1 [Yang, Xiaofeng; Lin, Guosheng] Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Liu, Fayao] ASTAR, Singapore 138632, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR)
RP Lin, GS (corresponding author), Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM xiaofeng001@e.ntu.edu.sg; fayaoliu@gmail.com; gslin@ntu.edu.sg
OI LIU, Fayao/0000-0001-6649-7660; Yang, Xiaofeng/0000-0003-1882-9733
FU National Research Foundation, Singapore
FX No Statement Available
CR Aditya S, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6252
   Agarwal R., 2021, Advances in Neural Information Processing Systems, V34
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Cao Jize, 2020, ECCV, P565
   Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Das A., 2020, arXiv, DOI DOI 10.48550/ARXIV.2006.11371
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Evans R, 2018, J ARTIF INTELL RES, V61, P1
   Ge YH, 2021, PROC CVPR IEEE, P2195, DOI 10.1109/CVPR46437.2021.00223
   Gokhale Tejas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P379, DOI 10.1007/978-3-030-58589-1_23
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guu Kelvin, 2015, P 2015 C EMPIRICAL M, DOI 10.18653/v1/d15-1038
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Ibrahim M, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P279, DOI 10.1145/3306618.3314230
   Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045
   Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kim W., 2021, PROC INT C MACH LEAR, P5583
   LANGER SK, 1953, INTRO SYMBOLIC LOGIC
   Lapuschkin S, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08987-4
   Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li HY, 2019, IMAGE VISION COMPUT, V83-84, P70, DOI 10.1016/j.imavis.2019.02.005
   Li Liunian Harold, 2020, P 58 ANN M ASS COMP, P5265, DOI DOI 10.18653/V1/2020.ACL-MAIN.469
   Li X., 2020, Oscar: Object-Semantics Aligned Pre-training for VisionLanguage Tasks, DOI 10.1007/978-3-030-58577-8_8
   Linardatos P, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23010018
   Littman M. L., 1996, ALGORITHMS SEQUENTIA, P3
   Lu JS, 2019, ADV NEUR IN, V32
   Marra G., 2021, P UNC ART INT, P908
   MUGGLETON S, 1995, NEW GENERAT COMPUT, V13, P245, DOI 10.1007/BF03037227
   MUGGLETON S, 1994, J LOGIC PROGRAM, V20, P629, DOI 10.1016/0743-1066(94)90035-3
   Ouyang NL, 2021, IEEE T MULTIMEDIA, V24, P3405, DOI 10.1109/TMM.2021.3097502
   Radford A, 2021, PR MACH LEARN RES, V139
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Rocktäschel T, 2017, ADV NEUR IN, V30
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Theodoridis S, 2006, PATTERN RECOGNITION, 3RD EDITION, P1
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic P, 2018, INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1710.10903
   Vilone G, 2020, Arxiv, DOI [arXiv:2006.00093, 10.48550/arXiv.2006.00093]
   Wang W., 2023, P INT C LEARN REPR, P1
   Wang WG, 2022, Arxiv, DOI [arXiv:2210.15889, 10.48550/arXiv.2210.15889]
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Xu K., 2020, P 8 INT C LEARN REPR
   Xu K., 2019, P INT C MACH LEARN I, P1
   Yang F, 2017, 31 ANN C NEURAL INFO, V30
   Yang Y, 2020, P 8 INT C LEARN REPR
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yi KX, 2018, ADV NEUR IN, V31
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 54
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3331
EP 3340
DI 10.1109/TMM.2023.3310277
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200040
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Zhao, ML
   Wang, WM
   Chen, TB
   Zhang, R
   Li, RC
AF Zhao, Minglu
   Wang, Wenmin
   Chen, Tongbao
   Zhang, Rui
   Li, Ruochen
TI TA2V: Text-Audio Guided Video Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal video generation; text-audio to video; VQ-GAN; diffusion;
   deep learning
ID NETWORK
AB Recent conditional and unconditional video generation tasks have been accomplished mainly based on generative adversarial network (GAN), diffusion, and autoregressive models. However, in some circumstances, using only one modality cannot provide enough semantic information. Therefore, in this paper, we propose text-audio to video (TA2V) generation, a new task for generating realistic videos from two different guided modalities, text and audio, which has not been explored much thus far. Compared to image generation, video generation is a harder task because of the complexity of processing higher-dimensional data and scarcer suitable datasets, especially for multimodal video generation. To overcome these limitations, (i) we propose the Text&Audio-guided-Video-Maker (TAgVM) model, which consists of two modules: a text-guided video generator and a text&audio-guided video modifier. (ii) This model uses a 3D VQ-GAN to compress high-dimension video data to a low-dimension discrete sequence, followed by an autoregressive model to guide text-conditional generation in the latent space. Then, we apply a text&audio-guided diffusion model to the generated video scenes, providing additional semantic details corresponding to the audio and text. (iii) We introduce a newly produced music performance video dataset, the University of Rochester Multimodal Music Performance with Video-Audio-Text (URMP-VAT), and a landscape dataset, Landscape with Video-Audio-Text (Landscape-VAT), both of which include three modalities (text, audio, and video) that are aligned with each other. The results demonstrate that our model can create videos with satisfactory quality and semantic information.
C1 [Zhao, Minglu; Wang, Wenmin; Chen, Tongbao; Li, Ruochen] Macau Univ Sci & Technol, Sch Comp Sci & Engn, Macau 999078, Peoples R China.
   [Zhang, Rui] Beijing Inst Technol, Sch Mech Engn, Beijing 100811, Peoples R China.
C3 Macau University of Science & Technology; Beijing Institute of
   Technology
RP Wang, WM (corresponding author), Macau Univ Sci & Technol, Sch Comp Sci & Engn, Macau 999078, Peoples R China.
EM 3220000901@student.must.edu.mo; wmwang@must.edu.mo;
   tongbaochenchn@163.com; m15540575962@163.com; lircsszz@outlook.com
OI Li, Ruochen/0000-0002-4341-6474; chen, tongbao/0000-0002-7719-8364;
   Zhao, Minglu/0009-0005-3632-4167
FU Science and Technology Development Fund of Macau
FX No Statement Available
CR Avrahami O, 2022, PROC CVPR IEEE, P18187, DOI 10.1109/CVPR52688.2022.01767
   Bar-Tal O, 2022, LECT NOTES COMPUT SC, V13675, P707, DOI 10.1007/978-3-031-19784-0_41
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chatterjee Moitreya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P701, DOI 10.1007/978-3-030-58583-9_42
   Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI [10.1109/ICASSP40776.2020.9053174, 10.1109/icassp40776.2020.9053174]
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ding Ming, 2022, Advances in Neural Information Processing Systems, V35, P16890
   Dinh L, 2015, Arxiv, DOI [arXiv:1410.8516, 10.48550/arXiv.1410.8516]
   Dinh L, 2017, Arxiv, DOI [arXiv:1605.08803, DOI 10.48550/ARXIV.1605.08803]
   Duan B, 2021, INT C PATT RECOG, P1336, DOI 10.1109/ICPR48806.2021.9412890
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Ge SW, 2022, LECT NOTES COMPUT SC, V13677, P102, DOI 10.1007/978-3-031-19790-1_7
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Ghandi T, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3617592
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Guzhov A, 2022, INT CONF ACOUST SPEE, P976, DOI 10.1109/ICASSP43922.2022.9747631
   Hayes T, 2022, LECT NOTES COMPUT SC, V13668, P431, DOI 10.1007/978-3-031-20074-8_25
   Hensel M, 2017, ADV NEUR IN, V30
   Hessel J., 2021, arXiv
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Ho JAT, 2022, Arxiv, DOI [arXiv:2210.02303, DOI 10.48550/ARXIV.2210.02303]
   Ho JAT, 2022, Arxiv, DOI [arXiv:2204.03458, DOI 10.48550/ARXIV.2204.03458,ARXIV]
   Hong W., 2022, arXiv
   Hu YS, 2022, PROC CVPR IEEE, P18198, DOI 10.1109/CVPR52688.2022.01768
   Jeong D., 2021, arXiv
   Jeong Y, 2023, IEEE I CONF COMP VIS, P7788, DOI 10.1109/ICCV51070.2023.00719
   Jing SQ, 2024, IEEE T MULTIMEDIA, V26, P2367, DOI 10.1109/TMM.2023.3295098
   Karras T, 2021, ADV NEUR IN, V34
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim Y., 2022, arXiv
   Kingma D. P., 2014, arXiv
   Kong Z., 2020, arXiv, DOI DOI 10.48550/ARXIV.2009.09761
   Krizhevsky A., 2010, Cifar-10 (canadian institute for advanced research)
   Lee S, 2023, Arxiv, DOI [arXiv:2304.06818, 10.48550/arXiv.2304.06818, DOI 10.48550/ARXIV.2304.06818]
   Lee SH, 2022, LECT NOTES COMPUT SC, V13677, P34, DOI 10.1007/978-3-031-19790-1_3
   Lee S, 2023, Arxiv, DOI arXiv:2305.04001
   Li BC, 2019, IEEE T MULTIMEDIA, V21, P522, DOI 10.1109/TMM.2018.2856090
   Li DX, 2023, PROCEEDINGS OF THE 61ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-DEMO 2023, VOL 3, P31
   Li YT, 2018, AAAI CONF ARTIF INTE, P7065
   Liu JW, 2024, IEEE T MULTIMEDIA, V26, P141, DOI 10.1109/TMM.2023.3262180
   Liu ST, 2023, Arxiv, DOI arXiv:2303.04761
   Liu V, 2023, Arxiv, DOI arXiv:2304.08551
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   McFee B., 2015, P 14 PYTH SCI C, V8, P18
   Molad E, 2024, arXiv
   Montesinos JF, 2020, IEEE INT WORKSH MULT, DOI 10.1109/mmsp48831.2020.9287124
   Nichol A., 2021, PMLR, P16784
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Singer U, 2022, Arxiv, DOI arXiv:2209.14792
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Su ST, 2024, IEEE T MULTIMEDIA, V26, P2354, DOI 10.1109/TMM.2023.3295094
   Sung-Bin K, 2023, PROC CVPR IEEE, P6430, DOI 10.1109/CVPR52729.2023.00622
   Tang Z., 2024, Adv. Neural Inf. Process. Syst., V36
   Tsuchiya Y., 2019, PROC IEEE C COMPUT V, P17
   Tu XG, 2022, IEEE T CIRC SYST VID, V32, P1805, DOI 10.1109/TCSVT.2021.3083257
   Unterthiner T, 2019, Arxiv, DOI [arXiv:1812.01717, 10.48550/arXiv.1812.01717]
   van den Oord A, 2017, ADV NEUR IN, V30
   Wu CF, 2022, LECT NOTES COMPUT SC, V13676, P720, DOI 10.1007/978-3-031-19787-1_41
   Wu CF, 2021, Arxiv, DOI arXiv:2104.14806
   Wu JZ, 2023, IEEE I CONF COMP VIS, P7589, DOI 10.1109/ICCV51070.2023.00701
   Yariv G, 2023, Arxiv, DOI arXiv:2309.16429
   Yoon Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417838
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Zhang Y, 2023, Arxiv, DOI arXiv:2209.06496
   Zhu H, 2021, INT C PATT RECOG, P3574, DOI 10.1109/ICPR48806.2021.9412698
   Zhu Junchen, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P9371, DOI 10.1145/3581783.3612667
   Zhu JC, 2023, IEEE T PATTERN ANAL, V45, P3311, DOI 10.1109/TPAMI.2022.3186752
NR 72
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7250
EP 7264
DI 10.1109/TMM.2024.3362149
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000019
DA 2024-08-05
ER

PT J
AU Chen, K
   Yu, L
AF Chen, Kang
   Yu, Lei
TI Motion Deblur by Learning Residual From Events
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event camera; motion deblur; residual
ID NETWORK; SENSOR; PIXEL
AB Conventional cameras face challenges when capturing motion information during the exposure due to their physical design, rendering the motion deblurring task ill-posed. To this end, we propose a Two-stage Residual-based Motion Deblurring (TRMD) framework for an event camera, which converts a blurry image into a sequence of sharp images, leveraging the abundant motion features encoded in events. In the first stage, a residual estimation network is trained to estimate the residual sequence, which measures the intensity difference between the intermediate frame and other frames sampled during the exposure. In the subsequent stage, the previously estimated residuals are combined with the blurry image to reconstruct the deblurred sequence based on the physical model of motion blur. To facilitate the efficient integration of image and event modalities for residual estimation, we propose a cross-modal fusion module based on spatial-channel attention, aiming to fuse the complementary spatial-temporal features of two modalities. Extensive experiments demonstrate that our method outperforms current state-of-the-art approaches on the synthetic dataset GOPRO and produces superior visualization with less noise and artifacts on the real blur event dataset REBlur.
C1 [Chen, Kang; Yu, Lei] Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Yu, L (corresponding author), Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
EM mrchenkang@whu.edu.cn; ly.wd@whu.edu.cn
OI Yu, Lei/0000-0002-7329-4631; Chen, Kang/0009-0001-2161-0364
FU National Natural Science Foundation of China
FX No Statement Available
CR Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Chen HY, 2023, IEEE T MULTIMEDIA, V25, P5826, DOI 10.1109/TMM.2022.3199556
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Dai Yuekun, 2023, P IEEECVF C COMPUTER, P2852
   Galassi A, 2021, IEEE T NEUR NET LEAR, V32, P4291, DOI 10.1109/TNNLS.2020.3019893
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Huang ZW, 2022, LECT NOTES COMPUT SC, V13674, P624, DOI 10.1007/978-3-031-19781-9_36
   Kim Kiyeon, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13802), P524, DOI 10.1007/978-3-031-25063-7_32
   Kim T, 2022, LECT NOTES COMPUT SC, V13678, P519, DOI 10.1007/978-3-031-19797-0_30
   Li Y., 2023, P IEEE CVF C COMP VI, P1904
   Liu H., 2022, P IEEE CVF C COMP VI, P5791
   Liu LB, 2021, PROC CVPR IEEE, P4821, DOI 10.1109/CVPR46437.2021.00479
   Liu Y, 2022, IEEE T MULTIMEDIA, V24, P2890, DOI 10.1109/TMM.2021.3090206
   Mao XT, 2023, AAAI CONF ARTIF INTE, P1905
   Nah S, 2019, PROC CVPR IEEE, P8094, DOI 10.1109/CVPR.2019.00829
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan LY, 2019, PROC CVPR IEEE, P6813, DOI 10.1109/CVPR.2019.00698
   Peterson B., 2016, Understanding exposure: how to shoot great photographs with any camera
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Rebecq H., 2018, C ROB LEARN, P969, DOI DOI 10.1007/978-3-319-24574-4_28
   Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   Shang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4511, DOI 10.1109/ICCV48922.2021.00449
   Son B, 2017, ISSCC DIG TECH PAP I, P66
   Song C, 2022, PROC CVPR IEEE, P7793, DOI 10.1109/CVPR52688.2022.00765
   Songnan Lin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P695, DOI 10.1007/978-3-030-58598-3_41
   Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366
   Sun L, 2022, LECT NOTES COMPUT SC, V13678, P412, DOI 10.1007/978-3-031-19797-0_24
   Vasluianu Florin-Alexandru, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1788, DOI 10.1109/CVPRW59228.2023.00179
   Vaswani A, 2017, ADV NEUR IN, V30
   Vitoria Patricia, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13805), P95, DOI 10.1007/978-3-031-25072-9_7
   Wang Bishan, 2020, ECCV
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2563, DOI 10.1109/ICCV48922.2021.00258
   Yang D, 2022, IEEE COMPUT SOC CONF, P1112, DOI 10.1109/CVPRW56347.2022.00120
   Zhang GQ, 2023, IEEE T IMAGE PROCESS, V32, P4555, DOI 10.1109/TIP.2023.3279673
   Zhang GQ, 2021, IEEE T IMAGE PROCESS, V30, P8913, DOI 10.1109/TIP.2021.3120054
   Zhang H., 2020, IEEE Access, V8
   Zhang KH, 2022, INT J COMPUT VISION, V130, P2103, DOI 10.1007/s11263-022-01633-5
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X, 2022, PROC CVPR IEEE, P17744, DOI 10.1109/CVPR52688.2022.01724
   Zhang XQ, 2021, IEEE T CIRC SYST VID, V31, P3025, DOI 10.1109/TCSVT.2020.3035722
   Zhang Yang, 2022, Proceedings of the 11th International Conference on Computer Engineering and Networks. Lecture Notes in Electrical Engineering (808), P90, DOI 10.1007/978-981-16-6554-7_10
   Zhang Z, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109740
   Zhao SY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6193, DOI 10.1145/3503161.3547962
   Zhao SY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6220, DOI 10.1145/3503161.3548113
   Zhu L, 2022, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR52688.2022.00358
NR 51
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6632
EP 6647
DI 10.1109/TMM.2024.3355630
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600041
DA 2024-08-05
ER

PT J
AU Chen, ZN
   Wang, WQ
   Zhao, ZC
   Su, F
   Men, A
   Dong, Y
AF Chen, Zining
   Wang, Weiqiu
   Zhao, Zhicheng
   Su, Fei
   Men, Aidong
   Dong, Yuan
TI Cluster-Instance Normalization: A Statistical Relation-Aware
   Normalization for Generalizable Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cluster-Instance normalization; group-based triplet loss; dynamic
   recalibration module; generalizable person re-identification
ID DOMAIN GENERALIZATION; NETWORK
AB Person re-identification (ReID) has achieved great improvement under supervised settings, but suffers from considerable degradation when large distribution shifts between training and testing sets exist. Domain generalization (DG ReID) emerges to promote the generalization ability of models, overcoming the distribution shifts issue between source domains and unseen target domains. Among most prior methods in DG ReID, instance normalization (IN) serves as a promising solution for removing domain-specific information, however, it damages the discriminative ability simultaneously. In this article, we propose a new normalization method called Cluster-Instance Normalization (CINorm) to extract information from clusters for information compensation. The relations between samples in a batch can be mined to establish evolving clusters with aggregated samples during the forward training process. In this way, high intra-cluster congregation can eliminate the impacts of outliers to avoid overfitting, and high inter-cluster variances can synthesize diverse novel statistics to compensate discriminative information. Therefore, a Relation-Aware Normalization (RANorm) with a Dynamic ReCalibration (DRC) module is designed to integrate normalized features between evolving clusters and instances efficiently. Furthermore, a novel Group-based Triplet (G-Triplet) loss is proposed to divide a batch into multiple groups with greater compactness for hard-pair mining. Extensive experiments show that our method outperforms state-of-the-art algorithms on multiple DG benchmarks by a large margin. The proposed method can also achieve superior performance on image classification tasks under DG settings without using domain labels.
C1 [Chen, Zining; Wang, Weiqiu; Zhao, Zhicheng; Su, Fei; Men, Aidong; Dong, Yuan] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing Key Lab Network Syst, Key Lab Interact Technol & Experience Syst,Minist, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Zhao, ZC (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing Key Lab Network Syst, Key Lab Interact Technol & Experience Syst,Minist, Beijing 100876, Peoples R China.
EM chenzn@bupt.edu.cn; wangweiqiu@bupt.edu.cn; zhaozc@bupt.edu.cn;
   sufei@bupt.edu.cn; menad@bupt.edu.cn; yuandong@bupt.edu.cn
OI Chen, Zining/0009-0007-4214-1054; Zhao, Zhicheng/0000-0001-6506-7298
FU Chinese National Natural Science Foundation
FX No Statement Available
CR Bai S, 2021, IEEE T PATTERN ANAL, V43, P2119, DOI 10.1109/TPAMI.2020.3031625
   Bai Y, 2021, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR46437.2021.00216
   Balaji Y, 2018, ADV NEUR IN, V31
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chang XY, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107569
   Chen PX, 2021, AAAI CONF ARTIF INTE, V35, P1054
   Chen Z., 2023, IEEE Trans. Circuits Syst. Video Technol., DOI [10.1109/TAI.2023.3298297, DOI 10.1109/TAI.2023.3298297]
   Choi S, 2021, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR46437.2021.00343
   Dai YX, 2021, PROC CVPR IEEE, P16140, DOI 10.1109/CVPR46437.2021.01588
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia JR, 2019, Arxiv, DOI arXiv:1905.03422
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jiao BL, 2022, LECT NOTES COMPUT SC, V13674, P285, DOI 10.1007/978-3-031-19781-9_17
   Jin X, 2021, IEEE T MULTIMEDIA, V24, P3636, DOI 10.1109/TMM.2021.3104379
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Kang J, 2022, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR52688.2022.00699
   Lee K., 2022, P COMP VIS 17 EUR C, P1
   Li JN, 2022, IEEE T PATTERN ANAL, V44, P622, DOI 10.1109/TPAMI.2019.2929036
   Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Liao SC, 2022, PROC CVPR IEEE, P7349, DOI 10.1109/CVPR52688.2022.00721
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Liu XC, 2022, PROC CVPR IEEE, P14320, DOI 10.1109/CVPR52688.2022.01394
   Loy CC, 2009, PROC CVPR IEEE, P1988, DOI 10.1109/CVPRW.2009.5206827
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo P, 2021, IEEE T PATTERN ANAL, V43, P712, DOI 10.1109/TPAMI.2019.2932062
   Lv FR, 2022, PROC CVPR IEEE, P8036, DOI 10.1109/CVPR52688.2022.00788
   Mahajan D, 2021, PR MACH LEARN RES, V139
   Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749
   Nam H, 2018, ADV NEUR IN, V31
   Nam H, 2021, PROC CVPR IEEE, P8686, DOI 10.1109/CVPR46437.2021.00858
   Ni H, 2022, PROC CVPR IEEE, P2477, DOI 10.1109/CVPR52688.2022.00252
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Piratla V., 2020, INT C MACH LEARN, P7728
   Qi L, 2023, IEEE T MULTIMEDIA, V25, P4856, DOI 10.1109/TMM.2022.3183393
   Shankar S, 2018, Arxiv, DOI arXiv:1804.10745
   Shengcai Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P456, DOI 10.1007/978-3-030-58621-8_27
   Song JF, 2019, PROC CVPR IEEE, P719, DOI 10.1109/CVPR.2019.00081
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Volpi R, 2018, ADV NEUR IN, V31
   Wang GQ, 2020, PROC CVPR IEEE, P6677, DOI 10.1109/CVPR42600.2020.00671
   Wang Zijian, 2021, P IEEE CVF INT C COM, P834
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu L, 2020, IEEE T CIRC SYST VID, V30, P2081, DOI 10.1109/TCSVT.2019.2909549
   Wu WY, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107424
   Xiao T., 2016, arXiv
   Xu BQ, 2022, Arxiv, DOI arXiv:2112.08684
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang KW, 2023, IEEE T MULTIMEDIA, V25, P3386, DOI 10.1109/TMM.2022.3160057
   Yao XF, 2022, PROC CVPR IEEE, P7087, DOI 10.1109/CVPR52688.2022.00696
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zeyi Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P124, DOI 10.1007/978-3-030-58536-5_8
   Zhang PY, 2022, LECT NOTES COMPUT SC, V13674, P215, DOI 10.1007/978-3-031-19781-9_13
   Zhang YF, 2023, IEEE T IMAGE PROCESS, V32, P509, DOI 10.1109/TIP.2022.3229621
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao Shanshan, 2020, P ADV NEURAL INFORM, V33, P16096
   Zhao YY, 2021, PROC CVPR IEEE, P6273, DOI 10.1109/CVPR46437.2021.00621
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Wei-Shi, 2009, BRIT MACH VIS C, P1, DOI 10.5244/C.23.23
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou KY, 2022, IEEE T PATTERN ANAL, V44, P5056, DOI 10.1109/TPAMI.2021.3069237
   Zhou KY, 2021, Arxiv, DOI arXiv:2104.02008
   Zhou KY, 2020, AAAI CONF ARTIF INTE, V34, P13025
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
NR 79
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3554
EP 3566
DI 10.1109/TMM.2023.3312939
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200042
DA 2024-08-05
ER

PT J
AU Deng, MQ
   Fan, ZY
   Lin, P
   Feng, XR
AF Deng, Muqing
   Fan, Zhuyao
   Lin, Peng
   Feng, Xiaoreng
TI Human Gait Recognition Based on Frontal-View Sequences Using Gait
   Dynamics and Deep Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gait recognition; frontal-view recognition; gait dynamics; deep
   learning; feature fusion
ID WALKING
AB To facilitate human gait recognition, this paper proposes a new frontal-view gait recognition method using gait dynamics and deep learning. Rather than adopting lateral-view parameters as gait features in the literature, we employ improved frontal-view features and classification methods to avoid the recognition rate drops due to the complicated surveillance environment. Specifically, we characterize the binary walking silhouettes with three different kinds of frontal-view gait features, including kinematic features, spatial ratio features and area features. In addition, we capture the gait dynamics underlying the time-varying gait features to reflect temporal dynamics information of human walking. Furthermore, we incorporate the deep feature learning information into the recognition procedure to take advantage of the deep learning technique. To obtain the optimal recognition accuracy and robustness performance against walking condition variations, we calculate the similarity between the appearing test gait dynamics and the trained gait dynamics, and propose an error-based feature fusion scheme for gait recognition. To validate the efficacy of the proposed method, we conduct experiments on published gait databases by comparing with other existing gait recognition methods.
C1 [Deng, Muqing] Guangdong Univ Technol, Sch Automat, Guangzhou 510006, Peoples R China.
   [Fan, Zhuyao; Lin, Peng] Hangzhou Dianzi Univ, Inst Informat & Control, Hangzhou 310005, Peoples R China.
   [Feng, Xiaoreng] Univ Hong Kong, Queen Mary Hosp, Dept Orthopaed & Traumatol, Hong Kong 999077, Peoples R China.
C3 Guangdong University of Technology; Hangzhou Dianzi University;
   University of Hong Kong
RP Deng, MQ (corresponding author), Guangdong Univ Technol, Sch Automat, Guangzhou 510006, Peoples R China.
EM mqdeng@gdut.edu.cn; fanzhuyao@hdu.edu.cn; penglin@hdu.edu.cn;
   fengxr@hku.hk
RI Fan, Zhuyao/HJY-3312-2023
OI fan, zhu yao/0009-0003-0213-5515
FU Natural Science Foundation of Guangdong Province
FX No Statement Available
CR [Anonymous], 2010, the British Machine Vision Conference, DOI DOI 10.1049/IC.2009.0230
   Barnich O, 2009, PATTERN RECOGN LETT, V30, P893, DOI 10.1016/j.patrec.2009.03.014
   Castro FM, 2020, NEURAL COMPUT APPL, V32, P14173, DOI 10.1007/s00521-020-04811-z
   Chattopadhyay Pratik, 2013, Pattern Recognition and Machine Intelligence. 5th International Conference, PReMI 2013. Proceedings: LNCS 8251, P196, DOI 10.1007/978-3-642-45062-4_27
   Chattopadhyay P, 2014, IEEE T INF FOREN SEC, V9, P1843, DOI 10.1109/TIFS.2014.2352114
   Chattopadhyay P, 2014, J VIS COMMUN IMAGE R, V25, P53, DOI 10.1016/j.jvcir.2013.02.010
   Das Choudhury S, 2012, PATTERN RECOGN, V45, P3414, DOI 10.1016/j.patcog.2012.02.032
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng MQ, 2020, J FRANKLIN I, V357, P2471, DOI 10.1016/j.jfranklin.2019.12.041
   Deng MQ, 2019, IEEE T CIRC SYST VID, V29, P3636, DOI 10.1109/TCSVT.2018.2883449
   Deng MQ, 2016, PATTERN RECOGN LETT, V78, P56, DOI 10.1016/j.patrec.2016.04.004
   Goffredo M., 2008, PROC IEEE 2 INT C BI, P1
   Gross R., 2001, The CMU Motion of Body (MoBo) Database
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu MD, 2013, IEEE T CYBERNETICS, V43, P77, DOI 10.1109/TSMCB.2012.2199310
   Jeevan M, 2013, IEEE IMAGE PROC, P4195, DOI 10.1109/ICIP.2013.6738864
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar M. N., 2012, PROC 8 INDIAN C COMP, P1
   Lam THW, 2011, PATTERN RECOGN, V44, P973, DOI 10.1016/j.patcog.2010.10.011
   Liao RJ, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107069
   Liu NN, 2011, IEEE SIGNAL PROC LET, V18, P431, DOI 10.1109/LSP.2011.2157143
   Lu JW, 2007, PATTERN RECOGN LETT, V28, P2401, DOI 10.1016/j.patrec.2007.08.004
   Lu JW, 2010, PATTERN RECOGN LETT, V31, P382, DOI 10.1016/j.patrec.2009.11.006
   Castro FM, 2017, LECT NOTES COMPUT SC, V10306, P257, DOI 10.1007/978-3-319-59147-6_23
   Mao MG, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020), DOI 10.1109/ijcb48548.2020.9304916
   Milovanovic M, 2013, IEEE MULTIMEDIA, V20, P28, DOI 10.1109/MMUL.2013.16
   Rashmi M, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103416
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivapalan Sabesan, 2011, P INT JOINT C BIOM I, P1, DOI 10.1155/2011/375897
   Song CF, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106988
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Veeraraghavan A, 2004, PROC CVPR IEEE, P730
   Wang C., 2009, Deterministic learning theory for identification, recognition, and control
   Wang C, 2012, IEEE T PATTERN ANAL, V34, P2164, DOI 10.1109/TPAMI.2011.260
   Wang C, 2007, IEEE T NEURAL NETWOR, V18, P617, DOI 10.1109/TNN.2006.889496
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xu K, 2021, IEEE T MULTIMEDIA, V24, P3265, DOI 10.1109/TMM.2021.3095809
   Yao LX, 2023, IEEE T MULTIMEDIA, V25, P4187, DOI 10.1109/TMM.2022.3171961
   Yu SQ, 2006, INT C PATT RECOG, P441
   Zeng W, 2015, NEUROCOMPUTING, V152, P139, DOI 10.1016/j.neucom.2014.10.079
   Zeng W, 2014, PATTERN RECOGN, V47, P3568, DOI 10.1016/j.patcog.2014.04.014
   Zhang SX, 2021, PROC CVPR IEEE, P9091, DOI 10.1109/CVPR46437.2021.00898
   Zhang ZY, 2022, IEEE T PATTERN ANAL, V44, P345, DOI 10.1109/TPAMI.2020.2998790
   Zhao AT, 2022, IEEE T MULTIMEDIA, V24, P846, DOI 10.1109/TMM.2021.3060280
   Zulcaffle TMA, 2019, IEEE T INF FOREN SEC, V14, P1067, DOI 10.1109/TIFS.2018.2870594
NR 48
TC 3
Z9 3
U1 45
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 117
EP 126
DI 10.1109/TMM.2023.3262131
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500002
DA 2024-08-05
ER

PT J
AU Du, ZR
   Wei, SK
   Liu, T
   Zhang, SL
   Chen, XT
   Zhang, SY
   Zhao, Y
AF Du, Zhuoran
   Wei, Shikui
   Liu, Ting
   Zhang, Shunli
   Chen, Xiaotong
   Zhang, Shiyin
   Zhao, Yao
TI Exploring the Applicability of Spectral Recovery in Semantic
   Segmentation of RGB Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hyperspectral imaging; Image color analysis; Feature extraction; Data
   mining; spectral recovery; semantic segmentation
ID NETWORK
AB Compared with RGB images, hyperspectral images (HSIs) offer a distinct advantage in that they can record continuous spectral bands of light reflectance in each pixel, reflecting the physical and chemical characteristics of materials. This capability enables differentiation between objects that may have similar textures but different spectral characteristics. It is desirable to recover spectral information from RGB images to improve semantic segmentation accuracy. Additionally, semantic information can serve as a guide for spectral information recovery, thereby ensuring the quality of the recovered spectral information. The two tasks are mutually beneficial in this regard. In light of these considerations, we propose a multi-task framework that exploits the complementary relationship between spectral recovery and semantic segmentation tasks, comprising a complementary spectral-semantic attentive fusion model (CSSF) that enables the two tasks to mutually facilitate each other by fusing information from both branches. Specifically, the proposed CSSF incorporates a window-based spectral-semantic attentive fusion (WSSAF) module to incorporate recovered spectral information into the segmentation process effectively, and a pixel-shuffle-based fusion (PSF) module to provide semantic guidance for spectral recovery. To evaluate the effectiveness of our approach, we built the first flower hyperspectral image dataset (FHRS) with corresponding segmentation annotations and RGB images. By doing so, we have made the first attempt to explore the complementary relationship between semantic segmentation and spectral recovery. Experimental results on both the FHRS dataset and the publicly available LIB-HSI dataset demonstrate that our proposed method has the ability to enhance both tasks by utilizing their complementary relationship, indicating the generalization ability of our method.
C1 [Du, Zhuoran; Wei, Shikui; Chen, Xiaotong; Zhang, Shiyin; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network, Beijing 100044, Peoples R China.
   [Zhang, Shunli] Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.
   [Liu, Ting] Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, Xian 710072, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Northwestern
   Polytechnical University
RP Liu, T (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, Xian 710072, Peoples R China.
EM 20112007@bjtu.edu.cn; shkwei@bjtu.edu.cn; liuting@nwpu.edu.cn;
   slzhang@bjtu.edu.cn; 22110091@bjtu.edu.cn; zhangshiyin@bjtu.edu.cn;
   yzhao@bjtu.edu.cn
OI Liu, Ting/0000-0003-3458-6567; Zhao, Yao/0000-0002-8581-9554; Du,
   Zhuoran/0000-0002-0074-0696
FU National Key Ramp;D Program of China
FX No Statement Available
CR Akhtar N, 2020, IEEE T PATTERN ANAL, V42, P100, DOI 10.1109/TPAMI.2018.2873729
   [Anonymous], 1992, Salina hyperspectral dataset
   [Anonymous], 1992, Pavia university hyperspectral dataset
   [Anonymous], 2002, Indian pines hyperspectral dataset
   Arad B, 2022, IEEE COMPUT SOC CONF, P881, DOI 10.1109/CVPRW56347.2022.00103
   Arad B, 2020, IEEE COMPUT SOC CONF, P1806, DOI 10.1109/CVPRW50498.2020.00231
   Arad B, 2018, IEEE COMPUT SOC CONF, P1042, DOI 10.1109/CVPRW.2018.00138
   Arad B, 2016, LECT NOTES COMPUT SC, V9911, P19, DOI 10.1007/978-3-319-46478-7_2
   Arnold S. E. J., 2010, Ph.D. dissertation
   Audebert N, 2019, IEEE GEOSC REM SEN M, V7, P159, DOI 10.1109/MGRS.2019.2912563
   Baiano A, 2012, COMPUT ELECTRON AGR, V87, P142, DOI 10.1016/j.compag.2012.06.002
   Cao X, 2011, IEEE T PATTERN ANAL, V33, P2423, DOI 10.1109/TPAMI.2011.80
   Chakrabarti A, 2011, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2011.5995660
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chlebda DK, 2016, APPL PHYS A-MATER, V122, DOI 10.1007/s00339-016-0494-9
   Coimbra G, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.558684
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Du B, 2017, IEEE T MULTIMEDIA, V19, P67, DOI 10.1109/TMM.2016.2608780
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Fu Y, 2022, IEEE T PATTERN ANAL, V44, P256, DOI 10.1109/TPAMI.2020.3009999
   Habili N, 2022, EUROPEAN C COMPUTER, V13807, P258
   Hang RL, 2021, IEEE T IMAGE PROCESS, V30, P7256, DOI 10.1109/TIP.2021.3104177
   Heath M., 2022, arXiv
   Li H., 2022, NeurIPS, P11934
   Li JJ, 2020, IEEE COMPUT SOC CONF, P1894, DOI 10.1109/CVPRW50498.2020.00239
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma LF, 2023, IEEE T MULTIMEDIA, V25, P2774, DOI 10.1109/TMM.2022.3151145
   Nguyen RMH, 2014, LECT NOTES COMPUT SC, V8695, P186, DOI 10.1007/978-3-319-10584-0_13
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi Z, 2018, IEEE COMPUT SOC CONF, P1052, DOI 10.1109/CVPRW.2018.00139
   Stiebel T, 2018, IEEE COMPUT SOC CONF, P1061, DOI 10.1109/CVPRW.2018.00140
   Tian Z, 2019, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2019.00324
   Uzair M, 2015, IEEE T IMAGE PROCESS, V24, P1127, DOI 10.1109/TIP.2015.2393057
   Wang Q, 2021, IEEE T MED IMAGING, V40, P218, DOI 10.1109/TMI.2020.3024923
   Wang YC, 2023, PROC CVPR IEEE, P19561, DOI 10.1109/CVPR52729.2023.01874
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Yan LB, 2020, IEEE T COMPUT IMAG, V6, P1070, DOI 10.1109/TCI.2020.3000320
   Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811
   Yin X, 2023, IEEE T MULTIMEDIA, V25, P6146, DOI 10.1109/TMM.2022.3205441
   You SD, 2020, Arxiv, DOI arXiv:1907.10270
   Alom MZ, 2018, Arxiv, DOI [arXiv:1802.06955, DOI 10.48550/ARXIV.1802.06955, 10.48550/arXiv.1802.06955]
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhang L, 2020, AAAI CONF ARTIF INTE, V34, P12821
   Zhao YZ, 2020, IEEE COMPUT SOC CONF, P1695, DOI 10.1109/CVPRW50498.2020.00219
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zheng XT, 2022, IEEE T IMAGE PROCESS, V31, P4251, DOI 10.1109/TIP.2022.3177322
   Zheng YR, 2022, KNOWL-BASED SYST, V236, DOI 10.1016/j.knosys.2021.107647
NR 52
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1932
EP 1943
DI 10.1109/TMM.2023.3290426
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800004
DA 2024-08-05
ER

PT J
AU Fu, ZH
   Zuo, WH
   Hu, ZH
   Liu, QJ
   Wang, YH
AF Fu, Zehua
   Zuo, Wenhang
   Hu, Zhenghui
   Liu, Qingjie
   Wang, Yunhong
TI Improving Multi-Person Pose Tracking With a Confidence Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bbox-revision; keypoint confidence network; multi-person pose tracking;
   pose estimation
ID NEURAL-NETWORKS
AB Human pose estimation and tracking are fundamental tasks for understanding human behaviors in videos. Existing top-down framework-based methods usually perform three-stage tasks: human detection, pose estimation and tracking. Although promising results have been achieved, these methods rely heavily on high-performance detectors and may fail to track persons who are occluded or miss-detected. To overcome these problems, in this article, we develop a novel keypoint confidence network and a tracking pipeline to improve human detection and pose estimation in top-down approaches. Specifically, the keypoint confidence network is designed to determine whether each keypoint is occluded, and it is incorporated into the pose estimation module. In the tracking pipeline, we propose the Bbox-revision module to reduce missing detection and the ID-retrieve module to correct lost trajectories, improving the performance of the detection stage. Experimental results show that our approach is universal in human detection and pose estimation, achieving state-of-the-art performance on both PoseTrack 2017 and 2018 datasets.
C1 [Fu, Zehua; Zuo, Wenhang; Liu, Qingjie; Wang, Yunhong] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Fu, Zehua; Hu, Zhenghui] Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310051, Peoples R China.
C3 Beihang University; Beihang University
RP Liu, QJ (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zehua_fu@buaa.edu.cn; zuowenhang@gmail.com; zhenghuihu2021@buaa.edu.cn;
   qingjie.liu@buaa.edu.cn; yhwang@buaa.edu.cn
RI Zuo, Wenhang/AAW-8928-2020
OI Hu, Zhenghui/0000-0002-6106-0416
FU National Natural Science Foundation of China
FX No Statement Available
CR Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bertasius G, 2019, ADV NEUR IN, V32
   Bulat A, 2020, IEEE INT CONF AUTOMA, P8, DOI 10.1109/FG47880.2020.00014
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen K., 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P4974, DOI DOI 10.1109/CVPR.2019.00511
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen TI, 2023, IEEE T MULTIMEDIA, V25, P291, DOI 10.1109/TMM.2021.3125195
   Chen XJ, 2015, PROC CVPR IEEE, P3945, DOI 10.1109/CVPR.2015.7299020
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng G, 2019, IEEE T IMAGE PROCESS, V28, P265, DOI 10.1109/TIP.2018.2867198
   Chunluan Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P680, DOI 10.1007/978-3-030-58542-6_41
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Gkioxari G, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.458
   Guo HK, 2019, LECT NOTES COMPUT SC, V11130, P209, DOI 10.1007/978-3-030-11012-3_17
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu PY, 2016, PROC CVPR IEEE, P5600, DOI 10.1109/CVPR.2016.604
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Hwang J, 2019, IEEE IJCNN
   Insafutdinov E, 2017, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2017.142
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Iqbal U, 2016, LECT NOTES COMPUT SC, V9914, P627, DOI 10.1007/978-3-319-48881-3_44
   Jin S, 2017, PROC IEEE INT C COMP, V2
   Jin S, 2019, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2019.00581
   Jocher G., 2020, Zenodo
   Kim G, 2023, IEEE T MULTIMEDIA, V25, P5789, DOI 10.1109/TMM.2022.3199098
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Kreiss S, 2022, IEEE T INTELL TRANSP, V23, P13498, DOI 10.1109/TITS.2021.3124981
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Liang TT, 2022, IEEE T IMAGE PROCESS, V31, P6893, DOI 10.1109/TIP.2022.3216771
   Lin T ..., 2023, COCO keypoints evaluation
   Liu H, 2023, IEEE T MULTIMEDIA, V25, P1390, DOI 10.1109/TMM.2022.3141888
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu Z, 2018, IEEE T MULTIMEDIA, V20, P1321, DOI 10.1109/TMM.2017.2767781
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Ning GH, 2020, IEEE COMPUT SOC CONF, P4456, DOI 10.1109/CVPRW50498.2020.00525
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Pishchulin L, 2012, PROC CVPR IEEE, P3178, DOI 10.1109/CVPR.2012.6248052
   Raaj Y, 2019, PROC CVPR IEEE, P4615, DOI 10.1109/CVPR.2019.00475
   Rafi Umer, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P36, DOI 10.1007/978-3-030-58565-5_3
   Rafi U, 2016, PROC BRIT MACH VIS C
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ruan WJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P284, DOI 10.1145/3343031.3350984
   Snower M, 2020, PROC CVPR IEEE, P6737, DOI 10.1109/CVPR42600.2020.00677
   Song J, 2017, PROC CVPR IEEE, P5563, DOI 10.1109/CVPR.2017.590
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Tompson J, 2014, ADV NEUR IN, V27
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang Manchen, 2020, IEEECVF C COMPUTVIS, P11088
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu YP, 2020, IEEE T MULTIMEDIA, V22, P2177, DOI 10.1109/TMM.2019.2953380
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiu Y, 2018, PROC BRIT MACH VIS C
   Xu YF, 2022, ADV NEUR IN
   Yang YD, 2021, PROC CVPR IEEE, P8070, DOI 10.1109/CVPR46437.2021.00798
   Zhang C, 2022, IEEE T MULTIMEDIA, V24, P2246, DOI 10.1109/TMM.2021.3078141
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhu X, 2017, PROC IEEE INT C COMP, V7
   Zou ZX, 2023, P IEEE, V111, P257, DOI 10.1109/JPROC.2023.3238524
NR 70
TC 0
Z9 0
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5223
EP 5233
DI 10.1109/TMM.2023.3330532
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600026
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Hou, GY
   Ou, B
   Long, M
   Peng, F
AF Hou, Gangyang
   Ou, Bo
   Long, Min
   Peng, Fei
TI Separable Reversible Data Hiding for Encrypted 3D Mesh Models Based on
   Octree Subdivision and Multi-MSB Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-MSB prediction; octree spatial subdivision; reversible data hiding
   in encrypted domain; three-dimensional model
ID ERROR EXPANSION
AB Reversible data hiding in encrypted domain (RDH-ED) can perform data encryption to fulfill the privacy protection of original media and embed additional data for covert communication or access control. However, current researches are focusing on the encrypted images, and little attention is paid to encrypted three-dimensional (3D) models. In this article, a high capacity separable RDH-ED method for encrypted 3D models is proposed based on octree spatial subdivision and multiple most significant bit (multi-MSB) prediction. Firstly, a 3D model is adaptively subdivided into non-overlapping subblocks by octree spatial subdivision, and the vertices in a subblock are classified into embedding set and reference set. To better utilize the spatial correlation of the two sets, the multi-MSB prediction error of the embedding set is used to embed the additional data, and the reference set is used to losslessly recover the embedded set. Then, the model is encrypted by a specified encrypted algorithm. At last, additional data is embedded into the reserved embedding room by multi-MSB substitution. Experimental results show that the proposed method can achieve a higher embedding capacity compared with the state-of-the-art methods, and guarantee the lossless recovery of the 3D model.
C1 [Hou, Gangyang; Ou, Bo] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Long, Min] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Peoples R China.
   [Peng, Fei] Guangzhou Univ, Inst Artificial Intelligence & Blockchain, Guangzhou, Peoples R China.
C3 Hunan University; Changsha University of Science & Technology; Guangzhou
   University
RP Ou, B (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.; Peng, F (corresponding author), Guangzhou Univ, Inst Artificial Intelligence & Blockchain, Guangzhou, Peoples R China.
EM hougangyang@hnu.edu.cn; oubo@hnu.edu.cn; caslongm@aliyun.com;
   eepengf@gmail.com
RI Peng, Fei/H-6951-2017
OI Peng, Fei/0000-0001-8053-4587
FU National Natural Science Foundation of China
FX No Statement Available
CR Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Deng H, 2020, IEEE T INF FOREN SEC, V15, P3168, DOI 10.1109/TIFS.2020.2985532
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   He JH, 2019, IEEE T CIRC SYST VID, V29, P3501, DOI 10.1109/TCSVT.2018.2882850
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Huang YH, 2015, 3D RES, V6, DOI 10.1007/s13319-015-0051-x
   Jiang RQ, 2018, MULTIMED TOOLS APPL, V77, P5263, DOI 10.1007/s11042-017-4430-6
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Ke Y, 2022, IEEE T CIRC SYST VID, V32, P2469, DOI 10.1109/TCSVT.2021.3081575
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Lin ZX, 2018, IEEE T INF FOREN SEC, V13, P2372, DOI 10.1109/TIFS.2018.2819122
   Lyu WL, 2022, SIGNAL PROCESS, V201, DOI 10.1016/j.sigpro.2022.108686
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Namasudra S, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3392665
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Peng F, 2023, IEEE T MULTIMEDIA, V25, P892, DOI 10.1109/TMM.2021.3134159
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Puyang Y, 2018, IEEE INT WORKS INFOR
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qiu YQ, 2022, IEEE T CIRC SYST VID, V32, P5874, DOI 10.1109/TCSVT.2022.3163905
   Tang Y, 2023, LECT NOTES COMPUT SC, V13825, P205, DOI 10.1007/978-3-031-25115-3_14
   Tsai YY, 2023, IEEE T DEPEND SECURE, V20, P3508, DOI 10.1109/TDSC.2022.3204291
   Tsai YY, 2021, IEEE T MULTIMEDIA, V23, P2286, DOI 10.1109/TMM.2020.3009492
   Tsai YY, 2016, ADV MULTIMED, V2016, DOI 10.1155/2016/4267419
   van Rensburg BJ, 2021, IEEE IMAGE PROC, P3068, DOI 10.1109/ICIP42928.2021.9506320
   Wang PC, 2007, J INF SCI ENG, V23, P1889
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang XY, 2021, IEEE SIGNAL PROC LET, V28, P1125, DOI 10.1109/LSP.2021.3080181
   Xu N, 2022, COGN COMPUT, V14, P1172, DOI 10.1007/s12559-021-09919-5
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yin ZX, 2021, LECT NOTES COMPUT SC, V13020, P336, DOI 10.1007/978-3-030-88007-1_28
   Zhang QL, 2019, MEASUREMENT, V135, P738, DOI 10.1016/j.measurement.2018.12.016
   Zhang WM, 2014, SIGNAL PROCESS, V94, P118, DOI 10.1016/j.sigpro.2013.06.023
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zheng SL, 2021, IEEE T DEPEND SECURE, V18, P692, DOI 10.1109/TDSC.2019.2913422
NR 38
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2395
EP 2407
DI 10.1109/TMM.2023.3295578
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100012
DA 2024-08-05
ER

PT J
AU Jiang, Y
   Wang, YH
   Li, SQ
   Zhang, YJ
   Zhao, MH
   Gao, Y
AF Jiang, Yu
   Wang, Yuehang
   Li, Siqi
   Zhang, Yongji
   Zhao, Minghao
   Gao, Yue
TI Event-Based Low-Illumination Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event camera; low-light; image enhancement; transformer
ID HISTOGRAM EQUALIZATION; CONTRAST; RETINEX
AB Event cameras are bio-inspired vision sensors with a high dynamic range (140 dB for event cameras vs. 60 dB for traditional cameras) and can be used to tackle the image degradation problem under extremely low-illumination scenarios, which is still not well-explored yet. In this article, we propose a joint framework to compose the underexposed frames and event streams captured by the event camera to reconstruct clear images with detailed textures under almost dark conditions. A residual fusion module is proposed to reduce the domain gap between event streams and frames by using the residuals of both modalities. A multi-level reconstruction loss based on the variability of the contrast distribution is proposed to reduce the perceptual errors of the output image. In addition, we construct the first real-world low-illumination image enhancement dataset (mainly under 2 lux illumination scenes), named LIE, containing event streams and frames collected under indoor and outdoor low-light scenarios together with the ground truth clear images. Experimental results on our LIE dataset demonstrate that our proposed method could achieve significant improvements compared with existing methods.
C1 [Jiang, Yu; Wang, Yuehang; Zhang, Yongji] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
   [Jiang, Yu; Wang, Yuehang; Zhang, Yongji] Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.
   [Li, Siqi; Gao, Yue] Tsinghua Univ, Sch Software, BNRist, KLISS,THUIBCS,BLBCI, Beijing 100084, Peoples R China.
   [Zhao, Minghao] Jilin Univ, Coll Earth Sci, Changchun, Peoples R China.
C3 Jilin University; Jilin University; Tsinghua University; Jilin
   University
RP Gao, Y (corresponding author), Tsinghua Univ, Sch Software, BNRist, KLISS,THUIBCS,BLBCI, Beijing 100084, Peoples R China.
EM jiangyu2011@jlu.edu.cn; yuehang22@mails.jlu.edu.cn;
   lsq19@mails.tsinghua.edu.cn; zhangyongji1998@gmail.com;
   zhaomh20@mails.jlu.edu.cn; gaoyue@tsinghua.edu.cn
RI Li, siqi/KDN-4520-2024; Wang, Yuehang/KIG-2756-2024
OI Wang, Yuehang/0000-0001-6793-6506; Zhang, Yongji/0000-0002-8576-5285;
   Li, Siqi/0000-0001-9720-826X
FU National Natural Science Foundation of China
FX No Statement Available
CR Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Carion N., 2020, EUR C COMP VIS, P213
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen HY, 2023, IEEE T MULTIMEDIA, V25, P5826, DOI 10.1109/TMM.2022.3199556
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Cadena PRG, 2021, IEEE T IMAGE PROCESS, V30, P2488, DOI 10.1109/TIP.2021.3052070
   Gu SH, 2019, IEEE I CONF COMP VIS, P2511, DOI 10.1109/ICCV.2019.00260
   Huang J, 2023, IEEE T MULTIMEDIA, V25, P2978, DOI 10.1109/TMM.2022.3154152
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jiang Z, 2020, PROC CVPR IEEE, P3317, DOI 10.1109/CVPR42600.2020.00338
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kingma D.P., 2014, Proc. of ICLR
   Lamba M, 2021, PROC CVPR IEEE, P3486, DOI 10.1109/CVPR46437.2021.00349
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li JN, 2022, IEEE T IMAGE PROCESS, V31, P2975, DOI 10.1109/TIP.2022.3162962
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Limeng Zhang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P352, DOI 10.1007/978-3-030-67832-6_29
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Ma L, 2023, IEEE T MULTIMEDIA, V25, P3573, DOI 10.1109/TMM.2022.3162493
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Maharjan P, 2019, IEEE INT CON MULTI, P916, DOI 10.1109/ICME.2019.00162
   Mostafavi ISM, 2020, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR42600.2020.00284
   Paszke A, 2019, ADV NEUR IN, V32
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P666, DOI 10.1007/978-3-030-58523-5_39
   Sun ZN, 2022, LECT NOTES COMPUT SC, V13694, P341, DOI 10.1007/978-3-031-19830-4_20
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang WJ, 2023, IEEE T PATTERN ANAL, V45, P1250, DOI 10.1109/TPAMI.2022.3152562
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZHW, 2020, PROC CVPR IEEE, P1606, DOI 10.1109/CVPR42600.2020.00168
   Wu XY, 2021, PROC CVPR IEEE, P15764, DOI 10.1109/CVPR46437.2021.01551
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang JQ, 2022, PROC CVPR IEEE, P8791, DOI 10.1109/CVPR52688.2022.00860
   Zhang JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13023, DOI 10.1109/ICCV48922.2021.01280
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou C, 2021, IEEE INT CONF COMP V, P1155, DOI 10.1109/ICCVW54120.2021.00135
   Zou YH, 2021, PROC CVPR IEEE, P2024, DOI 10.1109/CVPR46437.2021.00206
NR 54
TC 4
Z9 4
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1920
EP 1931
DI 10.1109/TMM.2023.3290432
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800017
DA 2024-08-05
ER

PT J
AU Jin, L
   Wang, XJ
   Nie, XC
   Wang, WD
   Guo, YD
   Yan, SC
   Zhao, J
AF Jin, Lei
   Wang, Xiaojuan
   Nie, Xuecheng
   Wang, Wendong
   Guo, Yandong
   Yan, Shuicheng
   Zhao, Jian
TI Rethinking the Person Localization for Single-Stage Multi-Person Pose
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; multi-person pose estimation; single-stage
ID NETWORK
AB Single-stage models for multi-person pose estimation have garnered significant attention due to their streamlined approach in generating person position localization and body structure perception in a single pass. These two parts, however, are processed individually by existing methods, leading to suboptimal results, e.g., candidates with high confidences for person localization while poor structure estimations. To this end, we propose a simple yet effective approach, namely Structure-guided Person Localization (SPL), jointly leveraging the advantages of the two aspects to solve the multi-person pose estimation problem, with two complementary novelties. First, we propose to incorporate body structure perception to guide person position localization, consequently, we introduce the Structure-guided Center Learning (SCL) to unify the quality of the body structure perception in the displacement map with the confidence of the person existence in the center map, thus achieving more accurate keypoint position localization results even with extreme poses. Second, to facilitate the end-to-end training of SPL, we propose the efficient Agency-based Scale-adaptive Learning (ASL). Specifically, we predict an agency map of the same size as the center map, which focuses on the foreground area and can adaptively adjust the scale size for each central area with the body structure perception confidence. Comprehensive experiments on challenging benchmarks including COCO and CrowdPose clearly verify the superiority of our framework, which achieves new state-of-the-art single-stage multi-person pose estimation results. Specifically, SPL obtains 72.1 AP scores and 69.5 AP scores in COCO test-dev2017 and CrowdPose test set, respectively.
C1 [Jin, Lei; Wang, Xiaojuan] Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing 100876, Peoples R China.
   [Nie, Xuecheng] Meitu, Beijing 100124, Peoples R China.
   [Wang, Wendong] Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100876, Peoples R China.
   [Guo, Yandong] OPPO, Dongguan 523068, Peoples R China.
   [Yan, Shuicheng] Sea AI Lab SAIL, Singapore 138522, Singapore.
   [Zhao, Jian] Inst North Elect Equipment, Beijing 560100, Peoples R China.
   [Zhao, Jian] Peng Cheng Lab, Dept Math & Theories, Shenzhen 518066, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications; Peng Cheng Laboratory
RP Wang, WD (corresponding author), Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100876, Peoples R China.; Zhao, J (corresponding author), Inst North Elect Equipment, Beijing 560100, Peoples R China.
EM jinlei@bupt.edu.cn; wj2718@bupt.edu.cn; nxc@meitu.com;
   wdwang@bupt.edu.cn; yandong.guo@live.com; yansc@sea.com;
   zhaojian90@u.nus.edu
RI Yan, Shuicheng/HCI-1431-2022
OI Jin, Lei/0000-0003-4855-2464; Nie, Xuecheng/0000-0003-2433-5983; Zhao,
   Jian/0000-0002-3508-756X; Wang, Wendong/0000-0002-6418-8087
FU National Nature
FX No Statement Available
CR Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Fan XC, 2015, PROC CVPR IEEE, P1347, DOI 10.1109/CVPR.2015.7298740
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fangyun Wei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P527, DOI 10.1007/978-3-030-58607-2_31
   Fieraru M, 2018, IEEE COMPUT SOC CONF, P318, DOI 10.1109/CVPRW.2018.00058
   Geng ZG, 2021, PROC CVPR IEEE, P14671, DOI 10.1109/CVPR46437.2021.01444
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gkioxari G, 2016, LECT NOTES COMPUT SC, V9908, P728, DOI 10.1007/978-3-319-46493-0_44
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329
   Jian Z., 2017, P BRIT MACH VIS C, P1
   Jin L, 2023, IEEE T MULTIMEDIA, V25, P3364, DOI 10.1109/TMM.2022.3159111
   Jin L, 2022, PROC CVPR IEEE, P13076, DOI 10.1109/CVPR52688.2022.01274
   Jin Sheng, 2020, EUR C COMP VIS, P718, DOI DOI 10.1007/978-3-030-58571-642
   Kingma D. P., 2014, arXiv
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   [李家正 Li Jiazheng], 2020, [长江科学院院报, Journal of Yangtze River Scientific Research Institute], V37, P1
   Li JF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11005, DOI 10.1109/ICCV48922.2021.01084
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Lifshitz I, 2016, LECT NOTES COMPUT SC, V9906, P246, DOI 10.1007/978-3-319-46475-6_16
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HL, 2016, IEEE T MULTIMEDIA, V18, P1233, DOI 10.1109/TMM.2016.2556859
   Liu WT, 2018, AAAI CONF ARTIF INTE, P7170
   Luo ZX, 2021, PROC CVPR IEEE, P13259, DOI 10.1109/CVPR46437.2021.01306
   Moon G, 2019, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2019.00796
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P705, DOI 10.1007/978-3-030-01228-1_42
   Nie XC, 2019, IEEE I CONF COMP VIS, P6950, DOI 10.1109/ICCV.2019.00705
   Nie XC, 2018, PROC CVPR IEEE, P2100, DOI 10.1109/CVPR.2018.00224
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Peng X, 2018, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2018.00237
   Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82
   Qin YL, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108791
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronchi MR, 2017, IEEE I CONF COMP VIS, P369, DOI 10.1109/ICCV.2017.48
   Sekii T, 2018, LECT NOTES COMPUT SC, V11217, P350, DOI 10.1007/978-3-030-01261-8_21
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun K, 2017, IEEE I CONF COMP VIS, P5600, DOI 10.1109/ICCV.2017.597
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Tian Z, 2019, Arxiv, DOI arXiv:1911.07451
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wen H, 2023, PROC CVPR IEEE, P8937, DOI 10.1109/CVPR52729.2023.00863
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiao YB, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12040857
   Xiao YB, 2022, AAAI CONF ARTIF INTE, P2822
   Xiao YB, 2022, AAAI CONF ARTIF INTE, P2813
   Xiong H., 2022, ACM Trans. Knowl. Discov. Data, V16, P1
   Xue N, 2022, PROC CVPR IEEE, P13055, DOI 10.1109/CVPR52688.2022.01272
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Yanrui Bin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P606, DOI 10.1007/978-3-030-58529-7_36
   Zhang JF, 2021, PROC CVPR IEEE, P546, DOI 10.1109/CVPR46437.2021.00061
   Zhao J, 2020, INT J COMPUT VISION, V128, P2185, DOI 10.1007/s11263-019-01181-5
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
   Zhao J, 2017, IEEE COMPUT SOC CONF, P1595, DOI 10.1109/CVPRW.2017.204
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
NR 68
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1436
EP 1447
DI 10.1109/TMM.2023.3282139
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700046
DA 2024-08-05
ER

PT J
AU Li, H
   Liang, JY
   Wu, RQ
   Cong, RM
   Wu, WH
   Kwong, STW
AF Li, Hua
   Liang, Junyan
   Wu, Ruiqi
   Cong, Runmin
   Wu, Wenhui
   Kwong, Sam Tak Wu
TI Stereo Superpixel Segmentation via Decoupled Dynamic Spatial-Embedding
   Fusion Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereo image; superpixel segmentation; stereo corresponding capturing;
   spatiality embedding
ID SALIENCY DETECTION; FRAMEWORK
AB Stereo superpixel segmentation aims at grouping the discretizing pixels into perceptual regions through left and right views more collaboratively and efficiently. Existing superpixel segmentation algorithms mostly utilize color and spatial features as input, which may impose strong constraints on spatial information while utilizing the disparity information in terms of stereo image pairs. To alleviate this issue, we propose a stereo superpixel segmentation method with a decoupling mechanism of spatial information in this work. To decouple stereo disparity information and spatial information, the spatial information is temporarily removed before fusing the features of stereo image pairs, and a decoupled stereo fusion module (DSFM) is designed to handle the stereo features alignment as well as occlusion problems. Moreover, since the spatial information is vital to superpixel segmentation, we further design a dynamic spatiality embedding module (DSEM) to re-add spatial information, and the weights of spatial information will be adaptively adjusted through the dynamic fusion (DF) mechanism in DSEM for achieving a finer segmentation. Comprehensive experimental results demonstrate that our method can achieve the state-of-the-art performance on the KITTI2015 and Cityscapes datasets, and also verify the efficiency when applied in salient object detection on NJU2K dataset. The source code will be available publicly after paper is accepted.
C1 [Li, Hua; Liang, Junyan] Hainan Univ, Sch Comp Sci & Technol, Haikou 570228, Peoples R China.
   [Wu, Ruiqi] Wuhan Univ Technol, Sch Comp Sci & Technol, Wuhan 430070, Peoples R China.
   [Cong, Runmin] Shandong Univ, Sch Control Sci & Engn, Jinan 250100, Peoples R China.
   [Wu, Wenhui] Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen 518060, Peoples R China.
   [Wu, Wenhui] Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Kwong, Sam Tak Wu] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
C3 Hainan University; Wuhan University of Technology; Shandong University;
   Shenzhen University; City University of Hong Kong; Shenzhen Research
   Institute, City University of Hong Kong; City University of Hong Kong
RP Cong, RM (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250100, Peoples R China.
EM huali27-c@my.cityu.edu.hk; liangjunyan@hainanu.edu.cn;
   wuruiqi0722@gmail.com; rmcong@sdu.edu.cn; wuwenhui@szu.edu.cn;
   cssamk@cityu.edu.hk
RI Elango, Jeevithan/Y-8106-2019; wu, ruiqi/GSI-4102-2022; Kwong,
   Sam/C-9319-2012
OI Elango, Jeevithan/0000-0003-4115-9918; Kwong, Sam/0000-0001-7484-7261;
   LI, Hua/0000-0003-0740-0691
FU Hainan Provincial Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2017, PROC CVPR IEEE, P4895, DOI 10.1109/CVPR.2017.520
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2016, INPROC INT JOINT C A
   Chen JS, 2017, IEEE T IMAGE PROCESS, V26, P3317, DOI 10.1109/TIP.2017.2651389
   Chen ZD, 2023, IEEE T PATTERN ANAL, V45, P5158, DOI 10.1109/TPAMI.2022.3195759
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng FY, 2015, PATTERN RECOGN, V48, P2269, DOI 10.1016/j.patcog.2015.01.002
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Cong RM, 2019, IEEE T IMAGE PROCESS, V28, P4819, DOI 10.1109/TIP.2019.2910377
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding YY, 2011, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2011.5995445
   Dong X., 2020, IEEE CVF C COMP VIS, DOI [DOI 10.1109/CVPR42600.2020.01291, 10.1109/CVPR42600.2020.01291]
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fengting Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13961, DOI 10.1109/CVPR42600.2020.01398
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jampani V, 2018, LECT NOTES COMPUT SC, V11211, P363, DOI 10.1007/978-3-030-01234-2_22
   Ju R, 2015, SIGNAL PROCESS-IMAGE, V38, P115, DOI 10.1016/j.image.2015.07.002
   Li H, 2021, INFORM SCIENCES, V556, P209, DOI 10.1016/j.ins.2020.12.031
   Li H, 2019, IEEE T MULTIMEDIA, V21, P2625, DOI 10.1109/TMM.2019.2907047
   Liu LM, 2016, INFORM SCIENCES, V372, P72, DOI 10.1016/j.ins.2016.08.029
   Menze M, 2015, ISPRS ANN PHOTO REM, VII-3, P427, DOI 10.5194/isprsannals-II-3-W5-427-2015
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Shi C, 2020, IEEE T MULTIMEDIA, V22, P487, DOI 10.1109/TMM.2019.2928491
   Sun J, 2011, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2011.6126409
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang LG, 2019, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR.2019.01253
   Wang MR, 2017, SIGNAL PROCESS-IMAGE, V56, P28, DOI 10.1016/j.image.2017.04.007
   Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y. Du, 2021, IEEE INT C MULTIMEDI, P1
   Xu MZ, 2019, IEEE T MULTIMEDIA, V21, P2790, DOI 10.1109/TMM.2019.2914889
   Yang MM, 2018, IEEE T MULTIMEDIA, V20, P3008, DOI 10.1109/TMM.2018.2820327
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 37
TC 2
Z9 2
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 367
EP 378
DI 10.1109/TMM.2023.3265843
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000021
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, RM
   Xiang, JJ
   Sun, FX
   Yuan, Y
   Yuan, LW
   Gou, SP
AF Li, Ruimin
   Xiang, Jiajun
   Sun, Feixiang
   Yuan, Ye
   Yuan, Longwu
   Gou, Shuiping
TI Multiscale Cross-Modal Homogeneity Enhancement and Confidence-Aware
   Fusion for Multispectral Pedestrian Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE confidence transfer loss; cross-modal homogeneity enhancement;
   multispectral pedestrian detection
AB Multispectral pedestrian detection has shown many advantages in a variety of environments, particularly poor illumination conditions, by leveraging visible-thermal modalities. However, in-depth insight into distinguishing the complementary content of multimodal data and exploring the extent of multimodal feature fusion is still lacking. In this paper, we propose a novel multispectral pedestrian detector with multiscale cross-modal homogeneity enhancement and confidence-aware feature fusion. RGB and thermal streams are constructed to extract features and generate candidate proposals. During feature extraction, multiscale cross-modal homogeneity enhancement is proposed to enhance single-modal features using the separated homogeneous features via modal interactions. Homogeneity features encode the semantic information of the scene and are extracted from the RGB-thermal pairs by employing a channel attention mechanism. Proposals from two modalities are united to obtain multimodal proposals. Then, confidence measurement fusion is proposed to achieve multispectral feature fusion in each proposal by measuring the internal confidence of each modality and the interaction confidence between modalities. In addition, a confidence transfer loss function is designed to focus more on hard-to-detect samples during training. Experimental results on two challenging datasets demonstrate that the proposed method achieves better performance compared to existing methods.
C1 [Li, Ruimin] Xidian Univ, Acad Adv Interdisciplinary Res, Xian 710071, Peoples R China.
   [Xiang, Jiajun; Gou, Shuiping] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Xian 710071, Peoples R China.
   [Sun, Feixiang; Yuan, Ye; Yuan, Longwu] China Gen Nucl Power Grp, Shenzhen 518026, Peoples R China.
C3 Xidian University; Xidian University; China General Nuclear Power Group
RP Gou, SP (corresponding author), Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Xian 710071, Peoples R China.
EM rmli@xidian.edu.cn; jjxiang@stu.xidian.edu.cn; sunfeixiang@cgnpc.com.cn;
   yuanye@cgnpc.com.cn; yuanlongwu@cgnpc.com.cn; shpgou@mail.xidian.edu.cn
RI LI, RUIMIN/AAG-6616-2019
OI LI, RUIMIN/0000-0003-2393-2225
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2016, P BRIT MACH VIS C
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Brazil G, 2017, IEEE I CONF COMP VIS, P4960, DOI 10.1109/ICCV.2017.530
   Choi H, 2016, INT C PATT RECOG, P621, DOI 10.1109/ICPR.2016.7899703
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Feng TT, 2020, 2020 5TH INTERNATIONAL CONFERENCE ON COMMUNICATION, IMAGE AND SIGNAL PROCESSING (CCISP 2020), P145, DOI [10.1109/ccisp51026.2020.9273507, 10.1109/CCISP51026.2020.9273507]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   González A, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060820
   Guan DY, 2019, INFORM FUSION, V50, P148, DOI 10.1016/j.inffus.2018.11.017
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Jingchao P., 2023, Int. J. Mach. Learn. Cybern., P1
   Kailai Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P787, DOI 10.1007/978-3-030-58523-5_46
   Kim J, 2021, IEEE ROBOT AUTOM LET, V6, P7846, DOI 10.1109/LRA.2021.3099870
   Koenig D, 2017, IEEE COMPUT SOC CONF, P243, DOI 10.1109/CVPRW.2017.36
   Lei Pang, 2019, 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2902, DOI 10.1109/ROBIO49542.2019.8961523
   Li C., 2018, BRIT MACH VIS C BMVC, P1
   Li CY, 2019, PATTERN RECOGN, V85, P161, DOI 10.1016/j.patcog.2018.08.005
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Ngiam A., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Ophoff T., 2018, P IEEE 15 INT C ADV, P1
   Park K, 2018, PATTERN RECOGN, V80, P143, DOI 10.1016/j.patcog.2018.03.007
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143
   Wanchaitanawong N, 2021, PROCEEDINGS OF 17TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA 2021), DOI 10.23919/MVA51890.2021.9511366
   Xiang JJ, 2022, INT GEOSCI REMOTE SE, P3532, DOI 10.1109/IGARSS46834.2022.9883131
   Zhang H, 2021, IEEE WINT CONF APPL, P72, DOI 10.1109/WACV48630.2021.00012
   Zhang H, 2020, IEEE IMAGE PROC, P276, DOI [10.1109/ICIP40778.2020.9191080, 10.1109/icip40778.2020.9191080]
   Zhang L, 2019, IEEE I CONF COMP VIS, P5126, DOI 10.1109/ICCV.2019.00523
   Zhang L, 2019, INFORM FUSION, V50, P20, DOI 10.1016/j.inffus.2018.09.015
NR 31
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 852
EP 863
DI 10.1109/TMM.2023.3272471
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700003
DA 2024-08-05
ER

PT J
AU Liu, H
   Wu, JJ
   Li, F
   Jiang, JG
   Hong, RC
AF Liu, Hao
   Wu, Jingjing
   Li, Feng
   Jiang, Jianguo
   Hong, Richang
TI SYRER: Synergistic Relational Reasoning for RGB-D Cross-Modal
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Pedestrians; Task analysis; Transformers;
   Correlation; Cameras; Cognition; Cross-modal relational reasoning;
   point-wise depth extractor; relation contrast learning; RGB-D
   cross-modal person re-identification
ID PERSON REIDENTIFICATION
AB RGB-D cross-modal person re-identification is designed to match the people across the RGB and depth image modalities, where the large modality discrepancy makes this task intractable to tackle. To alleviate the negative effect brought by the discrepancy, this paper proposes a novel SYnergistic RElational Reasoning (SYRER) method, which targets at exploring the synergy between hetero-modalities for recognizing persons. We design a heterogeneous relationship contrast branch to establish intra-class and inter-class cross-modal relationships, which implements the cross-modal relation contrast learning to cope with imperceptible cross-modal inter-class differences and large cross-modal intra-class discrepancy. Additionally, in order to adequately represent the irregular depth images, we propose a point-wise depth extractor to extract non-uniform discriminative point features from depth images. Experimental results on two public datasets indicate the proposed SYRER surpasses the state-of-the-arts. And we also perform a series of analytic experiments to verify the effectiveness of each submodule of our SYRER.
C1 [Liu, Hao] Tencent Youtu Lab, Shenzhen 518057, Guangdong, Peoples R China.
   [Wu, Jingjing; Li, Feng; Jiang, Jianguo; Hong, Richang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Anhui, Peoples R China.
C3 Tencent; Hefei University of Technology
RP Wu, JJ (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Anhui, Peoples R China.
EM hfut.haoliu@gmail.com; hfutwujingjing@mail.hfut.edu.cn;
   fengli@hfut.edu.cn; jgjiang@hfut.edu.cn; hongrc@hfut.edu.cn
OI Li, Feng/0000-0001-9862-0432; Wu, Jingjing/0000-0002-3818-4277
FU National Natural Science Foundation of China
FX No Statement Available
CR Carion N., 2020, EUR C COMP VIS, P213
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan X, 2019, J VIS COMMUN IMAGE R, V60, P51, DOI 10.1016/j.jvcir.2019.01.010
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   Hafner F. M., 2019, P 16 IEEE INT C ADV, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu WP, 2022, IEEE T CIRC SYST VID, V32, P5095, DOI 10.1109/TCSVT.2022.3147813
   Huang NC, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109145
   Huang Y, 2022, IEEE T MULTIMEDIA, V24, P1570, DOI 10.1109/TMM.2021.3067760
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   John V, 2013, IEEE IMAGE PROC, P3345, DOI 10.1109/ICIP.2013.6738689
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Li XY, 2022, IEEE T COGN DEV SYST, V14, P246, DOI 10.1109/TCDS.2020.3048883
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu H, 2017, IEEE I CONF COMP VIS, P493, DOI 10.1109/ICCV.2017.61
   Liu H, 2017, CAAI T INTELL TECHNO, V2, P48, DOI 10.1016/j.trit.2017.04.001
   Liu JL, 2022, PROC CVPR IEEE, P19344, DOI 10.1109/CVPR52688.2022.01876
   Liu JA, 2023, COMPUT VIS IMAGE UND, V232, DOI 10.1016/j.cviu.2023.103708
   Liu JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3450, DOI 10.1145/3394171.3413878
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Mogelmose A, 2013, I W BIOMETRIC FORENS
   Munaro M, 2014, ADV COMPUT VIS PATT, P161, DOI 10.1007/978-1-4471-6296-4_8
   Pala F, 2016, IEEE T CIRC SYST VID, V26, P788, DOI 10.1109/TCSVT.2015.2424056
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CW, 2019, IEEE I C EMBED SOFTW, DOI 10.1109/icess.2019.8782483
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P225, DOI 10.1109/ICCV48922.2021.00029
   Weng XS, 2020, PROC CVPR IEEE, P6498, DOI 10.1109/CVPR42600.2020.00653
   Wojke N, 2018, IEEE WINT CONF APPL, P748, DOI 10.1109/WACV.2018.00087
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu J. Jiang, 2022, ACM Trans. Multimedia Comput., Commun. Appl., V18, P1
   Wu L, 2016, Arxiv, DOI arXiv:1601.07255
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu XX, 2015, IEEE T NEUR NET LEAR, V26, P3150, DOI 10.1109/TNNLS.2015.2405574
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Zhang L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473341
   Zhang P, 2020, IEEE T CIRC SYST VID, V30, P4554, DOI 10.1109/TCSVT.2019.2939564
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhao ZH, 2022, IEEE SENS J, V22, P989, DOI 10.1109/JSEN.2021.3130181
   Zhonghao Luo, 2021, 2021 2nd International Conference on Computing and Data Science (CDS), P265, DOI 10.1109/CDS52072.2021.00053
   Zhuo JX, 2017, COMM COM INF SC, V773, P280, DOI 10.1007/978-981-10-7305-2_25
NR 66
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5600
EP 5614
DI 10.1109/TMM.2023.3338058
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600008
DA 2024-08-05
ER

PT J
AU Liu, XZ
   Liu, KP
   Guo, JF
   Zhao, PP
   Quan, YN
   Miao, QG
AF Liu, Xiangzeng
   Liu, Kunpeng
   Guo, Jianfeng
   Zhao, Peipei
   Quan, Yining
   Miao, Qiguang
TI Pose-Guided Attention Learning for Cloth-Changing Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Clothing; Shape; Task analysis; Pose estimation;
   Finite element analysis; Faces; Person re-identification;
   cloth-changing; human pose estimation; feature enhancement;
   multi-granularity
AB The change in appearance is a great challenge for cloth-changing person re-identification. Existing methods tackle this challenge by learning the shape features of the human body, however, these features are easily affected by the human pose or camera perspective. Thus, the ability to learn and extract the invariant features of person in varying conditions is crucial to overcome the above challenge. To address the issue of invariant feature extraction for cloth-changing person re-identification, a Pose-Guided Attention Learning (PGAL) framework is proposed in this paper. First, we introduce the human pose estimation network to remove the background effects and align the fine-grained key points features of human body. Then, to fully exploit the available appearance information, we develop a Feature Enhancement Module (FEM) that improves the feature representation of non-key point regions of human body through the Multi-Head Self Attention. Finally, in order to adaptively learn the invariant features of the person, we construct an Attention Learning Module (ALM) to achieve automatic selection of multi-granularity features by utilizing three different loss functions. Comparing with current popular methods on four cloth-changing person Re-ID datasets, the experimental results show the superiority of our method.
C1 [Liu, Xiangzeng; Liu, Kunpeng; Zhao, Peipei; Quan, Yining; Miao, Qiguang] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
   [Guo, Jianfeng] Xidian Univ, Guangzhou Inst Technol, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University
RP Liu, XZ (corresponding author), Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
EM xzliu@xidian.edu.cn; kunpengliu@stu.xidian.edu.cn;
   jianfengguo@stu.xidian.edu.cn; peipeizhao@stu.xidian.edu.cn;
   ynquan@xidian.edu.cn; qgmiao@xidian.edu.cn
OI JianFeng, Guo/0009-0000-5723-6768; Zhao, Peipei/0000-0002-2275-2495
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR Chen JX, 2021, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR46437.2021.00805
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Feng JW, 2023, PROC CVPR IEEE, P22752, DOI 10.1109/CVPR52729.2023.02179
   Feng YJ, 2023, IEEE T MULTIMEDIA, V25, P1401, DOI 10.1109/TMM.2022.3229969
   Gao Z, 2021, IEEE T MULTIMEDIA, V23, P3332, DOI 10.1109/TMM.2020.3023784
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   Han K., 2023, CVPR, P22066
   He Lingxiao, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P9664, DOI 10.1145/3581783.3613460
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   He ZJ, 2023, CHINESE J AERONAUT, V36, P447, DOI 10.1016/j.cja.2022.11.017
   Hong PX, 2021, PROC CVPR IEEE, P10508, DOI 10.1109/CVPR46437.2021.01037
   Huang Q., 2019, IEEEINT JOINT C NEUR, P1
   Huang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11875, DOI 10.1109/ICCV48922.2021.01168
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Jin X., 2022, P IEEE CVF C COMP VI, P14278
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li WJ, 2023, PROC CVPR IEEE, P13824, DOI 10.1109/CVPR52729.2023.01328
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Qian X, 2020, PROCASIAN C COMPUT V
   Sabour S, 2017, ADV NEUR IN, V30
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Vaswani A., 2017, C WORKSHOP NEURAL IN, P6000
   Wan FB, 2020, IEEE COMPUT SOC CONF, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xue J, 2018, IEEE COMPUT SOC CONF, P2193, DOI 10.1109/CVPRW.2018.00285
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Yang ZW, 2023, PROC CVPR IEEE, P1472, DOI 10.1109/CVPR52729.2023.00148
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu SJ, 2020, PROC CVPR IEEE, P3397, DOI 10.1109/CVPR42600.2020.00346
   Yuan Y, 2020, IEEE COMPUT SOC CONF, P1454, DOI 10.1109/CVPRW50498.2020.00185
   Zajdel W, 2005, IEEE INT CONF ROBOT, P2081
   Zhang P, 2018, IEEE WINT CONF APPL, P494, DOI 10.1109/WACV.2018.00060
   Zhang YK, 2023, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR52729.2023.00214
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhedong Zheng, 2017, ACM Transactions on Multimedia Computing, Communications and Applications, V14, DOI 10.1145/3159171
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   ZHU K, 2020, ECCV, DOI DOI 10.1007/978-3-030-58580-8_21
NR 45
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5490
EP 5498
DI 10.1109/TMM.2023.3334975
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600010
DA 2024-08-05
ER

PT J
AU Liu, YB
   Wang, JH
   Wang, WJ
   Hu, Y
   Wang, YW
   Xu, Y
AF Liu, Yabo
   Wang, Jinghua
   Wang, Weijia
   Hu, Yu
   Wang, Yaowei
   Xu, Yong
TI CRADA: Cross Domain Object Detection With Cyclic Reconstruction and
   Decoupling Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Confidence-guide graph completion; cyclic adaptation; domain adaptive
   object detection; feature decoupling
AB Unsupervised domain adaptive object detection (UDA-OD) is a challenging task that aims to improve the generalization of detectors across domains. Although the existing UDA-OD methods have demonstrated their capabilities, they fail to investigate two critical correlations in the adaptation procedure, i.e., 1) the correlation between the features inside an image and 2) the correlation between the domain-invariant and domain-specific features across domains. To take full advantage of these two correlations, we propose a Cyclic Reconstruction and Decoupling Adaptation (CRADA) framework to efficiently decouple and align the features from different domains. Our CRADA builds graphs for images to capture the correlation between the informative points, and decouples it into two components, one for the domain-specific features and the other for the domain-invariant features. To enhance the qualities of the decoupled features, we also propose a cyclic decoupling-reconstruction-decoupling strategy and a swap-and-reconstruction procedure for the decoupled features of different domains. To make the training procedure easier, we introduce a confidence-guided update scheme for the memory bank and overcome the problem of asymmetric categories in each training batch. We conduct comprehensive experiments to verify the effectiveness of our proposed CRADA.
C1 [Liu, Yabo; Xu, Yong] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Liu, Yabo; Wang, Yaowei; Xu, Yong] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Wang, Jinghua; Wang, Weijia; Hu, Yu] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Harbin Institute
   of Technology
RP Xu, Y (corresponding author), Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.; Wang, JH (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM yaboliu.ug@gmail.com; wangjh2012@foxmail.com; 21s051039@stu.hit.edu.cn;
   21s15-1155@stu.hit.edu.cn; wangyw@pcl.ac.cn; yongxu@ymail.com
OI Wang, Jinghua/0000-0002-2629-1198
FU National Natural Science Foundation of China
FX No Statement Available
CR Chang-Dong Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11721, DOI 10.1109/CVPR42600.2020.01174
   Chen C., 2020, P IEEECVF C COMPUTER, P8869, DOI DOI 10.1109/CVPR42600.2020.00889
   Chen CQ, 2023, IEEE T PATTERN ANAL, V45, P3677, DOI 10.1109/TPAMI.2022.3179445
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Chen TI, 2023, IEEE T MULTIMEDIA, V25, P291, DOI 10.1109/TMM.2021.3125195
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Cheng-Chun Hsu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P733, DOI 10.1007/978-3-030-58545-7_42
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JH, 2021, PROC CVPR IEEE, P4089, DOI 10.1109/CVPR46437.2021.00408
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He MZ, 2022, PROC CVPR IEEE, P9560, DOI 10.1109/CVPR52688.2022.00935
   He Z., 2020, P EUR C COMP VIS, P403
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Jiang J., 2022, P INT C LEARN REPR
   Khodabandeh M, 2019, IEEE I CONF COMP VIS, P480, DOI 10.1109/ICCV.2019.00057
   Kim T, 2019, PROC CVPR IEEE, P12448, DOI 10.1109/CVPR.2019.01274
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li W., 2022, P IEEECVF C COMPUTER, P5291
   Li WY, 2022, AAAI CONF ARTIF INTE, P1421
   Lin C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8751, DOI 10.1109/ICCV48922.2021.00865
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mikolov T., 2014, P INT C LEARN REPR
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Munir M.A., 2021, Advances in neural information processing systems, V34, P22770
   Peng X, 2019, PMLR, P5102
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren S., 2015, Advances in neural information processing systems, P91
   Rezaeianaran F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9184, DOI 10.1109/ICCV48922.2021.00907
   Ridgeway K, 2018, ADV NEUR IN, V31
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Scott T., 2018, P INT C ADV NEUR INF, P76
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song S, 2013, IEEE GLOB CONF SIG, P245, DOI 10.1109/GlobalSIP.2013.6736861
   Su P., 2020, ECCV, P403
   Tian K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9113, DOI 10.1109/ICCV48922.2021.00900
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Q, 2022, IEEE T MULTIMEDIA, V24, P1031, DOI 10.1109/TMM.2021.3104141
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wang W, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1730, DOI 10.1145/3474085.3475317
   Wang Y, 2021, PROC CVPR IEEE, P9598, DOI 10.1109/CVPR46437.2021.00948
   Wu AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9322, DOI 10.1109/ICCV48922.2021.00921
   Wu A, 2022, IEEE T PATTERN ANAL, V44, P4178, DOI 10.1109/TPAMI.2021.3060446
   Wu Aming, 2022, IEEE CVF C COMP VIS, P847
   Wu S, 2022, IEEE T MULTIMEDIA, V24, P2058, DOI 10.1109/TMM.2021.3075323
   Wu ZH, 2022, INT CONF ACOUST SPEE, P2020, DOI 10.1109/ICASSP43922.2022.9746194
   Wu ZH, 2023, IEEE T MULTIMEDIA, V25, P267, DOI 10.1109/TMM.2021.3125130
   Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376
   Zhang B, 2022, IEEE T MULTIMEDIA, V24, P4102, DOI 10.1109/TMM.2021.3114550
   Zhang YX, 2021, PROC CVPR IEEE, P12420, DOI 10.1109/CVPR46437.2021.01224
   Zheng Yangtao, 2020, P IEEE CVF C COMP VI, P13766
   Zhou WZ, 2022, PROC CVPR IEEE, P9571, DOI 10.1109/CVPR52688.2022.00936
   Zhu XG, 2019, PROC CVPR IEEE, P687, DOI 10.1109/CVPR.2019.00078
NR 60
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6250
EP 6261
DI 10.1109/TMM.2023.3347645
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600033
DA 2024-08-05
ER

PT J
AU Lou, JX
   Lin, HH
   Young, P
   White, R
   Yang, ZL
   Shelmerdine, S
   Marshall, D
   Spezi, E
   Palombo, M
   Liu, HT
AF Lou, Jianxun
   Lin, Hanhe
   Young, Philippa
   White, Richard
   Yang, Zelei
   Shelmerdine, Susan
   Marshall, David
   Spezi, Emiliano
   Palombo, Marco
   Liu, Hantao
TI Predicting Radiologists' Gaze With Computational Saliency Models in
   Mammogram Reading
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; mammograms; radiology; saliency; transfer learning
ID EYE-TRACKING; VISUAL-ATTENTION; SEGMENTATION; EXPERTISE; MOVEMENTS;
   NETWORK
AB Previous studies have shown that there is a strong correlation between radiologists' diagnoses and their gaze when reading medical images. The extent to which gaze is attracted by content in a visual scene can be characterised as visual saliency. There is a potential for the use of visual saliency in computer-aided diagnosis in radiology. However, little is known about what methods are effective for diagnostic images, and how these methods could be adapted to address specific applications in diagnostic imaging. In this study, we investigate 20 state-of-the-art saliency models including 10 traditional models and 10 deep learning-based models in predicting radiologists' visual attention while reading 196 mammograms. We found that deep learning-based models represent the most effective type of methods for predicting radiologists' gaze in mammogram reading; and that the performance of these saliency models can be significantly improved by transfer learning. In particular, an enhanced model can be achieved by pre-training the model on a large-scale natural image saliency dataset and then fine-tuning it on the target medical image dataset. In addition, based on a systematic selection of backbone networks and network architectures, we proposed a parallel multi-stream encoded model which outperforms the state-of-the-art approaches for predicting saliency of mammograms.
C1 [Lou, Jianxun; Marshall, David; Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 4AG, Wales.
   [Lin, Hanhe] Univ Dundee, Sch Sci & Engn, Dundee DD1 4HN, Scotland.
   [Young, Philippa] Natl Hlth Serv, Breast Test Wales, Cardiff CF24 4AG, Wales.
   [White, Richard; Yang, Zelei] Univ Hosp Wales, Dept Radiol, Cardiff CF24 4AG, Wales.
   [Shelmerdine, Susan] Great Ormond St Hosp Sick Children, Dept Clin Radiol, London WC1N 3JH, England.
   [Spezi, Emiliano] Cardiff Univ, Sch Engn, Cardiff CF24 3AA, Wales.
   [Palombo, Marco] Cardiff Univ, Brain ResearchImaging Ctr, Sch Psychol, Cardiff CF24 4HQ, Wales.
C3 Cardiff University; University of Dundee; NHS Blood & Transplant
   (NHSBT); Cardiff University; University of London; University College
   London; Great Ormond Street Hospital for Children NHS Foundation Trust;
   Cardiff University; Cardiff University
RP Liu, HT (corresponding author), Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 4AG, Wales.
EM louj2@cardiff.ac.uk; hlin001@dundee.ac.uk; philippa.young2@wales.nhs.uk;
   richard.white3@wales.nhs.uk; zelei.yang@wales.nhs.uk;
   susan.shelmerdine@gosh.nhs.uk; marshallad@cardiff.ac.uk;
   espezi@cardiff.ac.uk; palombom@cardiff.ac.uk; liuh35@cardiff.ac.uk
RI Spezi, Emiliano/A-8917-2008; Lou, Jianxun/GLN-8300-2022; White, Richard
   D/AAT-9034-2020; Palombo, Marco/AAY-2713-2021
OI Spezi, Emiliano/0000-0002-1452-8813; Lou, Jianxun/0000-0002-2982-595X;
   White, Richard D/0000-0002-2370-4575; Palombo,
   Marco/0000-0003-4892-7967; Lin, Hanhe/0000-0002-0297-3549
FU China Scholarship Council
FX No Statement Available
CR Alexander RG, 2020, J VISION, V20, DOI 10.1167/jov.20.10.17
   Ashraf H, 2018, MED TEACH, V40, P62, DOI 10.1080/0142159X.2017.1391373
   Assens M, 2017, IEEE INT CONF COMP V, P2331, DOI 10.1109/ICCVW.2017.275
   Banerjee S, 2018, INFORM SCIENCES, V424, P337, DOI 10.1016/j.ins.2017.10.011
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Bertram R, 2016, RADIOLOGY, V281, P805, DOI 10.1148/radiol.2016151255
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Born J, 2021, PATTERNS, V2, DOI 10.1016/j.patter.2021.100269
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Che ZH, 2020, IEEE T IMAGE PROCESS, V29, P2287, DOI 10.1109/TIP.2019.2945857
   Chen HM, 2022, NPJ DIGIT MED, V5, DOI 10.1038/s41746-022-00699-2
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XY, 2021, PROC CVPR IEEE, P10871, DOI 10.1109/CVPR46437.2021.01073
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Esteva A, 2019, NAT MED, V25, P24, DOI 10.1038/s41591-018-0316-z
   Fang S, 2017, IEEE T NEUR NET LEAR, V28, P1095, DOI 10.1109/TNNLS.2016.2522440
   Goodfellow I., 2022, P 27 INT C NEUR INF, P2672
   Gu DH, 2020, IEEE T MULTIMEDIA, V22, P1720, DOI 10.1109/TMM.2020.2971170
   Harel J., 2007, ADV NEURAL INF PROCE, V19, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Hu FY, 2021, INT C PATT RECOG, P9054, DOI 10.1109/ICPR48806.2021.9413057
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jampani V., 2012, P 8 IND C COMP VIS G, P1
   Jia S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103887
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Judd T., 2012, A benchmark of computational models of saliency to predict human fixations
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kerkouri MA, 2021, IEEE IMAGE PROC, P1464, DOI 10.1109/ICIP42928.2021.9506295
   Khosravan N, 2019, MED IMAGE ANAL, V51, P101, DOI 10.1016/j.media.2018.10.010
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Kümmerer M, 2022, J VISION, V22, DOI 10.1167/jov.22.5.7
   Kummerer M., MIT/Tubingen saliency benchmark
   Lêvêque L, 2019, SIGNAL PROCESS-IMAGE, V78, P86, DOI 10.1016/j.image.2019.06.008
   Lévêque L, 2018, IEEE ACCESS, V6, P37023, DOI 10.1109/ACCESS.2018.2851451
   Li Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-77550-9
   Lin YS, 2021, IEEE ACCESS, V9, P33144, DOI 10.1109/ACCESS.2021.3060765
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lou JX, 2022, NEUROCOMPUTING, V494, P455, DOI 10.1016/j.neucom.2022.04.080
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Pan JT, 2018, Arxiv, DOI arXiv:1701.01081
   Pershin I, 2022, IEEE J BIOMED HEALTH, V26, P4541, DOI 10.1109/JBHI.2022.3183299
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Raghu M, 2019, ADV NEUR IN, V32
   Riche N, 2013, SIGNAL PROCESS-IMAGE, V28, P642, DOI 10.1016/j.image.2013.03.009
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schauerte B, 2012, LECT NOTES COMPUT SC, V7573, P116, DOI 10.1007/978-3-642-33709-3_9
   Souza LS, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107028
   Stec N, 2018, AM J ROENTGENOL, V210, P799, DOI 10.2214/AJR.17.18613
   Stewart EEM, 2020, J VISION, V20, DOI 10.1167/jov.20.12.2
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Tariq A, 2020, J AM COLL RADIOL, V17, P1371, DOI 10.1016/j.jacr.2020.08.018
   Tavakoli HR, 2011, LECT NOTES COMPUT SC, V6688, P666, DOI 10.1007/978-3-642-21227-7_62
   Tliba M, 2022, IEEE COMPUT SOC CONF, P1538, DOI 10.1109/CVPRW56347.2022.00160
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   Tourassi G, 2013, J AM MED INFORM ASSN, V20, P1067, DOI 10.1136/amiajnl-2012-001503
   van Opbroek A, 2015, IEEE T MED IMAGING, V34, P1018, DOI 10.1109/TMI.2014.2366792
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang S, 2022, IEEE T MED IMAGING, V41, P1688, DOI 10.1109/TMI.2022.3146973
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wen GZ, 2017, J MED IMAGING, V4, DOI 10.1117/1.JMI.4.2.025503
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yang XH, 2021, IEEE T MULTIMEDIA, V23, P4326, DOI 10.1109/TMM.2020.3040529
   Yang XH, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3108538
   Yuan YX, 2018, IEEE J BIOMED HEALTH, V22, P1250, DOI 10.1109/JBHI.2017.2734329
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 74
TC 3
Z9 3
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 256
EP 269
DI 10.1109/TMM.2023.3263553
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500023
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Peng, CL
   Kong, ZM
   Liu, DC
   Wang, NN
   Gao, XB
AF Peng, Chunlei
   Kong, Zimo
   Liu, Decheng
   Wang, Nannan
   Gao, Xinbo
TI Disguised Heterogeneous Face Generation With Iterative-Adversarial Style
   Unification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Databases; Three-dimensional displays; Image quality;
   Training; Task analysis; Feature extraction; Adversarial training;
   disguised face dataset; face generation; heterogeneous face recognition;
   style transfer
AB Heterogeneous face recognition (HFR), which refers to matching face images with different modalities, is essential to public safety. Although HFR has made promising progress in recent years, disguised faces in HFR scenarios still remain a major challenge for the following reasons. First, most existing HFR methods focus on traditional scenarios without disguised accessories, and the performance degrades when dealing directly with disguised faces. Second, there is a need for disguised heterogeneous face datasets, which is essential for developing the related research community. Third, colorful accessories are distinct from heterogeneous face images in terms of their modalities, and their direct combination results in style inconsistency and poor quality. Therefore, we propose a disguised heterogeneous face generation method based on an iterative-adversarial style unification framework. Our approach aims to gradually learn frame textures to detail textures in multiple confrontation iterations, resulting in style unification for disguised accessories and heterogeneous faces. We also construct a disguised heterogeneous face dataset, which contains a disguised NIR-VIS subset and a disguised sketch-photo subset. Moreover, we provide benchmark evaluations conducted on our proposed dataset with face recognition and image quality assessment, demonstrating the superiority of our method over direct addition and two representative disguised face generation techniques.
C1 [Peng, Chunlei; Kong, Zimo; Liu, Decheng] Xidian Univ, Sch Cyber Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Wang, Nannan] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; Xidian University; Chongqing University of Posts &
   Telecommunications
RP Wang, NN (corresponding author), Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM clpeng@xidian.edu.cn; kzm.xidian@gmail.com; dchliu@xidian.edu.cn;
   nnwang@xidian.edu.cn; gaoxb@cqupt.edu.cn
RI Liu, Decheng/JBJ-5232-2023
OI Liu, Decheng/0000-0002-6550-212X; Wang, Nannan/0000-0002-4695-6134
FU National Natural Science Foundation of China
FX No Statement Available
CR Anwar A, 2020, Arxiv, DOI arXiv:2008.11104
   Bhavsar K, 2018, Instagram face filter system
   Boutros F, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108473
   Cao Y., 2023, P AAAI C ARTIFICIAL, P268
   Cho M, 2021, IEEE T INF FOREN SEC, V16, P376, DOI 10.1109/TIFS.2020.3013186
   Cong WY, 2022, PROC CVPR IEEE, P18449, DOI 10.1109/CVPR52688.2022.01792
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng ZY, 2019, IEEE T IMAGE PROCESS, V28, P3102, DOI 10.1109/TIP.2019.2894272
   Din NU, 2020, IEEE ACCESS, V8, P44276, DOI 10.1109/ACCESS.2020.2977386
   Du H, 2021, IEEE SIGNAL PROC LET, V28, P768, DOI 10.1109/LSP.2021.3071663
   Fang YK, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107249
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Fu CY, 2022, IEEE T PATTERN ANAL, V44, P2938, DOI 10.1109/TPAMI.2021.3052549
   Ge SM, 2020, IEEE T CIRC SYST VID, V30, P3387, DOI 10.1109/TCSVT.2020.2967754
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   Guan Q, 2022, COMPUT BIOL MED, V145, DOI 10.1016/j.compbiomed.2022.105444
   Guo JZ, 2018, LECT NOTES COMPUT SC, V10996, P275, DOI 10.1007/978-3-319-97909-0_30
   Han H, 2013, IEEE T INF FOREN SEC, V8, P191, DOI 10.1109/TIFS.2012.2228856
   Hang YC, 2022, PROC CVPR IEEE, P19678, DOI 10.1109/CVPR52688.2022.01909
   Hariri W, 2022, SIGNAL IMAGE VIDEO P, V16, P605, DOI 10.1007/s11760-021-02050-w
   Hensel M, 2017, ADV NEUR IN, V30
   Hu WP, 2022, IEEE T INF FOREN SEC, V17, P1435, DOI 10.1109/TIFS.2022.3160612
   Huang M., 2008, P WORKSH FAC REAL LI
   Huang ZZ, 2021, IEEE T INF FOREN SEC, V16, P2031, DOI 10.1109/TIFS.2020.3047753
   Huber M, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9667081
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Gulrajani I, 2017, ADV NEUR IN, V30
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Karras T., 2018, INT C LEARNING REPRE
   Li HL, 2011, IEEE T MULTIMEDIA, V13, P1230, DOI 10.1109/TMM.2011.2168814
   Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liang JM, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102461
   Liang JT, 2022, LECT NOTES COMPUT SC, V13667, P334, DOI 10.1007/978-3-031-20071-7_20
   Ling J, 2021, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR46437.2021.00924
   Liu DC, 2020, IEEE T NEUR NET LEAR, V31, P4699, DOI 10.1109/TNNLS.2019.2957285
   Luo MD, 2022, IEEE T INF FOREN SEC, V17, P2095, DOI 10.1109/TIFS.2022.3177960
   Mare T, 2021, P NEURAL INFORM PROC, P1
   Mei Y., 2022, P IEEE CVF C COMP VI, P18676
   Meng Q, 2021, PROC CVPR IEEE, P14220, DOI 10.1109/CVPR46437.2021.01400
   Montero D, 2022, 2022 16TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS, SITIS, P184, DOI 10.1109/SITIS57111.2022.00042
   Neto PC, 2022, 2022 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), DOI 10.1109/IJCB54206.2022.10007963
   Peng CL, 2017, IEEE T PATTERN ANAL, V39, P301, DOI 10.1109/TPAMI.2016.2542816
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Singh A, 2017, IEEE INT CONF COMP V, P1648, DOI 10.1109/ICCVW.2017.193
   Singh Maneet, 2019, IEEE Transactions on Biometrics, Behavior, and Identity Science, V1, P97, DOI 10.1109/TBIOM.2019.2903860
   Sunkavalli K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778862
   Terhorst P, 2020, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR42600.2020.00569
   Wang H., 2021, P CHIN C BIOM REC, P180
   Wang J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3779, DOI 10.1145/3474085.3478324
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang Zhongyuan, 2023, IEEE Transactions on Biometrics, Behavior, and Identity Science, P298, DOI 10.1109/TBIOM.2023.3242085
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weng RL, 2016, IEEE T IMAGE PROCESS, V25, P1163, DOI 10.1109/TIP.2016.2515987
   Wu X, 2019, AAAI CONF ARTIF INTE, P9005
   Yang SM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4137, DOI 10.1145/3474085.3475546
   Yuen S, 2021, 2020/2021 HKUST CSE FYP masked facial recognition
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324
   Zhong YY, 2021, Arxiv, DOI arXiv:2103.14803
   Zhu MR, 2022, IEEE T NEUR NET LEAR, V33, P893, DOI 10.1109/TNNLS.2020.3030536
NR 64
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3741
EP 3753
DI 10.1109/TMM.2023.3314977
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300014
DA 2024-08-05
ER

PT J
AU Shi, R
   Li, TX
   Zhang, LG
   Yamaguchi, Y
AF Shi, Rui
   Li, Tianxing
   Zhang, Liguo
   Yamaguchi, Yasushi
TI Visualization Comparison of Vision Transformers and Convolutional Neural
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Vision Transformer; convolutional neural network; feature
   representation; optimization visualization
AB Recent research has demonstrated that Vision Transformers (ViTs) are capable of comparable or even better performance than convolutional neural network (CNN) baselines. The differences in their structural designs are obvious, but our understanding of the differences in their feature representations remains limited. In this work, we propose several techniques to achieve high-quality visualization of representations in ViTs. Both qualitative and quantitative experiments show that our technical improvements can observably improve ViT visualization quality compared to previous studies. Furthermore, we conduct visualizations to explore the disparities between ViTs and CNNs pre-trained on ImageNet1K, revealing three intriguing properties of ViTs: a) ViT feature propagation retains image detail information with minimal loss, whereas CNNs discard most image details for class discrimination. b) Different from CNNs, object-related features do not show in ViT higher layers, suggesting that class-discriminative features may not be required for ViT classification. c) Our visualization-assisted texture-bias experiment reveals that both ViTs and CNNs exhibit texture bias, of which ViTs seem to be more biased towards local textures.
C1 [Shi, Rui; Li, Tianxing; Zhang, Liguo] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Yamaguchi, Yasushi] Univ Tokyo, Dept Gen Syst Studies, Tokyo 1538902, Japan.
C3 Beijing University of Technology; University of Tokyo
RP Li, TX (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM ruishi@bjut.edu.cn; litianxing@bjut.edu.cn; zhangliguo@bjut.edu.cn;
   yama@g.ecc.utokyo.ac.jp
OI YAMAGUCHI, Yasushi/0000-0003-0790-4144; Li,
   Tianxing/0000-0002-2489-4884; Zhang, Liguo/0000-0002-2705-1399
FU National Natural Science Foundation of China
FX No Statement Available
CR Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Bhojanapalli S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10211, DOI 10.1109/ICCV48922.2021.01007
   Chefer H, 2021, PROC CVPR IEEE, P782, DOI 10.1109/CVPR46437.2021.00084
   Chen H, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-31384-3
   Cortes C, 2012, J MACH LEARN RES, V13, P795
   Cui XR, 2020, IEEE T MULTIMEDIA, V22, P1847, DOI 10.1109/TMM.2020.2976985
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eitel F., 2022, arXiv
   Fong R, 2019, IEEE I CONF COMP VIS, P2950, DOI 10.1109/ICCV.2019.00304
   Geirhos Robert, 2019, INT C LEARNING REPRE
   Hatamizadeh A, 2022, PROC CVPR IEEE, P10011, DOI 10.1109/CVPR52688.2022.00978
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hohman F, 2020, IEEE T VIS COMPUT GR, V26, P1096, DOI 10.1109/TVCG.2019.2934659
   Hongxu Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8712, DOI 10.1109/CVPR42600.2020.00874
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Joshi G, 2022, INT GEOSCI REMOTE SE, P473, DOI 10.1109/IGARSS46834.2022.9884409
   Joshi G, 2023, IEEE J-STARS, V16, P2819, DOI 10.1109/JSTARS.2023.3247788
   Kingma D. P., 2014, arXiv
   Kornblith S, 2019, PR MACH LEARN RES, V97
   LEE S, 1994, IEEE T NEURAL NETWOR, V5, P409, DOI 10.1109/72.286912
   Li T, 2023, COMPUT GRAPH FORUM, V42, P231, DOI 10.1111/cgf.14651
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu BL, 1999, IEEE T NEURAL NETWOR, V10, P1271, DOI 10.1109/72.809074
   Lundberg SM, 2017, ADV NEUR IN, V30
   Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Nguyen A., 2019, EXPLAINABLE INTERPRE, P55, DOI [DOI 10.1007/978-3-030-28954-64, DOI 10.1007/978-3-030-28954-6_4]
   Olah C, 2020, OVERVIEW EARLY VISIO
   Protas É, 2019, IEEE T NEUR NET LEAR, V30, P2231, DOI 10.1109/TNNLS.2018.2881194
   Raghu M, 2021, ADV NEUR IN, V34
   Ren J., 2023, P 11 INT C LEARN REP
   Santurkar S, 2019, 33 C NEURAL INFORM P, V32
   Shi R, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104516
   Shi R, 2020, NEURAL NETWORKS, V129, P75, DOI 10.1016/j.neunet.2020.05.026
   Simonyan K., 2014, 13126034 ARXIV
   Simonyan K, 2014, Arxiv, DOI [arXiv:1312.6034, DOI 10.48550/ARXIV.1312.6034]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singla S, 2021, PROC CVPR IEEE, P12848, DOI 10.1109/CVPR46437.2021.01266
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tang XO, 1998, IEEE T IMAGE PROCESS, V7, P1602, DOI 10.1109/83.725367
   Tesfaldet M, 2019, IEEE INT CONF COMP V, P3173, DOI 10.1109/ICCVW.2019.00392
   Wang YL, 2020, IEEE T MULTIMEDIA, V22, P1796, DOI 10.1109/TMM.2019.2949872
   Williams R. J., 1986, P 8 ANN C COGN SCI S, P859
   Xie L, 2007, IEEE T MULTIMEDIA, V9, P500, DOI 10.1109/TMM.2006.888009
   Yin HX, 2021, PROC CVPR IEEE, P16332, DOI 10.1109/CVPR46437.2021.01607
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou DQ, 2022, PR MACH LEARN RES
NR 50
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2327
EP 2339
DI 10.1109/TMM.2023.3294805
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100007
DA 2024-08-05
ER

PT J
AU Tian, YT
   Li, JX
   Fu, HZ
   Zhu, L
   Yu, LQ
   Wan, L
AF Tian, Yuntong
   Li, Jiaxi
   Fu, Huazhu
   Zhu, Lei
   Yu, Lequan
   Wan, Liang
TI Self-Mining the Confident Prototypes for Source-Free Unsupervised Domain
   Adaptation in Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reliability; Image segmentation; Adaptation models; Data models;
   Prototypes; Federated learning; Predictive models; source-free
   unsupervised domain adaptation
ID FEATURE ALIGNMENT; NORMALIZATION
AB This paper studies a practical Source-free unsupervised domain adaptation (SFUDA) problem, which transfers knowledge of source-trained models to the target domain, without accessing the source data. It has received increasing attention in recent years, while the prior arts focus on designing adaptation strategies, ignoring that different target samples exhibit different transfer abilities on the source model. Additionally, we observe pixel-wise class prediction is typically accompanied by ambiguity issue, i.e., prediction errors often occur between several confusing classes. In this study, we propose a dual-branch collaborative learning framework that aims to achieve reliable knowledge transfer from important samples to the rest by fully mining confident prototypes in the target data. Concretely, we first partition the target data into confident samples and uncertain samples via a new class-ranking reliability score and then utilize the latent features from the confident branch as guidance to promote the learning of the uncertain branch. For ambiguity issue, we propose a feature relabelling module, which exploits reliable prototypes in the mini-batch as well as in the target data to refine labels of uncertain features. We further deploy the proposed framework to commonly used CNN and state-of-the-art Transformer architectures and reveal the potential to promote the generalization ability of backbone models. Experimental results on both natural and medical benchmark datasets verify that our proposed approach exceeds state-of-the-art SFUDA methods with large margins, and achieves comparable performance to existing UDA methods.
C1 [Tian, Yuntong; Wan, Liang] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Li, Jiaxi] Tianjin Univ, Med Sch, Tianjin 300350, Peoples R China.
   [Fu, Huazhu] ASTAR, Inst High Performance Comp, Singapore 138632, Singapore.
   [Zhu, Lei] Hong Kong Univ Sci & Technol Guangzhou, Guangzhou 511400, Peoples R China.
   [Zhu, Lei] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Yu, Lequan] Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong, Peoples R China.
C3 Tianjin University; Tianjin University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute of High Performance Computing
   (IHPC); Hong Kong University of Science & Technology (Guangzhou); Hong
   Kong University of Science & Technology; University of Hong Kong
RP Wan, L (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM tianyuntong@tju.edu.cn; lijiaxi_1121@tju.edu.cn; hzfu@ieee.org;
   leizhu@ust.hk; lqyu@hku.hk; lwan@tju.edu.cn
RI Fu, Huazhu/A-1411-2014
OI Fu, Huazhu/0000-0002-9702-5524; Zhu, Lei/0000-0003-3871-663X
FU Tianjin Natural Science Foundation
FX No Statement Available
CR Araslanov N, 2021, PROC CVPR IEEE, P15379, DOI 10.1109/CVPR46437.2021.01513
   Bateson M, 2022, MED IMAGE ANAL, V82, DOI 10.1016/j.media.2022.102617
   Campello VM, 2021, IEEE T MED IMAGING, V40, P3543, DOI 10.1109/TMI.2021.3090082
   Chen C, 2021, LECT NOTES COMPUT SC, V12905, P225, DOI 10.1007/978-3-030-87240-3_22
   Chen C, 2020, IEEE T MED IMAGING, V39, P2494, DOI 10.1109/TMI.2020.2972701
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Damodaran B. B., 2018, P EUR C COMP VIS ECC, P447, DOI DOI 10.1007/978-3-030-01225-028
   Guan DY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8033, DOI 10.1109/ICCV48922.2021.00795
   Guo XQ, 2022, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR52688.2022.00690
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P642, DOI 10.1007/978-3-030-58568-6_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J., 2018, International conference on machine learning, P1989
   Hoyer L, 2022, PROC CVPR IEEE, P9914, DOI 10.1109/CVPR52688.2022.00969
   Huai Z, 2023, LECT NOTES COMPUT SC, V14226, P618, DOI 10.1007/978-3-031-43990-2_58
   Huang JX, 2021, ADV NEUR IN, V34
   Huang JX, 2022, PROC CVPR IEEE, P1193, DOI 10.1109/CVPR52688.2022.00127
   Jin X, 2021, IEEE T MULTIMEDIA, V24, P3636, DOI 10.1109/TMM.2021.3104379
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Karim N, 2023, PROC CVPR IEEE, P24120, DOI 10.1109/CVPR52729.2023.02310
   Kumar H., 2021, PROC 15 INT WORKSHOP, P438
   Lee J, 2022, 39 INT C MACHINE LEA
   Li Rui, 2020, P IEEE CVF C COMP VI, P9641
   Li ZY, 2023, IEEE T MED IMAGING, V42, P2666, DOI 10.1109/TMI.2023.3263465
   Liang J., 2020, International Conference on Machine Learning, P6028
   Liu Y, 2021, PROC CVPR IEEE, P1215, DOI 10.1109/CVPR46437.2021.00127
   Mengxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13933, DOI 10.1109/CVPR42600.2020.01395
   Pan F, 2020, PROC CVPR IEEE, P3763, DOI 10.1109/CVPR42600.2020.00382
   Qiu Zhen, 2021, P 30 INT JOINT C ART, P2921, DOI DOI 10.24963/IJCAI.2021/402
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito K, 2017, PR MACH LEARN RES, V70
   Teja SP, 2021, PROC CVPR IEEE, P9608, DOI 10.1109/CVPR46437.2021.00949
   Tian JY, 2022, IEEE T CIRC SYST VID, V32, P3749, DOI 10.1109/TCSVT.2021.3111034
   Tomar D, 2021, IEEE T MED IMAGING, V40, P2926, DOI 10.1109/TMI.2021.3059265
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang D., 2021, ARXIV
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Xia HF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8990, DOI 10.1109/ICCV48922.2021.00888
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Yang C, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102457
   Yang SQ, 2023, COMPUT VIS IMAGE UND, V234, DOI 10.1016/j.cviu.2023.103747
   Yao HF, 2022, AAAI CONF ARTIF INTE, P3099
   Zhang P, 2021, PROC CVPR IEEE, P12409, DOI 10.1109/CVPR46437.2021.01223
   Zhang YY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2526
   Zheng YH, 2019, IEEE T MULTIMEDIA, V21, P2292, DOI 10.1109/TMM.2019.2900166
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 54
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7709
EP 7720
DI 10.1109/TMM.2024.3370678
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000075
DA 2024-08-05
ER

PT J
AU Wei, XX
   Zhao, SJ
AF Wei, Xingxing
   Zhao, Shiji
TI Boosting Adversarial Transferability With Learnable Patch-Wise Masks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Perturbation methods; Adaptation models; Visualization; Training;
   Predictive models; Iterative methods; Statistics; DNNs; Adversarial
   Attack; Adversarial Transferability
AB Adversarial examples have attracted widespread attention in security-critical applications because of their transferability across different models. Although many methods have been proposed to boost adversarial transferability, a gap still exists between capabilities and practical demand. In this article, we argue that the model-specific discriminative regions are a key factor causing overfitting to the source model, and thus reducing the transferability to the target model. For that, a patch-wise mask is utilized to prune the model-specific regions when calculating adversarial perturbations. To accurately localize these regions, we present a learnable approach to automatically optimize the mask. Specifically, we simulate the target models in our framework, and adjust the patch-wise mask according to the feedback of the simulated models. To improve the efficiency, the differential evolutionary (DE) algorithm is utilized to search for patch-wise masks for a specific image. During iterative attacks, the learned masks are applied to the image to drop out the patches related to model-specific regions, thus making the gradients more generic and improving the adversarial transferability. The proposed approach is a preprocessing method and can be integrated with existing methods to further boost the transferability. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our method. We incorporate the proposed approach with existing methods to perform ensemble attacks and achieve an average success rate of 93.01% against seven advanced defense methods, which can effectively enhance the state-of-the-art transfer-based attack performance.
C1 [Wei, Xingxing; Zhao, Shiji] Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China.
C3 Beihang University
RP Wei, XX (corresponding author), Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China.
EM xxwei@buaa.edu.cn; zhaoshiji123@buaa.edu.cn
RI Zhao, Shiji/GYJ-4922-2022
OI Zhao, Shiji/0000-0001-6033-6049
FU National Natural Science Foundation of China
FX No Statement Available
CR Chakraborty UK, 2008, STUD COMPUT INTELL, V143, P1, DOI 10.1007/978-3-540-68830-3
   Chen ZS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P569, DOI 10.1109/ICCV48922.2021.00063
   d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heo B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11916, DOI 10.1109/ICCV48922.2021.01172
   HOLLAND PW, 1971, COMP GROUP STUD, V2, P107, DOI 10.1177/104649647100200201
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia X., 2020, P INT C LEARN REPR, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurakin A., 2018, Artificial Intelligence Safety and Security, P99, DOI DOI 10.1201/9781351251389-8
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Lin Jiadong, 2019, P INT C LEARN REPR
   Liu AS, 2019, AAAI CONF ARTIF INTE, P1028
   Liu Y., 2017, P INT C LEARN REPR
   Liu ZH, 2019, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2019.00095
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long YY, 2022, LECT NOTES COMPUT SC, V13664, P549, DOI 10.1007/978-3-031-19772-7_32
   Naseer M, 2020, PROC CVPR IEEE, P259, DOI 10.1109/CVPR42600.2020.00034
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smilkov D, 2017, Arxiv, DOI arXiv:1706.03825
   Szegedy C., 2017, AAAI C ARTIF INTELL, V31, DOI DOI 10.1609/AAAI.V31I1.11231
   Szegedy C., 2014, P 2 INT C LEARN REPR, P1
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131
   Wan C, 2023, IEEE T MULTIMEDIA, V25, P9572, DOI 10.1109/TMM.2023.3255742
   Wan F., 2023, IEEE Trans. Multimedia, early access, DOI [10.1109/TMM.2023.3255742.[47]S., DOI 10.1109/TMM.2023.3255742.[47]S]
   Wang XS, 2021, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR46437.2021.00196
   Wang XS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16138, DOI 10.1109/ICCV48922.2021.01585
   Wang Z., 2021, Proceedings of the IEEE/CVF international conference on computer vision, P7639
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wei XX, 2023, IEEE T PATTERN ANAL, V45, P2711, DOI 10.1109/TPAMI.2022.3176760
   Wei XX, 2023, IEEE T PATTERN ANAL, V45, P9041, DOI 10.1109/TPAMI.2022.3231886
   Wu WB, 2021, PROC CVPR IEEE, P9020, DOI 10.1109/CVPR46437.2021.00891
   Wu WB, 2020, PROC CVPR IEEE, P1158, DOI 10.1109/CVPR42600.2020.00124
   Xie CY, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2809731
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Xie J., 2018, P INT C LEARN REPR, P1
   Xiong YF, 2022, PROC CVPR IEEE, P14963, DOI 10.1109/CVPR52688.2022.01456
   Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198
   Yuan HJ, 2023, IEEE T MULTIMEDIA, V25, P203, DOI 10.1109/TMM.2021.3124083
   Zhang JP, 2022, PROC CVPR IEEE, P14973, DOI 10.1109/CVPR52688.2022.01457
   Zhang SH, 2023, IEEE T MULTIMEDIA, V25, P4296, DOI 10.1109/TMM.2022.3173533
   Zhao SJ, 2022, LECT NOTES COMPUT SC, V13664, P585, DOI 10.1007/978-3-031-19772-7_34
NR 49
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3778
EP 3787
DI 10.1109/TMM.2023.3315550
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300024
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wu, JB
   Liu, H
   Shi, W
   Liu, MY
   Li, WH
AF Wu, Jianbing
   Liu, Hong
   Shi, Wei
   Liu, Mengyuan
   Li, Wenhao
TI Style-Agnostic Representation Learning for Visible-Infrared Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visible-infrared person re-identification; style-agnostic representation
   learning; cross-modality retrieval
AB One main challenge of visible-infrared person re-identification (VI Re-ID) lies in the large style discrepancy between the heterogeneous data. We present a STyle-Agnostic Representation learning (STAR) framework that bridges the modality gaps at both data and feature levels in a progressive manner. At the data level, we present Cross Modality Blending (CMB), a powerful and parameter-free data augmentation scheme that smoothly synthesizes intermediate modalities by conducting identity-preserving patch exchange and smooth cross-modality blending. At the feature level, we explore the inter-modality feature alignment problem from a new perspective of the style-related feature statistics. Specifically, we design a plug-and-play Adaptive Style Normalization (ASN) module to discard the intrinsic style distractors without losing discriminative content via dual-level adaptive distribution normalization and discriminability compensation. Moreover, considering that an appropriate modality intermediary can convey relevant information on the inter-modality distribution shift, we propose Reciprocal Modality Bridging Learning (RMBL) to better steer the modality bridging process. Two lightweight modality transformation modules are designed in RMBL to model an appropriate intermediate space by manipulating high-order statistics under our shortest distance constraint. Meanwhile, intermediary-guided distribution alignment is reciprocally conducted to align heterogeneous features to the modality intermediary. Experiments on VI Re-ID benchmarks demonstrate the superiority and flexibility of STAR over state-of-the-art methods.
C1 [Wu, Jianbing; Liu, Hong; Shi, Wei; Liu, Mengyuan; Li, Wenhao] Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing 100871, Peoples R China.
C3 Peking University
RP Liu, H (corresponding author), Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing 100871, Peoples R China.
EM kimbing.ng@stu.pku.edu.cn; hongliu@pku.edu.cn; pkusw@pku.edu.cn;
   nkliuyifang@gmail.com; wenhaoli@pku.edu.cn
OI Liu, Hong/0000-0002-7498-6541
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Fan X, 2022, VISUAL COMPUT, V38, P279, DOI 10.1007/s00371-020-02015-z
   Feng YJ, 2023, IEEE T MULTIMEDIA, V25, P7647, DOI 10.1109/TMM.2022.3224663
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Jin X, 2021, IEEE T MULTIMEDIA, V24, P3636, DOI 10.1109/TMM.2021.3104379
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Miao Z., 2021, PROC JCAI, P916
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Pang ZQ, 2023, IEEE T MULTIMEDIA, V25, P6171, DOI 10.1109/TMM.2022.3206662
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang PY, 2021, IEEE T MULTIMEDIA, V23, P1474, DOI 10.1109/TMM.2020.2999180
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Xie QK, 2023, IEEE T MULTIMEDIA, V25, P6384, DOI 10.1109/TMM.2022.3207949
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang MX, 2022, PROC CVPR IEEE, P14288, DOI 10.1109/CVPR52688.2022.01391
   Ye HR, 2021, IEEE T IMAGE PROCESS, V30, P1583, DOI 10.1109/TIP.2020.3045261
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARNING REPRE
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zhong X, 2022, IEEE T CIRC SYST VID, V32, P1418, DOI 10.1109/TCSVT.2021.3072171
   Zhou RW, 2020, IEEE T NEUR NET LEAR, V31, P1592, DOI 10.1109/TNNLS.2019.2920905
NR 38
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2263
EP 2275
DI 10.1109/TMM.2023.3294002
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100011
DA 2024-08-05
ER

PT J
AU Xiao, S
   Lan, GP
   Yang, JC
   Lu, W
   Meng, QG
   Gao, XB
AF Xiao, Shuai
   Lan, Guipeng
   Yang, Jiachen
   Lu, Wen
   Meng, Qinggang
   Gao, Xinbo
TI MCS-GAN: A Different Understanding for Generalization of Deep Forgery
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Forgery; Faces; Deepfakes; Anomaly detection; Generative adversarial
   networks; Data models; Feature extraction; GANs; deepfakes
   generalization detection; image reconstruction
AB After several years of development, deep synthesis technology has made significant progress in image and video synthesis. Deep forgery represented by Deepfakes has become a research hotspot, which is used as a tool for disinformation attacks. The current strongly discriminative models can have good performance on specific datasets, even close to 100% accuracy. Unfortunately, since a specific discriminative method only fits a specific data distribution, and different forgery methods or datasets have different data distributions. These methods fail to achieve high performance in cross-dataset detection. In response to this problem and focusing on the actual situation, we adjust the strong generalization detection across the dataset to the generalization detection of unseen fake video. We propose Multi-Crise-Cross Attention and StyleGANv2 Generative Adversarial Network (MCS-GAN). Firstly, we built a Generative Adversarial Network (GAN) framework to learn the distribution of real face data and generate corresponding face images. Secondly, to break the high stitch between the fake region and the background, the model needs to have strong enough feature analysis and pixel restoration capabilities. Therefore, we propose a generator consisting of a Multi-Crise-Cross-Attention (MC) encoder and a StyleGANv2 (SG2) decoder. Finally, to avoid the situation where as long as a face is normal or different faces are abnormal, we set a latent space encoding discriminator and increase the ratio of latent space vector, so as to detect anomaly generated by the forgery operation acting on latent space. We conduct some model generalization experiments on videos on the Internet and some popular deepfake databases. The results show that the accuracy of our method is better compared with the best methods.
C1 [Xiao, Shuai; Lan, Guipeng; Yang, Jiachen] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Lu, Wen] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Meng, Qinggang] Loughborough Univ, Dept Comp Sci, Loughborough LE11 3TU, England.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Tianjin University; Xidian University; Loughborough University;
   Chongqing University of Posts & Telecommunications
RP Yang, JC (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM xs611@tju.edu.cn; lgp@tju.edu.cn; yangjiachen@tju.edu.cn;
   luwen@xidian.edu.cn; q.meng@lboro.ac.uk; gaoxb@cqupt.edu.cn
RI Han, Guifang/AAS-5700-2020; Yang, Jiachen/ABH-5032-2020; Meng,
   Qinggang/B-9207-2018
OI Han, Guifang/0000-0001-6220-6424; Yang, Jiachen/0000-0003-2558-552X;
   Lan, Guipeng/0000-0001-7321-7460; Meng, Qinggang/0000-0002-9483-5724;
   Xiao, Shuai/0000-0003-4058-8120
FU National Natural Science Foundation of China
FX No Statement Available
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Akçay S, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8851808
   Akcay S, 2019, LECT NOTES COMPUT SC, V11363, P622, DOI 10.1007/978-3-030-20893-6_39
   Amerini I, 2019, IEEE INT CONF COMP V, P1205, DOI 10.1109/ICCVW.2019.00152
   Aneja S, 2020, Arxiv, DOI arXiv:2006.11863
   Bonettini N, 2021, INT C PATT RECOG, P5012, DOI 10.1109/ICPR48806.2021.9412711
   Changtao Miao, 2022, IEEE Transactions on Biometrics, Behavior, and Identity Science, V4, P71, DOI 10.1109/TBIOM.2021.3119403
   Ciftci UA, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020)
   Dang H, 2020, PROC CVPR IEEE, P5780, DOI 10.1109/CVPR42600.2020.00582
   Ding F, 2021, IEEE T MULTIMEDIA, V24, P3429, DOI 10.1109/TMM.2021.3098422
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Dong XY, 2022, PROC CVPR IEEE, P9458, DOI 10.1109/CVPR52688.2022.00925
   Du MN, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P325, DOI 10.1145/3340531.3411892
   Gengyun Jia, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P308, DOI 10.1109/TBIOM.2021.3086109
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guarnera L, 2020, IEEE COMPUT SOC CONF, P2841, DOI 10.1109/CVPRW50498.2020.00341
   Guo G., Comput. Vis. Image Understanding, V204
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   Huang N., 2022, P IEEE INT C DIG IM, P1
   Huang ZL, 2023, IEEE T PATTERN ANAL, V45, P6896, DOI 10.1109/TPAMI.2020.3007032
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Kakar P, 2011, IEEE T MULTIMEDIA, V13, P443, DOI 10.1109/TMM.2011.2121056
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Khalid H, 2020, IEEE COMPUT SOC CONF, P2794, DOI 10.1109/CVPRW50498.2020.00336
   Kim M, 2021, IEEE COMPUT SOC CONF, P1001, DOI 10.1109/CVPRW53098.2021.00111
   Lee S., 2021, IFIP INT C ICT SYSTE, P351
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li X., 2020, P IEEE CVF C COMP VI, P3207
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Masi Iacopo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P667, DOI 10.1007/978-3-030-58571-6_39
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Qi MS, 2021, IEEE T IMAGE PROCESS, V30, P2989, DOI 10.1109/TIP.2020.3048680
   Rssler A., 2019, P IEEE CVF INT C COM, P1
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Shiohara K, 2022, PROC CVPR IEEE, P18699, DOI 10.1109/CVPR52688.2022.01816
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Wu JQ, 2018, LECT NOTES COMPUT SC, V11209, P673, DOI 10.1007/978-3-030-01228-1_40
   Xiao S, 2023, INFORM SCIENCES, V634, P1, DOI 10.1016/j.ins.2023.03.006
   Xu Y., 2023, P IEEE CVF WINTERCON, P1
   Xu Y, 2022, IEEE WINT CONF APPL, P379, DOI 10.1109/WACVW54805.2022.00044
   Yang JC, 2021, IEEE T INF FOREN SEC, V16, P4234, DOI 10.1109/TIFS.2021.3102487
   Yang T, 2021, PROC CVPR IEEE, P672, DOI 10.1109/CVPR46437.2021.00073
   Yu PP, 2022, IEEE T INF FOREN SEC, V17, P547, DOI 10.1109/TIFS.2022.3146781
   Yu Y, 2023, IEEE T MULTIMEDIA, V25, P8487, DOI 10.1109/TMM.2023.3237322
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zenati C. S., 2018, P IEEE INT C DATAMIN, P727
   Zenati H, 2018, IEEE DATA MINING, P727, DOI 10.1109/ICDM.2018.00088
   Zhang X, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035107
   Zhao TC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15003, DOI 10.1109/ICCV48922.2021.01475
   Zhou ZQ, 2018, IEEE T MULTIMEDIA, V20, P1392, DOI 10.1109/TMM.2017.2772438
   Zhu Y, 2020, IEEE T IND INFORM, V16, P6714, DOI 10.1109/TII.2020.2982705
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
NR 52
TC 14
Z9 14
U1 41
U2 132
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1333
EP 1345
DI 10.1109/TMM.2023.3279993
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700025
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Xu, QW
   Zhang, RP
   Zhang, Y
   Wu, YY
   Wang, YF
AF Xu, Qinwei
   Zhang, Ruipeng
   Zhang, Ya
   Wu, Yi-Yan
   Wang, Yanfeng
TI Federated Adversarial Domain Hallucination for Privacy-Preserving Domain
   Generalization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain shift; domain generalization; federated learning; privacy
   preserving
ID IMAGE SEGMENTATION
AB Domain generalization aims to reduce the vulnerability of deep neural networks in the out-of-domain distribution scenario. With the recent and increasing data privacy concerns, federated domain generalization, where multiple domains are distributed on different local clients, has become an important research problem and brings new challenges for learning domain-invariant information from separated domains. In this paper, we address the problem of federated domain generalization from the perspective of domain hallucination. We propose a novel federated domain hallucination learning framework, with no additional data exchange between clients other than model weights, based on the idea that a domain hallucination with enlarged prediction uncertainty for the global model is more likely to transform the samples into an unseen domain. These types of desired domain hallucinations are achieved by generating samples that maximize the entropy of the global model and minimize the cross-entropy of the local model, where the latter loss is further introduced to maintain the sample semantics. By training the local models with the learned domain hallucinations, the final model is expected to be more robust to unseen domain shifts. We perform extensive experiments on three object classification benchmarks and one medical image segmentation benchmark. The proposed method outperforms state-of-the-art methods on all the benchmarks, demonstrating its effectiveness.
C1 [Xu, Qinwei; Zhang, Ruipeng; Zhang, Ya; Wang, Yanfeng] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Xu, Qinwei; Zhang, Ruipeng; Zhang, Ya; Wang, Yanfeng] Shanghai AI Lab, Shanghai 200240, Peoples R China.
   [Wu, Yi-Yan] Commun Res Ctr Canada, Ottawa, ON K2H 8S2, Canada.
C3 Shanghai Jiao Tong University; Shanghai Artificial Intelligence
   Laboratory; Communications Research Centre Canada
RP Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.; Zhang, Y (corresponding author), Shanghai AI Lab, Shanghai 200240, Peoples R China.
EM qinweixu@sjtu.edu.cn; zhangrp@sjtu.edu.cn; ya_zhang@sjtu.edu.cn;
   yiyan.wu@ieee.org; wangyanfeng@sjtu.edu.cn
RI wang, yi/HOF-6668-2023; zhang, ruipeng/GPS-7820-2022
OI zhang, ruipeng/0000-0002-4372-4987; Xu, Qinwei/0000-0002-7055-3765;
   Zhang, Ya/0000-0002-5390-9053; Wu, Yiyan/0000-0001-8890-5389
FU National Key Ramp;D Program of China
FX No Statement Available
CR Ahmed S. M., 2021, P IEEE CVF C COMP VI, P10103
   Balaji Y, 2018, ADV NEUR IN, V31
   Carlucci FM, 2019, IEEE INT CONF COMP V, P3227, DOI 10.1109/ICCVW.2019.00403
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chattopadhyay Prithvijit, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P301, DOI 10.1007/978-3-030-58545-7_18
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   D'Innocente Antonio, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P187, DOI 10.1007/978-3-030-12939-2_14
   Dou Q., 2019, ADV NEUR IN, P579
   Feng HZ, 2021, PR MACH LEARN RES, V139
   Finn C, 2017, PR MACH LEARN RES, V70
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Gretton A., 2006, P ADV NEUR INF PROC, V19
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang Zijun, 2020, INT C LEARN REPR ICL
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jin X, 2021, IEEE T MULTIMEDIA, V24, P3636, DOI 10.1109/TMM.2021.3104379
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Karimireddy SP, 2020, PR MACH LEARN RES, V119
   Kim D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9599, DOI 10.1109/ICCV48922.2021.00948
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li QB, 2021, PROC CVPR IEEE, P10708, DOI 10.1109/CVPR46437.2021.01057
   Li Rui, 2020, P IEEE CVF C COMP VI, P9641
   Li T., 2020, P MACH LEARN SYST ML, V2, P429, DOI DOI 10.48550/ARXIV.1812.06127
   Li X., 2021, P INT C LEARN REPR
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Li YJ, 2019, PR MACH LEARN RES, V97
   Liang J., 2020, International Conference on Machine Learning, P6028
   Liu QD, 2021, PROC CVPR IEEE, P1013, DOI 10.1109/CVPR46437.2021.00107
   Liu YJ, 2023, IEEE T MULTIMEDIA, V25, P126, DOI 10.1109/TMM.2021.3121564
   Ma CS, 2019, IEEE T MULTIMEDIA, V21, P173, DOI 10.1109/TMM.2018.2851446
   Makhzani A., 2015, arXiv preprint arXiv:1511.05644
   Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749
   McMahan HB, 2017, PR MACH LEARN RES, V54, P1273
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Netzer Y., 2011, NIPS WORKSHOP DEEP L
   Orlando JI, 2020, MED IMAGE ANAL, V59, DOI 10.1016/j.media.2019.101570
   Quiñonero-Candela J, 2009, NEURAL INF PROCESS S, pXI
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sanchez-Matilla R, 2020, IEEE T MULTIMEDIA, V22, P1862, DOI 10.1109/TMM.2020.2987694
   Sarawagi Sunita, 2018, INT C LEARN REPR
   Seonguk Seo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P68, DOI 10.1007/978-3-030-58542-6_5
   Shi D., 2020, P INT C MACH LEARN, P8828
   Shujun Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P159, DOI 10.1007/978-3-030-58545-7_10
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang D., 2021, INT C LEARN REPR
   Wang SJ, 2020, IEEE T MED IMAGING, V39, P4237, DOI 10.1109/TMI.2020.3015224
   Xia HF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8990, DOI 10.1109/ICCV48922.2021.00888
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Zeyi Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P124, DOI 10.1007/978-3-030-58536-5_8
   Zhang L, 2023, IEEE T MULTIMEDIA, V25, P3319, DOI 10.1109/TMM.2022.3158072
   Zhang L, 2020, IEEE T MED IMAGING, V39, P2531, DOI 10.1109/TMI.2020.2973595
   Zhang RP, 2022, I S BIOMED IMAGING, DOI 10.1109/ISBI52829.2022.9761561
   Zhao Shanshan, 2020, P ADV NEURAL INFORM, V33, P16096
   Zhou KY, 2020, AAAI CONF ARTIF INTE, V34, P13025
   Zhou Kaiyang, 2021, P INT C LEARN REPR
NR 66
TC 0
Z9 0
U1 16
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1
EP 14
DI 10.1109/TMM.2023.3257566
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500004
DA 2024-08-05
ER

PT J
AU Yang, AJ
   Lin, SH
   Yeh, CH
   Shu, ML
   Yang, Y
   Chang, XJ
AF Yang, Aijia
   Lin, Sihao
   Yeh, Chung-Hsing
   Shu, Minglei
   Yang, Yi
   Chang, Xiaojun
TI Context Matters: Distilling Knowledge Graph for Enhanced Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge distillation; object detection; knowledge graph
AB The human visual system is capable of not only recognizing individual objects but also comprehending the contextual relationship between them in real-world scenarios, making it highly advantageous for object detection. However, in practical applications, such contextual information is often not available. Previous attempts to compensate for this by utilizing cross-modal data such as language and statistics to obtain contextual priors have been deemed sub-optimal due to a semantic gap. To overcome this challenge, we present a seamless integration of context into an object detector through Knowledge Distillation. Our approach intuitively represents context as a knowledge graph, describing the relative location and semantic relevance of different visual concepts. Leveraging recent advancements in graph representation learning with Transformer, we exploit the contextual information among objects using edge encoding and graph attention. Specifically, each image region propagates and aggregates the representation from its highly similar neighbors to form the knowledge graph in the Transformer encoder. Extensive experiments and a thorough ablation study conducted on challenging benchmarks MS-COCO, Pascal VOC and LVIS demonstrate the superiority of our method.
C1 [Yang, Aijia; Yeh, Chung-Hsing] Monash Univ, Fac Informat Technol, Dept Data Sci & Artificial Intelligence, Clayton, Vic 3800, Australia.
   [Lin, Sihao] RMIT Univ, Sch Comp Technol, Melbourne, Vic 3000, Australia.
   [Shu, Minglei] Qilu Univ Technol, Shandong Acad Sci, Shandong Artificial Intelligence Inst, Jinan 250316, Peoples R China.
   [Yang, Yi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Yang, Yi] Univ Technol Sydney, Fac Engeering & Informat Technol, AAII, Ultimo, NSW 2007, Australia.
   [Chang, Xiaojun] Univ Technol Sydney, Fac Engeering & Informat Technol, ReLER Lab, AAII, Ultimo, NSW 2007, Australia.
C3 Monash University; Royal Melbourne Institute of Technology (RMIT); Qilu
   University of Technology; Zhejiang University; University of Technology
   Sydney; University of Technology Sydney
RP Chang, XJ (corresponding author), Univ Technol Sydney, Fac Engeering & Informat Technol, ReLER Lab, AAII, Ultimo, NSW 2007, Australia.
EM aijia.yang@monash.edu; linsihao6@gmail.com; chunghsing.yeh@monash.edu;
   smlsmile1624@163.com; yangyics@zju.edu.cn; cxj273@gmail.com
RI ; Chang, Xiaojun/A-2055-2015
OI Yeh, Chung-Hsing/0000-0002-2938-1455; Chang,
   Xiaojun/0000-0002-7778-8807; Lin, Sihao/0009-0004-3235-373X
FU Natural Science Outstanding Youth Fund of Shandong Province
FX No Statement Available
CR Battaglia PW, 2018, ARXIV, DOI DOI 10.48550/ARXIV.1806.01261
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen L. OBray, 2022, INT C MACHINE LEARNI, P3469
   Chen PX, 2022, LECT NOTES COMPUT SC, V13670, P70, DOI 10.1007/978-3-031-20080-9_5
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5
   Chen Z, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01370-7
   Chenchen Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P91, DOI 10.1007/978-3-030-58545-7_6
   Cho JH, 2019, IEEE I CONF COMP VIS, P4793, DOI 10.1109/ICCV.2019.00489
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai X, 2021, PROC CVPR IEEE, P7838, DOI 10.1109/CVPR46437.2021.00775
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan Z., 2022, PROC IEEECVF C COMPU, P10986
   Fang Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1661
   Galleguillos C, 2008, PROC CVPR IEEE, P3552
   Gao ZT, 2022, PROC CVPR IEEE, P5354, DOI 10.1109/CVPR52688.2022.00529
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Gould Stephen., 2009, ADV NEURAL INFORM PR, P655
   Guo JY, 2021, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR46437.2021.00219
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P549, DOI 10.1007/978-3-030-58452-8_32
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu W., 2020, Advances in neural information processing systems, V33, P22118
   Hu ZN, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2704, DOI 10.1145/3366423.3380027
   Jiang CH, 2018, ADV NEUR IN, V31
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kipf T. N., 2017, P 5 INT C LEARN REPR, P1
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Ladicky L, 2010, LECT NOTES COMPUT SC, V6315, P239, DOI 10.1007/978-3-642-15555-0_18
   Li F, 2022, PROC CVPR IEEE, P13609, DOI 10.1109/CVPR52688.2022.01325
   Li G., 2020, Deepergcn: All you need to train deeper gcns
   Li S, 2022, PROC CVPR IEEE, P9377, DOI 10.1109/CVPR52688.2022.00917
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Li ZH, 2023, IEEE T PATTERN ANAL, V45, P10555, DOI 10.1109/TPAMI.2023.3257546
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Romero Adriana, 2015, ARXIV14126550
   Rong Y, 2020, Adv Neural Inf Process Syst, P12559, DOI DOI 10.48550/ARXIV.2007.02835
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Vapnik V, 2015, J MACH LEARN RES, V16, P2023
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic P., 2017, P 5 INT C LEARN REPR
   Wang LT, 2022, LECT NOTES COMPUT SC, V13669, P314, DOI 10.1007/978-3-031-20077-9_19
   Wang T, 2019, PROC CVPR IEEE, P4928, DOI 10.1109/CVPR.2019.00507
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Xu H, 2019, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2019.00952
   Xu K, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337923
   Yang CHY, 2022, LECT NOTES COMPUT SC, V13669, P123, DOI 10.1007/978-3-031-20077-9_8
   Yang ZD, 2022, PROC CVPR IEEE, P4633, DOI 10.1109/CVPR52688.2022.00460
   Ying C., 2021, ADV NEURAL INF PROCE, V34, P28877, DOI DOI 10.48550/ARXIV.2106.05234
   Zagoruyko S., 2017, ICLR, DOI DOI 10.1016/J.CVIU.2019.07.006.ARXIV:1612.0
   Zand M, 2022, LECT NOTES COMPUT SC, V13670, P390, DOI 10.1007/978-3-031-20080-9_23
   Zhang GJ, 2022, PROC CVPR IEEE, P939, DOI 10.1109/CVPR52688.2022.00102
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhu B., 2020, AUTOASSIGN DIFFERENT
NR 74
TC 0
Z9 0
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 487
EP 500
DI 10.1109/TMM.2023.3266897
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000018
DA 2024-08-05
ER

PT J
AU Yin, ZX
   Wang, JK
   Xiao, YS
   Zhao, HQ
   Li, TL
   Zhou, WB
   Liu, AS
   Liu, XL
AF Yin, Zixin
   Wang, Jiakai
   Xiao, Yisong
   Zhao, Hanqing
   Li, Tianlin
   Zhou, Wenbo
   Liu, Aishan
   Liu, Xianglong
TI Improving Deepfake Detection Generalization by Invariant Risk
   Minimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deepfake detection; invariant risk minimization; model generalization
ID FACE; GAN; MANIPULATION; NETWORKS
AB The abuse of deepfake techniques has raised serious concerns about social security and ethical problems, which motivates the development of deepfake detection. However, without fully addressing the domain gap issue, existing deepfake detection methods still show weak generalization ability among datasets belonging to different domains with domain-specific characteristics like identities and generation methods, limiting their practical applications. In this article, we propose the Invariant Domain-oriented Deepfake Detection method (ID3), which improves the generalization of deepfake detection on multiple domains through invariant risk minimization, a novel learning paradigm that addresses the domain gap problem by jointly training a purified invariant predictor and learning an aligned invariant representation. To train a purified invariant predictor, we design the Domain Refinement Data Augmentation strategy with self-face-swapping and region-erasing approaches, which suppresses domain-specific features and encourages the models to focus on critical domain-invariant characteristics. To learn an aligned invariant representation, we propose the Domain Calibration Batch Normalization approach with multiple BN branches, which normalizes input features from different domains into aligned representations during both training and testing. Extensive experiments on multiple datasets demonstrate that our framework can boost the deepfake detection generalization ability and outperform other baselines by large margins. Our codes can be found here.
C1 [Yin, Zixin; Xiao, Yisong; Liu, Aishan; Liu, Xianglong] Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Wang, Jiakai] Zhongguancun Lab, Beijing 100191, Peoples R China.
   [Zhao, Hanqing; Zhou, Wenbo] Univ Sci & Technol China, Sch Cyberspace Secur, Hefei 230026, Peoples R China.
   [Li, Tianlin] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Liu, Xianglong] Beihang Univ, Beijing Adv Innovat Ctr Big Data Based Precis Med, Beijing 100191, Peoples R China.
C3 Beihang University; Zhongguancun Laboratory; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Nanyang
   Technological University; Beihang University
RP Liu, AS; Liu, XL (corresponding author), Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
EM liuaishan@buaa.edu.cn; xlliu@buaa.edu.cn
RI Wang, Jiakai/ABB-3894-2022
OI Wang, Jiakai/0000-0001-5884-3412; Li, Tianlin/0000-0002-2207-1622; ZHOU,
   WENBO/0000-0002-4703-4641
FU KZ46009501
FX No Statement Available
CR AGARWAL S., 2019, P IEEE C COMP VIS PA, V1, P38, DOI DOI 10.1109/ICCV.2015.425
   Ahuja Kartik, 2020, INT C MACH LEARN, V119, P145
   Arjovsky M, 2020, Arxiv, DOI [arXiv:1907.02893, DOI 10.48550/ARXIV.1907.02893]
   Bitesize B, 2019, deepfakes: what are they and why would I make one?
   Cao N. Z., 2021, INT C DIGITAL FORENS, P360
   Chen BJ, 2022, IEEE T CIRC SYST VID, V32, P3527, DOI 10.1109/TCSVT.2021.3116679
   Chen BJ, 2022, INFORM SCIENCES, V601, P58, DOI 10.1016/j.ins.2022.04.014
   Chen L, 2022, PROC CVPR IEEE, P18689, DOI 10.1109/CVPR52688.2022.01815
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Deepfakedetection, 2019, Contributing data to deepfake detection research
   Deepfakes, 2017, Deepfakes github
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Ding F, 2021, IEEE T MULTIMEDIA, V24, P3429, DOI 10.1109/TMM.2021.3098422
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Donahue J, 2014, PR MACH LEARN RES, V32
   Dong SC, 2023, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR52729.2023.00389
   FaceSwap, 2016, Faceswap github
   Guera D., 2018, P IEEE 15 INT C ADV, P1
   Hu J, 2022, IEEE T CIRC SYST VID, V32, P1089, DOI 10.1109/TCSVT.2021.3074259
   Hu X., 2023, Image Vis. Comput., V130
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jain P., 2021, P IEEE 23 INT WORK S, P1
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Karras T., 2018, INT C LEARNING REPRE
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim M, 2021, IEEE COMPUT SOC CONF, P1001, DOI 10.1109/CVPRW53098.2021.00111
   Kong CQ, 2022, IEEE T CIRC SYST VID, V32, P423, DOI 10.1109/TCSVT.2021.3057457
   Korshunov Pavel, 2022, IEEE Transactions on Biometrics, Behavior, and Identity Science, V4, P386, DOI 10.1109/TBIOM.2022.3143404
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Li JM, 2021, PROC CVPR IEEE, P6454, DOI 10.1109/CVPR46437.2021.00639
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   [李艳歌 Li Yange], 2018, [高分子通报, Polymer Bulletin], P46
   Li YH, 2018, PATTERN RECOGN, V80, P109, DOI 10.1016/j.patcog.2018.03.005
   Li YZ, 2018, IEEE INT WORKS INFOR
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu A., Int. J. Comput. Vis, DOI [10.1007/s11263-023-01884-w, DOI 10.1007/S11263-023-01884-W]
   Liu A., 2023, International Journal of Computer Vision
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083
   Luo YC, 2021, PROC CVPR IEEE, P16312, DOI 10.1109/CVPR46437.2021.01605
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Nadimpalli AV, 2022, IEEE COMPUT SOC CONF, P91, DOI 10.1109/CVPRW56347.2022.00019
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Sabir E, 2019, INTERFACES GUI, V3, P1
   Schneider S., 2020, Advances in Neural Information Processing Systems, V33, P11539
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Tariq S, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P3625, DOI 10.1145/3442381.3449809
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tolosana R, 2020, INFORM FUSION, V64, P131, DOI 10.1016/j.inffus.2020.06.014
   Trinh Y., 2021, P 13 INT JOINT C ART, P567, DOI DOI 10.24963/IJCAI.2021/79.[55]C
   Wang CR, 2021, PROC CVPR IEEE, P14918, DOI 10.1109/CVPR46437.2021.01468
   Wu L, 2020, IEEE T CIRC SYST VID, V30, P2081, DOI 10.1109/TCSVT.2019.2909549
   Xie C, 2020, PROC CVPR IEEE, P816, DOI 10.1109/CVPR42600.2020.00090
   Yang JC, 2022, IEEE T CIRC SYST VID, V32, P4854, DOI 10.1109/TCSVT.2021.3133859
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yu PP, 2022, IEEE T INF FOREN SEC, V17, P547, DOI 10.1109/TIFS.2022.3146781
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang Y, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P15, DOI 10.1109/SIPROCESS.2017.8124497
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zixin Yin, 2021, ADVM '21: Proceedings of the 1st International Workshop on Adversarial Learning for Multimedia, P21, DOI 10.1145/3475724.3483603
NR 67
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6785
EP 6798
DI 10.1109/TMM.2024.3355651
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600014
DA 2024-08-05
ER

PT J
AU Zhang, H
   Li, YD
   Li, XL
AF Zhang, Han
   Li, Yiding
   Li, Xuelong
TI Constrained Bipartite Graph Learning for Imbalanced Multi-Modal
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Constrained bipartite graph; imbalanced modalities; multi-modal
   retrieval; query graph
ID NETWORKS
AB Real-worldmulti-modal retrieval tasks always encounter modal imbalance scenarios. The scales of instances from different modalities are inconsistent and unaligned with each other. Though several methods alleviate the issue by establishing miscellaneous representations of multi-modal data, they still suffer from difficulties like laborious human-being annotations and complex common-space optimization. In the research, we present Constrained Bipartite Graph Learning (CBGL) for imbalanced correlations, where a size-flexible correlation graph is learned from instances' representations. To guide the graph learning, we take advantage of prior side information, including positive pairs and negative pairs, which readily express intra-modality affinity and inter-modality discrepancy. Accordingly, both positive and negative correlations are propagated over instances, and a similarity graph with satisfactory neighbors is achieved. Benefiting from the probabilistic similarities, a query graph is then naturally constructed that directly achieves multi-modal retrieval. To validate the effect, we build a Music-Video-Image (MVI) dataset in regard to music and images with imbalanced scales. Experimental results reported on MVI dataset and three benchmarks demonstrate our prominent superiority over ten representative competitors in multi-modal retrieval.
C1 [Zhang, Han; Li, Yiding; Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence OPt & Elect iOPEN, Xian 710072, Peoples R China.
C3 Northwestern Polytechnical University
RP Li, XL (corresponding author), Northwestern Polytech Univ, Sch Artificial Intelligence OPt & Elect iOPEN, Xian 710072, Peoples R China.
EM zhanghan9937@gmail.com; liyiding@mail.nwpu.edu.cn; li@nwpu.edu.cn
OI Li, Yiding/0000-0002-3650-7909
FU National Key Research and Development Program of China
FX No Statement Available
CR Andrew R., 2013, INT C MACHINE LEARNI, P1247, DOI DOI 10.5555/3042817.3043076
   Bennett KP, 1999, ADV NEUR IN, V11, P368
   Bertsekas DP, 2014, Constrained optimization and Lagrange multiplier methods, DOI DOI 10.1016/B978-0-12-093480-5.50005-2
   Cheng MJ, 2022, PROC CVPR IEEE, P5174, DOI 10.1109/CVPR52688.2022.00512
   Cheng QR, 2021, NEURAL NETWORKS, V134, P143, DOI 10.1016/j.neunet.2020.11.011
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Chun S, 2021, PROC CVPR IEEE, P8411, DOI 10.1109/CVPR46437.2021.00831
   Dejie Yang, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P44, DOI 10.1145/3372278.3390673
   Guan Weili, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P268, DOI 10.1145/3503161.3548020
   Guan WL, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P482, DOI 10.1145/3477495.3532038
   Guan WL, 2022, IEEE T IMAGE PROCESS, V31, P4733, DOI 10.1109/TIP.2022.3187290
   Guan WL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2299, DOI 10.1145/3474085.3475392
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu P, 2021, PROC CVPR IEEE, P5399, DOI 10.1109/CVPR46437.2021.00536
   Hu P, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P635, DOI 10.1145/3331184.3331213
   Huang X, 2020, IEEE T CYBERNETICS, V50, P1047, DOI 10.1109/TCYB.2018.2879846
   Hwang SJ, 2012, IEEE T PATTERN ANAL, V34, P1145, DOI 10.1109/TPAMI.2011.190
   Khan A, 2022, INT J INTELL SYST, V37, P6508, DOI 10.1002/int.22853
   Li X., 2023, Commun. CCF, V18, P44
   [李学龙 Li Xuelong], 2023, [中国科学. 信息科学, Scientia Sinica Informationis], V53, P1
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Liang J, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P569, DOI 10.1145/2911451.2911527
   Liu S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1379, DOI 10.1145/3397271.3401086
   Liu X, 2023, IEEE T MULTIMEDIA, V25, P3811, DOI 10.1109/TMM.2022.3166668
   Liu YX, 2023, IEEE T MULTIMEDIA, V25, P2851, DOI 10.1109/TMM.2022.3152086
   Ma HY, 2022, PROC CVPR IEEE, P18030, DOI 10.1109/CVPR52688.2022.01752
   Mandal D, 2020, IEEE T MULTIMEDIA, V22, P2345, DOI 10.1109/TMM.2019.2954741
   Mikriukov G, 2022, INT CONF ACOUST SPEE, P4463, DOI 10.1109/ICASSP43922.2022.9746251
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Oramas Sergio, 2017, P 2 WORKSH DEEP LEAR, P32, DOI DOI 10.1145/3125486.3125492
   Rasiwasia Nikhil, 2010, P 18 ACM INT C MULT, P251
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Shen X, 2022, INFORM SCIENCES, V604, P45, DOI 10.1016/j.ins.2022.05.006
   Shi G, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22082921
   Shi YF, 2022, IEEE T CIRC SYST VID, V32, P7255, DOI 10.1109/TCSVT.2022.3172716
   Shu ZQ, 2022, INFORM SCIENCES, V609, P304, DOI 10.1016/j.ins.2022.07.095
   Su MY, 2023, IEEE T MULTIMEDIA, V25, P662, DOI 10.1109/TMM.2021.3129623
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tang J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2564638
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   Wagstaff K., 2001, 18th International Conference on Machine Learning, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang L, 2018, J VIS COMMUN IMAGE R, V54, P213, DOI 10.1016/j.jvcir.2018.05.006
   Wang YX, 2022, IEEE T CIRC SYST VID, V32, P8822, DOI 10.1109/TCSVT.2022.3195874
   Wei JW, 2022, IEEE T PATTERN ANAL, V44, P6534, DOI 10.1109/TPAMI.2021.3088863
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Wu F, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109211
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu GW, 2020, ALEX ENG J, V59, P1333, DOI 10.1016/j.aej.2020.02.034
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhai XH, 2012, INT CONF ACOUST SPEE, P2337, DOI 10.1109/ICASSP.2012.6288383
   Zhang CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412847
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhu L, 2023, IEEE T KNOWL DATA EN, V35, P8838, DOI 10.1109/TKDE.2022.3218656
   Zhu L, 2020, IEEE MULTIMEDIA, V27, P79, DOI 10.1109/MMUL.2020.3015764
   Zhu X., 2002, Learning from Labeled and Unlabeled Data with Label Propagation
NR 63
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4502
EP 4514
DI 10.1109/TMM.2023.3323884
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100049
DA 2024-08-05
ER

PT J
AU Zhang, SP
   Han, XY
   Zhang, WG
   Lan, XY
   Yao, HX
   Huang, QM
AF Zhang, Shengping
   Han, Xiaoyu
   Zhang, Weigang
   Lan, Xiangyuan
   Yao, Hongxun
   Huang, Qingming
TI Limb-Aware Virtual Try-On Network With Progressive Clothing Warping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Virtual try-on; image synthesis; appearance flow
ID STYLE; DRESS
AB Image-based virtual try-on aims to transfer an in-shop clothing image to a person image. Most existing methods adopt a single global deformation to perform clothing warping directly, which lacks fine-grained modeling of in-shop clothing and leads to distorted clothing appearance. In addition, existing methods usually fail to generate limb details well because they are limited by the used clothing-agnostic person representation without referring to the limb textures of the person image. To address these problems, we propose Limb-aware Virtual Try-on Network named PL-VTON, which performs fine-grained clothing warping progressively and generates high-quality try-on results with realistic limb details. Specifically, we present Progressive Clothing Warping (PCW) that explicitly models the location and size of in-shop clothing and utilizes a two-stage alignment strategy to progressively align the in-shop clothing with the human body. Moreover, a novel gravity-aware loss that considers the fit of the person wearing clothing is adopted to better handle the clothing edges. Then, we design Person Parsing Estimator (PPE) with a non-limb target parsing map to semantically divide the person into various regions, which provides structural constraints on the human body and therefore alleviates texture bleeding between clothing and body regions. Finally, we introduce Limb-aware Texture Fusion (LTF) that focuses on generating realistic details in limb regions, where a coarse try-on result is first generated by fusing the warped clothing image with the person image, then limb textures are further fused with the coarse result under limb-aware guidance to refine limb details. Extensive experiments demonstrate that our PL-VTON outperforms the state-of-the-art methods both qualitatively and quantitatively.
C1 [Zhang, Shengping; Han, Xiaoyu; Zhang, Weigang] Harbin Inst Technol, Weihai 264209, Peoples R China.
   [Lan, Xiangyuan] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Yao, Hongxun] Harbin Inst Technol, Harbin 150001, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Harbin Institute
   of Technology; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Zhang, WG (corresponding author), Harbin Inst Technol, Weihai 264209, Peoples R China.
EM s.zhang@hit.edu.cn; xyhan@stu.hit.edu.cn; wgzhang@hit.edu.cn;
   lanxy@pcl.ac.cn; h.yao@hit.edu.cn; qmhuang@ucas.ac.cn
RI Huang, Qingming/GLR-3473-2022
OI Huang, Qingming/0000-0002-3025-7099; Zhang, Weigang/0000-0003-0042-7074
FU National Natural Science Foundation of China
FX No Statement Available
CR Al-Halah Z, 2021, IEEE T MULTIMEDIA, V23, P4143, DOI 10.1109/TMM.2020.3037459
   Bai S, 2022, LECT NOTES COMPUT SC, V13675, P409, DOI 10.1007/978-3-031-19784-0_24
   Chen C.-Y., 2021, P IEEE CVF INT C COM, P13809
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58
   Chen ZX, 2018, IEEE T MULTIMEDIA, V20, P2126, DOI 10.1109/TMM.2017.2785253
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Chopra A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5413, DOI 10.1109/ICCV48922.2021.00538
   Corbière C, 2017, IEEE INT CONF COMP V, P2268, DOI 10.1109/ICCVW.2017.266
   D'Innocente A, 2021, IEEE COMPUT SOC CONF, P3905, DOI 10.1109/CVPRW53098.2021.00435
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding YJ, 2022, IEEE T MULTIMEDIA, V24, P2687, DOI 10.1109/TMM.2021.3088281
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Feng H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P273, DOI 10.1145/3474085.3475388
   Fincato M, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491226
   Fincato M, 2021, INT C PATT RECOG, P7669, DOI 10.1109/ICPR48806.2021.9412052
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Han XY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2420, DOI 10.1145/3503161.3547999
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He S, 2022, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR52688.2022.00346
   Hensel M, 2017, ADV NEUR IN, V30
   Hidayati SC, 2021, IEEE T MULTIMEDIA, V23, P365, DOI 10.1109/TMM.2020.2980195
   Hidayati SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P438, DOI 10.1145/3240508.3240546
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hsiao WL, 2017, IEEE I CONF COMP VIS, P4213, DOI 10.1109/ICCV.2017.451
   Hu BW, 2022, IEEE T MULTIMEDIA, V24, P1233, DOI 10.1109/TMM.2022.3143712
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Issenhuth Thibaut, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P619, DOI 10.1007/978-3-030-58565-5_37
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Kingma D.P., 2014, Proc. of ICLR
   Lang YN, 2020, PROC CVPR IEEE, P2592, DOI 10.1109/CVPR42600.2020.00267
   Lee S, 2022, LECT NOTES COMPUT SC, V13677, P204, DOI 10.1007/978-3-031-19790-1_13
   Lee S, 2019, IEEE INT CONF COMP V, P3153, DOI 10.1109/ICCVW.2019.00387
   Li KD, 2021, PROC CVPR IEEE, P15541, DOI 10.1109/CVPR46437.2021.01529
   Li XY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356563
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15
   Ma K, 2018, PROC CVPR IEEE, P4700, DOI 10.1109/CVPR.2018.00494
   Markovitz Amir, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P208, DOI 10.1007/978-3-030-58610-2_13
   Minar M.R., 2020, CVPR WORKSH, V3, P10
   Ming Yang, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2937, DOI 10.1109/ICIP.2011.6116276
   Morelli D, 2022, LECT NOTES COMPUT SC, V13668, P345, DOI 10.1007/978-3-031-20074-8_20
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Rosca M, 2017, Arxiv, DOI arXiv:1706.04987
   Salimans T, 2016, ADV NEUR IN, V29
   Sekhavat YA, 2017, IEEE T MULTIMEDIA, V19, P1041, DOI 10.1109/TMM.2016.2639380
   Siam M, 2017, IEEE IMAGE PROC, P3090, DOI 10.1109/ICIP.2017.8296851
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Teed Z., 2020, EUR C COMP VIS, P402, DOI DOI 10.1007/978-3-030-58536-524
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang N, 2011, IEEE I CONF COMP VIS, P1535, DOI 10.1109/ICCV.2011.6126412
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu J, 2021, IEEE T MULTIMEDIA, V23, P2222, DOI 10.1109/TMM.2021.3070972
   Yamaguchi K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P773, DOI 10.1145/2647868.2654958
   Yang F, 2021, PROC CVPR IEEE, P9894, DOI 10.1109/CVPR46437.2021.00977
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
NR 73
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1731
EP 1746
DI 10.1109/TMM.2023.3286278
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800015
DA 2024-08-05
ER

PT J
AU Zhou, WT
   Kong, LT
   Han, YS
   Qin, J
   Sun, ZN
AF Zhou, Wanting
   Kong, Longteng
   Han, Yushan
   Qin, Jie
   Sun, Zhenan
TI Contextualized Relation Predictive Model for Self-Supervised Group
   Activity Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Group activity representation learning; group activity recognition;
   self-supervised learning; transformer; predictive coding
AB Group activity analysis has attracted remarkable attention recently due to the widespread applications in security, entertainment and military. This article targets at learning group activity representations with self-supervision, which differs from the majorities relying heavily on manually annotated labels. Moreover, existing Self-Supervised Learning (SSL) methods for videos are sub-optimal to generate such representations because of the complex context dynamics in group activities. In this article, an end-to-end framework termed Contextualized Relation Predictive Model (Con-RPM) is proposed for self-supervised group activity representation learning with predictive coding. It involves the Serial-Parallel Transformer Encoder (SPTrans-Encoder) to model the context of spatial interactions and temporal variations, and the Hybrid Context Transformer Decoder (HConTrans-Decoder) to predict the future spatio-temporal relations guided by holistic scene context. Additionally, to improve the discriminability and consistency of prediction, we introduce a united loss integrating group-wise and person-wise contrastive losses in frame-level as well as the adversarial loss in global sequence-level. Consequently, our Con-RPM learns robust group representations via describing temporal evolutions of individual relationships and scene semantics explicitly. Extensive experimental results on downstream tasks indicate the effectiveness and generalization of our model in self-supervised learning, and present state-of-the-art performance on the Volleyball, Collective Activity, VolleyTactic, and Choi's New datasets.
C1 [Zhou, Wanting; Kong, Longteng; Han, Yushan] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
   [Kong, Longteng] Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, Beijing 100191, Peoples R China.
   [Qin, Jie] Nanjing Univ Aeronaut & Astronaut, Nanjing 211106, Peoples R China.
   [Sun, Zhenan] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beihang University;
   Nanjing University of Aeronautics & Astronautics; Chinese Academy of
   Sciences; Institute of Automation, CAS
RP Kong, LT (corresponding author), Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.; Kong, LT (corresponding author), Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, Beijing 100191, Peoples R China.; Qin, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, Nanjing 211106, Peoples R China.
EM wanting.zhou@bupt.edu.cn; konglongteng@bupt.edu.cn;
   hanyushan@bupt.edu.cn; jie.qin@nuaa.edu.cn; znsun@nlpr.ia.ac.cn
OI Qin, Jie/0000-0002-0306-534X; Zhou, Wanting/0000-0003-0867-4117
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahsan U, 2019, IEEE WINT CONF APPL, P179, DOI 10.1109/WACV.2019.00025
   [Anonymous], 2020, ARXIV
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Assefa M, 2023, IEEE T MULTIMEDIA, V25, P5500, DOI 10.1109/TMM.2022.3193559
   Azar SM, 2019, PROC CVPR IEEE, P7884, DOI 10.1109/CVPR.2019.00808
   Benaim Sagie, 2020, P IEEE CVF C COMP VI, P9919, DOI DOI 10.1109/CVPR42600.2020.00994
   Chengxu Zhuang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9560, DOI 10.1109/CVPR42600.2020.00958
   Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16
   Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Fan LF, 2019, IEEE I CONF COMP VIS, P5723, DOI 10.1109/ICCV.2019.00582
   Fan LF, 2018, PROC CVPR IEEE, P6460, DOI 10.1109/CVPR.2018.00676
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Gavrilyuk K, 2020, PROC CVPR IEEE, P836, DOI 10.1109/CVPR42600.2020.00092
   Gidaris S., 2018, PROC INT C LEARN PRE
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Han M., 2022, P IEEE C COMPUTER VI, P2990
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Huang CQ, 2023, IEEE T MULTIMEDIA, V25, P4426, DOI 10.1109/TMM.2022.3175611
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Khan NS, 2020, COGN COMPUT, V12, P748, DOI 10.1007/s12559-020-09731-7
   Kim D, 2022, PROC CVPR IEEE, P20051, DOI 10.1109/CVPR52688.2022.01945
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Knights J, 2021, INT C PATT RECOG, P8914, DOI 10.1109/ICPR48806.2021.9412071
   Kong LT, 2022, IEEE T CIRC SYST VID, V32, P6086, DOI 10.1109/TCSVT.2022.3156634
   Kong LT, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1328, DOI 10.1109/ICASSP.2018.8461770
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Li R., 2021, P 59 ANN M ASS COMP, P6319, DOI 10.18653/v1/2021.acl-long.494
   Li Shuaicheng, 2021, P IEEE CVF INT C COM, P13668
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Neimark D, 2021, IEEE INT CONF COMP V, P3156, DOI [arXiv:2102.00719, 10.1109/ICCVW54120.2021.00355]
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qi MS, 2018, LECT NOTES COMPUT SC, V11214, P104, DOI 10.1007/978-3-030-01249-6_7
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Rui Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P208, DOI 10.1007/978-3-030-58598-3_13
   Sermanet P, 2018, IEEE INT CONF ROBOT, P1134
   Shu TM, 2017, PROC CVPR IEEE, P4255, DOI 10.1109/CVPR.2017.453
   Tamura M, 2022, LECT NOTES COMPUT SC, V13664, P19, DOI 10.1007/978-3-031-19772-7_2
   Tengda Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P312, DOI 10.1007/978-3-030-58580-8_19
   Tong Z, 2022, Arxiv, DOI arXiv:2203.12602
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J., 2020, ECCV, P504
   Wang MS, 2017, PROC CVPR IEEE, P7408, DOI 10.1109/CVPR.2017.783
   Wang R, 2022, PROC CVPR IEEE, P14713, DOI 10.1109/CVPR52688.2022.01432
   Wang XJ, 2016, CAAI T INTELL TECHNO, V1, P303, DOI 10.1016/j.trit.2016.12.004
   Wongun Choi, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1282, DOI 10.1109/ICCVW.2009.5457461
   Wu CY, 2022, PROC CVPR IEEE, P13577, DOI 10.1109/CVPR52688.2022.01322
   Wu CF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P519, DOI 10.1145/3240508.3240513
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Xu K, 2021, IEEE T MULTIMEDIA, V23, P3530, DOI 10.1109/TMM.2020.3026913
   Yan R, 2023, IEEE T PATTERN ANAL, V45, P6955, DOI 10.1109/TPAMI.2020.3034233
   Yang CY, 2020, Arxiv, DOI arXiv:2006.15489
   Yuan HJ, 2021, AAAI CONF ARTIF INTE, V35, P3261
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/ICCV48922.2021.01332, 10.1109/iccv48922.2021.01332]
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 66
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 353
EP 366
DI 10.1109/TMM.2023.3265280
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000028
DA 2024-08-05
ER

PT J
AU Fu, YJ
   Zhang, PJ
   Tang, FL
   Wu, YH
AF Fu, Yujie
   Zhang, Pengju
   Tang, Fulin
   Wu, Yihong
TI Covariant Peak Constraint for Accurate Keypoint Detection and
   Keypoint-Specific Descriptor Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image matching; local feature extraction; covariant peak constraint;
   conditional neural reprojection error
ID SCALE
AB Local feature extraction consists of keypoint detection and local descriptor extraction. Firstly, in keypoint detector learning, existing covariance constraint loss functions cannot constrain the probability distribution shapes in local probability maps that surround keypoints. And existing auxiliary peak loss functions, which are used to alleviate the problem, impair the performance of local feature methods. To solve this problem, we propose a novel Covariant Peak constraint Loss (CP Loss) which is defined as the expectations of local probability maps' position errors. Minimizing our CP Loss can make local probability maps accurately peak at reliable keypoints. Secondly, in descriptor learning, the Neural Reprojection Error (NRE) aims at constraining dense descriptor maps of images. But we argue that only those descriptors of keypoints need to be constrained. Thus, we propose a novel Conditional Neural Reprojection Error (CNRE) that is only conditioned on keypoints. Compared with the NRE, our CNRE can achieve much higher efficiency and produce more keypoint-specific descriptors with better matching performance. We use our CP Loss and CNRE to train a local feature network named as CPCN-Feat. Experimental results show that our CPCN-Feat achieves state-of-the-art performance on four challenging benchmarks.
C1 [Fu, Yujie; Zhang, Pengju; Tang, Fulin; Wu, Yihong] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Fu, Yujie; Wu, Yihong] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Zhang, PJ; Wu, YH (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
EM yujie.fu@nlpr.ia.ac.cn; pengju.zhang@ia.ac.cn; fulin.tang@nlpr.ia.ac.cn;
   yhwu@nlpr.ia.ac.cn
OI wu, yi hong/0000-0003-2198-9113; Tang, Fulin/0000-0002-8474-2671; Fu,
   Yujie/0000-0003-1703-0209
FU National Key Research and Development Program of China
FX No Statement Available
CR Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bai C, 2021, IEEE T MULTIMEDIA, V23, P2199, DOI 10.1109/TMM.2021.3065578
   Balntas V., 2016, BMVC, V1, P3, DOI DOI 10.5244/C.30.119
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Barroso-Laguna A, 2019, IEEE I CONF COMP VIS, P5835, DOI 10.1109/ICCV.2019.00593
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bian J.-W., 2019, P BRIT MACH VIS C
   Bresson G, 2017, IEEE T INTELL VEHICL, V2, P194, DOI 10.1109/TIV.2017.2749181
   Bursuc A, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P595, DOI 10.1145/2671188.2749379
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Ebel P, 2019, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2019.00034
   Fan B, 2023, IEEE T MULTIMEDIA, V25, P1713, DOI 10.1109/TMM.2022.3154165
   Fan B, 2021, IEEE T MULTIMEDIA, V23, P2770, DOI 10.1109/TMM.2020.3016122
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Germain H, 2021, PROC CVPR IEEE, P414, DOI 10.1109/CVPR46437.2021.00048
   Google, ARCore
   Harris C, 1988, A combined corner and edge detector, P147, DOI [10.5244/C.2.23.23.1-23.6, DOI 10.5244/C.2.23]
   He K, 2018, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2018.00069
   Jang E., 2017, ICLR (Poster)
   Lee J, 2022, PROC CVPR IEEE, P4837, DOI 10.1109/CVPR52688.2022.00480
   Lenc K, 2016, LECT NOTES COMPUT SC, V9915, P100, DOI 10.1007/978-3-319-49409-8_11
   Li J., 2021, P INT C ADV NEUR INF, P27236
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Liu Y, 2019, ADV NEUR IN, V32
   Loshchilov I., 2018, INT C LEARN REPR
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2018, LECT NOTES COMPUT SC, V11213, P170, DOI 10.1007/978-3-030-01240-3_11
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mehta S., 2022, P INT C LEARN REPR
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mishchuk A., 2017, ADV NEURAL INFORM PR, V30, P4826, DOI DOI 10.48550/ARXIV.1705.10872
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Qianqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P757, DOI 10.1007/978-3-030-58452-8_44
   Revaud J, 2019, ADV NEUR IN, V32
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schönberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sun K, 2020, IEEE T MULTIMEDIA, V22, P2246, DOI 10.1109/TMM.2019.2957984
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Tyszkiewicz M., 2020, Advances in Neural Information Processing Systems, V33, P14254
   Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165
   Wang CW, 2022, AAAI CONF ARTIF INTE, P2388
   Wang CW, 2023, IEEE T MULTIMEDIA, V25, P3989, DOI 10.1109/TMM.2022.3169331
   Yang X, 2021, IEEE T MULTIMEDIA, V23, P4208, DOI 10.1109/TMM.2020.3038323
   Zhang LG, 2018, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2018.00662
   Zhang X, 2017, PROC CVPR IEEE, P4923, DOI 10.1109/CVPR.2017.523
   Zhang ZC, 2021, INT J COMPUT VISION, V129, P821, DOI 10.1007/s11263-020-01399-8
   Zhao XM, 2023, IEEE T MULTIMEDIA, V25, P3101, DOI 10.1109/TMM.2022.3155927
NR 57
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5383
EP 5397
DI 10.1109/TMM.2023.3333211
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800008
DA 2024-08-05
ER

PT J
AU Gou, JP
   He, X
   Du, L
   Yu, BS
   Chen, WB
   Yi, Z
AF Gou, Jianping
   He, Xin
   Du, Lan
   Yu, Baosheng
   Chen, Wenbai
   Yi, Zhang
TI Hierarchical Locality-Aware Deep Dictionary Learning for Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep dictionary learning; deep learning; dictionary learning; pattern
   recognition
ID DISCRIMINATIVE DICTIONARY; FACE RECOGNITION; K-SVD; SPARSE;
   REPRESENTATION; ALGORITHM; AGE
AB Deep dictionary learning (DDL) shows good performance in visual classification tasks. However, almost all existing DDL methods ignore the locality relationships between the input data representations and the learned dictionary atoms, and learn sub-optimal representations in the feature coding stage, which are less conducive to classification. To this end, we propose a hierarchical locality-aware deep dictionary learning (HILADLE) framework for classification, which can learn locality-constrained dictionaries at different abstract levels through hierarchical dictionary learning. The locality constraints play an important role in learning informative dictionary atoms while preserving the data structure in the original input feature space. Moreover, instead of using an identity activation function like existing DDL methods, we further boost the generalization performance of our HILADLE method with a ReLU activation function to deal with the overfitting issue caused by over-parameterization, inspired by its effectiveness in deep neural networks. Finally, the concatenation of all feature representations learned at different layers is used as input to the final classifier. We demonstrate, through an extensive set of experiments on several benchmark face recognition, image classification, and age estimation datasets, that our method is able to surpass several dictionary learning, deep dictionary learning and deep learning methods.
C1 [Gou, Jianping] Southwest Univ, Coll Comp & Informat Sci, Coll Software, Chongqing 400715, Peoples R China.
   [He, Xin] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Peoples R China.
   [Du, Lan] Monash Univ, Fac Informat Technol, Mulgrave, Vic 3170, Australia.
   [Yu, Baosheng] Univ Sydney, Sch Comp Sci, Camperdown, NSW 2006, Australia.
   [Chen, Wenbai] Beijing Informat Sci & Technol Univ, Sch Automat, Beijing 100101, Peoples R China.
   [Yi, Zhang] Sichuan Univ, Sch Comp Sci, Chengdu 610017, Peoples R China.
C3 Southwest University - China; Jiangsu University; Monash University;
   University of Sydney; Beijing Information Science & Technology
   University; Sichuan University
RP Gou, JP (corresponding author), Southwest Univ, Coll Comp & Informat Sci, Coll Software, Chongqing 400715, Peoples R China.; Du, L (corresponding author), Monash Univ, Fac Informat Technol, Mulgrave, Vic 3170, Australia.
EM cherish.gjp@gmail.com; 2212008025@stmail.ujs.edu.cn; lan.du@monash.edu;
   baosheng.yu@sydney.edu.au; chenwb@bistu.edu.cn; zhangyi@scu.edu.cn
RI yi, zhang/KGL-4990-2024
OI Chen, Wenbai/0000-0001-7683-2776; Gou, Jianping/0000-0003-1413-0693; he,
   xin/0009-0005-7351-8374
FU National Natural Science Foundation of China
FX No Statement Available
CR Abavisani M, 2019, IEEE SIGNAL PROC LET, V26, P948, DOI 10.1109/LSP.2019.2913022
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2009, Advances in neural information processing systems
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Dimiccoli M., 2021, P IEEECVF INT C COMP, P1460
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Goel A., 2022, IEEE Geosci. Re-mote Sens. Lett., V19
   Gou JP, 2022, IEEE T INTELL TRANSP, V23, P25308, DOI 10.1109/TITS.2022.3177647
   Hinton Geoffrey E., 2009, Scholarpedia, V4, P5947
   Janjusevic N, 2021, Arxiv, DOI arXiv:2103.04779
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Lazebnik S, 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lewis DJ, 2018, LECT NOTES COMPUT SC, V11301, P3, DOI 10.1007/978-3-030-04167-0_1
   Liang YY, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/242846
   Liu HP, 2020, PATTERN RECOGN LETT, V130, P199, DOI 10.1016/j.patrec.2018.06.021
   Liu Y, 2018, AAAI CONF ARTIF INTE, P7178
   Martinez R., 1998, Tech. Rep., V24
   Mei SH, 2021, IEEE T MULTIMEDIA, V23, P732, DOI 10.1109/TMM.2020.2987683
   Mokhayeri F, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107129
   Montazeri A, 2021, VISUAL COMPUT, V37, P707, DOI 10.1007/s00371-020-01970-x
   Nene S. K., 1996, Tech. Rep., V6
   Peng X, 2014, PATTERN RECOGN, V47, P2794, DOI 10.1016/j.patcog.2014.03.013
   Qiao C, 2021, NEURAL NETWORKS, V135, P91, DOI 10.1016/j.neunet.2020.12.007
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Shao S, 2020, NEUROCOMPUTING, V385, P122, DOI 10.1016/j.neucom.2019.12.071
   Singhal V., 2019, PROC IEEE INT JOINT, P1
   Singhal V, 2018, IEEE IJCNN
   Singhal V, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107163
   Singhal V, 2017, IEEE T GEOSCI REMOTE, V55, P5274, DOI 10.1109/TGRS.2017.2704590
   Singhal V, 2017, IEEE IJCNN, P2679, DOI 10.1109/IJCNN.2017.7966184
   Song JQ, 2019, PATTERN RECOGN, V91, P135, DOI 10.1016/j.patcog.2019.02.018
   Tang H, 2021, IEEE T NEUR NET LEAR, V32, P2129, DOI 10.1109/TNNLS.2020.2997289
   Tang H, 2019, IEEE WINT CONF APPL, P386, DOI 10.1109/WACV.2019.00047
   Tariyal S, 2016, IEEE ACCESS, V4, P10096, DOI 10.1109/ACCESS.2016.2611583
   Wang J, 2020, IEEE T MULTIMEDIA, V22, P1470, DOI 10.1109/TMM.2019.2946075
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang LC, 2021, IEEE T MULTIMEDIA, V23, P2857, DOI 10.1109/TMM.2020.3017916
   Wei CP, 2013, PATTERN RECOGN, V46, P1277, DOI 10.1016/j.patcog.2012.11.014
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu S, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108007
   Yuan X., 2022, PROC IEEE INT C MULT, P1
   Zeng SN, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3449360
   Zhang GQ, 2020, NEUROCOMPUTING, V391, P177, DOI 10.1016/j.neucom.2020.01.101
   Zhang L., 2013, PROC ACM INT C PROCE, P88
   Zhang Q, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0230-3
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zhang SZ, 2017, PATTERN RECOGN, V64, P130, DOI 10.1016/j.patcog.2016.10.032
   Zhang Z, 2020, IEEE DATA MINING, P811, DOI 10.1109/ICDM50108.2020.00090
   Zhang Z, 2021, IEEE T NEUR NET LEAR, V32, P947, DOI 10.1109/TNNLS.2020.2979748
   Zhao Z, 2019, KNOWL-BASED SYST, V163, P533, DOI 10.1016/j.knosys.2018.09.014
   Zheng HY, 2021, PROC CVPR IEEE, P630, DOI 10.1109/CVPR46437.2021.00069
NR 57
TC 1
Z9 1
U1 17
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 447
EP 461
DI 10.1109/TMM.2023.3266610
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000011
DA 2024-08-05
ER

PT J
AU Han, X
   You, MY
   Lu, P
AF Han, Xuan
   You, Mingyu
   Lu, Ping
TI Improving the Conditional Fine-Grained Image Generation With Part
   Perception
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image synthesis; Task analysis; Semantics; Generators; Training;
   Benchmark testing; Indexes; Fine-grained image generation; conditional
   image generation; part perception
AB Synthesizing the images in line with the given condition is a cardinal issue of image generation. The fine-grained conditional image generation, due to its emphasis on the fidelity of details, is of profound worth to the studies in this field. To learn the conditional distribution of data, the discriminating to class semantic of generated samples is necessitated. Though, most existing methods realize it solely based on the condensed global feature, which potentially impedes the model's focus on the detailed local features and in turn causes the inaccuracy or unstable local appearances in generated images. In this context, we propose PartGAN, which features a novel part perception mechanism to strengthen the model's concentration on the nuts-and-bolts of fine-grained objects. In proposed method, the image given to the discriminator will be deconstructed and encoded into a set of embeddings that represent the semantics of parts. This scheme not only assists the model to capture the discriminative local features more accurately, but also prevents the omission of other general local features. Under the effect of the newly designed condition loss term, every part of generated image is equally encouraged to be closer to the corresponding real part, which helps to ensure that the general parts have a stable appearance that conforms to class semantic. The experiments on the popular benchmarks show that the proposed method significantly improves the effect of the generation for fine-grained images.
C1 [Han, Xuan; You, Mingyu; Lu, Ping] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
C3 Tongji University
RP You, MY (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
EM hanxuan@tongji.edu.cn; myyou@tongji.edu.cn; pinglu@tongji.edu.cn
OI LU, Ping/0000-0002-5802-7316
FU National Natural Science Foundation of China
FX No Statement Available
CR Benny Yaniv, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P514, DOI 10.1007/978-3-030-58574-7_31
   Brock A., 2018, Large Scale GAN Training for High Fidelity Natural Image Synthesis. pages, P1
   Chen S.-A., 2021, ADV NEUR IN, P27566
   Chen T., 2021, PROC IEEECVF INT C C, P9264
   Chen TY, 2022, IEEE T MULTIMEDIA, V24, P2975, DOI 10.1109/TMM.2021.3091859
   Choudhury S., 2021, ADV NEUR IN, P28104
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhariwal P, 2021, ADV NEUR IN, V34
   Gong M., 2019, PROC C NEURAL INF PR
   Grigoryev T., 2021, PROC INT C LEARN REP
   Han L., 2021, ICCV, P14438
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hou L, 2022, PR MACH LEARN RES
   Hou XX, 2023, IEEE T MULTIMEDIA, V25, P3409, DOI 10.1109/TMM.2022.3160360
   Kang M., 2020, Advances in Neural Information Processing Systems, P21357
   Kang M, 2021, ADV NEUR IN, V34
   Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI [10.1109/cvpr42600.2020.00813, DOI 10.1109/CVPR42600.2020.00813]
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Khosla Aditya, 2011, PROC CVPR WORKSHOP F
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kumari N, 2022, PROC CVPR IEEE, P10641, DOI 10.1109/CVPR52688.2022.01039
   Liao WT, 2022, PROC CVPR IEEE, P18166, DOI 10.1109/CVPR52688.2022.01765
   Liu DY, 2023, IEEE T NEUR NET LEAR, V34, P8589, DOI 10.1109/TNNLS.2022.3151631
   Mehmood Rayeesa, 2021, 2021 3rd International Conference on Advances in Computing, Communication Control and Networking (ICAC3N), P370, DOI 10.1109/ICAC3N53548.2021.9725424
   Miyato T., 2018, 6 INT C LEARNING REP
   Odena A, 2017, PR MACH LEARN RES, V70
   Parmar G, 2022, PROC CVPR IEEE, P11400, DOI 10.1109/CVPR52688.2022.01112
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Salimans T, 2016, ADV NEUR IN, V29
   Sanghi A, 2022, PROC CVPR IEEE, P18582, DOI 10.1109/CVPR52688.2022.01805
   Sauer A., 2021, Advances in Neural Information Processing Systems, P17480
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Singh KK, 2019, PROC CVPR IEEE, P6483, DOI 10.1109/CVPR.2019.00665
   Sohn K, 2015, ADV NEUR IN, V28
   Tao M, 2022, PROC CVPR IEEE, P16494, DOI 10.1109/CVPR52688.2022.01602
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wu C. H., 2022, Advances in Neural Information Processing Systems, P22422
   Wu L, 2021, IEEE T NEUR NET LEAR, V32, P722, DOI 10.1109/TNNLS.2020.2979190
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2019, IEEE T CYBERNETICS, V49, P1791, DOI 10.1109/TCYB.2018.2813971
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yan H, 2023, IEEE T MULTIMEDIA, V25, P2323, DOI [10.1109/TCSS.2022.3161996, 10.1109/TMM.2022.3146010]
   Yang Y., 2022, P IEEECVF WINTER C A, P2524
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhao Shengyu, 2020, ADV NEURAL INFORM PR, V33, P7559, DOI DOI 10.36227/TECHRXIV.21805797.V1
   Zhao ZL, 2021, AAAI CONF ARTIF INTE, V35, P11033
   Zhou P., 2021, ICCV, P14061
   Zixuan Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8659, DOI 10.1109/CVPR42600.2020.00869
NR 50
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4792
EP 4804
DI 10.1109/TMM.2023.3326649
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100015
DA 2024-08-05
ER

PT J
AU Liu, X
   Zhang, YT
   Yu, ZT
   Lu, H
   Yue, HJ
   Yang, JY
AF Liu, Xin
   Zhang, Yuting
   Yu, Zitong
   Lu, Hao
   Yue, Huanjing
   Yang, Jingyu
TI rPPG-MAE: Self-Supervised Pretraining With Masked Autoencoders for
   Remote Physiological Measurements
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Signal to noise ratio; Physiology; Task analysis; Heart rate;
   Feature extraction; Self-supervised learning; rPPG; remote heart rate
   measurement; self-supervised learning
ID PPG
AB Remote photoplethysmography (rPPG) is an important technique for detecting human vital signs and has received extensive attention. For a long time, researchers have focused attention on supervised methods that rely on large amounts of labeled data. These methods are limited by their need for large amounts of data and the difficulty of acquiring ground truth physiological signals. To address these issues, several self-supervised methods based on contrastive learning have been proposed. However, they focus on contrastive learning between samples, which neglects inherent self-similar priors in physiological signals and seems to have a limited ability to cope with noise. In this article, a linear self-supervised reconstruction task was designed for extracting the inherent self-similar priors in physiological signals. In addition, a specific noise-insensitive strategy was explored for reducing the interference of motion and illumination. The framework proposed in this article, rPPG-MAE, demonstrates excellent performance even on the challenging VIPL-HR dataset. We also evaluate the proposed method on two public datasets, namely, PURE and UBFC-rPPG. The results show that our method not only outperforms existing self-supervised methods but also outperforms state-of-the-art (SOTA) supervised methods. One important observation is that the quality of the dataset appears to be more important than the size of the dataset used in self-supervised pretraining of the rPPG.
C1 [Liu, Xin; Zhang, Yuting; Yue, Huanjing; Yang, Jingyu] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Liu, Xin] Lappeenranta Lahti Univ Technol LUT, Sch Engn Sci, Comp Vis & Pattern Recognit Lab, Lappeenranta 53850, Finland.
   [Yu, Zitong] Great Bay Univ, Sch Comp & Informat Technol, Dongguan 523000, Peoples R China.
   [Lu, Hao] Hong Kong Univ Sci & Technol Guangzhou, Informat Hub, Guangzhou, Peoples R China.
C3 Tianjin University; Lappeenranta-Lahti University of Technology LUT;
   Hong Kong University of Science & Technology (Guangzhou)
RP Yang, JY (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Yu, ZT (corresponding author), Great Bay Univ, Sch Comp & Informat Technol, Dongguan 523000, Peoples R China.
EM yuzitong@gbu.edu.cn; yjy@tju.edu.cn
OI Yu, Zitong/0000-0003-0422-6616; Liu, Xin/0000-0002-2242-6139
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2021, 16 IEEE INT C AUTOMA
   Baevski A, 2022, PR MACH LEARN RES
   Balakrishnan G, 2013, PROC CVPR IEEE, P3430, DOI 10.1109/CVPR.2013.440
   Biswas D, 2019, IEEE T BIOMED CIRC S, V13, P282, DOI 10.1109/TBCAS.2019.2892297
   Bobbia S, 2019, PATTERN RECOGN LETT, V124, P82, DOI 10.1016/j.patrec.2017.10.017
   Bousefsaf F, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9204364
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen Weixuan, 2018, ECCV
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Chen XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9620, DOI 10.1109/ICCV48922.2021.00950
   de Haan G, 2014, PHYSIOL MEAS, V35, P1913, DOI 10.1088/0967-3334/35/9/1913
   de Haan G, 2013, IEEE T BIO-MED ENG, V60, P2878, DOI 10.1109/TBME.2013.2266196
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dong X.Y., 2023, P AAAI C ARTIFICIAL, VVolume 37, P552
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao XH, 2023, IEEE T MULTIMEDIA, V25, P405, DOI 10.1109/TMM.2021.3127040
   Gidaris S, 2018, ICLR, DOI DOI 10.1109/ICCV.2019.00156
   Gideon J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3975, DOI 10.1109/ICCV48922.2021.00396
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Huang D, 2021, IEEE T MULTIMEDIA, V24, P3300, DOI 10.1109/TMM.2021.3096080
   Huo XY, 2022, IEEE T MULTIMEDIA, V24, P4224, DOI 10.1109/TMM.2021.3115335
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Lam A, 2015, IEEE I CONF COMP VIS, P3640, DOI 10.1109/ICCV.2015.415
   Lewandowska M., 2011, 2011 Federated Conference on Computer Science and Information Systems (FedCSIS), P405
   Li XB, 2014, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2014.543
   Liu X., 2020, Advances in Neural Information Processing Systems, V33, P19400
   Liu X, 2022, Arxiv, DOI arXiv:2110.04447
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu H., 2021, Virtual Reality Intell. Hardw., V3, P33, DOI DOI 10.1016/J.VRIH.2020.10.002
   Lu H, 2021, PROC CVPR IEEE, P12399, DOI 10.1109/CVPR46437.2021.01222
   McDuff D, 2014, IEEE T BIO-MED ENG, V61, P2593, DOI 10.1109/TBME.2014.2323695
   Niu X., 2020, PROC EUR C COMPUT VI, P4186
   Niu XS, 2019, PROC CVPR IEEE, P11909, DOI 10.1109/CVPR.2019.01219
   Niu XS, 2020, IEEE T IMAGE PROCESS, V29, P2409, DOI 10.1109/TIP.2019.2947204
   Niu XS, 2019, LECT NOTES COMPUT SC, V11365, P562, DOI 10.1007/978-3-030-20873-8_36
   Niu XS, 2018, INT C PATT RECOG, P3580, DOI 10.1109/ICPR.2018.8546321
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Perepelkina O, 2020, IEEE COMPUT SOC CONF, P1163, DOI 10.1109/CVPRW50498.2020.00152
   Poh MZ, 2011, IEEE T BIO-MED ENG, V58, P7, DOI 10.1109/TBME.2010.2086456
   Qiu Y, 2019, IEEE T MULTIMEDIA, V21, P1778, DOI 10.1109/TMM.2018.2883866
   Rehman YAU, 2017, SIG P ALGO ARCH ARR, P195, DOI 10.23919/SPA.2017.8166863
   Revanur A., 2022, Multimodal Al in Health-care: A Paradigm Shift in Health Intelligence, P307
   Song RC, 2021, IEEE J BIOMED HEALTH, V25, P1373, DOI 10.1109/JBHI.2021.3051176
   Speth J, 2023, PROC CVPR IEEE, P14464, DOI 10.1109/CVPR52729.2023.01390
   Spetlik R., 2018, P BRIT MACH VIS C NE, P3
   Stricker R, 2014, IEEE ROMAN, P1056, DOI 10.1109/ROMAN.2014.6926392
   Sun ZD, 2022, LECT NOTES COMPUT SC, V13672, P492, DOI 10.1007/978-3-031-19775-8_29
   Tang CX, 2018, IEEE COMPUT SOC CONF, P1390, DOI 10.1109/CVPRW.2018.00178
   Verkruysse W, 2008, OPT EXPRESS, V16, P21434, DOI 10.1364/OE.16.021434
   Wang H, 2022, AAAI CONF ARTIF INTE, P2431
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P1479, DOI 10.1109/TBME.2016.2609282
   Wang WJ, 2016, IEEE T BIO-MED ENG, V63, P1974, DOI 10.1109/TBME.2015.2508602
   Wang ZK, 2019, IEEE IMAGE PROC, P3327, DOI [10.1109/icip.2019.8803649, 10.1109/ICIP.2019.8803649]
   Wei C, 2022, PROC CVPR IEEE, P14648, DOI 10.1109/CVPR52688.2022.01426
   Xu K, 2021, IEEE T MULTIMEDIA, V23, P3530, DOI 10.1109/TMM.2020.3026913
   Yang Y., 2022, PROC 11 INT C LEARN
   Yu Z., 2019, PROC 30 BRIT MACH VI
   Yu ZT, 2023, INT J COMPUT VISION, V131, P1307, DOI 10.1007/s11263-023-01758-1
   Yu ZT, 2022, PROC CVPR IEEE, P4176, DOI 10.1109/CVPR52688.2022.00415
   Yu ZT, 2021, IEEE SIGNAL PROC MAG, V38, P50, DOI 10.1109/MSP.2021.3106285
   Yu ZT, 2021, IEEE SIGNAL PROC LET, V28, P1290, DOI 10.1109/LSP.2021.3089908
   Yu ZT, 2020, IEEE SIGNAL PROC LET, V27, P1245, DOI 10.1109/LSP.2020.3007086
   Yu ZT, 2019, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2019.00024
   Zhan Q, 2020, BIOMED OPT EXPRESS, V11, P1268, DOI 10.1364/BOE.382637
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhou J., 2022, PROC INT C LEARN REP
NR 68
TC 1
Z9 1
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7278
EP 7293
DI 10.1109/TMM.2024.3363660
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000070
DA 2024-08-05
ER

PT J
AU Lu, Y
   Chen, XY
   Wu, ZX
   Tan, M
   Yu, JZ
AF Lu, Yue
   Chen, Xingyu
   Wu, Zhengxing
   Tan, Min
   Yu, Junzhi
TI Binary Similarity Few-Shot Object Detection With Modeling of Hard
   Negative Samples
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot learning; object detection; computer vision; deep learning
AB For few-shot object detection, this work proposes a binary similarity detector (BSDet), which realizes a novel similarity-based multiple binary classification and enhances the feature margin between positive and hard negative samples. First, we revisit the classification paradigm, concluding that multiple binary classification paradigm is more suitable than multi-class classification paradigm for the few-shot task. Hence, we propose a binary similarity head (BSH) by posing the classification task as multiple binary similarity measurements rather than a multi-class prediction. Second, focusing on the hard negative samples, we propose a feature enhancement module (FEM). During training phase, the FEM can push the features of positive and hard negative samples far away from each other, and thus effectively suppresses false positives. Abundant experiments and visualizations indicate that our method achieves state-of-the-art performances on few-shot object detection tasks.
C1 [Lu, Yue; Wu, Zhengxing; Tan, Min; Yu, Junzhi] Chinese Acad Sci, Inst Automat, Lab Cognit & Decis Intelligence Complex Syst, Beijing 100190, Peoples R China.
   [Lu, Yue; Wu, Zhengxing; Tan, Min] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Chen, Xingyu; Yu, Junzhi] Peking Univ, Coll Engn, Dept Adv Mfg & Robot, State Key Lab Turbulence & Complex Syst, Beijing 100871, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peking University
RP Yu, JZ (corresponding author), Peking Univ, Coll Engn, Dept Adv Mfg & Robot, State Key Lab Turbulence & Complex Syst, Beijing 100871, Peoples R China.
EM luyue2018@ia.ac.cn; chenxy.sean@gmail.com; zhengxing.wu@ia.ac.cn;
   min.tan@ia.ac.cn; junzhi.yu@ia.ac.cn
RI Yu, Junzhi/A-7876-2010
OI Yu, Junzhi/0000-0002-6347-572X
FU National Natural Science Foundation of China
FX No Statement Available
CR Bar A, 2022, PROC CVPR IEEE, P14585, DOI 10.1109/CVPR52688.2022.01420
   Chai Z, 2022, IEEE T CYBERNETICS, V52, P9784, DOI 10.1109/TCYB.2021.3067786
   Chen DJ, 2021, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR46437.2021.01207
   Chen H, 2018, AAAI CONF ARTIF INTE, P2836
   Cheng BW, 2020, Arxiv, DOI arXiv:1810.04002
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan ZB, 2021, PROC CVPR IEEE, P4525, DOI 10.1109/CVPR46437.2021.00450
   Gao XQ, 2023, Arxiv, DOI arXiv:2304.04222
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   He K., 2017, P IEEE INT C COMP VI, P2961
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiaxi Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P456, DOI 10.1007/978-3-030-58517-4_27
   Kaiwen Duan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P399, DOI 10.1007/978-3-030-58580-8_24
   Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851
   Karlinsky L, 2019, PROC CVPR IEEE, P5192, DOI 10.1109/CVPR.2019.00534
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li YT, 2021, PROC CVPR IEEE, P15390, DOI 10.1109/CVPR46437.2021.01514
   Li YW, 2020, Arxiv, DOI arXiv:2012.15159
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin WD, 2021, Arxiv, DOI arXiv:2104.14984
   Liu Y, 2017, IEEE I CONF COMP VIS, P571, DOI 10.1109/ICCV.2017.69
   Lu Y, 2023, IEEE T CYBERNETICS, V53, P514, DOI 10.1109/TCYB.2022.3149825
   Oksuz K, 2018, LECT NOTES COMPUT SC, V11211, P521, DOI 10.1007/978-3-030-01234-2_31
   Osokin Anton, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P635, DOI 10.1007/978-3-030-58555-6_38
   Qiao LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8661, DOI 10.1109/ICCV48922.2021.00856
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Sun B, 2021, PROC CVPR IEEE, P7348, DOI 10.1109/CVPR46437.2021.00727
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang X., 2020, P INT C MACH LEARN, P9919
   Wang YX, 2019, IEEE I CONF COMP VIS, P9924, DOI 10.1109/ICCV.2019.01002
   Wu A, 2021, ADV NEUR IN, V34
   Wu Y., 2019, Detectron2
   Yan XP, 2019, IEEE I CONF COMP VIS, P9576, DOI 10.1109/ICCV.2019.00967
   Yang Y., 2020, P ADV NEUR INF PROC, V33, P3521
   Yang Z, 2020, AAAI CONF ARTIF INTE, V34, P12653
   Zhang S, 2022, PROC CVPR IEEE, P19185, DOI 10.1109/CVPR52688.2022.01861
   Zhang WL, 2021, PROC CVPR IEEE, P13003, DOI 10.1109/CVPR46437.2021.01281
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhu CC, 2021, PROC CVPR IEEE, P8778, DOI 10.1109/CVPR46437.2021.00867
NR 43
TC 0
Z9 0
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4805
EP 4818
DI 10.1109/TMM.2023.3326872
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100008
DA 2024-08-05
ER

PT J
AU Luo, YT
   Zhong, XY
   Zeng, MC
   Xie, JL
   Wang, SY
   Liu, GY
AF Luo, Yutong
   Zhong, Xinyue
   Zeng, Minchen
   Xie, Jialan
   Wang, Shiyuan
   Liu, Guangyuan
TI CGLF-Net: Image Emotion Recognition Network by Combining Global
   Self-Attention Features and Local Multiscale Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Emotion recognition; Transformers; Visualization;
   Image recognition; Data mining; Image color analysis; Class activation
   map; global and local features; image emotion recognition; local
   attention; multibranch loss function; multiscale; self-attention
ID PREDICTION
AB Convolutional neural networks (CNNs) are commonly employed for image emotion recognition owing to their ability to extract local features; however, they have difficulty capturing the global representations of images. In contrast, self-attention modules in a visual transformer network can capture long-range dependencies as global features. Some studies have shown that an image's local and global features determine the emotions of the image and that some local regions can generate an emotional prioritization effect. Therefore, we proposed combining global self-attention features and a local multiscale features network (CGLF-Net) to recognize an image's emotion, extracting image features from global and local perspectives. Specifically, the cross-scale transformer network is employed instead of convolution operations in the global feature branch to enhance its model feature representation. In the local feature branch, the improved feature pyramid module is applied to extract features from different sensory fields, thereby combining semantic information with different scales. Furthermore, the local attention module based on class activation maps guides the network to focus on locally salient regions. In addition, using multibranch loss functions, local and global feature branches are combined to enhance the ability to capture a comprehensive set of features. Consequently, the proposed network achieves recognition accuracies of 75.61% and 65.01% on the FI-8 benchmark dataset and Emotion-6 benchmark dataset, respectively. These results show that the proposed CGLF-Net reliably address the difficulty of extracting global features using CNNs, representing the classification performance of the state-of-the-art.
C1 [Luo, Yutong; Zhong, Xinyue; Zeng, Minchen; Xie, Jialan; Wang, Shiyuan; Liu, Guangyuan] Southwest China Univ, Sch Elect & Informat Engn, Chongqing 400715, Peoples R China.
C3 Southwest University - China
RP Liu, GY (corresponding author), Southwest China Univ, Sch Elect & Informat Engn, Chongqing 400715, Peoples R China.
EM lyt252012778@email.swu.edu.cn; xzhong3@utas.edu.au;
   zmczmc@email.swu.edu.cn; jialanxie@email.swu.edu.cn;
   wangshiyuan@email.swu.edu.cn; liugy@swu.edu.cn
OI Liu, Guangyuan/0000-0002-8058-5947; Xie, Jialan/0000-0002-8068-2077
FU National Natural Science Foundation of China
FX No Statement Available
CR Ali AR, 2017, IEEE WINT CONF APPL, P679, DOI 10.1109/WACV.2017.81
   Borth D., 2013, P 21 ACM INT C MULT, P223, DOI 10.1145/2502081.2502282
   CAMRAS L, 1980, AM J PSYCHOL, V93, P751, DOI 10.2307/1422394
   Chen M, 2015, IEEE IMAGE PROC, P4491, DOI 10.1109/ICIP.2015.7351656
   Chen T, 2014, Arxiv, DOI arXiv:1410.8586
   Chen YP, 2022, PROC CVPR IEEE, P5260, DOI 10.1109/CVPR52688.2022.00520
   Compton Rebecca J, 2003, Behav Cogn Neurosci Rev, V2, P115, DOI 10.1177/1534582303002002003
   Detenber BH, 1998, J BROADCAST ELECTRON, V42, P113
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Hanbury A., 2010, P 18 ACM INT C MULTI, V18, P83, DOI DOI 10.1145/1873951.1873965
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Jou B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P159, DOI 10.1145/2733373.2806246
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   Lang PJ, 1998, BIOL PSYCHIAT, V44, P1248, DOI 10.1016/S0006-3223(98)00275-3
   LANG PJ, 1979, PSYCHOPHYSIOLOGY, V16, P495, DOI 10.1111/j.1469-8986.1979.tb01511.x
   Lee J, 2011, IEEE T MULTIMEDIA, V13, P1031, DOI 10.1109/TMM.2011.2158530
   Li W., 2012, 20 ACMINT C MULTIMED, P721
   Liang Y, 2021, IEEE IMAGE PROC, P1039, DOI 10.1109/ICIP42928.2021.9506701
   Lim L, 2020, IEEE IMAGE PROC, P1886, DOI 10.1109/ICIP40778.2020.9191258
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu X, 2017, INT CONF AFFECT, P440, DOI 10.1109/ACII.2017.8273637
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Panda R, 2018, LECT NOTES COMPUT SC, V11206, P594, DOI 10.1007/978-3-030-01216-8_36
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Parrott W. G., 2001, Emotions in Social Psychology
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Qian Shengsheng, 2016, P 24 ACM INT C MULTI, P2
   Rao TR, 2020, NEURAL PROCESS LETT, V51, P2043, DOI 10.1007/s11063-019-10033-9
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   SCHLOSBERG H, 1954, PSYCHOL REV, V61, P81, DOI 10.1037/h0054570
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tong LJ, 2020, MULTIMED TOOLS APPL, V79, P9469, DOI 10.1007/s11042-019-07886-6
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XH, 2013, IEEE IMAGE PROC, P3230, DOI 10.1109/ICIP.2013.6738665
   Wei-Ning W, 2006, IEEE SYS MAN CYBERN, P3534, DOI 10.1109/ICSMC.2006.384667
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu BY, 2017, IEEE T MULTIMEDIA, V19, P1670, DOI 10.1109/TMM.2017.2655881
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang JF, 2017, AAAI CONF ARTIF INTE, P224
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1640, DOI 10.1109/TMM.2020.3001527
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yuan Jianbo, 2013, P 2 INT WORKSH ISS S, P1
   Zhang HM, 2021, IEEE T MULTIMEDIA, V23, P2033, DOI 10.1109/TMM.2020.3007352
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhao SC, 2022, IEEE T PATTERN ANAL, V44, P6729, DOI 10.1109/TPAMI.2021.3094362
   Zhao SC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P192, DOI 10.1145/3343031.3351062
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
NR 64
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1894
EP 1908
DI 10.1109/TMM.2023.3289762
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800009
OA Green Published
DA 2024-08-05
ER

PT J
AU Ping, P
   Guo, BB
   Bloh, OT
   Mao, YC
   Xu, F
AF Ping, Ping
   Guo, Bobiao
   Bloh, Olano Teah
   Mao, Yingchi
   Xu, Feng
TI Hiding Multiple Images into a Single Image Using Up-Sampling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Image color analysis; Compressed sensing; Image
   coding; Manganese; Artificial intelligence; Image quality; Image hiding;
   up-sampling; interpolation algorithms; euclidean distance
ID STEGANOGRAPHY METHOD; ENCRYPTED IMAGES; TRANSFORMATION; COMPRESSION
AB The goal of multiple-image hiding is to hide several secret images within another carrier image without significantly changing its appearance, and then perfectly reconstruct all of the secret images. The challenge is to ensure that the stego-image has great visual quality and can resist various steganalysis under the premise of hiding as much information as possible in one image. To address this issue, the majority of known image-hiding methods focus on hiding images using compression techniques. In this article, we present a novel multiple-image hiding method based on up-sampling and reversible color transformation. First, the interpolation algorithm up-samples the carrier image, so that the attribute of similar neighboring pixel values in the up-sampled image can significantly improve the effect of image hiding. The embedding procedure is then performed using the proposed Euclidean Distance (ED)-based block matching and reversible color transformation, which decreases the chance of local blurring in the stego-image. Experimental results show that the proposed method surpasses existing advanced methods by achieving an average of 33 dB and 28 dB of PSNR for the stego-image with a hiding capacity 2 BPP and 8 BPP, and obtaining 100% reconstructing accuracy for all secret images. It also has a high level of resistance to steganalysis and a strong robustness against various image-processing attacks.
C1 [Ping, Ping; Guo, Bobiao; Bloh, Olano Teah; Mao, Yingchi; Xu, Feng] Hohai Univ, Coll Comp & Informat, Nanjing 210098, Peoples R China.
   [Ping, Ping; Guo, Bobiao; Bloh, Olano Teah; Mao, Yingchi; Xu, Feng] Hohai Univ, Key Lab Water Big Data Technol, Minist Water Resources, Nanjing 210098, Peoples R China.
C3 Hohai University; Hohai University
RP Ping, P (corresponding author), Hohai Univ, Coll Comp & Informat, Nanjing 210098, Peoples R China.
EM pingpingnjust@163.com; guobobiao@163.com; otbloh@gmail.com;
   maoyingchi@gmail.com; njxufeng@163.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Baluja S, 2020, IEEE T PATTERN ANAL, V42, P1685, DOI 10.1109/TPAMI.2019.2901877
   BERNSTEIN R, 1976, IBM J RES DEV, V20, P40, DOI 10.1147/rd.201.0040
   Boehm B, 2014, Arxiv, DOI arXiv:1410.6656
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Chai XL, 2022, NONLINEAR DYNAM, V108, P2671, DOI 10.1007/s11071-022-07328-3
   Chai XL, 2021, INFORM SCIENCES, V556, P305, DOI 10.1016/j.ins.2020.10.007
   Chai XL, 2020, OPT LASER ENG, V124, DOI 10.1016/j.optlaseng.2019.105837
   Chai XL, 2017, SIGNAL PROCESS, V134, P35, DOI 10.1016/j.sigpro.2016.11.016
   Chen XY, 2022, IEEE T NETW SCI ENG, V9, P219, DOI 10.1109/TNSE.2020.3041529
   Chen XY, 2019, CMC-COMPUT MATER CON, V59, P239, DOI 10.32604/cmc.2019.03572
   Chuman T, 2019, IEEE T INF FOREN SEC, V14, P1515, DOI 10.1109/TIFS.2018.2881677
   Deng J, 2017, MULTIMED TOOLS APPL, V76, P10097, DOI 10.1007/s11042-016-3600-2
   Ding Y, 2022, IEEE T NEUR NET LEAR, V33, P4915, DOI [10.1080/00206814.2021.1969525, 10.1109/TNNLS.2021.3062754]
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Guan ZY, 2023, IEEE T PATTERN ANAL, V45, P372, DOI 10.1109/TPAMI.2022.3141725
   Hassaballah M, 2021, IEEE T IND INFORM, V17, P7743, DOI 10.1109/TII.2021.3053595
   He JH, 2018, IEEE T MULTIMEDIA, V20, P2645, DOI 10.1109/TMM.2018.2817065
   Hou DD, 2018, J VIS COMMUN IMAGE R, V53, P134, DOI 10.1016/j.jvcir.2017.11.014
   Hou DD, 2016, J VIS COMMUN IMAGE R, V40, P225, DOI 10.1016/j.jvcir.2016.06.018
   Hua ZY, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.107998
   Huang XL, 2022, ALEX ENG J, V61, P7637, DOI 10.1016/j.aej.2022.01.015
   Huo DM, 2021, OPT COMMUN, V492, DOI 10.1016/j.optcom.2021.126976
   Jiang DH, 2021, SIGNAL PROCESS, V188, DOI 10.1016/j.sigpro.2021.108220
   Jing JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4713, DOI 10.1109/ICCV48922.2021.00469
   Katzenbeisser S, 2002, PROC SPIE, V4675, P50, DOI 10.1117/12.465313
   Kong DZ, 2018, IEEE T IND INFORM, V14, P673, DOI 10.1109/TII.2017.2714261
   Lai IJ, 2011, IEEE T INF FOREN SEC, V6, P936, DOI 10.1109/TIFS.2011.2135853
   Lee YL, 2014, IEEE T CIRC SYST VID, V24, P695, DOI 10.1109/TCSVT.2013.2283431
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Liu Q, 2022, IEEE T CIRC SYST VID, V32, P4038, DOI 10.1109/TCSVT.2021.3108772
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Peng F, 2022, IEEE T CIRC SYST VID, V32, P5817, DOI 10.1109/TCSVT.2022.3161419
   Ping P, 2022, DIGIT SIGNAL PROCESS, V120, DOI 10.1016/j.dsp.2021.103263
   Ping P, 2019, IEEE ACCESS, V7, P170168, DOI 10.1109/ACCESS.2019.2955570
   Ponuma R, 2019, MULTIMED TOOLS APPL, V78, P25707, DOI 10.1007/s11042-019-07808-6
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Wang H, 2019, SIGNAL PROCESS, V155, P218, DOI 10.1016/j.sigpro.2018.10.001
   Wang KS, 2022, MULTIMED TOOLS APPL, V81, P20175, DOI 10.1007/s11042-022-12305-4
   Wang XY, 2021, INFORM SCIENCES, V574, P505, DOI 10.1016/j.ins.2021.06.032
   Wang ZY, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24070878
   Wu YR, 2023, IEEE T IND INFORM, V19, P2089, DOI 10.1109/TII.2022.3194590
   Xu YM, 2022, PROC CVPR IEEE, P7865, DOI 10.1109/CVPR52688.2022.00772
   Yao H, 2018, IEEE ACCESS, V6, P40569, DOI 10.1109/ACCESS.2018.2858858
   Ye GD, 2020, SIGNAL PROCESS, V172, DOI 10.1016/j.sigpro.2020.107563
   Zhang B, 2021, IEEE T MULTIMEDIA, V23, P2656, DOI 10.1109/TMM.2020.3014489
   Zhang L, 2023, NEURAL COMPUT APPL, V35, P10909, DOI 10.1007/s00521-023-08274-w
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
   Zhu LY, 2022, SIGNAL PROCESS, V195, DOI 10.1016/j.sigpro.2022.108489
   Zhu LY, 2022, IEEE T CIRC SYST VID, V32, P4052, DOI 10.1109/TCSVT.2021.3107342
   Zou LM, 2023, IEEE T MULTIMEDIA, V25, P5552, DOI 10.1109/TMM.2022.3194990
NR 54
TC 1
Z9 1
U1 16
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4401
EP 4415
DI 10.1109/TMM.2023.3322316
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100054
DA 2024-08-05
ER

PT J
AU Qi, Q
   Yan, Y
   Wang, HZ
AF Qi, Qiang
   Yan, Yan
   Wang, Hanzi
TI Class-Aware Dual-Supervised Aggregation Network for Video Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video object detection; distillation supervision; graph-guided feature
   aggregation; contrastive supervision
ID NAVIGATION
AB Video object detection has attracted increasing attention in recent years. Although great success has been achieved by off-the-shelf video object detection methods through delicately designing various types of feature aggregation, they overlook the class-aware supervision and thus still suffer from the problem of classification incapability, which means the classification between objects with deteriorated or similar appearances is error-prone. In this article, we propose a novel class-aware dual-supervised aggregation network (CDANet) for video object detection, including three substantial improvements to effectively alleviate the classification incapability problem of previous methods. First, we develop a class-aware cross-modality distillation supervision that transfers the semantic knowledge of label data to the features of video data, effectively enhancing the semantic representations of features. Second, we design a graph-guided feature aggregation module that effectively models the structural relations between features by leveraging the dynamic residual graph convolutional network, enabling our CDANet to perform more effective feature aggregation in the temporal domain. Third, we present a class-aware proposal contrastive supervision to maximize the intra-class agreement and inter-class disagreement, which is conducive to improving the semantic discriminability of features. The class-aware dual supervision and feature aggregation are tightly tied into a unified end-to-end framework to make our CDANet fully exploit class-specific semantic knowledge and inter-frame temporal dependencies to enhance object appearance representations, which facilitates the classification of detected objects. We conduct experiments on the challenging ImageNet VID dataset, and the results demonstrate the superiority of our CDANet against state-of-the-art methods. More remarkably, our CDANet achieves 85.4% mAP with ResNet-101 or 86.5% mAP with ResNeXt-101.
C1 [Qi, Qiang; Yan, Yan; Wang, Hanzi] Xiamen Univ, Sch Informat, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.
   [Qi, Qiang; Yan, Yan; Wang, Hanzi] Xiamen Univ, Key Lab Multimedia Trusted Percept & Efficient Com, Minist Educ China, Xiamen 361005, Peoples R China.
C3 Xiamen University; Xiamen University
RP Wang, HZ (corresponding author), Xiamen Univ, Sch Informat, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.
EM qiangq@stu.xmu.edu.cn; yanyan@xmu.edu.cn; hanzi.wang@xmu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Bertasius G, 2018, LECT NOTES COMPUT SC, V11216, P342, DOI 10.1007/978-3-030-01258-8_21
   Bochkovskiy A., 2020, ARXIV, DOI [10.48550/ARXIV.2004.10934, DOI 10.48550/ARXIV.2004.10934]
   Cai YX, 2021, AAAI CONF ARTIF INTE, V35, P955
   Chen K, 2018, PROC CVPR IEEE, P7814, DOI 10.1109/CVPR.2018.00815
   Chen Y., 2020, CVPR, P10337
   Cheng RJ, 2019, IEEE T CYBERNETICS, V49, P3816, DOI 10.1109/TCYB.2019.2915191
   Cui Y., 2021, P IEEECVF INT C COMP, P8138
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44
   Deng HM, 2019, IEEE I CONF COMP VIS, P6677, DOI 10.1109/ICCV.2019.00678
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JJ, 2021, IEEE T IMAGE PROCESS, V30, P6879, DOI 10.1109/TIP.2021.3099409
   Deng JJ, 2021, IEEE T MULTIMEDIA, V23, P846, DOI 10.1109/TMM.2020.2990070
   Deng JJ, 2019, IEEE I CONF COMP VIS, P7022, DOI 10.1109/ICCV.2019.00712
   Faghri F., 2018, PROC BRIT MACH VIS C, P586
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo CX, 2019, IEEE I CONF COMP VIS, P3908, DOI 10.1109/ICCV.2019.00401
   Han L, 2023, IEEE T MULTIMEDIA, V25, P3681, DOI 10.1109/TMM.2022.3164253
   Han L, 2022, IEEE T CIRC SYST VID, V32, P8165, DOI 10.1109/TCSVT.2021.3094533
   Han L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1469, DOI 10.1145/3394171.3413927
   Han L, 2021, INT J COMPUT VISION, V129, P2927, DOI 10.1007/s11263-021-01507-2
   Han Wei, 2016, SEQ NMS VIDEO OBJECT
   He F., 2022, PROC AAAI C ARTIF IN, P2620
   He F, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108587
   He F, 2020, AAAI CONF ARTIF INTE, V34, P10941
   He K., 2017, P IEEE INT C COMP VI, P2961
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1507, DOI 10.1145/3474085.3475285
   Ji CJ, 2022, IEEE T CYBERNETICS, V52, P12771, DOI 10.1109/TCYB.2021.3088880
   Jiang ZK, 2019, AAAI CONF ARTIF INTE, P8529
   Jin Y., 2022, IEEE Trans. Multimedia
   Kaiwen Duan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P399, DOI 10.1007/978-3-030-58580-8_24
   Kang K, 2018, IEEE T CIRC SYST VID, V28, P2896, DOI 10.1109/TCSVT.2017.2736553
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Lin LJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1855, DOI 10.1145/3394171.3413583
   Lin T., 2020, Advances in Neural Information Processing Systems (NeurIPS), V33, P2351
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Luo H, 2019, Object detection in video with spatial-temporal context aggregation
   Mingfei Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P431, DOI 10.1007/978-3-030-58589-1_26
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pang YW, 2021, IEEE T IMAGE PROCESS, V30, P207, DOI 10.1109/TIP.2020.3034487
   Pang YW, 2017, IEEE T CYBERNETICS, V47, P117, DOI 10.1109/TCYB.2015.2508603
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qi Q., 2023, IEEE Trans. Circuits Syst. Video Technol.
   Qi Q, 2022, IEEE T INTELL TRANSP, V23, P20926, DOI 10.1109/TITS.2022.3176721
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shvets M, 2019, IEEE I CONF COMP VIS, P9755, DOI 10.1109/ICCV.2019.00985
   Sun GX, 2021, AAAI CONF ARTIF INTE, V35, P2620
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P1272, DOI 10.1109/TPAMI.2019.2910529
   Toni L, 2015, IEEE T MULTIMEDIA, V17, P1604, DOI 10.1109/TMM.2015.2450020
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang M, 2018, IEEE T MULTIMEDIA, V20, P620, DOI 10.1109/TMM.2017.2748459
   Wang SY, 2018, LECT NOTES COMPUT SC, V11217, P557, DOI 10.1007/978-3-030-01261-8_33
   Wang X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P816, DOI 10.1145/3474085.3475253
   Wu HP, 2019, IEEE I CONF COMP VIS, P9216, DOI 10.1109/ICCV.2019.00931
   Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677
   Xiao FY, 2018, LECT NOTES COMPUT SC, V11212, P494, DOI 10.1007/978-3-030-01237-3_30
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu C, 2022, IEEE T CIRC SYST VID, V32, P7809, DOI 10.1109/TCSVT.2022.3183646
   Zheng W, 2021, PROC CVPR IEEE, P14489, DOI 10.1109/CVPR46437.2021.01426
   Zhengkai Jiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P18, DOI 10.1007/978-3-030-58517-4_2
   Zhong CL, 2016, IEEE T CYBERNETICS, V46, P3157, DOI 10.1109/TCYB.2015.2498760
   Zhu XZ, 2018, PROC CVPR IEEE, P7210, DOI 10.1109/CVPR.2018.00753
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
   Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441
NR 69
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2109
EP 2123
DI 10.1109/TMM.2023.3292615
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100015
DA 2024-08-05
ER

PT J
AU Shao, SW
   Pei, ZC
   Chen, WH
   Li, R
   Liu, Z
   Li, ZG
AF Shao, Shuwei
   Pei, Zhongcai
   Chen, Weihai
   Li, Ran
   Liu, Zhong
   Li, Zhengguo
TI URCDC-Depth: Uncertainty Rectified Cross-Distillation With CutFlip for
   Monocular Depth Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Monocular depth estimation; cross-distillation; Transformer; data
   augmentation
ID FUSION
AB This work aims to estimate a high-quality depth map from a single RGB image. Due to the lack of depth clues, making full use of the long-range correlation and local information is critical for accurate depth estimation. To this end, we introduce an uncertainty rectified cross-distillation between the Transformer and convolutional neural network (CNN) to achieve a comprehensive depth estimator. Specifically, we utilize the depth estimates from the Transformer branch and CNN branch as pseudo labels to teach each other. At the same time, the pixel-wise depth uncertainty is modeled to mitigate the negative impact of noisy pseudo labels. To avoid the large capacity gap induced by the strong Transformer branch deteriorating the cross-distillation, we transfer the feature maps from the Transformer to the CNN and develop coupling units to assist the weak CNN branch in leveraging the transferred features. Furthermore, we introduce CutFlip, a surprisingly simple yet highly effective data augmentation technique, which forces the model to focus on more valuable depth reasoning clues apart from the vertical image position. Extensive experiments demonstrate that our model, termed URCDC-Depth, exceeds in performance previous state-of-the-art approaches on the KITTI, NYU-Depth-v2 and SUN RGB-D datasets, with no additional computational burden in the evaluation phase. The source code will be publicly available upon acceptance. The source code is available at https://github.com/ShuweiShao/URCDC-Depth.
C1 [Shao, Shuwei; Pei, Zhongcai; Chen, Weihai; Li, Ran; Liu, Zhong] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [Shao, Shuwei; Pei, Zhongcai; Chen, Weihai; Li, Ran; Liu, Zhong] Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310052, Zhejiang, Peoples R China.
   [Li, Zhengguo] ASTAR, Inst Infocomm Res, SRO Dept, Singapore 138632, Singapore.
C3 Beihang University; Beihang University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Chen, WH; Li, R (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.; Chen, WH; Li, R (corresponding author), Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310052, Zhejiang, Peoples R China.
EM swshao@buaa.edu.cn; peizc@buaa.edu.cn; whchen@buaa.edu.cn;
   rnlee1998@buaa.edu.cn; liuzhong@buaa.edu.cn; ezgli@i2r.a-star.edu.sg
RI Liu, Zhirong/K-8703-2012; Qi, Ling/KHE-3068-2024; Liu,
   Yining/KHC-6217-2024; zhou, chen/KBC-4023-2024; Shen, Yan/KEJ-4617-2024;
   Li, Ran/GVT-7550-2022
OI Liu, Yining/0000-0002-2218-2349; Li, Ran/0000-0002-0487-1570; Shao,
   Shuwei/0000-0001-8057-1599; Chen, Weihai/0000-0001-7912-4505; Li,
   Zhengguo/0000-0002-4525-1204
FU National Natural Science Foundation of China
FX No Statement Available
CR Aich S, 2021, IEEE INT CONF ROBOT, P11746, DOI 10.1109/ICRA48506.2021.9560885
   Aleotti F, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21010015
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen PY, 2019, PROC CVPR IEEE, P2619, DOI 10.1109/CVPR.2019.00273
   Chen XT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P694
   Chunmei Liu, 2021, ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education, P354, DOI 10.1145/3482632.3482707
   de Lutio R, 2022, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR52688.2022.00202
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gal Y, 2016, PR MACH LEARN RES, V48
   Garcia NC, 2018, LECT NOTES COMPUT SC, V11212, P106, DOI 10.1007/978-3-030-01237-3_7
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Guizilini V, 2021, PROC CVPR IEEE, P11073, DOI 10.1109/CVPR46437.2021.01093
   Gustafsson FK, 2020, IEEE COMPUT SOC CONF, P1289, DOI 10.1109/CVPRW50498.2020.00167
   Hafner E., 2022, Compat. Vis Image Understanding, V216
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hornauer J, 2022, LECT NOTES COMPUT SC, V13680, P613, DOI 10.1007/978-3-031-20044-1_35
   Hu DT, 2022, LECT NOTES COMPUT SC, V13662, P237, DOI 10.1007/978-3-031-20086-1_14
   Hu J., 2023, PROC INT C KNOWL SCI, P39
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Ishii Y, 2021, Arxiv, DOI arXiv:2107.07684
   Johnston A, 2020, PROC CVPR IEEE, P4755, DOI 10.1109/CVPR42600.2020.00481
   Kendall A., 2017, PROC ADV NEARAL INF
   Kim Doyeon, 2022, arXiv
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Klodt M, 2018, LECT NOTES COMPUT SC, V11214, P713, DOI 10.1007/978-3-030-01249-6_43
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35
   Lee J.H., 2019, arXiv
   Lee S, 2021, AAAI CONF ARTIF INTE, V35, P1873
   Li ZG, 2022, IEEE T IMAGE PROCESS, V31, P6213, DOI 10.1109/TIP.2022.3207571
   Li ZY, 2022, Arxiv, DOI arXiv:2203.14211
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YX, 2021, IEEE ROBOT AUTOM LET, V6, P919, DOI 10.1109/LRA.2021.3052442
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12829, DOI 10.1109/ICCV48922.2021.01261
   Metzger N, 2023, PROC CVPR IEEE, P18237, DOI 10.1109/CVPR52729.2023.01749
   Paszke A., 2017, P 31 C NEUR INF PROC, P1, DOI DOI 10.1017/CB09781107707221.009
   Patil V, 2022, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR52688.2022.00166
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Pilzer A, 2019, PROC CVPR IEEE, P9760, DOI 10.1109/CVPR.2019.01000
   Poggi M, 2020, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR42600.2020.00329
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Saxena A., 2005, PROC ADV NEURAL INJ, P1
   Shao S., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM2022.3224810, DOI 10.1109/TMM2022.3224810]
   Shao SW, 2022, MED IMAGE ANAL, V77, DOI 10.1016/j.media.2021.102338
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song M, 2021, IEEE T CIRC SYST VID, V31, P4381, DOI 10.1109/TCSVT.2021.3049869
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Sun DW, 2019, PROC CVPR IEEE, P6990, DOI 10.1109/CVPR.2019.00716
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian Y., 2019, PROC INT C LEARN REP, P1
   van Dijk T, 2019, IEEE I CONF COMP VIS, P2183, DOI 10.1109/ICCV.2019.00227
   Vaswani A, 2017, ADV NEUR IN, V30
   Wannenwetsch AS, 2017, IEEE I CONF COMP VIS, P1182, DOI 10.1109/ICCV.2017.133
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu Y., 2021, PROC IEEE INT C MULT, P1
   Yang GL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16249, DOI 10.1109/ICCV48922.2021.01596
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   Yuan WH, 2022, PROC CVPR IEEE, P3906, DOI 10.1109/CVPR52688.2022.00389
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao ZX, 2023, Arxiv, DOI arXiv:2303.08942
   Zhao ZX, 2022, PROC CVPR IEEE, P5687, DOI 10.1109/CVPR52688.2022.00561
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou JS, 2019, IEEE I CONF COMP VIS, P6871, DOI 10.1109/ICCV.2019.00697
NR 76
TC 5
Z9 5
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3341
EP 3353
DI 10.1109/TMM.2023.3310259
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200038
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wu, DQ
   Li, HH
   Gu, C
   Liu, H
   Xu, CL
   Hou, YX
   Guo, L
AF Wu, Dongqing
   Li, Huihui
   Gu, Cang
   Liu, Hang
   Xu, Cuili
   Hou, Yinxuan
   Guo, Lei
TI Feature First: Advancing Image-Text Retrieval Through Improved Visual
   Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Feature extraction; Semantics; Task analysis; Mouth;
   Dogs; Aggregates; Graph attention networks; grid feature; image-text
   retrieval; region feature
ID ATTENTION
AB Current image-text retrieval methods mainly utilize region features that provide object-level information to represent images, making the retrieval results more accurate and interpretable. However, there are several issues with region features, such as lack of rich contextual information, loss of object details and risk of detection redundancy. The ideal visual features in image-text retrieval should have three characteristics: object-level, semantically-rich, and language-aligned. To this end, we propose a novel visual representation framework to capture more comprehensive and powerful visual features. Specifically, since these region feature disadvantages are the grid feature advantages, we first build a two-step interaction model to explore the complex relationship between them from the spatial and semantic perspectives to integrate their complementary information, making the fused visual features both object-level and semantic-rich. Then, we design a text-integrated visual embedding module that utilizes textual information as guidance to filter redundant regions, further endowing visual features with language-aligned capabilities. Finally, we develop a multi-attention pooling module to better aggregate these enhanced visual features in a more fine-grained manner. Extensive experiments demonstrate that our proposed model achieves state-of-the-art performance on the benchmark datasets Flickr30K and MS-COCO.
C1 [Wu, Dongqing; Li, Huihui; Gu, Cang; Guo, Lei] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Liu, Hang; Xu, Cuili; Hou, Yinxuan] Northwestern Polytech Univ, Sch Cybersecur, Xian 710072, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Li, HH (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
EM wudongqing@mail.nwpu.edu.cn; lihhui@nwpu.edu.cn; gccg@mail.nwpu.edu.cn;
   liuhang@nwpu.edu.cn; 2268628926@qq.com; houyinxuan@mail.nwpu.edu.cn;
   lguo@nwpu.edu.cn
OI Hou, YinXuan/0009-0000-3344-6664
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Cao J, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P19, DOI 10.1145/3460426.3463615
   Cao QQ, 2023, Arxiv, DOI arXiv:2305.17530
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Dosovitskiyet al A, 2020, PROC INT C LEARN REP
   Duan Y, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P82, DOI 10.1145/3460426.3463650
   Faghri F, 2018, Arxiv, DOI arXiv:1707.05612
   Frome A., 2013, Advances in neural information processing systems, V26
   Ge XR, 2023, IEEE WINT CONF APPL, P1022, DOI 10.1109/WACV56688.2023.00108
   Ge XR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5185, DOI 10.1145/3474085.3475634
   Geng J., 2023, 11THINT C LEARN REPR
   Gong Y, 2023, Arxiv, DOI arXiv:2302.06350
   Guo JY, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3266435
   Guo JY, 2020, AAAI CONF ARTIF INTE, V34, P10885
   Guo JY, 2021, IEEE T CIRC SYST VID, V31, P1114, DOI 10.1109/TCSVT.2020.2996231
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1865, DOI 10.1145/3404835.3463031
   Huang FR, 2019, IEEE T IMAGE PROCESS, V28, P2008, DOI 10.1109/TIP.2018.2882225
   Huang YF, 2023, Arxiv, DOI arXiv:2305.06152
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z, 2019, IEEE I CONF COMP VIS, P5753, DOI 10.1109/ICCV.2019.00585
   Ji Zhong, 2021, P 30 INT JOINT C ART, P765, DOI DOI 10.24963/IJCAI.2021/106
   Jiang Huaizu, 2020, P IEEE CVF C COMP VI, P10267, DOI DOI 10.1109/CVPR42600.2020.01028
   Karpathy A, 2014, ADV NEUR IN, V27
   Kim W, 2021, PR MACH LEARN RES, V139
   Kiros R., 2014, PROC31STINTCONFMACH
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li JH, 2021, ADV NEUR IN, V34
   Li KP, 2023, IEEE T PATTERN ANAL, V45, P641, DOI 10.1109/TPAMI.2022.3148470
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Chunxiao, 2020, CVPR, P10918, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long SQ, 2022, IEEE WINT CONF APPL, P2463, DOI 10.1109/WACV51458.2022.00252
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Qi XF, 2021, NEUROCOMPUTING, V450, P143, DOI 10.1016/j.neucom.2021.03.129
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Radford A, 2021, PR MACH LEARN RES, V139
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Wang T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P12, DOI 10.1145/3343031.3350875
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Wu DQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5055, DOI 10.1145/3503161.3548223
   Wu LX, 2021, IEEE T CIRC SYST VID, V31, P3118, DOI 10.1109/TCSVT.2020.3036860
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu RX, 2022, AAAI CONF ARTIF INTE, P11547
   Xue H., 2021, PROC ADV NEURAL INF, P4514
   Yongzhi Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12783, DOI 10.1109/CVPR42600.2020.01280
   Zeng S, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P239, DOI 10.1145/3512527.3531358
   Zhang HT, 2022, AAAI CONF ARTIF INTE, P3262
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhu J, 2021, IEEE T MULTIMEDIA, V23, P2614, DOI 10.1109/TMM.2020.3013531
NR 65
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3827
EP 3841
DI 10.1109/TMM.2023.3316077
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300004
DA 2024-08-05
ER

PT J
AU Yun, YK
   Lin, WS
AF Yun, Yi Ke
   Lin, Weisi
TI Towards a Complete and Detail-Preserved Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pixel shuffle; preserve details; salient object detection; transformer
ID PYRAMID NETWORK; REFINEMENT
AB Salient Object Detection (SOD) is dominated by Encoder-Decoder networks which involve multi-scale feature fusion and multi-resolution dense supervision. It is prevalent yet problematic to interpolate feature maps or pool ground truth (GT) to fit the size of decoder stages in SOD. Structural properties are unavoidably damaged since pixels are discarded or changed during scaling, resulting in restoration difficulties and poor predictions. Second, it is intuitive and suboptimal to posit the last layer of an encoder as global context, even though it has been widely accepted that high-level encoder features contain global information that contributes to the overall shape of a SOD. To this end, this paper aims to enhance the abovementioned techniques for richer details and a more complete shape. First, we developed a Global Context Branch (GCB) which is a patch-wise supervised SOD on top of the encoder for better global context modeling. Second, we developed a Context Refinement Module (CRM) to improve high/low-level feature fusion and enhance detail reconstruction. Lastly, we adopt Pixel Shuffle (PS) when scaling features and GT maps to preserve structural information. Experiments demonstrated that our proposed framework achieved state-of-the-art performance among all five benchmark datasets against six related existing evaluation metrics.
C1 [Yun, Yi Ke; Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore City 639798, Singapore.
C3 Nanyang Technological University
RP Lin, WS (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore City 639798, Singapore.
EM yu0001ke@e.ntu.edu.sg; wslin@ntu.edu.sg
RI Lin, Wei/D-3353-2012; Yun, Yi Ke/JMB-7801-2023; Lin, Weisi/A-3696-2011
OI Yun, Yi Ke/0000-0002-3237-5363; Lin, Weisi/0000-0001-9866-1947
FU CoE
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Chu XX, 2021, ADV NEUR IN
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai YT, 2021, PROC CVPR IEEE, P6837, DOI 10.1109/CVPR46437.2021.00677
   Dai ZG, 2021, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR46437.2021.00165
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Gao ZT, 2019, IEEE I CONF COMP VIS, P3354, DOI [10.1109/ICCV.2019.00345, 10.1007/s11263-022-01707-4]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He SF, 2017, IEEE I CONF COMP VIS, P1059, DOI 10.1109/ICCV.2017.120
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang NC, 2022, IEEE T IMAGE PROCESS, V31, P6621, DOI 10.1109/TIP.2022.3214092
   Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Khalid M, 2020, 2020 43RD INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P204, DOI [10.1109/tsp49548.2020.9163446, 10.1109/TSP49548.2020.9163446]
   Kim B, 2021, PROC CVPR IEEE, P74, DOI 10.1109/CVPR46437.2021.00014
   Kingma D., 2015, P INT C LEARN REPR S, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Lan Z., 2020, INT C LEARN REPR
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Li Z, 2021, IEEE T IMAGE PROCESS, V30, P4587, DOI 10.1109/TIP.2021.3072811
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu N, 2021, IEEE T IMAGE PROCESS, V30, P5862, DOI 10.1109/TIP.2021.3088282
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Q, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109191
   Liu Y, 2022, IEEE T CYBERNETICS, V52, P6131, DOI 10.1109/TCYB.2021.3051350
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Ma MC, 2021, AAAI CONF ARTIF INTE, V35, P2311
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qiu Y, 2023, IEEE T MULTIMEDIA, V25, P1991, DOI 10.1109/TMM.2022.3141933
   Ren QH, 2021, IEEE T MULTIMEDIA, V23, P1442, DOI 10.1109/TMM.2020.2997178
   Ren SC, 2024, Arxiv, DOI [arXiv:2108.02759, DOI 10.48550/ARXIV.2108.02759]
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Song DW, 2021, IEEE T IMAGE PROCESS, V30, P7567, DOI 10.1109/TIP.2021.3106798
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JQ, 2022, IEEE T PATTERN ANAL, V44, P4674, DOI 10.1109/TPAMI.2021.3074370
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu YH, 2022, IEEE T IMAGE PROCESS, V31, P3125, DOI 10.1109/TIP.2022.3164550
   Wu YH, 2021, IEEE T IMAGE PROCESS, V30, P3897, DOI 10.1109/TIP.2021.3065822
   Wu Z, 2021, IEEE T IMAGE PROCESS, V30, P6226, DOI 10.1109/TIP.2021.3093380
   Wu ZY, 2022, IEEE T IMAGE PROCESS, V31, P6649, DOI 10.1109/TIP.2022.3214332
   Xu BW, 2021, AAAI CONF ARTIF INTE, V35, P3004
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang D., 2020, Adv. Neural Info. Process. Syst., V33, P12236
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang M, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P667, DOI 10.1145/3474085.3475231
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhu H., 2022, arXiv
NR 85
TC 2
Z9 2
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4667
EP 4680
DI 10.1109/TMM.2023.3325731
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100025
DA 2024-08-05
ER

PT J
AU Zhao, L
   Zhou, H
   Zhu, XG
   Song, X
   Li, HS
   Tao, WB
AF Zhao, Lin
   Zhou, Hui
   Zhu, Xinge
   Song, Xiao
   Li, Hongsheng
   Tao, Wenbing
TI LIF-Seg: LiDAR and Camera Image Fusion for 3D LiDAR Semantic
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE LiDAR and Camera; LiDAR segmentation; contextual information; weak
   spatiotemporal synchronization
AB Camera and 3D LiDAR sensors have become indispensable devices in modern autonomous driving vehicles. Camera provides fine-grained texture and color information in 2D space, while LiDAR captures more precise and farther-away distance measurements of the surrounding environments. The complementary information from these two sensors makes the fusion of two modalities a desired option. However, two primary challenges in the fusion of camera and LiDAR hinder its performance, i.e., how to effectively fuse the information from these two modalities and how to precisely align them (suffering from the weak spatiotemporal synchronization problem). This article proposes a coarse-to-fine LiDAR and camera fusion-based network, named LIF-Seg, for LiDAR segmentation. For the first challenge, unlike these previous works fusing the point cloud and image information in a one-to-one manner, the proposed method introduces a simple but effective early-fusion strategy to fully utilize the contextual information of images. Second, to tackle the weak spatiotemporal synchronization problem, an offset rectification approach is designed to align the features of the two modalities. The cooperation of these two components leads to the success of the effective camera-LiDAR fusion. Experimental results on the nuScenes dataset show the superiority of LIF-Seg over existing methods by a large margin. Ablation studies and analyses further illustrate that the LIF-Seg can effectively address the weak spatiotemporal synchronization problem.
C1 [Zhao, Lin; Tao, Wenbing] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China.
   [Zhou, Hui; Song, Xiao] Sensetime Res, Beijing, Peoples R China.
   [Zhu, Xinge; Li, Hongsheng] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
C3 Huazhong University of Science & Technology; Chinese University of Hong
   Kong
RP Tao, WB (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China.
EM linzhao@hust.edu.cn; smarthuizhou@gmail.com; zhuxinge123@gmail.com;
   songxiao@sensetime.com; lihongsheng@gmail.com; wenbingtao@hust.edu.cn
RI Li, Hongsheng/AES-5328-2022
OI Li, Hongsheng/0000-0002-2664-7975; Tao, Wenbing/0000-0003-3284-864X
FU National Natural Science Foundation of China
FX No Statement Available
CR ang H., 2020, EUR C COMP VIS, P685, DOI DOI 10.1007/978-3-030-58604-1_41
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chen K, 2020, IEEE INT C INT ROBOT, P2288, DOI 10.1109/IROS45743.2020.9341450
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen S., 2021, IEEE Trans. Multi-media, V23, P2045
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P3022, DOI 10.1109/TMM.2021.3068606
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gerdzhev M, 2021, IEEE INT CONF ROBOT, P9543, DOI 10.1109/ICRA48506.2021.9562041
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492
   Kochanov D, 2020, P EUR C COMP VIS WOR, P1
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39
   Liong V. E., 2021, P INT JOINT C ART IN, P1
   Liu H., 2020, IEEE Trans. Multimedia, P1
   Liu ZJ, 2019, ADV NEUR IN, V32
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Pang S, 2020, IEEE INT C INT ROBOT, P10386, DOI 10.1109/IROS45743.2020.9341791
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Ran C, 2021, IEEE INT CONF ROBOT, P14040, DOI 10.1109/ICRA48506.2021.9561305
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Tengteng Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P35, DOI 10.1007/978-3-030-58555-6_3
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vora S, 2020, PROC CVPR IEEE, P4603, DOI 10.1109/CVPR42600.2020.00466
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Yue, 2020, ECCV
   Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI [10.1109/ICRA.2019.8793495, 10.1109/icra.2019.8793495]
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xie L, 2020, AAAI CONF ARTIF INTE, V34, P12460
   Yan X, 2021, AAAI CONF ARTIF INTE, V35, P3101
   Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9598, DOI 10.1109/CVPR42600.2020.00962
   Yoo JH, 2020, SELECTED PAPERS FROM THE NINETEENTH BIENNIAL IEEE CONFERENCE ON ELECTROMAGNETIC FIELD COMPUTATION (IEEE CEFC 2020), DOI [10.1007/978-3-030-58583-9_43, 10.1109/CEFC46938.2020.9451336]
   Zhang J., 2020, Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIV 16, P644
   Zhang JZ, 2020, PROC CVPR IEEE, P4533, DOI 10.1109/CVPR42600.2020.00459
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhao L, 2023, IEEE T CIRC SYST VID, V33, P1854, DOI 10.1109/TCSVT.2022.3218076
   Zhao L, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14184471
   Zhao L, 2020, AAAI CONF ARTIF INTE, V34, P12951
   Zhou Yin, 2020, CORL
   Zhu XG, 2021, PROC CVPR IEEE, P9934, DOI 10.1109/CVPR46437.2021.00981
NR 50
TC 23
Z9 24
U1 36
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1158
EP 1168
DI 10.1109/TMM.2023.3277281
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700009
OA Green Submitted
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Zhong, YH
   Yang, GX
   Zhong, DD
   Yang, X
   Wang, SS
AF Zhong, Yuanhong
   Yang, Guangxia
   Zhong, Daidi
   Yang, Xun
   Wang, Shanshan
TI Frame-Padded Multiscale Transformer for Monocular 3D Human Pose
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Three-dimensional displays; Feature extraction; Pose
   estimation; Pipelines; Correlation; Image restoration; 3D human pose
   estimation; temporal attention; transformer; frame padding; monocular
AB Monocular 3D human pose estimation is an ill-posed problem in computer vision due to its depth ambiguity. Most existing works supplement the depth information by extracting temporal pose features from video frames, and they have made notable progress. However, these approaches divide a long sequence of video frames into multiple short sequences for separate processing, which leads to the loss of complementary information between sequences. Furthermore, the short-term temporal correlation among frames in a sequence is often not fully exploited. To model temporal dependencies efficiently, we propose the frame-padded multiscale transformer approach, which includes a frame-padded video sequence preprocessing step and a multiscale temporal transformer backbone. Our approach addresses the omission of the temporal features of edge frames in existing approaches by padding video frames in the shallow layer. In addition, we extract the temporal information of 3D human poses using a multiscale transformer to enhance the short-term correlation of human pose skeleton keypoints. Extensive experiments validate the effectiveness of our approach on two popular datasets: Human3.6M and MPI-INF-3DHP. The results show that our approach achieves state-of-the-art performance.
C1 [Zhong, Yuanhong; Yang, Guangxia] Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
   [Zhong, Daidi] Chongqing Univ, Bioengn Coll, Chongqing 400044, Peoples R China.
   [Yang, Xun] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Wang, Shanshan] Anhui Univ, Inst Phys Sci, Hefei 230039, Peoples R China.
   [Wang, Shanshan] Anhui Univ, Inst Informat Technol, Hefei 230039, Peoples R China.
C3 Chongqing University; Chongqing University; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; Chinese Academy of
   Sciences; Hefei Institutes of Physical Science, CAS; Anhui University;
   Anhui University
RP Zhong, YH (corresponding author), Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
EM zhongyh@cqu.edu.cn; ygx777@cqu.edu.cn; daidi.zhong@cqu.edu.cn;
   xyang21@ustc.edu.cn; wang.shanshan@ahu.edu.cn
OI YANG, Xun/0000-0003-0201-1638; Wang, Shanshan/0000-0002-3824-687X;
   Zhong, Yuanhong/0000-0001-5689-1146; Zhong, Daidi/0000-0002-9879-4915
FU National Key Research and Development Program of China
FX No Statement Available
CR Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen HY, 2023, Arxiv, DOI arXiv:2302.01825
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Einfalt M, 2023, IEEE WINT CONF APPL, P2902, DOI 10.1109/WACV56688.2023.00292
   Fang HS, 2023, IEEE T PATTERN ANAL, V45, P7157, DOI 10.1109/TPAMI.2022.3222784
   Ghafoor M, 2023, IEEE T MULTIMEDIA, V25, P3311, DOI 10.1109/TMM.2022.3158068
   Gong J, 2022, PROC CVPR IEEE, P11069, DOI 10.1109/CVPR52688.2022.01080
   Gong KH, 2021, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR46437.2021.00847
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Hauptmann Alexander, 2023, ROBUST AUTOMATIC DET
   He JY, 2023, Arxiv, DOI arXiv:2303.17144
   He JY, 2021, NEUROCOMPUTING, V444, P319, DOI 10.1016/j.neucom.2020.05.118
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Ionescu C, 2011, IEEE I CONF COMP VIS, P2220, DOI 10.1109/ICCV.2011.6126500
   Jin L, 2023, IEEE T MULTIMEDIA, V25, P3364, DOI 10.1109/TMM.2022.3159111
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kingma D. P., 2014, arXiv
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kundu JN, 2020, PROC CVPR IEEE, P6151, DOI 10.1109/CVPR42600.2020.00619
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li H., 2023, P AAAI C ART INT WAS, VVolume 37, P1296
   Li SC, 2020, PROC CVPR IEEE, P6172, DOI 10.1109/CVPR42600.2020.00621
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Li WH, 2022, PROC CVPR IEEE, P13137, DOI 10.1109/CVPR52688.2022.01280
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Liu Hanbing, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P5542, DOI 10.1145/3581783.3612368
   Liu HB, 2024, Arxiv, DOI arXiv:2309.01365
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1021, DOI 10.1109/TMM.2020.2991532
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Shan WK, 2022, LECT NOTES COMPUT SC, V13665, P461, DOI 10.1007/978-3-031-20065-6_27
   Sutskever I., 2014, Advances in Neural Information Processing Systems, P3104
   Tu SY, 2023, Arxiv, DOI [arXiv:2304.10465, 10.48550/arXiv.2304.10465]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang R, 2022, NEURAL PROCESS LETT, V54, P3941, DOI 10.1007/s11063-022-10794-w
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Xia GY, 2018, IEEE T IMAGE PROCESS, V27, P3011, DOI 10.1109/TIP.2018.2812100
   Xu JW, 2020, PROC CVPR IEEE, P896, DOI 10.1109/CVPR42600.2020.00098
   Xu TH, 2021, PROC CVPR IEEE, P16100, DOI 10.1109/CVPR46437.2021.01584
   Xue YZ, 2022, IEEE T IMAGE PROCESS, V31, P4278, DOI 10.1109/TIP.2022.3182269
   Yeh RA, 2019, ADV NEUR IN, V32
   Zeng AL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11416, DOI 10.1109/ICCV48922.2021.01124
   Zhang HR, 2023, IEEE T MULTIMEDIA, V25, P3868, DOI 10.1109/TMM.2022.3167887
   Zhang JL, 2022, PROC CVPR IEEE, P13222, DOI 10.1109/CVPR52688.2022.01288
   Zhao QT, 2023, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR52729.2023.00857
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zhou YX, 2024, Arxiv, DOI arXiv:2305.11468
   Zhou YX, 2023, Arxiv, DOI arXiv:2211.09590
NR 62
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6191
EP 6201
DI 10.1109/TMM.2023.3347095
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600057
DA 2024-08-05
ER

PT J
AU Zhou, X
   Miao, CY
AF Zhou, Xin
   Miao, Chunyan
TI Disentangled Graph Variational Auto-Encoder for Multimodal
   Recommendation With Interpretability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Numerical models; Vectors; Visualization; Image reconstruction;
   Transformers; Semantics; Mutual information; Multimodal recommendation;
   variational auto-encoder; disentangled learning; interpretability
ID NETWORKS
AB Multimodal recommender systems amalgamate multimodal information (e.g., textual descriptions, images) into a collaborative filtering framework to provide more accurate recommendations. While the incorporation of multimodal information could enhance the interpretability of these systems, current multimodal models represent users and items utilizing entangled numerical vectors, rendering them arduous to interpret. To address this, we propose a Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both model and recommendation interpretability. DGVAE initially projects multimodal information into textual contents, such as converting images to text, by harnessing state-of-the-art multimodal pre-training technologies. It then constructs a frozen item-item graph and encodes the contents and interactions into two sets of disentangled representations utilizing a simplified residual graph convolutional network. DGVAE further regularizes these disentangled representations through mutual information maximization, aligning the representations derived from the interactions between users and items with those learned from textual content. This alignment facilitates the interpretation of user binary interactions via text. Our empirical analysis conducted on three real-world datasets demonstrates that DGVAE significantly surpasses the performance of state-of-the-art baselines by a margin of 10.02%. We also furnish a case study from a real-world dataset to illustrate the interpretability of DGVAE.
C1 [Zhou, Xin] Nanyang Technol Univ, Alibaba NTU Singapore Joint Res Inst, Singapore 639798, Singapore.
   [Miao, Chunyan] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Miao, CY (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM xin.zhou@ntu.edu.sg; ascymiao@ntu.edu.sg
OI Zhou, Xin/0000-0003-0948-8033
FU Alibaba-NTU Singapore Joint Research Institute
FX No Statement Available
CR Alwassel H., 2020, NEURIPS, V33, P9758
   Arandjelovic R., 2018, P EUROPEAN C COMPUTE, P435
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Arpit D, 2017, PR MACH LEARN RES, V70
   Askari B, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P2061, DOI 10.1145/3404835.3462986
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cheng H., 2021, PROC INT C LEARN REP
   Cuturi M., 2013, Ad-vances in Neural Information Processing Systems, V26, P1
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Han B, 2018, ADV NEUR IN, V31
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P9595, DOI 10.1109/TPAMI.2023.3247939
   Hu P, 2021, PROC CVPR IEEE, P5399, DOI 10.1109/CVPR46437.2021.00536
   Huang JC, 2019, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2019.00342
   Huang Z., 2021, Advances in Neural Informa- tion Processing Systems (NeurIPS-21), P29406
   Kong QQ, 2020, IEEE-ACM T AUDIO SPE, V28, P2880, DOI 10.1109/TASLP.2020.3030497
   Koren Y, 2008, KDD, P426
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li J., 2019, PROC INT C LEARN REP
   Li JC, 2022, LECT NOTES COMPUT SC, V13684, P128, DOI 10.1007/978-3-031-20053-3_8
   Li L, 2022, PROC CVPR IEEE, P18847, DOI 10.1109/CVPR52688.2022.01830
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li SK, 2022, PROC CVPR IEEE, P316, DOI 10.1109/CVPR52688.2022.00041
   Li SK, 2020, AAAI CONF ARTIF INTE, V34, P4667
   Li XP, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P305, DOI 10.1145/3097983.3098077
   Li YF, 2021, AAAI CONF ARTIF INTE, V35, P8547
   Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150
   Liu JY, 2019, IEEE T MULTIMEDIA, V21, P887, DOI 10.1109/TMM.2018.2871418
   Ma F, 2022, IEEE T NEUR NET LEAR, V33, P6275, DOI 10.1109/TNNLS.2021.3073248
   Ma JX, 2019, ADV NEUR IN, V32
   Ma X., 2020, INT C MACHINE LEARNI, P6543
   Morgado P, 2021, PROC CVPR IEEE, P12470, DOI 10.1109/CVPR46437.2021.01229
   Panda R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7556, DOI 10.1109/ICCV48922.2021.00748
   Petridis S, 2016, IEEE T AFFECT COMPUT, V7, P45, DOI 10.1109/TAFFC.2015.2446462
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ren MY, 2018, PR MACH LEARN RES, V80
   Rouditchenko A, 2019, INT CONF ACOUST SPEE, P2357, DOI 10.1109/icassp.2019.8682467
   Shu J, 2019, ADV NEUR IN, V32
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun ZR, 2022, PROC CVPR IEEE, P5301, DOI 10.1109/CVPR52688.2022.00524
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian YP, 2021, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR46437.2021.00555
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Wang W., 2020, P IEEE CVF C COMP VI, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie SN, 2018, Arxiv, DOI [arXiv:1712.04851, DOI 10.48550/ARXIV.1712.04851]
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang MX, 2021, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR46437.2021.00119
   Yao JC, 2019, IEEE T IMAGE PROCESS, V28, P1909, DOI 10.1109/TIP.2018.2877939
   Yi J, 2022, IEEE T MULTIMEDIA, V24, P1067, DOI 10.1109/TMM.2021.3111487
   Yuan X, 2021, PROC CVPR IEEE, P6991, DOI 10.1109/CVPR46437.2021.00692
   Zhang CY, 2022, IEEE T MULTIMEDIA, V24, P1198, DOI 10.1109/TMM.2021.3134156
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang Y., 2021, PROC INT C LEARN REP
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zheng GQ, 2021, AAAI CONF ARTIF INTE, V35, P11053
   Zhu YC, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P2379, DOI 10.1145/3485447.3512110
NR 63
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7543
EP 7554
DI 10.1109/TMM.2024.3369875
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000053
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Gao, YF
   Xie, YH
   Hu, ZZ
   Chen, TS
   Lin, L
AF Gao, Yuefang
   Xie, Yuhao
   Hu, Zeke Zexi
   Chen, Tianshui
   Lin, Liang
TI Adaptive Global-Local Representation Learning and Selection for
   Cross-Domain Facial Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain adaptation; adverserial learning; Pseudo label generation; Facial
   expression recognition
AB Domain shift poses a significant challenge in Cross-Domain Facial Expression Recognition (CD-FER) due to the distribution variation between the source and target domains. Current algorithms mainly focus on learning domain-invariant features through global feature adaptation, while neglecting the transferability of local features across different domains. Additionally, these algorithms lack discriminative supervision during training on target datasets, resulting in deteriorated feature representation in the target domain. To address these limitations, we propose an Adaptive Global-Local Representation Learning and Selection (AGLRLS) framework. The framework incorporates global-local adversarial adaptation and semantic-aware pseudo label generation to enhance the learning of domain-invariant and discriminative feature representation during training. Meanwhile, a global-local prediction consistency learning is introduced to improve classification results during inference. Specifically, the framework consists of separate global-local adversarial learning modules that learn domain-invariant global and local features independently. We also design a semantic-aware pseudo label generation module, which computes semantic labels based on global and local features. Moreover, a novel dynamic threshold strategy is employed to learn the optimal thresholds by leveraging independent prediction of global and local features, ensuring filtering out the unreliable pseudo labels while retaining reliable ones. These labels are utilized for model optimization through the adversarial learning process in an end-to-end manner. During inference, a global-local prediction consistency module is developed to automatically learn an optimal result from multiple predictions. To validate the effectiveness of our framework, we conduct comprehensive experiments and analysis based on a fair evaluation benchmark. The results demonstrate that the proposed framework outperforms the current competing methods by a substantial margin.
C1 [Gao, Yuefang] South China Agr Univ, Coll Math & Informat, Guangzhou 510642, Peoples R China.
   [Xie, Yuhao] Xidian Univ, Guangzhou Inst Technol, Guangzhou 510555, Peoples R China.
   [Hu, Zeke Zexi] Univ Sydney, Sch Comp Sci, Darlington, NSW 2050, Australia.
   [Chen, Tianshui] Guangdong Univ Technol, Guangzhou 510006, Peoples R China.
   [Lin, Liang] Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.
C3 South China Agricultural University; Xidian University; University of
   Sydney; Guangdong University of Technology; Sun Yat Sen University
RP Chen, TS (corresponding author), Guangdong Univ Technol, Guangzhou 510006, Peoples R China.
EM gaoyuefang@scau.edu.cn; yaoxie1001@gmail.com; zexi.hu@sydney.edu.au;
   tianshuichen@gmail.com; linliang@ieee.org
RI ; Liang, Lin/IQR-8601-2023
OI Chen, Tianshui/0000-0002-5848-5624; Liang, Lin/0000-0003-2248-3755
FU National Key Ramp;D Program of China
FX No Statement Available
CR Bozorgtabar B, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107111
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P9887, DOI 10.1109/TPAMI.2021.3131222
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Conti A., 2022, PROC 3 BRIT MACH VIS, P1
   de Carvalho M, 2022, KNOWL-BASED SYST, V253, DOI 10.1016/j.knosys.2022.109486
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Fatras K, 2021, PR MACH LEARN RES, V139
   Friedman M, 1937, J AM STAT ASSOC, V32, P675, DOI 10.2307/2279372
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang W, 2021, IEEE T MULTIMEDIA, V24, P3327, DOI 10.1109/TMM.2021.3096068
   Ji YL, 2023, IEEE T KNOWL DATA EN, V35, P4190, DOI 10.1109/TKDE.2021.3136606
   Ji YL, 2019, NEUROCOMPUTING, V333, P231, DOI 10.1016/j.neucom.2018.12.037
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P881, DOI 10.1109/TAFFC.2020.2973158
   Li S, 2018, INT C PATT RECOG, P3092, DOI 10.1109/ICPR.2018.8545284
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3312, DOI 10.1145/3474085.3475484
   Li YJ, 2023, IEEE T MULTIMEDIA, V25, P1359, DOI 10.1109/TMM.2022.3141604
   Li YJ, 2017, Arxiv, DOI arXiv:1511.05493
   Liang G., 2020, arXiv
   Long MS, 2018, ADV NEUR IN, V31
   Lu C, 2022, IEEE-ACM T AUDIO SPE, V30, P2217, DOI 10.1109/TASLP.2022.3178232
   Lucey P., 2010, 2010 IEEE COMP SOC C, P94, DOI DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Meng M, 2023, IEEE T MULTIMEDIA, V25, P2266, DOI 10.1109/TMM.2022.3145235
   Mengxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13933, DOI 10.1109/CVPR42600.2020.01395
   Miao YQ, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P326, DOI 10.1109/ICMLA.2012.178
   Ni TG, 2021, IEEE T COMPUT SOC SY, V8, P1213, DOI 10.1109/TCSS.2020.3013938
   Pu T, 2024, IEEE T IMAGE PROCESS, V33, P556, DOI 10.1109/TIP.2023.3345652
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sun Z, 2022, J VIS COMMUN IMAGE R, V85, DOI 10.1016/j.jvcir.2022.103458
   Tamura R., 1968, Mem. Shimane Univ., V1, P1
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang C., 2022, PROC ASIAN C COMPUT, P324
   Wang JH, 2022, IEEE T PATTERN ANAL, V44, P6264, DOI 10.1109/TPAMI.2021.3088859
   Wang XQ, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7208794
   Wang ZX, 2023, IEEE T PATTERN ANAL, V45, P15462, DOI 10.1109/TPAMI.2023.3315753
   Wen GH, 2020, IEEE T MULTIMEDIA, V22, P2914, DOI 10.1109/TMM.2020.2966858
   Wen J, 2019, AAAI CONF ARTIF INTE, P5401
   Xia KJ, 2021, IEEE T INTELL TRANSP, V22, P1765, DOI 10.1109/TITS.2020.2987724
   Xie Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1255, DOI 10.1145/3394171.3413822
   Xie YH, 2022, INT C PATT RECOG, P2489, DOI 10.1109/ICPR56361.2022.9956069
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Yan HB, 2016, NEUROCOMPUTING, V208, P165, DOI 10.1016/j.neucom.2015.11.113
   Yan KY, 2016, LECT NOTES COMPUT SC, V9948, P427, DOI 10.1007/978-3-319-46672-9_48
   Yang HY, 2018, IEEE INT CONF AUTOMA, P294, DOI 10.1109/FG.2018.00050
   Zavarez MV, 2017, SIBGRAPI, P405, DOI 10.1109/SIBGRAPI.2017.60
   Zhang BW, 2021, 35 C NEURAL INFORM P, V34
   Zhang T, 2022, IEEE T KNOWL DATA EN, V34, P544, DOI 10.1109/TKDE.2020.2985365
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang ZP, 2018, INT J COMPUT VISION, V126, P550, DOI 10.1007/s11263-017-1055-1
   Zhao R, 2021, INT C PATT RECOG, P4412, DOI 10.1109/ICPR48806.2021.9413000
   Zheng WM, 2018, IEEE T AFFECT COMPUT, V9, P21, DOI 10.1109/TAFFC.2016.2563432
   Zhu RH, 2016, INT CONF BIOMETR
   Zong Y, 2018, IEEE T IMAGE PROCESS, V27, P2484, DOI 10.1109/TIP.2018.2797479
   Zou XY, 2022, LECT NOTES COMPUT SC, V13679, P683, DOI 10.1007/978-3-031-19800-7_40
   Zou XY, 2022, AAAI CONF ARTIF INTE, P5367
NR 63
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6676
EP 6688
DI 10.1109/TMM.2024.3355637
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600004
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Hu, BY
   Liu, JW
   Zheng, KC
   Zha, ZJ
AF Hu, Bingyu
   Liu, Jiawei
   Zheng, Kecheng
   Zha, Zheng-Jun
TI Unleashing Knowledge Potential of Source Hypothesis for Source-Free
   Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Adaptation models; Feature extraction; Task analysis; Data
   models; Semantics; Transfer learning; Domain adaptation; hypothesis
   transfer; privacy protection; vision transformer
AB Source-Free Domain Adaptation (SFDA) task aims to transfer knowledge from a labeled source domain to a label-scarce target domain, in which the source data can not be accessed but only a pre-trained source model and unlabeled target data are available during adaptation. Previous methods for source model adaptation rely on hypothesis transfer learning that trains the feature extractor to learn target features aligned to the distribution of source features while freezing the source classifier. However, reusing only the source classifier without exploring the comprehensive knowledge of the source model can lead to biased feature alignment. To this end, we propose a novel method called Transformer-bAsed thorouGh Source HypOthesis Transfer (TagSHOT) framework to effectively unleash the thorough knowledge potential of pre-trained source hypothesis. Specifically, our approach delves into the correlation coefficient among CLS/patch tokens across different Transformer layers, uncovering the concealed insights within the pre-trained source model and constructing a comprehensive source hypothesis. By tailoring the target feature alignment to this thorough source hypothesis, our model facilitates the adaptation of a broader range of classification-related knowledge to the target domain. Furthermore, we introduce a Salient Token Extension (STE) module, designed to capture the target-specific discriminative information by propagating the salient information among tokens. This mechanism enriches our model's ability to understand and incorporate target-specific nuances. Extensive experiments have been conducted to validate the effectiveness of our method, which outperforms state-of-the-art approaches by a large margin.
C1 [Hu, Bingyu; Liu, Jiawei; Zha, Zheng-Jun] Univ Sci & Technol China, Hefei 230000, Peoples R China.
   [Zheng, Kecheng] Zhejiang Univ, Hangzhou 310000, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Zhejiang University
RP Liu, JW (corresponding author), Univ Sci & Technol China, Hefei 230000, Peoples R China.
EM hby0728@mail.ustc.edu.cn; jwliu6@ustc.edu.cn; zkcys001@mail.ustc.edu.cn;
   zhazj@ustc.edu.cn
OI Hu, Bingyu/0009-0007-6482-2803
FU National Natural Science Foundation of China
FX No Statement Available
CR Antoun W., 2020, P 4 WORKSHOP OPEN SO, P9
   Carion N., 2020, EUR C COMP VIS, P213
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Csurka G, 2016, LECT NOTES COMPUT SC, V9915, P458, DOI 10.1007/978-3-319-49409-8_37
   d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding N, 2022, PROC CVPR IEEE, P7202, DOI 10.1109/CVPR52688.2022.00707
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gupta A, 2022, PROC CVPR IEEE, P9225, DOI 10.1109/CVPR52688.2022.00902
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou YZ, 2021, PROC CVPR IEEE, P13819, DOI 10.1109/CVPR46437.2021.01361
   Hu W., 2017, P 34 INT C MACHINE L, P1558
   Huang JX, 2021, ADV NEUR IN, V34
   Jang E., 2016, INT C LEARN REPR
   Jinyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P480, DOI 10.1007/978-3-030-58583-9_29
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kundu JN, 2020, PROC CVPR IEEE, P4543, DOI 10.1109/CVPR42600.2020.00460
   Kurmi VK, 2021, IEEE WINT CONF APPL, P615, DOI 10.1109/WACV48630.2021.00066
   Kuzborskij I., 2013, P 30 INT C MACH LEAR, P942
   Li Rui, 2020, P IEEE CVF C COMP VI, P9641
   Liang J., 2020, International Conference on Machine Learning, P6028
   Liu DN, 2023, IEEE T MULTIMEDIA, V25, P1333, DOI 10.1109/TMM.2022.3141614
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long Z., 2018, Adv. Neural Inf. Process. Syst., V31
   Paul S, 2022, AAAI CONF ARTIF INTE, P2071
   Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271
   Qiu Z., 2021, PROC INT JOINT C ART
   Raghu M, 2021, ADV NEUR IN, V34
   Rahman A, 2021, BIOMED PHYS ENG EXPR, V7, DOI 10.1088/2057-1976/ac0e74
   Rao Yongming, 2021, Advances in neural information processing systems, V34
   Roy S, 2019, PROC CVPR IEEE, P9463, DOI 10.1109/CVPR.2019.00970
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shao Z., 2022, Trans. Mach. Learn. Res.
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang YH, 2022, PROC CVPR IEEE, P12155, DOI 10.1109/CVPR52688.2022.01185
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Verma A, 2023, IEEE T MULTIMEDIA, V25, P364, DOI 10.1109/TMM.2021.3126404
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Xia HF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8990, DOI 10.1109/ICCV48922.2021.00888
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xu W., 2021, INT C LEARN REPRESEN
   Yang G., 2021, arXiv
   Yang SQ, 2021, ADV NEUR IN, V34
   Yang SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8958, DOI 10.1109/ICCV48922.2021.00885
   Yang Y. Wang, 2023, Comput. Vis.Image Understanding, V234
   Youngeun Kim, 2021, IEEE Transactions on Artificial Intelligence, V2, P508, DOI 10.1109/TAI.2021.3110179
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Zhai XH, 2022, PROC CVPR IEEE, P12094, DOI 10.1109/CVPR52688.2022.01179
   Zhang B, 2022, IEEE T MULTIMEDIA, V24, P4102, DOI 10.1109/TMM.2021.3114550
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
   Zhang ZS, 2022, IEEE T PATTERN ANAL, V44, P3285, DOI 10.1109/TPAMI.2020.3046683
   Zhou HY, 2021, IEEE INT CONF COMP V, P2230, DOI 10.1109/ICCVW54120.2021.00252
NR 68
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5422
EP 5434
DI 10.1109/TMM.2023.3333190
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600014
DA 2024-08-05
ER

PT J
AU Ji, C
   Gao, GY
   Shi, YQ
AF Ji, Chi
   Gao, Guangyong
   Shi, Yun-Qing
TI Reversible Data Hiding in Encrypted Images With Adaptive Huffman Code
   Based on Dynamic Prediction Axes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive Huffman coding; dynamic prediction axes; high pure capacity;
   multidirectional median edge detector; reversible data hiding in
   encrypted images
ID RECOVERY
AB With the development of data security and privacy requirements in the field of cloud computing, Reversible Data Hiding in Encrypted Images (RDHEI) in encryption domain has received increasing attention. In order to take full advantage of the spatial and textural features of the original image, reversible data hiding in encrypted image with adaptive Huffman code based on Dynamic Prediction Axes (RDHEI-HDA) is proposed. First, the prediction errors of the original plaintext image are calculated according to the multidirectional median edge detector (M-MED) combined with the Dynamic Prediction Axes which are generated by the spatial correlation of the original image. After encryption process with the stream cipher, the adaptive Huffman coding labeling rule is created for pixel labeling and classification according to the Dynamic Prediction Axes and the distribution of prediction errors. Finally, bit substitution is employed to insert secret data and side information into the image. In Comparison to most of the state-of-the-art RDHEI methods, the experimental results show that the RDHEI-HDA method provides a higher pure payload while ensuring safety.
C1 [Ji, Chi; Gao, Guangyong] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
   [Gao, Guangyong] Zhengzhou Xinda Inst Adv Technol, Zhengzhou 450003, Peoples R China.
   [Gao, Guangyong] Jiujiang Univ, Sch Informat Sci & Technol, Jiujiang 332005, Peoples R China.
   [Shi, Yun-Qing] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
C3 Nanjing University of Information Science & Technology; Jiujiang
   University; New Jersey Institute of Technology
RP Gao, GY (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
EM ji_chi@163.com; gaoguangyong@163.com; shi@njit.edu
FU National Natural Science Foundation of China
FX No Statement Available
CR Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas P., 2017, Image database of bows-2, V20
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chen KM, 2023, IEEE T DEPEND SECURE, V20, P4519, DOI 10.1109/TDSC.2022.3228385
   Chen SS, 2022, J INF SECUR APPL, V69, DOI 10.1016/j.jisa.2022.103297
   Chunqiang Yu, 2022, IEEE Transactions on Circuits and Systems for Video Technology, V32, P451, DOI 10.1109/TCSVT.2021.3062947
   Ding WJ, 2022, PATTERN RECOGN LETT, V159, P116, DOI 10.1016/j.patrec.2022.05.014
   Fan GJ, 2021, SIGNAL PROCESS, V180, DOI 10.1016/j.sigpro.2020.107888
   Gao GY, 2023, DIGIT SIGNAL PROCESS, V133, DOI 10.1016/j.dsp.2022.103870
   Hu RW, 2021, IEEE SIGNAL PROC LET, V28, P464, DOI 10.1109/LSP.2021.3059202
   Hua ZY, 2023, IEEE T DEPEND SECURE, V20, P3669, DOI 10.1109/TDSC.2022.3218570
   Huang DL, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115632
   Khelifi F, 2018, SIGNAL PROCESS, V143, P336, DOI 10.1016/j.sigpro.2017.09.020
   Li FY, 2022, INFORM SCIENCES, V595, P142, DOI 10.1016/j.ins.2022.02.040
   Li Q, 2021, APPL SOFT COMPUT, V110, DOI 10.1016/j.asoc.2021.107618
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Malik A, 2019, J INF SECUR APPL, V48, DOI 10.1016/j.jisa.2019.102374
   Mohammadi A, 2020, IEEE T CIRC SYST VID, V30, P2366, DOI 10.1109/TCSVT.2020.2990952
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Peng F, 2014, DIGIT SIGNAL PROCESS, V25, P255, DOI 10.1016/j.dsp.2013.11.002
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qin C, 2017, SIGNAL PROCESS, V138, P280, DOI 10.1016/j.sigpro.2017.03.033
   Qiu YQ, 2022, IEEE T CIRC SYST VID, V32, P5874, DOI 10.1109/TCSVT.2022.3163905
   Qu LF, 2022, IEEE T MULTIMEDIA, V24, P2924, DOI 10.1109/TMM.2021.3090588
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Tang ZJ, 2019, IEEE T KNOWL DATA EN, V31, P549, DOI 10.1109/TKDE.2018.2837745
   Wang YM, 2022, IEEE T MULTIMEDIA, V24, P1288, DOI 10.1109/TMM.2021.3062699
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Weng SW, 2023, IEEE T MULTIMEDIA, V25, P5747, DOI 10.1109/TMM.2022.3198877
   Weng SW, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2020.102932
   Weng SW, 2021, INFORM SCIENCES, V549, P13, DOI 10.1016/j.ins.2020.10.063
   Wu YQ, 2020, IEEE T MULTIMEDIA, V22, P1929, DOI 10.1109/TMM.2019.2952979
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yin ZX, 2022, IEEE T DEPEND SECURE, V19, P992, DOI 10.1109/TDSC.2020.3019490
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Yu CQ, 2022, SIGNAL PROCESS, V196, DOI 10.1016/j.sigpro.2022.108527
   Yu CQ, 2022, INFORM SCIENCES, V584, P89, DOI 10.1016/j.ins.2021.10.050
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang Y, 2021, INFORM SCIENCES, V564, P306, DOI 10.1016/j.ins.2021.02.058
   Zhou N, 2020, IEEE ACCESS, V8, P81412, DOI 10.1109/ACCESS.2020.2990903
NR 43
TC 0
Z9 0
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5962
EP 5975
DI 10.1109/TMM.2023.3342700
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100025
DA 2024-08-05
ER

PT J
AU Li, JN
   Liu, XQ
   Luo, X
   Xu, XS
AF Li, Jia-Nan
   Liu, Xiao-Qian
   Luo, Xin
   Xu, Xin-Shun
TI VOLTER: Visual Collaboration and Dual-Stream Fusion for Scene Text
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene text recognition; vision model; multi-modal fusion
ID SINGLE-IMAGE SUPERRESOLUTION
AB Recently, the approaches of linguistic modeling for scene text recognition have gradually become mainstream, mainly consisting of a vision model (VM), a language model (LM), and an optional fusion module. These methods typically use LM and fusion modules to refine the results of VM-based predictions iteratively. However, the VM mainly consists of a Transformer on top of ResNet. It means the attention mechanism is only applied to the high layer of the VM, ignoring the internal image dependencies in the dense features at multiple scales. Therefore, the results in the VM become the performance bottleneck. Meanwhile, the visual and language features of these methods reside in their own space. In this way, it ignores the alignment before fusion, leading to a failure to achieve maximum information interaction. To address these issues, we propose Visual cOllaboration and duaL-stream fusion for scene TExt Recognition, VOLTER for short. Firstly, a multi-stage Local-Global Collaboration Vision Model (LGC-VM) is constructed to focus on both local and global features at multiple scales, breaking vision bottlenecks to provide a better vision prediction. Secondly, to explicitly align the feature space of VM and LM, we introduce a Vision-Language Contrastive (VLC) module by encouraging positive vision-language pairs to have similar representations. Moreover, a Dual-Stream Feature Enhancement (DSFE) module is proposed for the unidirectional interaction of visual-language features to alleviate the alignment problem of different modalities and execute fusion further. Extensive experiments on benchmark datasets demonstrate that the proposed framework can achieve state-of-the-art performance.
C1 [Li, Jia-Nan; Liu, Xiao-Qian; Luo, Xin; Xu, Xin-Shun] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Xu, Xin-Shun] Quan Cheng Lab, Jinan 28666, Peoples R China.
C3 Shandong University
RP Xu, XS (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.; Xu, XS (corresponding author), Quan Cheng Lab, Jinan 28666, Peoples R China.
EM jiananli_sdu@126.com; jlrxqxq370322@126.com; luoxin.lxin@gmail.com;
   xuxinshun@sdu.edu.cn
OI Liu, Xiaoqain/0000-0002-2187-8598
FU National Natural Science Foundation of China
FX No Statement Available
CR BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Chee Kheng Chng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1571, DOI 10.1109/ICDAR.2019.00252
   Da C, 2022, LECT NOTES COMPUT SC, V13688, P322, DOI 10.1007/978-3-031-19815-1_19
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Jaderberg M, 2015, ADV NEUR IN, V28
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Luo CJ, 2021, INT J COMPUT VISION, V129, P960, DOI 10.1007/s11263-020-01411-1
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Radford A, 2021, PR MACH LEARN RES, V139
   Ronneberger O., ARXIV150504597
   Shi BG, 2017, PROC INT CONF DOC, P1429, DOI 10.1109/ICDAR.2017.233
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Tan YL, 2022, LECT NOTES COMPUT SC, V13688, P481, DOI 10.1007/978-3-031-19815-1_28
   Veit A, 2016, Arxiv, DOI arXiv:1601.07140
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang P, 2022, LECT NOTES COMPUT SC, V13688, P339, DOI 10.1007/978-3-031-19815-1_20
   Wang WJ, 2019, Arxiv, DOI arXiv:1909.07113
   Wu J., 2022, BRIT MACH VIS C, P555
   Wu X, 2023, APPL INTELL, V53, P3444, DOI 10.1007/s10489-022-03728-5
   Xie XD, 2022, LECT NOTES COMPUT SC, V13688, P303, DOI 10.1007/978-3-031-19815-1_18
   Yan RJ, 2021, PROC CVPR IEEE, P284, DOI 10.1109/CVPR46437.2021.00035
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216
   Zhang BQ, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P1704
   Zhang M, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P385, DOI 10.1145/3460426.3463639
   Zhang Ying, 2017, SUNW SCENE UNDERSTAN, V2017, P5
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 33
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6437
EP 6448
DI 10.1109/TMM.2024.3350916
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600026
DA 2024-08-05
ER

PT J
AU Li, M
   Fu, HZ
   He, SF
   Fan, HH
   Liu, J
   Keppo, J
   Shou, MZ
AF Li, Ming
   Fu, Huazhu
   He, Shengfeng
   Fan, Hehe
   Liu, Jun
   Keppo, Jussi
   Shou, Mike Zheng
TI DR-FER: Discriminative and Robust Representation Learning for Facial
   Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Annotations; Task analysis; Electronic mail; Training; Representation
   learning; Schedules; Artificial neural networks; Facial expression
   recognition (FER); masked image modeling (MIM); self-paced learning
ID FEATURES; NETWORK
AB Learning discriminative and robust representations is important for facial expression recognition (FER) due to subtly different emotional faces and their subjective annotations. Previous works usually address one representation solely because these two goals seem to be contradictory for optimization. Their performances inevitably suffer from challenges from the other representation. In this article, by considering this problem from two novel perspectives, we demonstrate that discriminative and robust representations can be learned in a unified approach, i.e., DR-FER, and mutually benefit each other. Moreover, we make it with the supervision from only original annotations. Specifically, to learn discriminative representations, we propose performing masked image modeling (MIM) as an auxiliary task to force our network to discover expression-related facial areas. This is the first attempt to employ MIM to explore discriminative patterns in a self-supervised manner. To extract robust representations, we present a category-aware self-paced learning schedule to mine high-quality annotated (easy) expressions and incorrectly annotated (hard) counterparts. We further introduce a retrieval similarity-based relabeling strategy to correct hard expression annotations, exploiting them more effectively. By enhancing the discrimination ability of the FER classifier as a bridge, these two learning goals significantly strengthen each other. Extensive experiments on several popular benchmarks demonstrate the superior performance of our DR-FER. Moreover, thorough visualizations and extra experiments on manually annotation-corrupted datasets show that our approach successfully accomplishes learning both discriminative and robust representations simultaneously.
C1 [Li, Ming] Natl Univ Singapore, Inst Data Sci, Singapore, Singapore.
   [Fu, Huazhu] ASTAR, Agcy Sci Technol & Res ASTAR, Singapore 138632, Singapore.
   [He, Shengfeng] Singapore Management Univ, Sch Comp & Informat Syst, Singapore 178903, Singapore.
   [Fan, Hehe] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Liu, Jun] Singapore Univ Technol & Design, Sch Informat Syst Technol & Design, Singapore, Singapore.
   [Keppo, Jussi] Natl Univ Singapore, Business Sch, Singapore 119077, Singapore.
   [Shou, Mike Zheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
C3 National University of Singapore; Agency for Science Technology &
   Research (A*STAR); Singapore Management University; National University
   of Singapore; Singapore University of Technology & Design; National
   University of Singapore; National University of Singapore
RP Shou, MZ (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
EM ming.li@u.nus.edu; hzfu@ieee.org; shengfenghe7@gmail.com;
   hehe.fan@nus.edu.sg; jun_liu@sutd.edu.sg; keppo@nus.edu.sg;
   mike.zheng.shou@gmail.com
RI Fu, Huazhu/A-1411-2014; He, Shengfeng/E-5682-2016
OI Fu, Huazhu/0000-0002-9702-5524; He, Shengfeng/0000-0002-3802-4644; Fan,
   Hehe/0000-0001-9572-2345; Li, Ming/0000-0002-7852-0159; Liu,
   Jun/0000-0002-4365-4165
FU Ministry of Education, Singapore
FX No Statement Available
CR Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Chen Cunjian, 2021, PyTorch Face Landmark: A Fast and Accurate Facial Landmark Detector
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Ekman P., 1978, Facial action coding system: A technique for the measurement of facial movement
   Fan LF, 2019, IEEE I CONF COMP VIS, P5723, DOI 10.1109/ICCV.2019.00582
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Foret P., 2021, INT C LEARN REPR
   Friesen W. V., 1983, Uni Cali SF, V2, P1
   Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493
   Gong MG, 2019, IEEE T EVOLUT COMPUT, V23, P288, DOI 10.1109/TEVC.2018.2850769
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058
   He K., 2021, PROC IEEECVF C COMPU, P16000
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong Y, 2021, PROC CVPR IEEE, P6622, DOI 10.1109/CVPR46437.2021.00656
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang KK, 2017, IEEE T NEUR NET LEAR, V28, P1082, DOI 10.1109/TNNLS.2016.2522431
   Jiabei Zeng, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11217), P227, DOI 10.1007/978-3-030-01261-8_14
   Jiang L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P547, DOI 10.1145/2647868.2654918
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kumar M, 2010, Advances in Neural Information Processing Systems, V23
   Li H., 2021, arXiv
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Li J., 2019, P INT C LEARN REPR
   Li J, 2020, P INT C LEARN REPR
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P881, DOI 10.1109/TAFFC.2020.2973158
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li YJ, 2023, IEEE T MULTIMEDIA, V25, P1359, DOI 10.1109/TMM.2022.3141604
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3190, DOI 10.1109/TCSVT.2021.3103782
   Li Y, 2018, INT C PATT RECOG, P2209, DOI 10.1109/ICPR.2018.8545853
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Liang F, 2024, Arxiv, DOI arXiv:2205.14540
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Ma F., 2021, arXiv
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Paszke A, 2019, ADV NEUR IN, V32
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Pereyra G., 2017, Regularizing neural networks by penalizing confident output distributions
   Permuter H, 2006, PATTERN RECOGN, V39, P695, DOI 10.1016/j.patcog.2005.10.028
   Ruan DL, 2021, PROC CVPR IEEE, P7656, DOI 10.1109/CVPR46437.2021.00757
   Ryu B, 2017, IEEE T IMAGE PROCESS, V26, P6006, DOI 10.1109/TIP.2017.2726010
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Shi J., 2021, arXiv
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Su W., 2015, Proceedings of the 2015 International Conference on The Theory of Information Retrieval, ICTIR'15, P349, DOI 10.1145/2808194.2809481
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan F, 2021, AAAI CONF ARTIF INTE, V35, P10041
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512
   Wei C, 2022, PROC CVPR IEEE, P14648, DOI 10.1109/CVPR52688.2022.01426
   Welinder P., 2010, Computation & Neural Systems Technical Report
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3974
   Xu J., 2022, P IEEE CVF C COMP VI, P10184
   Xue FL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3581, DOI 10.1109/ICCV48922.2021.00358
   Yeasin M, 2006, IEEE T MULTIMEDIA, V8, P500, DOI 10.1109/TMM.2006.870737
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zhang FF, 2022, IEEE T MULTIMEDIA, V24, P1800, DOI 10.1109/TMM.2021.3072786
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zhang YH, 2021, ADV NEUR IN, V34
   Zhang YH, 2022, LECT NOTES COMPUT SC, V13686, P418, DOI 10.1007/978-3-031-19809-0_24
   Zhao Q, 2015, AAAI CONF ARTIF INTE, P3196
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
   Zheng C., 2022, P IEEE CVF INT C COM, P3146
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
   Zhong ZS, 2021, PROC CVPR IEEE, P16484, DOI 10.1109/CVPR46437.2021.01622
   Zhou TF, 2023, IEEE T PATTERN ANAL, V45, P8296, DOI 10.1109/TPAMI.2023.3239194
NR 76
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6297
EP 6309
DI 10.1109/TMM.2023.3347849
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600052
DA 2024-08-05
ER

PT J
AU Liu, QG
   Gao, P
   Han, K
   Liu, NZ
   Xiang, W
AF Liu, Qingguo
   Gao, Pan
   Han, Kang
   Liu, Ningzhong
   Xiang, Wei
TI Degradation-Aware Self-Attention Based Transformer for Blind Image
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Super-resolution; transformer; degradation-aware self-attention;
   contrastive learning
AB Compared to CNN-based methods, Transformer-based methods achieve impressive image restoration outcomes due to their ability to model remote dependencies. However, how to apply Transformer-based methods to the field of blind super-resolution (SR) and further make an SR network adaptive to degradation information is still an open problem. In this paper, we propose a new degradation-aware self-attention-based Transformer model, where we incorporate contrastive learning into the Transformer network for learning the degradation representations of input images with unknown noise. In particular, we integrate both CNN and Transformer components into the SR network, where we first use the CNN modulated by the degradation information to extract local features, and then employ the degradation-aware Transformer to extract global semantic features. We apply our proposed model to several popular large-scale benchmark datasets for testing, and achieve the state-of-the-art performance compared to existing methods. In particular, our method yields a PSNR of 32.43 dB on the Urban100 dataset at x2 scale, 0.94 dB higher than DASR, and 26.62 dB on the Urban100 dataset at x4 scale, 0.26 dB improvement over KDSR, setting a new benchmark in this area.
C1 [Liu, Qingguo; Gao, Pan; Liu, Ningzhong] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Han, Kang; Xiang, Wei] La Trobe Univ, Sch Comp Engn & Math Sci, Melbourne, Vic 3086, Australia.
C3 Nanjing University of Aeronautics & Astronautics; La Trobe University
RP Gao, P (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM liuqingguo@nuaa.edu.cn; Pan.Gao@nuaa.edu.cn; K.Han@latrobe.edu.au;
   liunz@163.com; w.xiang@latrobe.edu.au
FU Natural Science Foundation of China
FX No Statement Available
CR Bell-Kligler S, 2019, ADV NEUR IN, V32
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Cao JZ, 2023, Arxiv, DOI arXiv:2106.06847
   Carion N., 2020, EUR C COMP VIS, P213
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XY, 2023, PROC CVPR IEEE, P22367, DOI 10.1109/CVPR52729.2023.02142
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dyer C, 2014, Arxiv, DOI arXiv:1410.8251
   Fritsche M, 2019, IEEE INT CONF COMP V, P3599, DOI 10.1109/ICCVW.2019.00445
   Gu JJ, 2021, PROC CVPR IEEE, P9195, DOI 10.1109/CVPR46437.2021.00908
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang DK, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3445-y
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo ZW, 2022, PROC CVPR IEEE, P17621, DOI 10.1109/CVPR52688.2022.01712
   Qi HR, 2024, IEEE T MULTIMEDIA, V26, P1589, DOI 10.1109/TMM.2023.3283856
   Shi JA, 2024, IEEE T MULTIMEDIA, V26, P2608, DOI 10.1109/TMM.2023.3301225
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357
   Sun GL, 2023, Arxiv, DOI arXiv:2105.10926
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wu BC, 2020, Arxiv, DOI [arXiv:2006.03677, DOI 10.48550/ARXIV.2006.03677]
   Xia B., 2023, PROC INT C LEARN REP, P1
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yoo JS, 2023, IEEE T MULTIMEDIA, V25, P5972, DOI 10.1109/TMM.2022.3202018
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4771, DOI 10.1109/ICCV48922.2021.00475
   Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
NR 43
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7516
EP 7528
DI 10.1109/TMM.2024.3368923
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000059
DA 2024-08-05
ER

PT J
AU Mo, ST
   Xin, M
AF Mo, Shentong
   Xin, Miao
TI BSTG-Trans: A Bayesian Spatial-Temporal Graph Transformer for Long-Term
   Pose Forecasting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Long-term forecasting; spatial-temporal graph transformer; Bayesian
   transformer; uncertainty estimation
AB Human pose forecasting that aims to predict the body poses happening in the future is an important task in computer vision. However, long-term pose forecasting is particularly challenging because modeling long-range dependencies across the spatial-temporal level is hard for joint-based representation. Another challenge is uncertainty prediction since the future prediction is not a deterministic process. In this article, we present a novel <bold>B</bold>ayesian <bold>S</bold>patial-<bold>T</bold>emporal <bold>G</bold>raph <bold>Trans</bold>former (BSTG-Trans) for predicting accurate, diverse, and uncertain future poses. First, we apply a spatial-temporal graph transformer as an encoder and a temporal-spatial graph transformer as a decoder for modeling the long-range spatial-temporal dependencies across pose joints to generate the long-term future body poses. Furthermore, we propose a Bayesian sampling module for uncertainty quantization of diverse future poses. Finally, a novel uncertainty estimation metric, namely Uncertainty Absolute Error is introduced for measuring both the accuracy and uncertainty of each predicted future pose. We achieve state-of-the-art performance against other baselines on Human3.6 M and HumanEva-I in terms of accuracy, diversity, and uncertainty for long-term pose forecasting. Moreover, our comprehensive ablation studies demonstrate the effectiveness and generalization of each module proposed in our BSTG-Trans.
C1 [Mo, Shentong] Carnegie Mellon Univ, Elect & Comp Engn, Pittsburgh, PA 15213 USA.
   [Xin, Miao] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Carnegie Mellon University; Chinese Academy of Sciences; Institute of
   Automation, CAS
RP Xin, M (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
EM shentonm@andrew.cmu.edu; miao.xin@ia.ac.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Aksan E, 2021, INT CONF 3D VISION, P565, DOI 10.1109/3DV53792.2021.00066
   Ariz M, 2016, COMPUT VIS IMAGE UND, V148, P201, DOI 10.1016/j.cviu.2015.04.009
   Barsoum E, 2018, IEEE COMPUT SOC CONF, P1499, DOI 10.1109/CVPRW.2018.00191
   Bhattacharyya A, 2018, PROC CVPR IEEE, P8485, DOI 10.1109/CVPR.2018.00885
   Blundell C, 2015, PR MACH LEARN RES, V37, P1613
   Cai D, 2020, AAAI CONF ARTIF INTE, V34, P7464
   Dang LW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11447, DOI 10.1109/ICCV48922.2021.01127
   Dilokthanakul N., 2016, ARXIV161102648
   Djuric N, 2020, IEEE WINT CONF APPL, P2084, DOI [10.1109/WACV45572.2020.9093332, 10.1109/wacv45572.2020.9093332]
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Fortuin V, 2020, P ADV NEUR INF PROC, P1
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Gardner Jacob, 2018, Adv. Neural Inf. Process. Syst, V31, P7576
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo W, 2022, Back to MLP: A Simple Baseline for Human Motion Prediction
   Gurumurthy S, 2017, PROC CVPR IEEE, P4941, DOI 10.1109/CVPR.2017.525
   Hou XS, 2021, IEEE T MULTIMEDIA, V23, P716, DOI 10.1109/TMM.2020.2987693
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Kämäräinen T, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1181, DOI 10.1145/3240508.3240620
   Kingma DP., 2013, AUTOENCODING VARIATI, DOI DOI 10.1051/0004-6361/201527329
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Kipf T., 2016, NIPS WORKSH BAYES DE
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kitani K., 2020, P INT C LEARN REPR
   Koppula HS, 2013, IEEE INT C INT ROBOT, P2071, DOI 10.1109/IROS.2013.6696634
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li MZ, 2021, AAAI CONF ARTIF INTE, V35, P4189
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Liu JF, 2021, IEEE INT CONF ROBOT, P3374, DOI 10.1109/ICRA48506.2021.9561605
   Ma TZ, 2022, PROC CVPR IEEE, P6427, DOI 10.1109/CVPR52688.2022.00633
   Mao W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13289, DOI 10.1109/ICCV48922.2021.01306
   Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Mo ST, 2021, IEEE COMPUT SOC CONF, P2239, DOI 10.1109/CVPRW53098.2021.00253
   Paszke A, 2019, ADV NEUR IN, V32
   Petrovich M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10965, DOI 10.1109/ICCV48922.2021.01080
   Piergiovanni A. J., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P507, DOI 10.1007/978-3-030-58536-5_30
   Salzmann T, 2022, PROC CVPR IEEE, P6447, DOI 10.1109/CVPR52688.2022.00635
   Shridhar K., 2018, Uncertainty estimations by softplus normalization in Bayesian convolutional neural networks with variational inference
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Tang YY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P935
   Tran D, 2019, 33 C NEURAL INFORM P, V32
   Vaswani A, 2017, ADV NEUR IN, V30
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Wang BR, 2019, IEEE I CONF COMP VIS, P7123, DOI 10.1109/ICCV.2019.00722
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xue BY, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7378, DOI 10.1109/ICASSP39728.2021.9414046
   Yan XC, 2018, LECT NOTES COMPUT SC, V11209, P276, DOI 10.1007/978-3-030-01228-1_17
   Yang LX, 2019, IEEE I CONF COMP VIS, P6449, DOI 10.1109/ICCV.2019.00654
   Ye Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P346, DOI 10.1007/978-3-030-58545-7_20
   Ying C., 2021, Advances in Neural Information Processing Systems, P28877
   Yun S, 2019, ADV NEUR IN, V32
   Zhang Y, 2021, PROC CVPR IEEE, P3371, DOI 10.1109/CVPR46437.2021.00338
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhou Y., 2018, P INT C LEARN REPR
NR 60
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 673
EP 686
DI 10.1109/TMM.2023.3269219
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000019
DA 2024-08-05
ER

PT J
AU Pan, YH
   Liu, J
   Jin, L
   Li, ZC
AF Pan, Yonghua
   Liu, Jing
   Jin, Lu
   Li, Zechao
TI Unbiased Visual Question Answering by Leveraging Instrumental Variable
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Correlation; Instruments; Training; Predictive models;
   Color; Generators; Visual question answering; instrumental variable;
   causal inference; out of distribution
AB Existing unbiased visual question answering (VQA) models reduce the spurious correlation between questions and answers to force the models to focus on visual information. However, the visual information captured by these unbiased models is irrelevant to the correct answer, resulting in leveraging spurious correlation to predict incorrect answers. This makes these unbiased methods fail to obtain critical visual information, thus performing poorly on questions dominated by the visual information. To capture the valuable visual information, this article proposes a novel unbiased VQA model based on causal inference, leveraging Instrumental Variable (IVar) to increase the causal effect between visual features and answers. First, to obtain suitable instrumental variables, the noise generator is proposed according to the constraints of IVar. The generated noise can be regarded as IVar, which is used to pollute the original visual features. Then, this article proposes IVar loss which utilizes the generated IVar to increase the causal effect between visual features and answers. When the visual feature is polluted by IVar, IVar loss guides the model to predict incorrect answers to enhance the correlation between IVar and the answer. Since the correlation between IVar and the answer is proportional to the causal effect between the visual feature and the answer, IVar loss enhances the importance of the visual information, thereby rectifying the model to capture critical visual information. The extensive experimental results on widely-used benchmarks demonstrate the advantages of the proposed method. The proposed method gains the best accuracy on answer type Other of VQA-CP v2. These results demonstrate the superiority of the proposed method in capturing critical visual information since most questions on the answer type Other are dominated by visual information.
C1 [Pan, Yonghua; Jin, Lu; Li, Zechao] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Liu, Jing] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Liu, Jing] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Nanjing University of Science & Technology; Chinese Academy of Sciences;
   Institute of Automation, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Li, ZC (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM yonghuapan@njust.edu.cn; jliu@nlpr.ia.ac.cn; lu.jin@njust.edu.cn;
   zechao.li@njust.edu.cn
OI liu, jing/0000-0003-0903-9131
FU National Key Research and Development Program of China
FX No Statement Available
CR Abbasnejad Ehsan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10041, DOI 10.1109/CVPR42600.2020.01006
   Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Ahuja K, 2020, PR MACH LEARN RES, V119
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P3
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Arjovsky M, 2020, Arxiv, DOI [arXiv:1907.02893, DOI 10.48550/ARXIV.1907.02893]
   Atzmon Yuval, 2020, ADV NEURAL INFORM PR, P1462
   Cadene Remi, 2019, ADV NEUR IN, P839
   Chen L, 2022, LECT NOTES COMPUT SC, V13696, P95, DOI 10.1007/978-3-031-20059-5_6
   Chen S., 2023, arXiv, DOI [10.48550/arXiv.2304.0834, DOI 10.48550/ARXIV.2304.0834]
   Cho JW, 2023, PROC CVPR IEEE, P11681, DOI 10.1109/CVPR52729.2023.01124
   Chuang C.-Y., 2020, NeurIPS, V33, P8765
   Chung C., 2014, CoRR, P1, DOI DOI 10.48550/ARXIV.1412.3555
   Clark C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4069
   Dancette C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1554, DOI 10.1109/ICCV48922.2021.00160
   Glymour J. Pearl, 2016, Causal Inference in Statistics: APrimer
   Gokhale T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P878
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guo D, 2020, IEEE T IMAGE PROCESS, V29, P6655, DOI 10.1109/TIP.2020.2992888
   Han X., 2021, P IEEECVF INT C COMP, P1584
   Hartford J. S., 2021, INT C MACHINE LEARNI, P4096
   Hartford J, 2017, PR MACH LEARN RES, V70
   He Y, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107383
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Jiang L, 2014, ADV NEUR IN, V27
   Kervadec C, 2021, PROC CVPR IEEE, P4205, DOI 10.1109/CVPR46437.2021.00419
   Kervadec C, 2021, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR46437.2021.00280
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Kuangy Z, 2020, PR MACH LEARN RES, V108, P398
   Kumar M, 2010, Advances in Neural Information Processing Systems, V23
   Li X., 2020, Oscar: Object-Semantics Aligned Pre-training for VisionLanguage Tasks, DOI 10.1007/978-3-030-58577-8_8
   Li Y., 2023, IEEE Trans. Multimedia, early access, DOI [10.1109/TMM.2023.3275874, DOI 10.1109/TMM.2023.3275874]
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu Fei, 2021, P IEEE CVF INT C COM, P1698
   Liu J., 2021, Proceedings of Machine Learning Research, P6804
   Liu Jiashuo, 2021, Adv. Neural Inf. Process., V26, P21720
   Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081
   Lopez-Paz D, 2017, PROC CVPR IEEE, P58, DOI 10.1109/CVPR.2017.14
   Lu JS, 2019, ADV NEUR IN, V32
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Niu Yulei, 2021, ADV NEURAL INFORM PR, V34, P2
   Pan YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3487042
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qian TW, 2023, IEEE T MULTIMEDIA, V25, P3950, DOI 10.1109/TMM.2022.3169065
   Ramakrishnan S., 2018, INT C NEURAL INF PRO, P1548
   Ren K. He, 2015, inProc. Adv. Neural Inf.Process. Syst., V28
   Schölkopf B, 2021, P IEEE, V109, P612, DOI 10.1109/JPROC.2021.3058954
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tang K., 2021, arXiv
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Teney Damien, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P580, DOI 10.1007/978-3-030-58607-2_34
   Teney D, 2020, INT C NEURAL INF PRO, P407
   Teney D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1397, DOI 10.1109/ICCV48922.2021.00145
   Wang Tan, 2021, ADV NEURAL INF PROCE, P18225
   Wang Ziqiao, 2022, P INT C LEARN REPR
   Wen ZQ, 2021, ADV NEUR IN, V34
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yuan JK, 2022, ACM T KNOWL DISCOV D, V16, DOI 10.1145/3494568
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang C., 2020, Advances in Neural Information Processing Systems, V33, P289
   Zhang D., 2020, PROC ADV NEURAL IN, V33, P655, DOI DOI 10.5555/3495724.3495780
   Zhang XX, 2021, PROC CVPR IEEE, P5368, DOI 10.1109/CVPR46437.2021.00533
   Zhi X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1083
NR 66
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6648
EP 6662
DI 10.1109/TMM.2024.3355640
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600018
DA 2024-08-05
ER

PT J
AU Ren, YQ
   Peng, HP
   Li, LX
   Yang, YX
AF Ren, Yeqing
   Peng, Haipeng
   Li, Lixiang
   Yang, Yixian
TI Lightweight Voice Spoofing Detection Using Improved One-Class Learning
   and Knowledge Distillation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Voice spoofing detection; anti-spoofing; one-class classification;
   knowledge distillation
ID CLASSIFICATION; FEATURES
AB Voice spoofing detection is a technique for enhancing the security of automatic speaker verification system, but the existing research still faces problems such as weak detection capability and expensive computation. To address these problems, this work presents a lightweight voice anti-spoofing method by using improved one-class learning DOC-Softmax and knowledge distillation. The main idea of DOC-Softmax is to learn a feature space where the genuine samples have a compact space and the spoofing samples are parted from the bona fide space by a certain interval. And the dispersion loss is introduced for spoofing samples to cover the whole spoofing space as much as possible. Moreover, a lightweight voice spoofing detection model is designed to speed up inference, and the knowledge distillation is employed to improve representation power of the lightweight model. Without any data augmentation and ensemble learning, a series of experiments are conducted on LA and PA scenarios of the ASVspoof 2019 dataset, and the experimental results indicate that the proposed method performs better than most existing voice anti-spoofing methods.
C1 [Ren, Yeqing; Peng, Haipeng; Li, Lixiang; Yang, Yixian] Beijing Univ Posts & Telecommun, Informat Secur Ctr, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Ren, Yeqing; Peng, Haipeng; Li, Lixiang; Yang, Yixian] Beijing Univ Posts & Telecommun, Natl Engn Lab Disaster Backup & Recovery, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications
RP Peng, HP (corresponding author), Beijing Univ Posts & Telecommun, Informat Secur Ctr, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
EM yeqing_ren@bupt.edu.cn; penghaipeng@bupt.edu.cn; lixiang@bupt.edu.cn;
   yxyang@bupt.edu.cn
OI Lixiang, Li/0000-0001-9949-8731
FU Key Ramp;D Program of Shandong Province, China
FX No Statement Available
CR Alegre F, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS)
   Aljasem M, 2021, IEEE T INF FOREN SEC, V16, P3524, DOI 10.1109/TIFS.2021.3082303
   Bai ZX, 2021, NEURAL NETWORKS, V140, P65, DOI 10.1016/j.neunet.2021.03.004
   Bengio S., 2004, P OD SPEAK LANG REC, P237
   Chen A., 2020, P SPEAK LANG REC WOR, P132
   Dai X, 2021, PROC CVPR IEEE, P7838, DOI 10.1109/CVPR46437.2021.00775
   Disken G, 2023, APPL ACOUST, V211, DOI 10.1016/j.apacoust.2023.109568
   Evans N, 2013, INTERSPEECH, P925
   Feng PM, 2017, IEEE T MULTIMEDIA, V19, P725, DOI 10.1109/TMM.2016.2638206
   Ge Wanying, 2021, PROC ASV SPOOF WORKS, V22-28, DOI 10.21437/ASVSPOOF.2021-4
   Guo SX, 2023, PROC CVPR IEEE, P18633, DOI 10.1109/CVPR52729.2023.01787
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Hua G, 2021, IEEE SIGNAL PROC LET, V28, P1265, DOI 10.1109/LSP.2021.3089437
   Huang BY, 2023, IEEE SIGNAL PROC LET, V30, P185, DOI 10.1109/LSP.2023.3251895
   Javed A, 2021, APPL ACOUST, V183, DOI 10.1016/j.apacoust.2021.108283
   Jin H., 2019, PROC IEEE INT C BIOM, P1
   Jung JW, 2022, INT CONF ACOUST SPEE, P6367, DOI 10.1109/ICASSP43922.2022.9747766
   Khalid H, 2020, IEEE COMPUT SOC CONF, P2794, DOI 10.1109/CVPRW50498.2020.00336
   Khan SS, 2014, KNOWL ENG REV, V29, P345, DOI 10.1017/S026988891300043X
   Kim J.-H., 2023, PROC IEEE INT C ACOU, P1
   Kinnunen T, 2017, INTERSPEECH, P2, DOI 10.21437/Interspeech.2017-1111
   Lavrentyeva G, 2019, INTERSPEECH, P1033, DOI 10.21437/Interspeech.2019-1768
   Li JL, 2022, DIGIT SIGNAL PROCESS, V120, DOI 10.1016/j.dsp.2021.103256
   Li X, 2021, INTERSPEECH, P4314, DOI 10.21437/Interspeech.2021-2125
   Li X, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6354, DOI 10.1109/ICASSP39728.2021.9413828
   Liu XC, 2023, IEEE-ACM T AUDIO SPE, V31, P2507, DOI 10.1109/TASLP.2023.3285283
   Luo AW, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6359, DOI 10.1109/ICASSP39728.2021.9414670
   Ma KJ, 2023, IEEE SIGNAL PROC LET, V30, P359, DOI 10.1109/LSP.2023.3262419
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Nautsch Andreas, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P252, DOI 10.1109/TBIOM.2021.3059479
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Ngo PC, 2019, PROC INT C TOOLS ART, P141, DOI 10.1109/ICTAI.2019.00028
   Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026
   Ren YQ, 2023, IEEE-ACM T AUDIO SPE, V31, P2461, DOI 10.1109/TASLP.2023.3288416
   ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190
   Rostami AM, 2023, CIRC SYST SIGNAL PR, V42, P4252, DOI 10.1007/s00034-023-02314-5
   Sahidullah J., 2018, P SPEAK LANG REC WOR, P312
   Sahidullah M, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2087
   Sanchez J, 2015, IEEE T INF FOREN SEC, V10, P810, DOI 10.1109/TIFS.2015.2398812
   Soni MH, 2016, INTERSPEECH, P1820, DOI 10.21437/Interspeech.2016-668
   Tak H., 2021, PROC ASV SPOOF WORKS, P1
   Todisco M, 2019, INTERSPEECH, P1008, DOI 10.21437/Interspeech.2019-2249
   Turan MAT, 2021, IEEE T MULTIMEDIA, V23, P4220, DOI 10.1109/TMM.2020.3038315
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Villalba J, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2067
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang X, 2021, INTERSPEECH, P4259, DOI 10.21437/Interspeech.2021-702
   Wang X, 2020, COMPUT SPEECH LANG, V64, DOI [10.1016/j.csi.2020.101114, 10.1016/j.csl.2020.101114]
   Wei WL, 2021, IEEE T MULTIMEDIA, V23, P1731, DOI 10.1109/TMM.2020.3003631
   Wu ZZ, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2037
   Xue J., 2023, PROC IEEE INT C ACOU, P1
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang F, 2019, PROC CVPR IEEE, P3512, DOI 10.1109/CVPR.2019.00363
   Zhang Y, 2021, IEEE SIGNAL PROC LET, V28, P937, DOI 10.1109/LSP.2021.3076358
   Zhang YX, 2021, INTERSPEECH, P4279, DOI 10.21437/Interspeech.2021-1281
   Zheng PP, 2019, AAAI CONF ARTIF INTE, P1286
NR 58
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4360
EP 4374
DI 10.1109/TMM.2023.3321505
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100038
DA 2024-08-05
ER

PT J
AU Sun, TL
   Chen, HN
   Hu, GS
   He, LH
   Zhao, CR
AF Sun, Tianli
   Chen, Haonan
   Hu, Guosheng
   He, Lianghua
   Zhao, Cairong
TI Explainability of Speech Recognition Transformers via Gradient-Based
   Attention Visualization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Analytical models; Visualization; Predictive models; Data
   models; Computational modeling; Training; Explainability; transformer;
   speech recognition; attention visualization
AB In vision Transformers, attention visualization methods are used to generate heatmaps highlighting the class-corresponding areas in input images, which offers explanations on how the models make predictions. However, it is not so applicable for explaining automatic speech recognition (ASR) Transformers. An ASR Transformer makes a particular prediction for every input token to form a sentence, but a vision Transformer only makes an overall classification for the input data. Therefore, traditional attention visualization methods may fail in ASR Transformers. In this work, we propose a novel attention visualization method in ASR Transformers and try to explain which frames of the audio result in the output text. Inspired by the model explainability, we also explore ways of improving the effectiveness of the ASR model. Comparing with other Transformer attention visualization methods, our method is more efficient and intuitively understandable, which unravels the attention calculation from information flow of Transformer attention modules. In addition, we demonstrate the utilization of visualization result in three ways: (1) We visualize attention with respect to connectionist temporal classification (CTC) loss to train an ASR model with adversarial attention erasing regularization, which effectively decreases the word error rate (WER) of the model and improves its generalization capability. (2) We visualize the attention on some specific words, interpreting the model by effectively demonstrating the semantic and grammar relationships between these words. (3) Similarly, we analyze how the model manage to distinguish homophones, using contrastive explanation with respect to homophones.
C1 [Sun, Tianli; He, Lianghua; Zhao, Cairong] Tongji Univ, Sch Elect & Informat Engn, Shanghai 200092, Peoples R China.
   [Chen, Haonan] Alibaba Grp, Hangzhou 310000, Peoples R China.
   [Hu, Guosheng] Oosto, London BT1 2BE, England.
C3 Tongji University; Alibaba Group
RP Zhao, CR (corresponding author), Tongji Univ, Sch Elect & Informat Engn, Shanghai 200092, Peoples R China.
EM suntianli2009@163.com; haolan.chn@alibaba-inc.com;
   huguosheng100@gmail.com; helianghua@tongji.edu.cn;
   zhaocairong@tongji.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Abnar S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4190
   Ardila R, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2020), P4218
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Bu H, 2017, 2017 20TH CONFERENCE OF THE ORIENTAL CHAPTER OF THE INTERNATIONAL COORDINATING COMMITTEE ON SPEECH DATABASES AND SPEECH I/O SYSTEMS AND ASSESSMENT (O-COCOSDA), P58, DOI 10.1109/ICSDA.2017.8384449
   Chefer H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P387, DOI 10.1109/ICCV48922.2021.00045
   Chefer H, 2021, PROC CVPR IEEE, P782, DOI 10.1109/CVPR46437.2021.00084
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Dong LH, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5884, DOI 10.1109/ICASSP.2018.8462506
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gillick L., 1989, ICASSP-89: 1989 International Conference on Acoustics, Speech and Signal Processing (IEEE Cat. No.89CH2673-2), P532, DOI 10.1109/ICASSP.1989.266481
   Graves A, 2006, ICML, P369, DOI DOI 10.1145/1143844.1143891
   Gulati A, 2020, INTERSPEECH, P5036, DOI 10.21437/Interspeech.2020-3015
   Han W, 2020, INTERSPEECH, P3610, DOI 10.21437/Interspeech.2020-2059
   Jain S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P3543
   Kriman S, 2020, INT CONF ACOUST SPEE, P6124, DOI [10.1109/ICASSP40776.2020.9053889, 10.1109/icassp40776.2020.9053889]
   Krug A., 2018, P NEURIPS WORKSH INT, P1
   Kudo T, 2018, CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P66
   Li CY, 2020, INT CONF ACOUST SPEE, P6434, DOI [10.1109/ICASSP40776.2020.9054675, 10.1109/icassp40776.2020.9054675]
   Mang Q, 2020, INT CONF ACOUST SPEE, P7829, DOI [10.1109/ICASSP40776.2020.9053896, 10.1109/icassp40776.2020.9053896]
   Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Park DS, 2019, INTERSPEECH, P2613, DOI 10.21437/Interspeech.2019-2680
   Prabhushankar M, 2020, IEEE IMAGE PROC, P3289, DOI 10.1109/ICIP40778.2020.9190927
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Tang J, 2021, IEEE-ACM T AUDIO SPE, V29, P2816, DOI 10.1109/TASLP.2021.3101921
   Vaswani A, 2017, ADV NEUR IN, V30
   Voita E, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5797
   Watanabe S, 2017, IEEE J-STSP, V11, P1240, DOI 10.1109/JSTSP.2017.2763455
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wiegreffe S, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P11
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yao ZY, 2021, INTERSPEECH, P4054, DOI 10.21437/Interspeech.2021-1983
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou SY, 2018, INTERSPEECH, P791, DOI 10.21437/Interspeech.2018-1107
NR 35
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1395
EP 1406
DI 10.1109/TMM.2023.3282488
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700051
DA 2024-08-05
ER

PT J
AU Tang, LF
   Chen, Z
   Huang, J
   Ma, JY
AF Tang, Linfeng
   Chen, Ziang
   Huang, Jun
   Ma, Jiayi
TI CAMF: An Interpretable Infrared and Visible Image Fusion Network Based
   on Class Activation Mapping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Feature extraction; Transforms; Image reconstruction; Deep
   learning; Task analysis; Pollution measurement; learnable fusion rule;
   class activation mapping; deep learning
ID FRAMEWORK; INPUT
AB Image fusion aims to integrate the complementary information of source images and synthesize a single fused image. Existing image fusion algorithms apply hand-crafted fusion rules to merge deep features which cause information loss and limit the fusion performance of methods since the uninterpretability of deep learning. To overcome the above shortcomings, we propose a learnable fusion rule for infrared and visible image fusion based on class activation mapping. Our proposed fusion rule can selectively preserve meaningful information and reduce distortion. More specifically, we first train an encoder-decoder network and an auxiliary classifier based on the shared encoder. Then, the class activation weights are taken out from the auxiliary classifier, which indicates the importance of each channel. Finally, the deep features extracted by the encoder are adaptively fused according to the class activation weights and the fused image is reconstructed from the fused features via the pre-trained decoder. Note that our learnable fusion rule can automatically measure the importance of each deep feature without human participation. Moreover, it fully preserves the significant features of source images such as salient targets and texture details. Extensive experiments manifest our superiority over state-of-the-art algorithms. Visualization of feature maps and their corresponding weights reveals the high interpretability of our method.
C1 [Tang, Linfeng; Chen, Ziang; Huang, Jun; Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Huang, J; Ma, JY (corresponding author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
EM linfeng0419@gmail.com; frostcza@whu.edu.cn; junhwong@whu.edu.cn;
   jyma2010@gmail.com
RI Linfeng, Tang/HIZ-5784-2022; Ma, Jiayi/Y-2470-2019
OI Linfeng, Tang/0000-0002-8566-5743; Ma, Jiayi/0000-0003-3264-3265; Huang,
   Jun/0000-0001-5893-4090
FU National Natural Science Foundation of China
FX No Statement Available
CR Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Choi M, 2005, IEEE GEOSCI REMOTE S, V2, P136, DOI 10.1109/LGRS.2005.845313
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   Cvejic N, 2007, IEEE SENS J, V7, P743, DOI 10.1109/JSEN.2007.894926
   Deshmukh M., 2010, Int. J. Image Process., V4, P484
   Fu ZZ, 2016, INFRARED PHYS TECHN, V77, P114, DOI 10.1016/j.infrared.2016.05.012
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hussain AJ, 2018, NEUROCOMPUTING, V300, P44, DOI 10.1016/j.neucom.2018.02.094
   Jia XY, 2021, IEEE INT CONF COMP V, P3489, DOI 10.1109/ICCVW54120.2021.00389
   Kim J., 2019, PROC INT C LEARN REP, P1
   Kim M, 2016, INFORM FUSION, V27, P198, DOI 10.1016/j.inffus.2015.03.003
   Li HF, 2021, IEEE T IMAGE PROCESS, V30, P4070, DOI 10.1109/TIP.2021.3069339
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CH, 2017, INFRARED PHYS TECHN, V83, P94, DOI 10.1016/j.infrared.2017.04.018
   Liu JY, 2023, IEEE I CONF COMP VIS, P8081, DOI 10.1109/ICCV51070.2023.00745
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu XB, 2017, NEUROCOMPUTING, V235, P131, DOI 10.1016/j.neucom.2017.01.006
   Liu YP, 2014, SIGNAL PROCESS, V97, P9, DOI 10.1016/j.sigpro.2013.10.010
   Liu Y, 2019, IEEE SIGNAL PROC LET, V26, P485, DOI 10.1109/LSP.2019.2895749
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3075747
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mou J, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P1046, DOI 10.1109/CISP.2013.6745210
   Odena A., 2016, Distill, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Springenberg JT., 2014, STRIVING SIMPLICITY
   Tang LF, 2022, IEEE-CAA J AUTOMATIC, V9, P2121, DOI 10.1109/JAS.2022.106082
   Toet Alexander, 2014, Figshare
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu H, 2021, INFORM FUSION, V76, P177, DOI 10.1016/j.inffus.2021.06.001
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu JW, 2022, DISPLAYS, V74, DOI 10.1016/j.displa.2022.102188
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Zhang BH, 2015, INFRARED PHYS TECHN, V73, P286, DOI 10.1016/j.infrared.2015.10.004
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang Q, 2016, INFRARED PHYS TECHN, V74, P11, DOI 10.1016/j.infrared.2015.11.003
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou ZQ, 2016, INFORM FUSION, V30, P15, DOI 10.1016/j.inffus.2015.11.003
   Zhu ZQ, 2018, INFORM SCIENCES, V432, P516, DOI 10.1016/j.ins.2017.09.010
NR 64
TC 0
Z9 0
U1 20
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4776
EP 4791
DI 10.1109/TMM.2023.3326296
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100012
DA 2024-08-05
ER

PT J
AU Wang, D
   Tian, CN
   Liang, X
   Zhao, L
   He, LH
   Wang, Q
AF Wang, Di
   Tian, Changning
   Liang, Xiao
   Zhao, Lin
   He, Lihuo
   Wang, Quan
TI Dual-Perspective Fusion Network for Aspect-Based Multimodal Sentiment
   Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sentiment analysis; Task analysis; Data mining; Semantics; Syntactics;
   Feature extraction; Visualization; Aspect-based sentiment analysis;
   multimodal sentiment analysis; graph neural network
AB Aspect-based multimodal sentiment analysis (ABMSA) is an important sentiment analysis task that analyses aspect-specific sentiment in data with different modalities (usually multimodal data with text and images). Previous works usually ignore the overall sentiment tendency when analyzing the sentiment of each aspect term. However, the overall sentiment tendency is highly correlated with aspect-specific sentiment. In addition, existing methods neglect to explore and make full use of the fine-grained multimodal information closely related to aspect terms. To address these limitations, we propose a dual-perspective fusion network (DPFN) that considers both global and local fine-grained sentiment information in multimodal data. From the global perspective, we use text-image caption pairs to obtain a global representation containing information about the overall sentiment tendencies. From the local fine-grained perspective, we construct two graph structures to explore the fine-grained information in texts and images. Finally, aspect-level sentiment polarities can be obtained by analyzing the combination of global and local fine-grained sentiment information. Experimental results on two multimodal Twitter datasets show that the proposed DPFN model outperforms state-of-the-art methods.
C1 [Wang, Di; Tian, Changning; Liang, Xiao; He, Lihuo; Wang, Quan] Xidian Univ, Key Lab Smart Human Comp Interact & Wearable Techn, Xian 710071, Peoples R China.
   [Zhao, Lin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
C3 Xidian University; Nanjing University of Science & Technology
RP Wang, D (corresponding author), Xidian Univ, Key Lab Smart Human Comp Interact & Wearable Techn, Xian 710071, Peoples R China.
EM wangdi@xidian.edu.cn; cntian@stu.xidian.edu.cn; ecoxial2012@outlook.com;
   linzhao@njust.edu.cn; lhhe@mail.xidian.edu.cn; Qwang@xidian.edu.cn
OI Wang, Di/0000-0001-8027-4287; , Lin/0000-0002-8756-2027; He,
   Lihuo/0000-0002-0555-3574; Wang, Quan/0000-0001-6913-8604; liang,
   xiao/0000-0003-0382-2715
FU National Natural Science Foundation of China
FX No Statement Available
CR Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chen P., 2017, P 2017 C EMP METH NA, P452, DOI DOI 10.18653/V1/D17-1047
   Nguyen DQ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P9
   Delbrouck JB, 2020, PROCEEDINGS OF THE SECOND GRAND CHALLENGE AND WORKSHOP ON MULTIMODAL LANGUAGE (CHALLENGE-HML), VOL 1, P1
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan FF, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3433
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang BX, 2018, LECT NOTES COMPUT SC, V10899, P197, DOI 10.1007/978-3-319-93372-6_22
   Ju Xincheng, 2021, P 2021 C EMP METH NA, DOI DOI 10.18653/V1/2021.EMNLP-MAIN.360
   Ke P, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6975
   Khan Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3034, DOI 10.1145/3474085.3475692
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Li R., 2021, P 59 ANN M ASS COMP, P6319, DOI 10.18653/v1/2021.acl-long.494
   Ling Y, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2149
   Mrini K, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P731
   Truong QT, 2019, AAAI CONF ARTIF INTE, P305
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Song YW, 2019, LECT NOTES COMPUT SC, V11730, P93, DOI 10.1007/978-3-030-30490-4_9
   Sun C, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P380
   Tang Duyu, 2016, P 2016 C EMP METH NA, P214, DOI [10.18653/v1/D16-1021, DOI 10.18653/V1/D16-1021]
   Tang H., 2020, Dependency Graph Enhanced Dual-transformer Structure for Aspectbased Sentiment Classification, P6578, DOI DOI 10.18653/V1/2020.ACL-MAIN.588
   Tian H, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4067
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vinodhini G, 2012, International Journal, V2, P282
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P4909, DOI 10.1109/TMM.2022.3183830
   Wang K, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3229
   Wang Y., 2016, P 2016 C EMP METH NA, P606, DOI 10.18653/v1/D16-1058
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu N, 2019, AAAI CONF ARTIF INTE, P371
   Yang H., 2022, P 2022 C EMPIRICAL M, P3324
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yang Zhilin, 2019, NeurIPS, V32, DOI DOI 10.5555/3454287.3454804
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1008, DOI 10.1145/2964284.2964288
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   Yu J., 2022, P 31 INT JOINT C ART, P4482
   Yu JF, 2023, IEEE T AFFECT COMPUT, V14, P1966, DOI 10.1109/TAFFC.2022.3171091
   Yu JF, 2020, IEEE-ACM T AUDIO SPE, V28, P429, DOI 10.1109/TASLP.2019.2957872
   Yu JF, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5408
   Zadeh A., 2017, C EMP METH NAT LANG
   Zeng JD, 2023, IEEE T MULTIMEDIA, V25, P6301, DOI 10.1109/TMM.2022.3207572
   Zhang C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4568
   Zhang L, 2018, WIRES DATA MIN KNOWL, V8, DOI 10.1002/widm.1253
   Zhao F., 2022, P 29 INT C COMPUTATI, P6784
   Zhu T, 2023, IEEE T MULTIMEDIA, V25, P3375, DOI 10.1109/TMM.2022.3160060
NR 47
TC 0
Z9 0
U1 39
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4028
EP 4038
DI 10.1109/TMM.2023.3321435
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300021
DA 2024-08-05
ER

PT J
AU Wang, WX
   He, XJ
   Zhang, YS
   Guo, LT
   Shen, JC
   Li, JY
   Liu, J
AF Wang, Wenxuan
   He, Xingjian
   Zhang, Yisi
   Guo, Longteng
   Shen, Jiachen
   Li, Jiangyun
   Liu, Jing
TI CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Referring image segmentation; cross-modality guidance; masked
   self-distillation; vision and language
ID NETWORK
AB Referring image segmentation (RIS) is a fundamental vision-language task that intends to segment a desired object from an image based on a given natural language expression. Due to the essentially distinct data properties between image and text, most of existing methods either introduce complex designs towards fine-grained vision-language alignment or lack required dense alignment, resulting in scalability issues or mis-segmentation problems such as over- or under-segmentation. To achieve effective and efficient fine-grained feature alignment in the RIS task, we explore the potential of masked multimodal modeling coupled with self-distillation and propose a novel cross-modality masked self-distillation framework named CM-MaskSD, in which our method inherits the transferred knowledge of image-text semantic alignment from CLIP model to realize fine-grained patch-word feature alignment for better segmentation accuracy. Moreover, our CM-MaskSD framework can considerably boost model performance in a nearly parameter-free manner, since it shares weights between the main segmentation branch and the introduced masked self-distillation branches, and solely introduces negligible parameters for coordinating the multimodal features. Comprehensive experiments on three benchmark datasets (i.e. RefCOCO, RefCOCO+, G-Ref) for the RIS task convincingly demonstrate the superiority of our proposed framework over previous state-of-the-art methods.
C1 [Wang, Wenxuan; He, Xingjian; Guo, Longteng; Liu, Jing] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Wang, Wenxuan; Liu, Jing] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
   [Zhang, Yisi; Shen, Jiachen; Li, Jiangyun] Univ Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   University of Science & Technology Beijing
RP Li, JY (corresponding author), Univ Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.
EM wangwenxuan2023@ia.ac.cn; xingjian.he@nlpr.ia.ac.cn;
   pppppsang@outlook.com; longteng.guo@nlpr.ia.ac.cn;
   m202110559@xs.ustb.edu.cn; leejy@ustb.edu.cn; jliu@nlpr.ia.ac.cn
OI liu, jing/0000-0003-0903-9131; Wang, Wenxuan/0000-0002-9022-1370
FU National Key Research and Development Program of China
FX No Statement Available
CR Bai YT, 2023, PROC CVPR IEEE, P24256, DOI 10.1109/CVPR52729.2023.02323
   Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   Chen C, 2022, IEEE T MULTIMEDIA, V24, P3679, DOI 10.1109/TMM.2021.3105807
   Chen W, 2022, IEEE T MULTIMEDIA, V24, P1844, DOI 10.1109/TMM.2021.3073279
   Clark K, 2020, Arxiv, DOI [arXiv:2003.10555, DOI 10.48550/ARXIV.2003.10555]
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding HH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16301, DOI 10.1109/ICCV48922.2021.01601
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feng G, 2021, PROC CVPR IEEE, P15501, DOI 10.1109/CVPR46437.2021.01525
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hao ZW, 2022, IEEE T MULTIMEDIA, V24, P4262, DOI 10.1109/TMM.2022.3192663
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hu LM, 2023, Arxiv, DOI arXiv:2212.05699
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Hu ZW, 2020, PROC CVPR IEEE, P4423, DOI 10.1109/CVPR42600.2020.00448
   Huang L., 2022, Adv. Neural Inf. Process. Syst., V35, P19997
   Huang S., 2020, P IEEE CVF C COMP VI, P10485, DOI DOI 10.1109/CVPR42600.2020.01050
   Huang T, 2022, Arxiv, DOI arXiv:2205.14589
   Hui Tianrui, 2020, COMPUTER VISION ECCV, DOI DOI 10.1007/978-3-030-58607-2_4
   Jing Y, 2021, PROC CVPR IEEE, P9853, DOI 10.1109/CVPR46437.2021.00973
   Kim N, 2022, PROC CVPR IEEE, P18124, DOI 10.1109/CVPR52688.2022.01761
   Krahenbuhl P., 2011, NeurIPS, V24
   Li MC, 2021, 35 C NEURAL INFORM P, V34
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Lin L, 2022, IEEE T MULTIMEDIA, V24, P1922, DOI 10.1109/TMM.2021.3074008
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143
   Liu JH, 2022, Arxiv, DOI [arXiv:2205.13137, 10.48550/arXiv.2205.13137]
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Luo G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1274, DOI 10.1145/3394171.3414006
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Paszke A, 2019, ADV NEUR IN, V32
   Peng ZL, 2022, Arxiv, DOI arXiv:2208.06366
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Radford A, 2021, PR MACH LEARN RES, V139
   Rao YM, 2022, PROC CVPR IEEE, P18061, DOI 10.1109/CVPR52688.2022.01755
   Shi HC, 2021, IEEE T MULTIMEDIA, V23, P995, DOI 10.1109/TMM.2020.2991504
   Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3
   Sun SW, 2023, Arxiv, DOI arXiv:2302.10494
   Tian K., 2023, arXiv
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JG, 2024, IEEE T MULTIMEDIA, V26, P234, DOI 10.1109/TMM.2023.3263552
   Wang ZQ, 2022, PROC CVPR IEEE, P11676, DOI 10.1109/CVPR52688.2022.01139
   Wu JZ, 2022, Arxiv, DOI arXiv:2209.09554
   Xie ZD, 2022, PROC CVPR IEEE, P9643, DOI 10.1109/CVPR52688.2022.00943
   Yang Z, 2022, PROC CVPR IEEE, P18134, DOI 10.1109/CVPR52688.2022.01762
   Yang ZD, 2022, LECT NOTES COMPUT SC, V13671, P53, DOI 10.1007/978-3-031-20083-0_4
   Ye LW, 2020, IEEE T MULTIMEDIA, V22, P3224, DOI 10.1109/TMM.2020.2971171
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhu CY, 2022, LECT NOTES COMPUT SC, V13695, P598, DOI 10.1007/978-3-031-19833-5_35
NR 55
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6906
EP 6916
DI 10.1109/TMM.2024.3358085
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000040
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, ZX
   Li, F
   Zhang, YF
   Zhang, Y
AF Wang, Zixi
   Li, Fan
   Zhang, Yunfei
   Zhang, Yuan
TI Low-Rate Feature Compression for Collaborative Intelligence: Reducing
   Redundancy in Spatial and Statistical Levels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative intelligence; redundancy reduction; feature compression;
   human-in-loop; visual analysis
ID IMAGE COMPRESSION; OBJECT DETECTION; NEURAL-NETWORKS; EDGE; INTERNET;
   CLOUD; IOT
AB To distribute the storage and computation load caused by growing capacity of deep neural network (DNN), collaborative intelligence (CI) framework has been proposed, where a deep model is split and executed in two distributed devices respectively. Intermediate feature must be transferred from the front end to the back in order to perform distributed inference, thus transmission process is the bottleneck that influences the inference efficiency in terms of accuracy and delay. Specifically for a bandwidth-limited human-in-loop visual analysis task, feature compression approach needs exploration to reduce the data volume to be transmitted, in order to achieve low transmission delay as well as maintain analysis performance and human perception ability. In this article, the redundancy of intermediate feature both in spatial and statistical levels are firstly analyzed. A mathematical expression for the goal of feature compression is formulated, based on which a two-level redundancy removal based low-rate feature compression approach is proposed. For the front-end device, an information squeezing (IS) module is developed to squeeze the key information of input image and inject them into a low-resolution image. Then a backbone network is split into two parts with respects to the application demands of CI, and can be deployed at the front and back ends correspondingly. With a specifically designed objective function, IS module and the partitioned backbone network are optimized collaboratively to reduce the two-level redundancy, thus compressing the intermediate feature. A generative adversarial network (GAN)-based restoration module is proposed to recover an image with original resolution from the compressed feature, for satisfying human perception. Comprehensive experiments are conduct to validate the efficiency of the proposed method.
C1 [Wang, Zixi; Li, Fan; Zhang, Yunfei] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Shaanxi Key Lab Deep Space Explorat Intelligent In, Xian 710049, Peoples R China.
   [Zhang, Yuan] China Telecom Res Inst, Shanghai 200122, Peoples R China.
C3 Xi'an Jiaotong University
RP Li, F (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Shaanxi Key Lab Deep Space Explorat Intelligent In, Xian 710049, Peoples R China.
EM wzx284788391@stu.xjtu.edu.cn; lifan@mail.xjtu.edu.cn;
   lingdu@stu.xjtu.edu.cn; zhangy666@chinatelecom.cn
OI Zhang, Yuan/0000-0003-0032-8631
FU Xinjiang Uygur Autonomous Region
FX No Statement Available
CR Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031
   [Anonymous], 2018, libbpg
   [Anonymous], 2013, ABOUT US
   [Anonymous], 2020, VVC test model version 10.2
   [Anonymous], 1998, libjpeg
   [Anonymous], 2020, VERSATILE VIDEO CODI
   Bajic IV, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P8493, DOI 10.1109/ICASSP39728.2021.9413943
   Balle J., 2018, ICLR
   Balle J., 2017, P INT C LEARN REPR
   Banner R, 2019, ADV NEUR IN, V32
   Blau Y., 2018, EUROPEAN C COMPUTER
   Chen Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2414, DOI 10.1145/3343031.3350849
   Chen Z, 2020, IEEE T IMAGE PROCESS, V29, P2230, DOI 10.1109/TIP.2019.2941660
   Choi H, 2018, IEEE INT WORKSH MULT
   Choi H, 2022, IEEE T IMAGE PROCESS, V31, P2739, DOI 10.1109/TIP.2022.3160602
   Choi H, 2018, IEEE IMAGE PROC, P3743, DOI 10.1109/ICIP.2018.8451100
   Cover T. M., 2006, Elements of Information Theory
   David M, 2018, Tensorflow/compression
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding L, 2020, IEEE T IMAGE PROCESS, V29, P3734, DOI 10.1109/TIP.2020.2965306
   Eshratifar AE, 2019, I SYMPOS LOW POWER E, DOI 10.1109/islped.2019.8824955
   Eshratifar AE, 2021, IEEE T MOBILE COMPUT, V20, P565, DOI 10.1109/TMC.2019.2947893
   Guo YD, 2019, IEEE T MULTIMEDIA, V21, P2903, DOI 10.1109/TMM.2019.2912703
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hofmann M, 2022, IEEE T NEUR NET LEAR, V33, P3094, DOI 10.1109/TNNLS.2021.3050422
   Hu DY, 2020, 2020 ACM/IEEE FIFTH INTERNATIONAL CONFERENCE ON INTERNET OF THINGS DESIGN AND IMPLEMENTATION (IOTDI 2020), P157, DOI 10.1109/IoTDI49375.2020.00023
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Trinh H, 2018, IEEE T MULTIMEDIA, V20, P2562, DOI 10.1109/TMM.2018.2865661
   IEEEOpen J, 2021, Circuits Syst., V2, P350
   Iwai S, 2021, INT C PATT RECOG, P8235, DOI 10.1109/ICPR48806.2021.9412185
   Jankowski M, 2020, IEEE INT WORK SIGN P, DOI 10.1109/spawc48557.2020.9154306
   Jeong HJ, 2018, INT CON DISTR COMP S, P1492, DOI 10.1109/ICDCS.2018.00154
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   Jolicoeur-Martineau A., 2018, INT C LEARN REPR
   Kang YP, 2017, ACM SIGPLAN NOTICES, V52, P615, DOI 10.1145/3093336.3037698
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Li GL, 2018, LECT NOTES COMPUT SC, V11139, P402, DOI 10.1007/978-3-030-01418-6_40
   Li H, 2018, IEEE NETWORK, V32, P96, DOI 10.1109/MNET.2018.1700202
   Long CC, 2018, IEEE T MULTIMEDIA, V20, P1126, DOI 10.1109/TMM.2017.2764330
   Mao YL, 2018, 2018 THIRD IEEE/ACM SYMPOSIUM ON EDGE COMPUTING (SEC), P90, DOI 10.1109/SEC.2018.00014
   Matsubara Y, 2022, Supervised-compression
   Matsubara Y, 2021, INT C PATT RECOG, P2272, DOI 10.1109/ICPR48806.2021.9412388
   Matsubara Y, 2020, IEEE ACCESS, V8, P212177, DOI 10.1109/ACCESS.2020.3039714
   Matsubara Yoshitomo, 2022, P IEEE CVF WINT C AP, P2685
   Mei YX, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675387
   Mentzer F., 2020, Advances in Neural Information Processing Systems, V33, P11913
   Mireshghallah F, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P3, DOI 10.1145/3373376.3378522
   Patwa N, 2020, IEEE IMAGE PROC, P1281, DOI [10.1109/icip40778.2020.9191247, 10.1109/ICIP40778.2020.9191247]
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Shao JW, 2020, IEEE INT CONF COMM
   Shwartz-Ziv R, 2017, Arxiv, DOI arXiv:1703.00810
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Suzuki S, 2022, IEEE T CIRC SYST VID, V32, P3934, DOI 10.1109/TCSVT.2021.3107716
   Tishby N., 1999, P 37 ANN ALL C COMM, P368
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Torfason R., 2018, P INT C LEARN REPR, P1
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2407, DOI 10.1145/3219819.3220106
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang Y, 2020, IEEE T MULTIMEDIA, V22, P1193, DOI 10.1109/TMM.2019.2939753
   Wang ZX, 2022, IEEE INTERNET THINGS, V9, P16181, DOI 10.1109/JIOT.2022.3150417
   Wu DP, 2021, IEEE T MULTIMEDIA, V23, P2208, DOI 10.1109/TMM.2021.3066050
   Wu L., 2020, P 2020 IEEE CVF WINT, P2334
   Yan N, 2021, IEEE T IMAGE PROCESS, V30, P8939, DOI 10.1109/TIP.2021.3121131
   Yang F, 2021, PROC CVPR IEEE, P4996, DOI 10.1109/CVPR46437.2021.00496
   Yang S, 2021, IEEE T MULTIMEDIA, V23, P2957, DOI 10.1109/TMM.2021.3068580
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu SJ, 2021, IEEE T NEUR NET LEAR, V32, P435, DOI 10.1109/TNNLS.2020.2968509
   Zeng LK, 2019, IEEE NETWORK, V33, P96, DOI 10.1109/MNET.001.1800506
   Zhao LC, 2020, IEEE T INF FOREN SEC, V15, P1486, DOI 10.1109/TIFS.2019.2939713
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhou P, 2019, IEEE T MULTIMEDIA, V21, P539, DOI 10.1109/TMM.2018.2885509
NR 74
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2756
EP 2771
DI 10.1109/TMM.2023.3303716
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400016
DA 2024-08-05
ER

PT J
AU Wen, ZQ
   Niu, SC
   Li, G
   Wu, QY
   Tan, MK
   Wu, Q
AF Wen, Zhiquan
   Niu, Shuaicheng
   Li, Ge
   Wu, Qingyao
   Tan, Mingkui
   Wu, Qi
TI Test-Time Model Adaptation for Visual Question Answering With Debiased
   Self-Supervisions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Test-time adaptation; visual question answering; test-time debiased
   self-supervised
AB Visual question answering (VQA) is a prevalent task in real-world, and plays an essential role in helping the blind understand the physical world. However, due to the real-world complexity, VQA test samples may come from a different distribution from the training data, resulting in unavoidable performance degradation. This similar issue also exists in the image recognition field, in which one most recent effective solutions is a test-time adaptation (TTA). TTA adapts a trained model at test time using only test samples, which provides a new idea to alleviate the analogous issue in VQA. However, naively introducing existing TTA methods (e.g., test-time entropy minimisation) into VQA is imperfect and achieves only marginal performance gain. The reason is that prior methods do not consider the special nature of the VQA problem and ignore that 1) the biased samples in the dataset may have negative effects on test-time model adaptation, and 2) the model may have captured the biases in the dataset. In this paper, we propose Test-time Debiased Self-supervised (TDS) learning objectives for VQA model adaptation. Specifically, we minimise the entropy for those unbiased test samples. To identify these samples, we construct a negative sample for each test sample, and regard the test samples as unbiased if the output answers are different when feeding the test sample and the counterpart negative sample into the VQA model. Meanwhile, we also remove those samples with high prediction entropy from adaptation, making the test-time gradients more reliable. To hinder the model from excessively fitting the superficial correlations of the biased sample, we adopt the biased samples and the counterpart negative samples to assist the adaptation. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate the effectiveness of our TDS.
C1 [Wen, Zhiquan; Niu, Shuaicheng; Wu, Qingyao; Tan, Mingkui; Wu, Qi] South China Univ Technol, Sch Software Engn, Guangzhou 510000, Peoples R China.
   [Li, Ge] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
C3 South China University of Technology; Peking University
RP Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510000, Peoples R China.
EM sewenzhiquan@mail.scut.edu.cn; sensc@mail.scut.edu.cn;
   geli@ece.pku.edu.cn; qyw@scut.edu.cn; mingkuitan@scut.edu.cn;
   qi.wu01@adelaide.edu.au
OI Wu, Qi/0000-0003-3631-256X
FU Ministry of Science and Technology Foundation
FX No Statement Available
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bartler A, 2022, PR MACH LEARN RES, V151
   Boudiaf M, 2022, PROC CVPR IEEE, P8334, DOI 10.1109/CVPR52688.2022.00816
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Cadene Remi, 2019, ADV NEUR IN, P839
   Chi ZX, 2021, PROC CVPR IEEE, P9133, DOI 10.1109/CVPR46437.2021.00902
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Gao J., 2022, P ICML WORKSH UPD MA, P1
   Gidaris S, 2018, ICLR, DOI DOI 10.1109/ICCV.2019.00156
   Gokhale T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P878
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guo WY, 2020, AAAI CONF ARTIF INTE, V34, P91
   Hansen N., 2021, P INT C LEARN REPR, P1
   Ke LYM, 2019, PROC CVPR IEEE, P6734, DOI 10.1109/CVPR.2019.00690
   Li MX, 2022, AAAI CONF ARTIF INTE, P10983
   Li Y., 2021, Advances in Neural Information Processing Systems, P2583
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu H, 2022, PROC CVPR IEEE, P5821, DOI 10.1109/CVPR52688.2022.00574
   Liu Y., 2021, Advances in Neural Information Processing Systems, V34, P21808
   Liu YH, 2023, IEEE T MULTIMEDIA, V25, P5344, DOI 10.1109/TMM.2022.3190686
   Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081
   Lu JS, 2019, ADV NEUR IN, V32
   Nguyen K, 2019, PROC CVPR IEEE, P12519, DOI 10.1109/CVPR.2019.01281
   Niu S., 2023, P INT C LEARN REPR, P1
   Niu Shuaicheng, 2022, INT C MACHINE LEARNI
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Ouyang NL, 2021, IEEE T MULTIMEDIA, V24, P3405, DOI 10.1109/TMM.2021.3097502
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Pong Vitchyr H., 2022, P MACHINE LEARNING R
   Qi YK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1635, DOI 10.1109/ICCV48922.2021.00168
   Qian TW, 2023, IEEE T MULTIMEDIA, V25, P3950, DOI 10.1109/TMM.2022.3169065
   Qin BS, 2023, IEEE T MULTIMEDIA, V25, P4282, DOI 10.1109/TMM.2022.3173131
   Qiu Zhen, 2021, P 30 INT JOINT C ART, P2921, DOI DOI 10.24963/IJCAI.2021/402
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramakrishnan S., 2018, INT C NEURAL INF PRO, P1548
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saqur R., 2020, Advances in Neural Information Processing Systems. Ed. by, V33, P3070
   Selvaraju RR, 2019, IEEE I CONF COMP VIS, P2591, DOI 10.1109/ICCV.2019.00268
   Shu M., 2022, Advances in Neural Information Processing Systems, P14274
   Sun Y., 2020, PMLR, V119, P9229
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tan M., 2023, ACM Trans. Multimedia Comput. Commun. Appl., V1, P1
   Teney D, 2020, INT C NEURAL INF PRO, P407
   Wang D., 2021, INT C LEARN REPR
   Wang Q, 2022, PROC CVPR IEEE, P7191, DOI 10.1109/CVPR52688.2022.00706
   Wen Z., 2023, P FIND ASS COMP LING, P1
   Wen ZQ, 2021, ADV NEUR IN, V34
   Wu JL, 2022, AAAI CONF ARTIF INTE, P2712
   Wu JL, 2019, ADV NEUR IN, V32
   Xu GH, 2021, PROC CVPR IEEE, P12632, DOI 10.1109/CVPR46437.2021.01245
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Zhang M., 2022, Advances in Neural Information Processing Systems, V35, P38629
   Zhang X, 2022, IEEE T MULTIMEDIA, V24, P2986, DOI 10.1109/TMM.2021.3091882
   Zheng Y., 2020, PROC ASIAN C COMPUT, P137
   Zhi X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1083
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 59
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2137
EP 2147
DI 10.1109/TMM.2023.3292597
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100030
DA 2024-08-05
ER

PT J
AU Wu, XT
   Feng, XJ
AF Wu, Xiaotian
   Feng, Xinjie
TI Size Invariant Visual Cryptography Schemes With Evolving Threshold
   Access Structures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Cryptography; Simulated annealing; Image reconstruction;
   Generators; Brightness; Stacking; Secret sharing; visual cryptography;
   evolving access structure; contrast; simulated annealing
ID SECRET SHARING SCHEMES; ENCRYPTION
AB In this research, we consider the evolving threshold access structure, denoted as (k, infinity), for size invariant visual cryptography scheme (SIVCS). The so-called (k, infinity) threshold indicates the number of participants is supposed to be infinite and the access structure would be dynamically adjusted at any time by adding or deleting participants. First of all, the concept and definition of (k, infinity)-SIVCS are described. Shadow construction, constituted by random number generators and their choosing probabilities, for the (k, infinity)-SIVCS is then given. A contrast-maximizing problem for determining the generators and choosing probabilities is built based on the (k, infinity)-SIVCS. A simulated annealing-based algorithm is introduced to solve the optimization problem. The best solution from the simulated annealing-based algorithm forms a feasible (k, infinity)-SIVCS. To further improve the visual quality, a (k, infinity)-SIVCS using Boolean XOR decryption is also presented. Experimental results and comparisons are shown, demonstrating that the proposed techniques are feasible and advanced in the aspects of shadow size and contrast.
C1 [Wu, Xiaotian; Feng, Xinjie] Jinan Univ, Dept Comp Sci, Guangdong Key Lab Data Secur & Privacy Preserving, Guangzhou 510632, Peoples R China.
C3 Jinan University
RP Wu, XT (corresponding author), Jinan Univ, Dept Comp Sci, Guangdong Key Lab Data Secur & Privacy Preserving, Guangzhou 510632, Peoples R China.
EM wxt.sysu@gmail.com; fengxj19@outlook.com
FU National Natural Science Foundation of China
FX No Statement Available
CR ASMUTH C, 1983, IEEE T INFORM THEORY, V29, P208, DOI 10.1109/TIT.1983.1056651
   Ateniese G, 1996, INFORM COMPUT, V129, P86, DOI 10.1006/inco.1996.0076
   Beugnon S, 2019, IEEE T MULTIMEDIA, V21, P2171, DOI 10.1109/TMM.2019.2900905
   Blundo C, 2003, SIAM J DISCRETE MATH, V16, P224, DOI 10.1137/S0895480198336683
   Chen B, 2022, IEEE T DEPEND SECURE, V19, P978, DOI 10.1109/TDSC.2020.3011923
   Chen SK, 2012, J VIS COMMUN IMAGE R, V23, P677, DOI 10.1016/j.jvcir.2012.03.004
   Chen TH, 2011, J SYST SOFTWARE, V84, P1197, DOI 10.1016/j.jss.2011.02.023
   Chen TH, 2009, PATTERN RECOGN, V42, P2203, DOI 10.1016/j.patcog.2008.11.015
   Chih-Ching Thien, 2002, Computers & Graphics, V26, P765, DOI 10.1016/S0097-8493(02)00131-0
   Cimato S, 2006, COMPUT J, V49, P97, DOI 10.1093/comjnl/bxh152
   D'Arco P, 2021, THEOR COMPUT SCI, V859, P149, DOI 10.1016/j.tcs.2021.01.019
   De Prisco R, 2013, THEOR COMPUT SCI, V510, P62, DOI 10.1016/j.tcs.2013.09.005
   Fu ZX, 2019, MULTIMED TOOLS APPL, V78, P2367, DOI 10.1007/s11042-018-6364-z
   He WG, 2021, IEEE T MULTIMEDIA, V23, P52, DOI 10.1109/TMM.2020.2982042
   Hofmeister T, 2000, THEOR COMPUT SCI, V240, P471, DOI 10.1016/S0304-3975(99)00243-1
   Hu YX, 2019, J INF SECUR APPL, V47, P371, DOI 10.1016/j.jisa.2019.06.003
   Iwamoto M, 2007, IEICE T FUND ELECTR, VE90A, P101, DOI 10.1093/ietfec/e90-a.1.101
   Jia XX, 2022, INFORM SCIENCES, V595, P54, DOI 10.1016/j.ins.2022.02.016
   Jia XX, 2018, IEEE T CIRC SYST VID, V28, P1056, DOI 10.1109/TCSVT.2016.2631404
   KAFRI O, 1987, OPT LETT, V12, P377, DOI 10.1364/OL.12.000377
   Komargodski I, 2018, IEEE T INFORM THEORY, V64, P4179, DOI 10.1109/TIT.2017.2779121
   Komargodski I, 2016, LECT NOTES COMPUT SC, V9986, P485, DOI 10.1007/978-3-662-53644-5_19
   Lee JS, 2017, MULTIMED TOOLS APPL, V76, P1, DOI 10.1007/s11042-015-3011-9
   Li J. Ma, 2020, J. Vis. Commun. Image Representation, V72
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Li P, 2018, SIGNAL PROCESS-IMAGE, V65, P210, DOI 10.1016/j.image.2018.04.002
   Lin CC, 2004, J SYST SOFTWARE, V73, P405, DOI 10.1016/S0164-1212(03)00239-5
   Lin SJ, 2012, IEEE T INF FOREN SEC, V7, P197, DOI 10.1109/TIFS.2011.2167229
   Liu YX, 2021, IEEE T INTELL TRANSP, V22, P3952, DOI 10.1109/TITS.2020.2994386
   Liu YX, 2017, SIGNAL PROCESS-IMAGE, V58, P49, DOI 10.1016/j.image.2017.06.011
   Liu Z, 2020, ACM T SENSOR NETWORK, V16, DOI 10.1145/3409475
   Liu ZQ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3508394
   Liu ZQ, 2021, SIGNAL PROCESS-IMAGE, V92, DOI 10.1016/j.image.2020.116129
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shen G, 2017, DESIGN CODE CRYPTOGR, V85, P15, DOI 10.1007/s10623-016-0285-5
   Shyu SJ, 2018, IEEE T CIRC SYST VID, V28, P2397, DOI 10.1109/TCSVT.2017.2707923
   Shyu SJ, 2015, IEEE T CIRC SYST VID, V25, P1557, DOI 10.1109/TCSVT.2015.2389372
   Shyu SJ, 2011, IEEE T INF FOREN SEC, V6, P960, DOI 10.1109/TIFS.2011.2158096
   Tuyls P, 2005, DESIGN CODE CRYPTOGR, V37, P169, DOI 10.1007/s10623-004-3816-4
   Wang YM, 2021, IEEE T MULTIMEDIA, V23, P1466, DOI 10.1109/TMM.2020.2999187
   Wu XT, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3517140
   Wu XT, 2022, INFORM SCIENCES, V583, P73, DOI 10.1016/j.ins.2021.11.013
   Wu XT, 2020, J VIS COMMUN IMAGE R, V70, DOI 10.1016/j.jvcir.2020.102793
   Wu XT, 2019, DIGIT SIGNAL PROCESS, V93, P22, DOI 10.1016/j.dsp.2019.06.016
   Wu XT, 2013, SIGNAL PROCESS, V93, P977, DOI 10.1016/j.sigpro.2012.11.014
   Wu XT, 2012, J SYST SOFTWARE, V85, P1119, DOI 10.1016/j.jss.2011.12.041
   Xiong LZ, 2023, IEEE INTERNET THINGS, V10, P1933, DOI 10.1109/JIOT.2021.3139657
   Xiong LZ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3512797
   Xiong LZ, 2021, IEEE T INF FOREN SEC, V16, P2912, DOI 10.1109/TIFS.2021.3065794
   Yan XH, 2021, IEEE T CIRC SYST VID, V31, P2896, DOI 10.1109/TCSVT.2020.3025527
   Yan XH, 2018, J REAL-TIME IMAGE PR, V14, P61, DOI 10.1007/s11554-015-0540-4
   Yan XH, 2015, SIGNAL PROCESS, V109, P317, DOI 10.1016/j.sigpro.2014.12.002
   Yang CN, 2016, THEOR COMPUT SCI, V609, P143, DOI 10.1016/j.tcs.2015.09.016
   Yang CN, 2014, INFORM SCIENCES, V278, P141, DOI 10.1016/j.ins.2014.03.033
   Yang CN, 2004, PATTERN RECOGN LETT, V25, P481, DOI 10.1016/j.patrec.2003.12.011
   Yang P. Li, 2023, J. Inf. Secur. Appl., V73
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
NR 58
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1488
EP 1503
DI 10.1109/TMM.2023.3282573
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700029
DA 2024-08-05
ER

PT J
AU Xian, WZ
   Zhou, ML
   Fang, B
   Xiang, T
   Jia, WJ
   Chen, B
AF Xian, Weizhi
   Zhou, Mingliang
   Fang, Bin
   Xiang, Tao
   Jia, Weijia
   Chen, Bin
TI Perceptual Quality Analysis in Deep Domains Using Structure Separation
   and High-Order Moments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Visualization; Feature extraction; Computational modeling;
   Predictive models; Optimization; Indexes; Convex optimization; deep
   learning; high-order moments; probability distribution; perceptual
   quality; structure representations
ID IMAGE; SIMILARITY; INFORMATION; DEVIATION; DISTANCE
AB Images are composed of "things" (i.e., structured objects) and "stuff" (i.e., textured surfaces), which have completely different effects on the human visual system (HVS). A good image quality assessment (IQA) method should fully consider the visual salience effects of image structures and the masking effects of image textures. In this article, we propose a perceptual quality analysis model using structure separation and high-order moments (SSHMPQA) in the deep domain. First, we use a total variation (TV) model to separate the perceptual structures in images from their deep feature maps, thereby maintaining meaningful object shapes with texture suppression and defining perceptual structure-aware distances in the deep domain. Then, we use the first- to fourth-order moments to calculate the mean, skewness and kurtosis of the probability distributions of the deep features. On this basis, we define a perceptual texture-aware distance in the deep domain. We then formulate the final model by solving a well-defined perceptual optimization problem. The proposed SSHMPQA model has good interpretability and is data-driven; moreover, the model does not require a complex and long training process because the optimization problem is convex and has an exact analytical solution. To verify the effectiveness of our model, comprehensive experiments are conducted. The experimental results show that the proposed model is superior to other state-of-the-art traditional and deep learning-based full-reference (FR) IQA methods.
C1 [Xian, Weizhi; Zhou, Mingliang; Fang, Bin; Xiang, Tao] Chongqing Univ, Sch Comp Sci, Chongqing 400044, Peoples R China.
   [Xian, Weizhi; Chen, Bin] Harbin Inst Technol, Chongqing Res Inst, Chongqing 401151, Peoples R China.
   [Jia, Weijia] BNU HKBU United Int Coll Zhuhai, Beijing Normal Univ Zhuhai & Guangdong Key Lab AI, BNU UIC Inst Artificial Intelligence & Future Netw, Zhuhai 519087, Guangdong, Peoples R China.
   [Chen, Bin] Harbin Inst Technol, Int Res Inst Artificial Intelligence, Shenzhen, Peoples R China.
C3 Chongqing University; Harbin Institute of Technology; Harbin Institute
   of Technology
RP Zhou, ML (corresponding author), Chongqing Univ, Sch Comp Sci, Chongqing 400044, Peoples R China.
EM wasxxwz@163.com; zml-0913yy@163.com; fb@cqu.edu.cn; txiang@cqu.edu.cn;
   jiawj@uic.edu.cn; chenbin2020@hit.edu.cn
RI Chen, Bin/IQV-6112-2023
OI Chen, Bin/0000-0002-3979-021X; Zhou, Mingliang/0000-0002-1874-3641;
   Xian, Weizhi/0000-0001-5137-3542
FU National Natural Science Foundation of China
FX No Statement Available
CR Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Burghouts GJ, 2009, PATTERN RECOGN LETT, V30, P306, DOI 10.1016/j.patrec.2008.10.005
   Cao Y, 2022, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR52688.2022.00576
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ghiasi H., 2017, BRIT MACH VIS C, P1
   Gu J., Image quality assessment for perceptual image restoration: A new dataset benchmark and metric
   Gu JJ, 2021, IEEE COMPUT SOC CONF, P677, DOI 10.1109/CVPRW53098.2021.00077
   Gu Jinjin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P633, DOI 10.1007/978-3-030-58621-8_37
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZQ, 2021, IEEE T CIRC SYST VID, V31, P2808, DOI 10.1109/TCSVT.2020.3027001
   Iandola S., 2017, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuhn H.W., 2013, P 2 BERK S MATH STAT, P247
   Laparra V., 2016, ELECT IMAGING, V2016, P1, DOI DOI 10.2352/ISSN.2470-1173.2016.16.HVEI-103
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P154, DOI 10.1109/TMM.2021.3122347
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu Q, 2018, IEEE T IMAGE PROCESS, V27, P5178, DOI 10.1109/TIP.2018.2849928
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P5206, DOI 10.1109/TIP.2020.2980170
   Liu Y, 2017, PROC CVPR IEEE, P5872, DOI 10.1109/CVPR.2017.622
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Olivier J. H., 2016, PROC INT C LEARN REP, P1
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sim K, 2021, IEEE T MULTIMEDIA, V23, P4037, DOI 10.1109/TMM.2020.3037482
   Sim K, 2022, IEEE T MULTIMEDIA, V24, P1389, DOI 10.1109/TMM.2021.3064240
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5097, DOI 10.1109/ICCV48922.2021.00507
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Tang LM, 2019, APPL MATH MODEL, V69, P355, DOI 10.1016/j.apm.2018.12.021
   Ustyuzhaninov W., 2017, INT C LEARN REPRESEN, P1
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Xian WZ, 2023, IEEE T BROADCAST, V69, P130, DOI 10.1109/TBC.2022.3192997
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   Yang X, 2021, PR MACH LEARN RES, V139
   Yang Y, 2023, IEEE T MULTIMEDIA, V25, P4148, DOI 10.1109/TMM.2022.3171686
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 53
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2219
EP 2234
DI 10.1109/TMM.2023.3293730
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100014
DA 2024-08-05
ER

PT J
AU Xu, N
   Lu, ZM
   Tian, HS
   Kang, RB
   Cao, JB
   Zhang, YD
   Liu, AA
AF Xu, Ning
   Lu, Zimu
   Tian, Hongshuo
   Kang, Rongbao
   Cao, Jinbo
   Zhang, Yongdong
   Liu, An-An
TI Learning to Supervise Knowledge Retrieval Over a Tree Structure for
   Visual Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE KB-VQA; knowledge tree; supervised knowledge retrieva
AB Knowledge-based visual question answering (KBVQA) aims to retrieve the external knowledge out of images to answer questions. However, current methods always introduce various irrelevant knowledge due to two drawbacks: (1) Synonymy issue. Existing methods heavily rely on words from questions or object labels in images to match knowledge from databases, which disregards the same word may hold multiple meanings within different contexts. (2) Knowledge uncertainty issue. Due to the absence of supervisory signals, recent methods can not determine which knowledge is applicable for answer inference, which can mislead to admit useless knowledge. To address these two problems, we propose to supervise the process of knowledge retrieval over a tree structure for KB-VQA task. For the synonymy issue, we construct a hierarchical knowledge tree to capture the subordination information between knowledge facts, mitigating the impact of synonyms on knowledge retrieval. For the knowledge uncertainty issue, we use the retrieval history as the ground truth to supervise the knowledge retrieval, which facilitates the QA model to form an explicit path of knowledge facts for answer understanding. Finally, we integrate the image, question, and retrieved knowledge into a variant of transformer to predict answers. Experimental results validate the effectiveness of the proposed method on KR-VQA, OK-VQA and VQA v2 datasets.
C1 [Xu, Ning; Lu, Zimu; Tian, Hongshuo; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
   [Kang, Rongbao; Cao, Jinbo] 30th Res Inst China Elect Technol Corp, Chengdu 300072, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Hefei 230088, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
EM ningxu@tju.edu.cn; lu_zimu@126.com; kellyeden@tju.edu.cn; krbhl@126.com;
   jinbo4444@sina.cn; zhyd73@ustc.edu.cn; anan0422@gmail.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Asai A, 2023, P 61 ANN M ASS COMP, P41, DOI DOI 10.18653/V1/2023.ACL-TUTORIALS.6
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Cao QX, 2022, IEEE T NEUR NET LEAR, V33, P2758, DOI 10.1109/TNNLS.2020.3045034
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Gan JZ, 2024, IEEE T NEUR NET LEAR, V35, P196, DOI 10.1109/TNNLS.2022.3172588
   Gao F, 2022, PROC CVPR IEEE, P5057, DOI 10.1109/CVPR52688.2022.00501
   Gardères F, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P489
   Gui LK, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P956
   Heo YJ, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P373
   Hildebrandt M, 2020, AAAI CONF ARTIF INTE, V34, P4123
   Ji ZW, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3571730
   Kan BS, 2023, IEEE I CONF COMP VIS, P15624, DOI 10.1109/ICCV51070.2023.01436
   Kim JH, 2018, ADV NEUR IN, V31
   Kim W, 2021, PR MACH LEARN RES, V139
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Li GH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1227, DOI 10.1145/3394171.3413943
   Li J., 2023, P MACHINE LEARNING R, P19730
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li X., 2020, Oscar: Object-Semantics Aligned Pre-training for VisionLanguage Tasks, DOI 10.1007/978-3-030-58577-8_8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin Y., 2022, Advances in Neural Information Processing Systems, V35, P10560
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Liu Yuhang, 2022, P 31 INT JOINT C ART, P3264, DOI DOI 10.24963/IJCAI.2022/453
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lu JS, 2019, ADV NEUR IN, V32
   Marino K, 2021, PROC CVPR IEEE, P14106, DOI 10.1109/CVPR46437.2021.01389
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Mo YJ, 2023, IEEE T KNOWL DATA EN, V35, P12814, DOI 10.1109/TKDE.2023.3268069
   Narasimhan M, 2018, ADV NEUR IN, V31
   Ouyang NL, 2021, IEEE T MULTIMEDIA, V24, P3405, DOI 10.1109/TMM.2021.3097502
   Paszke A, 2019, ADV NEUR IN, V32
   Peng L, 2024, IEEE T NEUR NET LEAR, V35, P8609, DOI 10.1109/TNNLS.2022.3230979
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Qian TW, 2023, IEEE T MULTIMEDIA, V25, P3950, DOI 10.1109/TMM.2022.3169065
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren S., 2016, Faster r-cnn: Towards real-time object detection with region proposal networks
   Shah S, 2019, AAAI CONF ARTIF INTE, P8876
   Shao ZW, 2023, PROC CVPR IEEE, P14974, DOI 10.1109/CVPR52729.2023.01438
   Song YG, 2024, IEEE T MULTIMEDIA, V26, P837, DOI 10.1109/TMM.2023.3272224
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Su W., 2020, P INT C LEARN REPR
   Suchanek Fabian M., 2007, WWW, DOI DOI 10.1145/1242572.1242667
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tandon N, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P523, DOI 10.1145/2556195.2556245
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Vaswani A, 2017, ADV NEUR IN, V30
   Vrandecic D, 2014, COMMUN ACM, V57, P78, DOI 10.1145/2629489
   Wang P, 2022, 39 INT C MACHINE LEA
   Wang P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1290
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wang RN, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P2396
   Wang Z., 2022, Advances in Neural Information Processing Systems, P8483
   Wu JL, 2022, AAAI CONF ARTIF INTE, P2712
   Wu S, 2024, IEEE T MULTIMEDIA, V26, P1790, DOI 10.1109/TMM.2023.3289729
   Wu YH, 2016, Arxiv, DOI [arXiv:1609.08144, DOI 10.48550/ARXIV.1609.08144]
   Yang ZY, 2022, AAAI CONF ARTIF INTE, P3081
   Yu J, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107563
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zeng PP, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5210, DOI 10.1145/3503161.3548024
   Zhang LY, 2021, IEEE T NEUR NET LEAR, V32, P4362, DOI 10.1109/TNNLS.2020.3017530
   Zheng L., 2023, arXiv
   Zheng WB, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2360, DOI 10.1145/3447548.3467285
   Zheng WB, 2021, INFORM FUSION, V67, P14, DOI 10.1016/j.inffus.2020.10.007
   Zhou YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2054, DOI 10.1109/ICCV48922.2021.00208
   Zhu YH, 2022, INFORM FUSION, V77, P53, DOI 10.1016/j.inffus.2021.07.013
   Zhu ZH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1097
NR 71
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6689
EP 6700
DI 10.1109/TMM.2024.3355638
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600007
DA 2024-08-05
ER

PT J
AU Yang, B
   Xiang, XQ
   Kong, WZ
   Zhang, JH
   Peng, Y
AF Yang, Bing
   Xiang, Xueqin
   Kong, Wangzeng
   Zhang, Jianhai
   Peng, Yong
TI DMF-GAN: Deep Multimodal Fusion Generative Adversarial Networks for
   Text-to-Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep multimodal fusion; generative adversarial network; text-to-image
   (T2I) synthesis
AB Text-to-image synthesis aims to generate high-quality realistic images conditioned on text description. The great challenge of this task depends on deeply and seamlessly integrating image and text information. Thus, in this paper, we propose a deep multimodal fusion generative adversarial networks (DMF-GAN) that allows effective semantic interactions for fine-grained text-to-image generation. Specifically, through a novel recurrent semantic fusion network, DMF-GAN could consistently manipulate global assignment of text information among isolated fusion blocks. With the assistance of a multi-head attention module, DMF-GAN could model word information from different perspectives and further improve the semantic consistency. In addition, a word-level discriminator is proposed to provide the generator with fine-grained feedback related to each word. Compared with current state-of-the-art methods, our proposed DMF-GAN could efficiently synthesize realistic and text-alignment images and achieve better performance on challenging benchmarks.
C1 [Yang, Bing; Kong, Wangzeng; Zhang, Jianhai; Peng, Yong] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Yang, Bing; Kong, Wangzeng; Zhang, Jianhai; Peng, Yong] Zhejiang Key Lab Brain Machine Collaborat Intellig, Hangzhou 310018, Peoples R China.
   [Xiang, Xueqin] Rokid Inc, Hangzhou 311121, Peoples R China.
C3 Hangzhou Dianzi University
RP Kong, WZ (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
EM yb@hdu.edu.cn; xxueq@aliyun.com; kongwanzeng@hdu.edu.cn;
   jhzhang@hdu.edu.cn; yongpeng@hdu.edu.cn
RI ; peng, yong/JCO-0601-2023
OI xiang, xueqin/0009-0004-4767-8078; peng, yong/0000-0003-1208-972X; yang,
   bing/0000-0002-0585-0579; Kong, Wanzeng/0000-0002-0113-6968
FU Zhejiang Basic Public Welfare Research Program
FX No Statement Available
CR Agnese J, 2020, WIRES DATA MIN KNOWL, V10, DOI 10.1002/widm.1345
   Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374
   Bird S., 2009, Natural Language Processing with Python, DOI DOI 10.5555/1717171
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Celikyilmaz A, 2021, Arxiv, DOI arXiv:2006.14799
   Chefer H, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592116
   Cheng F., 2020, CVPR, P10911
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding M., 2021, Advances in Neural Information Processing Systems (NeurIPS-21), V34, P19822
   Gou YC, 2020, Arxiv, DOI arXiv:2005.12444
   Gui J, 2023, IEEE T KNOWL DATA EN, V35, P3313, DOI 10.1109/TKDE.2021.3130191
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   Hensel M, 2017, ADV NEUR IN, V30
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Jiadong Liang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P491, DOI 10.1007/978-3-030-58548-8_29
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   Kocasari U, 2022, IEEE WINT CONF APPL, P3441, DOI 10.1109/WACV51458.2022.00350
   Lee H, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS-ASIA (ICCE-ASIA), DOI 10.1109/ICCE-Asia53811.2021.9641929
   Li B., 2020, 2020 IEEE CVF C COMP, P7877
   Li B., 2020, Advances in Neural Information Processing Systems, P22020, DOI DOI 10.48550/ARXIV.2010.12136
   Li B, 2021, J NEUROL, V268, P2042, DOI 10.1007/s00415-019-09596-3
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Liao WT, 2022, PROC CVPR IEEE, P18166, DOI 10.1109/CVPR52688.2022.01765
   Lin JY, 2021, Arxiv, DOI arXiv:2103.00823
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BC, 2021, AAAI CONF ARTIF INTE, V35, P2082
   Melis G., 2020, PROC INT C LEARN REP, P1
   Qiao TT, 2019, ADV NEUR IN, V32
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Redmon J., 2018, CoRR
   Reed S, 2017, PR MACH LEARN RES, V70
   Reed S, 2016, PR MACH LEARN RES, V48
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ruan SL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13940, DOI 10.1109/ICCV48922.2021.01370
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T., 2016, Syst, V29, P2234
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shahriar S, 2022, DISPLAYS, V73, DOI 10.1016/j.displa.2022.102237
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stacchio C., 2023, PROC AIIA WORKSHOP A, P38
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan HC, 2023, IEEE T MULTIMEDIA, V25, P8620, DOI 10.1109/TMM.2023.3238554
   Tan HC, 2023, IEEE T NEUR NET LEAR, V34, P10309, DOI 10.1109/TNNLS.2022.3165573
   Tao M, 2023, PROC CVPR IEEE, P14214, DOI 10.1109/CVPR52729.2023.01366
   Tao M, 2022, PROC CVPR IEEE, P16494, DOI 10.1109/CVPR52688.2022.01602
   thao i, 2023, IEEE Trans. Multimedia, DOI [10.1109/1MM.2023.3297769, DOI 10.1109/1MM.2023.3297769]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xiao J, 2023, NEURAL NETWORKS, V167, P433, DOI 10.1016/j.neunet.2023.08.038
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Xue A, 2021, IEEE WINT CONF APPL, P3862, DOI 10.1109/WACV48630.2021.00391
   Ye SM, 2024, IEEE T MULTIMEDIA, V26, P462, DOI 10.1109/TMM.2023.3266607
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang CS, 2023, Arxiv, DOI arXiv:2303.07909
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang H, 2021, PROC CVPR IEEE, P833, DOI 10.1109/CVPR46437.2021.00089
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang I.., 2021, PROC INT JOINT C NEU, P1
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhou Y, 2024, TSINGHUA SCI TECHNOL, V29, P469, DOI 10.26599/TST.2023.9010023
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
   Zhu YZ, 2023, PROC CVPR IEEE, P14235, DOI 10.1109/CVPR52729.2023.01368
NR 71
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6956
EP 6967
DI 10.1109/TMM.2024.3358086
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000065
DA 2024-08-05
ER

PT J
AU Yang, X
   Zhao, CY
   Yang, JQ
   Song, Y
   Zhao, YF
AF Yang, Xin
   Zhao, Chenyang
   Yang, Jinqi
   Song, Yong
   Zhao, Yufei
TI Negative-Driven Training Pipeline for Siamese Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Negative example mining; Siamese network; training pipeline; visual
   tracking
AB Although Siamese trackers have recently been gradually replaced by complicated and computationally expensive Transformer trackers, they are simpler and more applicable to real-world deployment. We believe that room for improvement still exists in the Siamese tracking framework and attribute the performance limitation to inadequate annotation, insufficient augmentation and suboptimal assignment, which heavily weakens the discriminative power. As Siamese methods directly inherit the assignment manner from object detection, they both face the imbalance between sparse annotated objects and dense background examples and between easy and hard negative examples. Moreover, existing augmentations and negative pairs are insufficient to simulate practical tracking ambiguity and failure cases. Nevertheless, work in the training vein is still overlooked. Therefore, we strive to yield a negative-driven training pipeline to unleash the potential of the Siamese framework without any extra inference cost. Specifically, 1) We devise strong negative augmentations based on random copy-paste to take full advantage of available annotations and generate more challenging tracking scenarios, especially negative examples. 2) We propose a semisupervised two-phase assignment that jointly utilizes existing annotations and model outputs to mine more appropriate and challenging negative examples. 3) We formulate a complementary reweighting loss by modifying the loss weight matrix to bridge subtasks and highlight the contributions of hard negative examples more smoothly. We choose several classic Siamese trackers to validate the pipeline effectiveness. After training, these trackers can gain, at most, a nearly 14% relative increase in performance, which is comparable to advanced Siamese trackers and even Transformer trackers. The experimental results indicate that the tracking-specific training pipeline is an efficient method for strengthening trackers and requires further development.
C1 [Yang, Xin; Zhao, Chenyang; Yang, Jinqi; Song, Yong; Zhao, Yufei] Beijing Inst Technol, Sch Opt & Photon, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Song, Y (corresponding author), Beijing Inst Technol, Sch Opt & Photon, Beijing 100081, Peoples R China.
EM xinyang@bit.edu.cn; chenyangzhao@bit.edu.cn; jinqiyang@bit.edu.cn;
   yongsong@bit.edu.cn; zhaoyufeibit@outlook.com
OI Yang, Xin/0000-0003-1041-0030; Yang, Jinqi/0000-0001-5196-2427
FU National Natural Science Foundation of China
FX No Statement Available
CR Arora V, 2017, IEEE IJCNN, P2298, DOI 10.1109/IJCNN.2017.7966134
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Carion N., 2020, EUR C COMP VIS, P213
   Chen BY, 2022, LECT NOTES COMPUT SC, V13682, P375, DOI 10.1007/978-3-031-20047-2_22
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng Z., 2021, PROC IEEE INT C MULT, P1
   Choi J., 2019, PROC INT C ADV NEURA
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Fan BJ, 2022, IEEE T MULTIMEDIA, V24, P2766, DOI 10.1109/TMM.2021.3087347
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Huang LC, 2015, Arxiv, DOI arXiv:1509.04874
   Junliang Xing, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1698, DOI 10.1109/ICPR.2010.420
   Kim M, 2022, LECT NOTES COMPUT SC, V13682, P534, DOI 10.1007/978-3-031-20047-2_31
   Kristan M., 2020, ECCVW, P547
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Liang Justin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9128, DOI 10.1109/CVPR42600.2020.00915
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu TP, 2023, IEEE T MULTIMEDIA, V25, P5330, DOI 10.1109/TMM.2022.3190679
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Reddy Tharun Kumar, 2019, Computational Intelligence: Theories, Applications and Future DirectionsVolume II. ICCI-2017. Advances in Intelligent Systems and Computing (AISC 799), P229, DOI 10.1007/978-981-13-1135-2_18
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sharma RS, 2021, IEEE T IND INFORM, V17, P110, DOI 10.1109/TII.2020.2983646
   Sharma RS, 2019, IEEE INT CONF ROBOT, P8613, DOI [10.1109/ICRA.2019.8793911, 10.1109/icra.2019.8793911]
   Shen QH, 2022, PROC CVPR IEEE, P8091, DOI 10.1109/CVPR52688.2022.00793
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Sio CH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1948, DOI 10.1145/3394171.3413611
   Sun ZJ, 2019, MACH VISION APPL, V30, P487, DOI 10.1007/s00138-019-01004-0
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tokmakov P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10840, DOI 10.1109/ICCV48922.2021.01068
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang Y, 2020, KNOWL-BASED SYST, V194, DOI 10.1016/j.knosys.2020.105594
   Wu QQ, 2021, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR46437.2021.00301
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie F, 2022, PROC CVPR IEEE, P8741, DOI 10.1109/CVPR52688.2022.00855
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Xuan SY, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107698
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Yang T., 2018, Advances in Neural Information Processing Systems
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Yuan WH, 2020, IEEE INT C INT ROBOT, P10351, DOI 10.1109/IROS45743.2020.9341621
   Zha YF, 2022, IEEE MULTIMEDIA, V29, P80, DOI 10.1109/MMUL.2022.3207239
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zheng JL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13526, DOI 10.1109/ICCV48922.2021.01329
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhou LJ, 2021, AAAI CONF ARTIF INTE, V35, P3581
   Zhou ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9846, DOI 10.1109/ICCV48922.2021.00972
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Zhuang YH, 2021, IEEE INT CONF BIG DA, P3223, DOI 10.1109/BigData52589.2021.9671965
NR 73
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4416
EP 4429
DI 10.1109/TMM.2023.3323134
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100003
DA 2024-08-05
ER

PT J
AU Yang, ZJ
   Chen, KJ
   Zeng, K
   Zhang, WM
   Yu, NH
AF Yang, Zijin
   Chen, Kejiang
   Zeng, Kai
   Zhang, Weiming
   Yu, Nenghai
TI Provably Secure Robust Image Steganography
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Steganography; Security; Robustness; Generators; Social networking
   (online); Receivers; Transforms; provably secure; image; robust;
   optimization
AB The maturity of generative models and the popularity of generated data have brought new technical means and camouflage environments to steganography. Numerous generative image steganography methods have emerged, but achieving provable security, robustness, and relatively high capacity simultaneously remains challenging. This paper proposes a provably secure robust image steganography method via the generative adversarial network (GAN), named PARIS. The sender maps the secret message, following a uniform distribution, to latent vectors conforming to a standard Gaussian distribution using inverse transform sampling. Subsequently, the latent vector is fed into the generator, producing the stego image. In this way, the stego image cannot be distinguished from the normally generated image. The receiver extracts the secret message from the recovered latent vector via gradient descent optimization. To enhance the robustness, a noise layer is introduced while recovering the latent vector to simulate potential lossy operations in real scenarios. The security of the proposed method is theoretically proven. Extensive experiments have also verified the proposed method's robustness, security, and relatively high capacity in terms of different GAN architectures, noises, and datasets.
C1 [Yang, Zijin; Chen, Kejiang; Zeng, Kai; Zhang, Weiming; Yu, Nenghai] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
   [Yang, Zijin; Chen, Kejiang; Zeng, Kai; Zhang, Weiming; Yu, Nenghai] Anhui Prov Key Lab Cyberspace Secur Situat Awarene, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, KJ (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.; Chen, KJ (corresponding author), Anhui Prov Key Lab Cyberspace Secur Situat Awarene, Hefei 230026, Peoples R China.
EM bsmhmmlf@mail.ustc.edu.cn; chenkj@ustc.edu.cn; zk0128@mail.ustc.edu.cn;
   zhangwm@ustc.edu.cn; ynh@ustc.edu.cn
OI Zeng, Kai/0009-0007-6275-9767; Zhang, Weiming/0000-0001-5576-6108; Chen,
   Kejiang/0000-0002-9868-3414
FU National Natural Science Foundation of China
FX No Statement Available
CR Berthelot D, 2017, Arxiv, DOI [arXiv:1703.10717, DOI 10.48550/ARXIV.1703.10717]
   Cachin C, 1998, LECT NOTES COMPUT SC, V1525, P306
   Chen KJ, 2022, IEEE T DEPEND SECURE, V19, P3343, DOI 10.1109/TDSC.2021.3095072
   Daemen J, 2020, Information Security and Cryptography, V2nd
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Fridrich J, 2009, Steganography in Digital Media: Principles Algorithms and Applications
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guan ZY, 2023, IEEE T PATTERN ANAL, V45, P372, DOI 10.1109/TPAMI.2022.3141725
   Hensel M, 2017, ADV NEUR IN, V30
   Hopper NJ, 2002, LECT NOTES COMPUT SC, V2442, P77
   Hu DH, 2018, IEEE ACCESS, V6, P38303, DOI 10.1109/ACCESS.2018.2852771
   Gulrajani I, 2017, ADV NEUR IN, V30
   Kaptchuk G, 2021, CCS '21: PROCEEDINGS OF THE 2021 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1529, DOI 10.1145/3460120.3484550
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Ke Y, 2019, MULTIMED TOOLS APPL, V78, P13805, DOI 10.1007/s11042-018-6640-y
   Le T. V., 2003, IACR Cryptol. ePrint Arch., V2003, P156
   Li J., 2020, P INT C ARTIF INT S, P386
   Li WX, 2020, IEEE T COMMUN, V68, P3948, DOI 10.1109/TCOMM.2020.2982624
   Liu J, 2020, IEEE ACCESS, V8, P60575, DOI 10.1109/ACCESS.2020.2983175
   Liu JD, 2022, IEEE T MULTIMEDIA, V24, P2084, DOI 10.1109/TMM.2021.3075858
   Liu MM, 2017, Arxiv, DOI arXiv:1712.06951
   Liu Q, 2022, IEEE T CIRC SYST VID, V32, P4038, DOI 10.1109/TCSVT.2021.3108772
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Loshchilov I., 2019, P 3 INT C LEARN REPR
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Mittal A, 2011, CONF REC ASILOMAR C, P723, DOI 10.1109/ACSSC.2011.6190099
   Odena A, 2017, PR MACH LEARN RES, V70
   Peng F, 2022, IEEE T CIRC SYST VID, V32, P5817, DOI 10.1109/TCSVT.2022.3161419
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Ren YZ, 2022, Arxiv, DOI arXiv:2201.07444
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Dawn, 2017, NIPS 2017 WORKSH MAC, V1, P8
   Su X. F., 2021, P 20 INT WORKSH DIG, P197
   Sun WW, 2021, IEEE T IMAGE PROCESS, V30, P6292, DOI 10.1109/TIP.2021.3093794
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang WX, 2023, SCI CHINA INFORM SCI, V66, DOI 10.1007/s11432-021-3453-5
   Volkhonskiy D., 2016, Generative adversarial networks for image steganography
   Wei P, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1621, DOI 10.1145/3503161.3548217
   Wu HW, 2022, IEEE T INF FOREN SEC, V17, P443, DOI 10.1109/TIFS.2022.3144878
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Yedroudj M, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2092, DOI 10.1109/ICASSP.2018.8461438
   You ZX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2834, DOI 10.1145/3503161.3548139
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SY, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P3046
   Zhang W, 2018, Proceedings of the International Workshop on Digital Watermarking, P55, DOI DOI 10.1007/978-3-030-11389-65
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zhang Z, 2020, TSINGHUA SCI TECHNOL, V25, P516, DOI 10.26599/TST.2019.9010027
   Zhao JB, 2017, Arxiv, DOI arXiv:1609.03126
   Zhou H, 2019, IEEE T MULTIMEDIA, V21, P1384, DOI 10.1109/TMM.2018.2882088
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 52
TC 2
Z9 2
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5040
EP 5053
DI 10.1109/TMM.2023.3330098
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600031
DA 2024-08-05
ER

PT J
AU Zhang, HL
   Wang, JC
   Zhang, JW
   Zhang, TZ
   Zhong, BE
AF Zhang, Huanlong
   Wang, Jingchao
   Zhang, Jianwei
   Zhang, Tianzhu
   Zhong, Bineng
TI One-Stream Vision-Language Memory Network for Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Visualization; Linguistics; Iron; Feature extraction;
   Computational modeling; Adaptation models; Object tracking;
   vision-language; one-stream; memory network
ID ROBUST
AB Most existing tracking methods try to represent the target by exploiting visual information as much as possible based on the various deep networks. However, the appearance model hardly describes the attribute feature of the target well, which makes the trackers fail to adapt to the complex visual surrounding. In this article, inspired by brain-like intelligence, we propose an One-stream Vision-Language Memory network (OVLM) for object tracking. Firstly, we use the combination of vision and language to build the target model and use the semantic information in the language to compensate for the instability of visual information, making the target model more stable in the face of complex appearance changes. Secondly, to build a more compact target model, we propose a memory token selection mechanism that utilizes linguistic information to eliminate tokens that do not contain target information. Furthermore, to provide better visual information for target modeling, we propose a language-based evaluation method to select high-quality target samples to be stored in the memory. Finally, OVLM achieves a 64.7% success rate on the large-scale tracking benchmark dataset TNL2K, outperforming the previous best result (VLT) by 11.6%. By exposing the possibility of the vision-language memory network, we aim to draw greater attention to it and open up new avenues for vision-language tracking.
C1 [Zhang, Huanlong] Zhengzhou Univ Light Ind, Coll Elect & Informat Engn, Zhengzhou 450001, Peoples R China.
   [Wang, Jingchao; Zhang, Jianwei] Zhengzhou Univ Light Ind, Coll Software Engn, Zhengzhou 450001, Peoples R China.
   [Zhang, Tianzhu] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
   [Zhong, Bineng] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
C3 Zhengzhou University of Light Industry; Zhengzhou University of Light
   Industry; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Guangxi Normal University
RP Zhang, JW (corresponding author), Zhengzhou Univ Light Ind, Coll Software Engn, Zhengzhou 450001, Peoples R China.; Zhang, TZ (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
EM zzuli407@163.com; wjcmain@163.com; mailzjw@163.com; tzzhang@ustc.edu.cn;
   bnzhong@gxnu.edu.cn
RI 王, 景超/GXV-6569-2022; Zhang, Jianwei/AAL-5062-2020
OI 王, 景超/0000-0003-1848-320X; Zhang, Huanlong/0000-0002-5130-5555
FU National Natural Science Foundation of China
FX No Statement Available
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Botach A., 2022, IEEECVFCONF COMPUT V, P4985
   Chen BY, 2022, LECT NOTES COMPUT SC, V13682, P375, DOI 10.1007/978-3-031-20047-2_22
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   CHEN Z, 2020, IEEECVF C COMPUT VIS, P6668
   Cui YT, 2022, COMPUT VIS IMAGE UND, V224, DOI 10.1016/j.cviu.2022.103547
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Dong XP, 2023, IEEE T PATTERN ANAL, V45, P8049, DOI 10.1109/TPAMI.2022.3230064
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Feng Q, 2021, PROC CVPR IEEE, P5847, DOI 10.1109/CVPR46437.2021.00579
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Gao SY, 2022, LECT NOTES COMPUT SC, V13682, P146, DOI 10.1007/978-3-031-20047-2_9
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo M., 2022, NeurIPS, V35, P4446
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Huang Y., 2021, ADV NEURAL INFORM PR
   Kamath A, 2022, Language-driven semantic segmentation
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li LH, 2022, PROC CVPR IEEE, P10955, DOI 10.1109/CVPR52688.2022.01069
   Li ZY, 2017, PROC CVPR IEEE, P7350, DOI 10.1109/CVPR.2017.777
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Liu TP, 2023, IEEE T MULTIMEDIA, V25, P5330, DOI 10.1109/TMM.2022.3190679
   Maaz M, 2022, LECT NOTES COMPUT SC, V13670, P512, DOI 10.1007/978-3-031-20080-9_30
   Mayer C, 2022, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR52688.2022.00853
   Nie JH, 2023, IEEE T MULTIMEDIA, V25, P6194, DOI 10.1109/TMM.2022.3206668
   Rao YM, 2021, 35 C NEURAL INFORM P, V34
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shen WC, 2019, IEEE T CIRC SYST VID, V29, P2012, DOI 10.1109/TCSVT.2018.2862151
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JC, 2022, ELECTRON LETT, V58, P798, DOI 10.1049/ell2.12610
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang X, 2018, Describe and attend to track: Learning natural language guided structural representation and visual attention for object tracking
   Wang X, 2021, PROC CVPR IEEE, P13758, DOI 10.1109/CVPR46437.2021.01355
   Wu DM, 2023, PROC CVPR IEEE, P14633, DOI 10.1109/CVPR52729.2023.01406
   Wu JN, 2022, PROC CVPR IEEE, P4964, DOI 10.1109/CVPR52688.2022.00492
   Xie F, 2021, IEEE INT CONF COMP V, P2678, DOI 10.1109/ICCVW54120.2021.00302
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Yang ZY, 2021, IEEE T CIRC SYST VID, V31, P3433, DOI 10.1109/TCSVT.2020.3038720
   Ye BT, 2022, LECT NOTES COMPUT SC, V13682, P341, DOI 10.1007/978-3-031-20047-2_20
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Yu B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9836, DOI 10.1109/ICCV48922.2021.00971
   Zhang J., 2022, IET Image Process, V17, P215
   Zhang M. Z, 2022, Learning target-aware representation for visual tracking via informative interactions
   Zhang Y, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P136, DOI 10.1109/BigMM.2015.30
   Zhang ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13319, DOI 10.1109/ICCV48922.2021.01309
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
NR 62
TC 2
Z9 2
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1720
EP 1730
DI 10.1109/TMM.2023.3285441
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800025
DA 2024-08-05
ER

PT J
AU Zhang, W
   Zhou, KB
   Teng, LY
   Tang, FY
   Wu, NQ
   Teng, SH
   Li, J
AF Zhang, Wei
   Zhou, KangBin
   Teng, LuYao
   Tang, FeiYi
   Wu, NaiQi
   Teng, ShaoHua
   Li, Jian
TI Dynamic Confidence Sampling and Label Semantic Guidance Learning for
   Domain Adaptive Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Codes; Training; Navigation; Adaptive systems; Reliability;
   Headphones; Domain adaptive retrieval; dynamic confidence sampling;
   label semantic guidance learning; relaxation variables
AB To accurately retrieve similar objects from different domains, domain adaptive retrieval method is applied to cope with the domain shift problem in information retrieval. However, existing methods still have two problems: a) they fail to filter out low-confidence samples, leading to error accumulation; and b) they ignore the negative effect of domain discrepancy. To address these two issues, we propose an efficient method called Dynamic Confidence Sampling and Label Semantic Guidance Learning (DCS-LSG). First, Dynamic Confidence Sampling (DCS) is employed to dynamically select high-confidence samples from the target domain so as to improve the effectiveness of learning. Second, Label Semantic Guidance (LSG) learning is presented to enhance the label semantics of features during domain adaptive retrieval. In addition, we introduce a Dual-Projection Relaxation (DPR) strategy to learn more effective features on two specific projection spaces. At last, a two-step hashing strategy is used to generate high-quality hash codes. Experiments on multiple cross-domain retrieval datasets demonstrate that the proposed DCS-LSG can achieve a significant performance improvement.
C1 [Zhang, Wei; Zhou, KangBin; Teng, ShaoHua; Li, Jian] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Teng, LuYao; Tang, FeiYi] Guangzhou Panyu Polytech, Sch Informat Engn, Guangzhou 511483, Peoples R China.
   [Wu, NaiQi] Macau Univ Sci & Technol, Inst Syst Engn, Macau 999078, Peoples R China.
   [Wu, NaiQi] Macau Univ Sci & Technol, Collaborat Lab Intelligent Sci & Syst, Macau 999078, Peoples R China.
C3 Guangdong University of Technology; Guangzhou Panyu Polytechnic; Macau
   University of Science & Technology; Macau University of Science &
   Technology
RP Teng, LY (corresponding author), Guangzhou Panyu Polytech, Sch Informat Engn, Guangzhou 511483, Peoples R China.
EM weizhang@gdut.edu.cn; 2112005246@mail2.gdut.edu.cn; Luna.teng@qq.com;
   258927516@qq.com; nqwu@must.edu.mo; shteng@gdut.edu.cn;
   lijian@gdut.edu.cn
RI ; Wu, Naiqi/C-2953-2017
OI Jian, Li/0000-0002-0723-9351; TENG, Shaohua/0000-0002-7204-1288; teng,
   luyao/0000-0002-3872-4085; Wu, Naiqi/0000-0001-6782-458X
FU Key-Area Research and Development Program of Guangdong Province
FX No Statement Available
CR Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989
   Daume H, 2007, P 45 ANN M ASS COMPU, V45, P256
   Ding YJ, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102288
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Du Y, 2021, LECT NOTES COMPUT SC, V12682, P429, DOI 10.1007/978-3-030-73197-7_29
   Fang XZ, 2022, IEEE T CYBERNETICS, V52, P2618, DOI 10.1109/TCYB.2020.3004398
   Fuxiang Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9579, DOI 10.1109/CVPR42600.2020.00960
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Han N, 2020, IEEE T IMAGE PROCESS, V29, P2820, DOI 10.1109/TIP.2019.2952739
   Huang FX, 2022, IEEE T NEUR NET LEAR, V33, P5641, DOI 10.1109/TNNLS.2021.3071127
   Huang SS, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P84, DOI 10.1145/3126686.3126773
   Jin ZM, 2014, IEEE T CYBERNETICS, V44, P1362, DOI 10.1109/TCYB.2013.2283497
   Liu H, 2019, IEEE T PATTERN ANAL, V41, P941, DOI 10.1109/TPAMI.2018.2819978
   Liu J. Yi, 2022, IEEE Trans. Multime-dia, DOI [10.1109/TMM.2022.3166668.[19]L, DOI 10.1109/TMM.2022.3166668.[19]L]
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Long FC, 2018, ACM/SIGIR PROCEEDINGS 2018, P725, DOI 10.1145/3209978.3209999
   Luo LK, 2020, IEEE T CYBERNETICS, V50, P3914, DOI 10.1109/TCYB.2019.2962000
   Luo X, 2019, IEEE T IMAGE PROCESS, V28, P2962, DOI 10.1109/TIP.2019.2892703
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Song LY, 2023, IEEE T MULTIMEDIA, V25, P5305, DOI 10.1109/TMM.2022.3190222
   Teng SH, 2023, IEEE T COMPUT SOC SY, V10, P577, DOI 10.1109/TCSS.2022.3195704
   Teng SH, 2022, INT J INTELL SYST, V37, P365, DOI 10.1002/int.22629
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P2624, DOI 10.1109/TMM.2022.3149629
   Wang Q, 2020, AAAI CONF ARTIF INTE, V34, P6243
   Xia HF, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3853, DOI 10.1145/3474085.3475526
   Xu CY, 2023, IEEE T MULTIMEDIA, V25, P7428, DOI 10.1109/TMM.2022.3222598
   Yao T, 2019, PATTERN RECOGN, V89, P1, DOI 10.1016/j.patcog.2018.12.012
   Zhang HF, 2018, IEEE T IMAGE PROCESS, V27, P1626, DOI 10.1109/TIP.2017.2781422
   Zhang JY, 2018, LECT NOTES COMPUT SC, V11206, P304, DOI 10.1007/978-3-030-01216-8_19
   Zhang L, 2020, IEEE T CIRC SYST VID, V30, P3788, DOI 10.1109/TCSVT.2019.2943902
   Zhang W, 2023, WORLD WIDE WEB, V26, P1093, DOI 10.1007/s11280-022-01072-9
   Zhao JC, 2022, IEEE T CYBERNETICS, V52, P1193, DOI 10.1109/TCYB.2020.2994875
   Zhou JT, 2018, IEEE T NEUR NET LEAR, V29, P6191, DOI 10.1109/TNNLS.2018.2827036
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
NR 37
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2467
EP 2479
DI 10.1109/TMM.2023.3296940
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100002
DA 2024-08-05
ER

PT J
AU Zhao, WT
   Wu, XX
AF Zhao, Wentian
   Wu, Xinxiao
TI Boosting Entity-Aware Image Captioning With Multi-Modal Knowledge Graph
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image captioning; named entity; knowledge graph
AB Entity-aware image captioning aims to describe named entities and events related to the image by utilizing the background knowledge in the associated article. This task remains challenging as it is difficult to learn the association between named entities and visual cues due to the long-tail distribution of named entities. Furthermore, the complexity of the article brings difficulty in extracting fine-grained relationships between entities to generate informative event descriptions about the image. To tackle these challenges, we propose a novel approach that constructs a multi-modal knowledge graph (MMKG) to associate the visual objects with named entities and capture the relationship between entities simultaneously with the help of external knowledge collected from the web. Specifically, we build a text sub-graph by extracting named entities and their relationships from the article, and build an image sub-graph by detecting the objects in the image. To connect these two sub-graphs, we propose a cross-modal entity matching module trained using a knowledge base that contains Wikipedia entries and the corresponding images. Finally, the MMKG is integrated into the captioning model via a graph attention mechanism. Extensive experiments on both GoodNews and NYTimes800 k datasets demonstrate the effectiveness of our method.
C1 [Zhao, Wentian; Wu, Xinxiao] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Wu, Xinxiao] Shenzhen MSU BIT Univ, Guangdong Lab Machine Percept & Intelligent Comp, Shenzhen 518172, Peoples R China.
C3 Beijing Institute of Technology; Shenzhen MSU-BIT University
RP Wu, XX (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM wentian_zhao@bit.edu.cn; wuxinxiao@bit.edu.cn
OI Wu, Xinxiao/0000-0002-2056-6947; Zhao, Wentian/0009-0006-7645-8263
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Denkowski M., 2014, P WMT ACL, P376
   Du RY, 2022, IEEE T PATTERN ANAL, V44, P9521, DOI 10.1109/TPAMI.2021.3126668
   Biten AF, 2019, PROC CVPR IEEE, P12458, DOI 10.1109/CVPR.2019.01275
   Guo YR, 2022, IEEE T IMAGE PROCESS, V31, P4543, DOI 10.1109/TIP.2022.3184813
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Honnibal M., 2020, spaCy 2: Industrial strength natural language processing in python
   Hu AW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4217, DOI 10.1145/3394171.3413576
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jingjing Zhang, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P4365, DOI 10.1145/3503161.3547883
   Kannan AV, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P3417, DOI 10.1145/3340531.3417439
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D., 2014, P INT C LEARN REPR, P1
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Lu D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4013
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mike L, 2020, P 58 ANN M ASS COMP, P7871, DOI 10.18653/v1/2020.acl-main.703
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramisa A, 2018, IEEE T PATTERN ANAL, V40, P1072, DOI 10.1109/TPAMI.2017.2721945
   Redmon J., 2018, CoRR
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Sergieh H. M., 2018, P 7 JOINT C LEX COMP, P225, DOI DOI 10.18653/V1/S18-2027
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Spitkovsky VI, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3168
   Sun R, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1405, DOI 10.1145/3340531.3411947
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tran A., 2020, IEEECVF C COMPUT VIS, P13035
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang JB, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107075
   Wang YY, 2022, AAAI CONF ARTIF INTE, P2585
   Wynn JS, 2020, J EXP PSYCHOL GEN, V149, P518, DOI 10.1037/xge0000657
   Xie RB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3140
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yamada I, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P23
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang XW, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P5162
   Yang Z, 2020, P 28 INT C COMPUTATI, P1941
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zha ZJ, 2022, IEEE T PATTERN ANAL, V44, P710, DOI 10.1109/TPAMI.2019.2909864
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhao SQ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6485
   Zhu Y, 2020, arXiv
NR 56
TC 5
Z9 5
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2659
EP 2670
DI 10.1109/TMM.2023.3301279
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400020
OA Green Submitted
DA 2024-08-05
ER

PT J
AU An, P
   Zhu, D
   Quan, SW
   Ding, JF
   Ma, J
   Yang, Y
   Liu, Q
AF An, Pei
   Zhu, Di
   Quan, Siwen
   Ding, Junfeng
   Ma, Jie
   Yang, You
   Liu, Qiong
TI ESC-Net: Alleviating Triple Sparsity on 3D LiDAR Point Clouds for
   Extreme Sparse Scene Completion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Light detection and ranging; sparse point cloud; voxel representation;
   scene completion; multi-task learning
ID SEGMENTATION; NETWORK
AB 3D scene completion (SC) has made progress in the last three years. From the application of mobile robot system, SC should support the downstream task (i.e. mapping or perception), instead of only predicting the completed scenes. However, as the low-cost few-beam LiDAR is widely applied in mobile robot, gap between SC and downstream tasks is large. To generate the high quality completion result, the bottleneck lies in the triple sparsity of input, ground truth (GT) occupancy, and GT foreground. To deal with the triple sparsity, we present an extreme sparse scene completion network (ESC-Net). At first, input sparsity hides most of the spatial information of the scene. A feature completion (FC) decoder is designed to mine the spatial feature using feature-level completion. Then, GT occupancy sparsity hinders representation learning of the real scene with continuous surfaces. A multi-view multi-task attention (MMA) loss is presented to recover the high-quality object boundaries via correcting occupancy and semantic labels of regions from 3D and bird's eye view (BEV) spaces. After that, GT foreground sparsity is the imbalance of foreground and background GT labels. It causes the inaccuracy of local 3D object completion. A combination network (ESC-Net-D) is presented to recover 3D structural details of both foreground and background. Experiment is conducted on KITTI and SemanticPOSS datasets. It shows that ESC-Net has the performance higher than current methods not only on completion task, but also on the downstream tasks (i.e. 3D registration, 3D object detection). Hence, we believe that ESC-Net benefits to the community of mobile robot. Source code is released soon.
C1 [An, Pei; Yang, You; Liu, Qiong] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Zhu, Di; Ding, Junfeng; Ma, Jie] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China.
   [Quan, Siwen] Changan Univ, Sch Elect & Control Engn, Xian 710064, Peoples R China.
   [Yang, You; Liu, Qiong] Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Chang'an University; Huazhong University of
   Science & Technology
RP Yang, Y (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.; Ma, J (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China.; Yang, Y (corresponding author), Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
EM 2015088845@qq.com; u202115019@hust.edu.cn; siwenquan@chd.edu.cn;
   djfenghust@hust.edu.cn; majie@hust.edu.cn; yangyou@hust.edu.cn;
   q.liu@hust.edu.cn
OI Ding, Junfeng/0000-0001-7328-9923
FU National Key Ramp;D Program of China
FX No Statement Available
CR An P, 2023, IEEE T INTELL TRANSP, V24, P10165, DOI 10.1109/TITS.2023.3266727
   Behley J, 2021, INT J ROBOT RES, V40, P959, DOI 10.1177/02783649211006735
   Bijelic Mario, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11679, DOI 10.1109/CVPR42600.2020.01170
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Cheng M, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3105551
   Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Li J, 2023, IEEE T MULTIMEDIA, V25, P8294, DOI 10.1109/TMM.2023.3234441
   Li J, 2020, PROC CVPR IEEE, P3348, DOI 10.1109/CVPR42600.2020.00341
   Li L, 2022, IEEE T MULTIMEDIA, V24, P4504, DOI 10.1109/TMM.2021.3119872
   Li PF, 2023, IEEE INT CONF ROBOT, P8269, DOI 10.1109/ICRA48891.2023.10160552
   Li XL, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3094230
   Li ZZ, 2023, IEEE T MULTIMEDIA, V25, P3432, DOI 10.1109/TMM.2022.3160604
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Oleynikova H, 2017, IEEE INT C INT ROBOT, P1366
   Pan YC, 2020, IEEE INT VEH SYM, P687, DOI 10.1109/IV47402.2020.9304596
   Park C, 2020, IEEE ROBOT AUTOM LET, V5, P1556, DOI 10.1109/LRA.2020.2969164
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Rist C, 2022, IEEE T PATTERN ANAL, V44, P7205, DOI 10.1109/TPAMI.2021.3095302
   Roldao L, 2022, INT J COMPUT VISION, V130, P1978, DOI 10.1007/s11263-021-01504-5
   Roldao L, 2020, INT CONF 3D VISION, P111, DOI 10.1109/3DV50981.2020.00021
   Shan TX, 2018, IEEE INT C INT ROBOT, P4758, DOI 10.1109/IROS.2018.8594299
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Tang JX, 2022, AAAI CONF ARTIF INTE, P2352
   Tang JS, 2022, PROC CVPR IEEE, P1716, DOI 10.1109/CVPR52688.2022.00177
   Vizzo I, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031296
   Vizzo IV, 2022, IEEE ROBOT AUTOM LET, V7, P8534, DOI 10.1109/LRA.2022.3187255
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545
   Xie Haozhe, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P365, DOI 10.1007/978-3-030-58545-7_21
   Xu J., 2023, P AAAI C ART INT WAS, VVolume 37, P3018
   Yan X, 2021, AAAI CONF ARTIF INTE, V35, P3101
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang XM, 2021, IEEE INT C INT ROBOT, P3555, DOI 10.1109/IROS51168.2021.9636662
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Zhang YN, 2021, AAAI CONF ARTIF INTE, V35, P3430, DOI 10.1609/aaai.v35i4.16456
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
NR 41
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6799
EP 6810
DI 10.1109/TMM.2024.3355647
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600036
DA 2024-08-05
ER

PT J
AU Bian, Y
   Liu, M
   Wang, XP
   Tang, Y
   Wang, YN
AF Bian, Yuan
   Liu, Min
   Wang, Xueping
   Tang, Yi
   Wang, Yaonan
TI Occlusion-Aware Feature Recover Model for Occluded Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Training; Data models; Knowledge based systems;
   Interference; Image reconstruction; Computational modeling; Person
   Re-ID; occlusion simulation; self-supervised learning; feature recovery
ID TRANSFORMER
AB Occluded person re-identification (Re-ID) is a challenging task, as various object-to-person (OTP) and person-to-person (PTP) occlusion scenarios cause diverse occlusion interference and target person feature loss problems in person matching. Most existing methods, which utilize auxiliary models to evaluate the unoccluded person parts for occlusion feature elimination, are inefficient and cannot handle the PTP occlusion scenarios and person feature loss problems. To solve these issues, we propose a novel Occlusion-Aware Feature Recover (OAFR) model. OAFR simulates diverse occlusions to facilitate the model perceiving OTP, PTP occlusions and recovers occluded query features with unoccluded retrieved gallery features. Concretely, the Prior Knowledge-based Occlusion Simulation method is firstly introduced to synthesize OTP, PTP occlusions and corresponding occlusion labels, empowering model target person perception and occlusion-aware capability through self-supervised learning. Afterward, the feature recovery module reconstructs occluded query features with corresponding unoccluded local features of the top-$K$ retrieved images by the visibility weighted average scheme, thus recovering the occluded query features to maintain more comprehensive features for better retrieval. Extensive experiments demonstrate that the proposed OAFR achieves superior performance to the state-of-the-art for both holistic and occluded Re-ID. Especially for Occluded-DukeMTMC dataset, OAFR outperforms the state-of-the-art by 6.0% for Rank-1 accuracy and 2.2% for mAP score.
C1 [Bian, Yuan; Liu, Min; Tang, Yi; Wang, Yaonan] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
   [Bian, Yuan; Liu, Min; Tang, Yi; Wang, Yaonan] Natl Engn Res Ctr Robot Visual Percept & Control T, Changsha 410082, Peoples R China.
   [Wang, Xueping] Hunan Normal Univ, Coll Informat Sci & Engn, Changsha 410082, Peoples R China.
   [Wang, Xueping] Hunan Prov Key Lab Intelligent Comp & Language Inf, Changsha 410082, Peoples R China.
C3 Hunan University; Hunan Normal University
RP Liu, M (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
EM yuanbian@hnu.edu.cn; liu_min@hnu.edu.cn; wang_xueping@hnu.edu.cn;
   tyhnu@hnu.edu.cn; yaonan@hnu.edu.cn
RI Tang, Yi/AGZ-2170-2022
OI Tang, Yi/0000-0003-4006-428X; Wang, Xueping/0000-0003-4862-8975
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Ge Y., 2018, PROC ADV NEURAL INFO
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735
   Huang HJ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102789
   Iodice S, 2019, LECT NOTES COMPUT SC, V11366, P101, DOI 10.1007/978-3-030-20876-9_7
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11173
   Ke L, 2021, PROC CVPR IEEE, P4018, DOI 10.1109/CVPR46437.2021.00401
   Kong J, 2023, IEEE T MULTIMEDIA, V25, P1903, DOI 10.1109/TMM.2022.3220115
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma ZX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1487, DOI 10.1145/3474085.3475283
   Mekhazni Djebril, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P159, DOI 10.1007/978-3-030-58583-9_10
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Qian W, 2022, LECT NOTES COMPUT SC, V13674, P336, DOI 10.1007/978-3-031-19781-9_20
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Verma A, 2023, IEEE T MULTIMEDIA, V25, P364, DOI 10.1109/TMM.2021.3126404
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang XP, 2021, IEEE T IMAGE PROCESS, V30, P3017, DOI 10.1109/TIP.2021.3056223
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P4651, DOI 10.1109/TIP.2022.3186759
   Yan C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11855, DOI 10.1109/ICCV48922.2021.01166
   Yang JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11865, DOI 10.1109/ICCV48922.2021.01167
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu SJ, 2021, Arxiv, DOI arXiv:2105.07345
   Zhang P, 2021, IEEE T MULTIMEDIA, V23, P3562, DOI 10.1109/TMM.2020.3028461
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zheng KC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4537, DOI 10.1145/3474085.3475610
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhu J, 2021, IEEE T MULTIMEDIA, V23, P2614, DOI 10.1109/TMM.2020.3013531
   Zhu K., 2021, early access
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 59
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5284
EP 5295
DI 10.1109/TMM.2023.3331192
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600023
DA 2024-08-05
ER

PT J
AU Duan, JR
   Hao, YB
   Zhu, B
   Cheng, LC
   Zhou, PY
   Wang, X
AF Duan, Jingru
   Hao, Yanbin
   Zhu, Bin
   Cheng, Lechao
   Zhou, Pengyuan
   Wang, Xiang
TI Efficient Unsupervised Video Hashing With Contextual Modeling and
   Structural Controlling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codes; Data structures; Hash functions; Feature extraction; Context
   modeling; Computational modeling; Transformers; Video hashing; deep
   neural network; data structure; large-scale retrieval
ID QUANTIZATION
AB The most important effect of the video hashing technique is to support fast retrieval, which is benefiting from the high efficiency of binary calculation. Current video hash approaches are thus mainly targeted at learning compact binary codes to represent video content accurately. However, they may overlook the generation efficiency for hash codes, i.e., designing lightweight neural networks. This article proposes an Efficient Unsupervised Video Hashing (EUVH) method, which is not only for computing compact hash codes but also for designing a lightweight deep model. Specifically, we present an MLP-based model, where the video tensor is split into several groups and multiple axial contexts are explored to separately refine them in parallel. The axial contexts are referred to as the dynamics aggregated from different axial scales, including long/middle/short-range dependencies. The group operation significantly reduces the computational cost of the MLP backbone. Moreover, to achieve compact video hash codes, three structural losses are utilized. As demonstrated by the experiment, the three structures are highly complementary for approximating the real data structure. We conduct extensive experiments on three benchmark datasets for the unsupervised video hashing task and show the superior trade-off between performance and computational cost of our EUVH to the state of the arts.
C1 [Duan, Jingru; Wang, Xiang] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
   [Hao, Yanbin; Zhou, Pengyuan] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Zhu, Bin] Singapore Management Univ, Sch Comp & Informat Syst, Singapore 188065, Singapore.
   [Cheng, Lechao] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Singapore Management University; Hefei
   University of Technology
RP Hao, YB (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM duanjr@mail.ustc.edu.cn; haoyanbin@hotmail.com; binzhu@smu.edu.sg;
   chenglc@hfut.edu.cn; zpymyyn@gmail.com; xiangwang1223@gmail.com
OI Hao, Yanbin/0000-0002-0695-1566; Cheng, Lechao/0000-0002-7546-9052;
   Zhou, Pengyuan/0000-0002-7909-4059
FU National Natural Science Foundation of China
FX No Statement Available
CR Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao Liangliang., 2012, ACM Multimedia, P299
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Dong Yajiao, 2018, PROC 3 INT C MULTIME, P12
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gu Y., 2016, P 24 ACM INT C MULT, P272
   Han N, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3826, DOI 10.1145/3474085.3475241
   Hao Y., 2022, P IEEECVF C COMPUTER, P928
   Hao YB, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3754, DOI 10.1145/3503161.3547836
   Hao YB, 2017, IEEE T IMAGE PROCESS, V26, P5531, DOI 10.1109/TIP.2017.2737329
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Kingma D. P., 2014, arXiv
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Li C, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P9, DOI 10.1145/3132847.3133030
   Li PD, 2022, LECT NOTES COMPUT SC, V13674, P181, DOI 10.1007/978-3-031-19781-9_11
   Li SY, 2022, IEEE T CIRC SYST VID, V32, P2441, DOI 10.1109/TCSVT.2021.3093258
   Li SY, 2021, PROC CVPR IEEE, P13544, DOI 10.1109/CVPR46437.2021.01334
   Li SY, 2019, IEEE I CONF COMP VIS, P8211, DOI 10.1109/ICCV.2019.00830
   Li SY, 2020, IEEE T MULTIMEDIA, V22, P1542, DOI 10.1109/TMM.2019.2946096
   Li YQ, 2021, AAAI CONF ARTIF INTE, V35, P2002
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Luo X, 2021, Arxiv, DOI arXiv:2010.07804
   Over P., 2014, TRECVID 2013-An overview of the goals, tasks, data, evaluationmechanisms and metrics
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356316
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song Jingkuan, 2011, P 19 ACM INT C MULT, P423, DOI [DOI 10.1145/2072298.2072354, 10.1145/2072298.2072354]
   Su MY, 2023, IEEE T MULTIMEDIA, V25, P662, DOI 10.1109/TMM.2021.3129623
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Thomee B, 2016, Arxiv, DOI arXiv:1503.01817
   Tolstikhin I, 2021, ADV NEUR IN, V34
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A., 2017, PROC ANN C NEURALINF, P1
   Weiss Y., 2008, NIPS, V21, P1
   Wu GS, 2019, IEEE T IMAGE PROCESS, V28, P1993, DOI 10.1109/TIP.2018.2882155
   Wu Xiao, 2007, P 15 ACM INT C MULT, P218, DOI DOI 10.1145/1291233.1291280
   Xu LM, 2022, IEEE T MULTIMEDIA, V24, P534, DOI 10.1109/TMM.2021.3054503
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Zhang D, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P18
   Zhang HW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P781, DOI 10.1145/2964284.2964308
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 60
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7438
EP 7450
DI 10.1109/TMM.2024.3368924
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000068
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Gao, GY
   Zhang, H
   Xia, ZH
   Luo, XY
   Shi, YQ
AF Gao, Guangyong
   Zhang, Hui
   Xia, Zhihua
   Luo, Xiangyang
   Shi, Yun-Qing
TI Reversible Data Hiding-Based Contrast Enhancement With Multi-Group
   Stretching for ROI of Medical Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Histograms; Biomedical imaging; Image segmentation; Semantics; Medical
   diagnostic imaging; Visualization; Medical services; Automatic contrast
   enhancement; deep learning; reversible data hiding; ROI segmentation;
   stretching group
ID HIGH-CAPACITY; SCHEME
AB Reversible data hiding-based contrast enhancement (RDHCE) can be used in contrast enhancement for medical images, and it has been a popular research topic in recent years. However, the existing RDHCE methods suffer from the problem of inaccurate segmentation of the region of interest (ROI) in medical images, which can impact the contrast enhancement effect of the images. Moreover, some methods face limitations in their universality for ROI histograms with few empty bins on both sides, which results in unsatisfactory embedding capacity and contrast enhancement effect. To solve these problems, this study proposes an improved RDHCE method for medical images. The proposed method uses the UNet3+ network model, which makes the segmented ROI histograms more consistent with the subjective judgment of doctors compared to those obtained by traditional segmentation approaches. In addition, a multi-group stretching method is proposed to address the limitation of histogram expansion caused by the empty bins on both histogram sides, enabling adaptation to different ROI histograms with varying gray distributions. Compared to state-of-the-art RDHCE methods, the proposed method offers better generalizability, superior contrast enhancement performance and a larger ROI embedding capacity. It can greatly improve the visual quality of medical images in the field of medical imaging and aid doctors in making more accurate diagnoses.
C1 [Gao, Guangyong; Zhang, Hui] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
   [Gao, Guangyong; Zhang, Hui] Nanjing Univ Informat Sci & Technol, Sch Comp Sci, Nanjing 210044, Peoples R China.
   [Gao, Guangyong; Zhang, Hui] Zhengzhou Xinda Inst Adv Technol, Zhengzhou 450003, Peoples R China.
   [Xia, Zhihua] Jinan Univ, Coll Cyber Secur, Guangzhou 510632, Peoples R China.
   [Luo, Xiangyang] State Key Lab Math Engn & Adv Comp, Zhengzhou 450001, Peoples R China.
   [Shi, Yun-Qing] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; Jinan University; PLA
   Information Engineering University; New Jersey Institute of Technology
RP Xia, ZH (corresponding author), Jinan Univ, Coll Cyber Secur, Guangzhou 510632, Peoples R China.
EM gaoguangyong@163.com; 2291520135@qq.com; xia_zhihua@163.com;
   xiangyangluo@126.com; shi@njit.edu
FU National Key Research and Development Plan of China
FX No Statement Available
CR Abdulla AA, 2015, Ph.D. dissertation
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chen KM, 2019, MATH BIOSCI ENG, V16, P3947, DOI 10.3934/mbe.2019195
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chunqiang Yu, 2022, IEEE Transactions on Circuits and Systems for Video Technology, V32, P451, DOI 10.1109/TCSVT.2021.3062947
   Deyin Fu, 2021, 2021 IEEE Seventh International Conference on Big Data Computing Service and Applications (BigDataService), P208, DOI 10.1109/BigDataService52369.2021.00033
   Fu HZ, 2018, IEEE T MED IMAGING, V37, P1597, DOI 10.1109/TMI.2018.2791488
   Gao GY, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107817
   Gao M.-Z., 2013, Adv. Intell. Syst. Appl., V2, P331, DOI [10.1007/978-3-642-35473-133, DOI 10.1007/978-3-642-35473-133]
   He WG, 2021, IEEE T IMAGE PROCESS, V30, P5045, DOI 10.1109/TIP.2021.3078088
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/icassp40776.2020.9053405, 10.1109/ICASSP40776.2020.9053405]
   Hussan M, 2019, 2019 FIFTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP 2019), P221, DOI 10.1109/ICIIP47207.2019.8985824
   Jafar IF, 2016, J VIS COMMUN IMAGE R, V39, P239, DOI 10.1016/j.jvcir.2016.06.002
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Ke Y, 2020, IEEE T CIRC SYST VID, V30, P2353, DOI 10.1109/TCSVT.2019.2963393
   Kim S, 2019, IEEE T CIRC SYST VID, V29, P2271, DOI 10.1109/TCSVT.2018.2869935
   Kim S, 2015, IEEE INT WORKS INFOR
   Lee CF, 2018, PROCEEDINGS OF 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS), P56, DOI 10.1109/CCOMS.2018.8463244
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu YL, 2016, J VIS COMMUN IMAGE R, V39, P51, DOI 10.1016/j.jvcir.2016.05.008
   Lu YJ, 2019, AAAI CONF ARTIF INTE, P9989
   National Cancer Institute, 2014, TCIA
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Ou B, 2020, IEEE T CIRC SYST VID, V30, P2329, DOI 10.1109/TCSVT.2019.2921812
   Pai PY, 2011, INFORM SCIENCES, V181, P1463, DOI 10.1016/j.ins.2010.12.007
   Parah SA, 2017, J BIOMED INFORM, V66, P214, DOI 10.1016/j.jbi.2017.01.006
   Seo H, 2020, IEEE T MED IMAGING, V39, P1316, DOI 10.1109/TMI.2019.2948320
   Streijl RC, 2016, MULTIMEDIA SYST, V22, P213, DOI 10.1007/s00530-014-0446-1
   Vinoth Kumar C., 2014, 2014 International Conference on Communications and Signal Processing (ICCSP), P720, DOI 10.1109/ICCSP.2014.6949937
   Wang YM, 2021, IEEE T MULTIMEDIA, V23, P1466, DOI 10.1109/TMM.2020.2999187
   Weng SW, 2019, INFORM SCIENCES, V489, P136, DOI 10.1016/j.ins.2019.03.032
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HT, 2022, IEEE T CIRC SYST VID, V32, P7605, DOI 10.1109/TCSVT.2022.3180007
   Wu HT, 2019, IEEE ACCESS, V7, P83332, DOI 10.1109/ACCESS.2019.2921407
   Wu HT, 2018, SIGNAL PROCESS-IMAGE, V62, P64, DOI 10.1016/j.image.2017.12.006
   Wu HT, 2015, J VIS COMMUN IMAGE R, V31, P146, DOI 10.1016/j.jvcir.2015.06.010
   Wu HT, 2015, IEEE SIGNAL PROC LET, V22, P81, DOI 10.1109/LSP.2014.2346989
   Xu ZW, 2021, ENVIRON MODELL SOFTW, V140, DOI 10.1016/j.envsoft.2021.104992
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P1860, DOI 10.1109/TCSVT.2021.3084676
   Yang Y, 2018, MULTIMED TOOLS APPL, V77, P18043, DOI 10.1007/s11042-017-4444-0
   Yang Y, 2016, SIGNAL PROCESS, V120, P797, DOI 10.1016/j.sigpro.2015.03.019
   Zhang TC, 2022, IEEE T CIRC SYST VID, V32, P5041, DOI 10.1109/TCSVT.2022.3146159
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
NR 44
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3909
EP 3923
DI 10.1109/TMM.2023.3318048
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300023
DA 2024-08-05
ER

PT J
AU Hu, F
   Ma, YB
   Zhong, W
   Ye, L
   Yang, XY
   Fang, L
   Zhang, Q
AF Hu, Fei
   Ma, Yibo
   Zhong, Wei
   Ye, Long
   Yang, Xinyan
   Fang, Li
   Zhang, Qin
TI A Dataset and Benchmark for 3D Scene Plausibility Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D scene graph; graph network; indoor 3D scene; plausibility assessment
ID BLIND QUALITY ASSESSMENT
AB The surge in popularity of 3D scene synthesis has driven the development of diverse methods for assessing the quality of synthesized scenes. While subjective assessment methods are widespread, their time-consuming and labor-intensive nature prompts exploration into more efficient objective alternatives. This paper introduces an objective approach to evaluating scene plausibility, aiming to overcome the limitations associated with subjective methods. To underpin our objective evaluation, we present the 3D-SPAD dataset, comprising plausibility scores for 3000 scenes across 46 object categories. Leveraging this dataset, we propose a graph attention-based network designed to accurately estimate scene plausibility. A comprehensive evaluation of our network is conducted through a series of experiments, showcasing its feasibility and reliability.
C1 [Hu, Fei; Ye, Long; Zhang, Qin] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
   [Ma, Yibo; Zhong, Wei; Yang, Xinyan; Fang, Li] Commun Univ China, Key Lab Media Audio & Video, Minist Educ, Beijing 100024, Peoples R China.
C3 Communication University of China; Communication University of China
RP Zhong, W (corresponding author), Commun Univ China, Key Lab Media Audio & Video, Minist Educ, Beijing 100024, Peoples R China.
EM hufei@cuc.edu.cn; mayibo@cuc.edu.cn; wzhong@cuc.edu.cn;
   yelong@cuc.edu.cn; xyyang@cuc.edu.cn; lifang8902@cuc.edu.cn;
   zhangqin@cuc.edu.cn
OI , Li/0000-0001-9963-6110
FU National Key Ramp;D Program of China
FX No Statement Available
CR Adams W., 2015, J. Vis., V15
   Alaei Alireza, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1244, DOI 10.1109/ICDAR.2019.00201
   Armeni I, 2019, IEEE I CONF COMP VIS, P5663, DOI 10.1109/ICCV.2019.00576
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Chen WL, 2021, IEEE T MULTIMEDIA, V23, P1008, DOI 10.1109/TMM.2020.2991546
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Fu Q, 2020, GRAPH MODELS, V110, DOI 10.1016/j.gmod.2020.101073
   Garcia-Garcia A, 2018, IEEE INT C INT ROBOT, P6790, DOI 10.1109/IROS.2018.8594495
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hua BS, 2016, INT CONF 3D VISION, P92, DOI 10.1109/3DV.2016.18
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Lee J, 2019, PR MACH LEARN RES, V97
   Li HY, 2018, INT C PATT RECOG, P3622, DOI 10.1109/ICPR.2018.8545433
   Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766
   Li WB, 2018, Arxiv, DOI [arXiv:1809.00716, 10.48550/arXiv.1809.00716]
   Lin LQ, 2022, LECT NOTES COMPUT SC, V13668, P93, DOI 10.1007/978-3-031-20074-8_6
   Luo A, 2020, PROC CVPR IEEE, P3753, DOI 10.1109/CVPR42600.2020.00381
   Ma R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275035
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Qi SY, 2018, PROC CVPR IEEE, P5899, DOI 10.1109/CVPR.2018.00618
   Shahkolaei A, 2019, SIGNAL PROCESS-IMAGE, V76, P11, DOI 10.1016/j.image.2019.04.009
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun W, 2023, IEEE J-STSP, V17, P1178, DOI 10.1109/JSTSP.2023.3270621
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wald J, 2020, PROC CVPR IEEE, P3960, DOI 10.1109/CVPR42600.2020.00402
   Wald J, 2019, IEEE I CONF COMP VIS, P7657, DOI 10.1109/ICCV.2019.00775
   Wei Sun, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P856, DOI 10.1145/3503161.3548329
   Wu HS, 2021, IEEE T MULTIMEDIA, V23, P4502, DOI 10.1109/TMM.2020.3043130
   Xiongkuo Min, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457921
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981
   Zeng JQ, 2021, AAAI CONF ARTIF INTE, V35, P10824
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang SK, 2021, GRAPH MODELS, V116, DOI 10.1016/j.gmod.2021.101104
   Zhang SH, 2022, IEEE T VIS COMPUT GR, V28, P3082, DOI 10.1109/TVCG.2021.3050143
   Zhang Song-Hai, 2020, PREPRINT
   Zhang SY, 2021, IEEE T VIS COMPUT GR, V27, P2250, DOI 10.1109/TVCG.2019.2949295
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zhang Z, 2019, Arxiv, DOI arXiv:1911.05954
   Zhang ZC, 2023, IEEE INT CON MULTI, P2483, DOI 10.1109/ICME55011.2023.00423
NR 54
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6529
EP 6541
DI 10.1109/TMM.2024.3353456
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600017
DA 2024-08-05
ER

PT J
AU Ju, Y
   Jia, S
   Cai, JL
   Guan, HY
   Lyu, S
AF Ju, Yan
   Jia, Shan
   Cai, Jialing
   Guan, Haiying
   Lyu, Siwei
TI GLFF: Global and Local Feature Fusion for AI-Synthesized Image Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Faces; Task analysis; Image synthesis; Semantics;
   Generative adversarial networks; Fuses; AI-synthesized Image Detection;
   Synthesized Face Image Dataset; Image Forensics; Feature Fusion;
   Attention Mechanism
AB With the rapid development of deep generative models (such as Generative Adversarial Networks and Diffusion models), AI-synthesized images are now of such high quality that humans can hardly distinguish them from pristine ones. Although existing detection methods have shown high performance in specific evaluation settings, e.g., on images from seen models or on images without real-world post-processing, they tend to suffer serious performance degradation in real-world scenarios where testing images can be generated by more powerful generation models or combined with various post-processing operations. To address this issue, we propose a Global and Local Feature Fusion (GLFF) framework to learn rich and discriminative representations by combining multi-scale global features from the whole image with refined local features from informative patches for AI-synthesized image detection. GLFF fuses information from two branches: the global branch to extract multi-scale semantic features and the local branch to select informative patches for detailed local artifacts extraction. Due to the lack of a synthesized image dataset simulating real-world applications for evaluation, we further create a challenging fake image dataset, named DeepFakeFaceForensics (DF3), which contains 6 state-of-the-art generation models and a variety of post-processing techniques to approach the real-world scenarios. Experimental results demonstrate the superiority of our method to the state-of-the-art methods on the proposed DF3 dataset and three other open-source datasets.
C1 [Ju, Yan; Jia, Shan; Cai, Jialing; Lyu, Siwei] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
   [Guan, Haiying] Natl Inst Stand & Technol, Gaithersburg, MD USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo; National Institute of Standards & Technology (NIST) -
   USA
RP Lyu, S (corresponding author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
EM yanju@buffalo.edu; shanjia@buffalo.edu; jialingc@buffalo.edu;
   haiying.guan@nist.gov; siweilyu@buffalo.edu
OI Ju, Yan/0000-0002-8610-9298
FU US Defense Advanced Research Projects Agency
FX No Statement Available
CR 100K-Faces, 2018, 100000 FACES GENERAT
   [Anonymous], 2021, IEEECVF INT C COMPUT
   Asnani V., 2023, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Brock A., 2018, P INT C LEARN REPR
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chai Lucy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P103, DOI 10.1007/978-3-030-58574-7_7
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chen BJ, 2022, IEEE T CIRC SYST VID, V32, P3527, DOI 10.1109/TCSVT.2021.3116679
   Chen BJ, 2022, CHINESE J ELECTRON, V31, P59, DOI 10.1049/cje.2020.00.372
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Corvi Riccardo, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P973, DOI 10.1109/CVPRW59228.2023.00104
   Corvi R., 2023, PROC IEEE INT C ACOU, P1
   Cozzolino D, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675329
   Cozzolino D, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P159, DOI 10.1145/3082031.3083247
   Dang H, 2020, PROC CVPR IEEE, P5780, DOI 10.1109/CVPR42600.2020.00582
   Dhariwal P, 2021, ADV NEUR IN, V34
   Ding F, 2021, IEEE T MULTIMEDIA, V24, P3429, DOI 10.1109/TMM.2021.3098422
   Durall R., 2020, P IEEE CVF C COMP VI, P7890
   Durall R, 2020, Arxiv, DOI arXiv:1911.00686
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Fan Zhang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P136, DOI 10.1007/978-3-030-67832-6_12
   Frank Joel, 2020, INT C MACH LEARN, P3247
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gragnaniello D., 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME51207.2021.9428429
   Guo H, 2022, IEEE ACCESS, V10, P32574, DOI 10.1109/ACCESS.2022.3157297
   HandBrake, 2019, HandBrake
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Yang, 2021, P 30 INT JOINT C ART, P2534, DOI DOI 10.24963/IJCAI
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hulzebosch N, 2020, IEEE COMPUT SOC CONF, P2729, DOI 10.1109/CVPRW50498.2020.00329
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jeon Hyeonseong, 2020, INT C MACHINE LEARNI, P4746
   Jia S, 2021, IEEE T CIRC SYST VID, V31, P4031, DOI 10.1109/TCSVT.2020.3044986
   Ju Y, 2022, IEEE IMAGE PROC, P3465, DOI 10.1109/ICIP46576.2022.9897820
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, INT C LEARNING REPRE
   Karras T., 2022, Adv. Neural Inf. Process. Syst., V35, p26 565
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Ke ZH, 2022, AAAI CONF ARTIF INTE, P1140
   Kim M, 2021, IEEE COMPUT SOC CONF, P1001, DOI 10.1109/CVPRW53098.2021.00111
   Korshunov P, 2020, Arxiv, DOI arXiv:2009.03155
   Li YZ, 2019, Arxiv, DOI arXiv:1811.00656
   Liu J, 2019, IEEE T IMAGE PROCESS, V28, P4926, DOI 10.1109/TIP.2019.2912294
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   LYU S, 2020, IEEE INT CONF MULTI, pNI249, DOI DOI 10.1109/icmew46912.2020.9105991
   Mandelli S, 2022, IEEE IMAGE PROC, P3091, DOI 10.1109/ICIP46576.2022.9897310
   Marra F, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035099
   Marra F, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P384, DOI 10.1109/MIPR.2018.00084
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Neves JC, 2020, IEEE J-STSP, V14, P1038, DOI 10.1109/JSTSP.2020.3007250
   Nichol A, 2022, PR MACH LEARN RES
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Paszke A, 2019, ADV NEUR IN, V32
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Sauer A., 2023, arXiv
   Schwarcz S, 2021, IEEE COMPUT SOC CONF, P933, DOI 10.1109/CVPRW53098.2021.00104
   Vahdat A, 2021, 35 C NEURAL INFORM P, V34
   van den Oord A, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Viazovetskyi Yuri, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P170, DOI 10.1007/978-3-030-58542-6_11
   Wang S.-Y., 2020, P IEEE CVF C COMP VI, P8692
   Wang X, 2023, Arxiv, DOI arXiv:2202.07145
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Yu MM, 2022, NEUROCOMPUTING, V501, P583, DOI 10.1016/j.neucom.2022.06.013
   Yu N, 2019, IEEE I CONF COMP VIS, P7555, DOI 10.1109/ICCV.2019.00765
   Yu PP, 2022, IEEE T INF FOREN SEC, V17, P547, DOI 10.1109/TIFS.2022.3146781
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P3349, DOI 10.1109/TPAMI.2020.3046647
   Zhang X., 2021, PROC 18 PACIFIC RIM, P337
   Zhang X, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035107
   Zhang ZL, 2018, LECT NOTES COMPUT SC, V11214, P273, DOI 10.1007/978-3-030-01249-6_17
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuge MC, 2023, IEEE T PATTERN ANAL, V45, P3738, DOI 10.1109/TPAMI.2022.3179526
   Zou ZX, 2023, P IEEE, V111, P257, DOI 10.1109/JPROC.2023.3238524
NR 80
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4073
EP 4085
DI 10.1109/TMM.2023.3313503
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100040
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, DC
   Zheng, ZY
   Peng, CL
   Wang, YK
   Wang, NN
   Gao, XB
AF Liu, Decheng
   Zheng, Zeyang
   Peng, Chunlei
   Wang, Yukai
   Wang, Nannan
   Gao, Xinbo
TI Hierarchical Forgery Classifier on Multi-Modality Face Forgery Clues
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Forgery; Faces; Face recognition; Feature extraction; Videos; Task
   analysis; Frequency-domain analysis; Face forgery detection;
   multi-modality face; frequency domain feature; hierarchical classifier
ID ALGORITHM; SCALE; GAN
AB Face forgery detection plays an important role in personal privacy and social security. With the development of adversarial generative models, high-quality forgery images become more and more indistinguishable from real to humans. Existing methods always regard as forgery detection task as the common binary or multi-label classification, and ignore exploring diverse multi-modality forgery image types, e.g. visible light spectrum and near-infrared scenarios. In this article, we propose a novel <bold>H</bold>ierarchical <bold>F</bold>orgery <bold>C</bold>lassifier for <bold>M</bold>ulti-modality <bold>F</bold>ace <bold>F</bold>orgery <bold>D</bold>etection <bold>(HFC-MFFD)</bold>, which could effectively learn robust patches-based hybrid domain representation to enhance forgery authentication in multiple modality scenarios. The local hybrid domain representation is designed to explore strong discriminative forgery clues both in the image and frequency domain with the intra-attention mechanism. Furthermore, the specific hierarchical face forgery classifier is designed through the authenticity feedback strategy to integrate diverse discriminative clues. Experimental results on representative multi-modality face forgery datasets demonstrate the superior performance of the proposed HFC-MFFD compared with state-of-the-art algorithms.
C1 [Liu, Decheng; Peng, Chunlei; Wang, Yukai] Xidian Univ, Sch Cyber Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
   [Zheng, Zeyang] Xidian Univ, Sch Cyber Engn, Xian 710071, Shaanxi, Peoples R China.
   [Wang, Nannan] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; Xidian University; Xidian University; Chongqing
   University of Posts & Telecommunications
RP Peng, CL (corresponding author), Xidian Univ, Sch Cyber Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
EM dchliu@xidian.edu.cn; wh1tezy0321@gmail.com; clpeng@xidian.edu.cn;
   ykwang.xidian@gmail.com; nnwang@xidian.edu.cn; gaoxb@cqupt.edu.cn
OI Liu, Decheng/0000-0002-6550-212X; Wang, Nannan/0000-0002-4695-6134
FU National Natural Science Foundation of China
FX No Statement Available
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Alex AT, 2013, IEEE SYS MAN CYBERN, P1211, DOI 10.1109/SMC.2013.210
   Aloraini M, 2021, IEEE T CIRC SYST VID, V31, P917, DOI 10.1109/TCSVT.2020.2993004
   Cao JY, 2022, PROC CVPR IEEE, P4103, DOI 10.1109/CVPR52688.2022.00408
   Chai Lucy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P103, DOI 10.1007/978-3-030-58574-7_7
   Chandrasegaran Keshigeyan, 2021, P IEEE CVF C COMP VI, P7200
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081
   Chen SD, 2016, IEEE T CIRC SYST VID, V26, P2138, DOI 10.1109/TCSVT.2015.2473436
   Chen YF, 2020, IEEE J-STSP, V14, P997, DOI 10.1109/JSTSP.2020.2998401
   Chintha A, 2020, IEEE J-STSP, V14, P1024, DOI 10.1109/JSTSP.2020.2999185
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chunlei Peng, 2020, Machine Learning for Cyber Security. Third International Conference, ML4CS 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12488), P55, DOI 10.1007/978-3-030-62463-7_6
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   D'Amiano L, 2019, IEEE T CIRC SYST VID, V29, P669, DOI 10.1109/TCSVT.2018.2804768
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dzanic T., 2020, Advances in neural information, P3022
   Frank Joel, 2020, INT C MACH LEARN, P3247
   Giudice O, 2021, J IMAGING, V7, DOI 10.3390/jimaging7080128
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   Han H, 2013, IEEE T INF FOREN SEC, V8, P191, DOI 10.1109/TIFS.2012.2228856
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He PS, 2020, IEEE T CIRC SYST VID, V30, P4034, DOI 10.1109/TCSVT.2019.2951630
   He YA, 2021, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR46437.2021.00434
   Hu J, 2022, IEEE T CIRC SYST VID, V32, P1089, DOI 10.1109/TCSVT.2021.3074259
   Hu WP, 2021, IEEE T MULTIMEDIA, V23, P145, DOI 10.1109/TMM.2020.2980201
   Huang QD, 2021, AAAI CONF ARTIF INTE, V35, P1619
   Huo J, 2018, IEEE T CYBERNETICS, V48, P1814, DOI 10.1109/TCYB.2017.2715660
   Kan MN, 2012, LECT NOTES COMPUT SC, V7572, P808, DOI 10.1007/978-3-642-33718-5_58
   Khormali A, 2021, BIG DATA COGN COMPUT, V5, DOI 10.3390/bdcc5040049
   Krizhevsky A., 2012, PROC ANN C NEURAL IN
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Lei Z, 2012, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2012.6247967
   Li HL, 2020, IEEE J-STSP, V14, P933, DOI 10.1109/JSTSP.2020.3001719
   Li JM, 2021, PROC CVPR IEEE, P6454, DOI 10.1109/CVPR46437.2021.00639
   Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu DC, 2023, IEEE T INF FOREN SEC, V18, P4272, DOI 10.1109/TIFS.2023.3293951
   Liu DC, 2022, IEEE T NEUR NET LEAR, V33, P5611, DOI 10.1109/TNNLS.2021.3071119
   Liu DC, 2020, IEEE T NEUR NET LEAR, V31, P4699, DOI 10.1109/TNNLS.2019.2957285
   Liu DC, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107579
   Liu DC, 2018, NEUROCOMPUTING, V302, P46, DOI 10.1016/j.neucom.2018.03.042
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083
   Liu QS, 2005, PROC CVPR IEEE, P1005
   Liy C.M., 2018, 2018 IEEE INT G NAT, P11
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mi ZJ, 2020, IEEE J-STSP, V14, P969, DOI 10.1109/JSTSP.2020.2994523
   Mo HX, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P43, DOI 10.1145/3206004.3206009
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Platt JC, 2000, ADV NEUR IN, P61
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2022, IEEE T MULTIMEDIA, V24, P791, DOI 10.1109/TMM.2021.3059336
   Song LC, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4102, DOI 10.1145/3503161.3547806
   Song LC, 2022, LECT NOTES COMPUT SC, V13694, P467, DOI 10.1007/978-3-031-19830-4_27
   Su LC, 2018, IEEE T MULTIMEDIA, V20, P825, DOI 10.1109/TMM.2017.2760098
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang XO, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P687, DOI 10.1109/ICCV.2003.1238414
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CR, 2021, PROC CVPR IEEE, P14918, DOI 10.1109/CVPR46437.2021.01468
   Wang JK, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P615, DOI 10.1145/3512527.3531415
   Wang P, 2022, INT CONF ACOUST SPEE, P2899, DOI 10.1109/ICASSP43922.2022.9746888
   Wang S.-Y., 2020, P IEEE CVF C COMP VI, P8692
   Wang YK, 2022, IEEE T INF FOREN SEC, V17, P500, DOI 10.1109/TIFS.2022.3146766
   Wolter M, 2022, MACH LEARN, V111, P4295, DOI 10.1007/s10994-022-06225-5
   Wu T.-F., 2003, PROC ADV NEURAL INF, P1
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Yang PP, 2020, IEEE J-STSP, V14, P947, DOI 10.1109/JSTSP.2020.3008088
   Ye F, 2022, IEEE T MULTIMEDIA, V24, P116, DOI 10.1109/TMM.2020.3046884
   Yu N, 2019, IEEE I CONF COMP VIS, P7555, DOI 10.1109/ICCV.2019.00765
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zhang L, 2023, IEEE T MULTIMEDIA, V25, P4785, DOI 10.1109/TMM.2022.3182509
   Zhang LL, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P627, DOI 10.1145/2671188.2749321
   Zhang X, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035107
   Zhao H., 2022, arXiv
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
NR 77
TC 5
Z9 5
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2894
EP 2905
DI 10.1109/TMM.2023.3304913
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400010
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Lu, ZF
   Lin, RH
   Hu, HF
AF Lu, Zefeng
   Lin, Ronghao
   Hu, Haifeng
TI Tri-Level Modality-Information Disentanglement for Visible-Infrared
   Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Complexity theory; Cameras; Representation learning;
   Bridges; Pedestrians; Adaptation models; NIR-VIS Re-ID;
   modality-invariant repre- sentation learning
ID NETWORK
AB Aiming to match the person identity between daytime VISible (VIS) and nighttime Near-InfraRed (NIR) images, VIS-NIR re-identification (Re-ID) has attracted increasing attention due to its wide applications in low-light scenes. However, dramatic modality discrepancies between VIS and NIR images lead to a considerable intra-class gap in the feature space, which impacts identity matching. To bridge the modality gap, we propose a Tri-level Modality-information Disentanglement (TMD) to disentangle modality information at the levels of raw image, features distribution and instance features. Our model consists of three key modules, including Style-Aligned Converter (SAC), Two-Steps Wasserstein Loss (TSWL) and Self-supervised Orthogonal Disentanglement (SOD) to handle the modality information at the three levels. Firstly, aiming at reducing modality discrepancy at image-level, the SAC is introduced to generate style-aligned images by the designed style converter and A-distance learning approach. The SAC can effectively alleviate the style discrepancy between VIS and NIR images with a negligible increase in model complexity. Secondly, considering the heterogeneity of VIS and NIR feature distribution caused by the structure- and style-misaligned raw images, we propose the TSWL to decrease the VIS-NIR gap at distribution-level by two distribution alignment steps. Specifically, after generating style-consistent images, we eliminate modality-related discrepancy by aligning the distribution between structure-aligned original and generated VIS/NIR images and bridge the modality-unrelated gap by aligning the style-consistent generated VIS-NIR images. Thirdly, focusing on further reducing the modality discrepancy at instance-level, the SOD is presented to construct orthogonal constraints between the extracted modality- and identity-related features. Since the modality-related factors are disentangled from the instance features, the proposed TMD efficiently learns the modality-unrelated and identity-discriminative representations, which are productive to conduct person Re-ID task on the VIS-NIR images. Comprehensive experiments are carried out on two cross-modality pedestrian Re-ID datasets to demonstrate the effectiveness of TMD.
C1 [Lu, Zefeng; Lin, Ronghao; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Guangdong, Peoples R China.
EM luzf@mail2.sysu.edu.cn; linrh7@mail2.sysu.edu.cn;
   huhaif@mail.sysu.edu.cn
RI ; Lin, Ronghao/HGD-5967-2022
OI Hu, Haifeng/0000-0002-4884-323X; Lin, Ronghao/0000-0003-4530-4529
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai Y, 2022, IEEE T PATTERN ANAL, V44, P6854, DOI 10.1109/TPAMI.2021.3099253
   Ben-David S., 2007, Advances in neural information processing systems, V19, P137
   Bousmalis K, 2016, ADV NEUR IN, V29
   Boyd L., 2004, Convex Optimization
   Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen X, 2021, IEEE T INTELL TRANSP, V22, P1276, DOI 10.1109/TITS.2020.2968517
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Feng YJ, 2023, IEEE T MULTIMEDIA, V25, P7647, DOI 10.1109/TMM.2022.3224663
   Fu CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11803, DOI 10.1109/ICCV48922.2021.01161
   Gao Z, 2021, IEEE T MULTIMEDIA, V23, P3332, DOI 10.1109/TMM.2020.3023784
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770
   He YG, 2019, IEEE IMAGE PROC, P3108, DOI [10.1109/icip.2019.8803323, 10.1109/ICIP.2019.8803323]
   Hu WP, 2022, IEEE T CIRC SYST VID, V32, P5095, DOI 10.1109/TCSVT.2022.3147813
   Huang NC, 2022, PATTERN RECOGN, V128, DOI 10.1016/j.patcog.2022.108653
   Huang Y, 2022, IEEE T MULTIMEDIA, V24, P1570, DOI 10.1109/TMM.2021.3067760
   Huang ZP, 2022, AAAI CONF ARTIF INTE, P1034
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jia XM, 2022, IEEE T IMAGE PROCESS, V31, P4227, DOI 10.1109/TIP.2022.3183469
   Khorramshahi Pirazh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P369, DOI 10.1007/978-3-030-58568-6_22
   Leng QM, 2020, IEEE T CIRC SYST VID, V30, P1092, DOI 10.1109/TCSVT.2019.2898940
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P194, DOI 10.1109/ICCV48922.2021.00026
   Li SZ, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107016
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Ling YG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P889, DOI 10.1145/3394171.3413821
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Liu HJ, 2023, IEEE T NEUR NET LEAR, V34, P1958, DOI 10.1109/TNNLS.2021.3105702
   Liu JN, 2022, IEEE T CIRC SYST VID, V32, P7226, DOI 10.1109/TCSVT.2022.3168999
   Lu ZF, 2023, IEEE T INTELL TRANSP, V24, P4333, DOI 10.1109/TITS.2022.3233565
   Lu ZF, 2023, IEEE T INTELL TRANSP, V24, P1994, DOI 10.1109/TITS.2022.3219593
   Lu ZF, 2022, IEEE T INTELL TRANSP, V23, P19001, DOI 10.1109/TITS.2022.3157463
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wan L, 2023, IEEE T INF FOREN SEC, V18, P3044, DOI 10.1109/TIFS.2023.3273911
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang Z, 2020, IEEE T IMAGE PROCESS, V29, P2013, DOI 10.1109/TIP.2019.2946975
   Wei ZY, 2022, IEEE T CYBERNETICS, V52, P10988, DOI 10.1109/TCYB.2022.3183395
   Wei ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P225, DOI 10.1109/ICCV48922.2021.00029
   Wei ZY, 2022, IEEE T NEUR NET LEAR, V33, P4676, DOI 10.1109/TNNLS.2021.3059713
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yang MX, 2022, PROC CVPR IEEE, P14288, DOI 10.1109/CVPR52688.2022.01391
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2022, IEEE T INF FOREN SEC, V17, P386, DOI 10.1109/TIFS.2021.3139224
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang LY, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3085978
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhang Q, 2021, IEEE T IMAGE PROCESS, V30, P8019, DOI 10.1109/TIP.2021.3112035
   Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76
   Zhang SZ, 2021, IEEE T IMAGE PROCESS, V30, P8861, DOI 10.1109/TIP.2021.3120881
   Zhang YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P788, DOI 10.1145/3474085.3475250
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zhao ZW, 2021, AAAI CONF ARTIF INTE, V35, P3520
   Zhong X, 2022, IEEE T CIRC SYST VID, V32, P1418, DOI 10.1109/TCSVT.2021.3072171
   Zhu XR, 2019, LECT NOTES COMPUT SC, V11296, P16, DOI 10.1007/978-3-030-05716-9_2
NR 68
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2700
EP 2714
DI 10.1109/TMM.2023.3302132
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400024
DA 2024-08-05
ER

PT J
AU Luo, G
   Zhou, YY
   Sun, JM
   Sun, XS
   Ji, RR
AF Luo, Gen
   Zhou, Yiyi
   Sun, Jiamu
   Sun, Xiaoshuai
   Ji, Rongrong
TI A Survivor in the Era of Large-Scale Pretraining: An Empirical Study of
   One-Stage Referring Expression Comprehension
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Training; Head; Cognition; Systematics;
   Sun; Computer vision; object recognition
ID LANGUAGE
AB One-stage Referring Expression Comprehension (REC) is a task that requires accurate alignment between text descriptions and visual content. In recent years, numerous efforts have been devoted to cross-modal learning for REC, while the influence of other factors in this task still lacks a systematic study. To fill this gap, we conduct an empirical study in this article. Concretely, we ablate 42 candidate designs/settings based on a common REC framework, and these candidates cover the entire process of one-stage REC from network design to model training. Afterwards, we conduct over 100 experimental trials on three REC benchmark datasets. The extensive experimental results reveal the key factors that affect REC performance in addition to multi-modal fusion, e.g., multi-scale features and data augmentation. Based on these findings, we further propose a simple yet strong model called SimREC, which achieves new state-of-the-art performance on these benchmarks. In addition to these progresses, we also find that with much less training overhead and parameters, SimREC can achieve better performance than a set of large-scale pre-trained models, e.g., UNITER and VILLA, portraying the special role of REC in existing V&L research.
C1 [Luo, Gen; Zhou, Yiyi; Sun, Jiamu; Sun, Xiaoshuai; Ji, Rongrong] Xiamen Univ, Key Lab Multimedia Trusted Percept & Efficient Com, Minist Educ China, Fujian 361005, Peoples R China.
   [Luo, Gen; Ji, Rongrong] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Xiamen University; Peng Cheng Laboratory
RP Zhou, YY (corresponding author), Xiamen Univ, Key Lab Multimedia Trusted Percept & Efficient Com, Minist Educ China, Fujian 361005, Peoples R China.
EM luogen@stu.xmu.edu.cn; zhouyiyi@xmu.edu.cn; sunjiamu@stu.xmu.edu.cn;
   xssun@xmu.edu.cn; rrji@xmu.edu.cn
OI Luo, Gen/0000-0001-5334-1843
FU National Key Ramp;D Program of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bajaj M, 2019, IEEE I CONF COMP VIS, P4280, DOI 10.1109/ICCV.2019.00438
   Bu Y., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM2022.3219642, DOI 10.1109/TMM2022.3219642]
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gan Z., 2020, Adv. Neural Inf .Process. Syst, V33, P6616
   Ge Z., 2021, ARXIV
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Huang BB, 2021, PROC CVPR IEEE, P16883, DOI 10.1109/CVPR46437.2021.01661
   Jiang Huaizu, 2020, P IEEE CVF C COMP VI, P10267, DOI DOI 10.1109/CVPR42600.2020.01028
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kim W., 2021, PROC INT C MACH LEAR, P5583
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lan Z., 2019, PROC INT C LEARN REP, P1
   Liao Y, 2022, IEEE T IMAGE PROCESS, V31, P4266, DOI 10.1109/TIP.2022.3181516
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Liu J, 2023, PROC CVPR IEEE, P18653, DOI 10.1109/CVPR52729.2023.01789
   Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu JS, 2019, ADV NEUR IN, V32
   Luo G, 2024, INT J COMPUT VISION, V132, P1, DOI 10.1007/s11263-023-01871-1
   Luo G, 2022, IEEE T IMAGE PROCESS, V31, P3386, DOI 10.1109/TIP.2021.3139234
   Luo G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1274, DOI 10.1145/3394171.3414006
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Nair V., 2010, ICML, P807
   Ordonez V., 2011, NeurIPS, V24
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Sadhu A, 2019, IEEE I CONF COMP VIS, P4693, DOI 10.1109/ICCV.2019.00479
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun MY, 2023, IEEE T MULTIMEDIA, V25, P2446, DOI 10.1109/TMM.2022.3147385
   Sun MJ, 2021, PROC CVPR IEEE, P14055, DOI 10.1109/CVPR46437.2021.01384
   Tarvainen A., 2017, P 31 ANN C NEUR INF, P1195, DOI DOI 10.1137/0330046
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang P., 2022, PMLR, P23318
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Yan B, 2023, PROC CVPR IEEE, P15325, DOI 10.1109/CVPR52729.2023.01471
   Yang ZY, 2022, LECT NOTES COMPUT SC, V13696, P521, DOI 10.1007/978-3-031-20059-5_30
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Ye JB, 2022, PROC CVPR IEEE, P15481, DOI 10.1109/CVPR52688.2022.01506
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P3208
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Yu Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1114
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang Y, 2017, ADV SOC SCI EDUC HUM, V130, P557
   Zhang Z, 2019, Arxiv, DOI arXiv:1902.04103
   Zhang ZP, 2023, NEUROCOMPUTING, V518, P523, DOI 10.1016/j.neucom.2022.10.022
   Zhao H., 2022, IEEE Trans. Neural Netw Learn. Syst., early access, DOI [10.1109/TNNLS.20223183827, DOI 10.1109/TNNLS.20223183827]
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2054, DOI 10.1109/ICCV48922.2021.00208
   Zhou YY, 2022, IEEE T PATTERN ANAL, V44, P697, DOI 10.1109/TPAMI.2019.2956699
   Zhou YY, 2023, IEEE T NEUR NET LEAR, V34, P134, DOI 10.1109/TNNLS.2021.3090426
   Zhu CY, 2022, LECT NOTES COMPUT SC, V13695, P598, DOI 10.1007/978-3-031-19833-5_35
NR 81
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3689
EP 3700
DI 10.1109/TMM.2023.3314153
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200006
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Qin, B
   Meng, FQ
   Yuan, SJ
   Mu, B
AF Qin, Bo
   Meng, Fanqing
   Yuan, Shijin
   Mu, Bin
TI CAU: A Causality Attention Unit for Spatial-Temporal Sequence Forecast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Causal inference; causality attention unit (CAU); spatial-temporal
   sequence forecasting (STSF); transfer entropy (TE)
ID ACTION RECOGNITION; INFERENCE; NETWORKS; MODEL
AB Existing convolution recurrent neural networks (ConvRNNs)-based memory cells majorly take advantage of gated structures and attention mechanisms to extract discontinuous latent associations for spatial-temporal sequence forecast (STSF) problems, which may lead to serious over-fitting and spurious relationships with correlated noise. It is a consensus that incorporating cause-effect relationships in modeling can alleviate these problems. In this paper, we propose a Causality Attention Unit (CAU) to assist ConvRNNs by complementing the causal inference ability in a plug-and-play way. Specifically, CAU serially consists of the attention module and causality module. The former is constructed by a spatial-channel attention layer, which preliminarily generates the correlated future with the correlations between historical memories and the current state. The latter borrows the idea of transfer entropy (T E) to detect the latent cause-effect relationships and precisely correct the correlated future. A space-time exchange strategy for accelerating the calculation of T E in CAU is also designed. CAU can be easily combined with the existing ConvRNN cells, and we construct a simple general model to predict long-term spatial-temporal series, which consists of encoder/decoder and stacked CAU paralleled to stacked ConvRNN cells. After determining the optimal model structure, we carry out a series of experiments to evaluate model performance, including comparisons with other advanced models, training loss analysis, and multiple ablation and sensitivity studies. Experimental results show that our proposed model can effectively improve the performances of existing ConvRNNs to the state-of-the-are level on representative public datasets, including Moving MNIST, KTH, BAIR, and WeatherBench. The ablation and sensitivity studies verify the superiority of CAU. The learned causal maps precisely distinguish the pixel attributions and motion characteristics in sophisticated entangled scenarios.
C1 [Qin, Bo] Minist Educ, Key Lab Polar Atmosphere ocean ice Syst Weather &, Shanghai 200438, Peoples R China.
   [Qin, Bo] Minist Educ, Key Lab Polar Atmosphere ocean ice Syst Weatherand, Shanghai, Peoples R China.
   [Meng, Fanqing] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
   [Yuan, Shijin; Mu, Bin] State Key Lab Intelligent Autonomous Syst, Shanghai 201804, Peoples R China.
C3 Shanghai Jiao Tong University
RP Yuan, SJ; Mu, B (corresponding author), State Key Lab Intelligent Autonomous Syst, Shanghai 201804, Peoples R China.
EM boqin@fudan.edu.cn; mengfanqing33@gmail.com; yuanshijin@tongji.edu.cn;
   binmu@tongji.edu.cn
RI ; Yuan, Shijin/M-3177-2017
OI Qin, Bo/0000-0001-7093-6531; Dai, Guokun/0000-0001-5303-2952; Yuan,
   Shijin/0000-0002-8102-3137
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai C, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3162882
   Bin M, 2022, GEOSCI MODEL DEV, V15, P4105, DOI 10.5194/gmd-15-4105-2022
   Chang Z, 2021, ADV NEUR IN, V34
   Chang Z, 2023, IEEE T MULTIMEDIA, V25, P2354, DOI 10.1109/TMM.2022.3146721
   Chen J, 2021, AAAI CONF ARTIF INTE, V35, P1027
   Chen XT, 2020, IEEE T MULTIMEDIA, V22, P1591, DOI 10.1109/TMM.2019.2946475
   Denton Emily, 2018, P MACHINE LEARNING R, V80
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Ebert Frederik, 2017, ARXIV171005268, P344, DOI DOI 10.48550/ARXIV.1710.05268
   Gao W, 2021, IEEE T CIRC SYST VID, V31, P4147, DOI 10.1109/TCSVT.2021.3104305
   Gao Z., 2022, ADV NEURAL INF PROCE, VVolume 35, P25390, DOI DOI 10.48550/ARXIV.2207.05833
   Gu A., 2020, INT C MACHINE LEARNI, P3800
   Guen V. L., 2020, P IEEE COMP SOC C CO, P11474, DOI 10.1109/CVPR42600.2020.01149
   Guo HZ, 2022, IEEE INTERNET THINGS, V9, P3215, DOI 10.1109/JIOT.2021.3100068
   Guo MS, 2019, AAAI CONF ARTIF INTE, P6489
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Kalbkhani H, 2017, IEEE T MULTIMEDIA, V19, P999, DOI 10.1109/TMM.2016.2639379
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li SY, 2024, Arxiv, DOI arXiv:2211.03295
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Lin XR, 2022, AAAI CONF ARTIF INTE, P1620
   Lin ZH, 2020, AAAI CONF ARTIF INTE, V34, P11531
   Liu SH, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/9134942
   Luo YA, 2020, NAT MACH INTELL, V2, P426, DOI 10.1038/s42256-020-0218-x
   Lv ZQ, 2023, COMPUT J, V66, P565, DOI 10.1093/comjnl/bxab178
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Mu B, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3250642
   Mu B, 2022, J ADV MODEL EARTH SY, V14, DOI 10.1029/2022MS003132
   Mu B, 2021, GEOSCI MODEL DEV, V14, P6977, DOI 10.5194/gmd-14-6977-2021
   Peters J, 2017, ADAPT COMPUT MACH LE
   Rasp S, 2020, J ADV MODEL EARTH SY, V12, DOI 10.1029/2020MS002203
   Schreiber T, 2000, PHYS REV LETT, V85, P461, DOI 10.1103/PhysRevLett.85.461
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shi XJ, 2018, Arxiv, DOI arXiv:1808.06865
   Shi XJ, 2015, ADV NEUR IN, V28
   Shi XJ, 2017, ADV NEUR IN, V30
   Tan C, 2023, PROC CVPR IEEE, P18770, DOI 10.1109/CVPR52729.2023.01800
   Tan C, 2022, Arxiv, DOI arXiv:2211.12509
   Tu LF, 2020, T ASSOC COMPUT LING, V8, P621, DOI 10.1162/tacl_a_00335
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2018, P 35 INT C MACH LEAR, P5123, DOI 10.48550/arXiv.1804.06300
   Wang Y., 2018, PROC INT C LEARN REP
   Wang YB, 2019, PROC CVPR IEEE, P9146, DOI 10.1109/CVPR.2019.00937
   Wang YB, 2017, ADV NEUR IN, V30
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z., 2020, FINDINGS ASS COMPUTA, P3431, DOI DOI 10.18653/V1/2020.FINDINGS-EMNLP.308
   Woo G., 2021, PROC INT C LEARN REP
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang BS, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4449
   Yang SY, 2021, J ADV TRANSPORT, V2021, DOI 10.1155/2021/6616800
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Zheng X, 2018, ADV NEUR IN, V31
NR 56
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4749
EP 4763
DI 10.1109/TMM.2023.3326289
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100009
OA hybrid
DA 2024-08-05
ER

PT J
AU Song, XG
   Hu, HY
   Liang, L
   Shi, WW
   Xie, G
   Lu, XF
   Hei, XH
AF Song, Xiaogang
   Hu, Haoyue
   Liang, Li
   Shi, Weiwei
   Xie, Guo
   Lu, Xiaofeng
   Hei, Xinhong
TI Unsupervised Monocular Estimation of Depth and Visual Odometry Using
   Attention and Depth-Pose Consistency Loss
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unsupervised learning; monocular depth estimation; global consistency;
   attention
ID NETWORK
AB Recent studies have shown that joint depth and pose estimation using convolutional neural networks (CNNs) can learn unlabelled monocular frames. However, three problems remain: 1) CNNs can only extract local features due to the limited receptive field, 2) scale ambiguity is inherent in the monocular task, and 3) illness regions violate the photometric consistency assumption and produce large errors. We propose a novel framework, ADPDepth, with corresponding effective strategies to ameliorate the above problems. First, a PCAtt module is designed to capture the correlation between channels and efficiently extract multiscale spatial information using a multibranch parallel strategy. Second, depth-pose consistency loss is proposed based on the geometric consistency in depth and pose to constrain the scale between samples, eliminate scale ambiguity and obtain a globally consistent scale. To further improve performance, a cover mask is derived from depth-pose consistency for filtering dynamic objects and outliers to reduce the adverse effects of these illness regions. Extensive experiments are conducted on the KITTI, NYU-Depth and Make3D datasets. Based on public benchmarks, the experimental results confirm that the proposed ADPDepth framework achieves state-of-the-art performance. The effectiveness of each strategy is also verified in subsequent ablation experiments.
C1 [Song, Xiaogang; Hu, Haoyue; Liang, Li; Shi, Weiwei; Lu, Xiaofeng; Hei, Xinhong] Xian Univ Technol, Sch Comp Sci & Engn, Xian 710048, Peoples R China.
   [Xie, Guo] Xian Univ Technol, Sch Automat & Informat Engn, Xian 710048, Peoples R China.
C3 Xi'an University of Technology; Xi'an University of Technology
RP Song, XG (corresponding author), Xian Univ Technol, Sch Comp Sci & Engn, Xian 710048, Peoples R China.
EM songxg@xaut.edu.cn; 786059151@qq.com; liangli@xaut.edu.cn;
   296988125@qq.com; guoxie@xaut.edu.cn; luxiaofeng@xuat.edu.cn;
   heixinhong@xaut.edu.cn
OI Song, Xiaogang/0000-0001-9841-9624; HEI, Xinhong/0000-0002-6394-0492
FU National Defence Pre-Research Foundation of China
FX No Statement Available
CR Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bian JW, 2019, ADV NEUR IN, V32
   Cao YZH, 2020, IEEE T CIRC SYST VID, V30, P2674, DOI 10.1109/TCSVT.2019.2929202
   Cao YZH, 2018, IEEE T CIRC SYST VID, V28, P3174, DOI 10.1109/TCSVT.2017.2740321
   CHAKRABARTI A, 2016, ADV NEURAL INFORM PR, P2658
   Chen S, 2022, IEEE T CIRC SYST VID, V32, P1328, DOI 10.1109/TCSVT.2021.3068834
   Dan X., 2018, P ADV NEUR INF PROC, P3961
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   Guizifini V., 2020, PROC INT C LEARN REP, P1
   Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256
   Hartley A.Z. R., 2002, Multiple View Geometry in Computer Vision
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kim J., 2017, Assoc. Realistic Media Ind., V79, P1
   Kingma D. P., 2014, arXiv
   Kline J, 2020, PROCEEDINGS OF THE 2020 32ND INTERNATIONAL TELETRAFFIC CONGRESS (ITC 32), P1, DOI [10.1109/ITC3249928.2020.00009, 10.1007/978-3-030-58565-5_35]
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35
   Lee S, 2021, AAAI CONF ARTIF INTE, V35, P1863
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li H., 2020, C ROB LEARN PMLR, P1908
   Ling CW, 2022, IEEE T MULTIMEDIA, V24, P2938, DOI 10.1109/TMM.2021.3091308
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu YF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3133956
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Mohaghegh H, 2019, IEEE T CIRC SYST VID, V29, P683, DOI 10.1109/TCSVT.2018.2808682
   Paszke A, 2017, NIPS W
   Pillai S, 2019, IEEE INT CONF ROBOT, P9250, DOI [10.1109/icra.2019.8793621, 10.1109/ICRA.2019.8793621]
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Saxena A., 2005, P ADV NEUR INF PROC, P1
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song KY, 2019, IEEE T CIRC SYST VID, V29, P2972, DOI 10.1109/TCSVT.2018.2875449
   Song M, 2021, IEEE T CIRC SYST VID, V31, P4381, DOI 10.1109/TCSVT.2021.3049869
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Sun QY, 2022, IEEE T NEUR NET LEAR, V33, P2023, DOI 10.1109/TNNLS.2021.3100895
   Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wang Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3181062
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Y, 2018, Arxiv, DOI arXiv:1810.03654
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Yan H, 2019, IEEE T CIRC SYST VID, V29, P80, DOI 10.1109/TCSVT.2017.2772892
   Yanetal Y., 2022, IEEE Trans. Multimedia, V24, P1473
   Yang N, 2020, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR42600.2020.00136
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043
   Zhang YY, 2020, IEEE T IMAGE PROCESS, V29, P7019, DOI 10.1109/TIP.2020.2997247
   Zhao Wang, 2020, 2020 IEEE CVF C COMP, P9151, DOI DOI 10.1109/CVPR42600.2020.00917
   Zhou JS, 2019, IEEE I CONF COMP VIS, P8617, DOI 10.1109/ICCV.2019.00871
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhu JJ, 2011, IEEE T PATTERN ANAL, V33, P1400, DOI 10.1109/TPAMI.2010.172
NR 61
TC 2
Z9 2
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3517
EP 3529
DI 10.1109/TMM.2023.3312950
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200003
DA 2024-08-05
ER

PT J
AU Tan, XB
   Li, SM
   Wang, SY
   Liu, YY
   Zheng, Q
   Yang, J
AF Tan, Xiaobin
   Li, Simin
   Wang, Shunyi
   Liu, Yangyang
   Zheng, Quan
   Yang, Jian
TI Cooperative Bargaining Game Based Adaptive Video Multicast Over Mobile
   Edge Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile edge networks (MEN); adaptive video multicast; cooperative
   bargaining game; quality of experience (QoE)
ID BANDWIDTH ALLOCATION; MULTIRATE MULTICAST; TRANSMISSION; DELIVERY;
   SELECTION
AB Video delivery over wireless networks with limited network resources and dynamically changing channel quality is an important challenge, and one of the most promising solutions for tackling this problem is to employ multicast transmissions, which improves network resource utilization efficiency. This article focuses on delivering video concurrently to multiple users over mobile networks leveraging Multicast Broadcast Multimedia Service (MBMS) and Mobile Edge Computing (MEC) technology. We propose a $k$-means clustering and cooperative bargaining game-based adaptive video multicast solution (KGS) over mobile edge networks, with the goal of providing high-quality video delivery service in an envisaged MBMS service area across multiple cell sites. By taking user subgrouping, resource allocation, and bitrate adaptation into account, we establish a Cooperative Bargaining Game (CBG) based joint optimization model for multiple Multicast Broadcast Synchronized Frequency Network (MBSFN) users in mobile edge networks. Then we transform this model into a two-stage convex optimization problem and a nonlinear integer programming problem. We propose a heuristic approach to solve them and achieve a Pareto optimal video delivery strategy for all users. Finally, the efficiency of the proposed scheme is evaluated through extensive simulations.
C1 [Tan, Xiaobin; Li, Simin; Wang, Shunyi; Liu, Yangyang; Zheng, Quan; Yang, Jian] Univ Sci & Technol China USTC, Dept Automat, Hefei 230026, Peoples R China.
   [Tan, Xiaobin; Zheng, Quan; Yang, Jian] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zheng, Q (corresponding author), Univ Sci & Technol China USTC, Dept Automat, Hefei 230026, Peoples R China.
EM xbtan@ustc.edu.cn; lisimin@mail.ustc.edu.cn; wsy12@mail.ustc.edu.cn;
   lyy2017@mail.ustc.edu.cn; qzheng@ustc.edu.cn; jianyang@ustc.edu.cn
OI Yang, Jian/0000-0002-7329-4738; Wang, Shunyi/0000-0002-7149-8884; Tan,
   Xiaobin/0000-0001-7489-2839
FU National Key Ramp;D Program of China
FX No Statement Available
CR Araniti G, 2020, IEEE T WIREL COMMUN, V19, P808, DOI 10.1109/TWC.2019.2948846
   Barnett Thomas, 2018, AMERICASEMEAR CISCO, P1
   Bentaleb A, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3336496
   Boyd S., 2008, CONVEX OPTIMIZATION
   Chen C, 2014, IEEE T IMAGE PROCESS, V23, P2206, DOI 10.1109/TIP.2014.2312613
   Chen SW, 2020, IEEE T VEH TECHNOL, V69, P10227, DOI 10.1109/TVT.2020.3004048
   EBU, 2014, Tech. Rep. TR 027
   Han Z., 2012, Game Theory in Wireless and Communication Networks: Theory Models and Applications
   Hou RH, 2020, IEEE T BROADCAST, V66, P647, DOI 10.1109/TBC.2020.2979345
   Hung YH, 2020, IEEE T MOBILE COMPUT, V19, P922, DOI 10.1109/TMC.2019.2901786
   Jiasi Chen, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1266, DOI 10.1109/INFOCOM.2015.7218502
   Khalid A, 2019, IEEE INFOCOM SER, P433, DOI [10.1109/INFOCOM.2019.8737643, 10.1109/infocom.2019.8737643]
   Lederer S., 2012, P 3 MULTIMEDIA SYSTE, P89, DOI DOI 10.1145/2155555.2155570
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P361, DOI 10.1109/TMM.2017.2745709
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752
   Lohmar T., 2013, Celebrating 90 Years of Technology Insights
   Maniotis P, 2020, IEEE T MULTIMEDIA, V22, P2382, DOI 10.1109/TMM.2019.2957993
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mao YY, 2017, IEEE COMMUN SURV TUT, V19, P2322, DOI 10.1109/COMST.2017.2745201
   Mehrabi A, 2019, IEEE T MOBILE COMPUT, V18, P787, DOI 10.1109/TMC.2018.2850026
   Park J, 2018, IEEE T VEH TECHNOL, V67, P4487, DOI 10.1109/TVT.2018.2789899
   Säily M, 2020, IEEE T BROADCAST, V66, P404, DOI 10.1109/TBC.2020.2985906
   Sani Y, 2017, IEEE COMMUN SURV TUT, V19, P2985, DOI 10.1109/COMST.2017.2725241
   Spiteri K, 2020, IEEE ACM T NETWORK, V28, P1698, DOI 10.1109/TNET.2020.2996964
   Stockhammer T., 2011, Proceedings of the second annual ACM conference on Multimedia systems, P133
   Tan XB, 2021, IEEE T MULTIMEDIA, V24, P3491, DOI 10.1109/TMM.2021.3100768
   Taranetz M, 2015, IEEE ACCESS, V3, P725, DOI 10.1109/ACCESS.2015.2437903
   ul Zuhra S, 2019, IEEE T VEH TECHNOL, V68, P12239, DOI 10.1109/TVT.2019.2945987
   Villa Bjorn J., 2012, Information and Communication Technologies. Proceedings 18th EUNICE/IFIP WG 6.2, 6.6. International Conference, EUNICE 2012, P183, DOI 10.1007/978-3-642-32808-4_17
   Xu XD, 2017, IEEE ACCESS, V5, P16406, DOI 10.1109/ACCESS.2017.2739343
   Yang J, 2018, IEEE T MULTIMEDIA, V20, P1260, DOI 10.1109/TMM.2017.2760630
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Yuan H, 2018, IEEE T MULTIMEDIA, V20, P183, DOI 10.1109/TMM.2017.2724850
   Zhang ZL, 2022, IEEE T COMMUN, V70, P350, DOI 10.1109/TCOMM.2021.3115480
   Zhao RM, 2018, IEEE T MULTIMEDIA, V20, P3069, DOI 10.1109/TMM.2018.2827783
   Zhong H, 2021, IEEE T MULTIMEDIA, V23, P982, DOI 10.1109/TMM.2020.2991539
NR 36
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2380
EP 2394
DI 10.1109/TMM.2023.3295569
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100013
DA 2024-08-05
ER

PT J
AU Wang, HS
   Lou, JX
   Liu, XC
   Tan, HC
   Whitaker, R
   Liu, HT
AF Wang, Huasheng
   Lou, Jianxun
   Liu, Xiaochang
   Tan, Hongchen
   Whitaker, Roger
   Liu, Hantao
TI SSPNet: Predicting Visual Saliency Shifts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Distortion; Gaze tracking; Computational modeling;
   Predictive models; Image coding; Convolutional neural networks;
   Saliency; saliency shift; eye-tracking; transformer; convolutional
   neural networks
ID DETECTION MODEL; ATTENTION; IMAGE; QUALITY
AB When images undergo quality degradation caused by editing, compression or transmission, their saliency tends to shift away from its original position. Saliency shifts indicate visual behaviour change and therefore contain vital information regarding perception of visual content and its distortions. Given a pristine image and its distorted format, we want to be able to detect saliency shifts induced by distortions. The resulting saliency shift map (SSM) can be used to identify the region and degree of visual distraction caused by distortions, and consequently to perceptually optimise image coding or enhancement algorithms. To this end, we first create a largest-of-its-kind eye-tracking database, comprising 60 pristine images and their associated 540 distorted formats viewed by 96 subjects. We then propose a computational model to predict the saliency shift map (SSM), utilising transformers and convolutional neural networks. Experimental results demonstrate that the proposed model is highly effective in detecting distortion-induced saliency shifts in natural images.
C1 [Wang, Huasheng; Lou, Jianxun; Whitaker, Roger; Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF244AG, Wales.
   [Liu, Xiaochang] Sun Yat Sen Univ, Sch Math, Guangzhou 510275, Peoples R China.
   [Tan, Hongchen] Beijing Univ Technol, Inst Artificial Intelligence, Beijing 100124, Peoples R China.
C3 Cardiff University; Sun Yat Sen University; Beijing University of
   Technology
RP Lou, JX (corresponding author), Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF244AG, Wales.
EM wanghs@cardiff.ac.uk; LouJ2@cardiff.ac.uk; liuxiaochang8012@163.com;
   tanhongchenphd@bjut.edu.cn; whitakerrm@cardiff.ac.uk;
   liuh35@cardiff.ac.uk
RI ; Tan, Hongchen/AGZ-4796-2022
OI Liu, Xiaochang/0009-0009-4915-2375; Tan, Hongchen/0000-0001-6915-8736;
   Wang, Huasheng/0009-0003-9290-8445; LOU, JIANXUN/0000-0002-2982-595X;
   Tan, Hongchen/0000-0002-3179-8129
FU China Scholarship Council
FX No Statement Available
CR Alers H, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.043012
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   [Anonymous], 2002, Int. Telecommun. Union
   Borji A., 2015, PROC IEEE C COMPUT V
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bruckert A, 2021, NEUROCOMPUTING, V453, P693, DOI 10.1016/j.neucom.2020.06.131
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Cerf M, 2009, J VISION, V9, DOI 10.1167/9.12.10
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chollet F., 2017, Deep learning with Python, DOI DOI 10.1186/S12859-020-03546-X
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Engelke U, 2011, IEEE SIGNAL PROC MAG, V28, P50, DOI 10.1109/MSP.2011.942473
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Harel J., 2007, ADV NEURAL INF PROCE, V19, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jia S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103887
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Johnson R. W., 2004, Attention: Theory and Practice
   Judd F., 2012, Tech. Rep. MIT-CSAIL-TR-2012-001
   Kim H, 2015, IEEE T MULTIMEDIA, V17, P2198, DOI 10.1109/TMM.2015.2493367
   Kingma D.P., 2014, Proc. of ICLR
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Kummerer M, 2015, Arxiv, DOI [arXiv:1411.1045, 10.48550/arXiv.1411.1045]
   Kummereretal M, 2022, MIT/Tubingensaliencybenchmark
   Leveque L, 2020, IEEE IMAGE PROC, P116, DOI 10.1109/ICIP40778.2020.9190737
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001
   Liu HT, 2016, IEEE T IMAGE PROCESS, V25, P3087, DOI 10.1109/TIP.2016.2561406
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lou J, 2023, IEEE Trans. Multimedia, P1
   Lou JX, 2022, NEUROCOMPUTING, V494, P455, DOI 10.1016/j.neucom.2022.04.080
   Min G., 2014, IEEE INT C MULTIMEDI, P1
   Niebur E, 1998, ATTENTIVE BRAIN, P163
   Pate Y, 2021, IEEE WINT CONF APPL, P227, DOI 10.1109/WACV48630.2021.00027
   Peterson MS, 2004, PERCEPT PSYCHOPHYS, V66, P398, DOI 10.3758/BF03194888
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Redi H., 2011, Proc. SPIE, V7865, P267
   Salvucci Dario D., 2000, P 2000 S EYE TRACK R, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Shi R, 2012, IEEE SIGNAL PROC LET, V19, P215, DOI 10.1109/LSP.2012.2188388
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Souza LS, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107028
   Szegedy C., 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Vu CT, 2008, 2008 IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS & INTERPRETATION, P73, DOI 10.1109/SSIAI.2008.4512288
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yang XQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3009354
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P2424, DOI 10.1109/TIP.2017.2681424
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
NR 65
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4938
EP 4949
DI 10.1109/TMM.2023.3327886
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600038
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Wang, YF
   Liu, LY
   Yuan, C
   Li, MB
   Liu, J
AF Wang, Yifan
   Liu, Liyuan
   Yuan, Chun
   Li, Minbo
   Liu, Jing
TI Negative-Sensitive Framework With Semantic Enhancement for Composed
   Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Composed image retrieval; semantic learning; cross-modal retrieval;
   distribution learning; attention mechanism
AB Composed image retrieval is a challenging task in the field of multi-modal learning, aiming at measuring the similarities between target images and query images with modification sentences. Most previous methods either construct feature composition for the query image and modification text or concentrate on extracting cross-modal alignments. However, these methods are prone to neglect the negative impacts of the mismatched correspondences between the hybrid-modal query and target, which could be discriminative when comparing similar instances. Besides, localized textual representations are not fully explored when learning similarities between the query and the target. To overcome the above issues, we propose a Negative-Sensitive Framework with Semantic Enhancement (NSFSE) for mining the adaptive boundaries between matched and mismatched samples with comprehensive consideration of positive and negative correspondences. It can optimize the threshold dynamically based on distributions to explore the intrinsic characteristics of positive and negative correlations, which could further facilitate accurate similarity learning. A text-guided attention mechanism after infusing cross-modal affinities on localized word features is exploited in NSFSE to explore latent semantic-related visual similarity and cross-modal similarity simultaneously. The performance of extensive experiments and comprehensive analysis on three representative datasets CIRR, FashionIQ, and Fashion200 K demonstrate the effectiveness of negative mining of similarity with semantic enhancement in the proposed NSFSE.
C1 [Wang, Yifan; Liu, Liyuan; Yuan, Chun] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Li, Minbo] Fudan Univ, Software Sch, Shanghai 200437, Peoples R China.
   [Liu, Jing] Taian Univ, Tai An 271000, Peoples R China.
   [Liu, Jing] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
   [Liu, Jing] Shenyang Inst Comp Technol, Shenyang 110178, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Fudan University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Yuan, C (corresponding author), Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM yuanc@sz.tsinghua.edu.cn
RI Liu, Jing/GRX-8235-2022
OI Liu, Jing/0000-0002-2819-0200; Wang, Yifan/0000-0002-6719-5063
FU National Key R&D Program of China
FX No Statement Available
CR Anwaar MU, 2021, IEEE WINT CONF APPL, P1139, DOI 10.1109/WACV48630.2021.00118
   Baldrati A, 2023, IEEE I CONF COMP VIS, P15292, DOI 10.1109/ICCV51070.2023.01407
   Baldrati A, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3617597
   Banerjee P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1888, DOI 10.1109/ICCV48922.2021.00192
   Brants T., 2002, Proceedings of the Eleventh International Conference on Information and Knowledge Management. CIKM 2002, P211, DOI 10.1145/584792.584829
   Chawla P, 2021, IEEE COMPUT SOC CONF, P3973, DOI 10.1109/CVPRW53098.2021.00448
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Chen JY, 2023, Arxiv, DOI arXiv:2308.08131
   Chen Y., 2024, PROCINT C LEARN REPR, P1
   Chen YB, 2020, PROC CVPR IEEE, P2998, DOI 10.1109/CVPR42600.2020.00307
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   Delmas G., 2022, PROC INT C LEARN REP, P1
   Dodds E, 2020, Arxiv, DOI arXiv:2007.00145
   Gao YZ, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2023, P76, DOI 10.1145/3591106.3592238
   Goenka S, 2022, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR52688.2022.01371
   Gu G, 2024, Arxiv, DOI arXiv:2303.11916
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosseinzadeh M, 2020, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR42600.2020.00365
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Hui PY, 2014, IEEE-ACM T AUDIO SPE, V22, P417, DOI 10.1109/TASLP.2013.2294586
   Huynh D., 2020, P INT C NEUR INF PRO, V33, P19849
   Ji KX, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P949, DOI 10.1145/3477495.3531960
   Kong SY, 2006, INT CONF ACOUST SPEE, P941
   Kuo FF, 2013, IEEE INT CON MULTI
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lee S, 2021, PROC CVPR IEEE, P802, DOI 10.1109/CVPR46437.2021.00086
   Lin Y., 2022, Advances in Neural Information Processing Systems, V35, P10560
   Liu Y, 2021, PROC CVPR IEEE, P14949, DOI 10.1109/CVPR46437.2021.01471
   Liu ZJ, 2023, IEEE T CIRC SYST VID, V33, P2465, DOI 10.1109/TCSVT.2022.3220297
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2105, DOI 10.1109/ICCV48922.2021.00213
   Ma XH, 2022, IEEE T MULTIMEDIA, V24, P2998, DOI 10.1109/TMM.2021.3091888
   Malinowski M, 2018, Arxiv, DOI arXiv:1809.04482
   Neculai A, 2022, IEEE COMPUT SOC CONF, P4546, DOI 10.1109/CVPRW56347.2022.00501
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Qi XF, 2021, NEUROCOMPUTING, V450, P143, DOI 10.1016/j.neucom.2021.03.129
   Saini N, 2022, PROC CVPR IEEE, P13648, DOI 10.1109/CVPR52688.2022.01329
   Saito K, 2023, PROC CVPR IEEE, P19305, DOI 10.1109/CVPR52729.2023.01850
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi XJ, 2015, ADV NEUR IN, V28
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Suhr A, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6418
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Tu YB, 2023, IEEE T IMAGE PROCESS, V32, P2620, DOI 10.1109/TIP.2023.3268004
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang H, 2022, IEEE T MULTIMEDIA, V24, P2515, DOI 10.1109/TMM.2021.3083109
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P3362, DOI 10.1109/TMM.2020.3024822
   Wang YF, 2022, IEEE MULTIMEDIA, V29, P38, DOI 10.1109/MMUL.2022.3144972
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Xu K., 2015, PROC INT C MACH LEAR, P2048
   Xu X, 2023, INFORM FUSION, V91, P327, DOI 10.1016/j.inffus.2022.10.013
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3458281
   Xu ZW, 2021, IEEE T MULTIMEDIA, V24, P3652, DOI 10.1109/TMM.2021.3104411
   Yanbei Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P136, DOI 10.1007/978-3-030-58542-6_9
   Yang YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3303, DOI 10.1145/3474085.3475483
   Zhang FF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478642
   Zhang FF, 2022, IEEE T IMAGE PROCESS, V31, P1000, DOI 10.1109/TIP.2021.3138302
   Zhang GJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5353, DOI 10.1145/3474085.3475659
   Zhang Jiayan, 2023, 2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW), P411, DOI 10.1109/ICMEW59549.2023.00077
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhang X, 2023, Arxiv, DOI arXiv:2306.02092
   Zhang ZJ, 2022, IEEE T MULTIMEDIA, V24, P3101, DOI 10.1109/TMM.2021.3093725
   Zhao S, 2023, Arxiv, DOI arXiv:2310.01358
NR 63
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7608
EP 7621
DI 10.1109/TMM.2024.3369898
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000010
DA 2024-08-05
ER

PT J
AU Weng, ZY
   Zhuang, HP
   Luo, FL
   Li, HZ
   Lin, ZP
AF Weng, Zhenyu
   Zhuang, Huiping
   Luo, Fulin
   Li, Haizhou
   Lin, Zhiping
TI Few-Shot Contrastive Transfer Learning With Pretrained Model for Masked
   Face Verification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Data models; Training; Task analysis; Transfer learning; Training
   data; Prototypes; Face verification; transfer learning; few-shot
   learning; masked face; COVID-19
AB Face verification has seen remarkable progress that benefits from large-scale publicly available databases. However, it remains a challenge how to generalize a pretrained face verification model to a new scenario with a limited amount of data. In many real-world applications, the training database only contains a limited number of identities with two images for each identity due to the privacy concern. In this article, we propose to transfer knowledge from a pretrained unmasked face verification model to a new model for verification between masked and unmasked faces, to meet the application requirements during the COVID-19 pandemic. To overcome the lack of intra-class diversity resulting from only a pair of masked and unmasked faces for each identity ($\text{i.e.},$ two shots for each identity), a static prototype classification function is designed to learn features for masked faces by utilizing unmasked face knowledge from the pretrained model. Meanwhile, a contrastive constrained embedding function is designed to preserve unmasked face knowledge of the pretrained model during the transfer learning process. By combining these two functions, our method uses knowledge acquired from the pretrained unmasked face verification model to proceed with verification between masked and unmasked faces with a limited amount of training data. Extensive experiments demonstrate that our method can perform better than state-of-the-art methods for verification between masked and unmasked faces in the few-shot transfer learning setting.
C1 [Weng, Zhenyu; Lin, Zhiping] Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang 639798, Singapore.
   [Zhuang, Huiping] South China Univ Technol, Shien Ming Wu Sch Intelligent Engn, Guangzhou 510006, Peoples R China.
   [Luo, Fulin] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
   [Li, Haizhou] Chinese Univ Hong Kong, Shenzhen 518172, Peoples R China.
   [Li, Haizhou] Natl Univ Singpaore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
C3 Nanyang Technological University; South China University of Technology;
   Chongqing University; The Chinese University of Hong Kong, Shenzhen
RP Lin, ZP (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang 639798, Singapore.
EM zhenyu.weng@ntu.edu.sg; hpzhuang@scut.edu.cn; luoflyn@163.com;
   haizhouli@cuhk.edu.cn; ezplin@ntu.edu.sg
RI weng, zhenyu/KIG-5518-2024
OI Li, Haizhou/0000-0001-9158-9401; weng, zhenyu/0000-0001-7857-8687;
   ZHUANG, HUIPING/0000-0002-4612-5445
FU Agency for Science, Technology and Research
FX No Statement Available
CR Boutros F, 2022, IEEE COMPUT SOC CONF, P1577, DOI 10.1109/CVPRW56347.2022.00164
   Buzzega P., 2020, Advances in neural information processing systems, V33, P15920
   Cha H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9496, DOI 10.1109/ICCV48922.2021.00938
   Chen LQ, 2021, PROC CVPR IEEE, P16291, DOI 10.1109/CVPR46437.2021.01603
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Chen T, 2020, PR MACH LEARN RES, V119
   Cheng Y, 2017, IEEE INT CONF COMP V, P1924, DOI 10.1109/ICCVW.2017.227
   Choe J, 2017, IEEE INT CONF COMP V, P1940, DOI 10.1109/ICCVW.2017.229
   Cui YW, 2023, IEEE T MULTIMEDIA, V25, P6422, DOI 10.1109/TMM.2022.3208743
   De Lange M, 2022, IEEE T PATTERN ANAL, V44, P3366, DOI 10.1109/TPAMI.2021.3057446
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2021, IEEE INT CONF COMP V, P1437, DOI 10.1109/ICCVW54120.2021.00165
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Ding FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2281, DOI 10.1145/3394171.3413731
   Ding GC, 2023, IEEE T MULTIMEDIA, V25, P4665, DOI 10.1109/TMM.2022.3180222
   Du H., 2020, ECCV, P36
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Goodfellow I., 2014, PROC INT C LEARN REP, P1
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Guo XJ, 2019, Arxiv, DOI arXiv:1902.10859
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Guo YH, 2019, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2019.00494
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang BJ, 2021, IEEE INT CONF COMP V, P1487, DOI 10.1109/ICCVW54120.2021.00172
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   Huber M, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9667081
   Jian Wang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2612, DOI 10.1109/ICCV.2017.283
   Jung H, 2018, AAAI CONF ARTIF INTE, P3358
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kemker R, 2018, PROC INT C LEARN REP, P1
   Leffler CT, 2020, AM J TROP MED HYG, V103, P2400, DOI 10.4269/ajtmh.20-1015
   Li H., 2020, PROC INT C LEARN REP, P1
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Liu H, 2021, IEEE T CIRC SYST VID, V31, P3093, DOI 10.1109/TCSVT.2020.3035890
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Lu YW, 2021, IEEE T MULTIMEDIA, V23, P2056, DOI 10.1109/TMM.2020.3007340
   Lyu W, 2020, HEALTH AFFAIR, V39, P1419, DOI 10.1377/hlthaff.2020.00818
   van de Ven GM, 2019, Arxiv, DOI arXiv:1904.07734
   McCloskey M., 1989, Psychology of learning and motivation, V24, P165
   Meng Q, 2021, PROC CVPR IEEE, P14220, DOI 10.1109/CVPR46437.2021.01400
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Ostapenko O, 2019, PROC CVPR IEEE, P11313, DOI 10.1109/CVPR.2019.01158
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
   Park GM, 2021, IEEE T NEUR NET LEAR, V32, P2691, DOI 10.1109/TNNLS.2020.3007548
   Qiu HB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10860, DOI 10.1109/ICCV48922.2021.01070
   Ren MY, 2019, ADV NEUR IN, V32
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thuseethan S, 2022, IEEE T MULTIMEDIA, V24, P4367, DOI 10.1109/TMM.2021.3116434
   Tian Y., 2019, PROC INT C LEARN REP, P1
   Ulhaq A, 2020, IEEE ACCESS, V8, P179437, DOI 10.1109/ACCESS.2020.3027685
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Natta M, 2020, J LAW BIOSCI, V7, DOI 10.1093/jlb/lsaa038
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3779, DOI 10.1145/3474085.3478324
   Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081
   Wang M, 2019, IEEE I CONF COMP VIS, P692, DOI 10.1109/ICCV.2019.00078
   Wang XB, 2020, AAAI CONF ARTIF INTE, V34, P12241
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu Y, 2019, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2019.00046
   Xiaoyu Tao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12180, DOI 10.1109/CVPR42600.2020.01220
   Yi D, 2014, Arxiv, DOI [arXiv:1411.7923, DOI 10.48550/ARXIV.1411.7923]
   Zheng TY, 2017, Arxiv, DOI arXiv:1708.08197
   Zhong YY, 2022, IEEE T MULTIMEDIA, V24, P1186, DOI 10.1109/TMM.2021.3123478
   Zhu XY, 2019, INT J COMPUT VISION, V127, P684, DOI 10.1007/s11263-019-01162-8
NR 68
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3871
EP 3883
DI 10.1109/TMM.2023.3316920
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300013
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Wu, PH
   Wang, WQ
   Chang, FL
   Liu, CS
   Wang, B
AF Wu, Peihao
   Wang, Wenqian
   Chang, Faliang
   Liu, Chunsheng
   Wang, Bin
TI DSS-Net: Dynamic Self-Supervised Network for Video Anomaly Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anomaly detection; dynamic networks; pseudo-abnormal data;
   self-supervised learning
ID ABNORMAL EVENT DETECTION; LOCALIZATION; ENCODER
AB Video Anomaly detection, aiming to detect the abnormal behaviors in surveillance videos, is a challenging task since the anomalous events are diversified and complicated in different situations. And this makes it difficult to use one single static network architecture to extract useful information from diverse abnormal patterns. Therefore, in this article, we propose a novel Dynamic Self-Supervised Network (DSS-Net) to explore both spatial and temporal anomalous information. In our DSS-Net, we design a dynamic network to adaptively select suitable network architecture to extract latent features from different anomalous patterns and normal patterns. Specifically, we generate spatial and temporal pseudo-abnormal data as the input of the dynamic network to conduct self-supervised learning. And we have a specific design on Hybrid Anomaly Dynamic Convolution (HAD-Conv) to extract features for diversified anomalous events adaptively. We utilize both normal and pseudo-abnormal data to encourage the dynamic network to mine the discriminative information. Furthermore, we design a feature separation loss to maximize the difference between the anomalous and normal videos. We evaluate our proposed method on four public anomaly detection datasets and achieve competitive results compared with the state-of-the-art approaches.
C1 [Wu, Peihao; Wang, Wenqian; Chang, Faliang; Liu, Chunsheng; Wang, Bin] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
C3 Shandong University
RP Chang, FL (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM phwu@mail.sdu.edu.cn; wqwang@mail.sdu.edu.cn; flchang@sdu.edu.cn;
   liuchunsheng@sdu.edu.cn; sducvwangb@gmail.com
OI Wang, Wenqian/0000-0003-0285-9786
FU National Natural Science Foundation of China
FX No Statement Available
CR Abati D, 2019, PROC CVPR IEEE, P481, DOI 10.1109/CVPR.2019.00057
   Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Astrid M., 2021, BRIT MACH VIS C, P279
   Chang YP, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108213
   Chen TY, 2018, MULTIMED TOOLS APPL, V77, P14137, DOI 10.1007/s11042-017-5020-3
   Choi J, 2019, IEEE I CONF COMP VIS, P6829, DOI 10.1109/ICCV.2019.00693
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   De Brabandere B, 2016, ADV NEUR IN, V29
   Diba A, 2019, IEEE I CONF COMP VIS, P6191, DOI 10.1109/ICCV.2019.00629
   Doshi K, 2020, IEEE COMPUT SOC CONF, P2658, DOI 10.1109/CVPRW50498.2020.00320
   Fang ZW, 2021, IEEE T MULTIMEDIA, V23, P4106, DOI 10.1109/TMM.2020.3037538
   Foo LG, 2023, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR52729.2023.01013
   Georgescu MI, 2021, PROC CVPR IEEE, P12737, DOI 10.1109/CVPR46437.2021.01255
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hao Y, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108232
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Henrio J, 2018, IEEE SYS MAN CYBERN, P2503, DOI 10.1109/SMC.2018.00429
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang C., 2022, PROC IEEE T MULTIMED, P1
   Hyunjong Park, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14360, DOI 10.1109/CVPR42600.2020.01438
   Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008
   Jiang ZT, 2023, IEEE T IMAGE PROCESS, V32, P1583, DOI 10.1109/TIP.2023.3246792
   Kingma D. P., 2014, arXiv
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Li C., 2021, PROC INT C LEARN REP, P1
   Li CL, 2023, IEEE T PATTERN ANAL, V45, P4430, DOI 10.1109/TPAMI.2022.3194044
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li NJ, 2021, IEEE T MULTIMEDIA, V23, P203, DOI 10.1109/TMM.2020.2984093
   Li TJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13414, DOI 10.1109/ICCV48922.2021.01318
   Li TJ, 2021, PROC CVPR IEEE, P16261, DOI 10.1109/CVPR46437.2021.01600
   Li TJ, 2021, IEEE T IMAGE PROCESS, V30, P7677, DOI 10.1109/TIP.2021.3104183
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo WX, 2022, IEEE T PATTERN ANAL, V44, P7505, DOI 10.1109/TPAMI.2021.3129349
   Luo WX, 2021, IEEE T PATTERN ANAL, V43, P1070, DOI 10.1109/TPAMI.2019.2944377
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Lv H, 2021, PROC CVPR IEEE, P15420, DOI 10.1109/CVPR46437.2021.01517
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Markovitz Amir, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10536, DOI 10.1109/CVPR42600.2020.01055
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Mirzaei H., 2023, PROC 11 INT C LEARN, P1
   Morais R, 2019, PROC CVPR IEEE, P11988, DOI 10.1109/CVPR.2019.01227
   Munawar A, 2017, IEEE WINT CONF APPL, P1017, DOI 10.1109/WACV.2017.118
   Ningning Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P776, DOI 10.1007/978-3-030-58555-6_46
   Piciarelli C, 2006, PATTERN RECOGN LETT, V27, P1835, DOI 10.1016/j.patrec.2006.02.004
   Pourreza M, 2021, IEEE WINT CONF APPL, P2002, DOI 10.1109/WACV48630.2021.00205
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Quader Niamul, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P87, DOI 10.1007/978-3-030-58577-8_6
   Ravanbakhsh M, 2019, IEEE WINT CONF APPL, P1896, DOI 10.1109/WACV.2019.00206
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sabokrou M, 2021, IEEE T NEUR NET LEAR, V32, P675, DOI 10.1109/TNNLS.2020.2979049
   Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Salehi M., 2022, PROC T MACH LEARN RE, P1
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Slavic G, 2022, IEEE T MULTIMEDIA, V24, P1399, DOI 10.1109/TMM.2021.3065232
   Sun C, 2021, IEEE T MULTIMEDIA, V23, P3292, DOI 10.1109/TMM.2020.3023303
   Tianjiao Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P420, DOI 10.1007/978-3-030-58621-8_25
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang WQ, 2021, NEUROCOMPUTING, V433, P37, DOI 10.1016/j.neucom.2020.12.025
   Wang XZ, 2022, IEEE T NEUR NET LEAR, V33, P2301, DOI 10.1109/TNNLS.2021.3083152
   Xu D., 2015, PROC BRIT MACH VIS C, P1
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Xudong Lin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P701, DOI 10.1007/978-3-030-58523-5_41
   Ye ZP, 2023, IEEE T MULTIMEDIA, V25, P2033, DOI 10.1109/TMM.2022.3142387
   Yuan D, 2021, IEEE T IMAGE PROCESS, V30, P976, DOI 10.1109/TIP.2020.3037518
   Zaigham Zaheer Muhammad, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14171, DOI 10.1109/CVPR42600.2020.01419
   Zavrtanik V, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107706
   Zhang D, 2005, PROC CVPR IEEE, P611
   Zhang M., 2022, PROC IEEE T MULTIMED, P1
   Zhao J., 2019, PROC IEEE C COMPUT V, P133
   Zhao Yunqing, 2022, P ADV NEUR INF PROC, V35, P19427
   Zhou JT, 2019, IEEE T INF FOREN SEC, V14, P2537, DOI 10.1109/TIFS.2019.2900907
   Zong B, 2018, PROC 6 INT C LEARN, P1
NR 85
TC 3
Z9 3
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2124
EP 2136
DI 10.1109/TMM.2023.3292596
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100034
DA 2024-08-05
ER

PT J
AU Xiao, AR
   Guan, DY
   Zhang, XQ
   Lu, SJ
AF Xiao, Aoran
   Guan, Dayan
   Zhang, Xiaoqin
   Lu, Shijian
TI Domain Adaptive LiDAR Point Cloud Segmentation With 3D Spatial
   Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE LiDAR point clouds; semantic segmentation; domain adaptation; 3D vision;
   transfer learning; deep learning
ID ADAPTATION; NETWORK
AB Domain adaptive LiDAR point cloud segmentation aims to learn an effective target segmentation model from labelled source data and unlabelled target data, which has attracted increasing attention in recent years due to the difficulty in point-cloud annotation. It remains a very open research challenge as point clouds of different domains often have clear distribution discrepancies with variations in LiDAR sensor configurations, environmental conditions, occlusions, etc. We design a simple yet effective spatial consistency training framework that can learn superior domain-invariant feature representations from unlabelled target point clouds. The framework exploits three types of spatial consistency, namely, geometric-transform consistency, sparsity consistency, and mixing consistency which capture the semantic invariance of point clouds with respect to viewpoint changes, sparsity changes, and local context changes, respectively. With a concise mean teacher learning strategy, our experiments show that the proposed spatial consistency training outperforms the state-of-the-art significantly and consistently across multiple public benchmarks.
C1 [Xiao, Aoran; Lu, Shijian] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Guan, Dayan] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Zhang, Xiaoqin] Wenzhou Univ, Key Lab Intelligent Informat Safety & Emergency Zh, Wenzhou 325035, Peoples R China.
C3 Nanyang Technological University; Nanyang Technological University;
   Wenzhou University
RP Lu, SJ (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.; Zhang, XQ (corresponding author), Wenzhou Univ, Key Lab Intelligent Informat Safety & Emergency Zh, Wenzhou 325035, Peoples R China.
EM aoran.xiao@ntu.edu.sg; dayan.guan@ntu.edu.sg; zhangxiaoqinnan@gmail.com;
   shijian.lu@ntu.edu.sg
OI Guan, Dayan/0000-0001-9752-1520; Xiao, Aoran/0000-0002-2956-0613; Lu,
   Shijian/0000-0002-6766-2506
FU Ministry of Education Singapore
FX No Statement Available
CR Aila T., 2017, P INT C LEARN REPR
   ang H., 2020, EUR C COMP VIS, P685, DOI DOI 10.1007/978-3-030-58604-1_41
   Araslanov N, 2021, PROC CVPR IEEE, P15379, DOI 10.1109/CVPR46437.2021.01513
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Berthelot D., 2020, INT C LEARN REPR, P1
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chenfeng Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P1, DOI 10.1007/978-3-030-58604-1_1
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Cortinhal Tiago, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12510), P207, DOI 10.1007/978-3-030-64559-5_16
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fong WK, 2022, IEEE ROBOT AUTOM LET, V7, P3795, DOI 10.1109/LRA.2022.3148457
   French M., 2018, P INT C LEARN REPR, P1
   Graham B, 2017, Arxiv, DOI arXiv:1706.01307
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hou J, 2021, PROC CVPR IEEE, P15582, DOI 10.1109/CVPR46437.2021.01533
   Hou YN, 2022, PROC CVPR IEEE, P8469, DOI 10.1109/CVPR52688.2022.00829
   Huang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6515, DOI 10.1109/ICCV48922.2021.00647
   Jaritz M., 2020, CVPR
   Jiang L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6403, DOI 10.1109/ICCV48922.2021.00636
   Kong LD, 2023, PROC CVPR IEEE, P21705, DOI 10.1109/CVPR52729.2023.02079
   Langer F, 2020, IEEE INT C INT ROBOT, P8263, DOI 10.1109/IROS45743.2020.9341508
   Li GR, 2023, PROC CVPR IEEE, P20464, DOI 10.1109/CVPR52729.2023.01960
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Loiseau R, 2022, LECT NOTES COMPUT SC, V13698, P301, DOI 10.1007/978-3-031-19839-7_18
   Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927
   Luo Zhipeng, 2021, P IEEE CVF INT C COM, P8866
   Melas-Kyriazi L, 2021, PROC CVPR IEEE, P12430, DOI 10.1109/CVPR46437.2021.01225
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Pan YC, 2020, IEEE INT VEH SYM, P687, DOI 10.1109/IV47402.2020.9304596
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34
   Sajjadi M, 2016, ADV NEUR IN, V29
   Saltori C, 2022, LECT NOTES COMPUT SC, V13693, P586, DOI 10.1007/978-3-031-19827-4_34
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Tang Haotian, 2022, P MACHINE LEARNING S, V4, P302
   Tarvainen A, 2017, ADV NEUR IN, V30
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tranheden W, 2021, IEEE WINT CONF APPL, P1378, DOI 10.1109/WACV48630.2021.00142
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Weng TY, 2023, IEEE T MULTIMEDIA, V25, P6653, DOI 10.1109/TMM.2022.3212914
   Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI [10.1109/ICRA.2019.8793495, 10.1109/icra.2019.8793495]
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Xiao A., 2022, Advances in Neural Information Processing Systems, P11035
   Xiao AR, 2023, PROC CVPR IEEE, P9382, DOI 10.1109/CVPR52729.2023.00905
   Xiao AR, 2024, Arxiv, DOI arXiv:2305.19812
   Xiao AR, 2022, AAAI CONF ARTIF INTE, P2795
   Xiao AR, 2021, ISPRS J PHOTOGRAMM, V176, P237, DOI 10.1016/j.isprsjprs.2021.04.011
   Xie Q., 2020, Advances in Neural Information Processing Systems, V33, P6256
   Xing Y, 2022, LECT NOTES COMPUT SC, V13690, P621, DOI 10.1007/978-3-031-20056-4_36
   Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9598, DOI 10.1109/CVPR42600.2020.00962
   Yi Li, 2021, CVPR, P15363
   Zhao L., IEEE Trans. Multimedia, DOI [10.1109/IMM.2023.3277281, DOI 10.1109/IMM.2023.3277281]
   Zhao SC, 2021, AAAI CONF ARTIF INTE, V35, P3500
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu XG, 2021, PROC CVPR IEEE, P9934, DOI 10.1109/CVPR46437.2021.00981
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 60
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5536
EP 5547
DI 10.1109/TMM.2023.3335879
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800002
DA 2024-08-05
ER

PT J
AU Yu, Y
   Ni, RR
   Yang, SY
   Zhao, Y
   Kot, AC
AF Yu, Yang
   Ni, Rongrong
   Yang, Siyuan
   Zhao, Yao
   Kot, Alex C.
TI Narrowing Domain Gaps With Bridging Samples for Generalized Face Forgery
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generalized face forgery detection; domain gaps prevention; bridging
   samples generation; cross-domain alignment
ID DEEPFAKE DETECTION
AB Face forgery technology has developed rapidly, causing severe security issues in society. Recently, with the continuous emergence of forgery techniques and types, most forensics methods suffer from the generalization problem. In particular, it is difficult for existing generalized methods to detect fake faces with unseen fake types. The reason is that the distribution gaps among cross-forgery types are too large. In this article, we propose a novel generalized framework to narrow large gaps based on bridging cross-domain alignment to solve this problem. Specifically, our framework consists of three key steps: preventing, bridging and aligning distribution gaps. Firstly, in the feature mining stage, taking advantage of the ability of Instance Normalization (IN) to better tolerate domain gaps, we design Adaptive Batch and Instance Normalization (ABIN) to replace the commonly used BN to adaptively extract features to preliminarily prevent domain gaps. Secondly, we propose to generate bridging samples distributed among the inter-domains to fill large gaps based on progressive linear interpolation operation. Finally, with the help of bridging samples, the cross-domain alignment is performed to better narrow distribution gaps to refine data distribution, which helps to learn a more generalized framework. Extensive experiments show that our proposed framework achieves the state-of-the-art generalized performance.
C1 [Yu, Yang; Ni, Rongrong; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Yu, Yang; Ni, Rongrong; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Yang, Siyuan] Nanyang Technol Univ, Interdisciplinary Grad Programme, Rapid Rich Object Search Lab, Singapore 639798, Singapore.
   [Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Beijing Jiaotong University; Nanyang Technological University; Nanyang
   Technological University
RP Ni, RR (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM yuyuyang@bjtu.edu.cn; rrni@bjtu.edu.cn; siyuan005@e.ntu.edu.sg;
   yzhao@bjtu.edu.cn; eackot@ntu.edu.sg
OI Kot, Alex/0000-0001-6262-8125; Zhao, Yao/0000-0002-8581-9554
FU National Key Ramp;D Program of China
FX No Statement Available
CR Amerini I, 2019, IEEE INT CONF COMP V, P1205, DOI 10.1109/ICCVW.2019.00152
   Chai Lucy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P103, DOI 10.1007/978-3-030-58574-7_7
   Chen L, 2022, PROC CVPR IEEE, P18689, DOI 10.1109/CVPR52688.2022.01815
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081
   Chen W. Tan, 2020, IEEE Transactions on Multimedia (TMM), V23, P3506
   Chugh K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P439, DOI 10.1145/3394171.3413700
   Ciftci Umur Aybars, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3009287
   Dang H, 2020, PROC CVPR IEEE, P5780, DOI 10.1109/CVPR42600.2020.00582
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Ding F, 2023, IEEE T IND INFORM, V19, P6682, DOI 10.1109/TII.2022.3201572
   Ding F, 2021, IEEE T MULTIMEDIA, V24, P3429, DOI 10.1109/TMM.2021.3098422
   Dolhansky Brian, 2019, ARXIV191008854
   Dufour A., 2019, CONTRIBUTING DATA DE
   Fernandes S, 2019, IEEE INT CONF COMP V, P1721, DOI 10.1109/ICCVW.2019.00213
   Fernando T, 2021, IEEE T INF FOREN SEC, V16, P1973, DOI 10.1109/TIFS.2020.3047768
   Frank Joel, 2020, INT C MACH LEARN, P3247
   Ghimire S, 2020, IEEE T MULTIMEDIA, V22, P108, DOI 10.1109/TMM.2019.2925961
   Gu QQ, 2022, AAAI CONF ARTIF INTE, P735
   Gu ZH, 2022, AAAI CONF ARTIF INTE, P744
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hsu GS, 2022, PROC CVPR IEEE, P632, DOI 10.1109/CVPR52688.2022.00072
   Huang E., 2014, Tech. Rep. UM-CS-2014-003
   Hussain S, 2021, IEEE WINT CONF APPL, P3347, DOI 10.1109/WACV48630.2021.00339
   Jeong Y, 2022, AAAI CONF ARTIF INTE, P1060
   Kakar P, 2011, IEEE T MULTIMEDIA, V13, P443, DOI 10.1109/TMM.2011.2121056
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, INT C LEARNING REPRE
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Li HD, 2020, SIGNAL PROCESS, V174, DOI 10.1016/j.sigpro.2020.107616
   Li J., 2021, IEEE T KNOWL DATA EN, P1
   Li JM, 2021, PROC CVPR IEEE, P6454, DOI 10.1109/CVPR46437.2021.00639
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li YZ, 2018, IEEE INT WORKS INFOR
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083
   Liu Z., 2020, P IEEECVF C COMPUTER, P8060
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Nam H, 2018, ADV NEUR IN, V31
   Neekhara P, 2021, IEEE COMPUT SOC CONF, P923, DOI 10.1109/CVPRW53098.2021.00103
   Peng F, 2022, IEEE T CIRC SYST VID, V32, P6657, DOI 10.1109/TCSVT.2022.3177238
   Petrov I., 2020, DeepFaceLab: A simple, flexible and extensible face swapping framework
   Qiao T, 2019, IEEE T MULTIMEDIA, V21, P1077, DOI 10.1109/TMM.2018.2872863
   Qin L, 2022, IEEE T INF FOREN SEC, V17, P3649, DOI 10.1109/TIFS.2022.3212276
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Schneider S., 2020, Advances in Neural Information Processing Systems, V33, P11539
   Shang ZH, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107950
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Shiohara K, 2022, PROC CVPR IEEE, P18699, DOI 10.1109/CVPR52688.2022.01816
   Su LC, 2018, IEEE T MULTIMEDIA, V20, P825, DOI 10.1109/TMM.2017.2760098
   Sun K, 2021, AAAI CONF ARTIF INTE, V35, P2638
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang CR, 2021, PROC CVPR IEEE, P14918, DOI 10.1109/CVPR46437.2021.01468
   Wang S.-Y., 2020, P IEEE CVF C COMP VI, P8692
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu N, 2019, IEEE I CONF COMP VIS, P7555, DOI 10.1109/ICCV.2019.00765
   Yu Y, 2023, IEEE T CIRCUITS SYST
   Yu Y, 2023, IEEE T MULTIMEDIA, V25, P8487, DOI 10.1109/TMM.2023.3237322
   Yu Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499026
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zha R., 2022, Real-centric consistency learning for DeepFake detection
   Zhang BW, 2022, PROC CVPR IEEE, P11294, DOI 10.1109/CVPR52688.2022.01102
   Zhang HT, 2023, IEEE T CIRC SYST VID, V33, P661, DOI 10.1109/TCSVT.2022.3207310
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhang X, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035107
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhao TC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15003, DOI 10.1109/ICCV48922.2021.01475
   Zheng YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15024, DOI 10.1109/ICCV48922.2021.01477
   Zhu Y, 2020, IEEE T IND INFORM, V16, P6714, DOI 10.1109/TII.2020.2982705
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
   Zixin Yin, 2021, ADVM '21: Proceedings of the 1st International Workshop on Adversarial Learning for Multimedia, P21, DOI 10.1145/3475724.3483603
NR 77
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3405
EP 3417
DI 10.1109/TMM.2023.3310341
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200024
DA 2024-08-05
ER

PT J
AU Zhao, X
   Huang, L
   Nie, J
   Wei, ZQ
AF Zhao, Xian
   Huang, Lei
   Nie, Jie
   Wei, Zhiqiang
TI Towards Adaptive Multi-Scale Intermediate Domain via Progressive
   Training for Unsupervised Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Intermediate domain; multi-scale; progressive training; unsupervised
   domain adaptation
ID ALIGNMENT
AB Unsupervised domain adaptation (UDA) involves the transfer of knowledge from a labelled source domain to an unlabelled target domain. Recent studies have introduced the concept of intermediate domains to handle significant domain discrepancies between source and target domains. Constructing an appropriate intermediate domain is a crucial step in handling scenarios with substantial domain differences. In this article, we propose a novel progressive UDA method called adaptive multi-scale intermediate domain via progressive training (AMPT), which has achieved remarkable effectiveness in alleviating large discrepancies between domains. We design a multi-scale similarity metrics module to solve the issue of different scales across different domains by simultaneously computing the pairwise distance between source domain images and the target domain images on multiple scales. Furthermore, we explore a progressive training strategy that facilitates smooth adaptation of the source domain to the target domain by utilizing the intermediate domain. During the progressive training process, considering a positive feedback mechanism, we iteratively leverage task losses and distillation loss through a dynamic threshold. This ensures that the trained intermediate domain branch progressively constrains the target domain branch, and the intermediate domain can be generated dynamically, leading to a smoother gradual adaptation from the source domain to the target domain. Extensive experimental results demonstrate the superiority of our proposed method AMPT on well-known UDA datasets, including Office-Home, Office-31 and DomainNet.
C1 [Zhao, Xian; Huang, Lei; Nie, Jie; Wei, Zhiqiang] Ocean Univ China, Fac Informat Sci & Engn, Qingdao 266100, Peoples R China.
C3 Ocean University of China
RP Huang, L (corresponding author), Ocean Univ China, Fac Informat Sci & Engn, Qingdao 266100, Peoples R China.
EM zhaoxian@stu.ouc.edu.cn; huangl@ouc.edu.cn; niejie@ouc.edu.cn;
   weizhiqiang@ouc.edu.cn
RI wei, zhiqiang/M-8868-2013
OI Huang, Lei/0000-0003-4087-3677
FU National Natural Science Foundation of China
FX No Statement Available
CR Aila T., 2017, P INT C LEARN REPR
   Berthelot D, 2020, 8 INT C LEARNING REP
   Berthelot D, 2019, ADV NEUR IN, V32
   Chen JN, 2022, PROC CVPR IEEE, P12125, DOI 10.1109/CVPR52688.2022.01182
   Chen L., 2022, PROC IEEECVF C COMPU, P7181
   Chen XY, 2019, PR MACH LEARN RES, V97
   Choi J, 2020, AAAI CONF ARTIF INTE, V34, P10655
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Cui Z, 2014, IEEE T CYBERNETICS, V44, P2264, DOI 10.1109/TCYB.2014.2305701
   Dai YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11844, DOI 10.1109/ICCV48922.2021.01165
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P9943, DOI 10.1109/ICCV.2019.01004
   Du ZK, 2021, PROC CVPR IEEE, P3936, DOI 10.1109/CVPR46437.2021.00393
   Fang SC, 2023, IEEE T PATTERN ANAL, V45, P7123, DOI 10.1109/TPAMI.2022.3223908
   Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10
   Gidaris S, 2018, ICLR, DOI DOI 10.1109/ICCV.2019.00156
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258
   Gopalan R, 2014, IEEE T PATTERN ANAL, V36, P2288, DOI 10.1109/TPAMI.2013.249
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kang GL, 2018, LECT NOTES COMPUT SC, V11215, P420, DOI 10.1007/978-3-030-01252-6_25
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li PD, 2022, IEEE T IMAGE PROCESS, V31, P5909, DOI 10.1109/TIP.2022.3203612
   Li S, 2021, PROC CVPR IEEE, P11511, DOI 10.1109/CVPR46437.2021.01135
   Li S, 2020, AAAI CONF ARTIF INTE, V34, P11386
   Liang J., 2020, International Conference on Machine Learning, P6028
   Liang J, 2021, PROC CVPR IEEE, P16627, DOI 10.1109/CVPR46437.2021.01636
   Long M., 2019, PROC NEURIPS, P1951
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long Mingsheng, 2018, ADV NEURAL INFORM PR, V31
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Lu Z., 2022, Adv. Neural Inf. Process. Syst, V35, P14663
   Ma WX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5620, DOI 10.1145/3503161.3548229
   Na J, 2022, LECT NOTES COMPUT SC, V13694, P92, DOI 10.1007/978-3-031-19830-4_6
   Na J, 2021, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR46437.2021.00115
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Sajjadi M, 2016, ADV NEUR IN, V29
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Song SY, 2022, IEEE T MULTIMEDIA, V24, P128, DOI 10.1109/TMM.2020.3046868
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun T, 2022, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR52688.2022.00705
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang WX, 2023, IEEE T IMAGE PROCESS, V32, P72, DOI 10.1109/TIP.2022.3226405
   Wei GQ, 2021, PROC CVPR IEEE, P16638, DOI 10.1109/CVPR46437.2021.01637
   Wu H, 2018, IEEE T IMAGE PROCESS, V27, P1259, DOI 10.1109/TIP.2017.2772836
   Xu MH, 2020, AAAI CONF ARTIF INTE, V34, P6502
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Xu T., 2022, PROC INT C LEARN REP, P1
   Yuan Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P540, DOI 10.1007/978-3-030-58526-6_32
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang YH, 2023, IEEE T MULTIMEDIA, V25, P1749, DOI 10.1109/TMM.2022.3158069
   Zhu JJ, 2023, PROC CVPR IEEE, P3561, DOI 10.1109/CVPR52729.2023.00347
   Zhuo JB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P261, DOI 10.1145/3123266.3123292
NR 62
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5054
EP 5064
DI 10.1109/TMM.2023.3330088
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600044
DA 2024-08-05
ER

PT J
AU Zhou, XF
   Wu, ZC
   Cong, RM
AF Zhou, Xiaofei
   Wu, Zhicong
   Cong, Runmin
TI Decoupling and Integration Network for Camouflaged Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camouflaged object detection; decoupling; integration; feature
   interaction
ID SALIENCY DETECTION
AB Recently, camouflaged object detection (COD), which suffers from numerous challenges such as low contrast between camouflaged objects and background and large variations of camouflaged object appearances, has received more and more concerns. However, the performance of existing camouflaged object detection methods is still unsatisfactory, especially when dealing with complex scenes. Therefore, in this article, we propose a novel Decoupling and Integration Network (DINet) to detect camouflaged objects. Here, the depiction of camouflaged objects can be regarded as the iterative decoupling and integration of the body features and detail features, where the former focuses on the center of camouflaged objects and the latter contains pixels around edges. Concretely, firstly, we deploy two complementary decoder branches including a detail branch and a body branch to learn the decoupling features, namely body decoder features and detail decoder features. Particularly, each decoder block of the two branches incorporates features from three components, i.e., the previous interactive feature fusion (IFF) module, adjacent encoder layers, and corresponding encoder layer. Besides, to further elevate the body decoder features, the body blocks also introduce the global contextual information, which is the combination of all body encoder features via the global context (GC) unit, to provide coarse object location information. Secondly, to integrate the two decoupling decoder features, we deploy the interactive feature fusion (IFF) module based on the interactive combination and channel attention. Following this way, we can progressively provide a complete and accurate representation for camouflaged objects. Extensive experiments on three public challenging datasets, including CAMO, COD10 K, and NC4K, show that our DINet presents competitive performance when compared with the state-of-the-art models.
C1 [Zhou, Xiaofei; Wu, Zhicong] Hangzhou Dianzi Univ, Sch Automat, Hangzhou 310018, Peoples R China.
   [Cong, Runmin] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
C3 Hangzhou Dianzi University; Shandong University
RP Cong, RM (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM zxforchid@outlook.com; cong@hdu.edu.cn; rmcong@sdu.edu.cn
OI Zhou, Xiaofei/0000-0002-7977-9728
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdi A, 2023, IEEE T CYBERNETICS, V53, P4748, DOI 10.1109/TCYB.2022.3140394
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bao YQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3083561
   Bhajantri NU, 2006, ICIT 2006: 9TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY, PROCEEDINGS, P145
   Chen CLZ, 2022, IEEE T CIRC SYST VID, V32, P7662, DOI 10.1109/TCSVT.2022.3185252
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6981, DOI 10.1109/TCSVT.2022.3178173
   Chen TI, 2023, IEEE T MULTIMEDIA, V25, P291, DOI 10.1109/TMM.2021.3125195
   Cheng G, 2019, IEEE T IMAGE PROCESS, V28, P265, DOI 10.1109/TIP.2018.2867198
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2023, IEEE T MULTIMEDIA, V25, P6971, DOI 10.1109/TMM.2022.3216476
   Cong RM, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3196430
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu X., 2023, AAAI, P881
   Huang F, 2017, IEEE T IMAGE PROCESS, V26, P1911, DOI 10.1109/TIP.2017.2669878
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji GP, 2023, MACH INTELL RES, V20, P92, DOI 10.1007/s11633-022-1365-9
   Ji GP, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108414
   Ji Z, 2023, IEEE T MULTIMEDIA, V25, P7699, DOI 10.1109/TMM.2022.3225754
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Kang C, 2015, BEHAV ECOL, V26, P45, DOI 10.1093/beheco/aru150
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Li AX, 2021, PROC CVPR IEEE, P10066, DOI 10.1109/CVPR46437.2021.00994
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu JW, 2022, IEEE WINT CONF APPL, P2613, DOI 10.1109/WACV51458.2022.00267
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1937, DOI 10.1109/TIP.2014.2307434
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Lv YQ, 2021, PROC CVPR IEEE, P11586, DOI 10.1109/CVPR46437.2021.01142
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Neider MB, 2006, VISION RES, V46, P2217, DOI 10.1016/j.visres.2006.01.006
   Pang YW, 2022, PROC CVPR IEEE, P2150, DOI 10.1109/CVPR52688.2022.00220
   Pike TW, 2018, METHODS ECOL EVOL, V9, P1883, DOI 10.1111/2041-210X.13019
   Qian XL, 2023, IEEE T MULTIMEDIA, V25, P1810, DOI 10.1109/TMM.2022.3167805
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren JJ, 2023, IEEE T CIRC SYST VID, V33, P1157, DOI 10.1109/TCSVT.2021.3126591
   Shang-Hua Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P702, DOI 10.1007/978-3-030-58539-6_42
   Shu YC, 2023, IEEE T MULTIMEDIA, V25, P1700, DOI 10.1109/TMM.2022.3154159
   Song Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3513134
   Sun Y., 2021, P INT JOINT C ART IN, P1025
   Sun Y., 2022, P INT JOINT C ART IN, P1335, DOI DOI 10.24963/IJCAI.2022/186
   Talas L, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0351
   Tankus A, 2001, COMPUT VIS IMAGE UND, V82, P208, DOI 10.1006/cviu.2001.0912
   Troscianko J, 2017, BMC EVOL BIOL, V17, DOI 10.1186/s12862-016-0854-2
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Wang K, 2022, IEEE T IND ELECTRON, V69, P5364, DOI 10.1109/TIE.2021.3078379
   Webster RJ, 2015, CURR ZOOL, V61, P708, DOI 10.1093/czoolo/61.4.708
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wen HF, 2021, IEEE T IMAGE PROCESS, V30, P9179, DOI 10.1109/TIP.2021.3123548
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu Z, 2021, IEEE T IMAGE PROCESS, V30, P6226, DOI 10.1109/TIP.2021.3093380
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xu XQ, 2021, IMAGE VISION COMPUT, V114, DOI 10.1016/j.imavis.2021.104283
   Yan JN, 2021, IEEE ACCESS, V9, P43290, DOI 10.1109/ACCESS.2021.3064443
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Yang S., 2020, PROC ASIAN C COMPUT, P1
   Ye F, 2022, IEEE T MULTIMEDIA, V24, P116, DOI 10.1109/TMM.2020.3046884
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhai W, 2023, IEEE T MULTIMEDIA, V25, P5155, DOI 10.1109/TMM.2022.3188401
   Zhang J., 2020, P IEEE CVF C COMP VI, P8582, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang J., 2020, PROC IEEECVF C COMP, P12546
   Zhang LC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P906
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhou Q, 2023, IEEE T MULTIMEDIA, V25, P9254, DOI 10.1109/TMM.2023.3248966
   Zhou T, 2022, IEEE T IMAGE PROCESS, V31, P7036, DOI 10.1109/TIP.2022.3217695
   Zhou XF, 2023, INFORM SCIENCES, V628, P134, DOI 10.1016/j.ins.2023.01.106
   Zhou XF, 2023, IEEE T CYBERNETICS, V53, P539, DOI 10.1109/TCYB.2022.3163152
   Zhou XF, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3132082
   Zhou XF, 2021, IEEE ACCESS, V9, P149465, DOI 10.1109/ACCESS.2021.3124814
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhou XF, 2016, IEEE SIGNAL PROC LET, V23, P517, DOI 10.1109/LSP.2016.2536743
   Zhu HQ, 2023, IEEE T MULTIMEDIA, V25, P5291, DOI 10.1109/TMM.2022.3189778
   Zhu W., PROC IEEE C COMPUT V
   Zhuge MC, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108644
NR 90
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7114
EP 7129
DI 10.1109/TMM.2024.3360710
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000004
DA 2024-08-05
ER

PT J
AU Bao, LP
   Wei, LH
   Zhou, WA
   Liu, L
   Xie, LX
   Li, HQ
   Tian, Q
AF Bao, Liping
   Wei, Longhui
   Zhou, Wengang
   Liu, Lin
   Xie, Lingxi
   Li, Houqiang
   Tian, Qi
TI Multi-Granularity Matching Transformer for Text-Based Person Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Feature extraction; Task analysis; Pedestrians;
   Visualization; Search problems; Training; Text-based person search;
   transformer; vision-language pre-trained model
ID REIDENTIFICATION; ALIGNMENT
AB Text-based person search aims to retrieve the most relevant pedestrian images from an image gallery based on textual descriptions. Most existing methods rely on two separate encoders to extract the image and text features, and then elaborately design various schemes to bridge the gap between image and text modalities. However, the shallow interaction between both modalities in these methods is still insufficient to eliminate the modality gap. To address the above problem, we propose TransTPS, a transformer-based framework that enables deeper interaction between both modalities through the self-attention mechanism in transformer, effectively alleviating the modality gap. In addition, due to the small inter-class variance and large intra-class variance in image modality, we further develop two techniques to overcome these limitations. Specifically, Cross-modal Multi-Granularity Matching (CMGM) is proposed to address the problem caused by small inter-class variance and facilitate distinguishing pedestrians with similar appearance. Besides, Contrastive Loss with Weakly Positive pairs (CLWP) is introduced to mitigate the impact of large intra-class variance and contribute to the retrieval of more target images. Experiments on CUHK-PEDES and RSTPReID datasets demonstrate that our proposed framework achieves state-of-the-art performance compared to previous methods.
C1 [Bao, Liping; Zhou, Wengang; Liu, Lin; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Wei, Longhui] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Xie, Lingxi; Tian, Qi] Huawei Cloud, Shenzhen 518129, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Huawei Technologies
RP Zhou, WA (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.; Tian, Q (corresponding author), Huawei Cloud, Shenzhen 518129, Peoples R China.
EM baoliping@mail.ustc.edu.cn; weilh2568@gmail.com; zhwg@ustc.edu.cn;
   ll08205@mail.ustc.edu.cn; 198808xc@gmail.com; lihq@ustc.edu.cn;
   tian.qi1@huawei.com
OI bao, li ping/0000-0003-2812-688X
FU NSFC
FX No Statement Available
CR Aggarwal S, 2020, IEEE WINT CONF APPL, P2606, DOI [10.1109/wacv45572.2020.9093640, 10.1109/WACV45572.2020.9093640]
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chang XJ, 2018, LECT NOTES COMPUT SC, V11213, P86, DOI 10.1007/978-3-030-01240-3_6
   Chen DP, 2018, LECT NOTES COMPUT SC, V11220, P56, DOI 10.1007/978-3-030-01270-0_4
   Chen T, 2020, PR MACH LEARN RES, V119
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding ZF, 2021, Arxiv, DOI arXiv:2107.12666
   Dosovitskiy A., 2021, PROC ICLR
   Farooq A, 2022, AAAI CONF ARTIF INTE, P4477
   Gao CY, 2021, Arxiv, DOI arXiv:2101.03036
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han X., 2021, PROC BRIT MACH VIS C, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Jeong B., 2021, P IEEECVF INT C COMP, P12016
   Jia C, 2021, PR MACH LEARN RES, V139
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kim W., 2021, PROC INT C MACH LEAR, P5583
   Li H, 2022, IEEE T CIRC SYST VID, V32, P1624, DOI 10.1109/TCSVT.2021.3073718
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Liu JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P665, DOI 10.1145/3343031.3350991
   Liu YH, 2022, IEEE T MULTIMEDIA, V24, P3060, DOI 10.1109/TMM.2021.3092579
   Loshchilov I., 2018, INT C LEARN REPR
   Lu JS, 2019, ADV NEUR IN, V32
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Niu K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4032, DOI 10.1145/3394171.3413895
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Shu Xiujun, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13805), P624, DOI 10.1007/978-3-031-25072-9_42
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Su W., 2019, PROC INT C LEARN REP, P1
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wang Chengji, 2021, P 30 INT JOINT C ART, P1068
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang X, 2023, IEEE T PATTERN ANAL, V45, P5549, DOI 10.1109/TPAMI.2022.3203630
   Wang ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1984, DOI 10.1145/3503161.3548166
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu YS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1604, DOI 10.1109/ICCV48922.2021.00165
   Yang JW, 2022, PROC CVPR IEEE, P19141, DOI 10.1109/CVPR52688.2022.01857
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu-Tong Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P230, DOI 10.1007/978-3-030-58568-6_14
   Zha ZJ, 2020, IEEE T MULTIMEDIA, V22, P1836, DOI 10.1109/TMM.2020.2972168
   Zhang Y., 2018, P EUR C COMP VIS ECC, P686
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3441, DOI 10.1145/3394171.3413864
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhou RW, 2020, IEEE T NEUR NET LEAR, V31, P1592, DOI 10.1109/TNNLS.2019.2920905
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
NR 58
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4281
EP 4293
DI 10.1109/TMM.2023.3321504
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100018
DA 2024-08-05
ER

PT J
AU Bao, ZQ
   Chen, ZH
   Wang, CD
   Zheng, WS
   Huang, ZH
   Chen, YW
AF Bao, Zhiqiang
   Chen, Zihao
   Wang, Chang-Dong
   Zheng, Wei-Shi
   Huang, Zhenhua
   Chen, Yunwen
TI Post-Distillation via Neural Resuscitation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Neurons; Computational modeling; Standards; Optimization; Knowledge
   engineering; Task analysis; Probabilistic logic; Deep learning;
   knowledge distillation; model regularization; transfer learning
ID KNOWLEDGE DISTILLATION
AB Knowledge distillation, a widely adopted model compression technique, distils knowledge from a large teacher model to a smaller student model, with the goal of reducing the computational resources required for the student model. However, most existing distillation approaches focus on the types of knowledge and how to distil them, which neglect the student model's neuronal responses to the knowledge. In this article, we demonstrate that the kullback-leibler loss inhibits the neuronal responses in the opposite gradient direction, which injures the student model's potential during distilling. To address this problem, we introduce a principled dual-stage distillation scheme to rejuvenate all inhibited neurons at the neuronal level. In the first stage, we detect all the neurons in the student model during the standard distillation period and divide them into two parts according to their responses. In the second stage, we propose three strategies to resuscitate the neurons differently, which allows us to exploit the full potential of the student model. Through the experiments in various aspects of knowledge distillation, it is verified that the proposed approach outperforms the current state-of-the-art approaches. Our work provides a neuronal perspective for studying the response of the student model to the knowledge from the teacher model.
C1 [Bao, Zhiqiang; Chen, Zihao; Huang, Zhenhua] South China Normal Univ, Sch Comp Sci, Guangzhou 510631, Peoples R China.
   [Wang, Chang-Dong; Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510000, Peoples R China.
   [Chen, Yunwen] DataGrand Inc, Res & Dev Dept, Shanghai 201203, Peoples R China.
C3 South China Normal University; Sun Yat Sen University
RP Huang, ZH (corresponding author), South China Normal Univ, Sch Comp Sci, Guangzhou 510631, Peoples R China.
EM ZhiqiangBAO1995@163.com; jasonczh4@gmail.com; changdongwang@hotmail.com;
   wszheng@ieee.org; jukiehuang@163.com; chenyunwen@datagrand.com
OI Huang, Zhenhua/0000-0001-8659-4062
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bakker B, 2004, J MACH LEARN RES, V4, P83, DOI 10.1162/153244304322765658
   Bhatt A., 2015, INT J COMPUTER SCI I, V6, P5107
   Chandrasegaran K., 2022, International Conference on Machine Learning, P2890
   Chen DF, 2022, PROC CVPR IEEE, P11923, DOI 10.1109/CVPR52688.2022.01163
   Choi H, 2023, IEEE WINT CONF APPL, P2318, DOI 10.1109/WACV56688.2023.00235
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XH, 2019, PROC CVPR IEEE, P4938, DOI 10.1109/CVPR.2019.00508
   Du SS, 2019, 36 INT C MACHINE LEA, V97
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Ghojogh B, 2019, Arxiv, DOI arXiv:1905.12787
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Guo J, 2021, Arxiv, DOI arXiv:2010.07485
   Guo ZY, 2023, PROC CVPR IEEE, P11868, DOI 10.1109/CVPR52729.2023.01142
   Han S, 2015, ADV NEUR IN, V28
   Hao ZW, 2022, IEEE T MULTIMEDIA, V24, P4262, DOI 10.1109/TMM.2022.3192663
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2020, IEEE T CYBERNETICS, V50, P3594, DOI 10.1109/TCYB.2019.2933477
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Heo B, 2019, IEEE I CONF COMP VIS, P1921, DOI 10.1109/ICCV.2019.00201
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu HY, 2016, Arxiv, DOI [arXiv:1607.03250, DOI 10.48550/ARXIV.1607.03250]
   Huang ZH, 2017, Arxiv, DOI arXiv:1707.01219
   Huang Z, 2021, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR46437.2021.00358
   Huang ZH, 2022, IEEE T IMAGE PROCESS, V31, P1364, DOI 10.1109/TIP.2022.3141255
   Jin Y, 2023, PROC CVPR IEEE, P24276, DOI 10.1109/CVPR52729.2023.02325
   Joo D, 2022, IEEE COMPUT SOC CONF, P2771, DOI 10.1109/CVPRW56347.2022.00313
   Kingma D. P., 2014, arXiv
   Kornblith S, 2019, PR MACH LEARN RES, V97
   Krizhevsky A, 2009, CIFAR-10 dataset
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee K., 2022, P INT C ART INT IJCA, P3178
   Li G, 2022, AAAI CONF ARTIF INTE, P1306
   Li H, 2017, Arxiv, DOI arXiv:1608.08710
   Li YC, 2019, PROC CVPR IEEE, P2795, DOI 10.1109/CVPR.2019.00291
   Lin SH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2425
   Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290
   Lin SH, 2022, PROC CVPR IEEE, P10905, DOI 10.1109/CVPR52688.2022.01064
   Liu L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8251, DOI 10.1109/ICCV48922.2021.00816
   Liu Y., 2021, arXiv
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Mairal J, 2012, Arxiv, DOI arXiv:1205.0079
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Narkhede MV, 2022, ARTIF INTELL REV, V55, P291, DOI 10.1007/s10462-021-10033-z
   Park Dae Young, 2021, ADV NEURAL INFORM PR, V34, P5
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Prakash A, 2019, PROC CVPR IEEE, P10658, DOI 10.1109/CVPR.2019.01092
   Qiao SY, 2019, PROC CVPR IEEE, P61, DOI 10.1109/CVPR.2019.00015
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Rosenstein M. T., 2005, P NIPS WORKSH TRANS, P1
   Ruder S, 2017, Arxiv, DOI arXiv:1609.04747
   Sharma A, 2018, APPL SOFT COMPUT, V73, P1068, DOI 10.1016/j.asoc.2018.09.038
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song GC, 2018, ADV NEUR IN, V31
   Su TT, 2021, IEEE DATA MINING, P579, DOI 10.1109/ICDM51629.2021.00069
   Tang JX, 2021, Arxiv, DOI arXiv:2002.03532
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Tian Y., 2020, Contrastive representation distillation
   Toet Alexander, 2014, Figshare
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Wang ZR, 2019, PROC CVPR IEEE, P11285, DOI 10.1109/CVPR.2019.01155
   Wu AC, 2019, PROC CVPR IEEE, P1187, DOI 10.1109/CVPR.2019.00128
   Yang CG, 2022, PROC CVPR IEEE, P12309, DOI 10.1109/CVPR52688.2022.01200
   Yang ZD, 2022, PROC CVPR IEEE, P4633, DOI 10.1109/CVPR52688.2022.00460
   Ye Jianbo, 2018, Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958
   Yuan L, 2020, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR42600.2020.00396
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zagoruyko S, 2017, Arxiv, DOI [arXiv:1612.03928, DOI 10.48550/ARXIV.1612.03928]
   Zhang LB, 2021, IEEE T MULTIMEDIA, V23, P4158, DOI 10.1109/TMM.2020.3037502
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao BR, 2022, PROC CVPR IEEE, P11943, DOI 10.1109/CVPR52688.2022.01165
   Zhao ZX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P970
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
   Zhu Y., 2022, Advances in Neural Information Processing Systems, P32011
NR 86
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3046
EP 3060
DI 10.1109/TMM.2023.3306601
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700008
DA 2024-08-05
ER

PT J
AU Dai, Y
   Chen, XJ
   Wang, XH
   Pang, MH
   Gao, LL
   Shen, HT
AF Dai, Yan
   Chen, Xiaojia
   Wang, Xuanhan
   Pang, Minghui
   Gao, Lianli
   Shen, Heng Tao
TI ReSParser: Fully Convolutional Multiple Human Parsing With
   Representative Sets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pipelines; Task analysis; Kernel; Semantics; Pose estimation; Detectors;
   Crops; Multiple human parsing; human instance-level analysis;
   instance-aware modeling
AB Multiple human parsing (MHP) is typically treated as two sub-tasks, i.e., instance separation and body part segmentation. Existing methods usually tackle the sub-tasks by adopting a two-stage strategy, which regards MHP as an ROI-based (i.e., detect-then-segment) or grouping-based (i.e., segment-then-grouping) paradigm. However, the strong dependence between the two sub-tasks limits the potential of an MHP method, since it often requires qualified prior predictions. Besides, isolated models responsible for the two sub-tasks bring a significant computational burden. Unlike existing methods, we regard MHP as a hierarchical set prediction problem and handle two sub-tasks using several landmarks of body parts. Motivated by this, we propose a novel multiple human parser with representative sets, termed ReSParser. In ReSParser, several landmarks of body parts are hierarchically estimated, resulting in coarse-to-fine representative sets. After that, each representative set is adaptively responsible for segmenting pixels into semantically consistent regions belonging to the corresponding person. In such a manner, the ReSParser simultaneously addresses two sub-tasks in a fully convolutional fashion, thus eliminating the dependence between two sub-tasks and significantly alleviating computational complexity. Extensive experiments on two challenging benchmarks demonstrate that our proposed ReSParser is an efficient framework with a superior parsing performance, which significantly outperforms that of other ROI-free yet grouping-free methods. Besides, it achieves competitive results to that of the best two-stage methods such as RP-RCNN, but requires a much lower inference time, showing a good precision-speed trade-off. We hope the ReSParser serves as a new baseline for multiple human parsing research in the future.
C1 [Dai, Yan; Chen, Xiaojia; Wang, Xuanhan; Gao, Lianli; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.
   [Dai, Yan; Chen, Xiaojia; Wang, Xuanhan; Gao, Lianli; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Pang, Minghui] Univ Elect Sci & Technol China, Sch Med, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; University of Electronic
   Science & Technology of China
RP Gao, LL (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.; Gao, LL (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM yandai1019@gmail.com; josonchan1998@gmail.com; wxuanhan@hotmail.com;
   mhpang@uestc.edu.cn; lianli.gao@uestc.edu.cn; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021
OI Pang, Minghui/0009-0003-9705-8443; Dai, Yan/0000-0002-5256-7135
FU National Key Ramp;D Program of China
FX No Statement Available
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gong K, 2019, PROC CVPR IEEE, P7442, DOI [10.1109/CVPR.2019.00763, 10.1109/cvpr.2019.00763]
   He HY, 2020, AAAI CONF ARTIF INTE, V34, P10949
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li JS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P45, DOI 10.1145/3240508.3240515
   Li YW, 2021, PROC CVPR IEEE, P214, DOI 10.1109/CVPR46437.2021.00028
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu XC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P338, DOI 10.1145/3343031.3350857
   Lu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P421, DOI 10.1007/978-3-030-58610-2_25
   Mao WA, 2021, PROC CVPR IEEE, P9030, DOI 10.1109/CVPR46437.2021.00892
   Newell Z., 2017, Adv. Neural Inf. Pro-cess. Syst.
   Qin H., 2019, P BMVC
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Ruyi Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P205, DOI 10.1007/978-3-030-58601-0_13
   Shi DH, 2022, PROC CVPR IEEE, P11059, DOI 10.1109/CVPR52688.2022.01079
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11159, DOI 10.1109/ICCV48922.2021.01099
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang XH, 2023, IEEE T MULTIMEDIA, V25, P979, DOI 10.1109/TMM.2021.3135145
   Wang XH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P686, DOI 10.1145/3474085.3475233
   Wang XH, 2022, IEEE T CIRC SYST VID, V32, P7732, DOI 10.1109/TCSVT.2022.3181604
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Yang L, 2019, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.2019.00045
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yu XD, 2022, AAAI CONF ARTIF INTE, P3188
   Yun XL, 2022, IEEE T MULTIMEDIA, V24, P2580, DOI 10.1109/TMM.2021.3087000
   Zhang SY, 2022, IEEE T IMAGE PROCESS, V31, P5599, DOI 10.1109/TIP.2022.3192989
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhou TF, 2021, PROC CVPR IEEE, P1622, DOI 10.1109/CVPR46437.2021.00167
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Ziwei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8897, DOI 10.1109/CVPR42600.2020.00892
NR 40
TC 2
Z9 2
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1384
EP 1394
DI 10.1109/TMM.2023.3281070
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700004
DA 2024-08-05
ER

PT J
AU Ding, HX
   Zhang, SC
   Wu, Q
   Yu, SL
   Hu, J
   Cao, LJ
   Ji, RR
AF Ding, Haixin
   Zhang, Shengchuan
   Wu, Qiong
   Yu, Songlin
   Hu, Jie
   Cao, Liujuan
   Ji, Rongrong
TI Bilateral Knowledge Interaction Network for Referring Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Visualization; Kernel; Knowledge engineering;
   Feature extraction; Semantics; Convolution; Referring image
   segmentation; vision-language
ID AGGREGATION
AB Referring image segmentation aims to segment objects that are described by natural language expressions. Although remarkable advancements have been made to align natural language expressions with visual representations for better performance, the interaction between image-level and text-level information is still not formulated properly. Most of the previous works focus on building correlations between vision and language, ignoring the variety of objects. The target objects with unique appearances may not be correctly located or completely segmented. In this article, we propose a novel Bilateral Knowledge Interaction Network, termed BKINet, which reformulates the image-text interaction in a bilateral manner to adapt concrete knowledge of the target object in the image. BKINet contains two key components: a knowledge learning module (KLM) and a knowledge applying module (KAM). In the KLM, the abstract knowledge from text features is replenished with concrete knowledge from visual features to adapt to the target objects in the input images, which generates the knowledge interaction kernels (KI kernels) containing abundant referring information. With the referring information of KI kernels, the KAM is designed to highlight the most relevant visual features for predicting the accurate segmentation mask. Extensive experiments on three widely-used datasets, i.e. RefCOCO, RefCOCO+, and G-ref, demonstrate the superiority of BKINet over the state-of-the-art.
C1 [Ding, Haixin; Zhang, Shengchuan; Wu, Qiong; Yu, Songlin; Hu, Jie; Cao, Liujuan; Ji, Rongrong] Xiamen Univ, Key Lab Multimedia Trusted Percept & Efficient Com, Minist Educ China, Xiamen 361005, Peoples R China.
C3 Xiamen University
RP Zhang, SC (corresponding author), Xiamen Univ, Key Lab Multimedia Trusted Percept & Efficient Com, Minist Educ China, Xiamen 361005, Peoples R China.
EM dinghx@stu.xmu.edu.cn; zsc_2016@xmu.edu.cn; qiong@stu.xmu.edu.cn;
   31520211154004@stu.xmu.edu.cn; hujie.cpp@gmail.com;
   caoliujuan@xmu.edu.cn; rrji@xmu.edu.cn
OI Cao, Liujuan/0000-0002-7645-9606
FU National Key Research and Development Program of China
FX No Statement Available
CR Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen DJ, 2019, IEEE I CONF COMP VIS, P7453, DOI 10.1109/ICCV.2019.00755
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Ding HH, 2023, IEEE T PATTERN ANAL, V45, P7900, DOI 10.1109/TPAMI.2022.3217852
   Ding HH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16301, DOI 10.1109/ICCV48922.2021.01601
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Feng G, 2021, PROC CVPR IEEE, P15501, DOI 10.1109/CVPR46437.2021.01525
   Feng G, 2020, NEUROCOMPUTING, V403, P33, DOI 10.1016/j.neucom.2020.04.032
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Hu ZW, 2020, PROC CVPR IEEE, P4423, DOI 10.1109/CVPR42600.2020.00448
   Huang S., 2020, P IEEE CVF C COMP VI, P10485, DOI DOI 10.1109/CVPR42600.2020.01050
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Hui Tianrui, 2020, COMPUTER VISION ECCV, DOI DOI 10.1007/978-3-030-58607-2_4
   Jiao Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1331, DOI 10.1145/3474085.3475222
   Jing Y, 2021, PROC CVPR IEEE, P9853, DOI 10.1109/CVPR46437.2021.00973
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Ke L, 2021, PROC CVPR IEEE, P4018, DOI 10.1109/CVPR46437.2021.00401
   Kim N, 2022, PROC CVPR IEEE, P18124, DOI 10.1109/CVPR52688.2022.01761
   Kingma D. P., 2014, arXiv
   Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2023, IEEE T MULTIMEDIA, V25, P3657, DOI 10.1109/TMM.2022.3163578
   Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1274, DOI 10.1145/3394171.3414006
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904
   Newell A, 2017, Arxiv, DOI arXiv:1611.05424
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Radford A., 2019, OpenAI blog, V1, P9
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O., 2017, INF AKTUELL BILDVERA, P3, DOI [10.1007/978-3-662-54345-0_3, DOI 10.1007/978-3-662-54345-0_3, DOI 10.1007/978-3-662-54345-03]
   Ronneberger O, 2015, Arxiv, DOI arXiv:1505.04597
   Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani A, 2023, Arxiv, DOI arXiv:1706.03762
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Wang ZQ, 2022, PROC CVPR IEEE, P11676, DOI 10.1109/CVPR52688.2022.01139
   Wu JL, 2018, LECT NOTES COMPUT SC, V11214, P188, DOI 10.1007/978-3-030-01249-6_12
   Yang Z., 2021, arXiv
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang F, 2019, IEEE I CONF COMP VIS, P6797, DOI 10.1109/ICCV.2019.00690
   Zhang WW, 2021, ADV NEUR IN, V34
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 69
TC 2
Z9 2
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2966
EP 2977
DI 10.1109/TMM.2023.3305869
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700002
DA 2024-08-05
ER

PT J
AU Hu, Y
   Fang, XZ
   Kang, PP
   Chen, YH
   Fang, YT
   Xie, SL
AF Hu, Yan
   Fang, Xiaozhao
   Kang, Peipei
   Chen, Yonghao
   Fang, Yuting
   Xie, Shengli
TI Dual Noise Elimination and Dynamic Label Correlation Guided Partial
   Multi-Label Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Matrix decomposition; Training; Phase locked loops; Laplace
   equations; Task analysis; Sun; Partial multi-label learning; noise
   elimination; label correlations; dynamic Laplacian matrix
ID CLASSIFICATION; ANNOTATION
AB Partial multi-label learning (PML) needs to address the problem of multi-label learning when the dataset contains redundant information. PML is more challenging compared to traditional multi-label learning, because PML needs not only to perform the multi classification task, but also to reduce the impact of noise information on the model. Existing PML methods suffer from the following problems. (1) Only single source of noise is considered. (2) Some methods ignore the label correlations. To solve the above problems, we proposes a new dual noise elimination and dynamic label correlation guided partial multi-label learning (PML-DNDC). Specifically, the hidden ground-truth label matrix is decomposed into two compressed matrices of instance and classifier, which are used to approximate the candidate label matrix, to eliminate the negative effects of label noise on the model. On one hand, the compressed instance matrix maintains local structural consistency with the original instances, eliminating noise in the feature. On the other hand, dynamic label correlation guidance is designed to help classifier training by dynamically exploring the potential label correlations, which encourages relevant labels to obtain similar classifiers. After extensive experiments and analyses, we conclude that the proposed PML-DNDC is superior to the state-of-the-art methods.
C1 [Hu, Yan; Kang, Peipei; Chen, Yonghao] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Hu, Yan; Xie, Shengli] Minist Educ, Key Lab Intelligent Informat Proc & Syst Integrat, Guangzhou 510006, Peoples R China.
   [Fang, Xiaozhao; Xie, Shengli] Guangdong Univ Technol, Sch Automat, Guangzhou 510006, Peoples R China.
   [Fang, Xiaozhao] Guangdong Key Lab IoT Informat Technol, Guangzhou 510006, Peoples R China.
   [Fang, Yuting] Xian Jiaotong Liverpool Univ, Sch Math & Phys, Suzhou 215028, Peoples R China.
C3 Guangdong University of Technology; Guangdong University of Technology;
   Xi'an Jiaotong-Liverpool University
RP Kang, PP (corresponding author), Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
EM huyan515601@163.com; xzhfang168@126.com; ppkanggdut@126.com;
   yhchengdut@126.com; yuting.fang21@student.xjtlu.edu.cn;
   shlxie@gdut.edu.cn
OI Chen, Yonghao/0009-0000-5744-5930
FU National Natural Science Foundation of China
FX No Statement Available
CR BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Briggs F., 2013, IEEE INT WORKSHOPMAC, P1
   Clare A., 2001, Lecture Notes in Computer Science, P42
   Cour T, 2011, J MACH LEARN RES, V12, P1501
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Diplaris S, 2005, LECT NOTES COMPUT SC, V3746, P448
   Elisseeff A, 2002, ADV NEUR IN, V14, P681
   Feng L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2294
   Fürnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   Gao W, 2013, ARTIF INTELL, V199, P22, DOI 10.1016/j.artint.2013.03.001
   GOLUB GH, 1979, IEEE T AUTOMAT CONTR, V24, P909, DOI 10.1109/TAC.1979.1102170
   Grandvalet Y., 2004, Learning from partial labels with minimumentropy
   Hüllermeier E, 2006, INTELL DATA ANAL, V10, P419, DOI 10.3233/IDA-2006-10503
   Huiskes M.J., 2008, P ACM INT C MULT INF, P39, DOI 10.1145/1460096
   Jian L., 2016, IJCAI, P1627, DOI DOI 10.5555/3060832.3060848
   Katakis I., 2008, ECML PKDD Discovery Challenge, V18, P5
   Konstantinides K, 1997, IEEE T IMAGE PROCESS, V6, P479, DOI 10.1109/83.557359
   Liu T., 2012, Advances in Neural Information Processing Systems, P557
   Liu WW, 2022, IEEE T PATTERN ANAL, V44, P7955, DOI 10.1109/TPAMI.2021.3119334
   Lyu GY, 2021, INFORM SCIENCES, V543, P454, DOI 10.1016/j.ins.2020.09.019
   Lyu GY, 2021, IEEE T KNOWL DATA EN, V33, P521, DOI 10.1109/TKDE.2019.2933837
   Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1
   Mohri A., 2018, Foundations of MachineLearning
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Rong Y, 2016, IEEE IMAGE PROC, P719, DOI 10.1109/ICIP.2016.7532451
   Snoek M., 2006, P 14 ACM INT C MULT, P421, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   Srivastava AN, 2005, AEROSP CONF PROC, P3853
   Sun LJ, 2022, IEEE T MULTIMEDIA, V24, P581, DOI 10.1109/TMM.2021.3055959
   Sun LJ, 2019, AAAI CONF ARTIF INTE, P5016
   Trohidis K, 2011, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2011-426793
   Turnbull D, 2008, IEEE T AUDIO SPEECH, V16, P467, DOI 10.1109/TASL.2007.913750
   Wang H, 2011, PROC CVPR IEEE, P793, DOI 10.1109/CVPR.2011.5995379
   Xie MK, 2023, IEEE T PATTERN ANAL, V45, P154, DOI 10.1109/TPAMI.2022.3141240
   Xie MK, 2020, AAAI CONF ARTIF INTE, V34, P6454
   Xie MK, 2022, IEEE T PATTERN ANAL, V44, P3676, DOI 10.1109/TPAMI.2021.3059290
   Xie MK, 2018, AAAI CONF ARTIF INTE, P4302
   Yu GX, 2018, IEEE DATA MINING, P1398, DOI 10.1109/ICDM.2018.00192
   Yu M.-L., 2016, P ASIAN C MACHINE LE, P96
   Yu TT, 2020, IEEE DATA MINING, P761, DOI 10.1109/ICDM50108.2020.00085
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2021, IEEE T PATTERN ANAL, V43, P3587, DOI 10.1109/TPAMI.2020.2985210
   Zhang ML, 2022, IEEE T CYBERNETICS, V52, P4459, DOI 10.1109/TCYB.2020.3027509
   Zhang ML, 2018, FRONT COMPUT SCI-CHI, V12, P191, DOI 10.1007/s11704-017-7031-7
   Zhang ML, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4048
   Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhao P, 2022, KNOWL-BASED SYST, V245, DOI 10.1016/j.knosys.2022.108601
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
NR 50
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5641
EP 5656
DI 10.1109/TMM.2023.3338080
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600015
DA 2024-08-05
ER

PT J
AU Lin, ZH
   Yan, QS
   Liu, WM
   Wang, SP
   Wang, MH
   Tan, YC
   Yang, CR
AF Lin, Zhenghong
   Yan, Qishan
   Liu, Weiming
   Wang, Shiping
   Wang, Menghan
   Tan, Yanchao
   Yang, Carl
TI Automatic Hypergraph Generation for Enhancing Recommendation With Sparse
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Recommender systems; hypergraph generation; sparse optimization; graph
   convolutional network
AB With the rapid growth of activities on the web, large amounts of interaction data on multimedia platforms are easily accessible, including e-commerce, music sharing, and social media. By discovering various interests of users, recommender systems can improve user satisfaction without accessing overwhelming personal information. Compared to graph-based models, hypergraph-based collaborative filtering has the ability to model higher-order relations besides pair-wise relations among users and items, where the hypergraph structures are mainly obtained from specialized data or external knowledge. However, the above well-constructed hypergraph structures are often not readily available in every situation. To this end, we first propose a novel framework named HGRec, which can enhance recommendation via automatic hypergraph generation. By exploiting the clustering mechanism based on the user/item similarity, we group users and items without additional knowledge for hypergraph structure learning and design a cross-view recommendation module to alleviate the combinatorial gaps between the representations of the local ordinary graph and the global hypergraph. Furthermore, we devise a sparse optimization strategy to ensure the effectiveness of hypergraph structures, where a novel integration of the l( 2,1)-norm and optimal transport framework is designed for hypergraph generation. We term the model HGRec with sparse optimization strategy as HGRec++. Extensive experiments on public multi-domain datasets demonstrate the superiority brought by our HGRec++, which gains average 8.1% and 9.8% improvement over state-of-the-art baselines regarding Recall and NDCG metrics, respectively.
C1 [Lin, Zhenghong; Wang, Shiping; Tan, Yanchao] Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350116, Peoples R China.
   [Yan, Qishan] Fuzhou Univ, Coll Maynooth Int Engn, Fuzhou 350116, Peoples R China.
   [Liu, Weiming] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Wang, Menghan] eBay Inc, Shanghai 201203, Peoples R China.
   [Yang, Carl] Emory Univ, Dept Comp Sci, Atlanta, GA 30322 USA.
C3 Fuzhou University; Fuzhou University; Zhejiang University; eBay Inc.;
   Emory University
RP Tan, YC (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350116, Peoples R China.
EM hongzhenglin970323@gmail.com; viztiny@gmail.com; 21831010@zju.edu.cn;
   shipingwangphd@163.com; wangmengh@zju.edu.cn; yctan@fzu.edu.cn;
   j.carlyang@emory.edu
OI Yan, Qishan/0009-0006-1706-8409; Liu, Weiming/0000-0002-4115-7667; Wang,
   Menghan/0000-0003-0682-5866; Tan, Yanchao/0000-0002-3526-6859
FU National Natural Science Foundation of China
FX No Statement Available
CR Bahmani S, 2013, J MACH LEARN RES, V14, P807
   Bao SL, 2023, IEEE T PATTERN ANAL, V45, P1017, DOI 10.1109/TPAMI.2022.3141095
   Barratt S., 2018, arXiv
   Cai Derun, 2022, P 31 INT JOINT C ART, P1923, DOI DOI 10.24963/IJCAI.2022/267
   Chapel L, 2021, ADV NEUR IN
   Chen C, 2020, ACM T INFORM SYST, V38, DOI 10.1145/3373807
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P27
   Chen XS, 2022, IEEE T MULTIMEDIA, V24, P506, DOI 10.1109/TMM.2021.3054525
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P4054, DOI 10.1109/TMM.2021.3112230
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Chou MC, 2008, FLEX SERV MANUF J, V20, P59, DOI 10.1007/s10696-008-9053-9
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Cuturi M., 2013, P NIPS, P2292
   Dong W, 2022, PROC CVPR IEEE, P16599, DOI 10.1109/CVPR52688.2022.01612
   Dyer C, 2014, Arxiv, DOI arXiv:1410.8251
   Fan WQ, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P112, DOI 10.1145/3477495.3531985
   Feng YF, 2019, AAAI CONF ARTIF INTE, P3558
   Frank M, 1956, Naval Res. Logist. Q., V3, P95, DOI DOI 10.1002/NAV.3800030109
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Georgogiannis A., 2016, PROC INT C NEURAL IN, P2891
   Gui J, 2017, IEEE T NEUR NET LEAR, V28, P1490, DOI 10.1109/TNNLS.2016.2551724
   Guo ZC, 2023, IEEE T MULTIMEDIA, V25, P38, DOI 10.1109/TMM.2021.3120544
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P9595, DOI 10.1109/TPAMI.2023.3247939
   Hwang TH, 2008, IEEE DATA MINING, P293, DOI 10.1109/ICDM.2008.37
   Ida Y., 2023, PROC 37 AAAI C ARTIF, P7980
   Ji SY, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2020, DOI 10.1145/3394486.3403253
   Krantz S. G., 2002, IMPLICIT FUNCTION TH
   Li R, 2021, NEUROCOMPUTING, V425, P71, DOI 10.1016/j.neucom.2020.02.051
   Li XC, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P582, DOI 10.1145/3442381.3449869
   Li YF, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P1997, DOI 10.1145/3477495.3531794
   Lin Z., 2021, INT JOINT C ART INT, P2723
   Lin ZH, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P2320, DOI 10.1145/3485447.3512104
   Liu HD, 2023, IEEE T CYBERNETICS, V53, P352, DOI 10.1109/TCYB.2021.3100521
   Liu SJ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3203481
   Luo DX, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P6121
   Ma J, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P1202, DOI 10.1145/3534678.3539299
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Nie H., 2010, NIPS, P1813
   Pei SF, 2023, IEEE T PATTERN ANAL, V45, P167, DOI 10.1109/TPAMI.2022.3150981
   Rout L., 2022, PROC INT C LEARN REP, P1
   Su YX, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P849, DOI 10.1145/3404835.3462833
   Tan YC, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P3970, DOI 10.1145/3534678.3539089
   Tang H, 2023, IEEE T MULTIMEDIA, V25, P339, DOI 10.1109/TMM.2021.3126146
   Chaves PDV, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P2442, DOI 10.1145/3485447.3512116
   Wang JL, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1101, DOI 10.1145/3397271.3401133
   Wang XD, 2019, SIGBIOMED WORKSHOP ON BIOMEDICAL NATURAL LANGUAGE PROCESSING (BIONLP 2019), P165, DOI 10.1145/3331184.3331267
   Wang Y., 2021, IJCAI, P3134, DOI 10.24963/ijcai.2021/431
   Wang YM, 2023, IEEE T MULTIMEDIA, V25, P1008, DOI 10.1109/TMM.2021.3136098
   Wei CY, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P2108, DOI 10.1145/3511808.3557301
   Wu C., 2021, P 30 INT JOINT C ART, P1624, DOI DOI 10.24963/IJCAI.2021/224
   Wu JC, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P726, DOI 10.1145/3404835.3462862
   Xia LH, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P70, DOI 10.1145/3477495.3532058
   Yang Y., 2011, PROC INT JOINT C ART, P1589
   Yao J, 2022, INFORM SCIENCES, V612, P563, DOI 10.1016/j.ins.2022.08.117
   Yao TS, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P4321, DOI 10.1145/3459637.3481952
   Yi J, 2023, IEEE T MULTIMEDIA, V25, P515, DOI 10.1109/TMM.2021.3128254
   Yu JL, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P1294, DOI 10.1145/3477495.3531937
   Yu JL, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P413, DOI 10.1145/3442381.3449844
NR 61
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5680
EP 5693
DI 10.1109/TMM.2023.3338083
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100013
DA 2024-08-05
ER

PT J
AU Liu, YB
   Wang, JH
   Zhong, SH
   Ma, LY
   Xu, Y
AF Liu, Yabo
   Wang, Jinghua
   Zhong, Shenghua
   Ma, Lianyang
   Xu, Yong
TI Fine-Grained Representation Alignment for Zero-Shot Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain adaptation; transfer learning; zero-shot learning
AB Most existing domain adaptation methods learn with both (labeled) samples in the source domain and (unlabeled) samples in the target domain. Relying on the availability of target domain samples, however, is not always feasible in real-world applications. In this article, we propose a new method to address this issue, in which the target domain samples do not need to be available for the task of interest. To improve the performance of such a zero-shot domain adaptation (ZSDA), we learn with not only source samples in the task of interest, but also seek additional assistance from those dual-domain samples in an irrelevant task. To overcome the problems induced by the unavailability of target samples in the task of interest, we exploit the hypothesis that the domain correlation is consistent across tasks and learn to transfer it from the irrelevant task to the task of interest. Specifically, our method aims to learn a domain-invariant representation space in which the source-domain classifier is directly transferable to the target domain. We achieve this by restricting the two domains to share both inter-category structure and intra-category structure in the representation space. Experiment results on five benchmarking datasets indicate that our proposed method significantly outperforms the existing representative baselines.
C1 [Liu, Yabo; Xu, Yong] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Liu, Yabo; Xu, Yong] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Wang, Jinghua] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Zhong, Shenghua] Shenzhen Univ, Shenzhen 518060, Peoples R China.
   [Ma, Lianyang] Tencent Technol, Shenzhen 518057, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Harbin Institute
   of Technology; Shenzhen University; Tencent
RP Xu, Y (corresponding author), Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.; Wang, JH (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM yaboliu.ug@gmail.com; wangjh2012@foxmail.com; csshzhong@szu.edu.cn;
   mlysjtu@foxmail.com; yongxu@ymail.com
OI Wang, Jinghua/0000-0002-2629-1198
FU Natural Science Foundation China (NSFC)
FX No Statement Available
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Arjovsky M., 2017, INT C LEARNING REPRE
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Arora S, 2017, PR MACH LEARN RES, V70
   Ben-David Shai, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P139, DOI 10.1007/978-3-642-34106-9_14
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chattopadhyay Prithvijit, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P301, DOI 10.1007/978-3-030-58545-7_18
   Che T., 2017, PROC INT C LEARN REP
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen TI, 2023, IEEE T MULTIMEDIA, V25, P291, DOI 10.1109/TMM.2021.3125195
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P1558, DOI 10.1109/TMM.2021.3067439
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   Cortes C, 2019, J MACH LEARN RES, V20
   D'Innocente Antonio, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P187, DOI 10.1007/978-3-030-12939-2_14
   Dhouib S, 2020, PR MACH LEARN RES, V119
   Dou Q., 2019, ADV NEUR IN, P579
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Germain P., 2016, INT C MACHINE LEARNI, P859
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Grother PatrickJ., 2008, NIST special database 19, NIST Handprinted Forms and Characters Database
   Gu ZX, 2021, IEEE T MULTIMEDIA, V23, P3738, DOI 10.1109/TMM.2020.3035231
   Haifeng Xia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P55, DOI 10.1007/978-3-030-58583-9_4
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hu LQ, 2020, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR42600.2020.00410
   Jamal M. A., 2020, P IEEE CVF C COMP VI, P7610, DOI DOI 10.1109/CVPR42600.2020.00763
   Jianfei Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P589, DOI 10.1007/978-3-030-58586-0_35
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Jinghua Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P329, DOI 10.1007/978-3-030-58589-1_20
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Liu MY, 2017, ADV NEUR IN, V30
   Liu YJ, 2023, IEEE T MULTIMEDIA, V25, P126, DOI 10.1109/TMM.2021.3121564
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lv FM, 2020, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR42600.2020.00439
   Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749
   Mengxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13933, DOI 10.1109/CVPR42600.2020.01395
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Muandet K., 2013, INT C MACHINE LEARNI, P10
   Nguyen C. V., 2020, PROC 37 INT C MACH L
   Odena A, 2017, PR MACH LEARN RES, V70
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Peng KC, 2018, LECT NOTES COMPUT SC, V11215, P793, DOI 10.1007/978-3-030-01252-6_47
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887
   Seonguk Seo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P68, DOI 10.1007/978-3-030-58542-6_5
   Shujun Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P159, DOI 10.1007/978-3-030-58545-7_10
   Szegedy C., 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594]
   Taekyung Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P591, DOI 10.1007/978-3-030-58568-6_35
   Tuzel, 2016, Advances in Neural Information Processing Systems, P469
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Volpi R, 2018, ADV NEUR IN, V31
   Wang JH, 2022, IEEE T PATTERN ANAL, V44, P6264, DOI 10.1109/TPAMI.2021.3088859
   Wang JH, 2021, IEEE T IMAGE PROCESS, V30, P5505, DOI 10.1109/TIP.2021.3084354
   Wang JH, 2019, IEEE I CONF COMP VIS, P3374, DOI 10.1109/ICCV.2019.00347
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Xingchao Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P756, DOI 10.1007/978-3-030-58539-6_45
   Xinpeng Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P498, DOI 10.1007/978-3-030-58565-5_30
   Xiufeng Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9570, DOI 10.1109/CVPR42600.2020.00959
   Xu RJ, 2020, PROC CVPR IEEE, P4393, DOI 10.1109/CVPR42600.2020.00445
   Yabin Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P781, DOI 10.1007/978-3-030-58548-8_45
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1640, DOI 10.1109/TMM.2020.3001527
   Yin CX, 2022, IEEE T MULTIMEDIA, V24, P4183, DOI 10.1109/TMM.2021.3114541
   Yingjun Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P200, DOI 10.1007/978-3-030-58607-2_12
   Zeyi Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P124, DOI 10.1007/978-3-030-58536-5_8
   Zhang CJ, 2023, IEEE T MULTIMEDIA, V25, P6756, DOI 10.1109/TMM.2022.3214431
   Zhang YC, 2019, PR MACH LEARN RES, V97
   Zhao YH, 2020, PROC CVPR IEEE, P3327, DOI 10.1109/CVPR42600.2020.00339
   Zhihe Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9108, DOI 10.1109/CVPR42600.2020.00913
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 88
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6285
EP 6296
DI 10.1109/TMM.2023.3347841
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600013
DA 2024-08-05
ER

PT J
AU Pan, SD
   Zhang, ZQ
   Wei, K
   Yang, X
   Deng, C
AF Pan, Siduo
   Zhang, Ziqi
   Wei, Kun
   Yang, Xu
   Deng, Cheng
TI Few-Shot Generative Model Adaptation via Style-Guided Prompt
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation models; Training; Data models; Generators; Generative
   adversarial networks; Task analysis; Visualization; Contrastive
   learning; few-shot generative model adaption; prompt learning
AB Few-shot generative model adaptation aims to obtain an excellent model that generates high-quality and high-diversity images with a few training data. However, a small number of training samples often leads to overfitting of the model, which leads the generated images to lose generative diversity. Existing methods either fail to preserve structural information, leading to overfitting phenomena, or maintain too much structure in the source domain, failing to transfer styles well. To solve these problems, we propose an effective generative model adaptation method with style-guided prompt to balance generative diversity and style transformation. Firstly, by freezing the structure-related parameters of the pre-trained model, we preserve the robustness and diversity of the source domain's generative model, which helps to mitigate overfitting and maintain diversity in the generated images. Secondly, the proposed style-guided prompt method allows us to capture the target domain's style more naturally, facilitating more accurate and efficient style transfer in the generated images. Thirdly, the multi-layer deep contrastive loss is designed to further enhance the generated images' diversity and quality by preserving the generative diversity in the source domain without using extra target domain data. Extensive quantitative and qualitative experiments prove the effectiveness and superiority of our method.
C1 [Pan, Siduo; Zhang, Ziqi; Wei, Kun; Yang, Xu; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University
RP Wei, K; Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM pansiduo0@gmail.com; zqzh9116@gmail.com; weikunsk@gmail.com;
   xuyang.xidian@gmail.com; chdeng.xd@gmail.com
OI Yang, Xu/0000-0002-0405-6816
FU National Key R&D Program of China
FX No Statement Available
CR Alanov A., 2022, Advances in Neural Information Processing Systems, V35, P29414
   Bahng H, 2022, Arxiv, DOI arXiv:2203.17274
   Bartunov S, 2018, PR MACH LEARN RES, V84
   Brock A., 2018, P INT C LEARN REPR
   Chen H, 2024, Arxiv, DOI arXiv:2208.07463
   Chen S., 2022, P ADV NEUR INF PROC, V35, P16664, DOI DOI 10.48550/ARXIV.2205.13535
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen YC, 2020, PROC CVPR IEEE, P5273, DOI 10.1109/CVPR42600.2020.00532
   Clouƒtre L, 2019, Arxiv, DOI [arXiv:1901.02199, DOI 10.48550/ARXIV.1901.02199]
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GQ, 2022, PROC CVPR IEEE, P11184, DOI 10.1109/CVPR52688.2022.01091
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Houlsby N, 2019, PR MACH LEARN RES, V97
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia ML, 2022, LECT NOTES COMPUT SC, V13693, P709, DOI 10.1007/978-3-031-19827-4_41
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim S, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555416
   Kingma D. P., 2014, arXiv
   Li Y., 2020, Advances in Neural Information Processing Systems, P15885, DOI 10.48550/arXiv.2012.02780
   Liu MY, 2017, ADV NEUR IN, V30
   Mo SW, 2020, Arxiv, DOI arXiv:2002.10964
   Ojha U, 2021, PROC CVPR IEEE, P10738, DOI 10.1109/CVPR46437.2021.01060
   Pfeiffer J, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P46
   Ridnik Tal, 2021, 35 C NEUR INF PROC S
   Salimans T, 2016, ADV NEUR IN, V29
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Tan HC, 2023, IEEE T MULTIMEDIA, V25, P8620, DOI 10.1109/TMM.2023.3238554
   Tran NT, 2021, IEEE T IMAGE PROCESS, V30, P1882, DOI 10.1109/TIP.2021.3049346
   Vitoria P, 2020, IEEE WINT CONF APPL, P2434, DOI [10.1109/WACV45572.2020.9093389, 10.1109/wacv45572.2020.9093389]
   Wan ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4672, DOI 10.1109/ICCV48922.2021.00465
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang YX, 2018, LECT NOTES COMPUT SC, V11210, P220, DOI 10.1007/978-3-030-01231-1_14
   Wu YZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14357, DOI 10.1109/ICCV48922.2021.01411
   Xiao JY, 2022, PROC CVPR IEEE, P11194, DOI 10.1109/CVPR52688.2022.01092
   Xu Z, 2023, IEEE T MULTIMEDIA, V25, P6121, DOI 10.1109/TMM.2022.3205404
   Yang CY, 2023, IEEE I CONF COMP VIS, P7699, DOI 10.1109/ICCV51070.2023.00711
   Yaniv J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322984
   Yao Y, 2022, Arxiv, DOI arXiv:2109.11797
   Yaxing Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9329, DOI 10.1109/CVPR42600.2020.00935
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Yu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14094, DOI 10.1109/ICCV48922.2021.01385
   Zhang H., 2019, P INT C LEARN REPR, P1
   Zhang LF, 2022, PROC CVPR IEEE, P12454, DOI 10.1109/CVPR52688.2022.01214
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao Shengyu, 2020, ADV NEURAL INFORM PR, V33, P7559, DOI DOI 10.36227/TECHRXIV.21805797.V1
   Zhao YQ, 2023, PROC CVPR IEEE, P7380, DOI 10.1109/CVPR52729.2023.00713
   Zhao YQ, 2022, PROC CVPR IEEE, P9130, DOI 10.1109/CVPR52688.2022.00893
   Zhao Yunqing, 2022, P ADV NEUR INF PROC, V35, P19427
   Zhao ZL, 2020, Arxiv, DOI arXiv:2006.02595
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7661
EP 7672
DI 10.1109/TMM.2024.3371156
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000006
DA 2024-08-05
ER

PT J
AU Song, QP
   Li, JX
   Wu, S
   Wong, HS
AF Song, Quanpeng
   Li, Jiaxin
   Wu, Si
   Wong, Hau-San
TI A Graph-Based Discriminator Architecture for Multi-Attribute Facial
   Image Editing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative adversarial network; image editing; multi-attribute
   manipulation; graph-based discriminator
ID TRANSLATION
AB Multi-attribute editing aims to synthesize new facial images with multiple desired attributes while at the same time preserving other contents. Generative Adversarial Networks (GANs) with encoder-decoder-based generators are typically applied to this task, while the co-occurrence nature of attributes is overlooked by generic discriminators when identifying real and synthesized instances. To address the issue, we focus on precisely capturing semantics associated with target attributes in this work, and propose a Graph-based Discriminator architecture for a GAN model, which is referred to as GD-GAN, for explicitly modeling and leveraging the attribute dependencies. Specifically, the co-occurrence ratio between attributes is used to build a correlation matrix, which captures inter-attribute relationships. We design a discriminator with a Graph Convolutional Network (GCN) to integrate knowledge about the attribute dependencies into the adversarial training process. Different from the existing methods that identify the synthesized data conditioned on the attributes individually, we leverage the attribute correlations by performing feature propagation over the graph of attributes, which leads to interdependent representations for real-fake instance identification. Incorporating the relationships of attributes eventually induces the generator to capture precise semantics associated with the attributes. Empirical results on multiple benchmarks demonstrate the superior performance of GD-GAN in high-quality semantic manipulation.
C1 [Song, Quanpeng; Li, Jiaxin; Wu, Si] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510642, Peoples R China.
   [Wong, Hau-San] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 South China University of Technology; City University of Hong Kong
RP Wu, S (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510642, Peoples R China.
EM cssongquanpeng@mail.scut.edu.cn; csjiaxinli@mail.scut.edu.cn;
   cswusi@scut.edu.cn; cshswong@cityu.edu.hk
OI WONG, Hau-San/0000-0002-1530-7529
FU China Scholarship Council
FX No Statement Available
CR Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Almahairi A, 2018, PR MACH LEARN RES, V80
   [Anonymous], 2018, Proc. Int. Conf. Learn. Representation
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bhattarai Binod, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P69, DOI 10.1007/978-3-030-58571-6_5
   Brocker A, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3313247
   Chen ZM, 2023, IEEE T PATTERN ANAL, V45, P6969, DOI 10.1109/TPAMI.2021.3063496
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denton E, 2015, ADV NEUR IN, V28
   Ding GQ, 2022, PROC CVPR IEEE, P11184, DOI 10.1109/CVPR52688.2022.01091
   Gong MM, 2019, ADV NEUR IN, V32
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Harkonen E., 2020, Proc. Neural Inf. Process. Syst., P1
   He ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14388, DOI 10.1109/ICCV48922.2021.01414
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, INT C LEARNING REPRE
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kavalerov I, 2020, Arxiv, DOI arXiv:1912.04216
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Lample G., 2017, P ANN C NEUR INF PRO, P5969
   Li SJ, 2023, IEEE T MULTIMEDIA, V25, P3180, DOI 10.1109/TMM.2022.3156699
   Ling H., 2021, Advances in Neural Information Processing Systems (NIPS), P16331
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu MY, 2017, ADV NEUR IN, V30
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Maas A.L., 2013, ICML WORK DEEP LEARN, V28
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato T, 2018, CoRR
   Odena A, 2017, PR MACH LEARN RES, V70
   Or-El Roy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P739, DOI 10.1007/978-3-030-58539-6_44
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Perarnau Guim, 2016, NIPS WORKSH ADV TRAI
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Salimans T, 2016, ADV NEUR IN, V29
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   Springenberg J. T., 2016, ICLR, P1
   Tuzel, 2016, Advances in Neural Information Processing Systems, P469
   Wu PW, 2019, IEEE I CONF COMP VIS, P5913, DOI 10.1109/ICCV.2019.00601
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang H., 2019, PROC INT C LEARN REP, P1
   Zhao M., 2020, PMLR, P11340
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuang P., 2021, Proc. Int. Conf. Learn. Representation, P1
NR 58
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 436
EP 446
DI 10.1109/TMM.2023.3266611
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000012
DA 2024-08-05
ER

PT J
AU Wang, QD
   Liu, DX
   Liu, ZY
   Xu, JT
   Tan, JR
AF Wang, Qide
   Liu, Daxin
   Liu, Zhenyu
   Xu, Jiatong
   Tan, Jianrong
TI 3D Object Segmentation Using Cross-Window Point Transformer With Latent
   Semantic Boundary Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Industrial robot; point clouds; segmentation; transformer
AB Accurate 3D object segmentation in point clouds is a basis for industrial robot applications, such as robot manipulation and digital twin, which require an understanding of the 3D environment. However, the unstructured and disordered nature of point clouds makes it challenging, especially for the incomplete 3D data under a single view in the real-world scenario. To this end, this article proposes a novel 3D object segmentation framework (3DT-Seg) based on Cross-Window Point Transformer (CP-Former). CP-Former captures the long-range dependencies between local windows and latent semantic boundaries to enhance the point-wise features extracted from irregular point clouds via a bidirectional cross-attention mechanism. In addition, a contrastive learning loss and an adaptive dual aggregation strategy are introduced on semantic transition regions during the semantic supervising and instance clustering process, respectively. In this way, the latent boundary information is further utilized to improve the overall segmentation performance. Experiments on the popular benchmark dataset (S3DIS) show the state-of-the-art performance of the proposed approach in terms of semantic and instance segmentation. Furthermore, a real-world point cloud dataset (IP-Cloud) for the robotic grasping task is presented to fully validate the effectiveness of our method in practice, where it also achieves remarkable performance.
C1 [Wang, Qide; Liu, Daxin; Liu, Zhenyu; Xu, Jiatong; Tan, Jianrong] Zhejiang Univ, State Key Lab Comp Aided Design & Comp Graph CAD&C, Hangzhou 310027, Peoples R China.
   [Wang, Qide; Liu, Daxin; Liu, Zhenyu; Xu, Jiatong; Tan, Jianrong] Engn Res Ctr Design Engn & Digital Twin Zhejiang P, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Liu, DX (corresponding author), Zhejiang Univ, State Key Lab Comp Aided Design & Comp Graph CAD&C, Hangzhou 310027, Peoples R China.
EM btwqd@zju.edu.cn; liudx@zju.edu.cn; liuzy@zju.edu.cn;
   xjtacitron@zju.edu.cn; egi@zju.edu.cn
OI Wang, Qide/0000-0003-0656-1581; Liu, Daxin/0000-0002-0379-1990
FU National Natural Science Foundation of China
FX No Statement Available
CR Vo AV, 2015, ISPRS J PHOTOGRAMM, V104, P88, DOI 10.1016/j.isprsjprs.2015.01.011
   Armeni I., 2017, arXiv
   Carion N., 2020, EUR C COMP VIS, P213
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15447, DOI 10.1109/ICCV48922.2021.01518
   Cheng BW, 2021, PROC CVPR IEEE, P15329, DOI 10.1109/CVPR46437.2021.01508
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Frosst N., 2019, INT C MACHLEARN, P3531
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han XF, 2023, IEEE T MULTIMEDIA, V25, P5638, DOI 10.1109/TMM.2022.3198318
   Huan Lei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11608, DOI 10.1109/CVPR42600.2020.01163
   Huang J, 2016, INT C PATT RECOG, P2670, DOI 10.1109/ICPR.2016.7900038
   Hui L., 2022, P ADV NEUR INF PROC, P36804
   Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492
   Lai X, 2022, PROC CVPR IEEE, P8490, DOI 10.1109/CVPR52688.2022.00831
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Lawin FJ, 2017, LECT NOTES COMPUT SC, V10424, P95, DOI 10.1007/978-3-319-64689-3_8
   Lee HJ, 2020, PROC CVPR IEEE, P4816, DOI 10.1109/CVPR42600.2020.00487
   Li YY, 2018, ADV NEUR IN, V31
   Liang ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2763, DOI 10.1109/ICCV48922.2021.00278
   Lin ZH, 2022, IEEE T PATTERN ANAL, V44, P4212, DOI 10.1109/TPAMI.2021.3059758
   Liu SP, 2022, IEEE T MULTIMEDIA, V24, P2392, DOI 10.1109/TMM.2021.3080076
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Marin D, 2019, IEEE I CONF COMP VIS, P2131, DOI 10.1109/ICCV.2019.00222
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Vu T, 2022, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR52688.2022.00273
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vaswani A., 2017, C WORKSHOP NEURAL IN, P6000
   Wang K, 2021, ROBOT CIM-INT MANUF, V68, DOI 10.1016/j.rcim.2020.102089
   Wang K, 2020, ROBOT CIM-INT MANUF, V63, DOI 10.1016/j.rcim.2019.101890
   Wang QD, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3236334
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang XL, 2019, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2019.00422
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Xie YX, 2020, IEEE GEOSC REM SEN M, V8, P38, DOI 10.1109/MGRS.2019.2937630
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   YANG B, 2019, P 33 INT C NEUR INF, P605
   Yang X, 2021, IEEE T MULTIMEDIA, V23, P4208, DOI 10.1109/TMM.2020.3038323
   Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Zhang H, 2022, IEEE T IND ELECTRON, V69, P3876, DOI 10.1109/TIE.2021.3075836
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhu XG, 2022, IEEE T PATTERN ANAL, V44, P6807, DOI 10.1109/TPAMI.2021.3098789
NR 52
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5951
EP 5961
DI 10.1109/TMM.2023.3342697
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100016
DA 2024-08-05
ER

PT J
AU Xiao, J
   Bi, XJ
AF Xiao, Jian
   Bi, Xiaojun
TI Model-Guided Generative Adversarial Networks for Unsupervised
   Fine-Grained Image Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unsupervised fine-grained image generation; generative adversarial
   networks; attention mechanism; image inpainting
AB Unsupervised fine-grained image generation is a challenging issue in computer vision. Although many recent significant advances have improved performance, the ability to synthesize photo-realistic images in an unsupervised manner remains extremely difficult. The existing methods compose an image via complex three-stage generative adversarial networks and impose constraints between the latent codes. This pipeline focuses on the disentanglement and ignores the quality of generated images. In this article, we propose a novel two-stage approach for unsupervised fine-grained image generation, termed Model-Guided Generative Adversarial Networks (MG-GAN). We introduce an attention module for exploring the correlation between fine-grained latent codes and image features in the foreground generation stage. The attention module enables the network to automatically focus on the color details and semantic concepts of objects related to different fine-grained classes. Furthermore, we incorporate knowledge distillation strategy and design a simple but effective inverse background image generator as a teacher to guide the background image generation. With the help of knowledge learned in the pre-trained inverse background image generator, a comfortable canvas is synthesized and combined with foreground object more reasonably. Extensive experiments on three popularly fine-grained datasets demonstrate that our approach achieves state-of-the-art performance and is even competitive with semi-supervised method.
C1 [Xiao, Jian] Harbin Engn Univ, Coll Informat & Commun Engn, Harbin 150001, Peoples R China.
   [Bi, Xiaojun] Minzu Univ China, Sch Informat Engn, Beijing 100081, Peoples R China.
   [Bi, Xiaojun] Minzu Univ China, Key Lab Ethn Language Intelligent Anal & Secur Gov, MOE, Beijing 100081, Peoples R China.
C3 Harbin Engineering University; Minzu University of China
RP Bi, XJ (corresponding author), Minzu Univ China, Sch Informat Engn, Beijing 100081, Peoples R China.; Bi, XJ (corresponding author), Minzu Univ China, Key Lab Ethn Language Intelligent Anal & Secur Gov, MOE, Beijing 100081, Peoples R China.
EM xiaojian2015@hrbeu.edu.cn; bixiaojun@hrbeu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Barratt S, 2018, Arxiv, DOI arXiv:1801.01973
   Benny Yaniv, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P514, DOI 10.1007/978-3-030-58574-7_31
   Brock A., 2019, P INT C LEARN REPR
   Chen T., 2021, PROC IEEECVF INT C C, P9264
   Chen X, 2016, ADV NEUR IN, V29
   Dong YL, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107573
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hays J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276382, 10.1145/1239451.1239455]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Imankulova A, 2019, Arxiv, DOI arXiv:1907.03060
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang Y, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149440
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Khosla A., 2011, P IEEE INT WORKSH FI
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P5881, DOI 10.1109/TIP.2019.2922854
   Miyato Takeru, 2018, ICLR
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Salimans T, 2016, ADV NEUR IN, V29
   Singh KK, 2019, PROC CVPR IEEE, P6483, DOI 10.1109/CVPR.2019.00665
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Tan HC, 2021, IEEE T IMAGE PROCESS, V30, P1275, DOI 10.1109/TIP.2020.3026728
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Y, 2020, PROC CVPR IEEE, P5093, DOI 10.1109/CVPR42600.2020.00514
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yu F., 2016, P INT C LEARN REPR
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Yuheng Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8036, DOI 10.1109/CVPR42600.2020.00806
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhou Y, 2022, IEEE J BIOMED HEALTH, V26, P56, DOI 10.1109/JBHI.2020.3045475
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 45
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1188
EP 1199
DI 10.1109/TMM.2023.3277758
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700031
DA 2024-08-05
ER

PT J
AU Xie, CY
   Zhang, DH
   Wu, Z
   Yu, C
   Hu, Y
   Chen, Y
AF Xie, Chunyang
   Zhang, Dongheng
   Wu, Zhi
   Yu, Cong
   Hu, Yang
   Chen, Yan
TI RPM: RF-Based Pose Machines
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE RF sensing; 3D human pose estimation; deep learning; smart homes
ID LOCALIZATION; SIGNAL
AB Radio-frequency (RF) based human sensing technologies, due to their great practical value in various applications and privacy-preserving nature, have gained tremendous attention in recent years. However, without fully exploiting the characteristics of radio signals, the performance of existing methods are still limited. First, RF features of the moving human body have different representations in dimensions such as channel and scale, which is challenging when performing feature fusion. Besides, the human body is specularly reflective with respect to the radar, which means the human body cannot be fully captured by a single RF snapshot. Therefore, the radar signal reflected by the human body is sparse and incomplete, which is difficult to extract high-quality features for 3D human pose estimation. In this paper, we present the RF-based Pose Machines (RPM), a novel framework which can generate 3D skeletons from RF signals. Considering the characteristics of RF signals, RPM includes several modules to overcome the challenges. Firstly, a Feature Fusion Network (FFN) is designed to effectively fuse radio signals from horizontal and vertical planes based on the channels' correlation and maintain high-quality feature via a multi-scale fusion block. A Spatio-Temporal Attention network is then designed to reconstruct 3D skeletons from the sparse and incomplete RF signals. Specifically, a spatial attention module is designed to model non-local relationships among joints and reconstruct body parts that a single RF snapshot cannot capture. Afterwards, a temporal attention module is proposed to refine 3D pose based on temporal coherency learned from frame queries. To evaluate the performance of our RPM framework, we construct a large-scale dataset of synchronized 3 d skeletons and RF signals, RFSkeleton3D. Our experimental results show that RPM locates 3D key points of the human body with an average error of $5.71\;\mathrm{cm}$ and maintains its performance in new environments with occlusion or bad illumination. The dataset and codes will be made in public.
C1 [Xie, Chunyang; Yu, Cong] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 610056, Peoples R China.
   [Zhang, Dongheng; Wu, Zhi; Chen, Yan] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
   [Hu, Yang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Chen, Y (corresponding author), Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
EM chunyangxie@std.uestc.edu.cn; dongheng@ustc.edu.cn;
   wzwyyx@mail.ustc.edu.cn; congyu@std.uestc.edu.cn; eeyhu@ustc.edu.cn;
   eecyan@ustc.edu.cn
OI Xie, Chunyang/0000-0002-5074-314X; Zhang, Dongheng/0000-0001-6309-6626;
   wu, zhi/0000-0001-7097-4837
FU National Natural Science Foundation of China
FX No Statement Available
CR Adib F, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818072
   Ba JL, 2016, ARXIV
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bashirov R, 2021, IEEE WINT CONF APPL, P2806, DOI 10.1109/WACV48630.2021.00285
   Can Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P242, DOI 10.1007/978-3-030-58580-8_15
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen Y, 2021, IEEE INTERNET THINGS, V8, P2762, DOI 10.1109/JIOT.2020.3022071
   Chen Y, 2020, IEEE T MOBILE COMPUT, V19, P2891, DOI 10.1109/TMC.2019.2934106
   Choi Hongsuk, 2020, COMPUTER VISION ECCV
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gao RY, 2022, PROC ACM INTERACT MO, V6, DOI 10.1145/3517241
   Han Y, 2016, IEEE INTERNET THINGS, V3, P1036, DOI 10.1109/JIOT.2016.2548659
   Hartley R, 2003, Multiple view geometry in computer vision, DOI [10.1016/S0143-8166(01)00145-2, DOI 10.1017/CBO9780511811685]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2020, IEEE INTERNET THINGS, V7, P8296, DOI 10.1109/JIOT.2020.2989426
   Hsu R., 2019, CHI C HUM FACTORS CO, P1
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kotaru M, 2015, ACM SIGCOMM COMP COM, V45, P269, DOI 10.1145/2829988.2787487
   Li JF, 2021, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR46437.2021.00339
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Liu H, 2023, IEEE T MULTIMEDIA, V25, P1390, DOI 10.1109/TMM.2022.3141888
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Qian K, 2018, ACM T EMBED COMPUT S, V17, DOI 10.1145/3157677
   Qian K, 2017, MOBIHOC'17: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, DOI 10.1145/3084041.3084067
   Qiu CR, 2023, IEEE T MULTIMEDIA, V25, P2613, DOI 10.1109/TMM.2022.3149129
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sengupta A, 2020, IEEE SENS J, V20, P10032, DOI 10.1109/JSEN.2020.2991741
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Vaswani A, 2017, ADV NEUR IN, V30
   Vicon motion systems, 2015, About us
   Wang YM, 2021, IEEE COMMUN LETT, V25, P2235, DOI 10.1109/LCOMM.2021.3073271
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu CS, 2015, IEEE J SEL AREA COMM, V33, P2329, DOI 10.1109/JSAC.2015.2430294
   Xiang Li, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3130940
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xie CY, 2022, IEEE T CIRC SYST VID, V32, P6214, DOI 10.1109/TCSVT.2022.3164663
   Xu QY, 2017, IEEE INTERNET THINGS, V4, P723, DOI 10.1109/JIOT.2017.2663318
   Xu QY, 2017, IEEE T INF FOREN SEC, V12, P1141, DOI 10.1109/TIFS.2016.2647224
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yili Ren, 2021, SenSys '21: Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems, P363, DOI 10.1145/3485730.3492871
   Yu C, 2023, IEEE T MULTIMEDIA, V25, P2926, DOI 10.1109/TMM.2022.3153136
   Zhang B.-B., 2021, Unsupervised domain adaptation for device-free gesture recognition
   Zhang DH, 2021, IEEE INTERNET THINGS, V8, P3904, DOI 10.1109/JIOT.2020.3025820
   Zhang DH, 2020, IEEE SYST J, V14, P661, DOI 10.1109/JSYST.2019.2904714
   Zhang DH, 2019, IEEE INTERNET THINGS, V6, P3899, DOI 10.1109/JIOT.2019.2893330
   Zhang DH, 2018, IEEE T VEH TECHNOL, V67, P7101, DOI 10.1109/TVT.2018.2827408
   Zhang GL, 2023, IEEE T WIREL COMMUN, V22, P6534, DOI 10.1109/TWC.2023.3244369
   Zhang O, 2016, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'16), P83, DOI 10.1145/2999572.2999582
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao MM, 2018, PROC CVPR IEEE, P7356, DOI 10.1109/CVPR.2018.00768
   Zhao MM, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P267, DOI 10.1145/3230543.3230579
   Zhao MM, 2016, MOBICOM'16: PROCEEDINGS OF THE 22ND ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P95, DOI 10.1145/2973750.2973762
   Zheng XL, 2016, IEEE INFOCOM SER
   Zhou R, 2017, IEEE SENS J, V17, P7990, DOI 10.1109/JSEN.2017.2762428
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zimmermann C, 2018, IEEE INT CONF ROBOT, P1986, DOI 10.1109/ICRA.2018.8462833
NR 63
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 637
EP 649
DI 10.1109/TMM.2023.3268376
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000022
DA 2024-08-05
ER

PT J
AU Yang, S
   Wu, XX
   Shang, ZR
   Luo, JB
AF Yang, Shuo
   Wu, Xinxiao
   Shang, Zirui
   Luo, Jiebo
TI Dynamic Pathway for Query-Aware Feature Learning in Language-Driven
   Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic pathway; exploitation; exploration; language-driven action
   localization; video grounding; video moment retrieval
AB Language-driven action localization aims to search a video segment in an untrimmed video, which is semantically relevant to an input language query. This task is challenging since language queries describe diverse actions with different motion characteristics and semantic granularities. Some actions, such as "the person takes off their shoes, and goes to the door", are characterized by complex motion relationships, while others, such as "a person is standing holding a mirror in one hand", are distinguished by salient body postures. In this paper, we propose a dynamic pathway between an exploitation module and an exploration module for query-aware feature learning to handle the diversity of actions. The exploitation module works in a coarse-to-fine manner, first learns the feature of general motion relationships to search the coarse segment of the target action and then learns the feature of subtle motion changes to predict the refined action boundaries. The exploration module functions in a point-to-area diffusion fashion, first learns the feature of sub-action pattern to search the salient postures of the target action and then learns the feature of temporal dependency to expand the posture frames to the action segment. The exploitation module and the exploration module are dynamically and adaptively selected to learn comprehensive representations of diverse actions to improve the action localization accuracy. Extensive experiments on the Charades-STA and TACoS datasets demonstrate that our method performs better than existing methods.
C1 [Yang, Shuo; Wu, Xinxiao] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Yang, Shuo; Wu, Xinxiao] Shenzhen MSU BIT Univ, Guangdong Prov Lab Machine Percept & Intelligent C, Shenzhen 518172, Peoples R China.
   [Shang, Zirui] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Key Lab Intelligent Informat Technol, Beijing 100811, Peoples R China.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 Beijing Institute of Technology; Shenzhen MSU-BIT University; Beijing
   Institute of Technology; University of Rochester
RP Wu, XX (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM shuoyang@bit.edu.cn; wuxinxiao@bit.edu.cn; shangzirui@bit.edu.cn;
   jluo@cs.rochester.edu
RI Yang, Shuo/KFR-5084-2024
OI Yang, Shuo/0000-0003-2868-7070; Shang, Zirui/0000-0002-3073-0579; Wu,
   Xinxiao/0000-0002-2056-6947
FU Natural Science Foundation of China
FX No Statement Available
CR Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P898, DOI 10.1145/3394171.3413841
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen J, 2021, AAAI CONF ARTIF INTE, V35, P1027
   Chen JY, 2019, AAAI CONF ARTIF INTE, P8175
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Ding XP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11553, DOI 10.1109/ICCV48922.2021.01137
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gavrilyuk K, 2018, PROC CVPR IEEE, P5958, DOI 10.1109/CVPR.2018.00624
   Ghosh S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1984
   Hahn M., 2019, PROC BRIT MACH VIS C
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Hui TR, 2023, IEEE T PATTERN ANAL, V45, P8646, DOI 10.1109/TPAMI.2023.3235720
   Jang E., 2017, ICLR (Poster)
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Kingma D.P., 2014, Proc. of ICLR
   Li JP, 2023, IEEE I CONF COMP VIS, P11929, DOI 10.1109/ICCV51070.2023.01099
   Li K, 2021, AAAI CONF ARTIF INTE, V35, P1902
   Liang C, 2023, IEEE T PATTERN ANAL, V45, P10055, DOI 10.1109/TPAMI.2023.3262578
   Liang C, 2022, PROC CVPR IEEE, P15544, DOI 10.1109/CVPR52688.2022.01512
   Liu DZ, 2023, AAAI CONF ARTIF INTE, P1640
   Liu DZ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4536, DOI 10.1145/3503161.3547782
   Liu DZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4070, DOI 10.1145/3394171.3414026
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Ning K, 2021, IEEE T IMAGE PROCESS, V30, P2538, DOI 10.1109/TIP.2021.3052086
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/wacv45572.2020.9093328, 10.1109/WACV45572.2020.9093328]
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Sun X, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P1022, DOI 10.1145/3477495.3532083
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Wang H, 2021, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR46437.2021.00695
   Wang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4116, DOI 10.1145/3394171.3413975
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wu AM, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P961
   Wu AM, 2021, IEEE T CIRC SYST VID, V31, P2438, DOI 10.1109/TCSVT.2020.3020877
   Wu AM, 2020, IEEE T CIRC SYST VID, V30, P4299, DOI 10.1109/TCSVT.2019.2956593
   Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029
   Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Yang A, 2023, PROC CVPR IEEE, P10714, DOI 10.1109/CVPR52729.2023.01032
   Yang Shuo, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P5164, DOI 10.1145/3581783.3612512
   Yang Shuo, 2022, P 31 INT JOINT C ART, P1552
   Yuan YT, 2022, IEEE T PATTERN ANAL, V44, P2725, DOI 10.1109/TPAMI.2020.3038993
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, P IEEECVF C COMPUTER, P10287
   Zhang H, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P776
   Zhang H, 2022, IEEE T PATTERN ANAL, V44, P4252, DOI 10.1109/TPAMI.2021.3060449
   Zhang MX, 2021, PROC CVPR IEEE, P12664, DOI 10.1109/CVPR46437.2021.01248
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhao WT, 2021, ADV NEUR IN, V34
   Zhao Y, 2021, PROC CVPR IEEE, P4195, DOI 10.1109/CVPR46437.2021.00418
   Zhou H, 2021, PROC CVPR IEEE, P8441, DOI 10.1109/CVPR46437.2021.00834
NR 58
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7451
EP 7461
DI 10.1109/TMM.2024.3368919
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000073
DA 2024-08-05
ER

PT J
AU Zhang, DZ
   Chen, FL
   Chang, JL
   Chen, XY
   Tian, Q
AF Zhang, Duzhen
   Chen, Feilong
   Chang, Jianlong
   Chen, Xiuyi
   Tian, Qi
TI Structure Aware Multi-Graph Network for Multi-Modal Emotion Recognition
   in Conversations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Context modeling; Feature extraction;
   Visualization; Acoustics; Oral communication; Transformers; Structure
   learning; multi-graph network; dual-stream propagations; multi-modal
   fusion; emotion recognition in conversations
AB Multi-Modal Emotion Recognition in Conversations (MMERC) is an increasingly active research field that leverages multi-modal signals to understand the feelings behind each utterance. Modeling contextual interactions and multi-modal fusion lie at the heart of this field, with graph-based models recently being widely used for MMERC to capture global multi-modal contextual information. However, these models generally mix all modality representations in a single graph, and utterances in each modality are fully connected, potentially ignoring three problems: 1) the heterogeneity of the multi-modal context, 2) the redundancy of contextual information, and 3) over-smoothing of the graph networks. To address these problems, we propose a Structure Aware Multi-Graph Network (SAMGN) for MMERC. Specifically, we construct multiple modality-specific graphs to model the heterogeneity of the multi-modal context. Instead of fully connecting the utterances in each modality, we design a structure learning module that determines whether edges exist between the utterances. This module reduces redundancy by forcing each utterance to focus on the contextual ones that contribute to its emotion recognition, acting like a message propagating reducer to alleviate over-smoothing. Then, we develop the SAMGN via Dual-Stream Propagation (DSP), which contains two propagation streams, i.e., intra- and inter-modal, performed in parallel to aggregate the heterogeneous modality information from multi-graphs. DSP also contains a gating unit that adaptively integrates the co-occurrence information from the above two propagations for emotion recognition. Experiments on two popular MMERC datasets demonstrate that SAMGN achieves new State-Of-The-Art (SOTA) results.
C1 [Zhang, Duzhen; Chen, Feilong; Chang, Jianlong; Tian, Qi] Huawei Technol, Cloud & AI, Shenzhen 518129, Peoples R China.
   [Chen, Xiuyi] Baidu Inc, Beijing 100085, Peoples R China.
C3 Huawei Technologies; Baidu
RP Chen, XY (corresponding author), Baidu Inc, Beijing 100085, Peoples R China.
EM zhangduzhen2019@ia.ac.cn; chenfeilong2018@ia.ac.cn;
   jianlong.chang@huawei.com; chenxiuyi01@baidu.com; tian.qi1@huawei.com
OI Zhang, Duzhen/0000-0002-4280-431X; Chen, Xiuyi/0000-0002-9351-4160
CR Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Chatterjee A, 2019, COMPUT HUM BEHAV, V93, P309, DOI 10.1016/j.chb.2018.12.029
   Chen FL, 2023, MACH INTELL RES, V20, P38, DOI 10.1007/s11633-022-1369-5
   Chen FY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1064, DOI 10.1145/3474085.3475661
   Chen SY, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P1597
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Dong JH, 2024, IEEE T PATTERN ANAL, V46, P1664, DOI 10.1109/TPAMI.2021.3128560
   Dong JH, 2020, PROC CVPR IEEE, P4022, DOI 10.1109/CVPR42600.2020.00408
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eyben F, 2010, P 18 INT C MULT 2010, DOI DOI 10.1145/1873951.1874246
   Ghosal D, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P2470
   Ghosal D, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P154
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu Y, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2225
   Gumbel E. J, 1954, National Bereau of Standards Applied Mathematics Series, V33
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hazarika D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2594
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   Hu D, 2021, P 59 ANN M ASS COMP, V1, P7042
   Hu D, 2022, INT CONF ACOUST SPEE, P7037, DOI 10.1109/ICASSP43922.2022.9747397
   Hu J., 2021, P 59 ANN M ASS COMP, P5666
   Huang C., 2018, P C N AM CHAPT ASS C, V2, P49, DOI DOI 10.18653/V1/N18-2008
   Ishiwatari T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P7360
   Jang E., 2017, PROC 5 INT C LEARN R, P101
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Koehn Philipp, 2004, P 2004 C EMPIRICAL M, P388
   Li W, 2022, NEUROCOMPUTING, V467, P73, DOI 10.1016/j.neucom.2021.09.057
   Liu Yinhan, 2019, ROBERTA ROBUSTLY OPT
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Ma YK, 2020, INFORM FUSION, V64, P50, DOI 10.1016/j.inffus.2020.06.011
   Majumder N, 2019, AAAI CONF ARTIF INTE, P6818
   Mao YZ, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P2694
   Nguyen D, 2022, IEEE T MULTIMEDIA, V24, P1313, DOI 10.1109/TMM.2021.3063612
   Ni JJ, 2022, AAAI CONF ARTIF INTE, P11112
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P3793, DOI 10.1109/TMM.2020.3032037
   Poria S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P527
   Poria S, 2021, COGN COMPUT, V13, P229, DOI 10.1007/s12559-020-09738-0
   Poria S, 2019, IEEE ACCESS, V7, P100943, DOI 10.1109/ACCESS.2019.2929050
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Rong Yu, 2020, INT C LEARN REPR
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Shen WZ, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P1551
   Shen WZ, 2021, AAAI CONF ARTIF INTE, V35, P13789
   Shirian A, 2022, IEEE T MULTIMEDIA, V24, P780, DOI 10.1109/TMM.2021.3059169
   Vashishth S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3308
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y., 2020, P NEURIPS, V33, P4835
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yang ZL, 2019, ADV NEUR IN, V32
   Young T, 2022, AAAI CONF ARTIF INTE, P11622
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhang D. Z., 2020, P 28 INT C COMPUTATI, P4429
   Zhang D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P503, DOI 10.1145/3394171.3413949
   Zhang D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5415
   Zhang Duzhen, 2022, P 29 INT C COMP LING, P6762
   Zhang HN, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3721
   Zhong PX, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P165
   Zhou H, 2018, AAAI CONF ARTIF INTE, P730
   Zhu LX, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P1571
NR 62
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3987
EP 3997
DI 10.1109/TMM.2023.3238314
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300002
DA 2024-08-05
ER

PT J
AU Zhang, XQ
   Luo, C
   Wang, X
   Li, JH
   Zhao, S
   Jiang, DJ
AF Zhang, Xiaoqian
   Luo, Chao
   Wang, Xiao
   Li, Jinghao
   Zhao, Shuai
   Jiang, Daojian
TI Learnable Tensor Graph Fusion Framework for Natural Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Tensors; Watersheds; Feature extraction; Transforms;
   Correlation; Image color analysis; affinity graph learning; tensor;
   spectral clustering
ID LOW-RANK; ALGORITHM
AB Existing natural image segmentation tasks often face some troubles in getting accurate contours or causing over-segmentation of local areas during segmentation, and the multi-feature information mining of images is insufficient. To address these problems, learnable tensor graph fusion framework for natural image segmentation (LTGF-NIS) is proposed in this paper. Firstly, we pre-process the original images by adaptive morphological reconstruction watershed transform, and the multi-feature data is extracted, the multi-feature data matrix contains information about the image. Then we design an adaptive weighted tensor affinity graph fusion method (AWTAGF), which learns the higher-order correlation of the multi-feature information by two coupling tensor, while achieving consistent representation of the multi-feature information by using adaptive weighted graph fusion. Finally, the obtained affinity graph is clustered and segmented using spectral clustering to realize the segmentation of natural images. We test affinity graph learning ability and natural image segmentation effect of the proposed algorithm on several datasets, the experimental data indicates that the segmentation effect of our framework is superior to some existing advanced methods.
C1 [Zhang, Xiaoqian; Luo, Chao; Wang, Xiao; Li, Jinghao; Zhao, Shuai] Southwest Univ Sci & Technol, Sch Informat Engn, Mianyang 621010, Peoples R China.
   [Jiang, Daojian] China Acad Engn Phys, Inst Chem Mat, Mianyang 621999, Peoples R China.
C3 Southwest University of Science & Technology - China; Chinese Academy of
   Engineering Physics
RP Zhang, XQ (corresponding author), Southwest Univ Sci & Technol, Sch Informat Engn, Mianyang 621010, Peoples R China.
EM zhxq0528@163.com; 798508913@qq.com; wangxiao@mails.swust.edu.cn;
   lijinghao0506@163.com; zhaoshuai980124@163.com; jiangdj@caep.ac.cn
RI Li, Jinghao/ABG-8992-2020
OI Li, Jinghao/0000-0002-9122-7184; Zhang, Xiaoqian/0000-0002-4007-4501;
   Luo, Chao/0009-0001-1412-9548
FU National Natural Science Foundation of China
FX No Statement Available
CR Aflalo A, 2023, IEEE INT CONF COMP V, P32, DOI 10.1109/ICCVW60793.2023.00010
   Beck A, 2015, SIAM J OPTIMIZ, V25, P185, DOI 10.1137/13094829X
   Bertsekas DP, 2014, Constrained optimization and Lagrange multiplier methods, DOI DOI 10.1016/B978-0-12-093480-5.50005-2
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen MS, 2020, AAAI CONF ARTIF INTE, V34, P3513
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Das A, 2022, NEURAL COMPUT APPL, V34, P4531, DOI 10.1007/s00521-021-06610-6
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   FAN K, 1949, P NATL ACAD SCI USA, V35, P652, DOI 10.1073/pnas.35.11.652
   Fang SG, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3261460
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Gao QX, 2020, AAAI CONF ARTIF INTE, V34, P3930
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Hettiarachchi R, 2017, PATTERN RECOGN, V65, P119, DOI 10.1016/j.patcog.2016.12.011
   Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569
   Ji Jintian, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P328, DOI 10.1145/3581783.3611733
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lei T, 2019, IEEE T IMAGE PROCESS, V28, P5510, DOI 10.1109/TIP.2019.2920514
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Liang YW, 2024, IEEE T NEUR NET LEAR, V35, P2848, DOI 10.1109/TNNLS.2022.3192445
   Luo XD, 2021, AAAI CONF ARTIF INTE, V35, P8801
   Merényi E, 2020, NEURAL COMPUT APPL, V32, P18161, DOI 10.1007/s00521-019-04198-6
   Nie FP, 2017, ADV NEUR IN, V30
   Pereyra M, 2017, IEEE T IMAGE PROCESS, V26, P2577, DOI 10.1109/TIP.2017.2675165
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877
   Ren ZW, 2020, IEEE T IMAGE PROCESS, V29, P2094, DOI 10.1109/TIP.2019.2938859
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen QQ, 2023, IEEE T MULTIMEDIA, V25, P6371, DOI 10.1109/TMM.2022.3207922
   Sun MJ, 2022, IEEE T MULTIMEDIA, V24, P2567, DOI 10.1109/TMM.2021.3086727
   Vidal R, 2014, PATTERN RECOGN LETT, V43, P47, DOI 10.1016/j.patrec.2013.08.006
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Yang Cheng, 2021, 2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC), P1174, DOI 10.1109/IPEC51340.2021.9421206
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang GQ, 2022, IEEE T CIRC SYST VID, V32, P6766, DOI 10.1109/TCSVT.2022.3169422
   Zhang GQ, 2021, IEEE T IMAGE PROCESS, V30, P8913, DOI 10.1109/TIP.2021.3120054
   Zhang XQ, 2022, NEUROCOMPUTING, V475, P38, DOI 10.1016/j.neucom.2021.12.029
   Zhang Y, 2022, IEEE T MULTIMEDIA, V24, P440, DOI 10.1109/TMM.2021.3053393
NR 45
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7160
EP 7173
DI 10.1109/TMM.2024.3360689
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000038
DA 2024-08-05
ER

PT J
AU Zhang, Y
   Su, YN
   Sun, XY
AF Zhang, Yan
   Su, Yuning
   Sun, Xiaoying
TI A QoE Physiological Measure of VR With Vibrotactile Feedback Based on
   Frontal Lobe Power Asymmetry
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; Physiology; Electroencephalography; Correlation;
   Power measurement; Frontal lobe; Psychology; Frontal lobe power
   asymmetry; physiological measurement; quality of experience;
   vibrotactile feedback; virtual reality
ID EEG ASYMMETRY; QUALITY
AB Quality of experience (QoE) has been widely recognized as the primary metric to evaluate user experience in multimedia applications. However, the QoE assessment of tactile virtual environments is still highly dependent on subjective measures. Inspired by the fact that physiological signals can characterize the user's emotional state, we propose a QoE measurement method for virtual reality (VR) with vibrotactile feedback based on frontal lobe power asymmetry (FLPA). The subjective score of vibrotactile experience in VR is used as the ground truth of QoE. The selection of QoE measurement indicators consists of two steps. First, the relationship between FLPA phenomenon and scores of QoE is preliminarily established by statistical methods and Spearman Correlation Coefficient. Then, the most important FLPA feature is selected by random forest, which is the best indicator for QoE measurement. The brain neural images show that vibrotactile feedback in VR can evoke FLPA phenomenon. Correlation analysis shows that there is a significant correlation between subjective scores of QoE and FLPA features. The classification results show that the selected best FLPA feature can be used as a physiological indicator to measure and predict QoE. We achieve mutual interpretation of EEG-based physiological measurements and subjective cognitive outcomes of QoE.
C1 [Zhang, Yan; Sun, Xiaoying] Jilin Univ, Coll Commun Engn, Changchun 130012, Peoples R China.
   [Su, Yuning] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China.
C3 Jilin University; Jilin University
RP Sun, XY (corresponding author), Jilin Univ, Coll Commun Engn, Changchun 130012, Peoples R China.
EM yanz19@mails.jlu.edu.cn; suyn19@mails.jlu.edu.cn; sunxy@jlu.edu.cn
OI Su, Yuning/0009-0004-9350-4060
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2016, ITU-R Rec. 18001.
   [Anonymous], 2016, ITU-R Rec. P.800.2.
   Baumani R., 2019, Maltimedia Tools Appl, V78, P18113
   Coan JA, 2004, BIOL PSYCHOL, V67, P7, DOI 10.1016/j.biopsycho.2004.03.002
   Comsa IS, 2020, IEEE MULTIMEDIA, V27, P27, DOI 10.1109/MMUL.2019.2954405
   DAVIDSON RJ, 1979, PSYCHOPHYSIOLOGY, V16, P202
   Davidson RJ, 2004, BIOL PSYCHOL, V67, P219, DOI 10.1016/j.biopsycho.2004.03.008
   Engelke U, 2017, IEEE J-STSP, V11, P6, DOI 10.1109/JSTSP.2016.2609843
   Gan Y., 2022, IEEE Trans, Multimedia., DOI [10.1109/TMM.20223199666, DOI 10.1109/TMM.20223199666]
   Golnaz B., 2021, Chaos, Solitons Fractals., V142
   Hagemann D., 1999, Pers. Individual Differences., V27, P341
   Hamam A, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540991
   Hamam A, 2013, IEEE T INSTRUM MEAS, V62, P3315, DOI 10.1109/TIM.2013.2272859
   Hu I, 2019, EEG Signal Processing and Feature Extraction
   ITU-T, 2007, Amendment for ITU-T-P.10, new Appendix I: Definition of quality of experience (QoE).
   Jung S, 2021, IEEE T VIS COMPUT GR, V27, P2669, DOI 10.1109/TVCG.2021.3067773
   Katsigiannis S, 2019, IEEE T CONSUM ELECTR, V65, P119, DOI 10.1109/TCE.2018.2879065
   Kawamoto Taishi, 2013, Neurosci J, V2013, P304674, DOI 10.1155/2013/304674
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2589, DOI 10.1109/TMM.2019.2903722
   Linetal C-T., 2007, IEEE Trans. Biomed. Eng, V54, P1349
   McGlone F, 2014, NEURON, V82, P737, DOI 10.1016/j.neuron.2014.05.001
   Moghimi M, 2020, IEEE T AFFECT COMPUT, V11, P45, DOI 10.1109/TAFFC.2017.2764896
   Özdenizci O, 2021, BIOMED SIGNAL PROCES, V67, DOI 10.1016/j.bspc.2021.102507
   Papousek I, 2011, PERS INDIV DIFFER, V51, P1018, DOI 10.1016/j.paid.2011.08.013
   Petrantonakis PC, 2012, IEEE T SIGNAL PROCES, V60, P2604, DOI 10.1109/TSP.2012.2187647
   Pezzulli S, 2021, IEEE T MULTIMEDIA, V23, P2505, DOI 10.1109/TMM.2020.3013349
   Ranasinghe N., 2018, PROC CHI C HAM FACTO, P1
   Reznik SJ, 2018, PSYCHOPHYSIOLOGY, V55, DOI 10.1111/psyp.12965
   Rodrigues TB, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0230570
   Russell JA, 2003, PSYCHOL REV, V110, P145, DOI 10.1037/0033-295X.110.1.145
   Smith EE, 2017, INT J PSYCHOPHYSIOL, V111, P98, DOI 10.1016/j.ijpsycho.2016.11.005
   Song JR, 2016, IEEE T MULTIMEDIA, V18, P444, DOI 10.1109/TMM.2016.2520090
   Su YN, 2021, 2021 IEEE WORLD HAPTICS CONFERENCE (WHC), P361, DOI 10.1109/WHC49131.2021.9517202
   Tao XM, 2019, IEEE J SEL AREA COMM, V37, P1549, DOI 10.1109/JSAC.2019.2916453
   Wacker J, 2018, PSYCHOPHYSIOLOGY, V55, DOI 10.1111/psyp.12727
   Xue T, 2023, IEEE T MULTIMEDIA, V25, P243, DOI 10.1109/TMM.2021.3124080
   Zhao L, 2021, IEEE T VIS COMPUT GR, V27, P2618, DOI 10.1109/TVCG.2021.3067778
NR 37
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2932
EP 2942
DI 10.1109/TMM.2023.3305813
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400007
DA 2024-08-05
ER

PT J
AU Zhang, YF
   Liu, C
   Zhou, Y
   Wang, WP
   Ye, QX
   Ji, XY
AF Zhang, Yifei
   Liu, Chang
   Zhou, Yu
   Wang, Weiping
   Ye, Qixiang
   Ji, Xiangyang
TI Beyond Instance Discrimination: Relation-Aware Contrastive
   Self-Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Global distribution relation; local interpolation relation;
   relation-aware contrastive self-supervised learning; self-supervised
   learning
ID REPRESENTATION
AB Contrastive self-supervised learning (CSL) based on instance discrimination typically attracts positive samples while repelling negatives to learn representations with pre-defined binary self-supervision. However, vanilla CSL is inadequate in modeling sophisticated instance relations, limiting the learned model to retain fine semantic structure. On the one hand, samples with the same semantic category are inevitably pushed away as negatives. On the other hand, differences among samples cannot be captured. In this paper, we present relation-aware contrastive self-supervised learning (ReCo) to integrate instance relations, i.e., global distribution relation and local interpolation relation, into the CSL framework in a plug-and-play fashion. Specifically, we align similarity distributions calculated between the positive anchor views and the negatives at the global level to exploit diverse similarity relations among instances. Local-level interpolation consistency between the pixel space and the feature space is applied to quantitatively model the feature differences of samples with distinct apparent similarities. Through explicitly instance relation modeling, our ReCo avoids irrationally pushing away semantically identical samples and carves a well-structured feature space. Extensive experiments conducted on commonly used benchmarks justify that our ReCo consistently gains remarkable performance improvements.
C1 [Zhang, Yifei; Zhou, Yu; Wang, Weiping] Chinese Acad Sci, Inst Informat Engn, Beijing 100089, Peoples R China.
   [Zhang, Yifei; Zhou, Yu; Wang, Weiping] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100089, Peoples R China.
   [Liu, Chang; Ji, Xiangyang] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Ye, Qixiang] Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 101408, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Tsinghua University; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Zhou, Y (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100089, Peoples R China.
EM zhangyifei0115@iie.ac.cn; liuchang2022@tsinghua.edu.cn;
   zhouyu@iie.ac.cn; wangweiping@iie.ac.cn; qxye@ucas.ac.cn;
   xyji@tsinghua.edu.cn
OI ZHOU, Yu/0000-0003-4188-9953; Liu, Chang/0000-0001-6747-0646; ye, qi
   xiang/0000-0003-1215-6259
FU National Natural Science Foundation of China
FX No Statement Available
CR Aila T, 2017, INT C LEARN REPR
   Arora S, 2019, PR MACH LEARN RES, V97
   Asano Y. M., 2020, PROC INT C LEARN REP
   Bardes A., 2022, PROC INT C LEARN REP
   Berthelot D, 2019, ADV NEUR IN, V32
   Cao CT, 2024, Arxiv, DOI arXiv:2212.10888
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen PG, 2021, PROC CVPR IEEE, P11521, DOI 10.1109/CVPR46437.2021.01136
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Dwibedi D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9568, DOI 10.1109/ICCV48922.2021.00945
   Ericsson L, 2021, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR46437.2021.00537
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan HH, 2022, IEEE T CYBERNETICS, V52, P8851, DOI 10.1109/TCYB.2021.3054978
   Fang QK, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P7050
   Fang Z., 2021, PROC INT C LEARN REP
   Feng WX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10150, DOI 10.1109/ICCV48922.2021.01001
   Feng ZY, 2019, PROC CVPR IEEE, P10356, DOI 10.1109/CVPR.2019.01061
   Gidaris S, 2018, ICLR, DOI DOI 10.1109/ICCV.2019.00156
   Grill JB, 2020, P 34 INT C NEUR INF, V33, P21271
   Guo YF, 2022, PROC CVPR IEEE, P9696, DOI 10.1109/CVPR52688.2022.00948
   Hao XS, 2023, IEEE WINT CONF APPL, P379, DOI 10.1109/WACVW58289.2023.00042
   He H., 2020, P IEEE CVF C COMP VI, P9729
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu QJ, 2021, PROC CVPR IEEE, P1074, DOI 10.1109/CVPR46437.2021.00113
   Huang Jiabo, 2019, P MACHINE LEARNING R, V97
   Huo XY, 2022, IEEE T MULTIMEDIA, V24, P4224, DOI 10.1109/TMM.2021.3115335
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kalantidis Y., 2020, ADV NEURAL INFORM PR, V33, P21798
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kim S, 2020, Arxiv, DOI arXiv:2010.06300
   Krizhevsky A, 2009, CIFAR-10 dataset
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kullback S., 1978, Information Theory and Statistics
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Li J., 2021, PROC INT C LEARN REP
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li T, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE & ENGINEERING, VOLS 1 AND 2, P531
   Li Z., 2021, P IEEECVF C COMPUTER, P9767
   Ma LF, 2023, IEEE T MULTIMEDIA, V25, P2774, DOI 10.1109/TMM.2022.3151145
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Milbich T, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107107
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng XY, 2022, PROC CVPR IEEE, P16010, DOI 10.1109/CVPR52688.2022.01556
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shen ZQ, 2022, AAAI CONF ARTIF INTE, P2216
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song GL, 2021, IEEE T MULTIMEDIA, V23, P1882, DOI 10.1109/TMM.2020.3004963
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tejankar A., 2021, INT C COMP VIS, P9609
   Tian Y., 2020, Advances in neural information processing systems, V33, P6827, DOI DOI 10.5555/3495724.3496297
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Van Der Maaten L., 2009, P MACHINE LEARNING R, P384
   Verma V, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3635
   Wang F, 2020, ACCOUNT RES, V27, P271, DOI 10.1080/08989621.2020.1756265
   Wang F, 2021, PROC CVPR IEEE, P2495, DOI 10.1109/CVPR46437.2021.00252
   Wang T., 2020, International Conference on Machine Learning, P9929
   Wang X, 2023, IEEE T PATTERN ANAL, V45, P5549, DOI 10.1109/TPAMI.2022.3203630
   Wang ZW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10316, DOI 10.1109/ICCV48922.2021.01017
   Wei C., 2021, PROC INT C LEARN REP
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yoon S, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P3225
   Yuan X, 2021, PROC CVPR IEEE, P6991, DOI 10.1109/CVPR46437.2021.00692
   Yun S., 2020, arXiv
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zbontar J, 2021, PR MACH LEARN RES, V139
   Zhan XH, 2020, PROC CVPR IEEE, P6687, DOI 10.1109/CVPR42600.2020.00672
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang CJ, 2018, IEEE T MULTIMEDIA, V20, P903, DOI 10.1109/TMM.2017.2759500
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76
   Zhang YH, 2023, IEEE T MULTIMEDIA, V25, P1749, DOI 10.1109/TMM.2022.3158069
   Zhao YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10140, DOI 10.1109/ICCV48922.2021.01000
   Zheng Mingkai, 2021, ADV NEURAL INF PROCE, P2543
   Zhu R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10286, DOI 10.1109/ICCV48922.2021.01014
   Zhuang CX, 2019, IEEE I CONF COMP VIS, P6001, DOI 10.1109/ICCV.2019.00610
NR 94
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4628
EP 4640
DI 10.1109/TMM.2023.3324588
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100004
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, Y
   Lin, HQ
   Sun, J
   Zhu, LW
   Kwong, S
AF Zhang, Yun
   Lin, Haoqin
   Sun, Jing
   Zhu, Linwei
   Kwong, Sam
TI Learning to Predict Object-Wise Just Recognizable Distortion for Image
   and Video Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; just recognizable distortion; object detection; video
   coding for machine
ID NOTICEABLE DIFFERENCE
AB Just Recognizable Distortion (JRD) refers to the minimum distortion that notably affects the recognition performance of a machine vision model. If a distortion added to images or videos falls within this JRD threshold, the degradation of the recognition performance will be unnoticeable. Based on this JRD property, it will be useful to Video Coding for Machine (VCM) to minimize the bit rate while maintaining the recognition performance of compressed images. In this study, we propose a deep learning-based JRD prediction model for image and video compression. We first construct a large image dataset of Object-Wise JRD (OW-JRD) containing 29 218 original images with 80 object categories, and each image was compressed into 64 distorted versions using Versatile Video Coding (VVC). Secondly, we analyze the distribution of the OW-JRD, formulate JRD prediction as binary classification problems and propose a deep learning-based OW-JRD prediction framework. Thirdly, we propose a deep learning based binary OW-JRD predictor to predict whether an image object is still detectable or not under different compression levels. Also, we propose an error-tolerance strategy that corrects misclassifications from the binary classifier. Finally, extensive experiments on large JRD image datasets demonstrate that the Mean Absolute Errors (MAEs) of the predicted OW-JRD are 4.90 and 5.92 on different numbers of the classes, which is significantly better than the state-of-the-art JRD prediction model. Moreover, ablation studies on deep network structures, object sizes, features, data padding strategies and image/video coding schemes are presented to validate the effectiveness of the proposed JRD model.
C1 [Zhang, Yun] Sun Yat Sen Univ, Sch Elect & Commun Engn, Shenzhen 518107, Peoples R China.
   [Lin, Haoqin; Sun, Jing; Zhu, Linwei] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Kwong, Sam] Lingnan Univ, Dept Comp & Decis Sci, Hong Kong 999077, Peoples R China.
C3 Sun Yat Sen University; Chinese Academy of Sciences; Shenzhen Institute
   of Advanced Technology, CAS; Lingnan University
RP Zhang, Y (corresponding author), Sun Yat Sen Univ, Sch Elect & Commun Engn, Shenzhen 518107, Peoples R China.; Zhu, LW (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM zhangyun2@mail.sysu.edu.cn; hq.lin@siat.ac.cn; sun.jing1@siat.ac.cn;
   lw.zhu@siat.ac.cn; samkwong@ln.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261
FU National Natural Science Foundation of China
FX No Statement Available
CR Bae SH, 2017, IEEE T CIRC SYST VID, V27, P1196, DOI 10.1109/TCSVT.2016.2539862
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Chen ZZ, 2020, IEEE T CIRC SYST VID, V30, P4064, DOI 10.1109/TCSVT.2019.2952675
   Chen Z, 2020, IEEE T IMAGE PROCESS, V29, P2230, DOI 10.1109/TIP.2019.2941660
   Choi H, 2022, IEEE T IMAGE PROCESS, V31, P2739, DOI 10.1109/TIP.2022.3160602
   Duan LY, 2020, IEEE T IMAGE PROCESS, V29, P8680, DOI 10.1109/TIP.2020.3016485
   Feng RY, 2022, LECT NOTES COMPUT SC, V13697, P510, DOI 10.1007/978-3-031-19836-6_29
   Gao W., 2021, arXiv
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Jiang QP, 2022, IEEE T IMAGE PROCESS, V31, P3697, DOI 10.1109/TIP.2022.3174398
   Jin J, 2022, IEEE T CIRC SYST VID, V32, P3452, DOI 10.1109/TCSVT.2021.3113572
   Lin Haoqin, 2023, IEEE DataPort, DOI 10.21227/Q5FQ-D638
   Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743
   Misra K., 2022, P IEEE INT C MULT EX, P1
   Nami S, 2023, IEEE T MULTIMEDIA, V25, P5077, DOI 10.1109/TMM.2022.3187259
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shitole V., 2021, NeurIPS, V34, P11352
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan M., 2021, P INT C MACHINE LEAR, DOI DOI 10.48550/ARXIV.2104.00298
   Tian Y, 2022, arXiv
   Wang SQ, 2016, IEEE T IMAGE PROCESS, V25, P3838, DOI 10.1109/TIP.2016.2573597
   Wang SR, 2022, IEEE T MULTIMEDIA, V24, P3169, DOI 10.1109/TMM.2021.3094300
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu Y. Yang, 2023, IEEE Trans. Multimedia, DOI [10.1109/TMM.2023.3306072.[4]Y., DOI 10.1109/TMM.2023.3306072.[4]Y]
   Yang S, 2021, IEEE T MULTIMEDIA, V23, P2957, DOI 10.1109/TMM.2021.3068580
   Zhang Q, 2024, Arxiv, DOI arXiv:2211.06797
   Zhang Q, 2021, INT J COMPUT VISION, V129, P2889, DOI 10.1007/s11263-021-01505-4
   Zhang XY, 2019, IEEE IMAGE PROC, P4140, DOI [10.1109/ICIP.2019.8803454, 10.1109/icip.2019.8803454]
   Zhang Y., 2020, ISO/IEC JTC 1/SC 29/WG 11, document: w19365
   Zhang Y, 2023, IEEE T IMAGE PROCESS, V32, P5933, DOI 10.1109/TIP.2023.3327003
   Zhang Y, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3571727
   Zhang Y, 2022, IEEE T CIRC SYST VID, V32, P1197, DOI 10.1109/TCSVT.2021.3076224
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 35
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5925
EP 5938
DI 10.1109/TMM.2023.3340882
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100030
DA 2024-08-05
ER

PT J
AU Zhao, KJ
   Zhao, HT
   Wang, ZZ
   Peng, JC
   Hu, ZW
AF Zhao, Kaijie
   Zhao, Haitao
   Wang, Zhongze
   Peng, Jingchao
   Hu, Zhengwei
TI Object-Preserving Siamese Network for Single-Object Tracking on Point
   Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Search problems; Feature extraction; Point
   cloud compression; Location awareness; Task analysis; Deep learning;
   Object tracking; Siamese network; point clouds; autonomous driving; deep
   learning
AB Undoubtedly, the object is the primary factor in 3D single-object tracking (SOT) tasks. However, prior Siamese-based trackers overlook the adverse effects resulting from randomly dropped object points during backbone sampling, hindering the prediction of accurate bounding boxes (BBoxes). Therefore, developing an approach that maximizes the preservation of object points and their object-aware features is of the utmost significance. To address this, we propose an object-preserving Siamese network (OPSNet) that can effectively maintain object integrity and boost tracking performance. First, an object<bold> </bold>highlighting module amplifies the object-aware features and extracts discriminative features from the template and search area. Next, object-preserving sampling selects object candidates, obtains object-preserving search area seeds, and discards background points that have less impact on tracking. Finally, an object localization network accurately locates 3D BBoxes based on the object-preserving search area seeds. Extensive experiments demonstrate that the performance of OPSNet exceeds the state-of-the-art performance, achieving success gains of similar to 9.4% and similar to 2.5% on the KITTI and Waymo Open datasets, respectively.
C1 [Zhao, Kaijie; Zhao, Haitao; Wang, Zhongze; Peng, Jingchao; Hu, Zhengwei] East China Univ Sci & Technol, Sch Informat Sci & Engn, Automat Dept, Shanghai 200237, Peoples R China.
C3 East China University of Science & Technology
RP Zhao, HT (corresponding author), East China Univ Sci & Technol, Sch Informat Sci & Engn, Automat Dept, Shanghai 200237, Peoples R China.
EM kjzhao@mail.ecust.edu.cn; haitaozhao@ecust.edu.cn;
   zzwang@mail.ecust.edu.cn; pjc@mail.ecust.edu.cn; zwh@mail.ecust.edu.cn
OI Wang, Zhongze/0009-0002-6938-2724
FU National Natural Science Foundation of China
FX No Statement Available
CR Brodeur T, 2021, IEEE INT CONF COMP V, P3910, DOI 10.1109/ICCVW54120.2021.00438
   Budiharto Widodo, 2020, ICIC Express Letters, V14, P83, DOI 10.24507/icicel.14.03.289
   Cui Y., 2021, arXiv
   Ding ZP, 2019, LECT NOTES COMPUT SC, V11766, P202, DOI [10.1007/978-3-030-32248-9_23, 10.1007/978-3-030-32248-9]
   Ge R., 2020, arXiv
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Giancola S, 2019, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2019.00145
   Han XF, 2023, IEEE T MULTIMEDIA, V25, P5638, DOI 10.1109/TMM.2022.3198318
   Hui L, 2021, ADV NEUR IN, V34
   Hui H, 2022, LECT NOTES COMPUT SC, V13662, P293, DOI 10.1007/978-3-031-20086-1_17
   Jiang MX, 2021, NEUROCOMPUTING, V434, P268, DOI 10.1016/j.neucom.2020.12.113
   Junliang Xing, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1698, DOI 10.1109/ICPR.2010.420
   Khan A. A., 2022, IEEE Transactions on Multimedia, DOI DOI 10.1109/TMM.2022.3230330
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu H, 2023, IEEE T MULTIMEDIA, V25, P8793, DOI 10.1109/TMM.2023.3241548
   Ma LF, 2021, IEEE T INTELL TRANSP, V22, P821, DOI 10.1109/TITS.2019.2961060
   Nie J, 2022, P AAAI C ART INT, V37, P1957
   Nie JH, 2023, Arxiv, DOI arXiv:2304.00242
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi HZ, 2020, PROC CVPR IEEE, P6328, DOI 10.1109/CVPR42600.2020.00636
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394
   Wang S., 2023, IEEE Trans. Multimedia, early access, DOI DOI 10.1109/TMM.2023.3245359
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang B, 2022, IEEE T IMAGE PROCESS, V31, P5828, DOI 10.1109/TIP.2022.3202367
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang WR, 2022, IEEE SIGNAL PROC LET, V29, P787, DOI 10.1109/LSP.2022.3154317
   Zhang YF, 2022, PROC CVPR IEEE, P18931, DOI 10.1109/CVPR52688.2022.01838
   Zheng C., 2021, PROC IEEECVF INT C C, P13199
   Zheng C., 2022, P IEEE CVF C COMP VI, P8111
   Zhou CQ, 2022, PROC CVPR IEEE, P8521, DOI 10.1109/CVPR52688.2022.00834
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhuang Y, 2022, INT J MACH LEARN CYB, V13, P2105, DOI 10.1007/s13042-022-01508-8
NR 40
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3007
EP 3017
DI 10.1109/TMM.2023.3306490
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700005
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zuo, LY
   Wang, BY
   Zhang, L
   Xu, J
   Zhen, XT
AF Zuo, Liyun
   Wang, Baoyan
   Zhang, Lei
   Xu, Jun
   Zhen, Xiantong
TI Variational Neuron Shifting for Few-Shot Image Classification Across
   Domains
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Meta learning; few-shot image classification; domain generalization;
   variational inference
ID DATASET; NETWORK
AB Few-shot image classification aims to recognize unseen classes with few labeled samples. Existing meta-learning models learn the ability of learning good representation or model parameters, in order to adapt to new tasks with a few training samples. However, when there exists a domain gap between training and test tasks, the learned ability often does not generalize well across domains, resulting in degraded performance on new tasks. In this article, we propose variational neuron shifting to generate adapted feature representations for few-shot learning. To do so, we introduce a working memory module to store the shifted neurons from the support set, which will be accessed to generate adapted feature representations of query samples. Under the meta-learning paradigm, the model is learned to acquire the ability of adaptation with single sample at meta-training time so as to further adapt itself to each single test sample at meta-test time. We formulate the adaptation process as a variational Bayesian inference problem, which incorporates the test sample as the condition into the generation of the model neuron shifting. We conduct extensive experiments on both within and across domain few-shot classification tasks. The new state-of-the-art performance substantiates the effectiveness of our variational neuron shifting. The thorough ablation studies further demonstrate the benefit of each component in our model.
C1 [Zuo, Liyun; Wang, Baoyan; Zhang, Lei; Zhen, Xiantong] Guangdong Univ Petrochem Technol, Maoming 525011, Peoples R China.
   [Xu, Jun] Nankai Univ, Sch Stat & Data Sci, Tianjin 300071, Peoples R China.
C3 Guangdong University of Petrochemical Technology; Nankai University
RP Zhen, XT (corresponding author), Guangdong Univ Petrochem Technol, Maoming 525011, Peoples R China.
EM liyunzuo@foxmail.com; wangbaoyan2020@gdupt.edu.cn;
   zhanglei@gdupt.edu.cn; nankaimathxujun@gmail.com; zhenxt@gmail.com
OI Zhang, Lei/0000-0002-8494-0504
FU National Key Ramp;D Program of China
FX No Statement Available
CR Allen KR, 2019, PR MACH LEARN RES, V97
   Arnsten AFT, 2009, NAT REV NEUROSCI, V10, P410, DOI 10.1038/nrn2648
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Bateni Peyman, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14481, DOI 10.1109/CVPR42600.2020.01450
   Bateni P, 2022, IEEE WINT CONF APPL, P1597, DOI 10.1109/WACV51458.2022.00166
   Bertinetto J. F., 2019, INT C LEARN REP RESE, P1
   Bornschein A., 2017, 31 ANN C NEURALINF P, P1
   Chen TI, 2023, IEEE T MULTIMEDIA, V25, P291, DOI 10.1109/TMM.2021.3125195
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Doersch C., 2020, NeurIPS, V33, P21981
   Du X., 2022, INT C LEARNREPRESENT, P1
   Du Y., 2021, PROC INT C LEARN REP, P1
   Dvornik Nikita, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P769, DOI 10.1007/978-3-030-58607-2_45
   Fei N., 2021, INT C LEARN REPR
   Fink M., 2005, 2005 Advances in Neural Information Processing Systems, P449
   Finn C, 2018, ADV NEUR IN, V31
   Finn C, 2017, PR MACH LEARN RES, V70
   Garcia Victor., 2017, 6 INT C LEARN REPR
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Gordon J, 2019, PROC INT C LEARN REP
   Gordon J., 2018, 3 WORKSHOPBAYESIAN D, P1
   Grant E., 2018, PROC INT C LEARN REP, P1
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Hao FS, 2019, IEEE I CONF COMP VIS, P8459, DOI 10.1109/ICCV.2019.00855
   Hasanzadeh A., 2019, PROC NIPS, P10711
   Helber P, 2019, IEEE J-STARS, V12, P2217, DOI 10.1109/JSTARS.2019.2918242
   Houben S, 2013, IEEE IJCNN
   Hu SX, 2022, PROC CVPR IEEE, P9058, DOI 10.1109/CVPR52688.2022.00886
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Huang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8651, DOI 10.1109/ICCV48922.2021.00855
   Islam A, 2021, P ADV NEUR INF PROC, P3584
   Jeong M, 2021, PROC CVPR IEEE, P12561, DOI 10.1109/CVPR46437.2021.01238
   Jongejan J., 2016, The quick, draw!-ai experiment, V17, P4
   Kaiser O., 2017, INT C LEARN REPRESEN, P1
   Kingma D. P., 2014, arXiv
   Kohl SAA, 2018, ADV NEUR IN, V31
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Laenen S, 2021, ADV NEUR IN, V34
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Le DH, 2021, ADV NEUR IN, V34
   Le H., 2018, 32 ANN C NEURAL INF, V31, P1508
   Le J., 2021, INT C LEARNREPRESENT, P1
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li WH, 2022, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR52688.2022.00702
   Li X., 2021, IEEECVF INTCONF COMP, P9526
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W. L., 2020, INT C LEARN REPRESEN
   Liu Y., 2018, PROC INT C LEARN REP
   Luo X, 2021, ADV NEUR IN, V34
   Luo Z., 2017, P 31 INT C NEUR INF, P165
   Maji E., 2013, Tech. Rep.
   Mathieu E, 2019, ADV NEUR IN, V32
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Montague PR, 1996, J NEUROSCI, V16, P1936, DOI 10.1523/jneurosci.16-05-01936.1996
   Motiian S., 2017, ADV NEURAL INFORM PR, V30, P6670
   Munkhdalai T, 2019, ADV NEUR IN, V32
   Munkhdalai T, 2018, PR MACH LEARN RES, V80
   Munkhdalai Tsendsuren, 2017, Proc Mach Learn Res, V70, P2554
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Phaphuangwittayakul A, 2022, IEEE T MULTIMEDIA, V24, P2205, DOI 10.1109/TMM.2021.3077729
   Phoo B., 2020, INT C LEARN REPRESEN, P1
   Pritzel A., 2017, P 34 INT C MACHINE L, P2827
   Ramalho M., 2019, INT CONFLEARN REPRES, P1
   Ravi S., 2017, C TRACK P, P1
   Ren M., 2018, 6 INT C LEARN REPR
   Requeima J., 2019, ANN C NEURAL INF PRO, P1
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saikia T, 2020, Arxiv, DOI arXiv:2001.07926
   Santoro A, 2016, PR MACH LEARN RES, V48
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Schroeder Y., 2018, FGVCx fungi classification challenge
   Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419
   Sinha S, 2019, IEEE I CONF COMP VIS, P5971, DOI 10.1109/ICCV.2019.00607
   Snell J., 2017, Advances in Neural Information Processing Systems, V30, P4077
   Sohn K, 2015, ADV NEUR IN, V28
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Sygnowski J., 2019, METALEARNING LATENT
   Thrun S., 2012, Learning to learn
   Triantafillou E, 2020, P INT C LEARN REPR I
   Triantafillou E, 2021, PR MACH LEARN RES, V139, P7435
   Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161
   Tseng H.-Y., 2020, PROC INT C LEARN REP, P1
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Vuorio S.-H., 2019, ANN C NEURALINF PROC, P1
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Haoqing, 2021, P 30 INT JOINT C ART, P1075
   Wang JX, 2018, NAT NEUROSCI, V21, P860, DOI 10.1038/s41593-018-0147-8
   Wang XS, 2017, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2017.369
   Wang Zhili, 2021, Advances in Neural Information Processing Systems, V34
   Xiao X., 2022, INT C LEARN REPRESEN, P1
   Xiao Zehao, 2021, PR MACH LEARN RES
   Yingjun Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P200, DOI 10.1007/978-3-030-58607-2_12
   Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16
   Yoon J, 2018, ADV NEUR IN, V31
   Yoon SW, 2019, PR MACH LEARN RES, V97
   Yunhui Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P124, DOI 10.1007/978-3-030-58583-9_8
   Zhang J, 2019, IEEE I CONF COMP VIS, P1685, DOI 10.1109/ICCV.2019.00177
   Zhang L, 2021, IEEE T CIRC SYST VID, V31, P4283, DOI 10.1109/TCSVT.2021.3052785
   Zhang Xueting, 2021, P IEEECVF INT C COMP, P651
   Zhen X., 2020, Advances in Neural Information Processing Systems, P9122
   Zhen X., 2020, ICML
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
   Zintgraf L., 2019, INT C MACHINE LEARNI, V97, P7693
NR 109
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1460
EP 1473
DI 10.1109/TMM.2023.3282492
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700033
DA 2024-08-05
ER

PT J
AU Chen, T
   Guo, YR
   Hao, SJ
   Hong, RC
AF Chen, Tao
   Guo, Yanrong
   Hao, Shijie
   Hong, Richang
TI Semi-Supervised Domain Adaptation for Major Depressive Disorder
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Class imbalance; graph neural network; MDD detection; pseudo-label
   optimization; semi-supervised domain adaptation; uncertainty estimation
ID MENTAL-DISORDERS; SOCIAL MEDIA; NETWORK; REPRESENTATION; CLASSIFICATION
AB Major Depressive Disorder (MDD) detection with cross-domain datasets is a crucial yet challenging application due to the data scarcity and isolated data island issues in multimedia computing research. Given the domain shift issue in MDD datasets and a continuous stream of incoming data in clinical settings, Semi-supervised Domain Adaptation (SDA) is suitable for addressing these challenges in MDD detection. However, existing mainstream Domain Adaptation (DA) methods have the following limitations that still need to be addressed, such as semantic misalignment, challenges in extending to various DA paradigms, and difficulty in addressing the classifier bias caused by class imbalance issues. To relieve the above issues, we propose a flexible Graph Neural Network-based Semi-supervised Domain Adaptation (GNN-SDA) for MDD detection. The proposed framework comprises a feature extraction backbone along with two essential modules: a GNN-based domain alignment module and an uncertainty-guided optimization module. The GNN-based domain alignment module is designed to reduce the domain gap in a flexible manner, which is able to align multiple domains through the information propagation mechanism instead of the explicit alignment operation. The uncertainty-guided optimization module discusses the uncertainty of pseudo-labels, mitigating the adverse impact of noisy predictions and taking into account the class distribution of unlabeled data. Finally, we evaluate the proposed GNN-SDA framework for MDD detection under different domain adaptation paradigms on four benchmark datasets, i.e., DAIC-WOZ, EATD, CMDC, and MODMA. The promising results indicate the flexibility and effectiveness of the proposed framework for MDD detection.
C1 [Chen, Tao; Guo, Yanrong; Hao, Shijie; Hong, Richang] Hefei Univ Technol, Key Lab Knowledge Engn withBig Data, Hefei 230009, Peoples R China.
   [Chen, Tao; Guo, Yanrong; Hao, Shijie; Hong, Richang] Hefei Univ Technol, Minist Educ, Hefei 230009, Peoples R China.
   [Chen, Tao; Guo, Yanrong; Hao, Shijie; Hong, Richang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology; Hefei
   University of Technology
RP Guo, YR; Hong, RC (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn withBig Data, Hefei 230009, Peoples R China.; Guo, YR; Hong, RC (corresponding author), Hefei Univ Technol, Minist Educ, Hefei 230009, Peoples R China.; Guo, YR; Hong, RC (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
EM 2017110927@mail.hfut.edu.cn; yrguo@hfut.edu.cn; hfut.hsj@gmail.com;
   hongrc.hfut@gmail.com
OI Hao, Shijie/0000-0003-3181-1220
FU National Key Research and Development Program of China
FX No Statement Available
CR Al Jazaery M, 2021, IEEE T AFFECT COMPUT, V12, P262, DOI 10.1109/TAFFC.2018.2870884
   Alghowinem S, 2018, IEEE T AFFECT COMPUT, V9, P478, DOI 10.1109/TAFFC.2016.2634527
   Aragón ME, 2023, IEEE T AFFECT COMPUT, V14, P211, DOI 10.1109/TAFFC.2021.3075638
   Baki P, 2022, IEEE T AFFECT COMPUT, V13, P2119, DOI 10.1109/TAFFC.2022.3193054
   Bone D, 2014, IEEE T AFFECT COMPUT, V5, P201, DOI 10.1109/TAFFC.2014.2326393
   Cai HS, 2020, Arxiv, DOI arXiv:2002.09283
   Cai HS, 2020, IEEE T AFFECT COMPUT, V11, P383, DOI 10.1109/TAFFC.2018.2801289
   Cao L, 2022, IEEE T MULTIMEDIA, V24, P87, DOI 10.1109/TMM.2020.3046867
   Cao ZH, 2019, IEEE T BIO-MED ENG, V66, P1668, DOI 10.1109/TBME.2018.2877651
   Chen HF, 2021, IEEE T MULTIMEDIA, V23, P4171, DOI 10.1109/TMM.2020.3037496
   Chen L, 2022, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR52688.2022.00704
   Chen SJ, 2021, PROC CVPR IEEE, P11013, DOI 10.1109/CVPR46437.2021.01087
   Chen T, 2022, IEEE T AFFECT COMPUT, V13, P2106, DOI 10.1109/TAFFC.2022.3210958
   Chen WY, 2019, IEEE T IMAGE PROCESS, V28, P4620, DOI 10.1109/TIP.2019.2912126
   Damodaran B. B., 2018, P EUR C COMP VIS ECC, P447, DOI DOI 10.1007/978-3-030-01225-028
   de Melo WC, 2023, IEEE T AFFECT COMPUT, V14, P578, DOI 10.1109/TAFFC.2021.3072579
   de Melo WC, 2022, IEEE T AFFECT COMPUT, V13, P1581, DOI 10.1109/TAFFC.2020.3021755
   Dong JH, 2021, ADV NEUR IN, V34
   Dundar A, 2021, IEEE T PATTERN ANAL, V43, P2360, DOI 10.1109/TPAMI.2020.2969421
   France DJ, 2000, IEEE T BIO-MED ENG, V47, P829, DOI 10.1109/10.846676
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gao HY, 2021, IEEE T PATTERN ANAL, V43, P4512, DOI 10.1109/TPAMI.2021.3062794
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Hang Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P727, DOI 10.1007/978-3-030-58598-3_43
   Jiang C, 2021, IEEE T NEUR SYS REH, V29, P566, DOI 10.1109/TNSRE.2021.3059429
   Jiang XD, 2023, IEEE T PATTERN ANAL, V45, P7075, DOI 10.1109/TPAMI.2020.3029762
   Jiao YF, 2023, IEEE T PATTERN ANAL, V45, P7338, DOI 10.1109/TPAMI.2022.3218569
   Jiao YF, 2021, IEEE T IMAGE PROCESS, V30, P2155, DOI 10.1109/TIP.2021.3049948
   Jing LL, 2022, INT J OCCUP SAF ERGO, V28, P842, DOI [10.1080/10803548.2020.1835234, 10.1109/TPAMI.2020.2991050]
   Kuang JC, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3196948
   Li B, 2021, PROC CVPR IEEE, P1104, DOI 10.1109/CVPR46437.2021.00116
   Li D., 2020, ECCV, P382
   Li JC, 2021, PROC CVPR IEEE, P2505, DOI 10.1109/CVPR46437.2021.00253
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Mao KN, 2023, IEEE T AFFECT COMPUT, V14, P2251, DOI 10.1109/TAFFC.2022.3154332
   Moore E, 2008, IEEE T BIO-MED ENG, V55, P96, DOI 10.1109/TBME.2007.900562
   Niu MY, 2023, IEEE T AFFECT COMPUT, V14, P294, DOI 10.1109/TAFFC.2020.3031345
   Pampouchidou A, 2019, IEEE T AFFECT COMPUT, V10, P445, DOI 10.1109/TAFFC.2017.2724035
   Ringeval F., 2019, P 9 INT AUD VIS EM C, P3, DOI DOI 10.1145/3347320.3357688
   Roy S, 2021, PROC CVPR IEEE, P5347, DOI 10.1109/CVPR46437.2021.00531
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Seal A, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3053999
   Shen Y, 2022, INT CONF ACOUST SPEE, P6247, DOI 10.1109/ICASSP43922.2022.9746569
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Shuai HH, 2018, IEEE T KNOWL DATA EN, V30, P1212, DOI 10.1109/TKDE.2017.2786695
   Singh A, 2021, Advances in Neural Information Processing Systems, V34
   Song SY, 2022, IEEE T AFFECT COMPUT, V13, P829, DOI 10.1109/TAFFC.2020.2970712
   Nguyen T, 2014, IEEE T AFFECT COMPUT, V5, P217, DOI 10.1109/TAFFC.2014.2315623
   Trotzek M, 2020, IEEE T KNOWL DATA EN, V32, P588, DOI 10.1109/TKDE.2018.2885515
   Uddin MA, 2023, IEEE T AFFECT COMPUT, V14, P2153, DOI 10.1109/TAFFC.2022.3179478
   Wang ML, 2020, IEEE T MED IMAGING, V39, P644, DOI 10.1109/TMI.2019.2933160
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wang W, 2023, IEEE T NEUR NET LEAR, V34, P264, DOI 10.1109/TNNLS.2021.3093468
   Xia HF, 2023, IEEE T PATTERN ANAL, V45, P3434, DOI 10.1109/TPAMI.2022.3174526
   Xia HF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8990, DOI 10.1109/ICCV48922.2021.00888
   Xu XZ, 2022, IEEE T MULTIMEDIA, V24, P2752, DOI 10.1109/TMM.2021.3087098
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yan YG, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2969
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang L, 2021, IEEE T AFFECT COMPUT, V12, P239, DOI 10.1109/TAFFC.2018.2870398
   Yang X, 2022, IEEE T PATTERN ANAL, V44, P1992, DOI 10.1109/TPAMI.2020.3026079
   Zhang W, 2021, IEEE T IND INFORM, V17, P7957, DOI 10.1109/TII.2021.3064377
   Zhao H, 2019, INT C MACHINE LEARNI, P7523
   Zhao H, 2021, IEEE T NEUR NET LEAR, V32, P535, DOI 10.1109/TNNLS.2020.3010780
   Zheng AH, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3113581
   Zheng JH, 2023, IEEE T MULTIMEDIA, V25, P2213, DOI 10.1109/TMM.2022.3144885
   Zhou XZ, 2020, IEEE T AFFECT COMPUT, V11, P542, DOI 10.1109/TAFFC.2018.2828819
   Zhu Y, 2018, IEEE T AFFECT COMPUT, V9, P578, DOI 10.1109/TAFFC.2017.2650899
   Zou BC, 2023, IEEE T AFFECT COMPUT, V14, P2823, DOI 10.1109/TAFFC.2022.3181210
NR 70
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3567
EP 3579
DI 10.1109/TMM.2023.3312917
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200026
DA 2024-08-05
ER

PT J
AU Chen, TR
   Ding, CT
   Zhu, LY
   Zang, Y
   Liao, YY
   Li, ZJ
   Sun, LY
AF Chen, Tianrun
   Ding, Chaotao
   Zhu, Lanyun
   Zang, Ying
   Liao, Yiyi
   Li, Zejian
   Sun, Lingyun
TI Reality3DSketch: Rapid 3D Modeling of Objects From Single Freehand
   Sketches
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Image reconstruction;
   Software; Shape; Estimation; Context modeling; 3D modeling; 3D
   reconstruction; computer/ human interaction; sketch
ID INTERFACE
AB The emerging trend of AR/VR places great demands on 3D content. However, most existing software requires expertise and is difficult for novice users to use. In this paper, we aim to create sketch-based modeling tools for user-friendly 3D modeling. We introduce Reality3DSketch with a novel application of an immersive 3D modeling experience, in which a user can capture the surrounding scene using a monocular RGB camera and can draw a single sketch of an object in the real-time reconstructed 3D scene. A 3D object is generated and placed in the desired location, enabled by our novel neural network with the input of a single sketch. Our neural network can predict the pose of a drawing and can turn a single sketch into a 3D model with view and structural awareness, which addresses the challenge of sparse sketch input and view ambiguity. We conducted extensive experiments synthetic and real-world datasets and achieved state-of-the-art (SOTA) results in both sketch view estimation and 3D modeling performance. According to our user study, our method of performing 3D modeling in a scene is >5x faster than conventional methods. Users are also more satisfied with the generated 3D model than the results of existing methods.
C1 [Chen, Tianrun; Sun, Lingyun] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Ding, Chaotao; Zang, Ying] Huzhou Univ, Sch Informat & Engn, Huzhou 313000, Peoples R China.
   [Zhu, Lanyun] Singapore Univ Technol & Design, Informat Syst Technol & Design Pillar, Singapore 487372, Singapore.
   [Liao, Yiyi] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   [Li, Zejian] Zhejiang Univ, Sch Software Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Huzhou University; Singapore University of
   Technology & Design; Zhejiang University; Zhejiang University
RP Zang, Y (corresponding author), Huzhou Univ, Sch Informat & Engn, Huzhou 313000, Peoples R China.; Liao, YY (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
EM tianrun.chen@zju.edu.cn; 2021388117@stu.zjhu.edu.cn;
   lanyun_zhu@mymail.sutd.edu.sg; 02750@zjhu.edu.cn; yiyi.liao@zju.edu.cn;
   zejianlee@zju.edu.cn; sunly@zju.edu.cn
RI Chen, Tianrun/KYP-2807-2024
OI Chen, Tianrun/0000-0003-0177-0157; Li, Zejian/0000-0001-5313-2742
FU National Key Ramp;D Program of China
FX No Statement Available
CR Albert B., 2022, Measuring the User Experience: Collecting, Analyzing, and Presenting UX Metrics
   Arora R, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173759
   Arora R, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5643, DOI 10.1145/3025453.3025474
   Bhavnani S. K., 1999, PROC SIGCHI C HUM FA, P183
   Bonnici A, 2019, AI EDAM, V33, P370, DOI 10.1017/S0890060419000349
   Cai Yujun, 2021, P IEEECVF INT C COMP, P11645
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen SC, 2022, IEEE MULTIMEDIA, V29, P125, DOI 10.1109/MMUL.2022.3156185
   Chester I, 2007, INT J TECHNOL DES ED, V17, P23, DOI 10.1007/s10798-006-9015-z
   Cohen J. M., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P17, DOI 10.1145/300523.300655
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Deering M.F., 1995, "ACM Trans. Comput.-Hum. Interact., V2, P220
   Deng CY, 2020, COMPUT VIS MEDIA, V6, P279, DOI 10.1007/s41095-019-0153-0
   Do T. V., 2010, Int. J. Math. Comput. Sci., V4, P377
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Gadelha M, 2019, IEEE I CONF COMP VIS, P22, DOI 10.1109/ICCV.2019.00011
   Gingold Y, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1616494, 10.1145/1618452.1618494]
   Giunchi D., 2018, PROC JOINT S COMPUT, P1
   Goh ES, 2019, IEEE ACCESS, V7, P40581, DOI 10.1109/ACCESS.2019.2906394
   Guillard B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13003, DOI 10.1109/ICCV48922.2021.01278
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou YX, 2019, IEEE I CONF COMP VIS, P2651, DOI 10.1109/ICCV.2019.00274
   Hu X., 2018, BMVC, P230
   Huang HB, 2017, IEEE T VIS COMPUT GR, V23, P2003, DOI 10.1109/TVCG.2016.2597830
   Hurst A., 2013, P 12 INT C INT DES C, P635, DOI [DOI 10.1145/2485760.2485883, 10.1145/2485760.2485883]
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Jorge J. A., 2003, PROC RAPID PRODUCT D, P167
   Kar Abhishek, 2017, NeurIPS, P364
   Karras T., 2018, INT C LEARNING REPRE
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Keefe D.F., 2001, P 2001 S INTERACTIVE, P85
   Kwan KC, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300406
   Li CJ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417807
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   Machuca MDB, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364254
   Martinet A, 2012, IEEE T VIS COMPUT GR, V18, P369, DOI 10.1109/TVCG.2011.129
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Michel O, 2022, PROC CVPR IEEE, P13482, DOI 10.1109/CVPR52688.2022.01313
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1962, DOI 10.1109/TMM.2020.3006371
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Oh C, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174223
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Reddy EJ, 2018, ALEX ENG J, V57, P3139, DOI 10.1016/j.aej.2018.07.010
   Renner RS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543590
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Seufert M, 2019, INT WORK QUAL MULTIM
   Shtof A, 2013, COMPUT GRAPH FORUM, V32, P245, DOI 10.1111/cgf.12044
   Sun JM, 2021, PROC CVPR IEEE, P15593, DOI 10.1109/CVPR46437.2021.01534
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang J., 2022, EUROPEAN C COMPUTER, P184
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Wesche H.-P., 2001, P ACM S VIRT REAL SO, P167
   Wiese E., 2010, P 7 SKETCH BASED INT, P135
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu PF, 2019, IEEE T VIS COMPUT GR, V25, P2927, DOI 10.1109/TVCG.2018.2860016
   Xu YZ, 2020, IEEE T MULTIMEDIA, V22, P2950, DOI 10.1109/TMM.2020.2966882
   Yao S., 2022, arXiv
   Zhang SH, 2021, PROC CVPR IEEE, P6008, DOI 10.1109/CVPR46437.2021.00595
   Zheng EL, 2009, INT CONF ACOUST SPEE, P1265, DOI 10.1109/ICASSP.2009.4959821
   Zhu XQ, 2021, PROC CVPR IEEE, P5857, DOI 10.1109/CVPR46437.2021.00580
NR 61
TC 1
Z9 1
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4859
EP 4870
DI 10.1109/TMM.2023.3327533
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100030
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Fang, X
   Liu, DZ
   Zhou, P
   Xu, ZC
   Li, RX
AF Fang, Xiang
   Liu, Daizong
   Zhou, Pan
   Xu, Zichuan
   Li, Ruixuan
TI Hierarchical Local-Global Transformer for Temporal Sentence Grounding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Semantics; Visualization; Grounding; Task analysis;
   Feature extraction; Decoding; Multi-modal representations; multimedia
   understanding; temporal sentence grounding; temporal transformer
ID NETWORK
AB This article studies the multimedia problem of temporal sentence grounding (TSG), which aims to accurately determine the specific video segment in an untrimmed video according to a given sentence query. Traditional TSG methods mainly follow the top-down or bottom-up framework and are not end-to-end. They severely rely on time-consuming post-processing to refine the grounding results. Recently, some transformer-based approaches are proposed to efficiently and effectively model the fine-grained semantic alignment between video and query. Although these methods achieve significant performance to some extent, they equally take frames of the video and words of the query as transformer input for correlating, failing to capture their different levels of granularity with distinct semantics. To address this issue, in this article, we propose a novel <bold>H</bold>ierarchical <bold>L</bold>ocal-<bold>G</bold>lobal <bold>T</bold>ransformer (HLGT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities for learning more fine-grained multi-modal representations. Specifically, we first split the video and query into individual clips and phrases to learn their local context (adjacent dependency) and global correlation (long-range dependency) via a temporal transformer. Then, a global-local transformer is introduced to learn the interactions between the local-level and global-level semantics for better multi-modal reasoning. Besides, we develop a new cross-modal cycle-consistency loss to enforce interaction between two modalities and encourage the semantic alignment between them. Finally, we design a brand-new cross-modal parallel transformer decoder to integrate the encoded visual and textual features for final grounding. Extensive experiments on three challenging datasets (ActivityNet Captions, Charades-STA and TACoS) show that our proposed HLGT achieves a new state-of-the-art performance, demonstrating its effectiveness and computational efficiency.
C1 [Fang, Xiang; Zhou, Pan] Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Hubei Engn Res Ctr Big Data Secur, Wuhan, Peoples R China.
   [Liu, Daizong] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
   [Xu, Zichuan] Dalian Univ Technol, Sch Software, Dalian 116024, Peoples R China.
   [Li, Ruixuan] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Peking University; Dalian
   University of Technology; Huazhong University of Science & Technology
RP Zhou, P (corresponding author), Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Hubei Engn Res Ctr Big Data Secur, Wuhan, Peoples R China.
EM xfang9508@gmail.com; dzliu@stu.pku.edu.cn; panzhou@hust.edu.cn;
   z.xu@dlut.edu.cn; rxli@hust.edu.cn
RI Xu, Zichuan/T-5498-2019
OI Xu, Zichuan/0000-0001-5438-1468; liu, daizong/0000-0001-8179-4508; Li,
   Ruixuan/0000-0002-7791-5511; Fang, Xiang/0000-0003-3231-5771
FU National Natural Science Foundation of China
FX No Statement Available
CR Aafaq N, 2023, IEEE T MULTIMEDIA, V25, P2309, DOI 10.1109/TMM.2022.3146005
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao M, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9810
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen J, 2022, ACM T ARCHIT CODE OP, V19, DOI 10.1145/3510422
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen WX, 2016, IEEE INT SYM MULTIM, P367, DOI [10.1109/ISM.2016.106, 10.1109/ISM.2016.0081]
   Ding J., 2022, arXiv
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang X., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM2022.3222965, DOI 10.1109/TMM2022.3222965]
   Fang X., 2020, EEE Trans. Art. Intell, V1, P233
   Fang X., 2020, arXiv
   Fang X, 2023, Arxiv, DOI arXiv:2303.07863
   Fang X, 2022, IEEE TETCI, V6, P913, DOI 10.1109/TETCI.2021.3077909
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Ging Simon, 2020, ADV NEURAL INFORM PR, P22605, DOI DOI 10.18653/V1/P19-1641
   Guo ZY, 2022, IEEE T MULTIMEDIA, V24, P2606, DOI 10.1109/TMM.2021.3087001
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1507, DOI 10.1145/3474085.3475285
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hou YZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1673, DOI 10.1145/3474085.3475310
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Jie Lei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P447, DOI 10.1007/978-3-030-58589-1_27
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Li M., 2021, PROC INT ADV C NEURA
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Liu CH, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P630, DOI 10.1145/3242969.3264989
   Liu DZ, 2023, IEEE T MULTIMEDIA, V25, P8539, DOI 10.1109/TMM.2023.3238514
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1674
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1665
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1683
   Liu DZ, 2023, Arxiv, DOI arXiv:2301.01871
   Liu DZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9292
   Liu DZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4070, DOI 10.1145/3394171.3414026
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Liu J., 2018, PROC INT C INTERNET, P151
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu Y, 2019, PATTERN RECOGN, V93, P365, DOI 10.1016/j.patcog.2019.05.008
   Ma J., 2022, arXiv
   Min WQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P393, DOI 10.1145/3394171.3414031
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053
   Regneri M., 2013, T ASSOC COMPUT LING, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Shah M, 2019, PROC CVPR IEEE, P6642, DOI 10.1109/CVPR.2019.00681
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Song PP, 2023, IEEE T MULTIMEDIA, V25, P1858, DOI 10.1109/TMM.2022.3183402
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Song YQ, 2022, IEEE T MULTIMEDIA, V24, P3013, DOI 10.1109/TMM.2021.3092187
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Tang SG, 2022, IEEE T MULTIMEDIA, V24, P4433, DOI 10.1109/TMM.2021.3117124
   Teng JY, 2022, IEEE T MULTIMEDIA, V24, P1141, DOI 10.1109/TMM.2021.3120545
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GM, 2022, IEEE T MULTIMEDIA, V24, P1221, DOI 10.1109/TMM.2022.3142420
   Wang H., 2022, arXiv
   Wang H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P630, DOI 10.1145/3474085.3475226
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang Q., 2022, arXiv
   Wang W, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1730, DOI 10.1145/3474085.3475317
   Wang XL, 2019, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2019.00267
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Wang YX, 2023, IEEE T MULTIMEDIA, V25, P3921, DOI 10.1109/TMM.2022.3168424
   Woo Sangmin, 2022, arXiv
   Wu SX, 2023, IEEE T MULTIMEDIA, V25, P1111, DOI 10.1109/TMM.2021.3139209
   Xiang Fang, 2022, IEEE Transactions on Artificial Intelligence, V3, P192, DOI 10.1109/TAI.2021.3116546
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu LW, 2022, Arxiv, DOI arXiv:2203.13443
   Yao T, 2022, Arxiv, DOI [arXiv:2207.04978, 10.48550/arXiv.2207.04978]
   Yao T, 2022, Arxiv, DOI arXiv:2207.04976
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Yuan Y., 2019, PROC INT ADV C NEURA, P1
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, P IEEECVF C COMPUTER, P10287
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zhang H., 2020, P 58 ANN M ASS COMPU, P6543
   Zhang H, 2022, IEEE T PATTERN ANAL, V44, P4252, DOI 10.1109/TPAMI.2021.3060449
   Zhang H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P917, DOI 10.1145/3474085.3475272
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang MX, 2021, PROC CVPR IEEE, P12664, DOI 10.1109/CVPR46437.2021.01248
   Zhang QM, 2022, Arxiv, DOI [arXiv:2202.10108, 10.48550/arXiv.2202.10108]
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang ZJ, 2021, IEEE T MULTIMEDIA, V23, P3306, DOI 10.1109/TMM.2020.3023339
   Zhao J, 2022, IEEE T MULTIMEDIA, V24, P2662, DOI 10.1109/TMM.2021.3087006
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 94
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3263
EP 3277
DI 10.1109/TMM.2023.3309551
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200019
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Gao, K
   Horng, JH
   Chang, CC
AF Gao, Kai
   Horng, Ji-Hwei
   Chang, Chin-Chen
TI Reversible Data Hiding for Encrypted 3D Mesh Models With Secret Sharing
   Over Galois Field
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Solid modeling; Three-dimensional displays; Cryptography; Encryption;
   Data models; Media; Payloads; 3D mesh model; adaptive vertex grouping;
   huffman coding; reversible data hiding; secret sharing
AB Reversible data hiding in encrypted 3D models (RDHEM) is an emerging steganography technique, capable of both encrypting the cover model to ensure confidentiality and embedding additional messages for covert communication. However, the embedding capacity provided by recent RDHEM methods is still at a low level. In this paper, an adaptive vertex grouping strategy is proposed, which can divide the vertices in the cover 3D model into groups. Then, the multi-MSB prediction and Huffman coding are exploited to compress the data volume of vertices. Through proper vertex grouping and efficient data compression of the model vertices, the embedding capacity of the RDHEM can be effectively improved. Additionally, two schemes for 3D model encryption are provided. One is based on a secret sharing method over the Galois field and the other leverages the stream cipher technique. Experimental results show that the embedding capacity of the two proposed schemes significantly outperforms state-of-the-art schemes.
C1 [Gao, Kai; Chang, Chin-Chen] Feng Chia Univ, Dept Informat Engn & Comp Sci, Taichung 407102, Taiwan.
   [Horng, Ji-Hwei] Natl Quemoy Univ, Dept Elect Engn, Jinning 892009, Taiwan.
RP Chang, CC (corresponding author), Feng Chia Univ, Dept Informat Engn & Comp Sci, Taichung 407102, Taiwan.; Horng, JH (corresponding author), Natl Quemoy Univ, Dept Elect Engn, Jinning 892009, Taiwan.
EM kaigao.phd@gmail.com; horng@email.nqu.edu.tw; alan3c@gmail.com
OI Gao, Kai/0000-0002-7505-9037; Horng, Ji-Hwei/0000-0002-2134-5257
CR Abdulla A. A., Ph.D. dissertation
   Buckingham U.K., 2015, P INT C RES SEC STAN, P151
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Chen KM, 2023, IEEE T DEPEND SECURE, V20, P4519, DOI 10.1109/TDSC.2022.3228385
   Gao K, 2022, J VIS COMMUN IMAGE R, V84, DOI 10.1016/j.jvcir.2022.103481
   Gao XY, 2020, SIGNAL PROCESS, V173, DOI 10.1016/j.sigpro.2020.107579
   Ge HL, 2019, IEEE T CIRC SYST VID, V29, P2285, DOI 10.1109/TCSVT.2018.2863029
   Girdhar A, 2019, J AMB INTEL HUM COMP, V10, P4947, DOI 10.1007/s12652-019-01179-4
   He WG, 2021, IEEE T IMAGE PROCESS, V30, P5045, DOI 10.1109/TIP.2021.3078088
   He WG, 2020, IEEE T INF FOREN SEC, V15, P3859, DOI 10.1109/TIFS.2020.3002377
   Hua ZY, 2023, IEEE T DEPEND SECURE, V20, P3669, DOI 10.1109/TDSC.2022.3218570
   Hua ZY, 2022, IEEE T CIRC SYST VID, V32, P4968, DOI 10.1109/TCSVT.2022.3140974
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Lyu WL, 2022, SIGNAL PROCESS, V201, DOI 10.1016/j.sigpro.2022.108686
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qin C, 2022, IEEE T CIRC SYST VID, V32, P1928, DOI 10.1109/TCSVT.2021.3091319
   Qiu YQ, 2020, SIGNAL PROCESS, V167, DOI 10.1016/j.sigpro.2019.107288
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Shilane T., The Princetonshape benchmark
   Tsai YY, 2023, IEEE T DEPEND SECURE, V20, P3508, DOI 10.1109/TDSC.2022.3204291
   Tsai YY, 2021, IEEE T MULTIMEDIA, V23, P2286, DOI 10.1109/TMM.2020.3009492
   van Rensburg BJ, 2021, IEEE IMAGE PROC, P3068, DOI 10.1109/ICIP42928.2021.9506320
   Wu HT, 2005, 2005 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P774
   Wu XS, 2021, SIGNAL PROCESS, V188, DOI 10.1016/j.sigpro.2021.108200
   Xu N, 2022, COGN COMPUT, V14, P1172, DOI 10.1007/s12559-021-09919-5
   Xu S, 2023, IEEE T DEPEND SECURE, V20, P4199, DOI 10.1109/TDSC.2022.3219843
   Yi S, 2017, SIGNAL PROCESS, V133, P40, DOI 10.1016/j.sigpro.2016.10.017
   Yin ZX, 2021, LECT NOTES COMPUT SC, V13020, P336, DOI 10.1007/978-3-030-88007-1_28
   Yin ZX, 2021, SIGNAL PROCESS, V187, DOI 10.1016/j.sigpro.2021.108146
   Zhang WM, 2014, SIGNAL PROCESS, V94, P118, DOI 10.1016/j.sigpro.2013.06.023
   Zhang WM, 2013, IEEE T IMAGE PROCESS, V22, P2775, DOI 10.1109/TIP.2013.2257814
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
NR 37
TC 1
Z9 1
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5499
EP 5510
DI 10.1109/TMM.2023.3334972
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600006
DA 2024-08-05
ER

PT J
AU Guo, RH
   Ying, XH
   Qi, YY
   Qu, L
AF Guo, Ruohao
   Ying, Xianghua
   Qi, Yanyu
   Qu, Liao
TI UniTR: A Unified TRansformer-Based Framework for Co-Object and
   Multi-Modal Saliency Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Feature extraction; Task analysis; Transformers; Image
   segmentation; Semantics; Computer architecture; Co-object segmentation;
   multi-modal salient object detection; transformer; deep learning
ID SEGMENTATION; GRAPH; OPTIMIZATION; REFINEMENT; NETWORK; DEEP
AB Recent years have witnessed a growing interest in co-object segmentation and multi-modal salient object detection. Many efforts are devoted to segmenting co-existed objects among a group of images or detecting salient objects from different modalities. Albeit the appreciable performance achieved on respective benchmarks, each of these methods is limited to a specific task and cannot be generalized to other tasks. In this paper, we develop a <bold>Uni</bold>fied <bold>TR</bold>ansformer-based framework, namely <bold>UniTR</bold>, aiming at tackling the above tasks individually with a unified architecture. Specifically, a transformer module (CoFormer) is introduced to learn the consistency of relevant objects or complementarity from different modalities. To generate high-quality segmentation maps, we adopt a dual-stream decoding paradigm that allows the extracted consistent or complementary information to better guide mask prediction. Moreover, a feature fusion module (ZoomFormer) is designed to enhance backbone features and capture multi-granularity and multi-semantic information. Extensive experiments show that our UniTR performs well on <bold>17</bold> <bold>benchmarks</bold>, and surpasses existing state-of-the-art approaches.
C1 [Guo, Ruohao; Ying, Xianghua] Peking Univ, Sch Intelligence Sci & Technol, Natl Key Lab Gen Artificial Intelligence, Beijing 100871, Peoples R China.
   [Qi, Yanyu] China Agr Univ, Coll Informat & Elect Engn, Beijing 100091, Peoples R China.
   [Qu, Liao] Carnegie Mellon Univ, Elect & Comp Engn Dept, Pittsburgh, PA 15213 USA.
C3 Peking University; China Agricultural University; Carnegie Mellon
   University
RP Ying, XH (corresponding author), Peking Univ, Sch Intelligence Sci & Technol, Natl Key Lab Gen Artificial Intelligence, Beijing 100871, Peoples R China.
EM ruohguo@stu.pku.edu.cn; xhying@pku.edu.cn; yanyu.qi@cau.edu.cn;
   liaoq@andrew.cmu.edu
OI guo, ruohao/0000-0002-1091-272X
FU National Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Athanasiadis C, 2020, NEUROCOMPUTING, V397, P331, DOI 10.1016/j.neucom.2019.09.106
   Bai Z, 2023, IEEE T MULTIMEDIA, V25, P764, DOI 10.1109/TMM.2021.3138246
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Carion N., 2020, EUR C COMP VIS, P213
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen H, 2019, LECT NOTES COMPUT SC, V11364, P435, DOI 10.1007/978-3-030-20870-7_27
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165
   Chen YC, 2021, IEEE T PATTERN ANAL, V43, P3632, DOI 10.1109/TPAMI.2020.2985395
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Cheng-Feng Sun, 2020, Learning and Collaboration Technologies. Human and Technology Ecosystems. 7th International Conference, LCT 2020. Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12206), P520, DOI 10.1007/978-3-030-50506-6_35
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Cong RM, 2019, IEEE T IMAGE PROCESS, V28, P4819, DOI 10.1109/TIP.2019.2910377
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan Q, 2021, PROC CVPR IEEE, P12283, DOI 10.1109/CVPR46437.2021.01211
   Fanzeres LA, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app131910833
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Ghiasi G, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8836, DOI 10.1109/ICCV48922.2021.00873
   Girdhar R, 2022, PROC CVPR IEEE, P16081, DOI 10.1109/CVPR52688.2022.01563
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   Guo RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7137, DOI 10.1109/ICCV48922.2021.00707
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   Hao Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8570, DOI 10.1109/CVPR42600.2020.00860
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hsu KJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P748
   Hu SF, 2020, IEEE T MULTIMEDIA, V22, P2278, DOI 10.1109/TMM.2019.2952983
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Huang NAC, 2021, IEEE T MULTIMEDIA, V23, P2428, DOI 10.1109/TMM.2020.3011327
   Huo FS, 2022, IEEE T CIRC SYST VID, V32, P3111, DOI 10.1109/TCSVT.2021.3102268
   Jerripothula KR, 2017, PROC CVPR IEEE, P3881, DOI 10.1109/CVPR.2017.413
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Ji GP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4902, DOI 10.1109/ICCV48922.2021.00488
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Ji YZ, 2021, IEEE T NEUR NET LEAR, V32, P2676, DOI 10.1109/TNNLS.2020.3007534
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Kong YQ, 2022, IEEE T MULTIMEDIA, V24, P1515, DOI 10.1109/TMM.2021.3066775
   Li B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P818
   Li B, 2019, IEEE I CONF COMP VIS, P8518, DOI 10.1109/ICCV.2019.00861
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li SY, 2018, LECT NOTES COMPUT SC, V11207, P215, DOI 10.1007/978-3-030-01219-9_13
   Li TP, 2022, IEEE T MULTIMEDIA, V24, P492, DOI 10.1109/TMM.2021.3054526
   Li WH, 2019, LECT NOTES COMPUT SC, V11363, P638, DOI 10.1007/978-3-030-20893-6_40
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu H., 2023, IEEE Trans. Multimed., P1, DOI [10.1109/TMM.2023.3238548, DOI 10.1109/TMM.2023.3238548]
   Liu J., 2020, PROC EUR C COMPUT VI, P1
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941
   Mingmin Zhen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P445, DOI 10.1007/978-3-030-58583-9_27
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Park H, 2021, PROC CVPR IEEE, P8401, DOI 10.1109/CVPR46437.2021.00830
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ren SC, 2022, PROC CVPR IEEE, P10843, DOI 10.1109/CVPR52688.2022.01058
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Su YK, 2024, IEEE T MULTIMEDIA, V26, P313, DOI 10.1109/TMM.2023.3264883
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Tu ZZ, 2023, IEEE T MULTIMEDIA, V25, P4163, DOI 10.1109/TMM.2022.3171688
   Tu ZZ, 2022, IEEE T IMAGE PROCESS, V31, P3752, DOI 10.1109/TIP.2022.3176540
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2017, IEEE T IMAGE PROCESS, V26, P5825, DOI 10.1109/TIP.2017.2750410
   Wang G., 2018, P 13 C IM GRAPH TECH, P359
   Wang J, 2022, IEEE T CIRC SYST VID, V32, P2949, DOI 10.1109/TCSVT.2021.3099120
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wen HF, 2021, IEEE T IMAGE PROCESS, V30, P9179, DOI 10.1109/TIP.2021.3123548
   Xie ZD, 2021, Arxiv, DOI arXiv:2105.04553
   Xu L, 2022, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR52688.2022.00427
   Xu MZ, 2020, IEEE T CIRC SYST VID, V30, P2191, DOI 10.1109/TCSVT.2019.2920652
   Xu MZ, 2019, IEEE T MULTIMEDIA, V21, P2790, DOI 10.1109/TMM.2019.2914889
   Yan B, 2022, LECT NOTES COMPUT SC, V13681, P733, DOI 10.1007/978-3-031-19803-8_43
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Zhang C, 2021, IEEE T IMAGE PROCESS, V30, P5652, DOI 10.1109/TIP.2021.3087401
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang K., 2020, PROC IEEECVF C COMP, P9047
   Zhang KH, 2023, IEEE T MULTIMEDIA, V25, P5733, DOI 10.1109/TMM.2022.3198848
   Zhang KH, 2021, PROC CVPR IEEE, P13698, DOI 10.1109/CVPR46437.2021.01349
   Zhang KH, 2020, AAAI CONF ARTIF INTE, V34, P12813
   Zhang KH, 2019, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2019.00321
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4147, DOI 10.1109/ICCV48922.2021.00413
   Zhang PC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2978, DOI 10.1109/ICCV48922.2021.00299
   Zhang Q., 2020, P ADV NEUR INF PROC, P6959
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27
   Zhou WJ, 2022, IEEE T EM TOP COMP I, V6, P957, DOI 10.1109/TETCI.2021.3118043
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhu L, 2023, IEEE T MULTIMEDIA, V25, P676, DOI 10.1109/TMM.2021.3129730
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
NR 129
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7622
EP 7635
DI 10.1109/TMM.2024.3369922
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000060
DA 2024-08-05
ER

PT J
AU Hu, YF
   Gao, JY
   Xu, CS
AF Hu, Yufan
   Gao, Junyu
   Xu, Changsheng
TI Learning Multi-Expert Distribution Calibration for Long-Tailed Video
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Long-tailed distribution; video classification; multi-expert calibration
ID SMOTE
AB Most existing state-of-the-art video classification methods assume that the training data obey a uniform distribution. However, video data in the real world typically exhibit an imbalanced long-tailed class distribution, resulting in a model bias towards head class and relatively low performance on tail class. While the current long-tailed classification methods usually focus on image classification, adapting them to video data is not a trivial extension. We propose an end-to-end multi-expert distribution calibration method to address these challenges based on two-level distribution information. The method jointly considers the distribution of samples in each class (intra-class distribution) and the overall distribution of diverse data (inter-class distribution) to solve the issue of imbalanced data under long-tailed distribution. By modeling the two-level distribution information, the model can jointly consider the head classes and the tail classes and significantly transfer the knowledge from the head classes to improve the performance of the tail classes. Extensive experiments verify that our method achieves state-of-the-art performance on the long-tailed video classification task.
C1 [Hu, Yufan] Univ Sci & Technol Beijing, Sch Intelligence Sci & Technol, Beijing 100083, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 University of Science & Technology Beijing; Chinese Academy of Sciences;
   Institute of Automation, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
EM huyufanqaixuan@gmail.com; gaojunyu2015@ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Gao, Junyu/HDO-5516-2022
OI xu, chang sheng/0000-0001-8343-9665; Gao, Junyu/0000-0002-8105-5497
FU National Key Research amp; Development Plan of China
FX No Statement Available
CR Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Cai JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P112, DOI 10.1109/ICCV48922.2021.00018
   Cao KD, 2019, ADV NEUR IN, V32
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639
   Dentamaro V, 2019, LECT NOTES COMPUT SC, V11752, P618, DOI 10.1007/978-3-030-30645-8_56
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong Q, 2017, IEEE I CONF COMP VIS, P1869, DOI 10.1109/ICCV.2017.205
   Drummond Chris, 2003, Workshop on learning from imbalanced datasets II, International Conference on Machine Learning, V11, P1
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Gao JY, 2021, IEEE T MULTIMEDIA, V23, P3203, DOI 10.1109/TMM.2020.3021980
   Gao JY, 2021, IEEE T PATTERN ANAL, V43, P3476, DOI 10.1109/TPAMI.2020.2985708
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Goroshin R, 2015, IEEE I CONF COMP VIS, P4086, DOI 10.1109/ICCV.2015.465
   Guo H, 2021, PROC CVPR IEEE, P15084, DOI 10.1109/CVPR46437.2021.01484
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P235, DOI 10.1109/ICCV48922.2021.00030
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Hu YF, 2023, IEEE T MULTIMEDIA, V25, P2061, DOI 10.1109/TMM.2022.3142413
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Jaderberg M, 2015, ADV NEUR IN, V28
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168
   Kalai AT, 2010, ACM S THEORY COMPUT, P553
   Kang B., 2020, 8 INT C LEARN REPR I
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kataoka H, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020627
   Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155
   Kingma D. P., 2014, arXiv
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li BL, 2022, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR52688.2022.00684
   Li WB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2957
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Ling Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13387, DOI 10.1109/CVPR42600.2020.01340
   Liuyu Xiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P247, DOI 10.1007/978-3-030-58558-7_15
   Menon Aditya, 2013, PMLR, P603
   Mikolov T., 2013, Advances in Neural Information Processing Systems, P3111
   Nascimento JC, 2006, IEEE T MULTIMEDIA, V8, P761, DOI 10.1109/TMM.2006.876287
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860
   Ren MY, 2018, PR MACH LEARN RES, V80
   Samuel D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9475, DOI 10.1109/ICCV48922.2021.00936
   Simonyan K, 2014, ADV NEUR IN, V27
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Tao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P728, DOI 10.1007/978-3-030-58568-6_43
   Ting K. M., 2000, P 17 INT C MACH LEAR, P983
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wallace B. C., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P754, DOI 10.1109/ICDM.2011.33
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XD, 2022, Arxiv, DOI arXiv:2010.01809
   Wang Y-X., 2017, 31 ANN C NEUR INF PR, P7030
   Wu ZX, 2015, Arxiv, DOI arXiv:1509.06086
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xie GS, 2017, IEEE T CIRC SYST VID, V27, P1263, DOI 10.1109/TCSVT.2015.2511543
   Yan X, 2014, LECT NOTES COMPUT SC, V8692, P215, DOI 10.1007/978-3-319-10593-2_15
   Yang Shuo, 2021, INT C LEARN REPR, P1
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7940, DOI 10.1109/ICCV48922.2021.00786
   Zhang Y., 2021, Test-agnostic long-tailed recognition by test-time aggregating diverse experts with self-supervision
NR 74
TC 2
Z9 2
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 555
EP 567
DI 10.1109/TMM.2023.3267887
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000001
DA 2024-08-05
ER

PT J
AU Leng, JX
   Liu, YR
   Gao, XB
   Wang, ZH
AF Leng, Jiaxu
   Liu, Yiran
   Gao, Xinbo
   Wang, Zhihui
TI CRNet: Context-guided Reasoning Network for Detecting Hard Objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognition; Context modeling; Feature extraction; Object detection;
   Detectors; Proposals; Sports; Keypoint localization; object detection;
   cascade framework; size regression
AB Recent studies have shown impressive performance in object detection. However, most current detectors only explore the appearance feature to locate and classify objects but disregard or underestimate the valuable contextual information in the image, which limits the detection performance for those hard objects, such as small objects, occluded objects, blurred objects, etc. In this article, we instead seek to build a novel context modeling framework and conduct more effective context reasoning for object detection. Specifically, we design a Context-guided Reasoning Network (CRNet) to explore the relationships between objects and use easy detected objects to help understand hard ones. In our CRNet, an image is modeled as a graph and local features of objects are viewed as nodes of the graph to learn the relationships between objects. By passing contextual information in the built graph, the features of hard objects can be updated to discriminative features. To this end, we first develop a cascaded center prediction module built upon CenterNet to produce a set of high-quality proposals viewed as nodes of the graph. In addition, to maximize the value of global context information, we present a multi-granularity feature fusion network to encode the whole scene information which is also viewed as nodes of the graph. Then, the spatial and semantic relationships between objects are learned to initialize edges of the graph. Finally, context reasoning is conducted to update the node states iteratively. Extensive experiments are conducted on MS COCO and Pascal VOC to demonstrate the effectiveness of the proposed CRNet. Experimental results show that the proposed CRNet greatly improves the detection performance over existing context-based detectors, and it is comparable with state-of-the-art detectors.
C1 [Leng, Jiaxu; Liu, Yiran; Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Leng, Jiaxu; Liu, Yiran; Gao, Xinbo] Guangyang Bay Lab, Chongqing Inst Brain & Intelligence, Chongqing 400064, Peoples R China.
   [Wang, Zhihui] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266510, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Shandong University
   of Science & Technology
RP Gao, XB (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
EM lengjx@cqupt.edu.cn; 577040595@qq.com; gaoxb@cqupt.edu.cn;
   zh_wang@sdust.edu.cn
RI Wang, Zhihui/D-2915-2015
OI Wang, Zhihui/0000-0001-8140-1882; Yiran, Liu/0009-0008-9926-7325; leng,
   jiaxu/0000-0003-2802-8139
FU National Natural Science Foundation of China
FX No Statement Available
CR Angtian Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12642, DOI 10.1109/CVPR42600.2020.01266
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Carion N., 2020, EUR C COMP VIS, P213
   Chen J, 2021, IEEE T IMAGE PROCESS, V30, P3970, DOI 10.1109/TIP.2021.3066904
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Chen YK, 2023, IEEE T PATTERN ANAL, V45, P2367, DOI 10.1109/TPAMI.2022.3166905
   Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5
   Chen Z, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01370-7
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng ST, 2021, IEEE T IMAGE PROCESS, V30, P1556, DOI 10.1109/TIP.2020.3045636
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng MT, 2021, IEEE T IMAGE PROCESS, V30, P92, DOI 10.1109/TIP.2020.3031371
   Fu C, 2017, arXiv
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gupta S, 2015, Arxiv, DOI arXiv:1511.08177
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong QH, 2022, PROC CVPR IEEE, P4713, DOI 10.1109/CVPR52688.2022.00468
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Kaiwen Duan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P399, DOI 10.1007/978-3-030-58580-8_24
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Leng JX, 2022, IEEE T MULTIMEDIA, V24, P861, DOI 10.1109/TMM.2021.3060278
   Leng JX, 2021, NEUROCOMPUTING, V433, P287, DOI 10.1016/j.neucom.2020.12.093
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li W, 2020, IEEE T CIRC SYST VID, V30, P482, DOI 10.1109/TCSVT.2019.2890840
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu XP, 2022, AAAI CONF ARTIF INTE, P1810
   Meng DP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3631, DOI 10.1109/ICCV48922.2021.00363
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009
   Ouyang WL, 2015, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2015.7298854
   Pato Lourenco V., 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14598, DOI 10.1109/CVPR42600.2020.01462
   Peng JR, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108199
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Song Z, 2011, PROC CVPR IEEE, P1585, DOI 10.1109/CVPR.2011.5995330
   Sun ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3591, DOI 10.1109/ICCV48922.2021.00359
   Tang X, 2018, LECT NOTES COMPUT SC, V11213, P812, DOI 10.1007/978-3-030-01240-3_49
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Torralba A., 2001, Tech. Rep. AIM-2001-028CBCL-208
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang YM, 2022, AAAI CONF ARTIF INTE, P2567
   Xie Q., 2020, P IEEE CVF C COMP VI, P10447, DOI 10.1109/cvpr42600.2020.01046
   Xu H, 2019, PROC CVPR IEEE, P6412, DOI 10.1109/CVPR.2019.00658
   Yang CHY, 2022, PROC CVPR IEEE, P13658, DOI 10.1109/CVPR52688.2022.01330
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Zand M, 2022, LECT NOTES COMPUT SC, V13670, P390, DOI 10.1007/978-3-031-20080-9_23
   Zeng XY, 2016, LECT NOTES COMPUT SC, V9911, P354, DOI 10.1007/978-3-319-46478-7_22
   Zhao-Min Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P633, DOI 10.1007/978-3-030-58589-1_38
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
NR 61
TC 2
Z9 2
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3765
EP 3777
DI 10.1109/TMM.2023.3315558
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300005
DA 2024-08-05
ER

PT J
AU Li, CY
   Cheng, BP
   Cheng, Y
   Zhang, HC
   Liu, RS
   Zheng, YL
   Liao, J
   Cheng, X
AF Li, Chengyang
   Cheng, Baoping
   Cheng, Yao
   Zhang, Haocheng
   Liu, Renshuai
   Zheng, Yinglin
   Liao, Jing
   Cheng, Xuan
TI FaceRefiner: High-Fidelity Facial Texture Refinement With Differentiable
   Rendering-Based Style Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Three-dimensional displays; Rendering (computer graphics); Image
   reconstruction; Face recognition; Solid modeling; Cameras; Facial
   texture generation; 3D face reconstruction; style transfer
ID IMAGE; FACE
AB Recent facial texture generation methods prefer to use deep networks to synthesize image content and then fill in the UV map, thus generating a compelling full texture from a single image. Nevertheless, the synthesized texture UV map usually comes from a space constructed by the training data or the 2D face generator, which limits the methods' generalization ability for in-the-wild input images. Consequently, their facial details, structures and identity may not be consistent with the input. In this paper, we address this issue by proposing a style transfer-based facial texture refinement method named FaceRefiner. FaceRefiner treats the 3D sampled texture as style and the output of a texture generation method as content. The photo-realistic style is then expected to be transferred from the style image to the content image. Different from current style transfer methods that only transfer high and middle level information to the result, our style transfer method integrates differentiable rendering to also transfer low level (or pixel level) information in the visible face regions. The main benefit of such multi-level information transfer is that, the details, structures and semantics in the input can thus be well preserved. The extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that our refinement method can improve the texture quality and the face identity preserving ability, compared with state-of-the-arts.
C1 [Li, Chengyang; Liu, Renshuai; Zheng, Yinglin; Cheng, Xuan] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
   [Cheng, Baoping; Cheng, Yao] China Mobile Hangzhou Informat Technol Co Ltd, Hangzhou 311121, Peoples R China.
   [Zhang, Haocheng] Xiamen Univ Malaysia, Sch Comp & Data Sci, Sepang 43900, Malaysia.
   [Liao, Jing] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Xiamen University; Xiamen University Malaysia Campus; City University of
   Hong Kong
RP Cheng, X (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
EM chengyanglee@stu.xmu.edu.cn; chengbaoping@cmhi.chinamobile.com;
   chengyao@cmhi.chinamobile.com; cst2009153@xmu.edu.my;
   medalwill@stu.xmu.edu.cn; zhengyinglin@stu.xmu.edu.cn;
   jingliao@cityu.edu.hk; chengxuan@xmu.edu.cn
RI li, chengyang/KDN-7899-2024; Cheng, Xuan/HTO-5957-2023
OI Cheng, Xuan/0000-0002-1545-6567; LIAO, Jing/0000-0001-7014-5377; Li,
   Chengyang/0000-0001-8439-004X; Liu, Renshuai/0000-0002-4029-6569; Zheng,
   Yinglin/0000-0003-4671-6111
FU Natural Science Foundation of Fujian Province of China
FX No Statement Available
CR Alaluf Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6691, DOI 10.1109/ICCV48922.2021.00664
   An J, 2021, PROC CVPR IEEE, P862, DOI 10.1109/CVPR46437.2021.00092
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580
   Chen JS, 2022, AAAI CONF ARTIF INTE, P294
   Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gecer B, 2021, PROC CVPR IEEE, P7624, DOI 10.1109/CVPR46437.2021.00754
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang YJ, 2022, IEEE T MULTIMEDIA, V24, P3978, DOI 10.1109/TMM.2021.3111515
   Ichim AE, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766974
   Champandard AJ, 2016, Arxiv, DOI arXiv:1603.01768
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kalischek N, 2021, PROC CVPR IEEE, P9377, DOI 10.1109/CVPR46437.2021.00926
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, INT C LEARNING REPRE
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13970, DOI 10.1109/ICCV48922.2021.01373
   Kim Sunnie S. Y., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P246, DOI 10.1007/978-3-030-58574-7_15
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Kong XY, 2024, IEEE T NEUR NET LEAR, V35, P8482, DOI 10.1109/TNNLS.2022.3230084
   Laine S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417861
   Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li W, 2020, AAAI CONF ARTIF INTE, V34, P1717
   Li YJ, 2017, ADV NEUR IN, V30
   Lin JK, 2021, AAAI CONF ARTIF INTE, V35, P311
   Lin JK, 2020, PROC CVPR IEEE, P5890, DOI 10.1109/CVPR42600.2020.00593
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu RS, 2024, IEEE T MULTIMEDIA, V26, P799, DOI 10.1109/TMM.2023.3271816
   Liu XT, 2021, AAAI CONF ARTIF INTE, V35, P353
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Mun H, 2022, IEEE T MULTIMEDIA, V24, P3823, DOI 10.1109/TMM.2021.3108401
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Ploumpis S, 2019, PROC CVPR IEEE, P10926, DOI 10.1109/CVPR.2019.01119
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Shih YC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508419
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xu K, 2021, IEEE T IMAGE PROCESS, V30, P2501, DOI 10.1109/TIP.2021.3052709
   Yang J, 2020, AAAI CONF ARTIF INTE, V34, P12605
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhao J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4397
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
NR 56
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7225
EP 7236
DI 10.1109/TMM.2024.3361728
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000033
DA 2024-08-05
ER

PT J
AU Li, JK
   Wang, YH
   Li, WX
AF Li, Jiankai
   Wang, Yunhong
   Li, Weixin
TI MHRN: A Multimodal Hierarchical Reasoning Network for Topic Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal topic detection; image and text fusion; hierarchical learning
ID IMAGE; LANGUAGE
AB Multimodal topic detection is an important social media analysis task with a wide variety of real-world applications. However, modeling data jointly, and inferring their topics, is challenging due to the semantic gaps between different modalities. Our insights are from the psychological findings pretaining to the hierarchical structure in humans' inherent perception of images and texts. In this paper, we propose a Multimodal Hierarchical Reasoning Network (MHRN) to perform multimodal inference for topic detection. The images and texts are represented in a hierarchical model named the Multimodal Part-whole Aware Graph (MPAG). MHRN then performs reasoning for topic inference based on three modules, which include a Bottom-Up Aggregation (BUA) module for encoding the hierarchical connections and sibling relations in MPAG, a Top-Down Guidance (TDG) module for enriching features of the nodes in MPAG guided by their parents, and a Bottom-Up Cross Aggregation (BUCA) module for capturing and aggregating the cross-modality cues to achieve effective multimodal reasoning. Extensive experiments are conducted on two benchmarks, and the results demonstrate the superiority of our approach.
C1 [Li, Jiankai; Wang, Yunhong; Li, Weixin] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Li, Jiankai; Wang, Yunhong; Li, Weixin] Beihang Univ, Sch Comp Sci & Engn, IRIP Lab, Beijing 100191, Peoples R China.
   [Li, Jiankai; Wang, Yunhong; Li, Weixin] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
C3 Beihang University; Beihang University
RP Li, WX (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.; Li, WX (corresponding author), Beihang Univ, Sch Comp Sci & Engn, IRIP Lab, Beijing 100191, Peoples R China.
EM lijiankai@buaa.edu.cn; yhwang@buaa.edu.cn; weixinli@buaa.edu.cn
FU National Key R&D Program of China
FX No Statement Available
CR Abavisani M., 2020, P IEEE CVF C COMP VI, p14 679
   Alam F., 2018, P INT AAAI C WEB SOC, V12
   Allan J., 2012, TOPIC DETECTION TRAC, V12
   Blei D.M., 2006, ICML 06, P113, DOI DOI 10.1145/1143844.1143859
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cai YT, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2506
   Chen J., 2021, PROC INT C MULTIMEDI, P1
   Chen SZ, 2019, IEEE T MULTIMEDIA, V21, P2407, DOI 10.1109/TMM.2019.2896515
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fan WT, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P2126, DOI 10.1145/3404835.3462982
   Fukui A., 2016, ARXIV160601847, P457, DOI DOI 10.18653/V1/D16-1044
   Geng X, 2019, IEEE T COMPUT SOC SY, V6, P289, DOI 10.1109/TCSS.2019.2897641
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Habernal Ivan, 2014, P COLING 2014 25 INT, P213
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HINTON G, 1979, Cognitive Science, V3, P231, DOI 10.1016/S0364-0213(79)80008-7
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ji JY, 2023, IEEE T MULTIMEDIA, V25, P3962, DOI 10.1109/TMM.2022.3169061
   Jiang Huaizu, 2020, P IEEE CVF C COMP VI, P10267, DOI DOI 10.1109/CVPR42600.2020.01028
   Jiang YC, 2022, ACM T KNOWL DISCOV D, V16, DOI 10.1145/3480246
   Jin ZX, 2023, IEEE T MULTIMEDIA, V25, P1, DOI 10.1109/TMM.2021.3120194
   Joo J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2093, DOI 10.1145/3240508.3241470
   KAHNEMAN D, 1992, COGNITIVE PSYCHOL, V24, P175, DOI 10.1016/0010-0285(92)90007-O
   Kane B, 2018, IEEE INT CONF BIG DA, P3665, DOI 10.1109/BigData.2018.8622535
   Kiela D, 2018, AAAI CONF ARTIF INTE, P5198
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lan ZZ, 2014, MULTIMED TOOLS APPL, V71, P333, DOI 10.1007/s11042-013-1391-2
   Li ML, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2557
   Li WX, 2017, IEEE T MULTIMEDIA, V19, P367, DOI 10.1109/TMM.2016.2616279
   Liu XC, 2019, PROC CVPR IEEE, P3561, DOI 10.1109/CVPR.2019.00368
   Liu Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2853, DOI 10.1145/3474085.3475709
   Loshchilov I., 2018, INT C LEARN REPR
   Lyu H, 2023, IEEE T COMPUT SOC SY, V10, P335, DOI 10.1109/TCSS.2021.3136858
   Ma YW, 2023, IEEE T MULTIMEDIA, V25, P3723, DOI [10.1109/TMM.2022.3164787, 10.1080/00207454.2022.2098737]
   Mcauliffe J., 2007, NIPS, V20, P121, DOI DOI 10.5555/2981562.2981578
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Qi P, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020): SYSTEM DEMONSTRATIONS, P101
   Qian Shengsheng, 2016, P 24 ACM INT C MULTI, P2
   Qian XM, 2021, IEEE T MULTIMEDIA, V23, P378, DOI 10.1109/TMM.2020.2977478
   Qiang JP, 2022, IEEE T KNOWL DATA EN, V34, P1427, DOI 10.1109/TKDE.2020.2992485
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schifanella R., 2016, P 24 ACM INT C MULT, P1136, DOI 10.1145/2964284.2964321
   Sun MJ, 2023, IEEE T MULTIMEDIA, V25, P1611, DOI 10.1109/TMM.2021.3139467
   Tianlang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P549, DOI 10.1007/978-3-030-58601-0_33
   Tong CD, 2023, IEEE T KNOWL DATA EN, V35, P12295, DOI 10.1109/TKDE.2021.3119686
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Wang DP, 2023, IEEE T MULTIMEDIA, V25, P2966, DOI 10.1109/TMM.2022.3154149
   Wang JY, 2021, IEEE T MULTIMEDIA, V24, P3369, DOI 10.1109/TMM.2021.3097171
   Wang Y., 2016, P TWNTH INT AAAI C W, P719
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Wei YW, 2022, IEEE T MULTIMEDIA, V24, P2701, DOI 10.1109/TMM.2021.3088307
   Wu XB, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1772
   Xu N, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3777
   Yang S., 2021, ADV NEUR IN, V34
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P746, DOI 10.1109/TMM.2018.2865828
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zhang JH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3872, DOI 10.1145/3474085.3475259
   Zhang M, 2019, IEEE INT CON MULTI, P1618, DOI 10.1109/ICME.2019.00279
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zheng WF, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108153
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
NR 71
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6968
EP 6980
DI 10.1109/TMM.2024.3358696
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000047
DA 2024-08-05
ER

PT J
AU Li, X
   Yu, J
   Jiang, SC
   Lu, HC
   Li, ZY
AF Li, Xue
   Yu, Jiong
   Jiang, Shaochen
   Lu, Hongchun
   Li, Ziyang
TI MSViT: Training Multiscale Vision Transformers for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; multiscale feature; supervised deep hashing; vision
   transformer; triplet loss function
AB The recently developed vision transformer (ViT) has achieved promising results on image retrieval compared to convolutional neural networks. However, most of these vision transformer-based image retrieval methods use the original ViT model to extract global features, ignoring the importance of local features for image retrieval. In this work, we propose a vision transformer-based multiscale feature fusion image retrieval method (MSViT) to achieve the fusion of global features with local features. The challenge of this research work is how to learn the feature representation ability of transformer model, so as to improve the performance of image retrieval model. First, a transformer-based two-branch network structure is proposed to obtain different scale features by processing image patches with different granularities. Second, we present a multiscale feature fusion strategy, which can efficiently and effectively fuse the feature information of different sizes on two branches. Finally, to more fully utilize the label information to supervise the network training process, we optimize the construction rules for the triplet data. The comparison of experimental results with ten CNN-based and six transformer-based image retrieval methods on four publicly available image datasets shows that our method outperforms the state-of-the-art methods. And ablation experiments show that the designed multiscale feature fusion strategy and improved triplet loss function have an implicit improvement on the performance of MSViT.
C1 [Li, Xue; Yu, Jiong; Jiang, Shaochen] Xinjiang Univ, Sch Informat Sci & Engn, Urumqi 830046, Peoples R China.
   [Lu, Hongchun] Southwest Jiaotong Univ, Sch Comp & Artificial Intelligence, Chengdu 610031, Peoples R China.
   [Li, Ziyang] Xinjiang Univ, Sch Software, Urumqi 830046, Peoples R China.
C3 Xinjiang University; Southwest Jiaotong University; Xinjiang University
RP Yu, J (corresponding author), Xinjiang Univ, Sch Informat Sci & Engn, Urumqi 830046, Peoples R China.
EM lixue@stu.xju.edu.cn; yujiong@xju.edu.cn; 394935117@qq.com;
   brady_dl@163.com; liziyang@xju.edu.cn
OI Li, Ziyang/0000-0003-2419-2548; li, xue/0000-0002-8553-994X
FU National Natural Science Foundation of China
FX No Statement Available
CR Brogan J, 2021, IEEE T IMAGE PROCESS, V30, P6892, DOI 10.1109/TIP.2021.3097175
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Carion N., 2020, EUR C COMP VIS, P213
   Chaudhuri Ushasi, 2021, 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, P4783, DOI 10.1109/IGARSS47720.2021.9554838
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen Y., 2021, arXiv
   Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Donahue J, 2014, PR MACH LEARN RES, V32
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   El-Nouby A, 2021, Arxiv, DOI [arXiv:2102.05644, DOI 10.48550/ARXIV.2102.05644]
   Fan LX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P825
   Freeman I, 2018, IEEE IMAGE PROC, P6, DOI 10.1109/ICIP.2018.8451339
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Jun Long, 2020, 2020 13th International Conference on Intelligent Computation Technology and Automation (ICICTA), P284, DOI 10.1109/ICICTA51737.2020.00066
   Kan SC, 2021, IEEE T IMAGE PROCESS, V30, P501, DOI 10.1109/TIP.2020.3036779
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li T, 2022, IEEE SIGNAL PROC LET, V29, P827, DOI 10.1109/LSP.2022.3157517
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Li X, 2022, IEEE T CIRC SYST VID, V32, P933, DOI 10.1109/TCSVT.2021.3070129
   Li Y., 2019, arXiv
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Peng TQ, 2015, 2015 8TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P544, DOI 10.1109/CISP.2015.7407939
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Su S., 2018, P ADV NEUR INF PROC, P806, DOI DOI 10.5555/3326943.3327018
   Touvron H, 2021, Arxiv, DOI [arXiv:2012.12877, 10.48550/arXiv.2012.12877]
   Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542
   Wei SK, 2019, IEEE T IMAGE PROCESS, V28, P4580, DOI 10.1109/TIP.2019.2913513
   Weida Cao, 2021, VSIP 2021: 2021 3rd International Conference on Video, Signal and Image Processing, P37, DOI 10.1145/3503961.3503968
   Wu L, 2019, IEEE ACCESS, V7, P36489, DOI 10.1109/ACCESS.2019.2900489
   Xie YZ, 2023, IEEE T MULTIMEDIA, V25, P9161, DOI 10.1109/TMM.2023.3248170
   Xu BQ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3247103
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Xu Binqian, 2023, arXiv
   Xu CY, 2023, IEEE T MULTIMEDIA, V25, P7428, DOI 10.1109/TMM.2022.3222598
   Yao X., 2022, IEEE Trans. Multimedia,, DOI [10.1109/TMM2022.3213476, DOI 10.1109/TMM2022.3213476]
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zheng XT, 2020, NEUROCOMPUTING, V403, P224, DOI 10.1016/j.neucom.2020.04.037
   Zheng XT, 2019, IEEE T GEOSCI REMOTE, V57, P2596, DOI 10.1109/TGRS.2018.2875304
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 48
TC 3
Z9 3
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2809
EP 2823
DI 10.1109/TMM.2023.3304021
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400005
DA 2024-08-05
ER

PT J
AU Liu, YB
   Cao, G
   Shi, BS
   Hu, YX
AF Liu, Yanbo
   Cao, Guo
   Shi, Boshan
   Hu, Yingxiang
TI CCANet: A Collaborative Cross-Modal Attention Network for RGB-D Crowd
   Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative cross-modal attention; collaborative cross-modal fusion;
   crowd counting; RGB-D
AB Presently, to obtain a more accurate density map and crowd number, existing methods often count by combining training RGB images and depth images. However, these methods are not ideal for capturing and fusing complementary features in RGB-D. Therefore, to solve the above problems, we propose a collaborative cross-modal attention network named CCANet for accurate RGB-D crowd counting. CCANet is mainly composed of the collaborative cross-modal attention module (CCAM) and the collaborative cross-modal fusion module (CCFM). Specifically, CCAM focuses on adaptive, interleaved RGB-D information through channel and spatial cross-modal attentions to fully capture complementary features in different modes. CCFM can adaptively integrate these features by weighing the importance of the above complementary features. A large number of experiments on the ShanghaiTechRGBD and MICC benchmarks have proven the effectiveness of CCANet in RGB-D crowd counting. In addition, our CCANet is generally applicable to multimodal crowd counting and has achieved superior counting performance on the RGBT-CC benchmark.
C1 [Liu, Yanbo; Cao, Guo; Shi, Boshan] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210096, Peoples R China.
   [Hu, Yingxiang] Shandong Univ Sci & Technol, Sch Comp Sci & Engn, Qingdao 266400, Shandong, Peoples R China.
C3 Nanjing University of Science & Technology; Shandong University of
   Science & Technology
RP Cao, G (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210096, Peoples R China.
EM liuyanbo@njust.edu.cn; caoguo@njust.edu.cn; shiboshan@njust.edu.cn;
   2826756240@qq.com
OI Cao, Guo/0000-0002-2689-0932
FU Natural Science Foundation of Jiangsu Province
FX No Statement Available
CR Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bondi E, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P337, DOI 10.1109/AVSS.2014.6918691
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chattopadhyay P, 2017, PROC CVPR IEEE, P4428, DOI 10.1109/CVPR.2017.471
   Cheng J, 2021, IEEE T IMAGE PROCESS, V30, P2862, DOI 10.1109/TIP.2021.3055631
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fu HY, 2012, IEEE IMAGE PROC, P2685, DOI 10.1109/ICIP.2012.6467452
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Gong S, 2022, P IEEE CVF C COMP VI, P7542
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Li H, 2023, IEEE T IND INFORM, V19, P306, DOI 10.1109/TII.2022.3171352
   Lian DZ, 2022, IEEE T PATTERN ANAL, V44, P9056, DOI 10.1109/TPAMI.2021.3124956
   Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192
   Lin H, 2022, PROC CVPR IEEE, P19596, DOI 10.1109/CVPR52688.2022.01901
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545
   Liu LB, 2021, PROC CVPR IEEE, P4821, DOI 10.1109/CVPR46437.2021.00479
   Liu YB, 2022, IEEE T CIRC SYST VID, V32, P6821, DOI 10.1109/TCSVT.2022.3171235
   Ma ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3185, DOI 10.1109/ICCV48922.2021.00319
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Peng Tao, 2020, P ASIAN C COMPUTER V, P497, DOI DOI 10.1007/978-3-030-69544-6_30
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Reddy MKK, 2022, IEEE T MULTIMEDIA, V24, P1008, DOI 10.1109/TMM.2021.3062481
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song DP, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (IEEE ICIA 2017), P416, DOI 10.1109/ICInfA.2017.8078944
   Song QY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3345, DOI 10.1109/ICCV48922.2021.00335
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   Tang HH, 2022, IEEE INT SYMP CIRC S, P3299, DOI 10.1109/ISCAS48785.2022.9937583
   Pham VQ, 2015, IEEE I CONF COMP VIS, P3253, DOI 10.1109/ICCV.2015.372
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wang MJ, 2023, IEEE T MULTIMEDIA, V25, P2074, DOI 10.1109/TMM.2022.3142398
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Xiyang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P241, DOI 10.1007/978-3-030-58586-0_15
   Xu ML, 2019, PATTERN RECOGN LETT, V125, P563, DOI 10.1016/j.patrec.2019.02.026
   Yang SD, 2019, IEEE INT CONF COMP V, P4521, DOI 10.1109/ICCVW.2019.00553
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Yutao Hu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P747, DOI 10.1007/978-3-030-58542-6_45
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang J., 2020, P IEEE CVF C COMP VI, P8582, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849
   Zhang S., 2018, P EUR C COMP VIS ECC, P637
   Zhang SH, 2021, EXPERT SYST APPL, V180, DOI 10.1016/j.eswa.2021.115071
   Zhang XC, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P215, DOI 10.1109/AVSS.2012.82
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhou DS, 2020, IEEE ACCESS, V8, P101616, DOI 10.1109/ACCESS.2020.2998678
   Zhou WJ, 2022, IEEE T INTELL TRANSP, V23, P24540, DOI 10.1109/TITS.2022.3203385
NR 49
TC 4
Z9 4
U1 17
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 154
EP 165
DI 10.1109/TMM.2023.3262978
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500009
DA 2024-08-05
ER

PT J
AU Lu, X
   Liu, L
   Ning, LX
   Zhang, L
   Mu, SM
   Zhang, HX
AF Lu, Xu
   Liu, Li
   Ning, Lixin
   Zhang, Liang
   Mu, Shaomin
   Zhang, Huaxiang
TI Multi-Facet Weighted Asymmetric Multi-Modal Hashing Based on Latent
   Semantic Distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hashing; multi-modal data; multimedia retrieval; weight learning
AB With the advent of multi-modal data, multi-modal hashing has received increasing attention for it can configure complementary multi-modal fusion and support fast multimedia retrieval. Nevertheless, the "coarse-grained" modality weighting strategy widely used in existing methods always ignores the distinctive contributions of different features and is troubled by parameter adjustment. Besides, traditional supervised methods usually adopt "hard semantic" that reflects the logical relationship between data and labels, but fails to poring on the description degree of categories to data. To solve these problems, we propose a multi-Facet weIghting aSymmetric Multi-modal Hashing based on latent semantic distribution (FISMH) approach, which is divided into supervised paradigm SFISMH and unsupervised paradigm UFISMH. First, we design a Multi-facet Weighted Multi-modal Fusion module that utilizes both modality- and feature-wise weights to achieve multi-modal fusion, where the weight learning requires no additional parameter adjustment. Then, we design a Latent Semantic Distribution based Asymmetric Hash Learning module, which utilizes the pair-wise similarity and semantic distribution to guide hash learning, and avoids the challenging pair-wise factorization through asymmetric form. The semantic distribution is learned from the inherent information of feature space, which can further preserve the intra-class relationships. Finally, a discrete hash optimization is developed to reduce quantization and directly learn hash codes. The main difference between SFISMH and UFISMH is that the former utilizes category information while the latter explores the underlying data structure when constructing the pair-wise similarity. Extensive experiments demonstrate that both SFIMH and UFISMH outperform existing supervised and unsupervised multi-modal hashing methods, showcasing their exceptional performance.
C1 [Lu, Xu; Ning, Lixin; Zhang, Liang; Mu, Shaomin] Shandong Agr Univ, Coll Informat Sci & Engn, Tai An 271018, Peoples R China.
   [Liu, Li; Zhang, Huaxiang] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Zhang, Huaxiang] Aerosp Informat Technol Univ, Jinan 250200, Peoples R China.
C3 Shandong Agricultural University; Shandong Normal University
RP Lu, X (corresponding author), Shandong Agr Univ, Coll Informat Sci & Engn, Tai An 271018, Peoples R China.; Liu, L (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM lxuu306@sdau.edu.cn; liuli_790209@163.com
OI Lu, Xu/0000-0002-8459-3186; zhang, hua xiang/0000-0001-6259-7533
FU National Natural Science Foundation of China
FX No Statement Available
CR An JF, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2021.102743
   [Anonymous], 2010, PROC ACM SIGMM INT C
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chen Y, 2020, IEEE T IMAGE PROCESS, V29, P3596, DOI 10.1109/TIP.2020.2963952
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Fang X., 2020, arXiv
   Hansen C, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P2009, DOI 10.1145/3397271.3401220
   Junyeong Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10103, DOI 10.1109/CVPR42600.2020.01012
   Kim S, 2013, INT CONF ACOUST SPEE, P3123, DOI 10.1109/ICASSP.2013.6638233
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu LY, 2020, NEURAL PROCESS LETT, V52, P1765, DOI 10.1007/s11063-020-10221-y
   Liu W., 2011, PROC INT C MACH LEAR, P1
   Liu X., 2012, P 20 ACM INT C MULT, P881
   Lu JW, 2021, IEEE T IMAGE PROCESS, V30, P332, DOI 10.1109/TIP.2020.3036735
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Murty K. G., 2007, Technometrics, V49
   Nie FP, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2022, DOI 10.1145/3219819.3220049
   Nie XS, 2021, IEEE T CIRC SYST VID, V31, P401, DOI 10.1109/TCSVT.2020.2974877
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Shen X, 2022, INFORM SCIENCES, V604, P45, DOI 10.1016/j.ins.2022.05.006
   Shen XB, 2022, IEEE T CIRC SYST VID, V32, P8837, DOI 10.1109/TCSVT.2022.3197849
   Shen XB, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3178119
   Shen XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2733373.2806342
   Shen YM, 2020, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR42600.2020.00289
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Sun Y, 2024, IEEE T MULTIMEDIA, V26, P824, DOI 10.1109/TMM.2023.3272169
   Tang JH, 2020, IEEE T KNOWL DATA EN, V32, P855, DOI 10.1109/TKDE.2019.2893638
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wu XM, 2022, AAAI CONF ARTIF INTE, P4263
   Wu XZ, 2021, NEUROCOMPUTING, V465, P1, DOI 10.1016/j.neucom.2021.08.125
   Xie MK, 2022, IEEE T PATTERN ANAL, V44, P3676, DOI 10.1109/TPAMI.2021.3059290
   Yang EK, 2019, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2019.00306
   Yang R, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P180, DOI 10.1145/3078971.3078981
   Yazici Vacit Oguz, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13437, DOI 10.1109/CVPR42600.2020.01345
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zheng CQ, 2020, IEEE SIGNAL PROC LET, V27, P1270, DOI 10.1109/LSP.2020.3008335
   Zhu L, 2022, ACM T INFORM SYST, V40, DOI 10.1145/3477180
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 46
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7307
EP 7320
DI 10.1109/TMM.2024.3363664
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000049
DA 2024-08-05
ER

PT J
AU Lu, YX
   Sirejiding, S
   Ding, Y
   Wang, CL
   Lu, HT
AF Lu, Yuxiang
   Sirejiding, Shalayiding
   Ding, Yue
   Wang, Chunlin
   Lu, Hongtao
TI Prompt Guided Transformer for Multi-Task Dense Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-task learning; dense prediction; prompting; vision transformer
AB Task-conditional architecture offers advantage in parameter efficiency but falls short in performance compared to state-of-the-art multi-decoder methods. How to trade off performance and model parameters is an important and difficult problem. In this paper, we introduce a simple and lightweight task-conditional model called Prompt Guided Transformer (PGT) to optimize this challenge. Our approach designs a Prompt-conditioned Transformer block, which incorporates task-specific prompts in the self-attention mechanism to achieve global dependency modeling and parameter-efficient feature adaptation across multiple tasks. This block is integrated into both the shared encoder and decoder, enhancing the capture of intra- and inter-task features. Moreover, we design a lightweight decoder to further reduce parameter usage, which accounts for only 2.7% of the total model parameters. Extensive experiments on two multi-task dense prediction benchmarks, PASCAL-Context and NYUD-v2, demonstrate that our approach achieves state-of-the-art results among task-conditional methods while using fewer parameters, and maintains a significant balance between performance and parameter size.
C1 [Lu, Yuxiang; Sirejiding, Shalayiding; Ding, Yue; Lu, Hongtao] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Wang, Chunlin] Chuxiong Normal Univ, Sch Informat Sci & Technol, Chuxiong 675099, Peoples R China.
C3 Shanghai Jiao Tong University; Chuxiong Normal University
RP Lu, HT (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
EM luyuxiang_2018@sjtu.edu.cn; salaydin@sjtu.edu.cn; dingyue@sjtu.edu.cn;
   wcl@cxtc.edu.cn; htlu@sjtu.edu.cn
RI Lu, Yuxiang/HNS-1172-2023
OI Lu, Yuxiang/0009-0002-6344-3880; DING, Yue/0000-0002-2911-1244;
   Sirejiding, Shalayiding/0000-0003-1255-4994; Lu,
   Hongtao/0000-0003-2300-3039
FU National Nature Science Foundation of China
FX No Statement Available
CR Bahng H, 2022, Arxiv, DOI arXiv:2203.17274
   Bragman FJS, 2019, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2019.00147
   Bruggemann D., 2020, P 31 BRIT MACH VIS C, P1
   Bruggemann D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15849, DOI 10.1109/ICCV48922.2021.01557
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen S., 2022, P ADV NEUR INF PROC, V35, P16664, DOI DOI 10.48550/ARXIV.2205.13535
   Chen XJ, 2021, IEEE T NEUR NET LEAR, V32, P5034, DOI 10.1109/TNNLS.2020.3026669
   Chen Z, 2018, PR MACH LEARN RES, V80
   Crawshaw Michael, 2020, Multi-task learning with deep neural networks: A survey
   Dosovitskiy A., 2021, PROC ICLR
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fang YC, 2023, IEEE T MULTIMEDIA, V25, P6946, DOI 10.1109/TMM.2022.3216460
   Gao Y, 2019, PROC CVPR IEEE, P3200, DOI 10.1109/CVPR.2019.00332
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Guo P., 2020, ICML, P3854
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Yun, 2022, INT C MACH LEARN, P8678
   Houlsby N, 2019, PR MACH LEARN RES, V97
   Hu E. J., 2022, P INT C LEARN REPR
   Jia ML, 2022, LECT NOTES COMPUT SC, V13693, P709, DOI 10.1007/978-3-031-19827-4_41
   Jiang ZB, 2020, T ASSOC COMPUT LING, V8, P423, DOI 10.1162/tacl_a_00324
   Jiao JY, 2023, IEEE T MULTIMEDIA, V25, P8906, DOI 10.1109/TMM.2023.3243616
   Kanakis Menelaos, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P689, DOI 10.1007/978-3-030-58565-5_41
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Lester B, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3045
   Li XLS, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P4582
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu PF, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3560815
   Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197
   Liu X, 2022, Arxiv, DOI [arXiv:2110.07602, 10.48550/arXiv.2110.07602]
   Liu X, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022): (SHORT PAPERS), VOL 2, P61
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long M., 2017, Advances in Neural Information Processing Systems, P1594
   Loshchilov I., 2016, arXiv
   Loshchilov I., 2018, INT C LEARN REPR
   Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126
   Mahabadi RK, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P565
   Maninis KK, 2019, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2019.00195
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Paszke A, 2019, ADV NEUR IN, V32
   Popovic N, 2021, PROC CVPR IEEE, P6866, DOI 10.1109/CVPR46437.2021.00680
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Rao YM, 2022, PROC CVPR IEEE, P18061, DOI 10.1109/CVPR52688.2022.01755
   Ruder S, 2017, Arxiv, DOI [arXiv:1706.05098, DOI 10.48550/ARXIV.1706.05098]
   Ruder S, 2019, AAAI CONF ARTIF INTE, P4822
   Shao SW, 2023, IEEE T MULTIMEDIA, V25, P7660, DOI 10.1109/TMM.2022.3224810
   Shin T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4222
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sirejiding S, 2023, IEEE INT CON MULTI, P1859, DOI 10.1109/ICME55011.2023.00319
   Sun GL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8271, DOI 10.1109/ICCV48922.2021.00818
   Sung YL, 2022, PROC CVPR IEEE, P5217, DOI 10.1109/CVPR52688.2022.00516
   Vandenhende Simon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P527, DOI 10.1007/978-3-030-58548-8_31
   Vandenhende S., 2020, P BRIT MACH VIS C, P1
   Vandenhende S, 2022, IEEE T PATTERN ANAL, V44, P3614, DOI 10.1109/TPAMI.2021.3054719
   Vaswani A, 2017, ADV NEUR IN, V30
   Villa A, 2023, PROC CVPR IEEE, P24214, DOI 10.1109/CVPR52729.2023.02319
   Wallingford M, 2022, PROC CVPR IEEE, P7551, DOI 10.1109/CVPR52688.2022.00741
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wang ZF, 2022, PROC CVPR IEEE, P139, DOI 10.1109/CVPR52688.2022.00024
   Wu JY, 2023, Arxiv, DOI arXiv:2212.10556
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077
   Xu XG, 2022, LECT NOTES COMPUT SC, V13687, P304, DOI 10.1007/978-3-031-19812-0_18
   Xu Y., 2023, PROC AAAI C ARTIF IN, P3072, DOI DOI 10.1609/AAAI.V37I3.25411
   Xu YY, 2024, IEEE T CIRC SYST VID, V34, P1228, DOI 10.1109/TCSVT.2023.3292995
   Yang S, 2021, IEEE T MULTIMEDIA, V24, P3611, DOI 10.1109/TMM.2021.3103605
   Yang Y., 2017, ICLR
   Ye H., 2023, P 11 INT C LEARN REP, P1
   Ye HR, 2023, Arxiv, DOI arXiv:2306.04842
   Ye HR, 2022, LECT NOTES COMPUT SC, V13687, P514, DOI 10.1007/978-3-031-19812-0_30
   Zhang XY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P97, DOI 10.1145/3474085.3475501
   Zhang XY, 2023, IEEE T MULTIMEDIA, V25, P2503, DOI 10.1109/TMM.2022.3147664
   Zhang ZY, 2019, PROC CVPR IEEE, P4101, DOI 10.1109/CVPR.2019.00423
   Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15
   Zhou L, 2020, PROC CVPR IEEE, P4513, DOI 10.1109/CVPR42600.2020.00457
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
NR 83
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6375
EP 6385
DI 10.1109/TMM.2024.3349865
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600051
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ma, L
   Hong, HY
   Meng, FM
   Wu, QB
   Wu, JM
AF Ma, Lei
   Hong, Hanyu
   Meng, Fanman
   Wu, Qingbo
   Wu, Jinmeng
TI Deep Progressive Asymmetric Quantization Based on Causal Intervention
   for Fine-Grained Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quantization (signal); Image retrieval; Semantics; Task analysis;
   Visualization; Vector quantization; Training; Causal intervention; deep
   progressive asymmetric quantization; fine-grained image retrieval;
   structural causal model
ID BINARY-CODES; NETWORK; SEARCH
AB In the field of computer vision, fine-grained image retrieval is an extremely challenging task due to the inherently subtle intra-class object variations. In addition, the high-dimensional real-valued features extracted from large-scale fine-grained image datasets slow the retrieval speed and increase the storage cost. To solve above issues, existing fine-grained image retrieval methods mainly focus on finding more discriminative local regions for generating discriminative and compact hash codes, which achieve limited fine-grained image retrieval performance due to the large quantization errors and the confounding granularities and context of discriminative parts, i.e., the correct recognition of fine-grained objects mainly attribute to the discriminative parts and their context. To learn robust causal features and reduce the quantization errors, we propose a deep progressive asymmetric quantization (DPAQ) method based on causal intervention to learn compact and robust descriptions for fine-grained image retrieval task. Specifically, we introduce a structural causal model to learn robust casual features via causal intervention for fine-grained visual recognition. Subsequently, we design a progressive asymmetric quantization layer in the feature embedding space, which can preserve the semantic information and reduce the quantization errors sufficiently. Finally, we incorporate both the fine-grained image classification and retrieval tasks into an end-to-end deep learning architecture for generating robust and compact descriptions. Experimental results on several fine-grained image retrieval datasets demonstrate that the proposed DPAQ method performs the best for fine-grained image retrieval task and surpasses the state-of-the art fine-grained hashing methods by a large margin.
C1 [Ma, Lei; Hong, Hanyu; Wu, Jinmeng] Wuhan Inst Technol, Sch Elect & Informat Engn, Hubei Key Lab Opt Informat & Pattern Recognit, Wuhan 430079, Peoples R China.
   [Meng, Fanman; Wu, Qingbo] Univ Elect Sci & Technol China, Sch Elect Engn, Intelligent Visual Informat Proc & Commun Lab, Chengdu 610056, Peoples R China.
C3 Wuhan Institute of Technology; University of Electronic Science &
   Technology of China
RP Hong, HY (corresponding author), Wuhan Inst Technol, Sch Elect & Informat Engn, Hubei Key Lab Opt Informat & Pattern Recognit, Wuhan 430079, Peoples R China.
EM leima@wit.edu.cn; hhyhong@163.com; fmmeng@uestc.edu.cn;
   qbwu@uestc.edu.cn; jinmeng2004910@outlook.com
RI ; Wu, Qingbo/M-5065-2015
OI Wu, Jinmeng/0000-0002-0264-8025; Wu, Qingbo/0000-0003-2936-6340
FU National Natural Science Foundation of China
FX No Statement Available
CR Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen JJ, 2019, AAAI CONF ARTIF INTE, P8183
   Chen YJ, 2010, SENSORS-BASEL, V10, P11259, DOI 10.3390/s101211259
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Deng C, 2020, IEEE T NEUR NET LEAR, V31, P2189, DOI 10.1109/TNNLS.2019.2929068
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Ding YF, 2021, IEEE T IMAGE PROCESS, V30, P2826, DOI 10.1109/TIP.2021.3055617
   Eghbali S, 2019, PROC CVPR IEEE, P11682, DOI 10.1109/CVPR.2019.01196
   Farvardin N., 1994, IEEE Trans. Inf. Theory, V40, P287
   Gao LL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P723
   Geirhos R., 2019, INT C LEARN REPR ICL
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Han H, 2019, IEEE T PATTERN ANAL, V41, P2333, DOI 10.1109/TPAMI.2019.2891584
   Hu T, 2019, Arxiv, DOI arXiv:1901.09891
   Huang JQ, 2022, AAAI CONF ARTIF INTE, P998
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jin S, 2020, IEEE T IMAGE PROCESS, V29, P5336, DOI 10.1109/TIP.2020.2971105
   Kaiyue Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10344, DOI 10.1109/CVPR42600.2020.01036
   Khosla A., 2011, P WORKSH COMP VIS PA, P806
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Liu B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P755, DOI 10.1145/3240508.3240543
   Liu W., 2014, P ADV NEUR INF PROC, V27
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Ma L, 2020, IEEE SIGNAL PROC LET, V27, P2129, DOI 10.1109/LSP.2020.3039755
   Ma L, 2021, NEUROCOMPUTING, V443, P85, DOI 10.1016/j.neucom.2021.02.057
   Ma L, 2020, NEUROCOMPUTING, V380, P115, DOI 10.1016/j.neucom.2019.11.009
   Ma L, 2018, NEUROCOMPUTING, V312, P49, DOI 10.1016/j.neucom.2018.05.052
   Ma L, 2017, IEEE T MULTIMEDIA, V19, P2545, DOI 10.1109/TMM.2017.2703089
   Ma L, 2017, J VIS COMMUN IMAGE R, V44, P29, DOI 10.1016/j.jvcir.2017.01.014
   Martinez J, 2016, LECT NOTES COMPUT SC, V9906, P137, DOI 10.1007/978-3-319-46475-6_9
   Morozov S, 2019, IEEE I CONF COMP VIS, P3036, DOI 10.1109/ICCV.2019.00313
   Pearl Judea., 2009, Causality: Models, reasoning and inference, V2nd edn.
   Quan Cui, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P189, DOI 10.1007/978-3-030-58580-8_12
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Rubin DB, 2005, J AM STAT ASSOC, V100, P322, DOI 10.1198/016214504000001880
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Ruyi Ji, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10465, DOI 10.1109/CVPR42600.2020.01048
   Shen FM, 2019, IEEE T IMAGE PROCESS, V28, P3662, DOI 10.1109/TIP.2019.2899987
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Song JK, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P912
   Sun H, 2020, IEEE ACCESS, V8, P26199, DOI 10.1109/ACCESS.2020.2970223
   van der Maaten G. E., 2008, J. Mach. Learn. Res., V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang L, 2022, IEEE T MULTIMEDIA, V24, P3665, DOI 10.1109/TMM.2021.3105824
   Wang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3071, DOI 10.1109/ICCV48922.2021.00308
   Wang XF, 2017, LECT NOTES COMPUT SC, V10111, P70, DOI 10.1007/978-3-319-54181-5_5
   Wang XJ, 2016, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2016.222
   Wei H, 2020, IEEE ACCESS, V8, P104985, DOI 10.1109/ACCESS.2020.2999722
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Weiss Y., 2008, Advances in Neural Information Processing Systems, V21, P1753
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Yang X, 2021, PROC CVPR IEEE, P9842, DOI 10.1109/CVPR46437.2021.00972
   Yang YF, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P114, DOI 10.1145/3323873.3325015
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yao T., 2016, IJCAI, P3931
   Yu T, 2020, INT J COMPUT VISION, V128, P2325, DOI 10.1007/s11263-020-01326-x
   Zeng HE, 2019, Arxiv, DOI arXiv:1911.08028
   Zhang Hanwang, 2020, ADV NEUR IN, V33
   Zhang SF, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P212, DOI 10.1145/3323873.3325059
   Zhang T., 2014, ICML, P838
   Zhang T, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675376
   Zhang YB, 2018, LECT NOTES COMPUT SC, V11212, P241, DOI 10.1007/978-3-030-01237-3_15
   Zhe XF, 2020, IEEE T NEUR NET LEAR, V31, P1681, DOI 10.1109/TNNLS.2019.2921805
   Zhou C, 2022, IEEE T NEUR NET LEAR, V33, P1638, DOI 10.1109/TNNLS.2020.3043103
   Zhu L, 2017, IEEE T KNOWL DATA EN, V29, P472, DOI 10.1109/TKDE.2016.2562624
NR 78
TC 4
Z9 4
U1 30
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1306
EP 1318
DI 10.1109/TMM.2023.3279990
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700017
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Peng, L
   Cao, Y
   Sun, YJ
   Wang, Y
AF Peng, Long
   Cao, Yang
   Sun, Yuejin
   Wang, Yang
TI Lightweight Adaptive Feature De-Drifting for Compressed Image
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature drifting; feature enhancement; image classification; JPEG
   compression
ID SUPERRESOLUTION; DEBLOCKING
AB JPEG is a widely used compression scheme to efficiently reduce the volume of the transmitted images at the expense of visual perception drop. The artifacts appear among blocks due to the information loss in the compression process, which not only affects the quality of images but also harms the subsequent high-level tasks in terms of feature drifting. High-level vision models trained on high-quality images will suffer performance degradation when dealing with compressed images, especially on mobile devices. In recent years, numerous learning-based JPEG artifacts removal methods have been proposed to handle visual artifacts. However, it is not an ideal choice to use these JPEG artifacts removal methods as a pre-processing for compressed image classification for the following reasons: 1) These methods are designed for human vision rather than high-level vision models. 2) These methods are not efficient enough to serve as a pre-processing on resource-constrained devices. To address these issues, this paper proposes a novel lightweight adaptive feature de-drifting module (AFD-Module) to boost the performance of pre-trained image classification models when facing compressed images. First, a Feature Drifting Estimation Network (FDE-Net) is devised to generate the spatial-wise Feature Drifting Map (FDM) in the DCT domain. Next, the estimated FDM is transmitted to the Feature Enhancement Network (FE-Net) to generate the mapping relationship between degraded features and corresponding high-quality features. Specially, a simple but effective RepConv block equipped with structural re-parameterization is utilized in FE-Net, which enriches feature representation in the training phase while keeping efficiency in the deployment phase. After training on limited compressed images, the AFD-Module can serve as a "plug-and-play" module for pre-trained classification models to improve their performance on compressed images. Experiments on images compressed once (i.e. ImageNet-C) and multiple times demonstrate that our proposed AFD-Module can comprehensively improve the accuracy of the pre-trained classification models and significantly outperform the existing methods.
C1 [Peng, Long] Univ Sci & Technol China, Inst Adv Technol, Hefei 230026, Peoples R China.
   [Cao, Yang; Sun, Yuejin; Wang, Yang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Wang, Y (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM longp2001@mail.ustc.edu.cn; forrest@ustc.edu.cn;
   yjsun97@mail.ustc.edu.cn; ywang120@ustc.edu.cn
OI Sun, Yuejin/0000-0003-4731-9601; Peng, long/0000-0003-1012-5109
FU Natural Science Foundation of China
FX No Statement Available
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Asano Y. M., 2020, PROC INT C LEARN REP
   Barman N., 2020, PROC IEEE 12 INT C Q, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XH, 2021, PROC CVPR IEEE, P10881, DOI 10.1109/CVPR46437.2021.01074
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Ehrlich M, 2021, IEEE INT CONF COMP V, P2357, DOI 10.1109/ICCVW54120.2021.00267
   Fang YQ, 2023, Arxiv, DOI arXiv:2301.00265
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Fu XY, 2022, IEEE T NEUR NET LEAR, V33, P6802, DOI 10.1109/TNNLS.2021.3083504
   Fu XY, 2019, IEEE I CONF COMP VIS, P2501, DOI 10.1109/ICCV.2019.00259
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2018, P INT C LEARN REPR
   Hongxu Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8712, DOI 10.1109/CVPR42600.2020.00874
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Jiang JX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4977, DOI 10.1109/ICCV48922.2021.00495
   Jingwei Xin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P91, DOI 10.1007/978-3-030-58548-8_6
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li BY, 2017, Arxiv, DOI arXiv:1707.06543
   Li T, 2018, IEEE T MULTIMEDIA, V20, P1305, DOI 10.1109/TMM.2017.2766889
   Li Y., 2020, P EUR C COMP VIS ECC, P608
   Li YW, 2023, PROC CVPR IEEE, P18278, DOI 10.1109/CVPR52729.2023.01753
   Liu D, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P842
   Liu JY, 2020, IEEE T IMAGE PROCESS, V29, P7845, DOI 10.1109/TIP.2020.3007828
   Mao XJ, 2016, Arxiv, DOI [arXiv:1603.09056, DOI 10.48550/ARXIV.1603.09056]
   Mei YQ, 2023, INT J COMPUT VISION, V131, P3207, DOI 10.1007/s11263-023-01843-5
   Nayak GK, 2019, PR MACH LEARN RES, V97
   Noh J, 2019, IEEE I CONF COMP VIS, P9724, DOI 10.1109/ICCV.2019.00982
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Oza P., 2023, IEEE Trans. Pattern Anal. Mach. Intell., P1, DOI [10.1109/TΡΑΙ.2022.3217046, DOI 10.1109/T&URHO;&UALPHA;&UIOTA;.2022.3217046]
   Pei YT, 2018, LECT NOTES COMPUT SC, V11214, P697, DOI 10.1007/978-3-030-01249-6_42
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Szegedy C., 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594]
   Taeyoung Son, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P749, DOI 10.1007/978-3-030-58545-7_43
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tan WM, 2018, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2018.00420
   Teng G, 2024, VISUAL COMPUT, V40, P3033, DOI 10.1007/s00371-023-03008-4
   Verma V, 2018, SIGNAL PROCESS-IMAGE, V67, P22, DOI 10.1016/j.image.2018.04.014
   Wang X, 2022, LECT NOTES COMPUT SC, V13677, P615, DOI 10.1007/978-3-031-19790-1_37
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Welinder P., 2010, California Inst. Technol., Tech. Rep. CNS-TR-2010-001
   Wonkyung Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P465, DOI 10.1007/978-3-030-58586-0_28
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11046, DOI 10.1109/CVPR42600.2020.01106
   Yang ZZ, 2023, PROC CVPR IEEE, P14059, DOI 10.1109/CVPR52729.2023.01351
   Yinglan Ma, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P694, DOI 10.1109/CVPRW.2019.00096
   Yoo SB, 2014, IEEE T MULTIMEDIA, V16, P1536, DOI 10.1109/TMM.2014.2327563
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XS, 2018, IEEE IMAGE PROC, P390, DOI 10.1109/ICIP.2018.8451694
   Zhang Y., 2019, PROC INT C LEARN REP
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
NR 62
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6424
EP 6436
DI 10.1109/TMM.2024.3350917
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600022
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ren, QH
   Mao, QR
   Lu, SJ
AF Ren, Qinghua
   Mao, Qirong
   Lu, Shijian
TI Prototypical Bidirectional Adaptation and Learning for Cross-Domain
   Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic segmentation; domain adaptation; bidirectional adaptation;
   prototypical learning
AB Cross-domain semantic segmentation, which aims to address the distribution shift while adapting from a labeled source domain to an unlabeled target domain, has achieved great progress in recent years. However, most existing work adopts a source-to-target adaptation path, which often suffers from clear class mismatching or class imbalance issues. We design PBAL, a prototypical bidirectional adaptation and learning technique that introduces bidirectional prototype learning and prototypical self-training for optimal inter-domain alignment and adaptation. We perform bidirectional alignments in a complementary and cooperative manner which balances both dominant and tail categories as well as easy and hard samples effectively. In addition, We derive prototypes efficiently from a source-trained classifier, which enables class-aware adaptation as well as synchronous prototype updating and network optimization. Further, we re-examine self-training and introduce prototypical contrast above it which greatly improves inter-domain alignment by promoting better intra-class compactness and inter-class separability in the feature space. Extensive experiments over two widely studied benchmarks show that the proposed PBAL achieves superior domain adaptation performance as compared with the state-of-the-art.
C1 [Ren, Qinghua; Mao, Qirong] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
   [Mao, Qirong] Jiangsu Engn Res Ctr Big Data Ubiquitous Percept &, Zhenjiang 212013, Jiangsu, Peoples R China.
   [Lu, Shijian] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Jiangsu University; Nanyang Technological University
RP Lu, SJ (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM qinghua.ren@ujs.edu.cn; mao_qr@ujs.edu.cn; shijian.lu@ntu.edu.sg
OI Lu, Shijian/0000-0002-6766-2506
FU National Natural Science Foundation of China
FX No Statement Available
CR Araslanov N, 2021, PROC CVPR IEEE, P15379, DOI 10.1109/CVPR46437.2021.01513
   Chang WL, 2019, PROC CVPR IEEE, P1900, DOI 10.1109/CVPR.2019.00200
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T., 2020, Adv. Neural Inf. Process. Syst., V33, P22243, DOI DOI 10.48550/ARXIV.2006.10029
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189
   Choi J, 2019, IEEE I CONF COMP VIS, P6829, DOI 10.1109/ICCV.2019.00693
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   den Oord A. V., 2018, Representation learning with contrastive predictive coding.
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Gao L, 2021, AAAI CONF ARTIF INTE, V35, P7528
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   Guo XQ, 2021, PROC CVPR IEEE, P3926, DOI 10.1109/CVPR46437.2021.00392
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P642, DOI 10.1007/978-3-030-58568-6_38
   Huang J., 2020, P ECCV, P705
   Huang JX, 2021, PROC CVPR IEEE, P6887, DOI 10.1109/CVPR46437.2021.00682
   Huo X., 2022, IEEE C COMPUT VIS PA, P7075
   Huo YK, 2019, NEUROIMAGE, V194, P105, DOI 10.1016/j.neuroimage.2019.03.041
   Jiang ZK, 2022, LECT NOTES COMPUT SC, V13694, P36, DOI 10.1007/978-3-031-19830-4_3
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kang Guoliang, 2020, NEURIPS
   Lai X, 2022, LECT NOTES COMPUT SC, V13693, P369, DOI 10.1007/978-3-031-19827-4_22
   Lee G, 2022, LECT NOTES COMPUT SC, V13690, P38, DOI 10.1007/978-3-031-20056-4_3
   Lee S, 2021, AAAI CONF ARTIF INTE, V35, P8306
   Li RH, 2022, PROC CVPR IEEE, P11583, DOI 10.1109/CVPR52688.2022.01130
   Li TH, 2022, PROC CVPR IEEE, P6908, DOI 10.1109/CVPR52688.2022.00679
   Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710
   Liu YH, 2022, PROC CVPR IEEE, P7032, DOI 10.1109/CVPR52688.2022.00691
   Luc P, 2017, IEEE I CONF COMP VIS, P648, DOI 10.1109/ICCV.2017.77
   Luo YW, 2022, IEEE T PATTERN ANAL, V44, P3940, DOI 10.1109/TPAMI.2021.3064379
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Lv FM, 2020, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR42600.2020.00439
   Myeongjin Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12972, DOI 10.1109/CVPR42600.2020.01299
   Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Soja A, 2021, RAPID PROTOTYPING J, V27, P59, DOI 10.1108/RPJ-01-2020-0009
   Tanwisuth K, 2021, ADV NEUR IN, V34
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GT, 2019, FRONT COMPUT NEUROSC, V13, DOI 10.3389/fncom.2019.00056
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wang T., PROC INT C MACH LEAR, P9929
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Wu ZX, 2019, IEEE I CONF COMP VIS, P2121, DOI 10.1109/ICCV.2019.00221
   Xie BH, 2023, IEEE T PATTERN ANAL, V45, P9004, DOI 10.1109/TPAMI.2023.3237740
   Xue ZF, 2022, IEEE T MULTIMEDIA, V24, P1253, DOI 10.1109/TMM.2021.3062497
   Yang YC, 2020, PROC CVPR IEEE, P4084, DOI 10.1109/CVPR42600.2020.00414
   You FM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1866, DOI 10.1145/3503161.3548079
   You FM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3293, DOI 10.1145/3474085.3475482
   Zhang P, 2021, PROC CVPR IEEE, P12409, DOI 10.1109/CVPR46437.2021.01223
   Zhang QM, 2019, ADV NEUR IN, V32
   Zheng ZD, 2021, INT J COMPUT VISION, V129, P1106, DOI 10.1007/s11263-020-01395-y
   Zhou TF, 2022, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR52688.2022.00261
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
   Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608
NR 61
TC 1
Z9 1
U1 15
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 501
EP 513
DI 10.1109/TMM.2023.3266892
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000004
DA 2024-08-05
ER

PT J
AU Ru, JH
   Tian, J
   Xiao, CW
   Li, JJ
   Shen, HT
AF Ru, Jinghan
   Tian, Jun
   Xiao, Chengwei
   Li, Jingjing
   Shen, Heng Tao
TI Imbalanced Open Set Domain Adaptation via Moving-Threshold Estimation
   and Gradual Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Prototypes; Adaptation models; Target recognition; Feature
   extraction; Entropy; Humanities; Transfer learning; open set domain
   adaptation; imbalanced domain adaptation
AB Multimedia applications are often associated with cross-domain knowledge transfer, where Unsupervised Domain Adaptation (UDA) can be used to reduce the domain shifts. Open Set Domain Adaptation (OSDA) aims to transfer knowledge from a well-labeled source domain to an unlabeled target domain under the assumption that the target domain contains unknown classes. Existing OSDA methods consistently lay stress on the covariate shift, ignoring the potential label shift problem. The performance of OSDA methods degrades drastically under intra-domain class imbalance and inter-domain label shift. However, little attention has been paid to this issue in the community. In this paper, the Imbalanced Open Set Domain Adaptation (IOSDA) is explored where the covariate shift, label shift and category mismatch exist simultaneously. To alleviate the negative effects raised by label shift in OSDA, we propose Open-set Moving-threshold Estimation and Gradual Alignment (OMEGA) - a novel architecture that improves existing OSDA methods on class-imbalanced data. Specifically, a novel unknown-aware target clustering scheme is proposed to form tight clusters in the target domain to reduce the negative effects of label shift and intra-domain class imbalance. Furthermore, moving-threshold estimation is designed to generate specific thresholds for each target sample rather than using one for all. Extensive experiments on IOSDA, OSDA and OPDA benchmarks demonstrate that our method could significantly outperform existing state-of-the-arts.
C1 [Ru, Jinghan; Tian, Jun; Li, Jingjing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Xiao, Chengwei] Univ Elect Sci & Technol China, Med Sch, Chengdu 611731, Peoples R China.
   [Li, Jingjing] Univ Elect Sci & Technol China, Inst Elect & Informat Engn, Shenzhen 611731, Guangdong, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; University of Electronic
   Science & Technology of China
RP Li, JJ (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM jinghanru@std.uestc.edu.cn; juntian@std.uestc.edu.cn; carbinger@qq.com;
   lijin117@yeah.net; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021; Li, Jingjing/T-6522-2019
OI Xiao, Chengwei/0000-0001-7145-1595; Tian, Jun/0000-0003-4602-457X
FU National Natural Science Foundation of China
FX No Statement Available
CR Aslam Asra, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P261, DOI 10.1145/3372278.3390722
   Bo Fu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P567, DOI 10.1007/978-3-030-58555-6_34
   Bucci Silvia, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P422, DOI 10.1007/978-3-030-58517-4_25
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Cao D, 2020, PROC CVPR IEEE, P5670, DOI 10.1109/CVPR42600.2020.00571
   Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9
   Cao ZJ, 2019, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR.2019.00310
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Garg S., 2022, Advances in Neural Information Processing Systems, P22531
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Huang J., 2020, P ECCV, P705
   Huang JX, 2021, ADV NEUR IN, V34
   Huang JX, 2021, Arxiv, DOI arXiv:2106.02845
   Huang JX, 2022, PROC CVPR IEEE, P1193, DOI 10.1109/CVPR52688.2022.00127
   Huang JX, 2021, PROC CVPR IEEE, P10128, DOI 10.1109/CVPR46437.2021.01000
   Huang JX, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108384
   Huang Jiaxing, 2021, P IEEECVF INT C COMP, P8988
   Krizhevsky A, 2014, Arxiv, DOI [arXiv:1404.5997, 10.48550/ARXIV.1404.5997]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee J, 2022, 39 INT C MACHINE LEA
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P747, DOI 10.1145/3343031.3350902
   Li XH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3330, DOI 10.1145/3474085.3475487
   Lipton ZC, 2018, PR MACH LEARN RES, V80
   Liu H, 2019, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2019.00304
   Liu ZF, 2023, IEEE T CYBERNETICS, V53, P7353, DOI 10.1109/TCYB.2022.3228301
   Long MS, 2016, ADV NEUR IN, V29
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Mengxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13933, DOI 10.1109/CVPR42600.2020.01395
   Menon Aditya Krishna, 2020, ARXIV200707314
   Paszke A, 2019, ADV NEUR IN, V32
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271
   Peng YX, 2020, IEEE T CIRC SYST VID, V30, P4368, DOI 10.1109/TCSVT.2019.2953692
   Peyré G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073
   Ragab M, 2021, IEEE T IND INFORM, V17, P5239, DOI 10.1109/TII.2020.3032690
   Reed WJ, 2001, ECON LETT, V74, P15, DOI 10.1016/S0165-1765(01)00524-9
   Saito K., 2020, Advances in Neural Information Processing Systems, V33, P16282
   Saito K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8980, DOI 10.1109/ICCV48922.2021.00887
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Shen J, 2018, AAAI CONF ARTIF INTE, P4058
   Shuhan Tan, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P585, DOI 10.1007/978-3-030-66415-2_38
   Tanwisuth K, 2021, ADV NEUR IN, V34
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wang ZM, 2020, IEEE T NEUR NET LEAR, V31, P2387, DOI 10.1109/TNNLS.2019.2935608
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wen YD, 2019, INT J COMPUT VISION, V127, P668, DOI 10.1007/s11263-018-01142-4
   Wu KK, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3212525
   Yang JF, 2023, IEEE T CYBERNETICS, V53, P1106, DOI 10.1109/TCYB.2021.3093888
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Yue XY, 2021, PROC CVPR IEEE, P13829, DOI 10.1109/CVPR46437.2021.01362
   Zhang F., 2022, arXiv
   Zhang W, 2021, IEEE T IND INFORM, V17, P7957, DOI 10.1109/TII.2021.3064377
   Zhang W, 2021, IEEE T IND INFORM, V17, P7445, DOI 10.1109/TII.2021.3054651
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhao H, 2019, INT C MACHINE LEARNI, P7523
NR 67
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2504
EP 2514
DI 10.1109/TMM.2023.3297768
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100036
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Su, YK
   Deng, JL
   Sun, RZ
   Lin, GS
   Su, HJ
   Wu, QY
AF Su, Yukun
   Deng, Jingliang
   Sun, Ruizhou
   Lin, Guosheng
   Su, Hanjing
   Wu, Qingyao
TI A Unified Transformer Framework for Group-Based Segmentation:
   Co-Segmentation, Co-Saliency Detection and Video Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-object; long-range dependency; transformer; activation
ID GRAPH; OPTIMIZATION; DISTANCE
AB Humans tend to mine objects by learning from a group of images or several frames of video since we live in a dynamic world. In the computer vision area, many researchers focus on co-segmentation (CoS), co-saliency detection (CoSD) and video salient object detection (VSOD) to discover the co-occurrent objects. However, previous approaches design different networks for these similar tasks separately, and they are difficult to apply to each other. Besides, they fail to take full advantage of the cues among inter- and intra-feature within a group of images. In this paper, we introduce a unified framework to tackle these issues from a unified view, term as <bold>UFGS</bold> (<bold>U</bold>nified <bold>F</bold>ramework for <bold>G</bold>roup-based <bold>S</bold>egmentation). Specifically, we first introduce a transformer block, which views the image feature as a patch token and then captures their long-range dependencies through the self-attention mechanism. This can help the network to excavate the patch-structured similarities among the relevant objects. Furthermore, we propose an intra-MLP learning module to produce self-mask to enhance the network to avoid partial activation. Extensive experiments on four CoS benchmarks (PASCAL, iCoseg Internet and MSRC), three CoSD benchmarks (Cosal2015, CoSOD3k, and CocA) and five VSOD benchmarks (DAVIS(16), FBMS, ViSal, SegV2, and DAVSOD) show that our method outperforms other state-of-the-arts on three different tasks in both accuracy and speed by using the same network architecture, which can reach 140 FPS in real-time.
C1 [Su, Yukun; Deng, Jingliang; Sun, Ruizhou] South China Univ Technol, Sch Software Engn, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510006, Peoples R China.
   [Lin, Guosheng] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Su, Hanjing] Tencent, Shenzhen 518000, Peoples R China.
   [Wu, Qingyao] South China Univ, Sch Software Engn, Pazhou Lab, Guangzhou 510006, Peoples R China.
   [Wu, Qingyao] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 South China University of Technology; Nanyang Technological University;
   Tencent; Pazhou Lab; Peng Cheng Laboratory
RP Lin, GS (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.; Wu, QY (corresponding author), South China Univ, Sch Software Engn, Pazhou Lab, Guangzhou 510006, Peoples R China.
EM suyukun666@gmail.com; djl0628@126.com; ruizhousun@foxmail.com;
   gslin@ntu.edu.sg; hanjingsu@gmail.com; qyw@scut.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Bradski G, 2000, DR DOBBS J, V25, P120
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen H, 2019, LECT NOTES COMPUT SC, V11364, P435, DOI 10.1007/978-3-030-20870-7_27
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165
   Chen YC, 2021, IEEE T PATTERN ANAL, V43, P3632, DOI 10.1109/TPAMI.2020.2985395
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Cong RM, 2019, IEEE T IMAGE PROCESS, V28, P4819, DOI 10.1109/TIP.2019.2910377
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan Q, 2021, PROC CVPR IEEE, P12283, DOI 10.1109/CVPR46437.2021.01211
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Goyal A, 2022, P ROY SOC A-MATH PHY, V478, DOI 10.1098/rspa.2021.0068
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hsu KJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P748
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jerripothula KR, 2017, PROC CVPR IEEE, P3881, DOI 10.1109/CVPR.2017.413
   Jerripothula KR, 2016, LECT NOTES COMPUT SC, V9911, P187, DOI 10.1007/978-3-319-46478-7_12
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Ji GP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4902, DOI 10.1109/ICCV48922.2021.00488
   Ji YZ, 2021, IEEE T NEUR NET LEAR, V32, P2676, DOI 10.1109/TNNLS.2020.3007534
   Jiang B, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1375, DOI 10.1145/3343031.3350860
   Kingma D. P., 2014, arXiv
   Kong Y., 2021, IEEE Trans. Multimedia, V25, P557
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P818
   Li B, 2019, IEEE I CONF COMP VIS, P8518, DOI 10.1109/ICCV.2019.00861
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li SY, 2018, LECT NOTES COMPUT SC, V11207, P215, DOI 10.1007/978-3-030-01219-9_13
   Li TP, 2022, IEEE T MULTIMEDIA, V24, P492, DOI 10.1109/TMM.2021.3054526
   Li WH, 2019, LECT NOTES COMPUT SC, V11363, P638, DOI 10.1007/978-3-030-20893-6_40
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu R., 2021, P IEEE CVF INT C COM, P14040
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu YB, 2020, PROC CVPR IEEE, P4462, DOI 10.1109/CVPR42600.2020.00452
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P7885, DOI 10.1109/TPAMI.2021.3115815
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Mahadevan S, 2023, Arxiv, DOI arXiv:2008.11516
   Mémoli F, 2014, AXIOMS, V3, DOI 10.3390/axioms3030335
   Mingmin Zhen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P445, DOI 10.1007/978-3-030-58583-9_27
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Park H, 2021, PROC CVPR IEEE, P8401, DOI 10.1109/CVPR46437.2021.00830
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Qian XL, 2023, IEEE T MULTIMEDIA, V25, P1810, DOI 10.1109/TMM.2022.3167805
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Rother C., 2006, IEEE Computer Society Conference on, V1, P993
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Solomon J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925903
   Song HK, 2016, IEEE SIGNAL PROC LET, V23, P1722, DOI 10.1109/LSP.2016.2615293
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Su YK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6984, DOI 10.1109/ICCV48922.2021.00692
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang C, 2019, AAAI CONF ARTIF INTE, P8917
   Wang C, 2017, IEEE T IMAGE PROCESS, V26, P5825, DOI 10.1109/TIP.2017.2750410
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P873, DOI 10.1145/3343031.3350882
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Xu MZ, 2020, IEEE T CIRC SYST VID, V30, P2191, DOI 10.1109/TCSVT.2019.2920652
   Xu MZ, 2019, IEEE T MULTIMEDIA, V21, P2790, DOI 10.1109/TMM.2019.2914889
   Xu N, 2018, Arxiv, DOI arXiv:1809.03327
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Zhang C, 2021, IEEE T IMAGE PROCESS, V30, P5652, DOI 10.1109/TIP.2021.3087401
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang K., 2020, PROC IEEECVF C COMP, P9047
   Zhang KH, 2020, AAAI CONF ARTIF INTE, V34, P12813
   Zhang KH, 2019, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2019.00321
   Zhang Kaihua, 2021, P IEEE CVF C COMP VI, P13703
   Zhang N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4147, DOI 10.1109/ICCV48922.2021.00413
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
NR 107
TC 10
Z9 10
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 313
EP 325
DI 10.1109/TMM.2023.3264883
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500012
OA Green Submitted
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Wu, KJ
   Yang, Y
   Liu, Q
   Jiang, GY
   Zhang, XP
AF Wu, Kejun
   Yang, You
   Liu, Qiong
   Jiang, Gangyi
   Zhang, Xiao-Ping
TI Hierarchical Independent Coding Scheme for Varifocal Multiview Images
   Based on Angular-Focal Joint Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Redundancy; Three-dimensional displays; Encoding;
   Visualization; Focusing; Scalability; Varifocal multiview; multiview;
   image compression; video coding
ID STACK; REPRESENTATION; SYSTEM; DEPTH
AB Varifocal multiview (VFMV) images are dense views that focus on variable focal planes. Thus, VFMV images are highly redundant in the angular, spatial and focal dimensions. In this article, the redundancies of VFMV images are analyzed and represented by full parallaxes and focal inconsistency. To exploit these distinctive redundancies, we propose a hierarchical independent coding scheme based on angular-focal joint prediction. The scheme is constructed by hierarchical independent prediction structure (HIPS) and angular-focal joint prediction (AFJP). The HIPS separates all views into several independent subdivisions and assigns different hierarchies inside each subdivision, which enhances random access capability and scalability. The AFJP conducts motion estimation and focal approximation simultaneously to predict parallaxes and focal inconsistency. Therefore, the redundancies in the angular and focal dimensions can be exploited by the proposed coding scheme. We construct a VFMV dataset with 10 test sequences for different acquisition methods. The experimental results on these test sequences demonstrate that the proposed scheme outperforms all comparison schemes in objective quality, subjective quality and random access capability. Specifically, the proposed coding scheme achieves up to 2.661 dB PSNR gains and 52.817% bitrate savings compared with the HEVC random access benchmark scheme.
C1 [Wu, Kejun] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Yang, You; Liu, Qiong] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Zhang, Xiao-Ping] Tsinghua Berkeley Shenzhen Inst, Shenzhen 518055, Peoples R China.
   [Zhang, Xiao-Ping] Toronto Metropolitan Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada.
C3 Nanyang Technological University; Huazhong University of Science &
   Technology; Ningbo University; Tsinghua Shenzhen International Graduate
   School; Toronto Metropolitan University
RP Yang, Y (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM kejun.wu@ntu.edu.sg; yangyou@hust.edu.cn; q.liu@hust.edu.cn;
   jianggangyi@nbu.edu.cn; xpzhang@ieee.org
RI Zhang, Xiaoping/AFW-5367-2022; Zhang, Xiaoping/AAX-7947-2021
OI Zhang, Xiaoping/0000-0002-8891-0978; Zhang, Xiaoping/0000-0002-8891-0978
FU National Key Research and Development Program of China
FX No Statement Available
CR Aksit K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130892
   Ali U, 2021, IEEE T IMAGE PROCESS, V30, P7215, DOI 10.1109/TIP.2021.3100268
   Amirpour H, 2019, Arxiv, DOI arXiv:1901.11396
   Amirpour H, 2022, EUR W VIS INF PROCES, DOI 10.1109/EUVIP53989.2022.9922749
   Amirpour H, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3471905
   Chang JHR, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275015
   Chen G, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3234972
   Chowdhury SAH, 2021, NEURAL COMPUT APPL, V33, P7421, DOI 10.1007/s00521-021-05926-7
   Dai F, 2015, IEEE IMAGE PROC, P4733, DOI 10.1109/ICIP.2015.7351705
   Gomes P, 2019, EUR W VIS INF PROCES, P16, DOI [10.1109/euvip47703.2019.8946268, 10.1109/EUVIP47703.2019.8946268]
   Harding K., 2016, Proc. SPIE, V9868, P83
   Huang AP, 2021, IEEE T IMAGE PROCESS, V30, P6997, DOI 10.1109/TIP.2021.3101917
   Huang AP, 2021, IEEE T IMAGE PROCESS, V30, P6772, DOI 10.1109/TIP.2021.3096086
   Jiang H, 2021, IEEE T IMAGE PROCESS, V30, P2364, DOI 10.1109/TIP.2021.3052073
   Jing W, 2020, IEEE INT C INT ROBOT, P1480, DOI 10.1109/IROS45743.2020.9341089
   Khire S, 2012, IEEE ENG MED BIO, P5424, DOI 10.1109/EMBC.2012.6347221
   Kuthirummal S, 2011, IEEE T PATTERN ANAL, V33, P58, DOI 10.1109/TPAMI.2010.66
   Li HJ, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P216, DOI 10.1109/dicta47822.2019.8946006
   Lin JL, 2022, IEEE T BROADCAST, V68, P572, DOI 10.1109/TBC.2022.3174477
   Liu D., 2016, P IEEE INT C MULT EX, P1, DOI [10.1109/ICMEW.2016.7574674, DOI 10.1109/ICMEW.2016.7574674]
   Liu SP, 2022, IEEE T MULTIMEDIA, V24, P2392, DOI 10.1109/TMM.2021.3080076
   McCann K., 2014, 18 JCTVC M SAPP JAP
   Mery D, 2015, IEEE-ASME T MECH, V20, P338, DOI 10.1109/TMECH.2014.2311032
   Monteiro RJS, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116202
   Panzirsch M, 2022, J SOC INF DISPLAY, V30, P319, DOI 10.1002/jsid.1112
   Pereira F., 2019, Doc. ISO/IEC JTC 1/SC29/WG1N84025
   Pereira F., 2018, Academic press library in signal processing, V6, P75
   Pi JY, 2022, IEEE T CIRC SYST VID, V32, P8768, DOI 10.1109/TCSVT.2022.3192665
   Reda F, 2022, OPT EXPRESS, V30, P12695, DOI 10.1364/OE.455520
   Rizkallah M, 2016, EUR SIGNAL PR CONF, P898, DOI 10.1109/EUSIPCO.2016.7760378
   Santos JM, 2022, SIGNAL PROCESS-IMAGE, V105, DOI 10.1016/j.image.2022.116687
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Torres C, 2018, IEEE T MULTIMEDIA, V20, P3057, DOI 10.1109/TMM.2018.2829162
   Duong VV, 2019, IEEE INT SYM BROADB, DOI 10.1109/bmsb47279.2019.8971928
   Wang M, 2018, IEEE T MULTIMEDIA, V20, P620, DOI 10.1109/TMM.2017.2748459
   Wetzstein G., 2019, U.S. Patent, Patent No. [10,192,292, 10192292]
   Wu KJ, 2023, IEEE T MULTIMEDIA, V25, P3975, DOI 10.1109/TMM.2022.3169055
   Wu KJ, 2023, OPT EXPRESS, V31, P11659, DOI 10.1364/OE.482141
   Wu KJ, 2021, Arxiv, DOI arXiv:2111.10099
   Wu KJ, 2022, IEEE T CIRC SYST VID, V32, P523, DOI 10.1109/TCSVT.2021.3066523
   Wu KJ, 2020, OPT EXPRESS, V28, P40024, DOI 10.1364/OE.413523
   Wu KJ, 2019, IEEE DATA COMPR CONF, P608, DOI 10.1109/DCC.2019.00120
   Wu XJ, 2021, IEEE T CIRC SYST VID, V31, P4630, DOI 10.1109/TCSVT.2021.3101484
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Zarella MD, 2019, J MED IMAGING, V6, DOI 10.1117/1.JMI.6.4.047502
   Zhan T, 2020, PHOTONIX, V1, DOI 10.1186/s43074-020-00010-0
   Zhan T, 2019, OPT EXPRESS, V27, P27507, DOI 10.1364/OE.27.027507
   Zhao X, 2019, RESULTS PHYS, V12, P1520, DOI 10.1016/j.rinp.2019.01.045
NR 48
TC 7
Z9 7
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2993
EP 3006
DI 10.1109/TMM.2023.3306072
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700004
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Yang, X
   Wang, XQ
   Yang, D
AF Yang, Xi
   Wang, Xiaoqi
   Yang, Dong
TI Improving Cross-Modal Constraints: Text Attribute Person Search With
   Graph Attention Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; computer vision; text-image retrieval; attribute-based
   person re-identification; graph attention network
ID REIDENTIFICATION
AB Nowadays, video surveillance systems are widely deployed in public areas. However, in the unreachable corner of surveillance cameras, it still seems impossible to find the suspects only depending on eyewitness memory. Therefore, the technology that can detect particular pedestrians only by text-based attributes, or text-attribute person search, attracts lots of attention from academia. Most existing text-attribute person search methods focus on learning better feature representations by designing better network structures or using local information but lack direct constraints between modalities. This paper proposes a feature embedding motivated and graph attention network-based model, optimizing the feature extraction process by its attention mechanism. Meanwhile, this paper studies the effectiveness of the attention mechanism in feature alignment, and thus redesigns the cross-attention module, simplifying the complexity of the model and constraining the inter-modality gap in maximum by the self-attention mechanism of the graph attention network. In this way, the method simultaneously offsets the influence of modal-specific features and optimizes the number of parameters. Thus, the method improves performance and reduces time costs. Meanwhile, according to the inherent feature of attributes, this article introduces a novel embedding space, which effectively enhances the discrimination ability of the model. Extensive experiments illustrate the superiority of our model in two widely used text-attribute person search benchmarks among the state-of-the-art methods.
C1 [Yang, Xi; Wang, Xiaoqi] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Yang, Dong] Xian Inst Space Radio Technol, Xian 710100, Peoples R China.
C3 Xidian University
RP Yang, D (corresponding author), Xian Inst Space Radio Technol, Xian 710100, Peoples R China.
EM yangx@xidian.edu.cn; xqwang14@stu.xidian.edu.cn; yangd504@126.com
OI Wang, Xiaoqi/0000-0002-7142-9571
FU National Natural Science Foundation of China
FX No Statement Available
CR Andrew R., 2013, INT C MACHINE LEARNI, P1247, DOI DOI 10.5555/3042817.3043076
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Dalal N., CVPR, P886, DOI DOI 10.1109/CVPR.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P2470, DOI 10.1109/ICCV.2015.284
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jeong J., 2021, IEEE INT CONFCOMPUT, P2016
   Ji Z, 2023, IEEE T MULTIMEDIA, V25, P7699, DOI 10.1109/TMM.2022.3225754
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Li DW, 2018, IEEE INT CON MULTI
   Li DW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P111, DOI 10.1109/ACPR.2015.7486476
   Li QZ, 2019, AAAI CONF ARTIF INTE, P8634
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Li Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P300, DOI 10.1145/3474085.3475695
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Sarafianos N, 2018, LECT NOTES COMPUT SC, V11215, P708, DOI 10.1007/978-3-030-01252-6_42
   Specker A, 2023, IEEE WINT CONF APPL, P981, DOI 10.1109/WACV56688.2023.00104
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Sudowe P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P329, DOI 10.1109/ICCVW.2015.51
   Tan ZC, 2020, AAAI CONF ARTIF INTE, V34, P12055
   Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velickovi P., 2018, PROC INT C LEARNREPR, P1
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang X, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108220
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Yao L, 2019, AAAI CONF ARTIF INTE, P7370
   Yin Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1100
   Yu-Tong Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P230, DOI 10.1007/978-3-030-58568-6_14
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zha ZJ, 2020, IEEE T MULTIMEDIA, V22, P1836, DOI 10.1109/TMM.2020.2972168
   Zhang HH, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107799
   Zhang K., 2020, PROC IEEECVF C COMP, P9047
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhao H., 2020, P IEEECVF C COMPUTER, P10076, DOI [10.1109/CVPR42600.2020.01009, DOI 10.1109/CVPR42600.2020.01009]
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P184, DOI 10.1109/ICCV48922.2021.00025
NR 52
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2493
EP 2503
DI 10.1109/TMM.2023.3297391
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100029
DA 2024-08-05
ER

PT J
AU Zhang, KP
   Sato, Y
AF Zhang, Kaipeng
   Sato, Yoichi
TI Semantic Image Segmentation by Dynamic Discriminative Prototypes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; semantic segmentation
AB Semantic segmentation achieves significant success through large-scale training data. Meanwhile, few-shot semantic segmentation was proposed to segment image regions of novel classes through few labeled testing data. However, it ignores classes previously learned from training data. This paper proposes a new segmentation framework called segmentation by dynamic prototype (SDP), which can simultaneously segment the image regions of base classes learned from many training data and novel classes learned from a few testing data. SDP performs segmentation by searching for the nearest prototype of each pixel's features. Different prototypes are representative features for different classes. In testing, SDP dynamically constructs novel prototypes according to support images while maintaining base prototypes learned from training images. The main challenge of SDP is how to achieve intra-image compactness, intra-class compactness, and inter-class separability simultaneously in the feature space. To tackle these challenges, we first introduce a discriminative pixelwise feature and prototype training method to improve the above three types of feature discriminability. We then introduce a mask refinement process in testing, which refines support images' masks to extract more separable novel prototypes. In addition, we introduce a prototype adaptation process in testing, which allows all prototypes to adapt to query images to reduce the prototypes' intra-class variances. Our approach achieves state-of-the-art results for novel class segmentation against existing few-shot semantic segmentation methods on PASCAL-5(i) and COCO-20(i) benchmarks in combination with superior runtime efficiency. In addition, our method can maintain strong results in the segmentation of base classes.
C1 [Zhang, Kaipeng; Sato, Yoichi] Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan.
   [Zhang, Kaipeng] Shanghai AI Lab, Shanghai 200000, Peoples R China.
C3 University of Tokyo; Shanghai Artificial Intelligence Laboratory
RP Sato, Y (corresponding author), Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan.
EM kp_zhang@foxmail.com; ysato@iis.u-tokyo.ac.jp
FU JSPS KAKENHI
FX No Statement Available
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Antoniou A, 2018, Arxiv, DOI arXiv:1711.04340
   Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen WY, 2020, Arxiv, DOI arXiv:1904.04232
   Cheng G., 2022, IEEE Trans. Geosci. Remote Sens., V60
   Cheng G, 2018, IEEE T GEOSCI REMOTE, V56, P2811, DOI 10.1109/TGRS.2017.2783902
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Finn C, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He K., 2016, European conference on computer vision, P770, DOI DOI 10.1007/978-3-319-46493-0_38
   Hu T, 2019, AAAI CONF ARTIF INTE, P8441
   Huang PL, 2022, IEEE-CAA J AUTOMATIC, V9, P339, DOI 10.1109/JAS.2021.1004210
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Li PK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P64, DOI 10.1145/3394171.3413944
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu WD, 2020, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR42600.2020.00422
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu Y., 2020, COMPUTER VISION ECCV, P142, DOI DOI 10.1007/978-3-030-58545-79
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Rakelly K., 2018, Conditional networks for few-shot semantic segmentation
   Ravi S., 2017, INT C LEARN REPR
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shaban Amirreza, 2017, BMVC, DOI 10.5244/C.31.167
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Snell J., 2017, Advances in Neural Information Processing Systems, V30, P4077
   Sun Y, 2014, ADV NEUR IN, V27
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P497, DOI 10.1109/ICCV48922.2021.00056
   Xian YQ, 2019, PROC CVPR IEEE, P8248, DOI 10.1109/CVPR.2019.00845
   Yang XH, 2020, Arxiv, DOI arXiv:2008.06226
   Yao XW, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3119852
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P5866, DOI 10.1109/TPAMI.2021.3074313
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P3349, DOI 10.1109/TPAMI.2020.3046647
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang XL, 2022, IEEE T NEUR NET LEAR, V33, P6484, DOI 10.1109/TNNLS.2021.3081693
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
NR 63
TC 3
Z9 3
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 737
EP 749
DI 10.1109/TMM.2023.3270637
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000003
DA 2024-08-05
ER

PT J
AU Zhou, JC
   Wang, SY
   Lin, ZF
   Jiang, QP
   Sohel, F
AF Zhou, Jingchun
   Wang, Shiyin
   Lin, Zifan
   Jiang, Qiuping
   Sohel, Ferdous
TI A Pixel Distribution Remapping and Multi-Prior Retinex Variational Model
   for Underwater Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Underwater image enhancement; retinex theory; variational model;
   underwater prior knowledge
ID FRAMEWORK
AB High-quality underwater imaging is crucial for underwater exploration. However, particle scattering and light absorption by seawater significantly degrade image clarity. To address these issues, we propose a novel underwater image enhancement (UIE) method that combines pixel distribution remapping (PDR) with a multi-priority Retinex variational model. We design a pre-compensation method for severely attenuated channels that effectively prevents new color artifacts during color correction. By combining the inter-channel coupling relationships, we compute a limiting factor to remap pixel distribution curves to improve image contrast. In addition, considering the significant noise interference, we introduce the prior knowledge, including underwater noise and texture priors, while constructing the variational model, and design penalty terms that match the underwater characteristics to remove excessive noise in the reflectance component. Our approach efficiently decouples the illumination and reflectance components using a rapid solver. Subsequently, gamma correction adjusts the illumination component, and the corrected illumination and reflectance components are fused to reconstruct the final natural output image. Comprehensive evaluations across various datasets reveal that our approach significantly surpasses current state-of-the-art (SOTA) methods. These results demonstrate the effectiveness of our method in correcting color bias and compensating for luminance losses in underwater imagery.
C1 [Zhou, Jingchun; Wang, Shiyin] Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116026, Peoples R China.
   [Lin, Zifan] Univ Western Australia, Dept Elect & Elect Engn, Perth, WA 6009, Australia.
   [Jiang, Qiuping] Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Sohel, Ferdous] Murdoch Univ, Sch Informat Technol, Murdoch, WA 6150, Australia.
C3 Dalian Maritime University; University of Western Australia; Ningbo
   University; Murdoch University
RP Jiang, QP (corresponding author), Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM zhoujingchun03@gmail.com; w1026844615@163.com; zifan.lin@uwa.edu.au;
   jiangqiuping@nbu.edu.cn; f.sohel@murdoch.edu.au
RI Zhou, Jingchun/AAF-6817-2019; Sohel, Ferdous/C-2428-2013
OI Zhou, Jingchun/0000-0002-4111-6240; Sohel, Ferdous/0000-0003-1557-4907;
   Qiuping, Jiang/0000-0002-6025-9343; Lin, Zifan/0000-0002-7046-3102
FU National Natural Science Foundation of China
FX No Statement Available
CR Barrett R., 1994, Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, DOI [10.1137/1.9781611971538, DOI 10.1137/1.9781611971538]
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Chen L, 2021, IEEE T CIRC SYST VID, V31, P3078, DOI 10.1109/TCSVT.2020.3035108
   Chen XQ, 2019, EURASIP J IMAGE VIDE, V2019, DOI 10.1186/s13640-019-0479-7
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Fang ZK, 2023, IEEE T CIRC SYST VID, V33, P2950, DOI 10.1109/TCSVT.2022.3229296
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Guo PF, 2023, IEEE T MULTIMEDIA, V25, P5093, DOI 10.1109/TMM.2022.3187212
   Guo PF, 2022, IEEE T MULTIMEDIA, V24, P1980, DOI 10.1109/TMM.2021.3074825
   Hitam Muhammad Suzuri, 2013, 2013 INT C COMPUTER
   Hou GJ, 2024, IEEE T CIRC SYST VID, V34, P799, DOI 10.1109/TCSVT.2023.3290363
   Jiang NF, 2022, IEEE T MULTIMEDIA, V24, DOI 10.1109/TMM.2021.3115442
   Jiang QP, 2024, IEEE T MULTIMEDIA, V26, P4884, DOI 10.1109/TMM.2023.3327613
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang ZY, 2022, IEEE T CIRC SYST VID, V32, P6584, DOI 10.1109/TCSVT.2022.3174817
   Kang YZ, 2023, IEEE T CIRC SYST VID, V33, P988, DOI 10.1109/TCSVT.2022.3208100
   Ke K, 2023, MEAS SCI TECHNOL, V34, DOI 10.1088/1361-6501/acb72d
   Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li DY, 2023, ENG APPL ARTIF INTEL, V107, DOI 10.1016/j.engappai.2023.106457
   Li JY, 2023, MULTIMED TOOLS APPL, V82, P6625, DOI 10.1007/s11042-022-13605-5
   Li MY, 2023, IEEE T CIRC SYST VID, V33, P3622, DOI 10.1109/TCSVT.2023.3237993
   Li M, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3145568
   Liu JY, 2024, INT J COMPUT VISION, V132, P1748, DOI 10.1007/s11263-023-01952-1
   Liu YT, 2024, IEEE T MULTIMEDIA, V26, P2560, DOI 10.1109/TMM.2023.3301226
   Ma L, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01900-z
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Qi Q, 2022, IEEE T CIRC SYST VID, V32, P1133, DOI 10.1109/TCSVT.2021.3074197
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Song W, 2018, LECT NOTES COMPUT SC, V11164, P678, DOI 10.1007/978-3-030-00776-8_62
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Wang ZY, 2023, IEEE T IMAGE PROCESS, V32, P1442, DOI 10.1109/TIP.2023.3244647
   Wu JJ, 2022, SIGNAL PROCESS-IMAGE, V109, DOI 10.1016/j.image.2022.116855
   Zhang DH, 2023, EXPERT SYST APPL, V231, DOI 10.1016/j.eswa.2023.120842
   Zhang WD, 2024, IEEE T CIRC SYST VID, V34, P2469, DOI 10.1109/TCSVT.2023.3299314
   Zhang WD, 2019, IEEE ACCESS, V7, P72492, DOI 10.1109/ACCESS.2019.2920403
   Zhou J., 2024, IEEE Trans. Geosci. Remote Sens., V62, P1
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2021, MEAS SCI TECHNOL, V32, DOI 10.1088/1361-6501/ac16ef
   Zhou JC, 2022, MULTIMED TOOLS APPL, V81, P1811, DOI 10.1007/s11042-021-11327-8
   Zhou Y, 2019, IEEE T CIRC SYST VID, V29, P907, DOI 10.1109/TCSVT.2018.2884615
   Zhuang PX, 2021, IEEE IMAGE PROC, P1709, DOI 10.1109/ICIP42928.2021.9506104
   Zhuang PX, 2021, ENG APPL ARTIF INTEL, V101, DOI 10.1016/j.engappai.2021.104171
NR 48
TC 4
Z9 4
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7838
EP 7849
DI 10.1109/TMM.2024.3372400
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000076
DA 2024-08-05
ER

PT J
AU Zhou, XQ
   Huang, HB
   Wang, ZL
   He, R
AF Zhou, Xiaoqiang
   Huang, Huaibo
   Wang, Zilei
   He, Ran
TI RISTRA: Recursive Image Super-Resolution Transformer With Relativistic
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Super resolution; vision transformer; parameter sharing
AB Many recent image restoration methods use Transformer as the backbone network and redesign the Transformer blocks. Differently, we explore the parameter-sharing mechanism over Transformer blocks and propose a dynamic recursive process to address the image super-resolution task efficiently. We firstly present a Recursive Image Super-resolution Transformer (RIST). By sharing the weights across different blocks, a plain forward process through the whole Transformer network can be folded into recursive iterations through a Transformer block. Such a parameter-sharing based recursive process can not only reduce the model size greatly, but also enable restoring images progressively. Features in the recursive process are modeled as a sequence and propagated with a temporal attention network. Besides, by analyzing the prediction variation across different iterations in RIST, we design a dynamic recursive process that can allocate adaptive computation costs to different samples. Specifically, a quality assessment network estimates the restoration quality and terminates the recursive process dynamically. We propose a relativistic learning strategy to simplify the objective from absolute image quality assessment to relativistic quality comparison. The proposed Recursive Image Super-resolution Transformer with Relativistic Assessment (RISTRA) reduces the model size greatly with the parameter-sharing mechanism, and achieves an instance-wise dynamic restoration process as well. Extensive experiments on several image super-resolution benchmarks show the superiority of our approach over state-of-the-art counterparts.
C1 [Zhou, Xiaoqiang; Wang, Zilei] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
   [Huang, Huaibo; He, Ran] Chinese Acad Sci, Inst Automation, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Huang, Huaibo; He, Ran] Chinese Acad Sci, Inst Automat, Ctr Res Intelligent Percept & Comp, Beijing 100190, Peoples R China.
   [Huang, Huaibo; He, Ran] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Automation, CAS;
   Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP He, R (corresponding author), Chinese Acad Sci, Inst Automation, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
EM xq525@mail.ustc.edu.cn; huaibo.huang@cripac.ia.ac.cn;
   zlwang@ustc.edu.cn; rhe@nlpr.ia.ac.cn
OI Zhou, Xiaoqiang/0000-0002-8709-3574; Huang, Huaibo/0000-0001-5866-2283
FU National Natural Science Foundation of China
FX No Statement Available
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cao J., 2021, P IEEE CVF C COMP VI, P5687
   Chen C, 2020, IEEE T PATTERN ANAL, V42, P3071, DOI 10.1109/TPAMI.2019.2921548
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Dai T, 2019, IEEE INT CONF COMP V, P3879, DOI 10.1109/ICCVW.2019.00481
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao G., 2022, P 31 INT JOINT C ART, P913
   Greenspan H, 2009, COMPUT J, V52, P43, DOI 10.1093/comjnl/bxm075
   Gu SH, 2020, IEEE T PATTERN ANAL, V42, P2437, DOI 10.1109/TPAMI.2019.2961672
   Guo B., 2022, P IEEE CVF C COMP VI, P1909
   Guo QS, 2019, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR.2019.00529
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jiang K, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107475
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Jolicoeur-Martineau A., 2018, INT C LEARN REPR
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kong XT, 2022, PROC CVPR IEEE, P5992, DOI 10.1109/CVPR52688.2022.00591
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Li XM, 2023, IEEE T PATTERN ANAL, V45, P5904, DOI 10.1109/TPAMI.2022.3215251
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang J, 2022, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR52688.2022.00557
   Liang JY, 2022, Arxiv, DOI [arXiv:2201.12288, DOI 10.48550/ARXIV.2201.12288]
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu AR, 2023, IEEE T PATTERN ANAL, V45, P5461, DOI 10.1109/TPAMI.2022.3203009
   Liu D, 2018, ADV NEUR IN, V31
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu YQ, 2022, IEEE T MULTIMEDIA, V24, P2259, DOI 10.1109/TMM.2021.3078615
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo ZW, 2022, PROC CVPR IEEE, P17621, DOI 10.1109/CVPR52688.2022.01712
   Magid SA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4268, DOI [10.1109/ICCV48922.2021.00425, 10.1109/iccv48922.2021.00425]
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428
   Rasti P, 2016, LECT NOTES COMPUT SC, V9756, P175, DOI 10.1007/978-3-319-41778-3_18
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen ZQ, 2022, LECT NOTES COMPUT SC, V13684, P727, DOI 10.1007/978-3-031-20053-3_42
   Shermeyer J, 2019, IEEE COMPUT SOC CONF, P1432, DOI 10.1109/CVPRW.2019.00184
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang W, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1730, DOI 10.1145/3474085.3475317
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang Z, 2002, INT CONF ACOUST SPEE, P3313
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Xia B, 2022, AAAI CONF ARTIF INTE, P2759
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Xu X., 2022, P IEEE C COMP VIS PA, P5667
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yue ZS, 2022, PROC CVPR IEEE, P2118, DOI 10.1109/CVPR52688.2022.00217
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711
   Zha ZJ, 2022, IEEE T PATTERN ANAL, V44, P710, DOI 10.1109/TPAMI.2019.2909864
   Zhang HZ, 2020, IEEE T MULTIMEDIA, V22, P3210, DOI 10.1109/TMM.2020.2973828
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P1924, DOI 10.1109/TMM.2020.3005025
   Zhang Y., 2019, INT C LEARN REPR
   Zhang YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4258, DOI 10.1109/ICCV48922.2021.00424
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou S., 2020, Advances in Neural Information Processing Systems, V33, P3499
NR 79
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6475
EP 6487
DI 10.1109/TMM.2024.3352400
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600053
DA 2024-08-05
ER

PT J
AU Zhu, G
   Li, JB
   Guo, YH
AF Zhu, Ge
   Li, Jinbao
   Guo, Yahong
TI PriorNet: Two Deep Prior Cues for Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; salient object detection; deep learning; convolutional
   neural network; saliency prior
ID NETWORK
AB Current salient object detection methods achieve good performance by aggregating multi-level features from fully convolutional network. However, in the process of feature aggregation, the noise will be introduced due to the information difference between different level features. Besides, the semantics of high-level features will be diluted as they pass on the top-down pathway, which makes it difficult for model to separate the salient objects from background completely in complex scenes. To address the above problems, we propose two deep priors, including global location prior (GLP) and local contrast prior (LCP). The GLP is generated from prediction, which can enhance the semantics of aggregated features at each level to locate salient objects. Compared with aggregation-based models that directly use high-level features as enhanced semantics, the proposed GLP contains richer semantics and details. The LCP is inferred based on the weighted differences between center pixel and surrounding pixels in backbone features, which can select discriminative features and suppress the noise from aggregated features by multiplication with residual connection. Based on the two priors, we propose a novel twice-decoding network, where the first decoding is to generate GLP by aggregating multi-level features and LCP, and the second decoding is to refine salient objects by using GLP and LCP. Different from previous methods which use a recurrent structure to merge output into input images, the proposed network only applies the output in decoding to avoid interference of raw images. Comprehensive experiments on five datasets show that the proposed method outperforms state-of-the-art ones on five evaluation metrics.
C1 [Zhu, Ge] Heilongjiang Univ, Sch Elect Engn, Harbin 150080, Heilongjiang, Peoples R China.
   [Zhu, Ge] Heilongjiang Univ, Sch Data Sci & Technol, Harbin 150080, Heilongjiang, Peoples R China.
   [Li, Jinbao] Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Sch Math & Stat, Jinan 250353, Shandong, Peoples R China.
   [Guo, Yahong] Qilu Univ Technol, Shandong Acad Sci, Sch Comp Sci & Technol, Jinan 250353, Shandong, Peoples R China.
C3 Heilongjiang University; Heilongjiang University; Qilu University of
   Technology; Qilu University of Technology
RP Guo, YH (corresponding author), Qilu Univ Technol, Shandong Acad Sci, Sch Comp Sci & Technol, Jinan 250353, Shandong, Peoples R China.
EM zhuge@hlju.edu.cn; lijinb@sdas.org; guoyh@qlu.edu.cn
OI Li, Jinbao/0000-0002-2432-8807; ZHU, GE/0000-0003-0483-5956
FU National Natural Science Foundation of China
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Craye C, 2019, ROBOT AUTON SYST, V112, P244, DOI 10.1016/j.robot.2018.11.012
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Dong XP, 2023, IEEE T PATTERN ANAL, V45, P8049, DOI 10.1109/TPAMI.2022.3230064
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Guo Z., 2021, P IEEE CVF INT C COM, P14870
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2019, Arxiv, DOI [arXiv:1803.09860, 10.48550/arXiv.1803.09860, DOI 10.48550/ARXIV.1803.09860]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Krahenbuhl P., 2011, NeurIPS, V24
   Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li J, 2021, IEEE T IMAGE PROCESS, V30, P6855, DOI 10.1109/TIP.2021.3099405
   Li K, 2021, PROC CVPR IEEE, P1944, DOI 10.1109/CVPR46437.2021.00198
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu F, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1477, DOI 10.1109/ICME.2006.262821
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu JJ, 2020, IEEE T IMAGE PROCESS, V29, P8652, DOI 10.1109/TIP.2020.3017352
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu NA, 2020, IEEE T IMAGE PROCESS, V29, P6438, DOI 10.1109/TIP.2020.2988568
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Ma MC, 2021, AAAI CONF ARTIF INTE, V35, P2311
   Mohammadi S, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107303
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Singh Krishna Kumar, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P3544, DOI 10.1109/ICCV.2017.381
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang B, 2020, AAAI CONF ARTIF INTE, V34, P12128
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang W., 2022, IEEE T PATTERN ANAL, P1, DOI [10.1109/ΡΑΙ.2022.3168530, DOI 10.1109/&URHO;&UALPHA;&UIOTA;.2022.3168530]
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Wu Z, 2021, IEEE T IMAGE PROCESS, V30, P6226, DOI 10.1109/TIP.2021.3093380
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xu BW, 2021, AAAI CONF ARTIF INTE, V35, P3004
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu J, 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang M, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P667, DOI 10.1145/3474085.3475231
   Zhang PP, 2020, IEEE T IMAGE PROCESS, V29, P4556, DOI 10.1109/TIP.2019.2957915
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhao ZR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4967, DOI 10.1145/3474085.3475494
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhu G, 2023, Arxiv, DOI arXiv:2301.07431
   Zhu G, 2023, IEEE T NEUR NET LEAR, V34, P6615, DOI 10.1109/TNNLS.2021.3127959
   Zhuge MC, 2023, IEEE T PATTERN ANAL, V45, P3738, DOI 10.1109/TPAMI.2022.3179526
   Zhuge YZ, 2019, AAAI CONF ARTIF INTE, P9340
NR 85
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5523
EP 5535
DI 10.1109/TMM.2023.3335884
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600002
DA 2024-08-05
ER

PT J
AU Alotaibi, T
   Khan, IR
   Bourennani, F
AF Alotaibi, Theyab
   Khan, Ishtiaq Rasool
   Bourennani, Farid
TI Quality Assessment of Tone-Mapped Images Using Fundamental Color and
   Structural Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Full reference metric; high dynamic range (HDR); image quality
   assessment (IQA); TMQI; tone-mapping
ID REPRODUCTION; INDEX
AB High dynamic range (HDR) images require tone-mapping to be viewed on low dynamic range (LDR) displays. The performance of tone-mapping algorithms can be evaluated through a subjective study in which participants based on their liking rank or score tone-mapped images (TMIs). Subjective evaluation can be painstakingly slow; therefore, several quantitative metrics have been proposed for objective evaluation. This article presents a new robust metric that uses 16 features, measuring the loss of color, contrast, brightness, and structure, extracted from the test TMI and the reference HDR image. The effect of these attributes on image quality is investigated and combined into a single score in the [0, 1] range describing the quality of TMI. We validate the performance of the proposed metric by comparing it with 24 existing state-of-the-art metrics. The study uses two subjective datasets of TMIs, including one existing benchmark dataset and a new proposed dataset comprising HDR images of a variety of scenes, and a dataset of traditional images not generated through tone-mapping. In these studies, our method shows the highest correlation with subjective scores for both datasets of TMIs and remains in the second position for the dataset of traditional images.
C1 [Alotaibi, Theyab; Khan, Ishtiaq Rasool; Bourennani, Farid] Univ Jeddah, Coll Comp Sci & Engn, Jeddah 23218, Saudi Arabia.
C3 University of Jeddah
RP Khan, IR (corresponding author), Univ Jeddah, Coll Comp Sci & Engn, Jeddah 23218, Saudi Arabia.
EM 2000057@uj.edu.sa; irkhan@uj.edu.sa; fbourennani@uj.edu.sa
RI Alotaibi, Theyab/KMY-1079-2024; Khan, Ishtiaq R/M-3942-2013
OI Khan, Ishtiaq R/0000-0002-3887-9052
FU Deputyship of Research amp; Innovation, Ministry of Education in Saudi
   Arabia
FX No Statement Available
CR Aydin TO, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360668
   Banterle F, HDR toolbox for processing high dynamic range (HDR) images into MATLAB and Octave
   Cadík M, 2008, COMPUT GRAPH-UK, V32, P330, DOI 10.1016/j.cag.2008.04.003
   Chen PF, 2019, PATTERN RECOGN, V89, P108, DOI 10.1016/j.patcog.2019.01.010
   Cui M. Yu, 2022, IEEE Trans. Instrum. Meas., V71
   Fang YM, 2021, IEEE T MULTIMEDIA, V23, P955, DOI 10.1109/TMM.2020.2991528
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Hadizadeh I. V., 2018, IEEE Trans. Multimedia, V20, P657
   He Q, 2018, IEEE INT CONF MULTI
   He ZY, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22080850
   itu, BT.500: Methodologies for the subjective assessment of the quality of television images
   Jiang MX, 2020, IEEE T CONSUM ELECTR, V66, P153, DOI 10.1109/TCE.2020.2985742
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Khan IR, 2022, IEEE T IMAGE PROCESS, V31, P1751, DOI 10.1109/TIP.2022.3146640
   Khan IR, 2018, IEEE T IND ELECTRON, V65, P3469, DOI 10.1109/TIE.2017.2760247
   Krasula L, 2020, IEEE T MULTIMEDIA, V22, P2038, DOI 10.1109/TMM.2019.2952256
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P4725, DOI 10.1109/TIP.2017.2713945
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Laparra V, 2017, J OPT SOC AM A, V34, P1511, DOI 10.1364/JOSAA.34.001511
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sikudová E, 2016, IEEE COMPUT GRAPH, V36, P78, DOI 10.1109/MCG.2015.116
   Song Y, 2018, SIGNAL PROCESS, V146, P33, DOI 10.1016/j.sigpro.2017.12.020
   Varga D, 2021, J IMAGING, V7, DOI 10.3390/jimaging7020029
   Wanat R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601150
   Wang XJ, 2021, IEEE T MULTIMEDIA, V23, P692, DOI 10.1109/TMM.2020.2986583
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yue GH, 2020, IEEE T IND INFORM, V16, P1764, DOI 10.1109/TII.2019.2927527
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 38
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1244
EP 1254
DI 10.1109/TMM.2023.3278989
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700011
DA 2024-08-05
ER

PT J
AU Chai, XL
   Ma, YK
   Wang, YJ
   Gan, ZH
   Zhang, YS
AF Chai, Xiuli
   Ma, Yakun
   Wang, Yinjing
   Gan, Zhihua
   Zhang, Yushu
TI TPE-ADE: Thumbnail-Preserving Encryption Based on Adaptive Deviation
   Embedding for JPEG Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encryption; Transform coding; Ciphers; Privacy; Usability; Cryptography;
   Visualization; Thumbnail-preserving encryption; JPEG image encryption;
   privacy and usability; knowledge distillation; low-resolution image
   retrieval
AB The growing practice of outsourcing captured photos to the cloud has provided users with convenience while also raising privacy concerns. Traditional image encryption techniques prioritize privacy protection but often compromise usability, which is unacceptable for cloud users. To strike a balance between image privacy and usability, scholars have proposed thumbnail-preserving encryption (TPE), whose cipher image preserves the same thumbnail as the plain image while erasing details beyond the thumbnail, providing visual usability while protecting privacy. Regrettably, most of the proposed TPE schemes are not well-suited for widely used JPEG images, and existing TPE schemes supporting JPEG suffer from drawbacks such as poor visual usability, high expansion rate, and the inability to decrypt without loss. Besides, the retrieval designed for TPE-encrypted images exhibits limited generalization. To address these challenges, we pertinently introduce a TPE based on adaptive deviation embedding (TPE-ADE) for JPEG images, incorporating Huffman coding and reversible data hiding techniques. By leveraging JPEG in-compression encryption, we achieve perfectly reversible TPE that enhances visual usability and reduces expansion rates of TPE-encrypted images. Additionally, we encourage the TPE-encrypted images to resemble low-resolution images (LRIs). Then, the convolutional neural network (CNN) is employed to recognize and retrieve LRIs to verify the functionality of TPE-encrypted images. Also, a teacher-assistant-student (TAS) learning paradigm is proposed to optimize the CNN model, enhancing the performances of recognition and retrieval. Experimental results validate the superiority of our encryption algorithm and the effectiveness of TAS.
C1 [Chai, Xiuli; Ma, Yakun; Wang, Yinjing] Henan Univ, Henan Engn Res Ctr Ind Internet Things, Sch Artificial Intelligence, Zhengzhou 450046, Peoples R China.
   [Chai, Xiuli; Ma, Yakun; Wang, Yinjing] Henan Key Lab Cyberspace Situat Awareness, Zhengzhou 450001, Peoples R China.
   [Gan, Zhihua] Henan Univ, Inst Intelligent Network Syst, Intelligent Data Proc Engn Res Ctr Henan Prov, Sch Software, Kaifeng 475004, Peoples R China.
   [Zhang, Yushu] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
C3 Henan University; Henan University; Nanjing University of Aeronautics &
   Astronautics
RP Gan, ZH (corresponding author), Henan Univ, Inst Intelligent Network Syst, Intelligent Data Proc Engn Res Ctr Henan Prov, Sch Software, Kaifeng 475004, Peoples R China.; Zhang, YS (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM chaixiuli@henu.edu.cn; mayakun1998@163.com; 1220278489@qq.com;
   gzh@henu.edu.cn; yushu@nuaa.edu.cn
OI Chai, Xiuli/0000-0002-1609-0624; gan, zhihua/0000-0002-1138-1887
FU National Natural Science Foundation of China
FX No Statement Available
CR Bellarc M, 2009, LECT NOTES COMPUT SC, V5867, P295, DOI 10.1007/978-3-642-05445-7_19
   Butler DJ, 2015, ACMIEEE INT CONF HUM, P27, DOI 10.1145/2696454.2696484
   Chai XL, 2022, IEEE SIGNAL PROC LET, V29, P972, DOI 10.1109/LSP.2022.3163685
   Chen JX, 2021, IEEE T MULTIMEDIA, V23, P2372, DOI 10.1109/TMM.2020.3011315
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Han C., 2018, ACM Trans. Graph., V37, P1
   Hannabuss S, 2010, LIBR REV, V59, P562, DOI 10.1108/00242531011065163
   He JH, 2018, IEEE T MULTIMEDIA, V20, P2645, DOI 10.1109/TMM.2018.2817065
   He Y. Yuan, 2023, J. Vis.Commun. Image Represent., V90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang FJ, 2016, IEEE T CIRC SYST VID, V26, P1610, DOI 10.1109/TCSVT.2015.2473235
   Janani T, 2022, IEEE T MULTIMEDIA, V24, P3794, DOI 10.1109/TMM.2021.3107681
   Kang XJ, 2019, IEEE T CIRC SYST VID, V29, P1919, DOI 10.1109/TCSVT.2018.2859253
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Ma YK, 2024, IEEE INTERNET THINGS, V11, P4842, DOI 10.1109/JIOT.2023.3301042
   Marohn B, 2017, PROCEEDINGS OF THE 2017 WORKSHOP ON MULTIMEDIA PRIVACY AND SECURITY (MPS'17), P33, DOI 10.1145/3137616.3137621
   Maximov M, 2020, PROC CVPR IEEE, P5446, DOI 10.1109/CVPR42600.2020.00549
   Minemura K, 2012, IEEE IMAGE PROC, P261, DOI 10.1109/ICIP.2012.6466845
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Qin C, 2023, IEEE T MULTIMEDIA, V25, P2528, DOI 10.1109/TMM.2022.3148591
   Ren ZZ, 2018, LECT NOTES COMPUT SC, V11205, P639, DOI 10.1007/978-3-030-01246-5_38
   Ryoo MS, 2017, AAAI CONF ARTIF INTE, P4255
   Shimizu K, 2021, PICT COD SYMP, P166, DOI 10.1109/PCS50896.2021.9477508
   Srivastav V, 2019, LECT NOTES COMPUT SC, V11768, P583, DOI 10.1007/978-3-030-32254-0_65
   Sun JL, 2017, INT J BIFURCAT CHAOS, V27, DOI 10.1142/S0218127417500730
   Tajik K, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23432
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang YJ, 2023, APPL INTELL, V53, P4027, DOI 10.1007/s10489-022-03597-y
   Wright C.V., 2015, PROC 3 ACM WORKSHO, P141
   Xia ZH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3492705
   Xia ZH, 2022, IEEE T SERV COMPUT, V15, P202, DOI 10.1109/TSC.2019.2927215
   Xue HW, 2018, OPT LASER TECHNOL, V106, P506, DOI 10.1016/j.optlastec.2018.04.030
   Yang CH, 2022, J INF SECUR APPL, V70, DOI 10.1016/j.jisa.2022.103352
   Ye X, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103589
   Ye ZJ, 2022, INFORM SCIENCES, V617, P395, DOI 10.1016/j.ins.2022.10.114
   You S., 2021, IEEE INT C MULTIMEDI, P1
   Youssef A, 1999, INTERNATIONAL CONFERENCE ON IMAGING SCIENCE, SYSTEMS, AND TECHNOLOGY, PROCEEDINGS, P132
   Yu PP, 2023, IEEE T CLOUD COMPUT, V11, P2885, DOI 10.1109/TCC.2022.3233421
   Zhang JM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1423, DOI 10.1145/3394171.3413906
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YS, 2023, IEEE T INF FOREN SEC, V18, P3334, DOI 10.1109/TIFS.2023.3280341
   Zhang YS, 2023, IEEE T MULTIMEDIA, V25, P5877, DOI 10.1109/TMM.2022.3200310
   Zhang YS, 2022, IEEE T CIRC SYST VID, V32, P947, DOI 10.1109/TCSVT.2021.3070348
   Zhao RY, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108019
   Zhu MJ, 2019, INT CONF ACOUST SPEE, P3762, DOI [10.1109/icassp.2019.8682926, 10.1109/ICASSP.2019.8682926]
   Zhu ZW, 2022, J KING SAUD UNIV-COM, V34, P10167, DOI 10.1016/j.jksuci.2022.10.014
NR 49
TC 2
Z9 2
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6102
EP 6116
DI 10.1109/TMM.2023.3345158
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600030
DA 2024-08-05
ER

PT J
AU Cui, Q
   Zhou, ZL
   Meng, RH
   Wang, SW
   Yan, HY
   Wu, QMJ
AF Cui, Qi
   Zhou, Zhili
   Meng, Ruohan
   Wang, Shaowei
   Yan, Hongyang
   Wu, Q. M. Jonathan
TI ARES: On Adversarial Robustness Enhancement for Image Steganographic
   Cost Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Steganography; GAN; Adversarial training; Cost learning
ID FRAMEWORK; STEGANALYSIS
AB Taking the steganalytic discriminators as the adversaries, the existing Generative Adversarial Networks (GAN)-based steganographic approaches learn the implicit cost functions to measure the embedding distortion for steganography. However, the steganalytic discriminators in these approaches are trained by the stego-samples with insufficient diversity, and their network structures offer very limited representational capacity. As a result, these steganalytic discriminators will not exhibit robustness to various steganographic patterns, which causes learning suboptimal cost functions, thus compromising the anti-steganalysis capability. To address this issue, we propose a novel GAN-based steganographic approach, in which the Diversified Inverse-Adversarial Training (DIAT) strategy and the Steganalytic Feature Attention (SteFA) structure are designed to train a robust steganalytic discriminator. Specifically, the DIAT strategy provides the steganalytic discriminator with an expanded feature space by generating diversified adversarial stego-samples; the SteFA structure enables the steganalytic discriminator to capture more various steganalytic features by employing the channel-attention mechanism on higher-order statistics. Consequently, the steganalytic discriminator can build a more precise decision boundary to make it more robust, which facilitates learning a superior steganographic cost function. Extensive experiments demonstrate that the proposed steganographic approach achieves promising anti-steganalysis capability over the state-of-the-arts under the same embedding payloads.
C1 [Cui, Qi; Meng, Ruohan] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Sch Comp Sci, Minist Educ, Nanjing 210044, Peoples R China.
   [Cui, Qi; Meng, Ruohan] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Zhou, Zhili; Wang, Shaowei; Yan, Hongyang] Guangzhou Univ, Inst Artificial Intelligence, Guangzhou 510006, Peoples R China.
   [Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
C3 Nanjing University of Information Science & Technology; Nanyang
   Technological University; Guangzhou University; University of Windsor
RP Meng, RH (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Sch Comp Sci, Minist Educ, Nanjing 210044, Peoples R China.; Zhou, ZL (corresponding author), Guangzhou Univ, Inst Artificial Intelligence, Guangzhou 510006, Peoples R China.
EM qi.cui@ntu.edu.sg; zhou_zhili@163.com; ruohan.meng@ntu.edu.sg;
   wangsw@gzhu.edu.cn; hyang_yan@gzhu.edu.cn; jwu@uwindsor.ca
OI zhou, zhili/0000-0002-5641-7169; Cui, Qi/0000-0002-0282-9758
FU National Natural Science Foundation of China
FX No Statement Available
CR Agarwal Akshay, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P3354, DOI 10.1109/CVPRW50498.2020.00395
   Athalye A, 2018, PR MACH LEARN RES, V80
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bernard S, 2021, IEEE T INF FOREN SEC, V16, P812, DOI 10.1109/TIFS.2020.3021913
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Cheng H., 2021, arXiv
   Cogranne R, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P125
   Cui Q, 2019, IEEE ACCESS, V7, P90815, DOI 10.1109/ACCESS.2019.2913895
   Denemark T, 2014, IEEE INT WORKS INFOR, P48, DOI 10.1109/WIFS.2014.7084302
   Deng XQ, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P230, DOI 10.1145/3335203.3335739
   Dong Yihe, 2020, Advances in Neural Information Processing Systems, V33
   Fawzi A., 2016, P ADV NEUR INF PROC, V29, P131
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Franceschi J. Y., 2018, INT C ART INT STAT, P1280
   Fridrich J, 2007, PROC SPIE, V6505, DOI 10.1117/12.697471
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Li B, 2014, IEEE IMAGE PROC, P4206, DOI 10.1109/ICIP.2014.7025854
   Li B, 2015, IEEE T INF FOREN SEC, V10, P1905, DOI 10.1109/TIFS.2015.2434600
   Li FF, 2022, NEURAL COMPUT APPL, V34, P11507, DOI 10.1007/s00521-020-04882-y
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105
   Li WX, 2020, IEEE T COMMUN, V68, P3948, DOI 10.1109/TCOMM.2020.2982624
   Ma S, 2019, MULTIMED TOOLS APPL, V78, P32503, DOI 10.1007/s11042-019-07994-3
   Madry A, 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.06083
   Meng RH, 2022, IEEE T NETW SCI ENG, V9, P848, DOI 10.1109/TNSE.2021.3137829
   Meng RH, 2019, IEEE ACCESS, V7, P90574, DOI 10.1109/ACCESS.2019.2920956
   Mo XB, 2021, IEEE T INF FOREN SEC, V16, P4306, DOI 10.1109/TIFS.2021.3104140
   Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13
   Qin XH, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2705, DOI 10.1109/ICASSP39728.2021.9414055
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sedighi V, 2016, IEEE T INF FOREN SEC, V11, P221, DOI 10.1109/TIFS.2015.2486744
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tang WX, 2021, IEEE T INF FOREN SEC, V16, P952, DOI 10.1109/TIFS.2020.3025438
   Tang WX, 2019, IEEE T INF FOREN SEC, V14, P2074, DOI 10.1109/TIFS.2019.2891237
   Tang WX, 2017, IEEE SIGNAL PROC LET, V24, P1547, DOI 10.1109/LSP.2017.2745572
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Volkhonskiy D, 2020, PROC SPIE, V11433, DOI 10.1117/12.2559429
   [王耀杰 Wang Yaojie], 2018, [计算机应用, Journal of Computer Applications], V38, P2923
   Wu H., 2019, Lecture notes in computer science, V12022, P3
   Wu ST, 2020, IEEE T MULTIMEDIA, V22, P256, DOI 10.1109/TMM.2019.2920605
   Xiao Chang, 2020, 8 INT C LEARN REPR I
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Yang JH, 2020, IEEE T INF FOREN SEC, V15, P839, DOI 10.1109/TIFS.2019.2922229
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Yedroudj Mehdi, 2020, IH&MMSec '20: Proceedings of the 2020 ACM Workshop on Information Hiding and Multimedia Security, P39, DOI 10.1145/3369412.3395061
   Yedroudj M, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2092, DOI 10.1109/ICASSP.2018.8461438
   Yous Y, 2021, PROCEEDINGS OF THE 2021 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, IH&MMSEC 2021, P149, DOI 10.1145/3437880.3460397
   Yousfi Y., 2021, P IEEE INT WORKSH IN, P1
   Zhang CN, 2021, AAAI CONF ARTIF INTE, V35, P3296
   Zhang JS, 2022, IEEE T MULTIMEDIA, V24, P4538, DOI 10.1109/TMM.2021.3119994
   Zhang R, 2020, IEEE T INF FOREN SEC, V15, P1138, DOI 10.1109/TIFS.2019.2936913
   Zhang YW, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P67, DOI 10.1145/3206004.3206012
   Zhao JF, 2022, NEURAL COMPUT APPL, V34, P16073, DOI 10.1007/s00521-022-07270-w
   Zheng TH, 2019, AAAI CONF ARTIF INTE, P2253
   Zhou Y, 2021, NEUROCOMPUTING, V459, P131, DOI 10.1016/j.neucom.2021.06.078
NR 61
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6542
EP 6553
DI 10.1109/TMM.2024.3353543
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600059
DA 2024-08-05
ER

PT J
AU Fang, NY
   Qiu, LM
   Zhang, SY
   Wang, ZL
   Hu, KR
AF Fang, Naiyu
   Qiu, Lemiao
   Zhang, Shuyou
   Wang, Zili
   Hu, Kerui
TI PG-VTON: A Novel Image-Based Virtual Try-On Method via Progressive
   Inference Paradigm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Virtual try-on; PG-VTON; Garment warping; Skin inpainting; Vision
   Transformer
AB Virtual try-on is a promising computer vision topic with a high commercial value wherein a new garment is visually worn on a person with a photo-realistic effect. Previous studies conduct their shape and content inference at one stage, employing a single-scale warping mechanism and a relatively unsophisticated content inference mechanism. These approaches have led to suboptimal results in terms of garment warping and skin reservation under challenging try-on scenarios. To address these limitations, we propose a novel virtual try-on method via progressive inference paradigm (PGVTON) that leverages a top-down inference pipeline and a general garment try-on strategy. Specifically, we propose a robust try-on parsing inference method by disentangling semantic categories and introducing consistency. Exploiting the try-on parsing as the shape guidance, we implement the garment try-on via warping-mapping-composition. To facilitate adaptation to a wide range of try-on scenarios, we adopt a covering more and selecting one warping strategy and explicitly distinguish tasks based on alignment. Additionally, we regulate StyleGAN2 to implement re-naked skin inpainting, conditioned on the target skin shape and spatial-agnostic skin features. Experiments demonstrate that our method has state-of-the-art performance under two challenging scenarios.
C1 [Fang, Naiyu; Qiu, Lemiao; Zhang, Shuyou; Wang, Zili; Hu, Kerui] Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Qiu, LM (corresponding author), Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
EM fangnaiyu@zju.edu.cn; qiulm@zju.edu.cn; zsy@zju.edu.cn;
   ziliwang@zju.edu.cn; hkr457@zju.edu.cn
RI Wang, Zili/KBP-9936-2024; Fang, Naiyu/KMA-5576-2024; Qiu,
   Lemiao/JWO-6135-2024
OI Hu, Kerui/0000-0002-8055-6468; Fang, Naiyu/0000-0003-0145-1690; Qiu,
   Lemiao/0000-0001-9358-0099
FU National Natural Science Foundation of China
FX No Statement Available
CR Alom MZ, 2019, J MED IMAGING, V6, DOI 10.1117/1.JMI.6.1.014006
   Bai S, 2022, LECT NOTES COMPUT SC, V13675, P409, DOI 10.1007/978-3-031-19784-0_24
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao H., 2021, arXiv
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13789, DOI 10.1109/ICCV48922.2021.01355
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Cui Aiyu, 2021, P IEEECVF INT C COMP, P14638
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Dong HY, 2019, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2019.00125
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Fang NY, 2023, IEEE T MULTIMEDIA, V25, P6512, DOI 10.1109/TMM.2022.3209924
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han XY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2420, DOI 10.1145/3503161.3547999
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   He HY, 2020, AAAI CONF ARTIF INTE, V34, P10949
   He S, 2022, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR52688.2022.00346
   Hu BW, 2022, IEEE T MULTIMEDIA, V24, P1233, DOI 10.1109/TMM.2022.3143712
   Hu PP, 2022, IEEE T MULTIMEDIA, V24, P2139, DOI 10.1109/TMM.2021.3076340
   Issenhuth Thibaut, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P619, DOI 10.1007/978-3-030-58565-5_37
   Jaderberg M, 2015, ADV NEUR IN, V28
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Lewis KM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459884
   Liu GQ, 2021, AAAI CONF ARTIF INTE, V35, P2118
   Liu T, 2021, IEEE T IMAGE PROCESS, V30, P7499, DOI 10.1109/TIP.2021.3107235
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma LY, 2023, IEEE T MULTIMEDIA, V25, P930, DOI 10.1109/TMM.2021.3134157
   Minar M.R., 2020, CVPR WORKSH, V3, P10
   Neuberger A, 2020, PROC CVPR IEEE, P5183, DOI 10.1109/CVPR42600.2020.00523
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Peebles W, 2022, PROC CVPR IEEE, P13460, DOI 10.1109/CVPR52688.2022.01311
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Raj A, 2018, LECT NOTES COMPUT SC, V11216, P679, DOI 10.1007/978-3-030-01258-8_41
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Sekhavat YA, 2017, IEEE T MULTIMEDIA, V19, P1041, DOI 10.1109/TMM.2016.2639380
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Xie Z., 2021, NeurIPS, V34, P2598
   Yang Jianwei, 2021, Advances in Neural Information Processing Systems, V34
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
NR 54
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6595
EP 6608
DI 10.1109/TMM.2024.3354622
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600005
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Frants, V
   Agaian, S
   Panetta, K
AF Frants, Vladimir
   Agaian, Sos
   Panetta, Karen
TI QSAM-Net: Rain Streak Removal by Quaternion Neural Network With
   Self-Attention Module
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; object detection; quaternion image processing; quaternion
   neural networks; rain removal
ID IMAGE QUALITY ASSESSMENT
AB Real-world images captured in remote sensing, image or video retrieval, and outdoor surveillance are often degraded due to poor weather conditions, such as rain and mist. These conditions introduce artifacts that make visual analysis challenging and limit the performance of high-level computer vision methods. In time-critical applications, it is vital to develop algorithms that automatically remove rain without compromising the quality of the image contents. This article proposes a novel approach called QSAM-Net, a quaternion multi-stage multiscale neural network with a self-attention module. The algorithm requires significantly fewer parameters by a factor of 3.98 than the real-valued counterpart and state-of-the-art methods while improving the visual quality of the images. The extensive evaluation and benchmarking on synthetic and real-world rainy images demonstrate the effectiveness of QSAM-Net. This feature makes the network suitable for edge devices and applications requiring near real-time performance. Furthermore, the experiments show that the improved visual quality of images also leads to better object detection accuracy and training speed.
C1 [Frants, Vladimir] CUNY, Grad Ctr, New York, NY 10016 USA.
   [Agaian, Sos] CUNY, Coll Staten Isl, New York, NY 10314 USA.
   [Panetta, Karen] Tufts Univ, Elect & Comp Engn Dept, Medford, MA 02155 USA.
C3 City University of New York (CUNY) System; City University of New York
   (CUNY) System; College of Staten Island (CUNY); Tufts University
RP Frants, V (corresponding author), CUNY, Grad Ctr, New York, NY 10016 USA.
EM vfrants@gradcenter.cuny.edu; sos.agaian@csi.cuny.edu;
   karen@ece.tufts.edu
RI Agaian, Sos s/IZE-1724-2023
OI Agaian, Sos s/0000-0003-4601-4507
FU U.S. Department of Transportation
FX No Statement Available
CR Badrieva S., 2022, Proc. SPIE, V12100, P135
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chen WT, 2022, PROC CVPR IEEE, P17632, DOI 10.1109/CVPR52688.2022.01713
   Chen YY, 2020, IEEE T IMAGE PROCESS, V29, P1426, DOI 10.1109/TIP.2019.2941319
   Cui HS, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.837208
   Du SL, 2022, IET IMAGE PROCESS, V16, P11, DOI 10.1049/ipr2.12347
   Duan HY, 2023, IEEE T MULTIMEDIA, V25, P4267, DOI 10.1109/TMM.2022.3172882
   Frants V, 2023, IEEE T CYBERNETICS, V53, P5448, DOI 10.1109/TCYB.2023.3238640
   Fu X., 2021, PROC AAAI C ARTIF IN, P1
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Greenblatt AB, 2018, INFORM SCIENCES, V423, P326, DOI 10.1016/j.ins.2017.09.057
   Grigoryan A.M., 2014, Appl. Math. Sci.: An Int. J. (MathSJ), V1, P23
   Grigoryan A.M., 2018, Quaternion and Octonion Color Image Processing with Matlab, DOI 10.1117/3.2278810.ch1
   Grigoryan AM, 2020, PROC SPIE, V11399, DOI 10.1117/12.2558209
   Hovhannisyan SA, 2022, IEEE ACCESS, V10, P12465, DOI 10.1109/ACCESS.2022.3144402
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Islam MR, 2022, IEEE ACCESS, V10, P202, DOI 10.1109/ACCESS.2021.3136551
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kingma D. P., 2014, arXiv
   Kolaman A, 2012, IEEE T IMAGE PROCESS, V21, P1526, DOI 10.1109/TIP.2011.2181522
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li SY, 2019, PROC CVPR IEEE, P3833, DOI 10.1109/CVPR.2019.00396
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Luo LC, 2010, P 2010 INT C MULT TE
   Mehri A, 2021, IEEE WINT CONF APPL, P2703, DOI 10.1109/WACV48630.2021.00275
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Parcollet T, 2020, ARTIF INTELL REV, V53, P2957, DOI 10.1007/s10462-019-09752-1
   Parcollet T, 2019, INT CONF ACOUST SPEE, P8514, DOI [10.1109/ICASSP.2019.8682495, 10.1109/icassp.2019.8682495]
   Paszke A, 2019, ADV NEUR IN, V32
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Que Y, 2021, IEEE T MULTIMEDIA, V23, P3059, DOI 10.1109/TMM.2020.3019680
   Schavemaker JGM, 2000, PATTERN RECOGN, V33, P997, DOI 10.1016/S0031-3203(99)00160-0
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Valanarasu JMJ, 2022, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR52688.2022.00239
   Vu T, 2021, AAAI CONF ARTIF INTE, V35, P2701
   Wang H, 2021, PROC CVPR IEEE, P14786, DOI 10.1109/CVPR46437.2021.01455
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wei C., 2018, PROC BRIT MACH VIS C, P1
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Yang WH, 2021, IEEE T PATTERN ANAL, V43, P4059, DOI 10.1109/TPAMI.2020.2995190
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2545, DOI 10.1109/TMM.2019.2908375
   Yang Y, 2022, IEEE T MULTIMEDIA, V24, P1622, DOI 10.1109/TMM.2021.3068833
   Yin QL, 2019, IEEE ACCESS, V7, P20293, DOI 10.1109/ACCESS.2019.2897000
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
NR 57
TC 3
Z9 3
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 789
EP 798
DI 10.1109/TMM.2023.3271829
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000032
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Huan, RH
   Zhong, GW
   Chen, P
   Liang, RH
AF Huan, Ruohong
   Zhong, Guowei
   Chen, Peng
   Liang, Ronghua
TI UniMF: A Unified Multimodal Framework for Multimodal Sentiment Analysis
   in Missing Modalities and Unaligned Multimodal Sequences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; missing modalities; multimodal sentiment analysis;
   transformer; unaligned multimodal sequences
ID NETWORK; FUSION
AB In current multimodal sentiment analysis, aligned and complete multimodal sequences are often crucial. Obtaining complete multimodal data in the real world presents various challenges, and aligning multimodal sequences often requires a significant amount of effort. Unfortunately, most multimodal sentiment analysis methods fail when dealing with missing modalities or unaligned multimodal sequences. To tackle these two challenges simultaneously in a simple and lightweight manner, we present the Unified Multimodal Framework (UniMF). The primary components of UniMF comprise two distinct modules. The first module, Translation Module, translates missing modalities using information from existing modalities. The second module, Prediction Module, uses the attention mechanism to fuse the multimodal information and generate predictions. To enhance the translation performance of the Translation Module, we introduce the Multimodal Generation Mask (MGM) and utilize it to construct the Multimodal Generation Transformer (MGT). The MGT can generate the missing modality while focusing on information from existing modalities. Furthermore, we introduce the Multimodal Understanding Transformer (MUT) in the Prediction Module, which includes the Multimodal Understanding Mask (MUM) and a unique sequence, MultiModalSequence (MMSeq), representing a unified multimodality. To assess the performance of UniMF, we perform experiments on four multimodal sentiment datasets, and UniMF attains competitive or state-of-the-art outcomes with fewer learnable parameters. Furthermore, the experimental outcomes signify that UniMF, supported by MGT and MUT - two transformers utilizing special attention mechanisms, can efficiently manage both generating task of missing modalities and understanding task of unaligned multimodal sequences.
C1 [Huan, Ruohong; Zhong, Guowei; Chen, Peng; Liang, Ronghua] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China.
C3 Zhejiang University of Technology
RP Liang, RH (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China.
EM huanrh@zjut.edu.cn; guoweizhong@zjut.edu.cn; chenpeng@zjut.edu.cn;
   rhliang@zjut.edu.cn
OI Zhong, Guowei/0000-0002-1391-0957
FU National Natural Science Foundation of China
FX No Statement Available
CR Akiba T, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2623, DOI 10.1145/3292500.3330701
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Chen T, 2017, EXPERT SYST APPL, V72, P221, DOI 10.1016/j.eswa.2016.10.065
   Cheng JY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P2447
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Eyben F, 2010, P 18 INT C MULT 2010, DOI DOI 10.1145/1873951.1874246
   Ghosal D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3454
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo JW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3394, DOI 10.1145/3503161.3548137
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Hasan MK, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2046
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Kingma D., 2015, P INT C LEARN REPR S, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Lian Z, 2023, IEEE T PATTERN ANAL, V45, P8419, DOI 10.1109/TPAMI.2023.3234553
   Liang PP, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P150
   Liang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8128, DOI 10.1109/ICCV48922.2021.00804
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Lv F, 2021, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR46437.2021.00258
   Mai Sijie, 2023, IEEE Transactions on Multimedia, P4121, DOI 10.1109/TMM.2022.3171679
   Mai SJ, 2023, IEEE T AFFECT COMPUT, V14, P2276, DOI 10.1109/TAFFC.2022.3172360
   Mai SJ, 2022, IEEE T MULTIMEDIA, V24, P2488, DOI 10.1109/TMM.2021.3082398
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Majumder N, 2018, KNOWL-BASED SYST, V161, P124, DOI 10.1016/j.knosys.2018.07.041
   Majumder N, 2019, AAAI CONF ARTIF INTE, P6818
   Peng W, 2021, IEEE INTELL SYST, V36, P82, DOI 10.1109/MIS.2021.3057757
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Pham H., 2018, arXiv
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P527
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Sun H, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3722, DOI 10.1145/3503161.3548025
   Sun LC, 2024, IEEE T AFFECT COMPUT, V15, P309, DOI 10.1109/TAFFC.2023.3274829
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Sutskever I., 2014, Advances in Neural Information Processing Systems, P3104
   Tang J., 2021, LONG PAPERS, V1, P5301, DOI DOI 10.18653/V1/2021.ACL-LONG.412
   Tsai P. P., 2019, INT CONFLEARN REPRES, P1
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wang ZL, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2514, DOI 10.1145/3366423.3380000
   Yang DK, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1642, DOI 10.1145/3503161.3547754
   Yang DK, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1708, DOI 10.1145/3503161.3547755
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Yang Zhilin, 2019, NeurIPS, V32, DOI DOI 10.5555/3454287.3454804
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yuan Y., 2023, IEEE Trans. Multime-dia, DOI [10.1109/TMM.2023.3267882.[52]Z., DOI 10.1109/TMM.2023.3267882.[52]Z]
   Yuan ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4400, DOI 10.1145/3474085.3475585
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zeng J., 2022, P C EMP METH NAT LAN, P2924
   Zeng JD, 2023, IEEE T MULTIMEDIA, V25, P6301, DOI 10.1109/TMM.2022.3207572
   Zeng JDA, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P1545, DOI 10.1145/3477495.3532064
   Zhao JM, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2608
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 64
TC 1
Z9 1
U1 51
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5753
EP 5768
DI 10.1109/TMM.2023.3338769
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100011
DA 2024-08-05
ER

PT J
AU Jiang, HZ
   Zhang, X
   Xiang, SM
AF Jiang, Hangzhi
   Zhang, Xin
   Xiang, Shiming
TI Non-Maximum Suppression Guided Label Assignment for Object Detection in
   Crowd Scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Crowd scenes; Label assignment; Non-maximum
   suppression
ID PEDESTRIAN DETECTION; PROPOSAL
AB The detection performance in crowd scenes is limited by recalling hard objects (e.g., occluded objects). It requires that this kind of objects can be successfully detected and retained by the non-maximum suppression (NMS) while controlling false positives. The existing dynamic label assignment algorithms can help recall these objects by adaptively allocating appropriate positive samples, however, they ignore the alignment with the selecting rules of NMS. This leads to the fact that detecting objects in crowd scenes are still very sensitive to the NMS threshold setting. As a result, the existing methods can only set a low NMS threshold to avoid the excessive false positives, causing some objects failed to be recalled. And these methods also generally lack more excitation for positive samples, which hinders further facilitating the recall of hard instances in crowd scenes. This article proposes a novel dynamic label assignment strategy for object detection in crowd scenes, called non-maximum suppression guided label assignment (NGLA), which aligns the assignment strategy with NMS process and learns more prominent positive samples. Following NMS, NGLA introduces the IoU between samples with their corresponding best samples to define positive and negative samples. To cooperate with NGLA, an NMS-aware loss is proposed to dynamically assign sample weights when supervising sample predictions, which also considers the IoU with the best sample. In addition, for better classification prediction, a regression assisted classification branch is designed to help detectors perceive the relation between the regression predictions of each sample and the corresponding best sample. Experiments demonstrate that NGLA outperforms other label assignment methods on CrowdHuman and Citypersons, and is less sensitive to the NMS threshold in crowd scenes.
C1 [Jiang, Hangzhi; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Jiang, Hangzhi; Xiang, Shiming] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Zhang, Xin] Beijing Inst Technol, Sch Informat & Elect, Radar Res Lab, Beijing 100081, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Beijing
   Institute of Technology
RP Xiang, SM (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM jianghangzhi2018@ia.ac.cn; xin.zhang@bit.edu.cn; smxiang@nlpr.ia.ac.cn
RI Zhang, Xin/AHC-3321-2022
OI Zhang, Xin/0000-0003-2901-2593
FU National Key Research and Development Program of China
FX No Statement Available
CR Cao XP, 2022, AAAI CONF ARTIF INTE, P185
   Carion N., 2020, EUR C COMP VIS, P213
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chi C, 2020, AAAI CONF ARTIF INTE, V34, P10639
   Dai JF, 2016, ADV NEUR IN, V29
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Ge SM, 2017, PROC CVPR IEEE, P426, DOI 10.1109/CVPR.2017.53
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Ge Z, 2021, NEUROCOMPUTING, V462, P272, DOI 10.1016/j.neucom.2021.07.094
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X., 2020, CVPR, P10747
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li F, 2022, PROC CVPR IEEE, P13609, DOI 10.1109/CVPR52688.2022.01325
   Li SK, 2022, PROC CVPR IEEE, P316, DOI 10.1109/CVPR52688.2022.00041
   Li Shikun, 2022, Advances in Neural Information Processing Systems, P24184
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   [刘尚鹏 Liu Shangpeng], 2022, [高分子通报, Polymer Bulletin], P1
   Liu ST, 2019, PROC CVPR IEEE, P6452, DOI 10.1109/CVPR.2019.00662
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu RQ, 2020, NEUROCOMPUTING, V400, P343, DOI 10.1016/j.neucom.2020.03.037
   Ma YC, 2021, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR46437.2021.00176
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao S, 2018, Arxiv, DOI arXiv:1805.00123
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Sun Peize, 2021, PROC INT C MACH LEAR, P9934
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang JF, 2021, PROC CVPR IEEE, P15844, DOI 10.1109/CVPR46437.2021.01559
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811
   Wu S., 2022, P AS C COMP VIS, P2901
   Xiang L, 2022, Arxiv, DOI arXiv:2211.00826
   Xu Z., 2020, ADV NEURAL INFORM PR, V33, p19 953
   Xuangeng Chu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12211, DOI 10.1109/CVPR42600.2020.01223
   Yang CHY, 2022, LECT NOTES COMPUT SC, V13669, P123, DOI 10.1007/978-3-031-20077-9_8
   Yang T., 2018, Advances in Neural Information Processing Systems
   Zeng D, 2019, INFORM SCIENCES, V495, P136, DOI 10.1016/j.ins.2019.01.083
   Zhang CH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P565, DOI [10.1145/3394171.3413959, 10.1145/3394171.3413159]
   Zhang JL, 2021, IEEE T MULTIMEDIA, V23, P3085, DOI 10.1109/TMM.2020.3020691
   Zhang K., 2019, arXiv
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731
   Zhang SF, 2020, IEEE T MULTIMEDIA, V22, P380, DOI 10.1109/TMM.2019.2929005
   Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhen PN, 2022, AAAI CONF ARTIF INTE, P4716
   Zhou CL, 2019, IEEE I CONF COMP VIS, P9556, DOI 10.1109/ICCV.2019.00965
   Zhou HY, 2022, Arxiv, DOI arXiv:2212.07652
   Zhou X., 2019, ARXIV
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu BJ, 2020, Arxiv, DOI arXiv:2007.03496
NR 62
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2207
EP 2218
DI 10.1109/TMM.2023.3293333
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100023
DA 2024-08-05
ER

PT J
AU Li, TB
   Su, YT
   Song, D
   Li, WH
   Wei, ZQ
   Liu, AA
AF Li, Tian-Bao
   Su, Yu-Ting
   Song, Dan
   Li, Wen-Hui
   Wei, Zhi-Qiang
   Liu, An-An
TI Progressive Fourier Adversarial Domain Adaptation for Object
   Classification and Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain adaptation; metric learning; domain adaptive image
   classification; cross-domain 3D model retrieval
AB Domain adaptation has been extensively explored as a means of transferring knowledge from the labeled source domain to the unlabeled target domain with disparate data distributions. However, the absence of target annotations and significant domain discrepancies pose a great challenge to transfer knowledge directly from source domain to target domain. To address this challenge, we propose a Progressive Fourier Adversarial Domain Adaptation (PFADA) framework, an effective and versatile framework which can generalize across multiple domain adaptation tasks. Firstly, we propose a Fourier-based style transfer strategy to generate a Fourier intermediate domain that incorporates source images with target domain-specific styles, while preserving the domain-invariant representations of the source data. Secondly, we introduce a progressive adversarial domain adaptation approach that utilizes the Fourier intermediate domain to facilitate the learning of domain-invariant representations. Finally, we present cross-domain semantic alignment and discriminative enhancement approach, which effectively guides the learning of discriminative cross-domain representations utilizing labeled source and intermediate domain data. Extensive experimental evaluations consistently validate the superior performance of the proposed method across diverse visual tasks, encompassing multiple domain adaptive image classification and retrieval scenarios.
C1 [Li, Tian-Bao; Su, Yu-Ting; Song, Dan; Li, Wen-Hui; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Wei, Zhi-Qiang] Ocean Univ China, Sch Informat Sci & Engn, Qingdao 266100, Peoples R China.
   [Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230026, Peoples R China.
C3 Tianjin University; Ocean University of China
RP Song, D; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Liu, AA (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230026, Peoples R China.
EM litianbao@tju.edu.cn; ytsu@tju.edu.cn; dan.song@tju.edu.cn;
   liwenhui@tju.edu.cn; weizhiqiang@ouc.edu.cn; anan0422@gmail.com
RI wei, zhiqiang/M-8868-2013
OI Li, Tianbao/0000-0001-6543-5660; Jing, Peiguang/0000-0003-2648-7358
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai C, 2021, IEEE T MULTIMEDIA, V23, P2199, DOI 10.1109/TMM.2021.3065578
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen L, 2022, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR52688.2022.00704
   Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189
   Chen ZL, 2019, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2019.00235
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Frigo M, 1998, INT CONF ACOUST SPEE, P1381, DOI 10.1109/ICASSP.1998.681704
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hu N, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2021.103426
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Li RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9971, DOI 10.1109/ICCV48922.2021.00984
   Li S, 2021, PROC CVPR IEEE, P11511, DOI 10.1109/CVPR46437.2021.01135
   Li S, 2021, AAAI CONF ARTIF INTE, V35, P8455
   Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710
   Li ZQ, 2019, AAAI CONF ARTIF INTE, P8682
   Liang Q, 2023, IEEE T MULTIMEDIA, V25, P3443, DOI 10.1109/TMM.2022.3160616
   Liu AA, 2022, IEEE T CYBERNETICS, V52, P13862, DOI 10.1109/TCYB.2021.3139927
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Myeongjin Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12972, DOI 10.1109/CVPR42600.2020.01299
   Nguyen-Meidine LT, 2021, IEEE WINT CONF APPL, P1338, DOI 10.1109/WACV48630.2021.00138
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Park G. Y., 2021, P IEEE CVF INT C COM, P9214
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Roy S, 2021, PROC CVPR IEEE, P5347, DOI 10.1109/CVPR46437.2021.00531
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Savva M, 2017, P WORKSH 3D OBJ RETR, P39, DOI [DOI 10.2312/3DOR.20171050, 10.2312/3dor.20171050]
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Song D, 2021, IEEE T MULTIMEDIA, V23, P2721, DOI 10.1109/TMM.2020.3015554
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su YT, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P883
   Su YT, 2021, IEEE T MULTIMEDIA, V23, P2127, DOI 10.1109/TMM.2020.3008056
   Su YT, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3496, DOI 10.1145/3394171.3413655
   Su YT, 2020, IEEE T CIRC SYST VID, V30, P3765, DOI 10.1109/TCSVT.2019.2942688
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, DOI 10.48550/ARXIV.1412.3474]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie S., 2018, PMLR, P5423
   Xing EP, 2003, NIPS 15, V15, P505
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Zhai AD, 2019, Arxiv, DOI arXiv:1811.12649
   Zhang H, 2016, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR.2016.125
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang Y., 2019, P 36 INT C MACH LEAR, P7404
   Zhao SC, 2020, AAAI CONF ARTIF INTE, V34, P12975
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Zhou HY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P925, DOI 10.1145/3394171.3413631
   Zhou HY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P839
   Zhou HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1667, DOI 10.1145/3343031.3351011
   Zhou LH, 2022, IEEE T NEUR NET LEAR, V33, P5308, DOI 10.1109/TNNLS.2021.3070085
   Zhou YQ, 2022, IEEE T CIRC SYST VID, V32, P7147, DOI 10.1109/TCSVT.2022.3168967
NR 70
TC 1
Z9 1
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4540
EP 4553
DI 10.1109/TMM.2023.3323862
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100056
DA 2024-08-05
ER

PT J
AU Li, WJ
   Li, JC
   Gao, GW
   Deng, WH
   Zhou, JT
   Yang, J
   Qi, GJ
AF Li, Wenjie
   Li, Juncheng
   Gao, Guangwei
   Deng, Weihong
   Zhou, Jiantao
   Yang, Jian
   Qi, Guo-Jun
TI Cross-Receptive Focused Inference Network for Lightweight Image
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Computational modeling; Computed tomography; Feature
   extraction; Convolution; Adaptation models; Task analysis; SISR;
   Cross-receptive; contextual information; efficient model
AB Recently, Transformer-based methods have shown impressive performance in single image super-resolution (SISR) tasks due to the ability of global feature extraction. However, the capabilities of Transformers that need to incorporate contextual information to extract features dynamically are neglected. To address this issue, we propose a lightweight Cross-receptive Focused Inference Network (CFIN) that consists of a cascade of CT Blocks mixed with CNN and Transformer. Specifically, in the CT block, we first propose a CNN-based Cross-Scale Information Aggregation Module (CIAM) to enable the model to better focus on potentially helpful information to improve the efficiency of the Transformer phase. Then, we design a novel Cross-receptive Field Guided Transformer (CFGT) to enable the selection of contextual information required for reconstruction by using a modulated convolutional kernel that understands the current semantic information and exploits the information interaction within different self-attention. Extensive experiments have shown that our proposed CFIN can effectively reconstruct images using contextual information, and it can strike a good balance between computational cost and model performance as an efficient model.
C1 [Li, Wenjie; Gao, Guangwei] Nanjing Univ Posts & Telecommun, Inst Adv Technol, Intelligent Visual Informat Percept Lab, Nanjing, Peoples R China.
   [Li, Wenjie; Gao, Guangwei] Soochow Univ, Prov Key Lab Comp Informat Proc Technol, Suzhou 215006, Peoples R China.
   [Li, Juncheng] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Li, Juncheng] Nanjing Univ Sci & Technol, Jiangsu Key Lab Image & Video Understanding Social, Nanjing, Peoples R China.
   [Deng, Weihong] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Pattern Recognit & Intelligent Syst Lab, Beijing 100876, Peoples R China.
   [Zhou, Jiantao] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Peoples R China.
   [Qi, Guo-Jun] Westlake Univ, Res Ctr Ind Future, Hangzhou 310024, Peoples R China.
   [Qi, Guo-Jun] Westlake Univ, Sch Engn, Hangzhou 310024, Peoples R China.
   [Qi, Guo-Jun] OPPO Res, Seattle, WA 98101 USA.
C3 Nanjing University of Posts & Telecommunications; Soochow University -
   China; Shanghai University; Nanjing University of Science & Technology;
   Beijing University of Posts & Telecommunications; University of Macau;
   Nanjing University of Science & Technology; Westlake University;
   Westlake University
RP Gao, GW (corresponding author), Nanjing Univ Posts & Telecommun, Inst Adv Technol, Intelligent Visual Informat Percept Lab, Nanjing, Peoples R China.
EM lewj2408@gmail.com; cvjunchengli@gmail.com; csggao@gmail.com;
   whdeng@bupt.edu.cn; jtzhou@um.edu.mo; csjyang@njust.edu.cn;
   guojunq@gmail.com
RI Li, Wenjie/AAE-6867-2019; DENG, WEIHONG/JOZ-1320-2023
OI Li, Wenjie/0000-0003-1687-4186; DENG, WEIHONG/0000-0001-5952-6996; li,
   wenjie/0000-0003-3086-5307; Li, Juncheng/0000-0001-7314-6754
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Chen XY, 2022, Arxiv, DOI arXiv:2205.04437
   Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756
   De Brabandere B, 2016, ADV NEUR IN, V29
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du ZC, 2022, IEEE COMPUT SOC CONF, P852, DOI 10.1109/CVPRW56347.2022.00101
   Gao GW, 2023, IEEE T IMAGE PROCESS, V32, P1978, DOI 10.1109/TIP.2023.3261747
   Gao GW, 2022, AAAI CONF ARTIF INTE, P661
   Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476
   He LZ, 2021, PROC CVPR IEEE, P9225, DOI 10.1109/CVPR46437.2021.00911
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu Mengshun, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P847, DOI 10.1145/3503161.3547874
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Jang E, 2017, Arxiv, DOI arXiv:1611.01144
   Jiang K, 2022, IEEE T NEUR NET LEAR, V33, P378, DOI 10.1109/TNNLS.2020.3027849
   Jiang K, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107475
   Jie Liu, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P41, DOI 10.1007/978-3-030-67070-2_2
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P1443, DOI 10.1109/TCYB.2020.2970104
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li F., 2022, arXiv
   Li GY, 2022, PROC CVPR IEEE, P20604, DOI 10.1109/CVPR52688.2022.01998
   Li JC, 2021, IEEE T CIRC SYST VID, V31, P2547, DOI 10.1109/TCSVT.2020.3027732
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3203749
   Li W., 2020, Advances in Neural Information Processing Systems, V33, P20343
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Luo XT, 2023, IEEE T PATTERN ANAL, V45, P4826, DOI 10.1109/TPAMI.2022.3194090
   Magid SA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4268, DOI [10.1109/ICCV48922.2021.00425, 10.1109/iccv48922.2021.00425]
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Muqeet Abdul, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P103, DOI 10.1007/978-3-030-67070-2_6
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Salimans T, 2016, ADV NEUR IN, V29
   Tang Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2148, DOI 10.1145/3474085.3475373
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Vaswani A., 2017, P 31 INT C NEUR INF
   Wang CF, 2019, Arxiv, DOI arXiv:1904.02358
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wei Pengxu, 2020, P EUR C COMP VIS, P101, DOI DOI 10.1007/978-3-030-58598-3_7
   Wu FL, 2019, Arxiv, DOI arXiv:1901.10430
   Wu Y., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM.2022.3216115, DOI 10.1109/TMM.2022.3216115]
   Xiangxiang Chu, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P99, DOI 10.1007/978-3-030-66823-5_6
   Xiaotong Luo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P272, DOI 10.1007/978-3-030-58542-6_17
   Xudong Lin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P701, DOI 10.1007/978-3-030-58523-5_41
   Yang YB, 2018, PROC CVPR IEEE, P2413, DOI 10.1109/CVPR.2018.00256
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Yu R, 2022, PROC CVPR IEEE, P7257, DOI 10.1109/CVPR52688.2022.00712
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang DY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3682, DOI 10.1145/3474085.3475650
   Zhang R, 2019, PR MACH LEARN RES, V97
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H., 2020, EUROPEAN C COMPUTER, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 67
TC 5
Z9 5
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 864
EP 877
DI 10.1109/TMM.2023.3272474
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700019
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, ZZ
   Zhu, ZF
   Zheng, S
   Zhao, YW
   He, KL
   Zhao, Y
AF Liu, Zhizhe
   Zhu, Zhenfeng
   Zheng, Shuai
   Zhao, Yawei
   He, Kunlun
   Zhao, Yao
TI From Observation to Concept: A Flexible Multi-View Paradigm for Medical
   Report Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Medical report generation; image captioning; image-report alignment;
   multi-view knowledge
ID TRANSFORMER; ADAPTATION; ALIGNMENT; NETWORK; IMAGE
AB Automated radiology report generation aims to generate accurate and radiologist-like descriptions for the patient's images, which can greatly relieve the workload of radiologists. However, due to the data bias and long report problems, medical report generation has been a challenging task. In this article, we propose a Flexible Multi-view Paradigm (FMVP) for medical report generation in a novel observation-to-concept manner. It first makes some medical observations automatically or with the help of a radiologist on the patient's image to obtain patient-related priori knowledge, just as radiologists do in practice. Furthermore, to bridge the gap between pretrain and generation phases, the hierarchical alignment is proposed to jointly conduct the implicit alignment between region-tag and the explicit global alignment of the image-report pair. Finally, a compatible decoder towards decoding the fused multi-view knowledge is proposed to capture more complementary information for the report generation, which breaks the traditional entrenched decoding mechanism guided by visual information. Extensive quantitative and qualitative experiments on the public MIMIC-CXR and IU-Xray datasets show that our model achieves competitive performance compared to state-of-the-art methods.
C1 [Liu, Zhizhe; Zhu, Zhenfeng; Zheng, Shuai; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Liu, Zhizhe; Zhu, Zhenfeng; Zheng, Shuai; Zhao, Yao] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Zhao, Yawei; He, Kunlun] Chinese Peoples Liberat Army Gen Hosp, Med Big Data Res Ctr, Beijing 100853, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Chinese
   People's Liberation Army General Hospital
RP Zhu, ZF (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.; Zhu, ZF (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.; He, KL (corresponding author), Chinese Peoples Liberat Army Gen Hosp, Med Big Data Res Ctr, Beijing 100853, Peoples R China.
EM zhzliu@bjtu.edu.cn; zhfzhu@bjtu.edu.cn; zs1997@bjtu.edu.cn;
   csyawei.zhao@gmail.com; kunlunhe@plagh.org; yzhao@bjtu.edu.cn
RI He, Kunlun/IXN-1108-2023
OI He, Kunlun/0000-0002-3335-5700; Zhao, Yao/0000-0002-8581-9554; Liu,
   Zhizhe/0000-0001-7571-4161; Zheng, Shuai/0000-0001-8560-8135
FU Science and Technology Innovation 2030 #x2013; New Generation Artificial
   Intelligence Major Project
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Banerjee S., 2005, ACL WORKSH INTR EXTR, P65
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Chen C, 2020, IEEE T MED IMAGING, V39, P2494, DOI 10.1109/TMI.2020.2972701
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen Z., 2021, Long Papers, P5904
   Chen ZH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1439
   Demner-Fushman D, 2016, J AM MED INFORM ASSN, V23, P304, DOI 10.1093/jamia/ocv080
   Huang G, 2022, IEEE T PATTERN ANAL, V44, P8704, DOI 10.1109/TPAMI.2019.2918284
   Huang ZZ, 2023, PROC CVPR IEEE, P19809, DOI 10.1109/CVPR52729.2023.01897
   Jain Saahil, 2021, P NEUR INF PROC SYST, V1
   Jing BY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2577
   Johnson AEW, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0322-0
   Li JH, 2021, ADV NEUR IN, V34
   Li Y., 2018, P NEUR INF PROC SYST, V31
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin C, 2022, LECT NOTES COMPUT SC, V13437, P507, DOI 10.1007/978-3-031-16449-1_48
   Liu FL, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P269
   Liu FL, 2021, PROC CVPR IEEE, P13748, DOI 10.1109/CVPR46437.2021.01354
   Liu ZZ, 2022, IEEE J BIOMED HEALTH, V26, P638, DOI 10.1109/JBHI.2022.3140853
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo JJ, 2023, PROC CVPR IEEE, P23359, DOI 10.1109/CVPR52729.2023.02237
   Moor M, 2023, PR MACH LEARN RES, V225, P353
   Pan XX, 2021, IEEE T MED IMAGING, V40, P81, DOI 10.1109/TMI.2020.3022591
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Smit A, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1500
   van Sonsbeek T, 2023, LECT NOTES COMPUT SC, V14224, P726, DOI 10.1007/978-3-031-43904-9_70
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang J, 2022, LECT NOTES COMPUT SC, V13695, P563, DOI 10.1007/978-3-031-19833-5_33
   Wang ZY, 2022, IEEE T MED IMAGING, V41, P2803, DOI 10.1109/TMI.2022.3171661
   Wang ZY, 2021, PROC CVPR IEEE, P2433, DOI 10.1109/CVPR46437.2021.00246
   Xu MY, 2021, LECT NOTES COMPUT SC, V12904, P269, DOI 10.1007/978-3-030-87202-1_26
   Yang SX, 2022, MED IMAGE ANAL, V80, DOI 10.1016/j.media.2022.102510
   Yang Y, 2023, IEEE T MULTIMEDIA, V25, P167, DOI 10.1109/TMM.2021.3122542
   You D, 2021, LECT NOTES COMPUT SC, V12903, P72, DOI 10.1007/978-3-030-87199-4_7
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Zhang YX, 2020, AAAI CONF ARTIF INTE, V34, P12910
   Zhao ZX, 2021, MED IMAGE ANAL, V74, DOI 10.1016/j.media.2021.102240
   Zheng S, 2022, IEEE T MED IMAGING, V41, P2207, DOI 10.1109/TMI.2022.3159264
   Zhou Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3965, DOI 10.1109/ICCV48922.2021.00395
NR 41
TC 0
Z9 0
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5987
EP 5995
DI 10.1109/TMM.2023.3342691
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100006
DA 2024-08-05
ER

PT J
AU Long, Z
   Zhu, C
   Chen, J
   Li, ZH
   Ren, YZ
   Liu, YP
AF Long, Zhen
   Zhu, Ce
   Chen, Jie
   Li, Zihan
   Ren, Yazhou
   Liu, Yipeng
TI Multi-View MERA Subspace Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensors; Correlation; Matrix decomposition; Clustering algorithms; Task
   analysis; Sparse matrices; Singular value decomposition; Low-rank tensor
   approximation; MERA decomposition; multi-view subspace clustering;
   self-representation learning
ID REPRESENTATION; IMAGE; GRAPH
AB Tensor-based multi-view subspace clustering (MSC) can capture high-order correlation in the self-representation tensor. Current tensor decompositions for MSC suffer from highly unbalanced unfolding matrices or rotation sensitivity, failing to fully explore inter/intra-view information. Using the advanced tensor network, namely, multi-scale entanglement renormalization ansatz (MERA), we propose a low-rank MERA based MSC (MERA-MSC) algorithm, where MERA factorizes a tensor into contractions of one top core factor and the rest orthogonal/semi-orthogonal factors. Benefiting from multiple interactions among orthogonal/semi-orthogonal (low-rank) factors, the low-rank MERA has a strong representation power to capture the complex inter/intra-view information in the self-representation tensor. The alternating direction method of multipliers is adopted to solve the optimization model. Experimental results on five multi-view datasets demonstrate MERA-MSC has superiority against the compared algorithms on six evaluation metrics. Furthermore, we extend MERA-MSC by incorporating anchor learning and develop a scalable low-rank MERA based multi-view clustering method (sMREA-MVC). To our knowledge, this is the first work to introduce MERA to the multi-view clustering topic. The effectiveness and efficiency of sMERA-MVC have been validated on three large-scale multi-view datasets.
C1 [Long, Zhen; Zhu, Ce; Chen, Jie; Li, Zihan; Liu, Yipeng] Univ Elect Sci & Technol China, Sch Commun & Informat Engn, Chengdu 611731, Peoples R China.
   [Ren, Yazhou] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China
RP Zhu, C; Liu, YP (corresponding author), Univ Elect Sci & Technol China, Sch Commun & Informat Engn, Chengdu 611731, Peoples R China.
EM longzhen@std.uestc.edu.cn; eczhu@uestc.edu.cn;
   202252012138@std.uestc.edu.cn; zihanli@std.uestc.edu.cn;
   yazhou.ren@uestc.edu.cn; yipengliu@uestc.edu.cn
OI long, zhen/0000-0002-1772-916X; Ren, Yazhou/0000-0001-7705-4603
FU National Natural Science Foundation of China
FX No Statement Available
CR Batselier K, 2021, COM APPL MATH COMPUT, V3, P257, DOI 10.1007/s42967-020-00090-6
   Belhumeur P., EIGENFACES VS FISHER
   Bengua JA, 2017, IEEE T IMAGE PROCESS, V26, P2466, DOI 10.1109/TIP.2017.2672439
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai X., Joint stage recognition and anatomical annotation of drosophila gene expression patterns
   Chen J, 2022, IEEE T CYBERNETICS, V52, P12364, DOI 10.1109/TCYB.2021.3087114
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P4054, DOI 10.1109/TMM.2021.3112230
   Chen YY, 2022, IEEE T CIRC SYST VID, V32, P92, DOI 10.1109/TCSVT.2021.3055625
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fei-Fei Li., LEARNING GENERATIVE
   Fu LL, 2023, IEEE T MULTIMEDIA, V25, P4972, DOI 10.1109/TMM.2022.3185886
   Gao QX, 2020, AAAI CONF ARTIF INTE, V34, P3930
   Georghiades A. S., From few to many: Illumination cone models for face recognition under variable lighting and pose
   Geusebroek J.-M., The amsterdam library of object images
   Hu ZX, 2020, NEUROCOMPUTING, V384, P1, DOI 10.1016/j.neucom.2019.12.004
   Huang D, 2023, IEEE T KNOWL DATA EN, V35, P11388, DOI 10.1109/TKDE.2023.3236698
   Jia YH, 2021, IEEE T CIRC SYST VID, V31, P4784, DOI 10.1109/TCSVT.2021.3055039
   Jiang Y.-G., Consumer video understanding: A benchmark database and an evaluation of human and machine performance
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Liang YW, 2024, IEEE T NEUR NET LEAR, V35, P2848, DOI 10.1109/TNNLS.2022.3192445
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu YP, 2018, IEEE J-STSP, V12, P1378, DOI 10.1109/JSTSP.2018.2873142
   Liu Y, 2023, COMPUTATION STAT, V38, P935, DOI 10.1007/s00180-022-01252-1
   Long Z., 2023, P IEEE INT C AC SPEE, P1
   Long Z, 2021, IEEE T IMAGE PROCESS, V30, P3568, DOI 10.1109/TIP.2021.3062195
   Manning C.D., 2008, Introduction to information retrieval
   Ren YZ, 2020, KNOWL-BASED SYST, V197, DOI 10.1016/j.knosys.2020.105841
   Shu XC, 2023, IEEE T MULTIMEDIA, V25, P5485, DOI 10.1109/TMM.2022.3193855
   Si XM, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108196
   Sun MJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3528, DOI 10.1145/3474085.3475516
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tang YQ, 2023, IEEE T PATTERN ANAL, V45, P9357, DOI 10.1109/TPAMI.2023.3257407
   Tang YQ, 2022, IEEE T MULTIMEDIA, V24, P3920, DOI 10.1109/TMM.2021.3110098
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vidal G, 2008, PHYS REV LETT, V101, DOI 10.1103/PhysRevLett.101.110501
   Vidal R, 2014, PATTERN RECOGN LETT, V43, P47, DOI 10.1016/j.patrec.2013.08.006
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang HB, 2021, IEEE T MULTIMEDIA, V23, P3828, DOI 10.1109/TMM.2020.3032023
   Wang SQ, 2021, IEEE IMAGE PROC, P1534, DOI 10.1109/ICIP42928.2021.9506640
   Wang SW, 2022, IEEE T IMAGE PROCESS, V31, P556, DOI 10.1109/TIP.2021.3131941
   Wang WQ, 2017, IEEE I CONF COMP VIS, P5698, DOI 10.1109/ICCV.2017.607
   Wang Y., 2016, P 25 INT JOINT C ART, P2153
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3408317
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wen J, 2023, IEEE T SYST MAN CY-S, V53, P1136, DOI 10.1109/TSMC.2022.3192635
   Winn J., Locus: Learning object classes with unsupervised segmentation
   Xia W, 2023, IEEE T PATTERN ANAL, V45, P5187, DOI 10.1109/TPAMI.2022.3187976
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xia W, 2022, IEEE T CYBERNETICS, V52, P8962, DOI 10.1109/TCYB.2021.3052352
   Xiao XL, 2021, IEEE T MULTIMEDIA, V23, P4555, DOI 10.1109/TMM.2020.3045259
   Xiaobo Wang, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P1, DOI 10.1109/CVPR.2017.8
   Xie Y, 2021, IEEE T NEUR NET LEAR, V32, P868, DOI 10.1109/TNNLS.2020.2979685
   Xie Y, 2020, IEEE T CYBERNETICS, V50, P572, DOI 10.1109/TCYB.2018.2869789
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Xu J, 2023, IEEE T IMAGE PROCESS, V32, P1354, DOI 10.1109/TIP.2023.3243521
   Ye K., 2018, arXiv
   Yin M, 2019, IEEE T NEUR NET LEAR, V30, P851, DOI 10.1109/TNNLS.2018.2851444
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang GY, 2021, EXPERT SYST APPL, V166, DOI 10.1016/j.eswa.2020.113913
   Zhang YF, 2009, IEEE T MULTIMEDIA, V11, P1276, DOI 10.1109/TMM.2009.2030629
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zheng S, 2015, AAAI CONF ARTIF INTE, P1973
   Zhou T, 2020, IEEE T CYBERNETICS, V50, P3517, DOI 10.1109/TCYB.2019.2918495
NR 68
TC 2
Z9 2
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3102
EP 3112
DI 10.1109/TMM.2023.3307239
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700012
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ma, WT
   Wu, XY
   Zhao, S
   Zhou, TQ
   Guo, D
   Gu, LC
   Cai, ZP
   Wang, M
AF Ma, Wentao
   Wu, Xinyi
   Zhao, Shan
   Zhou, Tongqing
   Guo, Dan
   Gu, Lichuan
   Cai, Zhiping
   Wang, Meng
TI FedSH: Towards Privacy-Preserving Text-Based Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Training; Task analysis; Privacy; Visualization; Federated
   learning; Servers; Text-based Person ReID; Cross-modal Retrieval;
   Federated Learning; Multi-granularity Representation
AB Text-based person re-identification (ReID) has enabled canonical applications in searching for and tracking targets from large-scale surveillance images with textual descriptions. Yet, existing text-based person ReID systems employ centralized model training that gathers images captured by different institutes' cameras into one place, which poses severe privacy threats to sensitive institutional information. This work is then devoted to exploring privacy-preserving text-based person ReID and proposes the framework of FedSH by tailoring the federated learning paradigm for distributed searching knowledge extraction. Specifically, FedSH resolves the local model generalization and entity boundary obscuring limitations, caused by inner-institute data homogeneity and inter-institute data heterogeneity, via building multi-granularity feature representation and a semantically self-aligned network. Meanwhile, it reduces the communication burden introduced by the embedding for multiple modals by updating common representation subspaces during federated learning. Experimental results on two public benchmarks demonstrate that our method can achieve at most 16.47% and 16.02% person ReID performance improvement by the Rank-1 metric, compared with 6 State-of-The-Art (SoTA) baselines and 6 ablation studies. We believe that our work will inspire the community to investigate the potential of implementing Federated Learning in real-world image retrieval and ReID scenarios.
C1 [Ma, Wentao] Anhui Agr Univ, Sch Informat & Artificial Intelligence, Hefei 230036, Peoples R China.
   [Wu, Xinyi; Zhou, Tongqing; Cai, Zhiping] Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.
   [Zhao, Shan; Guo, Dan; Wang, Meng] Hefei Univ Technol, Sch Comp & Informat Engn, Hefei 230009, Peoples R China.
C3 Anhui Agricultural University; National University of Defense Technology
   - China; Hefei University of Technology
RP Zhou, TQ (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China.; Zhao, S (corresponding author), Hefei Univ Technol, Sch Comp & Informat Engn, Hefei 230009, Peoples R China.
EM wtma@ahau.edu.cn; wuxinyi17@nudt.edu.cn; zhaoshan@hfut.edu.cn;
   zhoutongqing@nudt.edu.cn; guodan@hfut.edu.cn; glc@ahau.edu.cn;
   zpcai@nudt.edu.cn; eric.mengwang@gmail.com
OI Gu, Lichuan/0000-0002-3768-8203; Ma, Wentao/0000-0003-3059-6629; Cai,
   Zhiping/0000-0001-5726-833X; Guo, Dan/0000-0003-2594-254X
FU National Natural Science Foundation of China
FX No Statement Available
CR Aggarwal S, 2020, IEEE WINT CONF APPL, P2606, DOI [10.1109/wacv45572.2020.9093640, 10.1109/WACV45572.2020.9093640]
   Caldas S., 2018, arXiv
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen S., 2020, CVPR, P10638
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Chen YH, 2022, NEUROCOMPUTING, V494, P171, DOI 10.1016/j.neucom.2022.04.081
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Ding ZF, 2021, Arxiv, DOI arXiv:2107.12666
   Farooq A, 2022, AAAI CONF ARTIF INTE, P4477
   Galiyawala M. S., 2021, IEEE INT C ADV VIDEO, P1
   Gao CY, 2021, Arxiv, DOI arXiv:2101.03036
   Guo YT, 2021, PROC ACM INTERACT MO, V5, DOI 10.1145/3448099
   Jia C, 2021, PR MACH LEARN RES, V139
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Kingma D. P., 2014, arXiv
   Konečny J, 2017, Arxiv, DOI arXiv:1610.05492
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Li T., 2020, P MACH LEARN SYST ML, V2, P429, DOI DOI 10.48550/ARXIV.1812.06127
   Liu Bingyan, 2021, Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., V5, P1
   Liu J, 2022, KNOWL INF SYST, V64, P885, DOI 10.1007/s10115-022-01664-x
   Liu ZP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4289, DOI 10.1145/3394171.3413689
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Ma WT, 2022, KNOWL-BASED SYST, V241, DOI 10.1016/j.knosys.2022.108213
   Mahlool Dhurgham Hassan, 2022, Mobile Computing and Sustainable Informatics: Proceedings of ICMCSI 2022. Lecture Notes on Data Engineering and Communications Technologies (126), P539, DOI 10.1007/978-981-19-2069-1_37
   McMahan HB, 2017, PR MACH LEARN RES, V54, P1273
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Radford A, 2021, PR MACH LEARN RES, V139
   Spolaôr N, 2020, ENG APPL ARTIF INTEL, V90, DOI 10.1016/j.engappai.2020.103557
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Chengji, 2021, P 30 INT JOINT C ART, P1068
   Wu G., 2021, arXiv
   Wu GL, 2021, AAAI CONF ARTIF INTE, V35, P2898
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Xiong BC, 2022, NEUROCOMPUTING, V480, P110, DOI 10.1016/j.neucom.2022.01.063
   Yang FX, 2022, Arxiv, DOI arXiv:2203.02689
   Yang FX, 2020, IEEE T MULTIMEDIA, V22, P2444, DOI 10.1109/TMM.2019.2957928
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Yao J, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P3757, DOI 10.1145/3442381.3449936
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zeng H, 2021, PROC INT CONF PARAL, DOI 10.1145/3472456.3472504
   Zhang GW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P516, DOI 10.1145/3474085.3475202
   Zhang SZ, 2023, PROCEEDINGS OF THE 4TH INTERNATIONAL WORKSHOP ON HUMAN-CENTRIC MULTIMEDIA ANALYSIS, HCMA 2023, P5, DOI 10.1145/3606041.3618058
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhao SZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11375, DOI 10.1109/ICCV48922.2021.01120
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
   Zhuang WM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P433, DOI 10.1145/3474085.3475182
   Zhuang WM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P955, DOI 10.1145/3394171.3413814
   Zong LL, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1672, DOI 10.1145/3404835.3462989
NR 55
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5065
EP 5077
DI 10.1109/TMM.2023.3330091
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600011
DA 2024-08-05
ER

PT J
AU Qi, HR
   Qiu, YW
   Luo, X
   Jin, Z
AF Qi, Haoran
   Qiu, Yuwei
   Luo, Xing
   Jin, Zhi
TI An Efficient Latent Style Guided Transformer-CNN Framework for Face
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face super-resolution; latent style; transformer-CNN; recursive learning
ID NETWORK
AB In the Face Super-Resolution (FSR) task, it is important to precisely recover facial textures while maintaining facial contours for realistic high resolution faces. Although several CNN-based FSR methods have achieved great performance, they fail in restoring the facial contours due to the limitation of local convolutions. In contrast, Transformer-based methods which use self-attention as the basic component, are expert in modeling long-range dependencies between image patches. However, learning long-range dependencies often deteriorates facial textures due to the lack of locality. Therefore, a question is naturally raised: how to effectively combine the superiority of CNN and Transformer for better reconstructing faces? To address this issue, we propose an Efficient Latent Style guided Transformer-CNN framework for FSR called <bold>ELSFace</bold>, which can sufficiently integrate the advantages of CNN and Transformer. The framework consists of a Feature Preparation Stage and a Feature Carving Stage. Basic facial contours and textures are generated in the Feature Preparation Stage, and separately guided by latent styles, so that facial details are better represented in reconstruction. CNN and Transformer streams in the Feature Carving Stage are used to individually restore facial textures and facial contours, respectively in a parallel recursive way. Considering the negligence of high-frequency features when learning the long-range dependencies, we design the High-Frequency Enhancement Block (HFEB) in the Transformer stream. The Sharp Loss is also proposed for better perceptual quality in optimization. Extensive experimental results demonstrate that our ELSFace can achieve the best results among all metrics compared to the state-of-the-art CNN and Transformer-based methods on commonly used datasets and real-world tasks. Meanwhile, our ELSFace method has the least model parameters and running time. The codes are released at https://github.com/FVL2020/ELSFace.
C1 [Qi, Haoran; Qiu, Yuwei; Jin, Zhi] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Shenzhen Campus, Shenzhen 518107, Peoples R China.
   [Qi, Haoran; Qiu, Yuwei; Jin, Zhi] Sun Yat Sen Univ, Shenzhen 518107, Peoples R China.
   [Luo, Xing] Peng Cheng Lab, Dept Math & Theories, Shenzhen 518055, Peoples R China.
   [Jin, Zhi] Guangdong Prov Key Lab Fire Sci & Technol, Guangzhou 510006, Peoples R China.
   [Jin, Zhi] Guangdong Prov Key Lab Robot & Digital Intelligent, Guangzhou 510535, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Peng Cheng Laboratory
RP Jin, Z (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Shenzhen Campus, Shenzhen 518107, Peoples R China.
EM qihr3@mail2.sysu.edu.cn; qiuyw9@mail2.sysu.edu.cn; luox@pcl.ac.cn;
   jinzh26@mail.sysu.edu.cn
RI Jin, Zhi/AAB-2440-2022
OI Jin, Zhi/0000-0001-9670-7366; , Haoran Qi/0009-0002-9463-6482; qiu,
   yuwei/0000-0003-1915-0749
FU National Natural Science Foundation of China
FX No Statement Available
CR Adjabi I, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9081188
   Amos B., 2016, Openface: A general-purpose face recognition library with mobile applications
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Chen CF, 2021, PROC CVPR IEEE, P11891, DOI 10.1109/CVPR46437.2021.01172
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gao GW, 2023, IEEE T IMAGE PROCESS, V32, P1978, DOI 10.1109/TIP.2023.3261747
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZY, 2022, IEEE T MULTIMEDIA, V24, P2877, DOI 10.1109/TMM.2021.3090166
   Hensel M, 2017, ADV NEUR IN, V30
   Hou H, 2023, IEEE T IMAGE PROCESS, V32, P1184, DOI 10.1109/TIP.2023.3240845
   Howse J., 2013, OpenCV computer vision with python
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Ji JY, 2023, IEEE T MULTIMEDIA, V25, P3962, DOI 10.1109/TMM.2022.3169061
   Jin Z, 2021, IEEE T CIRC SYST VID, V31, P467, DOI 10.1109/TCSVT.2020.2982174
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Nguyen K, 2018, PATTERN RECOGN, V78, P23, DOI 10.1016/j.patcog.2018.01.002
   Kingma D. P., 2014, arXiv
   Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin JX, 2023, IEEE T MULTIMEDIA, V25, P8396, DOI 10.1109/TMM.2023.3236845
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5501, DOI 10.1145/3474085.3475682
   Lu Z., 2021, arXiv
   Ma C, 2020, PROC CVPR IEEE, P5568, DOI 10.1109/CVPR42600.2020.00561
   Park N., 2022, P INT C LEARN REPR, P1
   Paszke A, 2019, ADV NEUR IN, V32
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Qi HR, 2021, PROC INT C TOOLS ART, P1409, DOI 10.1109/ICTAI52525.2021.00224
   Si C., 2022, Adv. Neural Inf. Process. Syst, V35, P23495
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wang YQ, 2023, IEEE T PATTERN ANAL, V45, P425, DOI 10.1109/TPAMI.2022.3152488
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wang ZN, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107694
   Xin JW, 2019, AAAI CONF ARTIF INTE, P9054
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang LB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1551, DOI 10.1145/3394171.3413965
   Yin Z, 2021, COGN NEURODYNAMICS, V15, P169, DOI 10.1007/s11571-020-09615-4
   Yu X, 2020, IEEE T PATTERN ANAL, V42, P2926, DOI 10.1109/TPAMI.2019.2916881
   Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang HR, 2021, IEEE J-STSP, V15, P253, DOI 10.1109/JSTSP.2020.3045282
   Zhang HR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2113, DOI 10.1145/3394171.3413664
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XD, 2022, LECT NOTES COMPUT SC, V13677, P649, DOI 10.1007/978-3-031-19790-1_39
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zheng HY, 2022, LECT NOTES COMPUT SC, V13678, P502, DOI 10.1007/978-3-031-19797-0_29
NR 58
TC 5
Z9 5
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1589
EP 1599
DI 10.1109/TMM.2023.3283856
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800028
DA 2024-08-05
ER

PT J
AU Qin, QB
   Huo, YD
   Huang, L
   Dai, JY
   Zhang, HH
   Zhang, WF
AF Qin, Qibing
   Huo, Yadong
   Huang, Lei
   Dai, Jiangyan
   Zhang, Huihui
   Zhang, Wenfeng
TI Deep Neighborhood-Preserving Hashing With Quadratic Spherical Mutual
   Information for Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Mutual information; Transformers; Feature extraction; Clamps;
   Binary codes; Tuning; Cross-modal retrieval; deep hashing; quadratic
   mutual information; neighborhood structure; transformer encoder
ID NETWORK
AB Driven by the high nonlinearity of deep neural networks, deep hashing has achieved the pictured great potential in cross-modal retrieval applications, significantly bridging the modality gap. Current deep cross-modal hashing usually utilizes affinity matching or local ranking to capture the local semantic relationships in the learned common space, leading to high neighborhood ambiguity. Simultaneously, most of these frameworks utilize additional regularization terms or margin thresholds to enhance the overall performance, in which searching the model's hyper-parameters under mass training data would have a substantial overhead. In this paper, with a novel extension of information-theoretic measures, a novel deep cross-modal hashing method, named Deep Neighborhood-preserving Hashing (DNpH), is designed to learn a highly separable discrete space, effectively mitigating the semantic gap across different modalities. Specifically, to minimize neighborhood ambiguity, the Quadratic Spherical Mutual Information (QSMI) is first introduced into deep cross-modal hashing to separate neighbors and non-neighbors well, while it is free of tuning parameters during model training compared with other similarity measures. To optimize quadratic mutual information loss smoothly, a square clamping method is developed to improve the stability of model optimization, avoiding converging on bad local optimum. Besides, two transformer encoders are exploited as feature extractors for multi-modal samples to learn the informative semantic representations. Finally, we compare our proposed DNpH framework with various state-of-the-art cross-modal hashing on four public datasets, and large amounts of experiment results demonstrate our contributions and show that DNpH outperforms the compared baselines on different evaluation metrics.
C1 [Qin, Qibing; Dai, Jiangyan; Zhang, Huihui] Weifang Univ, Sch Comp Engn, Weifang 261061, Peoples R China.
   [Qin, Qibing; Huang, Lei] Ocean Univ China, Fac Informat Sci & Engn, Qingdao 266005, Peoples R China.
   [Huo, Yadong] Qufu Normal Univ, Sch Comp Sci, Rizhao 273165, Peoples R China.
   [Zhang, Wenfeng] Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401333, Peoples R China.
   [Zhang, Wenfeng] Hangzhou Dianzi Univ, Lishui Inst, Lishui 310005, Peoples R China.
C3 Weifang University; Ocean University of China; Qufu Normal University;
   Chongqing Normal University; Hangzhou Dianzi University
RP Zhang, HH (corresponding author), Weifang Univ, Sch Comp Engn, Weifang 261061, Peoples R China.; Zhang, WF (corresponding author), Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401333, Peoples R China.
EM qinbing@wfu.edu.cn; hyd199810@163.com; huangl@ouc.edu.cn;
   daijy@wfu.edu.cn; huihui@wfu.edu.cn; itzhangwf@cqnu.edu.cn
OI Zhang, Huihui/0000-0002-1012-8089; Huo, Yadong/0009-0008-8805-8958; Qin,
   Qibing/0000-0001-7976-318X; Huang, Lei/0000-0003-4087-3677
FU National Natural Science Foundation of China
FX No Statement Available
CR Bouzas D, 2015, IEEE T NEUR NET LEAR, V26, P951, DOI 10.1109/TNNLS.2014.2329240
   Cakir F, 2019, IEEE T PATTERN ANAL, V41, P2424, DOI 10.1109/TPAMI.2019.2914897
   Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55
   Cao M., 2022, PROC 31 INT JOINT, P5410, DOI 10.24963/ijcai.2022/759
   Cao Y, 2018, LECT NOTES COMPUT SC, V11205, P207, DOI 10.1007/978-3-030-01246-5_13
   Chen W, 2023, IEEE T PATTERN ANAL, V45, P7270, DOI 10.1109/TPAMI.2022.3218591
   Chen YD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1921, DOI 10.1145/3474085.3475346
   Cong Bai, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P525, DOI 10.1145/3372278.3390711
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Dosovitskiy A., 2021, PROC ICLR
   El Zini J, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3529755
   Gu W, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/3323873.3325045
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoang T, 2023, IEEE T NEUR NET LEAR, V34, P6289, DOI 10.1109/TNNLS.2021.3135420
   Hu HT, 2020, PROC CVPR IEEE, P3120, DOI 10.1109/CVPR42600.2020.00319
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Hu YP, 2021, IEEE T IMAGE PROCESS, V30, P4667, DOI 10.1109/TIP.2021.3073867
   Huo YD, 2024, IEEE T CIRC SYST VID, V34, P576, DOI 10.1109/TCSVT.2023.3285266
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li L, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3712, DOI 10.1145/3503161.3548431
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Liu YX, 2023, IEEE T MULTIMEDIA, V25, P2851, DOI 10.1109/TMM.2022.3152086
   Luo X, 2023, ACM T KNOWL DISCOV D, V17, DOI 10.1145/3532624
   Passalis N, 2021, SIGNAL PROCESS-IMAGE, V93, DOI 10.1016/j.image.2021.116146
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Paszke A, 2019, ADV NEUR IN, V32
   Qin JY, 2022, IEEE T IMAGE PROCESS, V31, P5343, DOI 10.1109/TIP.2022.3195059
   Qin QB, 2024, IEEE T MULTIMEDIA, V26, P1881, DOI 10.1109/TMM.2023.3289765
   Qin QB, 2023, IEEE T CIRC SYST VID, V33, P7914, DOI 10.1109/TCSVT.2023.3281868
   Qin QB, 2021, IEEE T CIRC SYST VID, V31, P2852, DOI 10.1109/TCSVT.2020.3032402
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Shi YF, 2022, IEEE T CIRC SYST VID, V32, P7255, DOI 10.1109/TCSVT.2022.3172716
   Simonyan K., 2014, C TRACK P
   Su MY, 2023, IEEE T MULTIMEDIA, V25, P662, DOI 10.1109/TMM.2021.3129623
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Sun CC, 2022, IEEE COMPUT SOC CONF, P4937, DOI 10.1109/CVPRW56347.2022.00541
   Tan WT, 2023, IEEE T MULTIMEDIA, V25, P4520, DOI 10.1109/TMM.2022.3177901
   Tang J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2564638
   Torkkola K., 2003, Journal of Machine Learning Research, V3, P1415, DOI 10.1162/153244303322753742
   Tu Junfeng, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P453, DOI 10.1145/3503161.3548187
   Tu RC, 2023, IEEE T KNOWL DATA EN, V35, P6798, DOI 10.1109/TKDE.2022.3187023
   Tzelepi M, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107407
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P1217, DOI 10.1109/TMM.2022.3140656
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang LD, 2023, PROC CVPR IEEE, P23455, DOI 10.1109/CVPR52729.2023.02246
   Wang L, 2022, IEEE T MULTIMEDIA, V24, P3665, DOI 10.1109/TMM.2021.3105824
   Wang XZ, 2020, NEUROCOMPUTING, V400, P255, DOI 10.1016/j.neucom.2020.03.019
   Wankhade M, 2022, ARTIF INTELL REV, V55, P5731, DOI 10.1007/s10462-022-10144-1
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yu J, 2021, AAAI CONF ARTIF INTE, V35, P4626
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang Z, 2023, IEEE T KNOWL DATA EN, V35, P5091, DOI 10.1109/TKDE.2022.3144352
   Zhu L, 2024, IEEE T KNOWL DATA EN, V36, P239, DOI 10.1109/TKDE.2023.3282921
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
   Zhu X., 2013, P 21 ACM INT C MULT, P143, DOI DOI 10.1145/2502081.2502107
   Zou XT, 2022, NEUROCOMPUTING, V467, P138, DOI 10.1016/j.neucom.2021.09.053
NR 64
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6361
EP 6374
DI 10.1109/TMM.2023.3349075
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600035
DA 2024-08-05
ER

PT J
AU Su, HL
   Liu, Q
   Yuan, H
   Cheng, Q
   Hamzaoui, R
AF Su, Honglei
   Liu, Qi
   Yuan, Hui
   Cheng, Qiang
   Hamzaoui, Raouf
TI Support Vector Regression-Based Reduced- Reference Perceptual Quality
   Model for Compressed Point Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; perceptual quality metric; feature selection;
   LASSO regression; support vector regression
AB Video-based point cloud compression (V-PCC) is a state-of-the-art moving picture experts group (MPEG) standard for point cloud compression. V-PCC can be used to compress both static and dynamic point clouds in a lossless, near lossless, or lossy way. Many objective quality metrics have been proposed for distorted point clouds. Most of these metrics are full-reference metrics that require both the original point cloud and the distorted one. However, in some real-time applications, the original point cloud is not available, and no-reference or reduced-reference quality metrics are needed. Three main challenges in the design of a reduced-reference quality metric are how to build a set of features that characterize the visual quality of the distorted point cloud, how to select the most effective features from this set, and how to map the selected features to a perceptual quality score. We address the first challenge by proposing a comprehensive set of features consisting of compression, geometry, normal, curvature, and luminance features. To deal with the second challenge, we use the least absolute shrinkage and selection operator (LASSO) method, which is a variable selection method for regression problems. Finally, we map the selected features to the mean opinion score in a nonlinear space. Although we have used only 19 features in our current implementation, our metric is flexible enough to allow any number of features, including future more effective ones. Experimental results on the Waterloo point cloud dataset version 2 (WPC2.0) and the MPEG point cloud compression dataset (M-PCCD) show that our method, namely PCQAML, outperforms state-of-the-art full-reference and reduced-reference quality metrics in terms of Pearson linear correlation coefficient, Spearman rank order correlation coefficient, Kendall's rank-order correlation coefficient, and root mean squared error.
C1 [Su, Honglei; Liu, Qi] Qingdao Univ, Coll Elect & Informat, Qingdao 266071, Peoples R China.
   [Yuan, Hui] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Cheng, Qiang] Univ Kentucky, Dept Comp Sci, Lexington, KY 40506 USA.
   [Cheng, Qiang] Univ Kentucky, Inst Biomed Informat, Lexington, KY 40506 USA.
   [Hamzaoui, Raouf] De Montfort Univ, Sch Engn & Sustainable Dev, Leicester LE1 9BH, England.
C3 Qingdao University; Shandong University; University of Kentucky;
   University of Kentucky; De Montfort University
RP Liu, Q (corresponding author), Qingdao Univ, Coll Elect & Informat, Qingdao 266071, Peoples R China.
EM suhonglei@qdu.edu.cn; sdqi.liu@gmail.com; huiyuan@sdu.edu.cn;
   qiang.cheng@uky.edu; rhamzaoui@dmu.ac.uk
RI Yuan, Hui/HDO-3699-2022
OI Yuan, Hui/0000-0001-5212-3393; Cheng, Qiang/0000-0002-3596-2838; Su,
   Honglei/0000-0001-6144-4930
FU National Science Foundation of China
FX No Statement Available
CR 3DG, 2018, Standard ISO/IEC JTC1/SC29/WG11 MPEG N18030
   3DG, 2019, Standard ISO/IEC JTC1/SC29/WG11 MPEG N18478
   Abouelaziz I, 2018, MULTIMED TOOLS APPL, V77, P24365, DOI 10.1007/s11042-018-5706-1
   Alexiou E, 2020, IEEE INT CONF MULTI
   Alexiou E, 2018, IEEE INT CON MULTI
   Alexiou E, 2019, APSIPA TRANS SIGNAL, V8, DOI 10.1017/ATSIP.2019.20
   Alexiou E, 2018, PROC SPIE, V10752, DOI 10.1117/12.2321518
   [Anonymous], 2015, Standard Recommendation ITU-R BT.709-6
   Antkowiak J., 2000, PROC VQEG M
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Diniz R, 2022, COMPUT GRAPH-UK, V103, P31, DOI 10.1016/j.cag.2022.01.003
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Fang YM, 2021, IEEE T MULTIMEDIA, V23, P955, DOI 10.1109/TMM.2020.2991528
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Girardeau-Montaut D., 2005, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, V36, pW19
   Gramfort A., 2021, Lasso code source
   Guede C., 2021, SMPTE Motion Imag. J., V130, P36
   Hamo Y, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P728
   Javaheri A, 2021, IEEE T MULTIMEDIA, V23, P4049, DOI 10.1109/TMM.2020.3037481
   Liu H, 2020, IEEE T BROADCAST, V66, P701, DOI 10.1109/TBC.2019.2957652
   Liu Q, 2023, IEEE T MULTIMEDIA, V25, P4533, DOI 10.1109/TMM.2022.3177926
   Liu Q, 2023, IEEE T VIS COMPUT GR, V29, P3642, DOI 10.1109/TVCG.2022.3167151
   Liu Q, 2021, IEEE T CIRC SYST VID, V31, P4645, DOI 10.1109/TCSVT.2021.3100282
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Liu TJ, 2013, IEEE T IMAGE PROCESS, V22, P1793, DOI 10.1109/TIP.2012.2236343
   Liu YP, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3550274
   Lu ZA, 2022, IEEE SIGNAL PROC LET, V29, P1804, DOI 10.1109/LSP.2022.3198601
   Mekuria R., 2016, Standard ISO/IEC JTC1/SC29/WG11 MPEG N16332
   Mekuria R, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Meynet G, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123147
   Meynet G, 2019, INT WORK QUAL MULTIM
   MPEG 3DG, 2019, V-PCC test model v7
   NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Rusu R. B., 2020, Compute underlying surface's normal and curvature in local neighborhoods (PCL)
   Sedgwick P, 2012, BRIT MED J, V344, DOI 10.1136/bmj.e4483
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheskin D.J., 2007, Handbook of Parametric and Nonparametric Statistical Procedures, V4th, P1353
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Su HL, 2023, IEEE T IMAGE PROCESS, V32, P1815, DOI 10.1109/TIP.2023.3253252
   Tao WX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5266, DOI 10.1145/3474085.3475645
   Tezcan J, 2012, EARTHQ ENG STRUCT D, V41, P515, DOI 10.1002/eqe.1142
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tu RW, 2023, IEEE T EM TOP COMP I, V7, P462, DOI 10.1109/TETCI.2022.3201619
   Vapnik V., 1999, The nature of statistical learning theory
   Viola I., 2020, PCM_RR
   Viola I, 2020, IEEE SIGNAL PROC LET, V27, P1660, DOI 10.1109/LSP.2020.3024065
   Viola I, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123089
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Winkler S, 2001, VISION MODELS AND APPLICATIONS TO IMAGE AND VIDEO PROCESSING, P201
   Xu YL, 2022, IEEE T BROADCAST, V68, P33, DOI 10.1109/TBC.2021.3114510
   Yan JB, 2020, IEEE T IMAGE PROCESS, V29, P7443, DOI 10.1109/TIP.2020.3003218
   Yang Q, 2022, IEEE T PATTERN ANAL, V44, P3015, DOI 10.1109/TPAMI.2020.3047083
   Yang Q, 2023, IEEE T PATTERN ANAL, V45, P6037, DOI 10.1109/TPAMI.2022.3213831
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yue GH, 2023, IEEE T CIRC SYST VID, V33, P5549, DOI 10.1109/TCSVT.2023.3260212
   Yue GH, 2023, IEEE T MULTIMEDIA, V25, P6499, DOI 10.1109/TMM.2022.3209889
   Yue GH, 2019, IEEE T IMAGE PROCESS, V28, P2075, DOI 10.1109/TIP.2018.2875913
   Zhang YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1230, DOI 10.1145/3474085.3475294
   Zhou W, 2023, IEEE SIGNAL PROC LET, V30, P354, DOI 10.1109/LSP.2023.3264105
NR 66
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6238
EP 6249
DI 10.1109/TMM.2023.3347638
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600042
DA 2024-08-05
ER

PT J
AU Wang, YZ
   Hu, WB
   Hong, RC
AF Wang, Youze
   Hu, Wenbo
   Hong, Richang
TI Iterative Adversarial Attack on Image-Guided Story Ending Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Perturbation methods; Iterative methods; Data models;
   Visualization; Fuses; Computational modeling; Multimodal; adversarial
   attack; multimodal text generation
AB Multimodal learning involves developing models that can integrate information from various sources like images and texts. In this field, multimodal text generation is a crucial aspect that involves processing data from multiple modalities and outputting text. The image-guided story ending generation (IgSEG) is a particularly significant task, targeting on an understanding of complex relationships between text and image data with a complete story text ending. Unfortunately, deep neural networks, which are the backbone of recent IgSEG models, are vulnerable to adversarial samples. Current adversarial attack methods mainly focus on single-modality data and do not analyze adversarial attacks for multimodal text generation tasks that use cross-modal information. To this end, we propose an iterative adversarial attack method (Iterative-attack) that fuses image and text modality attacks, allowing for an attack search for adversarial text and image in a more effective iterative way. Experimental results demonstrate that the proposed method outperforms existing single-modal and non-iterative multimodal attack methods, indicating the potential for improving the adversarial robustness of multimodal text generation models, such as multimodal machine translation, multimodal question answering, etc.
C1 [Wang, Youze; Hu, Wenbo; Hong, Richang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
C3 Hefei University of Technology
RP Hu, WB (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
EM wangyouze@mail.hfut.edu.cn; wenbohu@hfut.edu.cn; hongrc@hfut.edu.cn
RI Hu, Wenbo/JKH-5582-2023
FU National Key Research and Development Program of China
FX No Statement Available
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Boateng George, 2020, ICMI '20: Proceedings of the 2020 International Conference on Multimodal Interaction, P748, DOI 10.1145/3382507.3421154
   Cao H., 2021, arXiv
   Chen J, 2021, NEURAL COMPUT APPL, V33, P8669, DOI 10.1007/s00521-020-05616-w
   Cheng MH, 2020, AAAI CONF ARTIF INTE, V34, P3601
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Du X, 2022, IEEE T MULTIMEDIA, V24, P4381, DOI 10.1109/TMM.2021.3116426
   EBRAHIMI J., 2018, P 27 INT C COMPUTAT, P653
   Elliott Desmond, 2016, P 5 WORKSH VIS LANG, P70, DOI [10.18653/v1/W16- 3210, DOI 10.18653/V1/W16-3210, 10.18653/v1/w16-3210, 10.18653/v1/W16-3210]
   Gao J, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P50, DOI 10.1109/SPW.2018.00016
   Gao LL, 2022, IEEE T MULTIMEDIA, V24, P2329, DOI 10.1109/TMM.2021.3079723
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Huang QB, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P3114
   Jin D, 2020, AAAI CONF ARTIF INTE, V34, P8018
   Lee S., 2021, P INT C LEARN REPR, P1
   Li JF, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23138
   Li L, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6193
   Liang JW, 2019, IEEE T PATTERN ANAL, V41, P1893, DOI 10.1109/TPAMI.2018.2890628
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Liu S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11895, DOI 10.1109/ICCV48922.2021.01170
   Long Y, 2021, ELECTRON LETT, V57, P758, DOI 10.1049/ell2.12255
   Luong T., 2015, P 2015 C EMPIRICAL M, P1412, DOI DOI 10.18653/V1/D15-1166
   Madry A, 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.06083
   Michel P, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P3103
   Naseer M, 2020, PROC CVPR IEEE, P259, DOI 10.1109/CVPR42600.2020.00034
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Popovic Maja, 2015, P 10 WORKSH STAT MAC, P392, DOI 10.1080/1472586x.2015.1113070.
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1085
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Sadrizadeh S, 2023, Arxiv, DOI arXiv:2302.00944
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P86
   Shah M, 2019, PROC CVPR IEEE, P6642, DOI 10.1109/CVPR.2019.00681
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Singh H, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P5317
   Su YH, 2019, PROC CVPR IEEE, P10474, DOI 10.1109/CVPR.2019.01073
   Sun QF, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2854
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wallace E, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P5531
   Wang BX, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6134
   Wang S., 2021, arXiv
   Wang TS, 2023, IEEE T CIRC SYST VID, V33, P6159, DOI 10.1109/TCSVT.2023.3263054
   Wang YX, 2022, INFORM SCIENCES, V610, P14, DOI 10.1016/j.ins.2022.07.157
   Xu XJ, 2018, PROC CVPR IEEE, P4951, DOI 10.1109/CVPR.2018.00520
   Xue Dizhan, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P750, DOI 10.1145/3503161.3548022
   Yang K, 2021, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR46437.2021.00335
   Yang YF, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020): SYSTEM DEMONSTRATIONS, P87
   Yao SW, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4346
   Youze Wang, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P540, DOI 10.1145/3372278.3390713
   Yuan HJ, 2023, IEEE T MULTIMEDIA, V25, P203, DOI 10.1109/TMM.2021.3124083
   Zhang JM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5005, DOI 10.1145/3503161.3547801
   Zhang Z, 2021, IEEE T MULTIMEDIA, V24, P3392, DOI 10.1109/TMM.2021.3097506
   Zhou Ziqi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P6311, DOI 10.1145/3581783.3612454
   Zhu L, 2023, ACM T INFORM SYST, V41, DOI 10.1145/3559758
NR 62
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6117
EP 6130
DI 10.1109/TMM.2023.3345167
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600002
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wu, H
   Wang, M
   Zhou, WG
   Li, HQ
AF Wu, Hui
   Wang, Min
   Zhou, Wengang
   Li, Houqiang
TI Structure Similarity Preservation Learning for Asymmetric Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image retrieval; Training; Computational modeling;
   Adaptation models; Data models; Task analysis; Asymmetric image
   retrieval; multimedia search
ID QUANTIZATION; FEATURES
AB Asymmetric image retrieval is a task that seeks to balance retrieval accuracy and efficiency by leveraging lightweight and large models for the query and gallery sides, respectively. The key to asymmetric image retrieval is realizing feature compatibility between different models. Despite the great progress, most existing approaches either rely on classifiers inherited from gallery models or simply impose constraints at the instance level, ignoring the structure of embedding space. In this work, we propose a simple yet effective structure similarity preserving method to achieve feature compatibility between query and gallery models. Specifically, we first train a product quantizer offline with the image features embedded by the gallery model. The centroid vectors in the quantizer serve as anchor points in the embedding space of the gallery model to characterize its structure. During the training of the query model, anchor points are shared by the query and gallery models. The relationships between image features and centroid vectors are considered as structure similarities and constrained to be consistent. Moreover, our approach makes no assumption about the existence of any labeled training data and thus can be extended to an unlimited amount of data. Comprehensive experiments on large-scale landmark retrieval demonstrate the effectiveness of our approach.
C1 [Wu, Hui; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
   [Wang, Min] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230030, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.; Wang, M (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230030, Peoples R China.
EM wh241300@mail.ustc.edu.cn; wangmin@iai.ustc.edu.cn; zhwg@ustc.edu.cn;
   lihq@ustc.edu.cn
OI Wang, Min/0000-0003-3048-6980
FU National Natural Science Foundation of China
FX No Statement Available
CR Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bai Y, 2023, IEEE T MULTIMEDIA, V25, P7287, DOI 10.1109/TMM.2022.3219680
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bingyi Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P726, DOI 10.1007/978-3-030-58565-5_43
   Budnik M, 2021, PROC CVPR IEEE, P8224, DOI 10.1109/CVPR46437.2021.00813
   Chen YT, 2018, AAAI CONF ARTIF INTE, P2852
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Duggal R, 2021, PROC CVPR IEEE, P10718, DOI 10.1109/CVPR46437.2021.01058
   Fang YQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1644, DOI 10.1145/3240508.3240532
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   González-Díaz I, 2017, IEEE T MULTIMEDIA, V19, P544, DOI 10.1109/TMM.2016.2616298
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Iandola S., 2017, arXiv
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6073, DOI 10.1109/TNNLS.2018.2817538
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6323, DOI 10.1109/TNNLS.2018.2829867
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu P, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3418, DOI 10.1145/3474085.3475499
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Meng Qiang, 2021, P IEEECVF INT C COMP, P9939
   Ouyang JB, 2021, IEEE T MULTIMEDIA, V23, P3646, DOI 10.1109/TMM.2020.3029886
   Pang SM, 2019, IEEE T MULTIMEDIA, V21, P1513, DOI 10.1109/TMM.2018.2876833
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598
   Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521
   Romero A., 2015, ICLR, P1
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen YT, 2020, PROC CVPR IEEE, P6367, DOI 10.1109/CVPR42600.2020.00640
   Siméoni O, 2019, PROC CVPR IEEE, P11643, DOI 10.1109/CVPR.2019.01192
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tian J., 2021, P ACM INT C MULT, P3418
   Tian Y., 2019, P INT C LEARN REPR
   Tolias Giorgos, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P460, DOI 10.1007/978-3-030-58452-8_27
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Tolias Giorgos, 2015, ARXIV151105879
   Weyand T, 2020, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR42600.2020.00265
   Wu H, 2022, PROC CVPR IEEE, P9479, DOI 10.1109/CVPR52688.2022.00927
   Wu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11396, DOI 10.1109/ICCV48922.2021.01122
   Wu S, 2022, AAAI CONF ARTIF INTE, P2722
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Zagoruyko S., 2016, Wide residual networks, DOI DOI 10.5244/C.30.87
   Zhang B., 2023, Proc. AAAI Conf. Artif. Intell., V37, P3393, DOI [10.1609/aaai.v37i3.25447, DOI 10.1609/AAAI.V37I3.25447]
   Zhang B., 2022, IJCAI, P1615
   Zhang B., 2022, P INT C LEARN REPR
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zhou RW, 2020, IEEE T NEUR NET LEAR, V31, P1592, DOI 10.1109/TNNLS.2019.2920905
   Zhou W, 2010, P 18 ACM INT C MULTI, P511
NR 61
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4693
EP 4705
DI 10.1109/TMM.2023.3325728
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100060
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xie, YL
   Niu, JJ
   Zhang, Y
   Ren, F
AF Xie, Yulai
   Niu, Jingjing
   Zhang, Yang
   Ren, Fang
TI Global-Shared Text Representation Based Multi-Stage Fusion Transformer
   Network for Multi-Modal Dense Video Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anchor-free target detection; dense video captioning; global-shared
   text; multi-modal analysis; multi-stage fusion
AB Dense video captioning aims to detect all events of an uncropped video and generate corresponding textual captions for each event. Multi-modal information is essential to improve the performance of this task, but the existing methods mainly rely on the single visual or dual audio-visual modal input, while completely ignoring the text modal input (subtitle). Since the text data has a similar data representation as video caption words, it is conducive to the performance improvement of video captioning. In this article, we propose a novel framework, called the multi-stage fusion transformer network (MS-FTN), to realize multi-modal dense video captioning by fusing the text, the audio, and the visual features in stages. We present a multi-stage feature fusion encoder that first fuses audio and visual modalities at a lower level and then fuses them with a global-shared text representation at a higher level to generate a set of multi-modal complementary context features. In addition, an anchor-free event proposal module is proposed to efficiently generate a set of event proposals without the complex anchor calculation. Extensive experiments on the subsets of the ActivityNet Captions dataset show that our proposed MS-FTN achieves superior performance and efficient computation. Moreover, the ablation studies demonstrate that the global-shared text representation is more suitable for multi-modal dense video captioning.
C1 [Xie, Yulai; Zhang, Yang] Hitachi China Res Lab, Beijing 100190, Peoples R China.
   [Niu, Jingjing; Ren, Fang] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
C3 University of Science & Technology Beijing
RP Ren, F (corresponding author), Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
EM xieyl@hitachi.cn; niujingjing1970@163.com; zhangyang@hitachi.cn;
   renfang@ustb.edu.cn
OI Xie, Yulai/0000-0003-0764-6579
FU Fundamental Research Funds for the Central University, China
FX No Statement Available
CR Ahmed K, 2017, Arxiv, DOI arXiv:1711.02132
   Aytar Y, 2017, Arxiv, DOI arXiv:1706.00932
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Denkowski M., 2014, P WMT ACL, P376
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Duan Xuguang, 2018, P NEUR INF PROC SYST, P3059
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Ging Simon, 2020, ADV NEURAL INFORM PR, P22605, DOI DOI 10.18653/V1/P19-1641
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo NN, 2019, 2019 IEEE 4TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS AND MECHATRONICS (ICARM 2019), P839, DOI [10.1109/icarm.2019.8834066, 10.1109/ICARM.2019.8834066]
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Hao WL, 2018, AAAI CONF ARTIF INTE, P6894
   Hasan MK, 2021, AAAI CONF ARTIF INTE, V35, P12972
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hessel Jack, 2019, ACL Anthology, P419, DOI DOI 10.18653/V1/K19-1039
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hori C, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P430, DOI 10.1109/ASRU.2017.8268968
   Iashin V, 2020, Arxiv, DOI arXiv:2003.07758
   Iashin V, 2020, IEEE COMPUT SOC CONF, P4117, DOI 10.1109/CVPRW50498.2020.00487
   Jonghwan Mun, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6581, DOI 10.1109/CVPR.2019.00675
   Kingma D., 2014, C LEARNING REPRESENT
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   lashin V., 2020, P 31 BRIT MACH VIS C, P1
   Li W, 2018, PATTERN RECOGN LETT, V105, P23, DOI 10.1016/j.patrec.2017.10.012
   Li Z., 2022, P 29 INT C COMP LING, P7136
   Liu AA, 2018, IEEE ACCESS, V6, P68463, DOI 10.1109/ACCESS.2018.2879642
   Niu JJ, 2022, INT J PATTERN RECOGN, V36, DOI 10.1142/S021800142255014X
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park JS, 2019, PROC CVPR IEEE, P6591, DOI 10.1109/CVPR.2019.00676
   Paszke A, 2019, ADV NEUR IN, V32
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Do QT, 2017, INTERSPEECH, P2640, DOI 10.21437/Interspeech.2017-896
   Rahman T, 2019, IEEE I CONF COMP VIS, P8907, DOI 10.1109/ICCV.2019.00900
   Rohrbach A, 2014, LECT NOTES COMPUT SC, V8753, P184, DOI 10.1007/978-3-319-11752-2_15
   Shi BT, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6382
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tian YP, 2018, Arxiv, DOI arXiv:1812.02872
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2015, P 2015 C N AM CHAPT, P1494
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751
   Wang X., 2018, NAACL HLT
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wei Han, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P6, DOI 10.1145/3462244.3479919
   Wu Y, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P4730
   Xiao J., 2022, P IEEE AS PAC C IM P, P712, DOI [10.1109/IPEC54454.2022.9777333, DOI 10.1109/IPEC54454.2022.9777333]
   Xiong YL, 2018, Arxiv, DOI arXiv:1807.10018
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu LA, 2023, INFORM FUSION, V95, P306, DOI 10.1016/j.inffus.2023.02.028
NR 64
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3164
EP 3179
DI 10.1109/TMM.2023.3307972
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200015
DA 2024-08-05
ER

PT J
AU Xu, ZZ
   Chai, ZH
   Xu, CY
   Yuan, C
   Yang, HQ
AF Xu, Zhengzhuo
   Chai, Zenghao
   Xu, Chengyin
   Yuan, Chun
   Yang, Haiqin
TI Towards Effective Collaborative Learning in Long-Tailed Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tail; Federated learning; Task analysis; Uncertainty; Training; Head;
   Feature extraction; Image classification; long tail recognition;
   collaborative learning; knowledge distillation
AB Real-world data usually suffers from severe class imbalance and long-tailed distributions, where minority classes are significantly underrepresented compared to the majority ones. Recent research prefers to utilize multi-expert architectures to mitigate the model uncertainty on the minority, where collaborative learning is employed to aggregate the knowledge of experts, i.e., online distillation. In this article, we observe that the knowledge transfer between experts is imbalanced in terms of class distribution, which results in limited performance improvement of the minority classes. To address it, we propose a re-weighted distillation loss by comparing two classifiers' predictions, which are supervised by online distillation and label annotations, respectively. We also emphasize that feature-level distillation will significantly improve model performance and increase feature robustness. Finally, we propose an Effective Collaborative Learning (ECL) framework that integrates a contrastive proxy task branch to further improve feature quality. Quantitative and qualitative experiments on four standard datasets demonstrate that ECL achieves state-of-the-art performance and the detailed ablation studies manifest the effectiveness of each component in ECL.
C1 [Xu, Zhengzhuo; Yang, Haiqin] Int Digital Econ Acad, Shenzhen 518045, Peoples R China.
   [Xu, Zhengzhuo; Xu, Chengyin; Yuan, Chun] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Chai, Zenghao] Natl Univ Singapore, Singapore 119077, Singapore.
C3 International Digital Economy Academy; Tsinghua Shenzhen International
   Graduate School; Tsinghua University; National University of Singapore
RP Yang, HQ (corresponding author), Int Digital Econ Acad, Shenzhen 518045, Peoples R China.; Yuan, C (corresponding author), Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM xzzthu@gmail.com; zenghaochai@gmail.com; xucy20@mails.tsinghua.edu.cn;
   yuanc@sz.tsinghua.edu.cn; hqyang@ieee.org
OI Chai, Zenghao/0000-0003-3709-4947; Yang, Haiqin/0000-0001-5453-476X
FU National Key Ramp;D Program of China
FX No Statement Available
CR Alshammari S, 2022, PROC CVPR IEEE, P6887, DOI 10.1109/CVPR52688.2022.00677
   Ashukha A., 2020, PROC INT C LEARN REP
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cai JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P112, DOI 10.1109/ICCV48922.2021.00018
   Cao K., 2019, PROC ANN C NEURAL IN
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen YK, 2021, PROC CVPR IEEE, P9558, DOI 10.1109/CVPR46437.2021.00944
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Cui JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P695, DOI 10.1109/ICCV48922.2021.00075
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Ding MY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2918, DOI 10.1145/3474085.3475573
   Ding Y., 2020, P IEEECVF C COMPUTER, P4
   Dong MJ, 2023, IEEE T NEUR NET LEAR, V34, P10419, DOI 10.1109/TNNLS.2022.3166861
   Gao JX, 2023, IEEE T MULTIMEDIA, V25, P4764, DOI 10.1109/TMM.2022.3181789
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   Hao JY, 2022, IEEE T MULTIMEDIA, V24, P1164, DOI 10.1109/TMM.2021.3120642
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P235, DOI 10.1109/ICCV48922.2021.00030
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hong Y, 2021, PROC CVPR IEEE, P6622, DOI 10.1109/CVPR46437.2021.00656
   Huang T., 2022, Adv. Neural Inf. Process. Syst., V35, P33716, DOI DOI 10.48550/ARXIV.2205.10536
   Iscen A., 2021, arXiv
   Jamal M. A., 2020, P IEEE CVF C COMP VI, P7610, DOI DOI 10.1109/CVPR42600.2020.00763
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim J, 2020, P IEEE CVF C COMP VI, P13896, DOI DOI 10.1109/CVPR42600.2020.01391
   Krizhevsky A., 2009, Learning multiple layers of features from tiny images
   Kull M, 2019, ADV NEUR IN, V32
   Lee S, 2023, PROC CVPR IEEE, P704, DOI 10.1109/CVPR52729.2023.00075
   Li H, 2018, ADV NEUR IN, V31
   Li J, 2022, PROC CVPR IEEE, P6939, DOI 10.1109/CVPR52688.2022.00682
   Li MK, 2022, PROC CVPR IEEE, P6919, DOI 10.1109/CVPR52688.2022.00680
   Li TH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P610, DOI 10.1109/ICCV48922.2021.00067
   Li TH, 2022, PROC CVPR IEEE, P6908, DOI 10.1109/CVPR52688.2022.00679
   Li XH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3330, DOI 10.1145/3474085.3475487
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Liuyu Xiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P247, DOI 10.1007/978-3-030-58558-7_15
   Loshchilov I., 2017, INT C LEARNING REPRE
   Menon A. K, 2021, P INT C LEARN REPR
   Mukhoti J., 2020, Advances in Neural Information Processing Systems, P15288
   Niu Y., 2022, P ADV NEUR INF PROC, V35, P21933
   Parisot S, 2022, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR52688.2022.00681
   Park S, 2022, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR52688.2022.00676
   Peng Chu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P694, DOI 10.1007/978-3-030-58526-6_41
   Peng ZL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3275, DOI 10.1145/3474085.3475479
   Ren J., 2020, ADV NEURAL INFORM PR, V33, P4175, DOI DOI 10.48550/ARXIV.2007.10740
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Samuel D, 2021, Proceedings of the IEEE CVF International Conference on Computer Vision, P9495
   Sun JN, 2020, IEEE T MULTIMEDIA, V22, P2833, DOI 10.1109/TMM.2020.2966863
   Tang K., 2020, P NIPS, P1513
   Tao L., 2023, PROC INT C MACH LEAR
   Tao ZL, 2023, IEEE T MULTIMEDIA, V25, P5107, DOI 10.1109/TMM.2022.3187556
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Vapnik V, 2015, J MACH LEARN RES, V16, P2023
   Wang JF, 2021, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR46437.2021.00378
   Wang P, 2021, PROC CVPR IEEE, P943, DOI 10.1109/CVPR46437.2021.00100
   Wang PY, 2023, IEEE T MULTIMEDIA, V25, P4610, DOI 10.1109/TMM.2022.3179902
   Wang X., 2021, INT C LEARNING REPRE
   Wu JL, 2022, IEEE T MULTIMEDIA, V24, P3693, DOI 10.1109/TMM.2021.3106096
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang Y., 2020, Adv. Neural Inf. Process. Syst.
   Yu SH, 2022, PROC CVPR IEEE, P70, DOI 10.1109/CVPR52688.2022.00017
   Yuan Z., 2020, PROC INT C LEARN REP
   Zhang SY, 2021, PROC CVPR IEEE, P2361, DOI 10.1109/CVPR46437.2021.00239
   Zhang X, 2017, IEEE T MULTIMEDIA, V19, P2425, DOI 10.1109/TMM.2017.2701645
   Zhang Y., 2022, Neural Information Processing Sys- tems (NeurIPS), V35, P34077
   Zhang YS, 2021, AAAI CONF ARTIF INTE, V35, P3447
   Zhengzhuo X., 2021, ADV NEURAL INFORM PR, V34, P7139
   Zhong ZS, 2021, PROC CVPR IEEE, P16484, DOI 10.1109/CVPR46437.2021.01622
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu JG, 2022, PROC CVPR IEEE, P6898, DOI 10.1109/CVPR52688.2022.00678
NR 74
TC 0
Z9 0
U1 15
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3754
EP 3764
DI 10.1109/TMM.2023.3314980
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300015
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yang, X
   Wang, ZH
   Wei, ZY
   Yang, D
AF Yang, Xi
   Wang, Zihan
   Wei, Ziyu
   Yang, Dong
TI SCSP: An Unsupervised Image-to-Image Translation Network Based on
   Semantic Cooperative Shape Perception
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative adversarial networks; Feature extraction; Training;
   unsupervised image-to-image translation; semantic; shape perception
AB This article introduces a novel approach to unsupervised image-to-image translation, aiming to overcome the limitations of existing methods in accurately capturing the shape of the source domain and the style of the target domain. The proposed method, called Semantic Cooperative Shape Perception (SCSP), focuses on enhancing the quality of generated images by addressing two key aspects. Firstly, the SCSP model employs a fusion generator that divides the mapping process into a unique texture part and a shared semantic part. By using different network structures and constraints, each part learns specific information. The unique texture generator emphasizes the style and texture details of the target domain, while the shared semantic generator focuses on the semantic information present in the source domain. This separation enables the sub-generators to extract and restore different aspects of the target domain more effectively. Secondly, a shape perception loss is introduced to improve the similarity of semantic images. It enhances the shared semantic generator's ability to perceive semantic information related to the same object by imposing constraints on the semantic graph of both the generated and input images. Therefore, the proposed method ensures semantic consistency during the translation process, leading to improved authenticity and image quality. Experimental results on four datasets, including horse2zebra, tiger2leopard, summer2winter, and photo2vangogh, demonstrate that the SCSP model achieves state-of-the-art visualization results and favorable evaluation metrics.
C1 [Yang, Xi; Wang, Zihan] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Wei, Ziyu] Fourth Mil Med Univ, Dept Biomed Engn, Xian 710032, Peoples R China.
   [Wei, Ziyu] Shaanxi Key Lab Bioelectromagnet Detect & Intellig, Xian 710032, Peoples R China.
   [Yang, Dong] Xian Inst Space Radio Technol, Xian 710100, Peoples R China.
C3 Xidian University; Air Force Military Medical University
RP Yang, D (corresponding author), Xian Inst Space Radio Technol, Xian 710100, Peoples R China.
EM yangx@xidian.edu.cn; zihan_w@stu.xidian.edu.cn; ziyu_wei@fmmu.edu.cn;
   yangd504@126.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn N, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108649
   Behjati P, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.108997
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Chan YH, 2020, IEEE T IMAGE PROCESS, V29, P859, DOI 10.1109/TIP.2019.2936097
   Chen R., 2020, P IEEE CVF C COMP VI, P8165
   Chen XY, 2018, LECT NOTES COMPUT SC, V11206, P167, DOI 10.1007/978-3-030-01216-8_11
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding LJ, 2001, PATTERN RECOGN, V34, P721, DOI 10.1016/S0031-3203(00)00023-6
   Fan YC, 2020, AAAI CONF ARTIF INTE, V34, P10770
   Fang FM, 2021, IEEE T NEUR NET LEAR, V32, P3956, DOI 10.1109/TNNLS.2020.3016321
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P1505, DOI 10.1109/TMM.2023.3240880
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Han K, 2022, IEEE T IMAGE PROCESS, V31, P5629, DOI 10.1109/TIP.2022.3197976
   Hensel M, 2017, ADV NEUR IN, V30
   Hong ZW, 2020, AAAI CONF ARTIF INTE, V34, P4140
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huo J, 2022, IEEE T IMAGE PROCESS, V31, P3347, DOI 10.1109/TIP.2022.3154238
   Huo YK, 2019, IEEE T MED IMAGING, V38, P1016, DOI 10.1109/TMI.2018.2876633
   Ikuta M, 2023, IEEE T NEUR NET LEAR, V34, P10612, DOI 10.1109/TNNLS.2022.3169569
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia Z., 2021, P IEEE CVF INT C COM, P14273
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J, 2020, Arxiv, DOI [arXiv:1907.10830, DOI 10.48550/ARXIV.1907.10830]
   Kim S, 2022, PROC CVPR IEEE, P18300, DOI 10.1109/CVPR52688.2022.01778
   Ko K, 2023, NEURAL NETWORKS, V162, P330, DOI 10.1016/j.neunet.2023.02.042
   Kwon G, 2022, Arxiv, DOI arXiv:2209.15264
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li ZY, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108911
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mo M., 2019, INT C LEARN REPR
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Rui An, 2021, 2021 China Automation Congress (CAC), P2526, DOI 10.1109/CAC53003.2021.9727816
   Saharia C, 2022, P C P, P1
   Shi CL, 2020, PATTERN RECOGN LETT, V138, P520, DOI 10.1016/j.patrec.2020.08.021
   Tang H, 2023, IEEE T NEUR NET LEAR, V34, P1972, DOI 10.1109/TNNLS.2021.3105725
   Torbunov D, 2023, IEEE WINT CONF APPL, P702, DOI 10.1109/WACV56688.2023.00077
   Wang H, 2020, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR42600.2020.00193
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XH, 2022, IEEE T IMAGE PROCESS, V31, P6761, DOI 10.1109/TIP.2022.3215899
   Wu SX, 2023, IEEE T MULTIMEDIA, V25, P1111, DOI 10.1109/TMM.2021.3139209
   Xide Xia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P327, DOI 10.1007/978-3-030-58598-3_20
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Zhan F., 2022, P IEEE CVF C COMP VI, P18280
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang TY, 2020, IEEE T MED IMAGING, V39, P1149, DOI 10.1109/TMI.2019.2944488
   Zheng W., 2022, arXiv
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 54
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4950
EP 4960
DI 10.1109/TMM.2023.3328176
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800011
DA 2024-08-05
ER

PT J
AU Yeo, JH
   Kim, M
   Choi, J
   Kim, DH
   Ro, YM
AF Yeo, Jeong Hun
   Kim, Minsu
   Choi, Jeongsoo
   Kim, Dae Hoe
   Ro, Yong Man
TI AKVSR: Audio Knowledge Empowered Visual Speech Recognition by
   Compressing Audio Knowledge of a Pretrained Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio Knowledge via memory; audio knowledge quantization; audio
   empowered visual speech recognition; audio pretrained model; VSR
ID END
AB Visual Speech Recognition (VSR) is the task of predicting spoken words from silent lip movements. VSR is regarded as a challenging task because of the insufficient information on lip movements. In this article, we propose an Audio Knowledge empowered Visual Speech Recognition framework (AKVSR) to complement the insufficient speech information of visual modality by using audio modality. Different from the previous methods, the proposed AKVSR 1) utilizes rich audio knowledge encoded by a large-scale pretrained audio model, 2) saves the linguistic information of audio knowledge in compact audio memory by discarding the non-linguistic information from the audio through quantization, and 3) includes Audio Bridging Module which can find the best-matched audio features from the compact audio memory, which makes our training possible without audio inputs, once after the compact audio memory is composed. We validate the effectiveness of the proposed method through extensive experiments, and achieve new state-of-the-art performances on the widely-used LRS3 dataset.
C1 [Yeo, Jeong Hun; Kim, Minsu; Choi, Jeongsoo; Ro, Yong Man] Korea Adv Institue Sci & Technol KAIST, Sch Elect Engn, Image & Video Syst Lab, Daejeon 34141, South Korea.
   [Kim, Dae Hoe] Elect & Telecommun Res Inst ETRI, Visual Intelligence Res Sect, Superintelligence Creat Res Lab, Daejeon 34129, South Korea.
C3 Electronics & Telecommunications Research Institute - Korea (ETRI)
RP Ro, YM (corresponding author), Korea Adv Institue Sci & Technol KAIST, Sch Elect Engn, Image & Video Syst Lab, Daejeon 34141, South Korea.
EM sedne246@kaist.ac.kr; ms.k@kaist.ac.kr; jeongsoo.choi@kaist.ac.kr;
   dhkim19@etri.re.kr; ymro@kaist.ac.kr
RI ; Ro, Yong Man/ABF-6817-2020
OI Kim, Minsu/0000-0002-6514-0018; Ro, Yong Man/0000-0001-5306-6853
FU IITP
FX No Statement Available
CR Afouras T, 2018, ARXIV
   Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2020, INT CONF ACOUST SPEE, P2143, DOI [10.1109/icassp40776.2020.9054253, 10.1109/ICASSP40776.2020.9054253]
   Akbari H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2516, DOI 10.1109/ICASSP.2018.8461856
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Bo Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14421, DOI 10.1109/CVPR42600.2020.01444
   Chang X., 2023, P INTERSPEECH, P1399
   Chibelushi CC, 2002, IEEE T MULTIMEDIA, V4, P23, DOI 10.1109/6046.985551
   Cho K., 2014, EMNLP, DOI 10.3115/v1/w14-4012
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dou ZY, 2022, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52688.2022.01763
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Elashmawy S, 2021, Arxiv, DOI arXiv:2108.03543
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Furlanello T, 2018, PR MACH LEARN RES, V80
   Graves A, 2006, ICML, P369, DOI DOI 10.1145/1143844.1143891
   Haliassos A., 2023, PROC 11 INT C LEARN
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hong J, 2022, INTERSPEECH, P2838, DOI 10.21437/Interspeech.2022-11311
   Hsu WN, 2021, IEEE-ACM T AUDIO SPE, V29, P3451, DOI 10.1109/TASLP.2021.3122291
   Huang X, 2018, PROC CVPR IEEE, P8837, DOI 10.1109/CVPR.2018.00921
   Ivanko D, 2022, EUR SIGNAL PR CONF, P1131
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kim M, 2022, LECT NOTES COMPUT SC, V13696, P576, DOI 10.1007/978-3-031-20059-5_33
   Kim M, 2022, AAAI CONF ARTIF INTE, P1174
   Kim M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P296, DOI 10.1109/ICCV48922.2021.00036
   Kim M, 2022, IEEE T MULTIMEDIA, V24, P4342, DOI 10.1109/TMM.2021.3115626
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Koumparoulis A, 2022, INT CONF ACOUST SPEE, P8467, DOI 10.1109/ICASSP43922.2022.9747729
   Kudo T, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P66
   Lakhotia K, 2021, T ASSOC COMPUT LING, V9, P1336, DOI 10.1162/tacl_a_00430
   Lee A, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P3327
   Lee JS, 2008, IEEE T MULTIMEDIA, V10, P767, DOI 10.1109/TMM.2008.922789
   Lee S, 2021, PROC CVPR IEEE, P3053, DOI 10.1109/CVPR46437.2021.00307
   Lin KY, 2020, AAAI CONF ARTIF INTE, V34, P11515
   Liu XB, 2023, PROC CVPR IEEE, P18806, DOI 10.1109/CVPR52729.2023.01803
   Lohrenz T, 2023, IEEE IJCNN, DOI 10.1109/IJCNN54540.2023.10191643
   Assael YM, 2016, Arxiv, DOI arXiv:1611.01599
   Ma PC, 2022, NAT MACH INTELL, V4, P930, DOI 10.1038/s42256-022-00550-z
   Ma PC, 2021, INTERSPEECH, P3011, DOI 10.21437/Interspeech.2021-1360
   Ma PC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7608, DOI 10.1109/ICASSP39728.2021.9415063
   Ma PC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7613, DOI 10.1109/ICASSP39728.2021.9414567
   Mabrouk H, 2022, Arxiv, DOI arXiv:2207.05692
   Makino T, 2019, 2019 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU 2019), P905, DOI [10.1109/asru46091.2019.9004036, 10.1109/ASRU46091.2019.9004036]
   Martinez B, 2020, INT CONF ACOUST SPEE, P6319, DOI [10.1109/icassp40776.2020.9053841, 10.1109/ICASSP40776.2020.9053841]
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Pasad A., 2023, PROC IEEE INT C ACOU, P1
   Peláez-Moreno C, 2001, IEEE T MULTIMEDIA, V3, P209, DOI 10.1109/6046.923820
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Prajwal KR, 2022, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR52688.2022.00510
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SC, 2021, PROC CVPR IEEE, P13320, DOI 10.1109/CVPR46437.2021.01312
   Sadeghi M, 2020, IEEE-ACM T AUDIO SPE, V28, P1788, DOI 10.1109/TASLP.2020.3000593
   Saitoh T, 2017, LECT NOTES COMPUT SC, V10117, P277, DOI 10.1007/978-3-319-54427-4_21
   SATALOFF RT, 1992, SCI AM, V267, P108, DOI 10.1038/scientificamerican1292-108
   Serdyuk D, 2022, INTERSPEECH, P2833, DOI 10.21437/Interspeech.2022-10920
   Sheng CC, 2023, IEEE T MULTIMEDIA, V25, P6563, DOI 10.1109/TMM.2022.3210761
   Sheng CC, 2021, IEEE T MULTIMEDIA, V24, P3545, DOI 10.1109/TMM.2021.3102433
   Shi B., 2022, PROC INT C LEARN REP
   Shillingford B, 2018, Arxiv, DOI arXiv:1807.05162
   Stafylakis T, 2017, INTERSPEECH, P3652, DOI 10.21437/Interspeech.2017-85
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van den Oord A, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Yingzhi, 2021, arXiv
   Watanabe S, 2017, IEEE J-STSP, V11, P1240, DOI 10.1109/JSTSP.2017.2763455
   Weng XS, 2019, Arxiv, DOI arXiv:1905.02540
   Weston J, 2015, Arxiv, DOI arXiv:1410.3916
   Xiao JY, 2020, IEEE INT CONF AUTOMA, P364, DOI 10.1109/FG47880.2020.00132
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yeo J. H., 2023, PROC IEEE INT C ACOU, P1
   Zhang J.-X., 2023, PROC IEEE INT C ACOU, P1
   Zhang SY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4373, DOI 10.1145/3394171.3413518
   Zhang XX, 2019, IEEE I CONF COMP VIS, P713, DOI 10.1109/ICCV.2019.00080
   Zhang YH, 2020, IEEE INT CONF AUTOMA, P356, DOI 10.1109/FG47880.2020.00134
   Zhao J, 2022, IEEE J-STSP, V16, P1227, DOI 10.1109/JSTSP.2022.3184480
   Zhao X, 2020, IEEE INT CONF AUTOMA, P420, DOI 10.1109/FG47880.2020.00133
   Zhao Y, 2020, AAAI CONF ARTIF INTE, V34, P6917
   Zhao YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10140, DOI 10.1109/ICCV48922.2021.01000
   Zheng GL, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P2765
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
NR 91
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6462
EP 6474
DI 10.1109/TMM.2024.3352388
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600028
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhou, CY
   Liu, DK
   Wang, TL
   Tian, JM
   Cao, JW
AF Zhou, Chunyi
   Liu, Dekang
   Wang, Tianlei
   Tian, Jiangmin
   Cao, Jiuwen
TI M<SUP>3</SUP>ANet: Multi-Modal and Multi-Attention Fusion Network for
   Ship License Plate Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ship license plate recognition (SLPR); text recognition; attention;
   language modeling; multi-modal and multi-attention fusion network
   (M(3)ANet)
ID NEURAL-NETWORK; TEXT
AB Shiplicense plate recognition (SLPR) plays an important role in intelligent waterway management, but few attention has been paid to SLPR in scene text recognition (STR) community. Inspired by various outstanding achievements on STR, combined the intrinsic properties of SLPR, we propose a Multi-Modal and Multi-Attention dynamic fusion network (M(3)ANet) for SLPR in this article. Specifically, the visual-language joint modeling for SLPR is developed and the channel-spatial-self attention dynamic fusion mechanism is proposed for accuracy boosting. Explicitly fusing linguistic information extracted from ship name related corpus improves the adaptability of the recognition model to occlusion, background confusion, blur, etc., which is integrated with vision features to establish a multi-modal recognition network. Gated fully fusion is utilized to fuse visual features re-weighted by multi-attention components, inducing flexible compatibility with multiple types of decoders and more refined recognition decoder inputs. Additionally, to comprehensively mine spatially salient text regions in ship license plate images, we investigate the grouped spatial attention. Extensive experiments empirically demonstrate the effectiveness of M(3)ANet and superior performance (93.80% with regular images, while 90.34% with irregular images) on two benchmarks.
C1 [Zhou, Chunyi; Liu, Dekang; Wang, Tianlei; Tian, Jiangmin; Cao, Jiuwen] Hangzhou Dianzi Univ, Machine Learning & I Hlth Int Cooperat Base Zhejia, Hangzhou 310018, Zhejiang, Peoples R China.
   [Zhou, Chunyi; Liu, Dekang; Wang, Tianlei; Tian, Jiangmin; Cao, Jiuwen] Hangzhou Dianzi Univ, Artificial Intelligence Inst, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University
RP Cao, JW (corresponding author), Hangzhou Dianzi Univ, Machine Learning & I Hlth Int Cooperat Base Zhejia, Hangzhou 310018, Zhejiang, Peoples R China.; Cao, JW (corresponding author), Hangzhou Dianzi Univ, Artificial Intelligence Inst, Hangzhou 310018, Zhejiang, Peoples R China.
EM cyzhoucn@163.com; tnak_liu@163.com; tianlei.wang.cn@gmail.com;
   jmtian@hdu.edu.cn; jwcao@hdu.edu.cn
OI Wang, Tianlei/0000-0002-4498-4326
FU Natural Science Key Foundation of Zhejiang Province
FX No Statement Available
CR Baek J, 2019, IEEE I CONF COMP VIS, P4714, DOI 10.1109/ICCV.2019.00481
   Bautista D, 2022, LECT NOTES COMPUT SC, V13688, P178, DOI 10.1007/978-3-031-19815-1_11
   Borisyuk F, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P71, DOI 10.1145/3219819.3219861
   Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543
   Dai XY, 2021, PROC CVPR IEEE, P7369, DOI 10.1109/CVPR46437.2021.00729
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Ding XY, 2023, IEEE T MULTIMEDIA, V25, P9071, DOI 10.1109/TMM.2023.3245404
   Du Y., 2022, P 31 INT JOINT C ART, P884
   Esrar H, 2023, BENCHMARKING, V30, P2458, DOI 10.1108/BIJ-11-2021-0716
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jin ZX, 2023, IEEE T MULTIMEDIA, V25, P1, DOI 10.1109/TMM.2021.3120194
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Li AN, 2020, NEUROCOMPUTING, V411, P416, DOI 10.1016/j.neucom.2020.06.021
   Li M, 2023, IEEE T MULTIMEDIA, V25, P649, DOI 10.1109/TMM.2021.3129651
   Li XT, 2020, AAAI CONF ARTIF INTE, V34, P11418
   Liu BL, 2017, PROC INT C TOOLS ART, P506, DOI 10.1109/ICTAI.2017.00083
   Liu BL, 2017, IEEE INTEL TRANSP SY, V9, P102, DOI 10.1109/MITS.2017.2743168
   Liu DK, 2022, IEEE T INTELL TRANSP, V23, P23831, DOI 10.1109/TITS.2022.3196814
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Lu N, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107980
   Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020
   Na B, 2022, LECT NOTES COMPUT SC, V13688, P446, DOI 10.1007/978-3-031-19815-1_26
   Qiao Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2046, DOI 10.1145/3474085.3475238
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang YX, 2022, IEEE T IMAGE PROCESS, V31, P5585, DOI 10.1109/TIP.2022.3197981
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HH, 2023, IEEE SIGNAL PROC LET, V30, P394, DOI 10.1109/LSP.2023.3262418
   Xu B., 2015, Empirical Evaluation of Rectified Activations in Convolutional Network, DOI DOI 10.48550/ARXIV.1505.00853
   Xu BQ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3247103
   Xu Binqian, 2023, arXiv
   Xu ZB, 2018, LECT NOTES COMPUT SC, V11217, P261, DOI 10.1007/978-3-030-01261-8_16
   Yan RJ, 2021, PROC CVPR IEEE, P284, DOI 10.1109/CVPR46437.2021.00035
   Yu F., 2016, ICLR, P1
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang XQ, 2022, IEEE T CIRC SYST VID, V32, P1986, DOI 10.1109/TCSVT.2021.3093928
   Zheng TL, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P1777
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
NR 43
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5976
EP 5986
DI 10.1109/TMM.2023.3342694
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100019
DA 2024-08-05
ER

PT J
AU Zhou, W
   Jiang, WT
   Chen, DH
   Hu, HF
   Su, T
AF Zhou, Wei
   Jiang, Weitao
   Chen, Dihu
   Hu, Haifeng
   Su, Tao
TI Mining Semantic Information With Dual Relation Graph Network for
   Multi-Label Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Semantics; Task analysis; Convolution; Transformers;
   Feature extraction; Convolutional neural networks; Multi-label image
   classification; graph convolution network; spatial relation; channel
   relation
ID ATTENTION
AB The purpose of multi-label image classification is to assign multiple labels for multiple objects presented in one image. Recent research efforts exploit graph convolution network (GCN) to learn the label co-occurrence dependencies for enhancing the semantic representation. Although these methods have achieved promising results, they can not capture the intrinsic correlation between objects in images and do not consider the inter-channel relationship. In addition, the previous methods treat each single image independently and fail to explore the relationship between different images. To address the above challenges, we propose a novel Dual Relation Graph Network (DRGN) model, which adopts a double branch structure to excavate rich semantic information from intra-image and cross-image simultaneously. Specifically, we first develop an intra-image channel-relation mining (ICM) module to mine the inter-channel relationship in features while learning the importance of different channels. Secondly, we design a new GCN-based intra-image spatial-relation exploring (ISE) module to capture the correlation between objects in individual image. Notably, ISE module and ICM module can complement and promote each other from the spatial and channel dimensions of images to improve the correlation between objects in individual image. Thirdly, we propose a novel GCN-based cross-image semantic learning (CSL) module to learn the semantic relationship between different images in the mini-batch. Through graph reasoning, our CSL module can iteratively refine input image features by acquiring common semantic information from other images in the mini-batch. Extensive experiments on the MS-COCO 2014, PASCAL VOC 2007, and VG-500 datasets demonstrate that the proposed DRGN model outperforms current state-of-the-art methods.
C1 [Zhou, Wei; Jiang, Weitao; Chen, Dihu; Hu, Haifeng; Su, Tao] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
EM zhouw75@mail2.sysu.edu.cn; jiangwt5@mail2.sysu.edu.cn;
   stscdh@mail.sysu.edu.cn; huhaif@mail.sysu.edu.cn; sutao@mail.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X; chen, dihu/0000-0001-5432-8149
FU National Natural Science Foundation of China
FX No Statement Available
CR An HR, 2021, IEEE T MULTIMEDIA, V23, P268, DOI 10.1109/TMM.2020.2975417
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cheng X., 2022, P IEEE INT C MULT EX, P1
   Dong XZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2615, DOI 10.1145/3474085.3475439
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fujii K, 2021, IEEE T MULTIMEDIA, V23, P3892, DOI 10.1109/TMM.2020.3033125
   Gao BB, 2021, IEEE T IMAGE PROCESS, V30, P5920, DOI 10.1109/TIP.2021.3088605
   Gao ZL, 2019, PROC CVPR IEEE, P3019, DOI 10.1109/CVPR.2019.00314
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hassanin M, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2022.103448
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou Z, 2022, PROC CVPR IEEE, P7246, DOI 10.1109/CVPR52688.2022.00711
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jin JR, 2016, INT C PATT RECOG, P2452, DOI 10.1109/ICPR.2016.7900004
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lanchantin J, 2021, PROC CVPR IEEE, P16473, DOI 10.1109/CVPR46437.2021.01621
   Li JB, 2020, LECT NOTES COMPUT SC, V12396, P736, DOI 10.1007/978-3-030-61609-0_58
   Li Q, 2019, Arxiv, DOI arXiv:1909.13005
   Liang J, 2022, NEUROCOMPUTING, V491, P14, DOI 10.1016/j.neucom.2022.03.057
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SL, 2021, Arxiv, DOI [arXiv:2107.10834, DOI 10.48550/ARXIV.2107.10834]
   Liu WD, 2023, IEEE T MULTIMEDIA, V25, P1148, DOI 10.1109/TMM.2021.3139459
   Liu Y, 2022, MULTIMED TOOLS APPL, V81, P18305, DOI 10.1007/s11042-022-12096-8
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Meng Q., 2019, P ACM MULT AS, P1
   Mondal A, 2021, Arxiv, DOI arXiv:2105.03237
   Patel K, 2022, PATTERN RECOGN LETT, V153, P176, DOI 10.1016/j.patrec.2021.12.004
   Pu T, 2023, NEUROCOMPUTING, V526, P121, DOI 10.1016/j.neucom.2023.01.018
   Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wang X., 2021, P IEEE INT C MULT EX, P1
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YT, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1575, DOI 10.1145/3340531.3411880
   Wang Z, 2022, IEEE T CIRC SYST VID, V32, P1848, DOI 10.1109/TCSVT.2021.3083978
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wen SP, 2021, IEEE T SYST MAN CY-S, V51, P7250, DOI 10.1109/TSMC.2020.2967071
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu XP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P284, DOI 10.1145/3394171.3414046
   Yan Z, 2019, IEEE ACCESS, V7, P98005, DOI 10.1109/ACCESS.2019.2929512
   Yu WJ, 2019, PATTERN RECOGN, V91, P322, DOI 10.1016/j.patcog.2019.03.006
   Yue KY, 2018, ADV NEUR IN, V31
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang K., 2020, PROC IEEECVF C COMP, P9047
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhao HY, 2020, IEEE ACCESS, V8, P225539, DOI 10.1109/ACCESS.2020.3044446
   Zhao JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P163, DOI 10.1109/ICCV48922.2021.00023
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zhou FT, 2022, IEEE T CIRC SYST VID, V32, P4513, DOI 10.1109/TCSVT.2021.3128054
   Zhou FT, 2021, AAAI CONF ARTIF INTE, V35, P3572
   Zhou W, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3519030
   Zhou W, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109203
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zhu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P184, DOI 10.1109/ICCV48922.2021.00025
   Zhu XL, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3598, DOI 10.1145/3503161.3548343
NR 70
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1143
EP 1157
DI 10.1109/TMM.2023.3277279
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700036
DA 2024-08-05
ER

PT J
AU Cai, X
   Shi, QJ
   Gao, YB
   Li, S
   Hua, W
   Xie, T
AF Cai, Xun
   Shi, Qingjie
   Gao, Yanbo
   Li, Shuai
   Hua, Wei
   Xie, Tian
TI A Structure-Preserving and Illumination-Consistent Cycle Framework for
   Image Harmonization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image harmonization; generative networks; illumination-consistent;
   structure-preserving
ID STYLE TRANSFER; TRANSLATION
AB Ina composite image, the foreground and background are filmed under different scenarios, such as different lighting conditions, causing inconsistency and reducing the overall realism of the image. Image harmonization aims to generate visually realistic composite images by adjusting the foreground to the background conditions while maintaining the structure. Existing methods focus on adjusting the foreground object by directly training the foreground generation network with the ground truth, neglecting the different roles of the illumination and structure of the foreground in image harmonization. Moreover, the use of background, except for providing illumination, is not thoroughly investigated in this task. In this paper, we propose a structure-preserving and illumination-consistent cycle (SP-IC cycle) framework for image harmonization by exploring the illumination and structure of both the foreground and background. It achieves image harmonization by specifically changing the illumination and keeping the structure instead of ambiguously changing the foreground. Then, an illumination-consistent foreground harmonization cycle is developed to change the foreground illumination, while a structure-preserving cycle is designed to keep the foreground structure. Background information is explored in both cycles to assist in decomposing the illumination and structure of the foreground. In addition, the proposed SP-IC cycle framework can be applied to any image harmonization method to further boost its performance. Experimental results demonstrate that our method achieves better harmonious image quality than state-of-the-art methods, especially on an illumination-varying dataset.
C1 [Cai, Xun; Shi, Qingjie; Gao, Yanbo] Shandong Univ, Sch Software, Jinan 250100, Peoples R China.
   [Cai, Xun; Shi, Qingjie; Gao, Yanbo] Shandong Univ, WeiHai Res Inst Ind Technol, Weihai 264209, Peoples R China.
   [Li, Shuai] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Hua, Wei; Xie, Tian] Zhejiang Lab, Res Inst Interdisciplinary Innovat, Hangzhou 311121, Peoples R China.
C3 Shandong University; Shandong University; Shandong University; Zhejiang
   Laboratory
RP Gao, YB (corresponding author), Shandong Univ, Sch Software, Jinan 250100, Peoples R China.
EM caixunzh@sdu.edu.cn; 202015205@sdu.edu.cn; ybgao@sdu.edu.cn;
   shuaili@sdu.edu.cn; huawei@zhejianglab.com; rickyskyxie@zhejianglab.com
OI Li, Shuai/0000-0002-9938-0917
FU National Key Ramp;D Program of China
FX No Statement Available
CR ALEXANDER KR, 1976, PERCEPT PSYCHOPHYS, V19, P72, DOI 10.3758/BF03199388
   An S, 2019, IEEE T MULTIMEDIA, V21, P3095, DOI 10.1109/TMM.2019.2918720
   Cao CJ, 2022, LECT NOTES COMPUT SC, V13675, P306, DOI 10.1007/978-3-031-19784-0_18
   Cao J., 2022, P BRIT MACH VIS C, P1
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Cong W., 2021, P IEEE INT C MULT EX, P1
   Cong WY, 2020, Arxiv, DOI arXiv:1911.13239
   Cong WY, 2022, PROC CVPR IEEE, P18449, DOI 10.1109/CVPR52688.2022.01792
   Cun XD, 2020, IEEE T IMAGE PROCESS, V29, P4759, DOI 10.1109/TIP.2020.2975979
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fan XT, 2020, IEEE T MULTIMEDIA, V22, P655, DOI 10.1109/TMM.2019.2932573
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo ZH, 2023, IEEE T PATTERN ANAL, V45, P12960, DOI 10.1109/TPAMI.2022.3207091
   Guo ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14850, DOI 10.1109/ICCV48922.2021.01460
   Guo ZH, 2021, PROC CVPR IEEE, P16362, DOI 10.1109/CVPR46437.2021.01610
   Han JL, 2022, LECT NOTES COMPUT SC, V13678, P218, DOI 10.1007/978-3-031-19797-0_13
   Hao G., 2020, BMVC
   Hensel M, 2017, ADV NEUR IN, V30
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang AP, 2020, IEEE T BIG DATA, V8, P1129, DOI 10.1109/TBDATA.2020.3009983
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lei JJ, 2021, IEEE T CIRC SYST VID, V31, P2686, DOI 10.1109/TCSVT.2020.3027616
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ling J, 2021, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR46437.2021.00924
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu SG, 2022, IEEE T MULTIMEDIA, V24, P1299, DOI 10.1109/TMM.2021.3063605
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Miyato T., 2018, INT C LEARN REPR ICL, P1
   Mun H, 2022, IEEE T MULTIMEDIA, V24, P3823, DOI 10.1109/TMM.2021.3108401
   Niu L, 2016, LECT NOTES COMPUT SC, V9910, P550, DOI 10.1007/978-3-319-46466-4_33
   Niu YZ, 2020, IEEE T CIRC SYST VID, V30, P697, DOI 10.1109/TCSVT.2019.2897123
   Oza P, 2024, IEEE T PATTERN ANAL, V46, P4018, DOI 10.1109/TPAMI.2022.3217046
   Pan ZQ, 2020, IEEE T EM TOP COMP I, V4, P500, DOI 10.1109/TETCI.2020.2991774
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Peng B, 2022, IEEE T CIRC SYST VID, V32, P8342, DOI 10.1109/TCSVT.2022.3190916
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ren C, 2021, PROC CVPR IEEE, P8592, DOI 10.1109/CVPR46437.2021.00849
   Sofiiuk K, 2021, IEEE WINT CONF APPL, P1619, DOI 10.1109/WACV48630.2021.00166
   Sun MJ, 2023, IEEE T MULTIMEDIA, V25, P3354, DOI 10.1109/TMM.2022.3159403
   Sunkavalli K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778862
   Tian JY, 2022, IEEE T CIRC SYST VID, V32, P3749, DOI 10.1109/TCSVT.2021.3111034
   Tian XY, 2022, IEEE T CIRC SYST VID, V32, P4804, DOI 10.1109/TCSVT.2021.3121987
   Tsai YH, 2017, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2017.299
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang Y, 2022, LECT NOTES COMPUT SC, V13675, P271, DOI 10.1007/978-3-031-19784-0_16
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wenyan Cong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8391, DOI 10.1109/CVPR42600.2020.00842
   Xue S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185580
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang BF, 2022, PATTERN RECOGN, V128, DOI 10.1016/j.patcog.2022.108663
   Zhang BF, 2022, IEEE T PATTERN ANAL, V44, P8082, DOI 10.1109/TPAMI.2021.3083269
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang WC, 2021, IEEE T IMAGE PROCESS, V30, P3293, DOI 10.1109/TIP.2021.3052083
   Zhang YB, 2022, PROC CVPR IEEE, P8025, DOI 10.1109/CVPR52688.2022.00787
   Zheng ZT, 2023, IEEE T MULTIMEDIA, V25, P6000, DOI 10.1109/TMM.2022.3203220
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 67
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 51
EP 64
DI 10.1109/TMM.2023.3260620
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500001
DA 2024-08-05
ER

PT J
AU Cong, RM
   Xiong, H
   Chen, JP
   Zhang, W
   Huang, QM
   Zhao, Y
AF Cong, Runmin
   Xiong, Hang
   Chen, Jinpeng
   Zhang, Wei
   Huang, Qingming
   Zhao, Yao
TI Query-Guided Prototype Evolution Network for Few-Shot Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot segmentation; few-shot learning; semantic segmentation;
   prototype generation
AB Previous Few-Shot Segmentation (FSS) approaches exclusively utilize support features for prototype generation, neglecting the specific requirements of the query. To address this, we present the Query-guided Prototype Evolution Network (QPENet), a new method that integrates query features into the generation process of foreground and background prototypes, thereby yielding customized prototypes attuned to specific queries. The evolution of the foreground prototype is accomplished through a support-query-support iterative process involving two new modules: Pseudo-prototype Generation (PPG) and Dual Prototype Evolution (DPE). The PPG module employs support features to create an initial prototype for the preliminary segmentation of the query image, resulting in a pseudo-prototype reflecting the unique needs of the current query. Subsequently, the DPE module performs reverse segmentation on support images using this pseudo-prototype, leading to the generation of evolved prototypes, which can be considered as custom solutions. As for the background prototype, the evolution begins with a global background prototype that represents the generalized features of all training images. We also design a Global Background Cleansing (GBC) module to eliminate potential adverse components mirroring the characteristics of the current foreground class. Experimental results on the PASCAL-5(i) and COCO-20(i) datasets attest to the substantial enhancements achieved by QPENet over prevailing state-of-the-art techniques, underscoring the validity of our ideas.
C1 [Cong, Runmin; Xiong, Hang; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Cong, Runmin] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Cong, Runmin] Minist Educ, Key Lab Machine Intelligence & Syst Control, Jinan 250061, Peoples R China.
   [Xiong, Hang; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Chen, Jinpeng; Huang, Qingming] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Zhang, Wei] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Zhang, Wei] Minist Educ, Key Lab Machine Intelligence & Syst Control, Jinan 250061, Peoples R China.
C3 Beijing Jiaotong University; Shandong University; City University of
   Hong Kong; Shandong University
RP Chen, JP (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM rmcong@sdu.edu.cn; xionghang@bjtu.edu.cn; jinpechen2-c@my.cityu.edu.hk;
   davidzhang@sdu.edu.cn; qmhuang@ucas.ac.cn; yzhao@bjtu.edu.cn
RI CHEN, Jinpeng/ABH-7707-2022
OI CHEN, Jinpeng/0000-0002-0469-4463; Zhao, Yao/0000-0002-8581-9554; ,
   xionghang/0009-0001-1777-6724
FU National Key Ramp;D Program of China
FX No Statement Available
CR Allen KR, 2019, PR MACH LEARN RES, V97
   Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Chen JP, 2024, IEEE T CYBERNETICS, V54, P3392, DOI 10.1109/TCYB.2023.3326165
   Chen L.C., 2014, ARXIV PREPRINT ARXIV, V6, P357, DOI DOI 10.48550/ARXIV.1412.7062
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P968, DOI 10.1109/TMM.2021.3061816
   Chen ZT, 2019, AAAI CONF ARTIF INTE, P3379
   Cheng B, 2021, ADV NEUR IN, V34
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Cong RM, 2023, IEEE T MULTIMEDIA, V25, P6971, DOI 10.1109/TMM.2022.3216476
   Cong RM, 2024, IEEE T NEUR NET LEAR, V35, P9495, DOI 10.1109/TNNLS.2022.3233883
   Cong RM, 2022, IEEE T CONSUM ELECTR, V68, P376, DOI 10.1109/TCE.2022.3205376
   Cong RM, 2023, IEEE T CIRC SYST VID, V33, P534, DOI 10.1109/TCSVT.2022.3205182
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Cong RM, 2023, IEEE T CYBERNETICS, V53, P1920, DOI 10.1109/TCYB.2022.3169431
   Dong E. P., 2018, BRIT MACH VIS C, P1
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan Q, 2022, LECT NOTES COMPUT SC, V13679, P701, DOI 10.1007/978-3-031-19800-7_41
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gordon J, 2019, PROC INT C LEARNREPR
   Grant C., 2018, INT CONFLEARN REPRES, V59
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou RB, 2019, ADV NEUR IN, V32
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Koch G., 2015, ICML DEEP LEARN WORK, V2, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lang C., 2022, INT JOINT C ART INT, P1024
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu J, 2022, PROC CVPR IEEE, P11543, DOI 10.1109/CVPR52688.2022.01126
   Liu N., 2022, Advances in neural information processing systems, V35, P38020
   Liu Y., 2020, COMPUTER VISION ECCV, P142, DOI DOI 10.1007/978-3-030-58545-79
   Liu YW, 2022, PROC CVPR IEEE, P11563, DOI 10.1109/CVPR52688.2022.01128
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mao BJ, 2022, AAAI CONF ARTIF INTE, P1953
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Paszke A, 2019, ADV NEUR IN, V32
   Ravi S., 2017, INT C LEARN REPR
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shaban Amirreza, 2017, BMVC, DOI 10.5244/C.31.167
   Snell J, 2017, ADV NEUR IN, V30
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Xie GS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7273, DOI 10.1109/ICCV48922.2021.00720
   Xie GS, 2021, PROC CVPR IEEE, P5471, DOI 10.1109/CVPR46437.2021.00543
   Yang LH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8701, DOI 10.1109/ICCV48922.2021.00860
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Yin CX, 2022, IEEE T MULTIMEDIA, V24, P4183, DOI 10.1109/TMM.2021.3114541
   Zhan C, 2020, IEEE T MULTIMEDIA, V22, P795, DOI 10.1109/TMM.2019.2931441
   Zhang BF, 2021, PROC CVPR IEEE, P8308, DOI 10.1109/CVPR46437.2021.00821
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang GW, 2021, ADV NEUR IN, V34
   Zhang L, 2020, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR42600.2020.00378
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang XL, 2022, IEEE T NEUR NET LEAR, V33, P6484, DOI 10.1109/TNNLS.2021.3081693
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhao GZ, 2023, CAAI T INTELL TECHNO, V8, P297, DOI 10.1049/cit2.12118
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng XT, 2022, CAAI T INTELL TECHNO, V7, P419, DOI 10.1049/cit2.12068
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 78
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6501
EP 6512
DI 10.1109/TMM.2024.3352921
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Jia, TY
   Li, JF
   Zhuo, L
   Yu, TJ
AF Jia, Tongyao
   Li, Jiafeng
   Zhuo, Li
   Yu, Tianjian
TI Semi-Supervised Single-Image Dehazing Network via Disentangled
   Meta-Knowledge
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Atmospheric modeling; Training; Metalearning; Image reconstruction; Task
   analysis; Prediction algorithms; Synthetic data; Disentangled
   representations; meta-learning; semi-supervised learning; single-image
   dehazing
ID FACE RECOGNITION; VISION; FRAMEWORK
AB Captured outdoor scene images are easily affected by haze. Most image dehazing methods have limited generalization capabilities for real-world hazy images owing to the complexities of real-world environments and domain gaps in the training datasets. This article proposes a semi-supervised single-image dehazing network based on disentangled meta-knowledge. The symmetric and heterogeneous design of the disentangled network is conducive to the separation of the content and mask features of hazy images and these features are used as meta-knowledge to guide feature fusion in the dehazing network. Moreover, functions describing constant-color and disentangled-reconstruction-checking losses are designed to ensure the subjective qualities of the generated dehazed images. The results of extensive experiments conducted on synthetic datasets and real-world images indicate that the proposed algorithm outperforms state-of-the-art single-image dehazing algorithms. In addition, the algorithm effectively improves the performance of object-detection tasks.
C1 [Jia, Tongyao; Li, Jiafeng; Zhuo, Li] Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing, Peoples R China.
   [Yu, Tianjian] Cent South Univ, Sch Traff & Transportat Engn, Changsha 410017, Hunan, Peoples R China.
C3 Beijing University of Technology; Central South University
RP Li, JF; Zhuo, L (corresponding author), Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing, Peoples R China.
EM jty@emails.bjut.edu.cn; lijiafeng@bjut.edu.cn; zhuoli@bjut.edu.cn;
   yutianjian@csu.edu.cn
RI li, jiafeng/KVY-4468-2024
OI Li, Jiafeng/0000-0001-6976-7275
FU Beijing Natural Science Foundation
FX No Statement Available
CR Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710
   Chi ZX, 2021, PROC CVPR IEEE, P9133, DOI 10.1109/CVPR46437.2021.00902
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Ge SM, 2020, IEEE T CIRC SYST VID, V30, P3387, DOI 10.1109/TCSVT.2020.2967754
   Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P6898, DOI 10.1109/TIP.2020.2995049
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Ge SM, 2017, PROC CVPR IEEE, P426, DOI 10.1109/CVPR.2017.53
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hong M, 2020, PROC CVPR IEEE, P3459, DOI 10.1109/CVPR42600.2020.00352
   Hu HM, 2020, IEEE T MULTIMEDIA, V22, P1485, DOI 10.1109/TMM.2019.2944260
   Huang G. B., 2008, WORKSHOP FACESREAL L, P1
   Jia TY, 2022, IEEE T IND INFORM, V18, P1511, DOI 10.1109/TII.2021.3059020
   Jiang HR, 2021, IEEE INT CONF COMP V, P1503, DOI 10.1109/ICCVW54120.2021.00175
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Li JF, 2020, IEEE T IND INFORM, V16, P4344, DOI 10.1109/TII.2019.2936467
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Lin CY, 2023, IEEE T MULTIMEDIA, V25, P3089, DOI 10.1109/TMM.2022.3155937
   Lu BY, 2019, PROC CVPR IEEE, P10217, DOI 10.1109/CVPR.2019.01047
   Nada H, 2018, INT CONF BIOMETR THE
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Rodriguez P, 2013, J ELECTR COMPUT ENG, V2013, DOI 10.1155/2013/217021
   Schechner YY, 2003, APPL OPTICS, V42, P511, DOI 10.1364/AO.42.000511
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Shen LH, 2019, IEEE T MULTIMEDIA, V21, P1093, DOI 10.1109/TMM.2018.2871955
   Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Tu Z., 2022, IEEE C COMPUT VIS PA, P5769
   Wenchao Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14471, DOI 10.1109/CVPR42600.2020.01449
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Ye Liu, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P50, DOI 10.1145/3474085.3475331
   Ye YT, 2021, PROC CVPR IEEE, P2053, DOI 10.1109/CVPR46437.2021.00209
   Zeng D, 2019, INFORM SCIENCES, V495, P136, DOI 10.1016/j.ins.2019.01.083
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang XY, 2020, INT J COMPUT VISION, V128, P1699, DOI 10.1007/s11263-019-01285-y
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhu HY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1234
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zou Zhengxia, 2020, P IEEE CVF C COMP VI, P12806
NR 55
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2634
EP 2647
DI 10.1109/TMM.2023.3301273
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400023
DA 2024-08-05
ER

PT J
AU Li, YX
   Cao, WC
   Xie, W
   Li, JL
   Benetos, E
AF Li, Yanxiong
   Cao, Wenchang
   Xie, Wei
   Li, Jialong
   Benetos, Emmanouil
TI Few-Shot Class-Incremental Audio Classification Using Dynamically
   Expanded Classifier With Self-Attention Modified Prototypes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio classification; few-shot learning; incremental learning; modified
   prototype; self-attention mechanism
AB Most existing methods for audio classification assume that the vocabulary of audio classes to be classified is fixed. When novel (unseen) audio classes appear, audio classification systems need to be retrained with abundant labeled samples of all audio classes for recognizing base (initial) and novel audio classes. If novel audio classes continue to appear, the existing methods for audio classification will be inefficient and even infeasible. In this work, we propose a method for few-shot class-incremental audio classification, which can continually recognize novel audio classes without forgetting old ones. The framework of our method mainly consists of two parts: an embedding extractor and a classifier, and their constructions are decoupled. The embedding extractor is the backbone of a ResNet based network, which is frozen after construction by a training strategy using only samples of base audio classes. However, the classifier consisting of prototypes is expanded by a prototype adaptation network with few samples of novel audio classes in incremental sessions. Labeled support samples and unlabeled query samples are used to train the prototype adaptation network and update the classifier, since they are informative for audio classification. Three audio datasets, named NSynth-100, FSC-89 and LS-100 are built by choosing samples from audio corpora of NSynth, FSD-MIX-CLIP and LibriSpeech, respectively. Results show that our method exceeds baseline methods in average accuracy and performance dropping rate. In addition, it is competitive compared to baseline methods in computational complexity and memory requirement.
C1 [Li, Yanxiong; Cao, Wenchang; Xie, Wei; Li, Jialong] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
   [Benetos, Emmanouil] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
C3 South China University of Technology; University of London; Queen Mary
   University London
RP Li, YX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
EM eeyxli@scut.edu.cn; wenchangcao98@163.com; chester.w.xie@gmail.com;
   lijialongjy@163.com; emmanouil.benetos@qmul.ac.uk
RI Chen, Xiao/KBD-1464-2024; li, lan/KCJ-5061-2024; Han,
   Liang/KFR-6745-2024; zhao, sheng/JWO-6127-2024; Li,
   Jialong/ISP-2969-2023; Chen, Zheng/KCY-2338-2024; Liu,
   Xiaohan/KBB-4246-2024; Benetos, Emmanouil/S-1932-2018
OI Li, Jialong/0000-0003-3416-5551; Liu, Xiaohan/0009-0009-5291-2494;
   Benetos, Emmanouil/0000-0002-6820-6764
FU National Natural Science Foundation of China
FX No Statement Available
CR AMARI S, 1993, NEUROCOMPUTING, V5, P185, DOI 10.1016/0925-2312(93)90006-O
   Ayub A, 2020, IEEE COMPUT SOC CONF, P897, DOI 10.1109/CVPRW50498.2020.00119
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Barchiesi D, 2015, IEEE SIGNAL PROC MAG, V32, P16, DOI 10.1109/MSP.2014.2326181
   Chen Hung-Jen, 2020, P INT C NEUR INF PRO, V33, P17466
   Cheraghian A, 2021, PROC CVPR IEEE, P2534, DOI 10.1109/CVPR46437.2021.00256
   Choi K, 2022, INT CONF ACOUST SPEE, P486, DOI 10.1109/ICASSP43922.2022.9747908
   Chou SY, 2019, INT CONF ACOUST SPEE, P26, DOI [10.1109/ICASSP.2019.8682558, 10.1109/icassp.2019.8682558]
   Dhar P, 2019, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR.2019.00528
   Drossos K, 2020, INT CONF ACOUST SPEE, P736, DOI [10.1109/icassp40776.2020.9052990, 10.1109/ICASSP40776.2020.9052990]
   Engel J, 2017, PR MACH LEARN RES, V70
   Finn C, 2017, PR MACH LEARN RES, V70
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu W., 2019, P ICLR, P1
   Hussain MA, 2023, IEEE MULTIMEDIA, V30, P84, DOI 10.1109/MMUL.2022.3208923
   Iqbal T, 2020, INT CONF ACOUST SPEE, P636, DOI [10.1109/ICASSP40776.2020.9054444, 10.1109/icassp40776.2020.9054444]
   Karam S, 2022, IEEE CONSUM ELECTR M, V11, P101, DOI 10.1109/MCE.2022.3145724
   Kingma D. P., 2014, arXiv
   Koh E, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102859
   Kvsn RR, 2020, IEEE ACCESS, V8, P57684, DOI 10.1109/ACCESS.2020.2978547
   Li Y, 2022, P IEEE MMSP, P1
   Li YX, 2022, APPL SOFT COMPUT, V126, DOI 10.1016/j.asoc.2022.109291
   Li YX, 2020, INT CONF ACOUST SPEE, P286, DOI [10.1109/icassp40776.2020.9054433, 10.1109/ICASSP40776.2020.9054433]
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1385, DOI 10.1109/TMM.2019.2947199
   Li YX, 2019, MULTIMED TOOLS APPL, V78, P33999, DOI 10.1007/s11042-019-07991-6
   Li YX, 2018, IEEE ACCESS, V6, P58043, DOI 10.1109/ACCESS.2018.2872931
   Liu YY, 2021, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR46437.2021.00257
   Liu Y, 2023, IEEE T NEUR NET LEAR, V34, P7529, DOI 10.1109/TNNLS.2022.3144183
   Ma D, 2022, INT CONF ACOUST SPEE, P4173, DOI 10.1109/ICASSP43922.2022.9746862
   Mazumder P., 2021, AAAI
   Mohaimenuzzaman M, 2022, IEEE ACCESS, V10, P6696, DOI 10.1109/ACCESS.2022.3140807
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Politis A, 2021, IEEE-ACM T AUDIO SPE, V29, P684, DOI 10.1109/TASLP.2020.3047233
   Pons J, 2019, INT CONF ACOUST SPEE, P16, DOI 10.1109/ICASSP.2019.8682591
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Riemer M, 2019, ICLR, P1
   Shi X, 2022, IEEE-ACM T AUDIO SPE, V30, P367, DOI 10.1109/TASLP.2022.3140549
   Shin H, 2017, ADV NEUR IN, V30
   Snell J., 2017, Advances in Neural Information Processing Systems, V30, P4077
   Terenzi A, 2022, IEEE-ACM T AUDIO SPE, V30, P112, DOI 10.1109/TASLP.2021.3133194
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang Y, 2022, INT CONF ACOUST SPEE, P651, DOI 10.1109/ICASSP43922.2022.9746118
   Wang Y, 2021, IEEE WORK APPL SIG, P36, DOI 10.1109/WASPAA52581.2021.9632677
   Wang Y, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P321, DOI 10.1109/ICASSP39728.2021.9413584
   Wang Y, 2020, INT CONF ACOUST SPEE, P81, DOI [10.1109/icassp40776.2020.9054708, 10.1109/ICASSP40776.2020.9054708]
   Wang ZP, 2019, IEEE WORK APPL SIG, P308, DOI [10.1109/waspaa.2019.8937236, 10.1109/WASPAA.2019.8937236]
   Xiaoyu Tao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12180, DOI 10.1109/CVPR42600.2020.01220
   Xie H, 2021, IEEE-ACM T AUDIO SPE, V29, P1233, DOI 10.1109/TASLP.2021.3065234
   Xie W., 2023, P INT, P1
   Yang BY, 2023, IEEE T PATTERN ANAL, V45, P2945, DOI 10.1109/TPAMI.2022.3175849
   Yang DC, 2022, INT CONF ACOUST SPEE, P811, DOI 10.1109/ICASSP43922.2022.9746042
   Yu L, 2020, PROC CVPR IEEE, P6980, DOI 10.1109/CVPR42600.2020.00701
   Zeinali H., 2019, P VOXCELEB CHALL WOR, P1
   Zeng YF, 2021, IEEE INT WORKSH MULT, DOI 10.1109/MMSP53017.2021.9733646
   Zhang C, 2021, PROC CVPR IEEE, P12450, DOI 10.1109/CVPR46437.2021.01227
   Zhang SL, 2019, INTERSPEECH, P3649, DOI 10.21437/Interspeech.2019-1532
NR 61
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1346
EP 1360
DI 10.1109/TMM.2023.3280011
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700056
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, Z
   Cao, LL
   Wang, HB
   Xu, LH
AF Li, Zhuang
   Cao, Leilei
   Wang, Hongbin
   Xu, Lihong
TI End-to-End Instance-Level Human Parsing by Segmenting Persons
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Instance-level human parsing; semantic segment- ation; instance
   segmentation; end-to-end training
AB Instance-level human parsing is aimed at separately partitioning the human body into different semantic parts for each individual, which remains a challenging task due to human appearance/pose variation, occlusion and complex backgrounds. Most state-of-the-art methods follow the "parsing-by-detection" paradigm, which relies on a trained detector to localize persons and then sequentially performs single-person parsing for each person. However, this paradigm is closely related to the detector, and the runtime is proportional to the number of persons in an image. In this paper, we present a novel detection-free framework for instance-level human parsing in an end-to-end manner. We decompose instance-level human parsing into two subtasks via a unified network: 1) semantic segmentation for pixel-level classification as a human part and 2) instance segmentation for mask-level classification as a person. The framework can directly predict the human-part semantic mask for all persons and binary masks for instance-level persons in parallel. The parsing result of each person can be acquired via a Hadamard product between the human-part semantic mask and the corresponding person's binary mask. Extensive experiments demonstrate that our proposed method performs favorably against state-of-the-art methods on the CIHP and MHP v2 datasets.
C1 [Li, Zhuang; Xu, Lihong] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Cao, Leilei] Northwestern Polytech Univ, Sch Software, Xian 710129, Peoples R China.
   [Wang, Hongbin] Ant Grp, Hangzhou 310013, Peoples R China.
   [Xu, Lihong] Michigan State Univ, BEACON Ctr Study Evolut Act, E Lansing, MI 48824 USA.
C3 Tongji University; Northwestern Polytechnical University; Michigan State
   University
RP Xu, LH (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
EM liz@tongji.edu.cn; mcaoleilei@sina.com; hongbin.whb@antgroup.com;
   xulhk@163.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Carion N., 2020, EUR C COMP VIS, P213
   Cheng B, 2021, ADV NEUR IN, V34
   Dosovitskiy A., 2021, INT C LEARN REPRESEN, P1
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gong K, 2019, PROC CVPR IEEE, P7442, DOI [10.1109/CVPR.2019.00763, 10.1109/cvpr.2019.00763]
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   He H., 2020, PROC AAAI C ARTIF IN
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZL, 2021, Arxiv, DOI arXiv:2106.03650
   Li JS, 2018, Arxiv, DOI arXiv:1705.07206
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Liu XC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P338, DOI 10.1145/3343031.3350857
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P421, DOI 10.1007/978-3-030-58610-2_25
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Qin H., 2019, PROC BRIT MACH VIS C, P1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Ruyi Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P205, DOI 10.1007/978-3-030-58601-0_13
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644
   Yang L, 2023, IEEE T MULTIMEDIA, V25, P7128, DOI 10.1109/TMM.2022.3217413
   Yang L, 2019, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.2019.00045
   Zeng D., 2021, P IEEECVF INT C COMP, P11385
   Zhang SY, 2022, IEEE T IMAGE PROCESS, V31, P5599, DOI 10.1109/TIP.2022.3192989
   Zhang XM, 2023, IEEE T MULTIMEDIA, V25, P2601, DOI 10.1109/TMM.2022.3148595
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou TF, 2021, PROC CVPR IEEE, P1622, DOI 10.1109/CVPR46437.2021.00167
   Ziwei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8897, DOI 10.1109/CVPR42600.2020.00892
NR 37
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 41
EP 50
DI 10.1109/TMM.2023.3260631
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500020
DA 2024-08-05
ER

PT J
AU Liao, JQ
   Li, L
   Liu, D
   Li, HQ
AF Liao, Junqi
   Li, Li
   Liu, Dong
   Li, Houqiang
TI Content-Adaptive Rate-Distortion Modeling for Frame-Level Rate Control
   in Versatile Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit rate; Encoding; Adaptation models; Video coding; Resource
   management; Rate-distortion; Standards; versatile video coding; rate
   control; rate-distortion model
ID OPTIMAL BIT ALLOCATION; OPTIMIZATION; ALGORITHM; STANDARD; QUALITY
AB Rate control (RC) plays an essential role in video coding. RC algorithms based on more accurate rate-distortion (R-D) models often achieve higher control precision and better R-D performance. The most widely used R-D model for HEVC and VVC is the hyperbolic R-D model, which is equivalent to a linear relationship between $\ln {R}$ and $\ln {D}$. Due to its high accuracy, few studies have attempted to improve the accuracy of the hyperbolic R-D model further. Intuitively, we consider that the accuracy of the hyperbolic R-D model could be further improved by increasing the order of the R-D model. However, this may also increase the number of model parameters to be estimated, which may not benefit the one-pass RC precision. In this paper, we first explicitly note that there is a trade-off between the order of the R-D model and the difficulty in estimating the model parameters in one-pass RC. Then, motivated by the trade-off, we propose high-order R-D models and the corresponding one-pass frame-level RC algorithms for video coding. Finally, we introduce the quadratic R-D model into frame-level RC in the VVC Test Model (VTM-19.2) and provide a content-adaptive model selection between the first-order and second-order R-D models. Experimental results show that the proposed frame-level RC algorithm based on the quadratic R-D model reduces the average frame-level bitrate error by 5.30%, 3.11%, and 13.45% and achieves 0.49%, 0.65%, and 0.35% BD-Rate savings under Low Delay B (LDB), Low Delay P (LDP), and Random Access (RA) configurations, respectively, when compared to the default RC algorithm used in VTM-19.2.
C1 [Liao, Junqi; Li, Li; Liu, Dong; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Li, L (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM liaojq@mail.ustc.edu.cn; lil1@ustc.edu.cn; dongeliu@ustc.edu.cn;
   lihq@ustc.edu.cn
RI Liu, Dong/K-7488-2012
OI Liu, Dong/0000-0001-9100-2906; Liao, Junqi/0009-0006-4030-174X
FU Natural Science Foundation of China
FX No Statement Available
CR Ardestani Majid R., 2010, 2010 17th International Conference on Telecommunications (ICT 2010), P923, DOI 10.1109/ICTEL.2010.5478827
   Berger L. D., 1975, Proc Adv. Source Coding, P1
   Bjontegaard G., 2001, Tech. Rep.VCEG-M33
   Bossen F., 2019, JointVideo Experts Team ITU-T SG, V16, P19
   BRANDENBURG K, 1994, J AUDIO ENG SOC, V42, P780
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Chen J, 2023, IEEE T MULTIMEDIA, V25, P7930, DOI 10.1109/TMM.2022.3232020
   Chen S. Wang, 2023, Inf.Sci., V645
   Chen ZZ, 2019, IEEE T IMAGE PROCESS, V28, P4541, DOI 10.1109/TIP.2019.2911180
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   Choi H, 2013, IEEE J-STSP, V7, P1112, DOI 10.1109/JSTSP.2013.2272241
   Cover T.M., 1999, ELEMENTS INFORM THEO
   De-Luxán-Hernández S, 2019, IEEE IMAGE PROC, P1203, DOI [10.1109/ICIP.2019.8803777, 10.1109/icip.2019.8803777]
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Hachicha W, 2015, IEEE T MULTIMEDIA, V17, P765, DOI 10.1109/TMM.2015.2417099
   Huang YW, 2021, IEEE T CIRC SYST VID, V31, P3818, DOI 10.1109/TCSVT.2021.3088134
   Kamaci N, 2005, IEEE T CIRC SYST VID, V15, P994, DOI 10.1109/TCSVT.2005.852400
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li L, 2023, IEEE T MULTIMEDIA, V25, P3855, DOI 10.1109/TMM.2022.3167810
   Li L, 2018, IEEE T CIRC SYST VID, V28, P130, DOI 10.1109/TCSVT.2016.2598672
   Li SX, 2017, IEEE T CIRC SYST VID, V27, P2409, DOI 10.1109/TCSVT.2016.2589878
   Li YM, 2020, IEEE IMAGE PROC, P1176, DOI 10.1109/ICIP40778.2020.9191125
   Liu FY, 2021, IEEE T IMAGE PROCESS, V30, P4706, DOI 10.1109/TIP.2021.3072225
   M. P. E. Group, 1993, MPEG Video Test Model, V5, P1
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Mallat S, 1998, IEEE T SIGNAL PROCES, V46, P1027, DOI 10.1109/78.668554
   Mao YH, 2022, IEEE T CIRC SYST VID, V32, P2371, DOI 10.1109/TCSVT.2021.3093315
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Wang Jing, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME51207.2021.9428259
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yilmaz MA, 2023, IEEE IMAGE PROC, P2475, DOI 10.1109/ICIP49359.2023.10223112
   Yuan H, 2015, IEEE T MULTIMEDIA, V17, P2134, DOI 10.1109/TMM.2015.2477682
   Zhang K, 2019, IEEE T IMAGE PROCESS, V28, P1456, DOI 10.1109/TIP.2018.2877355
   Zhang L, 2019, IEEE DATA COMPR CONF, P43, DOI 10.1109/DCC.2019.00012
   Zhao L, 2019, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2019.00013
   Zhao X, 2018, IEEE T IMAGE PROCESS, V27, P2514, DOI 10.1109/TIP.2018.2802202
   Zhihai Y. Kim, 2001, IEEE Trans. Circuits Syst. VideoTechnol., V11, P813
   Zhou ML, 2021, IEEE T MULTIMEDIA, V23, P1106, DOI 10.1109/TMM.2020.2992968
   Zhou ML, 2019, IEEE T MULTIMEDIA, V21, P1921, DOI 10.1109/TMM.2019.2895281
NR 42
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6864
EP 6879
DI 10.1109/TMM.2024.3358063
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600025
DA 2024-08-05
ER

PT J
AU Liu, HF
   Sheng, MM
   Sun, ZR
   Yao, YZ
   Hua, XS
   Shen, HT
AF Liu, Huafeng
   Sheng, Mengmeng
   Sun, Zeren
   Yao, Yazhou
   Hua, Xian-Sheng
   Shen, Heng-Tao
TI Learning With Imbalanced Noisy Data by Preventing Bias in Sample
   Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Training; Tail; Predictive models; Data models; Sun;
   Self-supervised learning; Imbalanced label noise; class-balance-based
   sample selection; confidence-based sample augmentation; consistency
   regularization; average confidence margin
ID SET
AB Learning with noisy labels has gained increasing attention because the inevitable imperfect labels in real-world scenarios can substantially hurt the deep model performance. Recent studies tend to regard low-loss samples as clean ones and discard high-loss ones to alleviate the negative impact of noisy labels. However, real-world datasets contain not only noisy labels but also class imbalance. The imbalance issue is prone to causing failure in the loss-based sample selection since the under-learning of tail classes also leans to produce high losses. To this end, we propose a simple yet effective method to address noisy labels in imbalanced datasets. Specifically, we propose <bold>C</bold>lass-<bold>B</bold>alance-based sample <bold>S</bold>election (<bold>CBS</bold>) to prevent the tail class samples from being neglected during training. We propose <bold>C</bold>onfidence-based <bold>S</bold>ample <bold>A</bold>ugmentation (<bold>CSA</bold>) for the chosen clean samples to enhance their reliability in the training process. To exploit selected noisy samples, we resort to prediction history to rectify labels of noisy samples. Moreover, we introduce the <bold>A</bold>verage <bold>C</bold>onfidence <bold>M</bold>argin (ACM) metric to measure the quality of corrected labels by leveraging the model's evolving training dynamics, thereby ensuring that low-quality corrected noisy samples are appropriately masked out. Lastly, consistency regularization is imposed on filtered label-corrected noisy samples to boost model performance. Comprehensive experimental results on synthetic and real-world datasets demonstrate the effectiveness and superiority of our proposed method, especially in imbalanced scenarios.
C1 [Liu, Huafeng; Sheng, Mengmeng; Sun, Zeren; Yao, Yazhou] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Hua, Xian-Sheng] Terminus Grp, Beijing 100027, Peoples R China.
   [Shen, Heng-Tao] Tongji Univ, Sch Elect & Informat Engn, Shanghai 201804, Peoples R China.
C3 Nanjing University of Science & Technology; Tongji University
RP Sun, ZR; Yao, YZ (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM liu.hua.feng@njust.edu.cn; shengmengmemg@njust.edu.cn;
   zerens@njust.edu.cn; yazhou.yao@njust.edu.cn; huaxiansheng@gmail.com;
   shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021
OI Hua, Xian-Sheng/0000-0002-8232-5049; Sheng,
   Mengmeng/0000-0002-2011-8597; Yao, Yazhou/0000-0002-0337-9410
FU National Natural Science Foundation of China
FX No Statement Available
CR Arazo E, 2019, PR MACH LEARN RES, V97
   Bai Y., 2021, P INT C ADV NEUR INF, V34, P24392
   Berthelot D, 2019, ADV NEUR IN, V32
   Boutros F, 2022, IEEE COMPUT SOC CONF, P1577, DOI 10.1109/CVPRW56347.2022.00164
   Chen H., 2023, PROC INT C LEARN REP, P1
   Chen MC, 2023, AAAI CONF ARTIF INTE, P14765
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Chen T, 2023, IEEE T IMAGE PROCESS, V32, P2960, DOI 10.1109/TIP.2023.3275913
   Cheng D, 2022, PROC CVPR IEEE, P16609, DOI 10.1109/CVPR52688.2022.01613
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fergus R, 2010, P IEEE, V98, P1453, DOI 10.1109/JPROC.2010.2048990
   Goldberger J., 2017, PROC INT C LEARN REP
   Gong C, 2023, IEEE T PATTERN ANAL, V45, P2835, DOI 10.1109/TPAMI.2022.3178690
   Gui X., 2021, IJCAI, P2469
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hendrycks D, 2019, ADV NEUR IN, V32
   Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374
   Huang L., 2020, ADV NEURAL INF PROCE, P19365
   Huang YS, 2022, AAAI CONF ARTIF INTE, P6960
   Huang ZZ, 2023, PROC CVPR IEEE, P11661, DOI 10.1109/CVPR52729.2023.01122
   Jiang SW, 2022, AAAI CONF ARTIF INTE, P7024
   Karim N, 2022, PROC CVPR IEEE, P9666, DOI 10.1109/CVPR52688.2022.00945
   Kim Y, 2021, PROC CVPR IEEE, P9437, DOI 10.1109/CVPR46437.2021.00932
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li J., 2020, PROC INT C LEARN REP
   Li JC, 2022, LECT NOTES COMPUT SC, V13684, P128, DOI 10.1007/978-3-031-20053-3_8
   Li SK, 2022, PROC CVPR IEEE, P316, DOI 10.1109/CVPR52688.2022.00041
   Liu HF, 2022, IEEE T MULTIMEDIA, V24, P546, DOI 10.1109/TMM.2021.3055024
   Liu S, 2020, P NEUIPS DEC 6 12 VI
   Liu ST, 2022, PR MACH LEARN RES
   Liu YP, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2568
   Lu Yangdi, 2022, P INT JOINT C ART IN, P3278, DOI [10.24963/ijcai. 2022/455, DOI 10.24963/IJCAI.2022/455]
   Malach E, 2017, ADV NEUR IN, V30
   Malfa E. L., 2021, PROC INT JOINT C ART, P2658
   Mandal D, 2020, IEEE WINT CONF APPL, P1370, DOI 10.1109/WACV45572.2020.9093342
   Patel D, 2023, IEEE WINT CONF APPL, P3921, DOI 10.1109/WACV56688.2023.00392
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Qian JJ, 2023, IEEE T MULTIMEDIA, V25, P1427, DOI 10.1109/TMM.2022.3230331
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren MY, 2018, PR MACH LEARN RES, V80
   Shao ZR, 2023, IEEE T MULTIMEDIA, V25, P112, DOI 10.1109/TMM.2021.3121571
   Shi XS, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109080
   Shu J, 2019, ADV NEUR IN, V32
   Sosea T, 2023, PROC CVPR IEEE, P15773, DOI 10.1109/CVPR52729.2023.01514
   Sun ZR, 2023, IEEE T MULTIMEDIA, V25, P3284, DOI 10.1109/TMM.2022.3158001
   Sun ZR, 2022, PROC CVPR IEEE, P5301, DOI 10.1109/CVPR52688.2022.00524
   Sun ZR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P92, DOI 10.1145/3394171.3413978
   Sun ZR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10582, DOI 10.1109/ICCV48922.2021.01043
   Sun ZR, 2022, IEEE T MULTIMEDIA, V24, P1093, DOI 10.1109/TMM.2021.3116430
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   WELINDER P., 2010, Advances in neural information processing systems, P2424
   Wu T, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P544
   Xia X., 2022, PROC INT C LEARN REP, P1
   Xia X., 2020, PROC INT C LEARN REP, P1
   Xia XB, 2023, IEEE I CONF COMP VIS, P1833, DOI 10.1109/ICCV51070.2023.00176
   Xia XB, 2019, ADV NEUR IN, V32
   Xia XB, 2023, IEEE T PATTERN ANAL, V45, P3047, DOI 10.1109/TPAMI.2022.3180545
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xiaojiang Peng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P786, DOI 10.1007/978-3-030-58517-4_46
   Yang EK, 2022, PROC CVPR IEEE, P7541, DOI 10.1109/CVPR52688.2022.00740
   Yao YZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1735, DOI 10.1145/3394171.3413851
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Ying Hui, 2021, INT JOINT C ARTIFICI, P1266
   Yu XR, 2019, PR MACH LEARN RES, V97
   Zhang C., 2017, PROC 5 INT C LEARN R, P1
   Zhang CY, 2023, IEEE T MULTIMEDIA, V25, P4691, DOI 10.1109/TMM.2022.3181439
   Zhang CY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4063, DOI 10.1145/3474085.3475536
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P352, DOI 10.1109/TMM.2021.3126430
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zheltonozhskii E, 2022, IEEE WINT CONF APPL, P387, DOI 10.1109/WACV51458.2022.00046
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhou X, 2023, IEEE T PATTERN ANAL, V45, P8094, DOI 10.1109/TPAMI.2023.3236459
   Zhou X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P72, DOI 10.1109/ICCV48922.2021.00014
NR 78
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7426
EP 7437
DI 10.1109/TMM.2024.3368910
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000056
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Luo, Y
   Kang, GL
   Liu, KX
   Zhuang, FZ
   Lü, JH
AF Luo, Ying
   Kang, Guoliang
   Liu, Kexin
   Zhuang, Fuzhen
   Lu, Jinhu
TI Taking a Closer Look at Factor Disentanglement: Dual-Path Variational
   Autoencoder Learning for Domain Generalization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain generalization (DG); cross-domain dis- entanglement; variational
   autoencoder (VAE); deep neural networks
AB Domain generalization (DG) aims to train a model with access to a limited number of source domains for generalizing it across various unseen target domains. The key to solving the DG problem is disentangling domain-invariant features (i.e., semantic factors) from domain-specific features (i.e., variation factors) to facilitate generalizable representation learning. Previous studies either implicitly model the semantic and variation factors or ineffectively constrain the disentangling process, thus rendering the disentanglement incomplete and ineffective. In this study, we propose a novel approach, named DualVAE, to explicitly model and disentangle both the semantic and variation factors. DualVAE is based on the variational autoencoder (VAE) architecture. However, it differs from the conventional VAE in that it consists of two paths, which explicitly model the semantic and variation factors. In addition to the reconstruction loss of VAE and the classification loss, three types of regularizations, namely statistical independence regularization, factorized prior regularization, and prediction consistency regularization, are proposed to further facilitate the disentanglement of factors. Experimental results on representative DG benchmarks show that our method performs favourably against previous state-of-the-art methods. Ablation and visualization results demonstrate that semantic and variation factors can be effectively disentangled.
C1 [Luo, Ying; Kang, Guoliang; Liu, Kexin] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [Luo, Ying] Beihang Univ, Shenyuan Honors Coll, Beijing 100191, Peoples R China.
   [Kang, Guoliang; Liu, Kexin; Zhuang, Fuzhen; Lu, Jinhu] Zhongguancun Lab, Beijing 100086, Peoples R China.
   [Zhuang, Fuzhen] Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China.
   [Lu, Jinhu] Beihang Univ, Sch Automat Sci & Elect Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University; Zhongguancun Laboratory; Beihang
   University; Beihang University
RP Lü, JH (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
EM luoying3@buaa.edu.cn; kgl.prml@gmail.com; skxliu@163.com;
   zhuangfuzhen@buaa.edu.cn; jhlu@iss.ac.cn
RI ; Lu, Jinhu/B-6851-2013
OI Zhuang, Fuzhen/0000-0001-9170-7009; Lu, Jinhu/0000-0003-0275-8387; LUO,
   YING/0000-0002-8227-478X
FU National Key Ramp;D Program of China
FX No Statement Available
NR 0
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5872
EP 5883
DI 10.1109/TMM.2023.3340552
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100015
DA 2024-08-05
ER

PT J
AU Pan, JH
   Yang, SY
   Foo, LG
   Ke, QH
   Rahmani, H
   Fan, ZP
   Liu, J
AF Pan, Jianhong
   Yang, Siyuan
   Foo, Lin Geng
   Ke, Qiuhong
   Rahmani, Hossein
   Fan, Zhipeng
   Liu, Jun
TI Progressive Channel-Shrinking Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Indexing; Convolution; Costs; Generators; Feature extraction;
   Testing; Progressive; network shrinking
ID ATTENTION; MODEL
AB Currently, salience-based channel pruning makes continuous breakthroughs in network compression. In the realization, the salience mechanism is used as a metric of channel salience to guide pruning. Therefore, salience-based channel pruning can dynamically adjust the channel width at run-time, which provides a flexible pruning scheme. However, there are two problems emerging: a gating function is often needed to truncate the specific salience entries to zero, which destabilizes the forward propagation; dynamic architecture brings more cost for indexing in inference which bottlenecks the inference speed. In this article, we propose a Progressive Channel-Shrinking (PCS) method to compress the selected salience entries at run-time instead of roughly approximating them to zero. We also propose a Running Shrinking Policy to provide a testing-static pruning scheme that can reduce the memory access cost for filter indexing. We evaluate our method on ImageNet and CIFAR10 datasets over two prevalent networks: ResNet and VGG, and demonstrate that our PCS outperforms all baselines and achieves state-of-the-art in terms of compression-performance tradeoff. Moreover, we observe a significant and practical acceleration of inference. The code is available at https://github.com/JianhongPan-VLG/Progressive.Channel-Shrinking.Network.
C1 [Pan, Jianhong; Foo, Lin Geng; Liu, Jun] Singapore Univ Technol & Design, Singapore 487372, Singapore.
   [Yang, Siyuan] Nanyang Technol Univ, Nanyang 639798, Singapore.
   [Ke, Qiuhong] Monash Univ, Clayton, Vic 3800, Australia.
   [Rahmani, Hossein] Univ Lancaster, Lancaster LA1 4YW, England.
   [Fan, Zhipeng] NYU, New York, NY 10003 USA.
C3 Singapore University of Technology & Design; Nanyang Technological
   University; Monash University; Lancaster University; New York University
RP Liu, J (corresponding author), Singapore Univ Technol & Design, Singapore 487372, Singapore.
EM poonkinwang@outlook.com; siyuan005@e.ntu.edu.sg;
   lingeng_foo@mymail.sutd.edu.sg; qiuhong.ke@monash.edu;
   h.rahmani@lancaster.ac.uk; zf606@nyu.edu; jun_liu@sutd.edu.sg
RI FAN, Zhipeng/AAD-6708-2021; Rahmani, Hossein/S-5134-2019; Foo, Lin
   Geng/IQU-9586-2023
OI FAN, Zhipeng/0000-0002-7540-0442; Rahmani, Hossein/0000-0003-1920-0371;
   Liu, Jun/0000-0002-4365-4165; Foo, Lin Geng/0000-0002-6082-6002
FU MOE AcRF Tier 2
FX No Statement Available
CR Bejnordi B. E., 2020, PROC INT C LEARN REP
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen JT, 2019, LECT NOTES COMPUT SC, V11953, P175, DOI 10.1007/978-3-030-36708-4_15
   Chen ZR, 2019, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR.2019.00939
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Dong XY, 2017, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR.2017.205
   Gao X., 2019, PROC INT C LEARN REP, P1
   Gumbel E. J, 1954, National Bereau of Standards Applied Mathematics Series, V33
   Guo JY, 2021, IEEE T CIRC SYST VID, V31, P1114, DOI 10.1109/TCSVT.2020.2996231
   Han YZ, 2022, IEEE T PATTERN ANAL, V44, P7436, DOI 10.1109/TPAMI.2021.3117837
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Herrmann C., 2018, arXiv
   Herrmann C, 2020, Arxiv, DOI arXiv:1812.04180
   Howard A. G., 2017, arXiv
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hua Weizhe, 2019, Advances in Neural Information Processing Systems, P1884
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jang E, 2017, Arxiv, DOI arXiv:1611.01144
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li CL, 2021, PROC CVPR IEEE, P8603, DOI 10.1109/CVPR46437.2021.00850
   Li FR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5310, DOI 10.1109/ICCV48922.2021.00528
   Li H, 2016, INT C LEARNING REPRE
   Li Y., 2020, P EUR C COMP VIS ECC, P608
   Liebenwein L, 2020, Arxiv, DOI arXiv:1911.07412
   Lin J, 2017, Proceedings of the 31st international conference on neural information processing systems, V30, P2178
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Ople JJM, 2023, IEEE T MULTIMEDIA, V25, P1125, DOI 10.1109/TMM.2021.3139215
   Park J., 2018, PROC BRIT MACH VIS C
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun K., 2018, PROC BRIT MACH VIS C
   Szegedy C., 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594]
   Tang YH, 2021, PROC CVPR IEEE, P5016, DOI 10.1109/CVPR46437.2021.00498
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Ulutan Oytun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13614, DOI 10.1109/CVPR42600.2020.01363
   Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang X, 2018, LECT NOTES COMPUT SC, V11217, P420, DOI 10.1007/978-3-030-01261-8_25
   Wang X, 2022, IEEE T MULTIMEDIA, V24, P2804, DOI 10.1109/TMM.2021.3088639
   Wen W, 2016, ADV NEUR IN, V29
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia WH, 2022, IEEE T EMERG TOP COM, V10, P962, DOI 10.1109/TETC.2021.3056031
   Xie GT, 2018, PROC CVPR IEEE, P8847, DOI 10.1109/CVPR.2018.00922
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xuefei Ning, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P592, DOI 10.1007/978-3-030-58580-8_35
   Yawei Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8015, DOI 10.1109/CVPR42600.2020.00804
   Ye J., 2018, PROC6TH INT C LEARN, P1
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang T, 2017, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2017.469
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P1924, DOI 10.1109/TMM.2020.3005025
   Zhao BX, 2021, IEEE T MULTIMEDIA, V23, P1722, DOI 10.1109/TMM.2020.3002614
   Zhuo Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P138, DOI 10.1007/978-3-030-58539-6_9
NR 65
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2016
EP 2026
DI 10.1109/TMM.2023.3291197
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800037
OA Green Submitted, Green Accepted
DA 2024-08-05
ER

PT J
AU Qing, ZW
   Zhang, SW
   Huang, ZY
   Wang, X
   Wang, YH
   Lv, YL
   Gao, CX
   Sang, N
AF Qing, Zhiwu
   Zhang, Shiwei
   Huang, Ziyuan
   Wang, Xiang
   Wang, Yuehuan
   Lv, Yiliang
   Gao, Changxin
   Sang, Nong
TI MAR: <underline>M</underline>asked Autoencoders for Efficient
   <underline>A</underline>ction <underline>R</underline>ecognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Efficient action recognition; masked autoencoders; spatio-temporal
   redundancy; vision transformer
ID SPATIOTEMPORAL ATTENTION NETWORKS; ACTION RECOGNITION
AB Standard approaches for video action recognition usually operate on full input videos, which is inefficient due to the widespread spatio-temporal redundancy in videos. The recent progress in masked video modelling, specifically VideoMAE, has shown the ability of vanilla Vision Transformers (ViT) to complement spatio-temporal contexts using limited visual content. Inspired by this, we propose Masked Action Recognition (MAR), which reduces redundant computation by discarding a proportion of patches and operating only on a portion of the videos. MAR includes two essential components: cell running masking and bridging classifier. Specifically, to enable the ViT to perceive the details beyond the visible patches, cell running masking is used to preserve the spatio-temporal correlations in videos. This ensures that the patches at the same spatial location can be observed in turn for easy reconstructions. Additionally, we notice that, although the partially observed features can reconstruct semantically explicit invisible patches, they fail to achieve accurate classification. To address this issue, we propose a bridging classifier that can help fill the semantic gap between the ViT encoded features used for reconstruction and the specialized features used for classification. Our proposed MAR can reduce the computational cost of ViT by 53%. Extensive experiments have demonstrated that MAR consistently outperforms existing ViT models by a notable margin. Notably, we found that a ViT-Large model fine-tuned by MAR achieves comparable performance to a ViT-Huge model fine-tuned by standard training methods on both Kinetics-400 and Something-Something v2 datasets. Moreover, the computation overhead of our ViT-Large model is only 14.5% of that of the ViT-Huge model.
C1 [Qing, Zhiwu; Wang, Xiang; Wang, Yuehuan; Gao, Changxin; Sang, Nong] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automation, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Peoples R China.
   [Zhang, Shiwei; Lv, Yiliang] Alibaba Grp, Hangzhou 311100, Peoples R China.
   [Huang, Ziyuan] Natl Univ Singapore, Adv Robot Ctr, Singapore 119077, Singapore.
C3 Huazhong University of Science & Technology; Alibaba Group; National
   University of Singapore
RP Sang, N (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automation, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Peoples R China.; Zhang, SW (corresponding author), Alibaba Grp, Hangzhou 311100, Peoples R China.
EM qzw@hust.edu.cn; zhangjin.zsw@alibaba-inc.com; ziyuan.huang@u.nus.edu;
   wxiang@hust.edu.cn; yuehwang@hust.edu.cn; yiliang.lyl@alibaba-inc.com;
   cgao@hust.edu.cn; nsang@hust.edu.cn
RI ; Gao, Changxin/L-4841-2016
OI Wang, Xiang/0000-0003-0785-3367; sang, nong/0000-0002-9167-1496; Wang,
   Yuehuan/0000-0001-7046-7587; Gao, Changxin/0000-0003-2736-3920
FU National Natural Science Foundation of China
FX No Statement Available
CR Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bao H., 2022, P INT C LEARN REPR
   Bao H., 2020, INT C MACHINE LEARNI, V119, P642
   Beltagy I, 2020, Arxiv, DOI arXiv:2004.05150
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen PH, 2021, AAAI CONF ARTIF INTE, V35, P1045
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Diba Ali, 2021, P IEEE CVF INT C COM, P1502
   Dong L., 2019, P 33 INT C NEUR INF
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Fan HH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P705
   Fan Q., 2022, P INT C LEARN REPR, P1
   Feichtenhofer C, 2021, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR46437.2021.00331
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer Christoph, 2022, ADV NEURAL INF PROCE, V35, P35946
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Han T., 2020, Advances in Neural Information Processing Systems, V33, P5679
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hoffer E., 2020, P IEEE CVF C COMP VI, P8129
   Hu K., 2021, P IEEE CVF INT C COM, P7939
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Huang Z., 2022, P INT C LEARN REPR
   Huang ZY, 2021, PROC CVPR IEEE, P1276, DOI 10.1109/CVPR46437.2021.00133
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633
   Kornblith S, 2019, PR MACH LEARN RES, V97
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li HD, 2021, PROC CVPR IEEE, P6151, DOI 10.1109/CVPR46437.2021.00609
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li K., 2022, P INT C LEARN REPR
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Liet Y., 2022, P IEEE CVF C COMP VI, P4804
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu SC, 2023, IEEE T MULTIMEDIA, V25, P2573, DOI 10.1109/TMM.2022.3148588
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13688, DOI 10.1109/ICCV48922.2021.01345
   Long FC, 2022, LECT NOTES COMPUT SC, V13695, P475, DOI 10.1007/978-3-031-19833-5_28
   Long FC, 2022, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR52688.2022.00319
   Loshchilov I., 2017, INT C LEARNING REPRE
   Loshchilov I., 2018, INT C LEARN REPR
   Neimark D, 2021, IEEE INT CONF COMP V, P3156, DOI [arXiv:2102.00719, 10.1109/ICCVW54120.2021.00355]
   Patrick M., 2021, ADV NEURAL INFORM PR, V34, P12493
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Qing ZW, 2023, IEEE T MULTIMEDIA, V25, P9002, DOI 10.1109/TMM.2023.3244126
   Qing ZW, 2022, PROC CVPR IEEE, P13811, DOI 10.1109/CVPR52688.2022.01345
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan H., 2021, arXiv
   Tengda Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P312, DOI 10.1007/978-3-030-58580-8_19
   Tong Z, 2022, P INT C ADV NEUR INF, P7780
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   van den Oord A, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang JP, 2022, IEEE T MULTIMEDIA, V24, P2553, DOI 10.1109/TMM.2021.3087023
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang R, 2022, PROC CVPR IEEE, P14713, DOI 10.1109/CVPR52688.2022.01432
   Wang SR, 2023, Arxiv, DOI arXiv:2205.14443
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2021, P IEEE CVF INT C COM, P16249
   Wang YL, 2022, PROC CVPR IEEE, P20030, DOI 10.1109/CVPR52688.2022.01943
   Wei C, 2022, PROC CVPR IEEE, P14648, DOI 10.1109/CVPR52688.2022.01426
   Wu W., 2020, P CVPR WORKSH, P676
   Wu WH, 2019, IEEE I CONF COMP VIS, P6231, DOI 10.1109/ICCV.2019.00632
   Wu ZX, 2019, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2019.00137
   Wu ZX, 2019, ADV NEUR IN, V32
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xie ZD, 2022, PROC CVPR IEEE, P9643, DOI 10.1109/CVPR52688.2022.00943
   Yang ZL, 2019, ADV NEUR IN, V32
   Yao T, 2021, AAAI CONF ARTIF INTE, V35, P10656
   Yue Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P86, DOI 10.1007/978-3-030-58571-6_6
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARNING REPRE
   Zheng YD, 2020, IEEE T IMAGE PROCESS, V29, P7970, DOI 10.1109/TIP.2020.3007826
   Zhi Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1493, DOI 10.1109/ICCV48922.2021.00154
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou JQ, 2023, IEEE T MULTIMEDIA, V25, P5192, DOI 10.1109/TMM.2022.3189253
   Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503
NR 100
TC 7
Z9 7
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 218
EP 233
DI 10.1109/TMM.2023.3263288
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500022
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Shao, MQ
   Xia, CK
   Duan, DX
   Wang, XQ
AF Shao, Mingqi
   Xia, Chongkun
   Duan, Dongxu
   Wang, Xueqian
TI Polarimetric Inverse Rendering for Transparent Shapes Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transparent shapes reconstruction; polarimetric rendering
AB The acquisition of transparent 3D shapes will facilitate many multimedia and computer vision tasks, such as game/movie production and virtual enrioment applications. In this work, we propose a novel method for detailed reconstruction of transparent objects by exploiting polarimetric cues. Most of existing transparent shapes reconstruction methods usually lack sufficient constraints and suffer from the over-smooth problem. Hence, we introduce polarization information as a complementary cue. Specifically, we employ the implicit representation for object's geometry with a neural network, while the polarization render is capable of differentiably rendering the object's polarization images from given illumination configuration. However, direct comparison of rendered polarization images to the real-world captured images will have additional errors due to the transmission in the transparent object. To make the polarimetric cues technically feasible on transparent shapes reconstruction, the concept of reflection percentage which represents proportion of the reflection component is introduced as the weight of the polarization loss. Based on controllable environment setup, we build a polarization dataset containing several solid and smooth transparent objects to verify our method. Experimental results show that our method is capable of recovering detailed shapes and improving reconstruction quality of transparent objects.
C1 [Shao, Mingqi; Duan, Dongxu; Wang, Xueqian] Tsinghua Univ, Beijing 100190, Peoples R China.
   [Xia, Chongkun] Sun Yat Sen Univ, Sch Adv Mfg, Guangzhou, Peoples R China.
C3 Tsinghua University; Sun Yat Sen University
RP Wang, XQ (corresponding author), Tsinghua Univ, Beijing 100190, Peoples R China.; Xia, CK (corresponding author), Sun Yat Sen Univ, Sch Adv Mfg, Guangzhou, Peoples R China.
EM smq21@mails.tsinghua.edu.cn; xiachk5@mail.sysu.edu.cn;
   ddx21@mails.tsinghua.edu.cn; wang.xq@sz.tsinghua.edu.cn
RI Wang, xueqian/X-4874-2018
OI Wang, xueqian/0000-0003-3542-0593
FU National Key R&D Program of China
FX No Statement Available
CR Atkinson GA, 2006, IEEE T IMAGE PROCESS, V15, P1653, DOI 10.1109/TIP.2006.871114
   Bahirat K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209661
   Born M., 2013, Principles of Optics: Electromagnetic Theory of Propagation, Interference and Diffraction of Light, DOI 10.1017/CBO9781139644181
   Bruno F, 2010, J CULT HERIT, V11, P42, DOI 10.1016/j.culher.2009.02.006
   Chen LX, 2018, LECT NOTES COMPUT SC, V11220, P21, DOI 10.1007/978-3-030-01270-0_2
   Chen SY, 2023, IEEE T MULTIMEDIA, V25, P3166, DOI 10.1109/TMM.2022.3156820
   Clarke D., 2009, Stellar Polarimetry
   Cui ZP, 2017, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2017.47
   Dave A, 2022, LECT NOTES COMPUT SC, V13667, P538, DOI 10.1007/978-3-031-20071-7_32
   Deschaintre V, 2021, PROC CVPR IEEE, P15562, DOI 10.1109/CVPR46437.2021.01531
   Ding YQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5017, DOI 10.1109/ICCV48922.2021.00499
   Fang HJ, 2022, IEEE ROBOT AUTOM LET, V7, P7383, DOI 10.1109/LRA.2022.3183256
   Gropp A., 2020, PMLR, P3789
   Guo YC, 2022, PROC CVPR IEEE, P18388, DOI 10.1109/CVPR52688.2022.01786
   Hafeez Jahanzeb, 2017, The International Journal of Advanced Smart Convergence, V6, P9
   Hendrikx M, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422957
   Ichnowski J., 2022, PMLR, P526
   Jinyu Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P85, DOI 10.1007/978-3-030-58586-0_6
   Kadambi A, 2015, IEEE I CONF COMP VIS, P3370, DOI 10.1109/ICCV.2015.385
   Kargas A, 2019, HERITAGE-BASEL, V2, P1799, DOI 10.3390/heritage2030110
   Kasten Y., 2020, PROC NEURIPS, V33, P2492, DOI 10.48550/arXiv.2003.09852
   Kingma D.P., 2014, Proc. of ICLR
   Kutulakos KN, 2008, INT J COMPUT VISION, V76, P13, DOI 10.1007/s11263-007-0049-9
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   Lei CY, 2020, PROC CVPR IEEE, P1747, DOI 10.1109/CVPR42600.2020.00182
   Levy D, 2023, PROC CVPR IEEE, P56, DOI 10.1109/CVPR52729.2023.00014
   Li ZQ, 2020, PROC CVPR IEEE, P1259, DOI 10.1109/CVPR42600.2020.00134
   Lou JW, 2020, IEEE T MULTIMEDIA, V22, P730, DOI 10.1109/TMM.2019.2933338
   Lyu JH, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417815
   Michalkiewicz M, 2019, IEEE I CONF COMP VIS, P4742, DOI 10.1109/ICCV.2019.00484
   Miyazaki D, 2005, PROC CVPR IEEE, P910
   Miyazaki D, 2004, IEEE T PATTERN ANAL, V26, P73, DOI 10.1109/TPAMI.2004.1261080
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498
   Oechsle M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5569, DOI 10.1109/ICCV48922.2021.00554
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Paszke A, 2019, ADV NEUR IN, V32
   Popescu Marius-Constantin, 2009, WSEAS Transactions on Circuits and Systems, V8, P579
   Qian YM, 2016, PROC CVPR IEEE, P4369, DOI 10.1109/CVPR.2016.473
   Sajjan S, 2020, IEEE INT CONF ROBOT, P3634, DOI 10.1109/ICRA40945.2020.9197518
   Shao MQ, 2023, IEEE I CONF COMP VIS, P9243, DOI 10.1109/ICCV51070.2023.00851
   Shihao Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P351, DOI 10.1007/978-3-030-58568-6_21
   Tewari A, 2022, COMPUT GRAPH FORUM, V41, P703, DOI 10.1111/cgf.14507
   Tsai CY, 2015, IEEE IMAGE PROC, P606, DOI 10.1109/ICIP.2015.7350870
   Wang P, 2021, 35 C NEURAL INFORM P, V34
   Wang XY, 2022, IEEE T MULTIMEDIA, V24, P4028, DOI 10.1109/TMM.2021.3111485
   Wu BJ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201286
   Wu ZQ, 2022, IEEE T MULTIMEDIA, V24, P3782, DOI 10.1109/TMM.2021.3107688
   Xu JM, 2023, Arxiv, DOI arXiv:2203.12613
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P3014, DOI 10.1109/TMM.2020.2967645
   Yang LW, 2018, PROC CVPR IEEE, P3857, DOI 10.1109/CVPR.2018.00406
   Yariv L, 2021, ADV NEUR IN
   Yunhao Ba, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P554, DOI 10.1007/978-3-030-58586-0_33
   Zhan Y., 2023, PROC IEEECVF INT C C, P18402
   Zhu C., 2022, P ADV NEUR INF PROC, V35, P38994
NR 55
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7801
EP 7811
DI 10.1109/TMM.2024.3371792
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000066
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shi, YY
   Fu, XL
   Li, YN
   Miao, KB
   Liu, XZ
   Zhao, BC
   Miao, QG
AF Shi, Yuanyuan
   Fu, Xiaolong
   Li, Yunan
   Miao, Kaibin
   Liu, Xiangzeng
   Zhao, Bocheng
   Miao, Qiguang
TI A Semi-Supervised Underexposed Image Enhancement Network With Supervised
   Context Attention and Multi-Exposure Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Visualization; Image color analysis; Fuses; Image edge
   detection; Lighting; Generative adversarial networks; Underexposed
   images enhancement; semi-supervised learning; multi-exposure fusion;
   supervised context attention
ID HISTOGRAM EQUALIZATION; RETINEX
AB Recently, image enhancement approaches yield impressive progress. However, most methods are still based supervised-learning, which requires plenty of paired data. Meanwhile, owing to the complex illumination condition in a real-world scenario, those methods trained on synthetic images cannot restore details in extremely dark or bright areas and lead to exposure errors. The traditional losses that deem all pixels the same in training also produce blurry edges in the result. To handle these problems, in this article, we present an effective semi-supervised framework for severely underexposed image enhancement. Our network consists of a supervised and an unsupervised branch, which shares weights and can make full use of paired data and plenty of unpaired data. Meanwhile, a multi-exposure fusion module is designed to adaptively fuse the corrected images to address the low contrast and color bias issues occurring in some extreme situations. Moreover, we propose a supervised context attention module to better use the edge information as supervision to recover fine image details. Extensive experiments have proved that the proposed method outperforms state-of-the-art approaches in enhancing exposure images.
C1 [Shi, Yuanyuan; Fu, Xiaolong; Li, Yunan; Liu, Xiangzeng; Zhao, Bocheng; Miao, Qiguang] Xidian Univ, Sch Comp Sci & Technol, Xian Key Lab Big Data & Intelligent Vis, Xian 710071, Peoples R China.
   [Shi, Yuanyuan; Fu, Xiaolong; Li, Yunan; Liu, Xiangzeng; Zhao, Bocheng; Miao, Qiguang] Xidian Univ, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
   [Miao, Kaibin] Xidian Univ, Shanxi Normal Univ, Fac Educ, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University; Shaanxi Normal University; Xidian
   University; Shanxi Normal University
RP Li, YN; Miao, QG (corresponding author), Xidian Univ, Sch Comp Sci & Technol, Xian Key Lab Big Data & Intelligent Vis, Xian 710071, Peoples R China.; Li, YN; Miao, QG (corresponding author), Xidian Univ, Key Lab Collaborat Intelligence Syst, Minist Educ, Xian 710071, Peoples R China.
EM yuanyuan_shi@stu.xidian.edu.cn; fxlcumt@gmail.com;
   yunanli@xidian.edu.cn; kbmiao@163.com; xzliu@xidian.edu.cn;
   zhaobocheng@xidian.edu.cn; qgmiao@xidian.edu.cn
RI Shi, YUANYUAN/HGC-7984-2022
OI Shi, YUANYUAN/0000-0002-7248-971X; Miao, Qiguang/0000-0002-2872-388X;
   Li, Yunan/0000-0001-7316-4354
FU National Key Ramp;D Program of China
FX No Statement Available
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chai Y, 2020, IEEE WINT CONF APPL, P981, DOI 10.1109/WACV45572.2020.9093321
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Fu XY, 2013, IEEE GLOB CONF SIG, P1085, DOI 10.1109/GlobalSIP.2013.6737082
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2023, INT J COMPUT VISION, V131, P48, DOI 10.1007/s11263-022-01667-9
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Han-Ul Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P339, DOI 10.1007/978-3-030-58595-2_21
   He Y., 2022, P 16 EUR C COMP VIS, P679
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1614, DOI 10.1145/3343031.3350855
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Lei XZ, 2023, IEEE T MULTIMEDIA, V25, P4439, DOI 10.1109/TMM.2022.3175634
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2023, IEEE T MULTIMEDIA, V25, P3573, DOI 10.1109/TMM.2022.3162493
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moran Sean, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12823, DOI 10.1109/CVPR42600.2020.01284
   Ni ZK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1697, DOI 10.1145/3394171.3413839
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vonikakis V, 2008, IET IMAGE PROCESS, V2, P19, DOI 10.1049/iet-ipr:20070012
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855
   Xu Xiaogang, 2022, IEEECVF C COMPUT VIS, P17714
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740
   Zhang Y., 2020, ARXIV
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106
   Zhuang YL, 2022, Arxiv, DOI arXiv:2209.07937
NR 64
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1229
EP 1243
DI 10.1109/TMM.2023.3278380
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700030
DA 2024-08-05
ER

PT J
AU Wei, WW
   Wei, P
   Qin, JL
   Liao, ZM
   Wang, SJ
   Cheng, X
   Liu, MQ
   Zheng, NN
AF Wei, Wenwen
   Wei, Ping
   Qin, Jialu
   Liao, Zhimin
   Wang, Shuaijie
   Cheng, Xiang
   Liu, Meiqin
   Zheng, Nanning
TI 3D Scene Graph Generation From Point Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Point cloud compression;
   Task analysis; Head; Semantics; Proposals; 3D scene graph generation;
   point RoI; location attention; point cloud
ID OBJECT; NETWORK
AB Scene graph generation is a significant and challenging task for scene understanding. Most existing methods are confined to the 2D space (i.e. images) or additional use of segmentation information, while neglecting the richer spatial and geometric information of 3D space. In this paper, we propose a novel method to generate scene graphs from 3D point clouds. Specifically, our model consists of three parts: a point feature extraction backbone, a box head, and a relation head. The feature extraction backbone extracts base features directly from raw point clouds, and the box head produces detected 3D bounding boxes. Final 3D scene graphs are obtained from the relation head which takes the extracted features and 3D boxes as inputs. We also design a point RoI module which sequentially processes points inside 3D boxes with a bidirectional LSTM. To further leverage the geometric characteristics of point clouds, we propose a location attention module which learns the influence of relative locations between objects. We introduce the RelationScanNet dataset with densely annotated semantic and geometric relationships, which extends one of the most widely used dataset ScanNetV2 in 3D indoor scene understanding. We test the proposed method on the RelationScanNet dataset and 3DSSG dataset. The results prove the strength of our method.
C1 [Wei, Wenwen; Wei, Ping; Qin, Jialu; Liao, Zhimin; Wang, Shuaijie; Liu, Meiqin; Zheng, Nanning] Xi An Jiao Tong Univ, Natl Key Lab Human Machine Hybrid Augmented Intell, Xian 710049, Peoples R China.
   [Wei, Wenwen; Wei, Ping; Qin, Jialu; Liao, Zhimin; Wang, Shuaijie; Liu, Meiqin; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
   [Cheng, Xiang] Peking Univ, Beijing 100871, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Peking University
RP Wei, P (corresponding author), Xi An Jiao Tong Univ, Natl Key Lab Human Machine Hybrid Augmented Intell, Xian 710049, Peoples R China.; Wei, P (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
EM wwwei@stu.xjtu.edu.cn; pingwei@xjtu.edu.cn; jlqin@stu.xjtu.edu.cn;
   liaozzm@gmail.com; sj1996@stu.xjtu.edu.cn; xiangcheng@pku.edu.cn;
   liumeiqin@xjtu.edu.cn; nnzheng@xjtu.edu.cn
OI Cheng, Xiang/0000-0002-5943-0326
FU Shaanxi Universities
FX No Statement Available
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Armeni I, 2019, IEEE I CONF COMP VIS, P5663, DOI 10.1109/ICCV.2019.00576
   Arvanitis G, 2022, IEEE T MULTIMEDIA, V24, P2230, DOI 10.1109/TMM.2021.3089838
   Beltrán J, 2018, IEEE INT C INTELL TR, P3517, DOI 10.1109/ITSC.2018.8569311
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Chen VS, 2019, IEEE I CONF COMP VIS, P2580, DOI [10.1109/ICCV.2019.00267, 10.1109/iccv.2019.00267]
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161
   Gay P, 2019, LECT NOTES COMPUT SC, V11363, P330, DOI 10.1007/978-3-030-20893-6_21
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Herzig R, 2018, ADV NEUR IN, V31
   Hough P. V. C., 1959, C P C, P554
   Huang JJ, 2022, IEEE T MULTIMEDIA, V24, P188, DOI 10.1109/TMM.2020.3047762
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li BY, 2019, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2019.00111
   Li GB, 2019, IEEE T IMAGE PROCESS, V28, P1591, DOI 10.1109/TIP.2018.2878956
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YY, 2023, IEEE T VIS COMPUT GR, V29, P5556, DOI 10.1109/TVCG.2022.3219451
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lv CL, 2022, IEEE T MULTIMEDIA, V24, P1815, DOI 10.1109/TMM.2021.3073265
   Mikolov T., 2013, ARXIV, DOI DOI 10.48550/ARXIV.1301.3781
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Wald J, 2020, PROC CVPR IEEE, P3960, DOI 10.1109/CVPR42600.2020.00402
   Wald J, 2019, IEEE I CONF COMP VIS, P7657, DOI 10.1109/ICCV.2019.00775
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei WW, 2021, AAAI CONF ARTIF INTE, V35, P2861
   Wenbin Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P222, DOI 10.1007/978-3-030-58601-0_14
   Wu F., 2022, Virtual Real. Intell. Harhu, V4, P76
   Wu SC, 2021, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR46437.2021.00743
   Xia CL, 2021, NEUROCOMPUTING, V437, P107, DOI 10.1016/j.neucom.2021.01.025
   Xia F, 2018, PROC CVPR IEEE, P9068, DOI 10.1109/CVPR.2018.00945
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P265, DOI 10.1145/3394171.3413722
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yu F, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1386, DOI 10.1145/3394171.3413566
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang S, 2021, Adv Neural Inf Process Syst, V34, P18620
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 64
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5358
EP 5368
DI 10.1109/TMM.2023.3331583
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600001
DA 2024-08-05
ER

PT J
AU Zhang, AR
   Ling, ZG
   Wang, YN
AF Zhang, Aoran
   Ling, Zhigang
   Wang, Yaonan
TI Multi-Layer Decoupling Attention Network for Weakly Supervised Object
   Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Location awareness; Cams; Annotations; Visualization; Fuses; Background
   noise; Semantics; Multi-layer comparison decoupling mechanism;
   multi-layer minimum decoupling feature fusion; channel attention
   activation decoupling (CAAD); spatial attention activation decoupling
   (SAAD); weakly supervised object localization
AB Weakly supervised object localization (WSOL) aims to localize the entire and well-defined objects only via image-level labels for reducing the need of labor-intensive annotation and mitigating annotation errors. However, many WSOL methods via class activation maps (CAMs) often suffer from incomplete activation and inaccurate boundaries for object localization. In this article, we propose a novel multi-layer decoupling attention localization (MDAL) network to address these issues. We first present a simple yet effective multi-layer comparison decoupling mechanism including a maximum decoupling function and a minimum decoupling function to sufficiently activate and fuse multi-layer features. Then, we introduce the multi-layer maximum decoupling function into the attention modules, and develop a channel attention activation decoupling (CAAD) module and a spatial attention activation decoupling (SAAD) module, which can mine much more useful information for more possible regions' activation. Furthermore, the multi-layer minimum decoupling function is introduced to efficiently fuse and refine multi-layer features, which can suppress the over-activation and background noise. Finally, we develop a joint loss function to train the MDAL network. Experimental results on CUB-200-2011 and ILSVRC2012 demonstrate that our proposed network can provide accurate and complete object localization.
C1 [Zhang, Aoran; Ling, Zhigang; Wang, Yaonan] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
   [Zhang, Aoran; Ling, Zhigang; Wang, Yaonan] Natl Engn Res Ctr Robot Visual Percept & Control T, Changsha 410082, Peoples R China.
C3 Hunan University
RP Ling, ZG (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.; Ling, ZG (corresponding author), Natl Engn Res Ctr Robot Visual Percept & Control T, Changsha 410082, Peoples R China.
EM zhangaoran@hnu.edu.cn; zgling_hunan@126.com; yaonan@hnu.edu.cn
OI Zhang, Aoran/0000-0003-3549-4808
FU National Natural Science Foundation of China
FX No Statement Available
CR Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Chen-Lin Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13457, DOI 10.1109/CVPR42600.2020.01347
   Choe J, 2021, IEEE T PATTERN ANAL, V43, P4256, DOI 10.1109/TPAMI.2020.2999099
   Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   GASTON PC, 1984, IEEE T PATTERN ANAL, V6, P257, DOI 10.1109/TPAMI.1984.4767518
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jinjie Mai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8763, DOI 10.1109/CVPR42600.2020.00879
   Jo S, 2021, IEEE IMAGE PROC, P639, DOI 10.1109/ICIP42928.2021.9506058
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   Lei CX, 2018, 2018 IEEE 18TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT), P1222, DOI 10.1109/ICCT.2018.8600125
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Meng M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3365, DOI 10.1109/ICCV48922.2021.00337
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Qin BS, 2023, IEEE T MULTIMEDIA, V25, P4282, DOI 10.1109/TMM.2022.3173131
   Qin J, 2022, AAAI CONF ARTIF INTE, P2117
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shih HC, 2009, IEEE T MULTIMEDIA, V11, P244, DOI 10.1109/TMM.2008.2009682
   Simonyan K, 2014, Arxiv, DOI [arXiv:1312.6034, DOI 10.48550/ARXIV.1312.6034]
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Tan MX, 2019, PR MACH LEARN RES, V97
   Vaswani A, 2017, ADV NEUR IN, V30
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei J, 2021, PROC CVPR IEEE, P5989, DOI 10.1109/CVPR46437.2021.00593
   Wei XS, 2019, PATTERN RECOGN, V88, P113, DOI 10.1016/j.patcog.2018.10.022
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu PY, 2022, PROC CVPR IEEE, P14228, DOI 10.1109/CVPR52688.2022.01385
   Xie JH, 2022, PROC CVPR IEEE, P979, DOI 10.1109/CVPR52688.2022.00106
   Xie JH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P132, DOI 10.1109/ICCV48922.2021.00020
   Xu JL, 2022, PROC CVPR IEEE, P9427, DOI 10.1109/CVPR52688.2022.00922
   Xu Z, 2017, IEEE T IMAGE PROCESS, V26, P135, DOI 10.1109/TIP.2016.2621661
   Xue HL, 2019, IEEE I CONF COMP VIS, P6588, DOI 10.1109/ICCV.2019.00669
   Yang S, 2020, IEEE WINT CONF APPL, P2930, DOI 10.1109/WACV45572.2020.9093566
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhang XL, 2018, LECT NOTES COMPUT SC, V11216, P610, DOI 10.1007/978-3-030-01258-8_37
   Zhang YF, 2022, PROC CVPR IEEE, P1248, DOI 10.1109/CVPR52688.2022.00132
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 48
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4469
EP 4479
DI 10.1109/TMM.2023.3323860
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100028
DA 2024-08-05
ER

PT J
AU Zhou, WJ
   Cai, YQ
   Zhang, LT
   Yan, WQ
   Yu, L
AF Zhou, Wujie
   Cai, Yuqi
   Zhang, Liting
   Yan, Weiqing
   Yu, Lu
TI UTLNet: Uncertainty-Aware Transformer Localization Network for RGB-Depth
   Mirror Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mirrors; Image segmentation; Feature extraction; Transformers;
   Uncertainty; Semantic segmentation; Semantics; RGB-D mirror
   segmentation; uncertainty-aware; graph convolution; transformer
ID CO-SALIENCY DETECTION; PREDICTION; FUSION
AB Mirror segmentation, an emerging discipline in the field of computer vision, involves the identification and marking of mirrors in an image. Current mirror segmentation methods rely on fixed mirror elements as features for object segmentation. However, these methods do not account for the varied quality of feature images obtained under complex real-world conditions, leading to inaccurate segmentation results. To address these limitations, we propose a novel uncertainty-aware transformer localization network (UTLNet) for RGB-D mirror segmentation. Our approach draws inspiration from biomimicry, specifically the behavior pattern of human observation. We aim to explore features from different angles and focus on complex features that are challenging to determine during the coding stage. Additionally, we employ graph convolution to construct complementary dual-modal fusion features. Furthermore, we design a multiscale interaction transformer module using the shifted-window self-attention mechanism to acquire precise position information. In our experiments, the proposed UTLNet surpasses the current state-of-the-art mirror segmentation method as well as alternative task-specific methods. It achieves superior performance across various evaluation scenarios.
C1 [Zhou, Wujie; Cai, Yuqi; Zhang, Liting] Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
   [Zhou, Wujie; Yan, Weiqing] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 308232, Singapore.
   [Yan, Weiqing] Yantai Univ, Sch Comp & Control Engn, Yantai 264005, Peoples R China.
   [Yu, Lu] Zhejiang Univ, Inst Informat & Commun Engn, Hangzhou 310027, Peoples R China.
C3 Zhejiang University of Science & Technology; Nanyang Technological
   University; Yantai University; Zhejiang University
RP Zhou, WJ (corresponding author), Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.; Zhou, WJ; Yan, WQ (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 308232, Singapore.
EM wujiezhou@163.com; wqyan@tju.edu.cn
FU National Key Research and Development Program of China
FX No Statement Available
CR Chen DW, 2022, IEEE T CIRC SYST VID, V32, P8730, DOI 10.1109/TCSVT.2022.3197159
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2024, IEEE T NEUR NET LEAR, V35, P4309, DOI 10.1109/TNNLS.2022.3202241
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Chen TY, 2022, NEURAL COMPUT APPL, V34, P7547, DOI 10.1007/s00521-021-06845-3
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Cheng QR, 2023, IEEE T MULTIMEDIA, V25, P7062, DOI 10.1109/TMM.2022.3217384
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Du H, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3204090
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fang X, 2022, Arxiv, DOI arXiv:2203.10785
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Gu JQ, 2022, PROC CVPR IEEE, P12084, DOI 10.1109/CVPR52688.2022.01178
   Guan HK, 2022, PROC CVPR IEEE, P5931, DOI 10.1109/CVPR52688.2022.00585
   Haibo Zhao, 2021, 2021 International Conference on Information Technology and Biomedical Engineering (ICITBE), P71, DOI 10.1109/ICITBE54178.2021.00025
   He LQ, 2022, IMAGE VISION COMPUT, V121, DOI 10.1016/j.imavis.2022.104402
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/icip.2019.8803025, 10.1109/ICIP.2019.8803025]
   Huang T., 2022, PROC AAAI C ARTIF IN, P935
   Li J, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3175055
   Li TP, 2022, IEEE T MULTIMEDIA, V24, P492, DOI 10.1109/TMM.2021.3054526
   Liao GB, 2022, IEEE T CIRC SYST VID, V32, P7646, DOI 10.1109/TCSVT.2022.3184840
   Lin JY, 2021, PROC CVPR IEEE, P13410, DOI 10.1109/CVPR46437.2021.01321
   Lin JY, 2020, PROC CVPR IEEE, P3694, DOI 10.1109/CVPR42600.2020.00375
   Liu C, 2022, Arxiv, DOI arXiv:2207.01172
   Liu JW, 2022, IEEE WINT CONF APPL, P2613, DOI 10.1109/WACV51458.2022.00267
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Mei HY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3566127
   Mei HY, 2021, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR46437.2021.00306
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Min DY, 2022, IEEE SIGNAL PROC LET, V29, P1674, DOI 10.1109/LSP.2022.3192753
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Pang YW, 2022, PROC CVPR IEEE, P2150, DOI 10.1109/CVPR52688.2022.00220
   Pei JL, 2022, LECT NOTES COMPUT SC, V13678, P19, DOI 10.1007/978-3-031-19797-0_2
   Pei JL, 2023, IEEE T MULTIMEDIA, V25, P1964, DOI 10.1109/TMM.2022.3141891
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pu MY, 2022, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR52688.2022.00146
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Seichter D, 2021, IEEE INT CONF ROBOT, P13525, DOI 10.1109/ICRA48506.2021.9561675
   Sun L, 2020, IEEE ROBOT AUTOM LET, V5, P5558, DOI 10.1109/LRA.2020.3007457
   Tang B, 2023, IEEE T CIRC SYST VID, V33, P728, DOI 10.1109/TCSVT.2022.3202563
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Junke, 2022, Advances in Neural Information Processing Systems
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Whelan T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201319
   Wu DM, 2022, PROC CVPR IEEE, P4986, DOI 10.1109/CVPR52688.2022.00494
   Wu LS, 2018, MULTIMED TOOLS APPL, V77, P21185, DOI 10.1007/s11042-017-5576-y
   Wu YH, 2023, IEEE T PATTERN ANAL, V45, P12760, DOI 10.1109/TPAMI.2022.3202765
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xue Y, 2023, IEEE T MULTIMEDIA, V25, P4002, DOI 10.1109/TMM.2022.3171400
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Yang X, 2019, IEEE I CONF COMP VIS, P8808, DOI 10.1109/ICCV.2019.00890
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang B., 2022, P C NEUR INF PROC SY, V35, P4971
   Zhang JM, 2023, IEEE T INTELL TRANSP, V24, P14679, DOI 10.1109/TITS.2023.3300537
   Zhang JM, 2021, IEEE INT CONF COMP V, P1760, DOI 10.1109/ICCVW54120.2021.00202
   Zhang M, 2023, IEEE T MULTIMEDIA, V25, P5142, DOI 10.1109/TMM.2022.3187856
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhang WQ, 2022, PROC CVPR IEEE, P12073, DOI 10.1109/CVPR52688.2022.01177
   Zhang Y, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13081440
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zheng DH, 2023, IEEE WINT CONF APPL, P6221, DOI 10.1109/WACV56688.2023.00617
   Zhou J., 2021, PROC IEEE INT C INST, P1
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou WJ, 2023, IEEE T IMAGE PROCESS, V32, P3027, DOI 10.1109/TIP.2023.3275538
   Zhou WJ, 2021, IEEE INTELL SYST, V36, P73, DOI 10.1109/MIS.2020.2999462
   Zhu CB, 2018, MULTIMED TOOLS APPL, V77, P25181, DOI 10.1007/s11042-018-5780-4
   Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178
   Zhu HW, 2022, AAAI CONF ARTIF INTE, P3608
NR 80
TC 8
Z9 8
U1 41
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4564
EP 4574
DI 10.1109/TMM.2023.3323890
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100045
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Zhuang, JM
   Yu, J
   Ding, Y
   Qu, XY
   Hu, Y
AF Zhuang, Jiamin
   Yu, Jing
   Ding, Yang
   Qu, Xiangyan
   Hu, Yue
TI Towards Fast and Accurate Image-Text Retrieval With Self-Supervised
   Fine-Grained Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Image coding; Training; Encoding;
   Computational modeling; Costs; Fast image-text retrieval; concept-level
   cross-modal alignment; context-level cross-modal alignment;
   self-supervised learning
AB Image-text retrieval requires the system to bridge the heterogenous gap between vision and language for accurate retrieval while keeping the network lightweight-enough for efficient retrieval. Existing trade-off solutions mainly study from the view of incorporating cross-modal interactions with the independent-embedding framework or leveraging stronger pre-trained encoders, which still demand time-consuming similarity measurement or heavyweight model structure in the retrieval stage. In this work, we propose an image-text alignment module SelfAlign on top of the independent-embedding framework, which improves the retrieval accuracy while maintains the retrieval efficiency without extra supervision. SelfAlign contains two collaborative sub-modules that force image-text alignment at both the concept level and context level by self-supervised contrastive learning. It doesn't require cross-modal embedding interactions during training while maintaining independent image and text encoders during retrieval. With comparable time cost, SelfAlign consistently boosts the accuracy of state-of-the-art non-pre-training independent-embedding models respectively by 9.1%, 4.2%, and 6.6% in terms of R@sum score on Flickr30 K, MS-COCO 1 K and MS-COCO 5 K datasets. The retrieval accuracy also outperforms most existing interactive-embedding models with orders of magnitude decrease in retrieval time. The source code is available at: https://github.com/Zjamie813/SelfAlign.
C1 [Zhuang, Jiamin; Yu, Jing; Ding, Yang; Qu, Xiangyan; Hu, Yue] Chinese Acad Sci, Inst Informat Engn, Beijing 100085, Peoples R China.
   [Zhuang, Jiamin; Yu, Jing; Ding, Yang; Qu, Xiangyan; Hu, Yue] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 101408, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Yu, J (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100085, Peoples R China.
EM zhuangjiamin@iie.ac.cn; yujing02@iie.ac.cn; dingyang@iie.ac.cn;
   quxiangyan@iie.ac.cn; huyue@iie.ac.cn
OI Yu, Jing/0000-0002-3966-511X; Qu, Xiangyan/0000-0003-3658-8099
FU National Natural Science Foundation of China
FX No Statement Available
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bachman P, 2019, ADV NEUR IN, V32
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Faghri F., 2018, PROC BRIT MACH VIS C, P344
   Frome A., 2013, Advances in neural information processing systems, V26
   Geigle G, 2022, T ASSOC COMPUT LING, V10, P503, DOI 10.1162/tacl_a_00473
   Hjelm R. D., 2018, INT C LEARN REPR
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Zhong, 2021, P 30 INT JOINT C ART, P765, DOI DOI 10.24963/IJCAI.2021/106
   Jia C, 2021, PR MACH LEARN RES, V139
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Ku A, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4392
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li C., 2022, PROC INT C LEARN REP
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Chunxiao, 2020, CVPR, P10918, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu HL, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9796
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Lu XP, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P5020
   Miech A, 2021, PROC CVPR IEEE, P9821, DOI 10.1109/CVPR46437.2021.00970
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun SQ, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P982
   Tian Yuandong, 2021, INT C MACH LEARN, P10268, DOI DOI 10.48550/ARXIV.2102.06810
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vendrov I., 2016, PROC INT C LEARN REP
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang XL, 2021, PROC CVPR IEEE, P3023, DOI 10.1109/CVPR46437.2021.00304
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
NR 49
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1361
EP 1372
DI 10.1109/TMM.2023.3280734
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700027
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ak, A
   Zerman, E
   Quach, M
   Chetouani, A
   Smolic, A
   Valenzise, G
   Le Callet, P
AF Ak, Ali
   Zerman, Emin
   Quach, Maurice
   Chetouani, Aladine
   Smolic, Aljosa
   Valenzise, Giuseppe
   Le Callet, Patrick
TI BASICS: Broad Quality Assessment of Static Point Clouds in a Compression
   Scenario
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud quality; 3D models; point cloud compression; subjective
   quality assessment; dataset
AB Point clouds have become increasingly prevalent in representing 3D scenes within virtual environments, alongside 3D meshes. Their ease of capture has facilitated a wide array of applications on mobile devices, from smartphones to autonomous vehicles. Notably, point cloud compression has reached an advanced stage and has been standardized. However, the availability of quality assessment datasets, which are essential for developing improved objective quality metrics, remains limited. In this paper, we introduce BASICS, a large-scale quality assessment dataset tailored for static point clouds. The BASICS dataset comprises 75 unique point clouds, each compressed with four different algorithms including a learning-based method, resulting in the evaluation of nearly 1500 point clouds by 3500 unique participants. Furthermore, we conduct a comprehensive analysis of the gathered data, benchmark existing point cloud quality assessment metrics and identify their limitations. By publicly releasing the BASICS dataset, we lay the foundation for addressing these limitations and fostering the development of more precise quality metrics.
C1 [Ak, Ali; Le Callet, Patrick] Nantes Univ, Ecole Cent Nantes, CNRS, LS2N, F-44000 Nantes, France.
   [Zerman, Emin] Mid Sweden Univ, Dept Comp & Elect Engn, SE-85170 Sundsvall, Sweden.
   [Quach, Maurice] Univ Paris Saclay, CNRS, Cent Supelec, Lab Signaux & Syst UMR 8506, F-91190 Gif Sur Yvette, France.
   [Chetouani, Aladine] Univ Orleans, Lab PRISME, F-45072 Orleans, France.
   [Smolic, Aljosa] Lucerne Univ Appl Sci & Arts HSLU, CH-6020 Rotkreuz, Switzerland.
   [Valenzise, Giuseppe] Univ Paris Saclay, CNRS, Cent Supelec, Lab Signaux & Syst UMR 8506, F-91190 Gif sur Yvette, France.
   [Le Callet, Patrick] Inst Univ France IUF, F-75005 Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); Nantes Universite;
   Ecole Centrale de Nantes; Mid-Sweden University; Universite Paris Cite;
   Centre National de la Recherche Scientifique (CNRS); Universite Paris
   Saclay; Universite de Orleans; Centre National de la Recherche
   Scientifique (CNRS); Universite Paris Cite; Universite Paris Saclay;
   Institut Universitaire de France
RP Ak, A (corresponding author), Nantes Univ, Ecole Cent Nantes, CNRS, LS2N, F-44000 Nantes, France.
EM ali.ak@univ-nantes.fr; emin.zerman@miun.se;
   maurice.quach@l2s.centralesupelec.fr; aladine.chetouani@univ-orleans.fr;
   aljosa.smolic@hslu.ch; giuseppe.valenzise@l2s.centralesupelec.fr;
   patrick.lecallet@univ-nantes.fr
OI chetouani, aladine/0000-0002-2066-4707; Le callet,
   Patrick/0000-0002-2143-7063; Ak, Ali/0000-0002-8572-3739
FU European Union#x0027;s Horizon 2020 Research and Innovation Programme
   under the Marie Sklodowska-Curie
FX No Statement Available
NR 0
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6730
EP 6742
DI 10.1109/TMM.2024.3355642
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600039
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Cao, SD
   Chai, WH
   Hao, SY
   Zhang, YT
   Chen, HY
   Wang, GA
AF Cao, Shidong
   Chai, Wenhao
   Hao, Shengyu
   Zhang, Yanting
   Chen, Hangyue
   Wang, Gaoang
TI DiffFashion: Reference-Based Fashion Design With Structure-Aware
   Transfer by Diffusion Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Task analysis; Noise reduction; Training; Semantics;
   Electronic mail; Artificial intelligence; Fashion design; diffusion
   models; structure-aware
AB Image-based fashion design with AI techniques has attracted increasing attention in recent years. We focus on the reference-based fashion design task, where we aim to combine a reference appearance image and a clothing image to generate a new fashion clothing image. Although existing diffusion-based image translation methods have enabled flexible style transfer, it is often difficult to transfer the appearance of the image realistically during reverse diffusion. When the referenced appearance domain greatly differs from the source domain, it often leads to the collapse in the translation. To tackle this issue, we present a novel diffusion model-based unsupervised structure-aware transfer method, namely DiffFashion. Our method is free of model tuning and structure-preserving and has high flexibility in transferring from images with large domain gaps. Specifically, based on the optimal transport properties, we keep a shared latent across the clothing image and reference appearance image to bridge the gap between the two domains in the denoising process, and the latent of the reference image is gradually adapted to the clothing domain. Simultaneously, the structure is transferred from the source clothing to the output fashion image with mixed guidance, including pre-trained Vision Transformer (ViT) guidance and a foreground mask guidance, to further preserve the structure and appearance semantics from source and reference images. Our experimental results show that the proposed method outperforms state-of-the-art baseline models, generating more realistic images in the fashion design task.
C1 [Cao, Shidong; Chai, Wenhao; Hao, Shengyu; Wang, Gaoang] Zhejiang Univ, Univ Illinois Urbana Champaign Inst, Haining 314400, Peoples R China.
   [Zhang, Yanting] Donghua Univ, Shanghai 201620, Peoples R China.
   [Chen, Hangyue] Hangzhou Dianzi Univ, Hangzhou 310005, Peoples R China.
   [Wang, Gaoang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Wang, Gaoang] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
C3 Zhejiang University; Donghua University; Hangzhou Dianzi University;
   Zhejiang University
RP Wang, GA (corresponding author), Zhejiang Univ, Univ Illinois Urbana Champaign Inst, Haining 314400, Peoples R China.; Chen, HY (corresponding author), Hangzhou Dianzi Univ, Hangzhou 310005, Peoples R China.
EM 22271126@zju.edu.cn; wenhaochai.19@intl.zju.edu.cn;
   shengyuhao@zju.edu.cn; ytzhang@dhu.edu.cn; chy@hdu.edu.cn;
   gaoangwang@intl.zju.edu.cn
OI Chai, Wenhao/0000-0003-2611-0008; hao, shengyu/0000-0002-8652-8556;
   Wang, Gaoang/0000-0002-8403-1538
FU National Key Research and Development Program of China
FX No Statement Available
CR Aggarwal P., 2018, Fashion product images (small), data retrieved from Kaggle
   Amir Shir, 2021, arXiv
   [Anonymous], 2017, Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS'17, DOI DOI 10.5555/3294771.3294816
   Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Couairon G., 2023, PROC INT C LEARN REP
   Cui YR, 2018, COMPUT GRAPH FORUM, V37, P109, DOI 10.1111/cgf.13552
   Date P, 2017, Arxiv, DOI arXiv:1707.09899
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dong H., 2017, Unsupervised image-to-image translation with generative adversarial networks
   Gal R., 2022, PROC 11 INT C LEARN
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Grigorev A., 2020, Clothing dataset (full, high resolu- tion)
   He K., 2017, P IEEE INT C COMP VI, P2961
   Hensel M, 2017, ADV NEUR IN, V30
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang SH, 2022, IEEE T NEUR NET LEAR, V33, P4538, DOI 10.1109/TNNLS.2021.3057892
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Khrulkov V., 2023, PROC INT C LEARN REP
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Kim K., 2021, Proc. NeurIPS, V34, P864
   Kim Sunnie S. Y., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P246, DOI 10.1007/978-3-030-58574-7_15
   Kwon G., 2023, PROC INT C LEARN REP
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47
   Nichol A, 2021, PR MACH LEARN RES, V139
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park T., 2020, Advances in Neural Information Processing Systems, V33, P7198
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Risken H., 1996, The Fokker-Planck Equation
   Saharia C., 2022, PROC ACM SIGGRAPH C, P1
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Saito Kuniaki, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P382, DOI 10.1007/978-3-030-58580-8_23
   Sbai O, 2019, LECT NOTES COMPUT SC, V11131, P37, DOI 10.1007/978-3-030-11015-4_5
   Song J., 2020, PROC INT C LEARN REP
   Song Y., 2020, PROC INT C LEARN REP
   Tumanyan N, 2022, PROC CVPR IEEE, P10738, DOI 10.1109/CVPR52688.2022.01048
   Yan H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3567596
   Yan H, 2023, IEEE T MULTIMEDIA, V25, P2323, DOI [10.1109/TCSS.2022.3161996, 10.1109/TMM.2022.3146010]
   Yang S, 2022, PROC CVPR IEEE, P18311, DOI 10.1109/CVPR52688.2022.01779
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
   Yuan CX, 2020, Arxiv, DOI arXiv:2007.10947
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou DL, 2023, IEEE T MULTIMEDIA, V25, P4986, DOI 10.1109/TMM.2022.3185894
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 0
Z9 0
U1 16
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3962
EP 3975
DI 10.1109/TMM.2023.3318297
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300009
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Chen, BL
   Zhu, LY
   Zhu, HW
   Yang, WH
   Song, LQ
   Wang, SQ
AF Chen, Baoliang
   Zhu, Lingyu
   Zhu, Hanwei
   Yang, Wenhan
   Song, Linqi
   Wang, Shiqi
TI Gap-Closing Matters: Perceptual Quality Evaluation and Optimization of
   Low-Light Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image quality assessment; Low-light image enhancement;
   optimization
ID FREE-ENERGY PRINCIPLE; FRAMEWORK; RETINEX; FUSION
AB There is a growing consensus in the research community that the optimization of low-light image enhancement approaches should be guided by the visual quality perceived by end users. Despite the substantial efforts invested in the design of low-light enhancement algorithms, there has been comparatively limited focus on assessing subjective and objective quality systematically. To mitigate this gap and provide a clear path towards optimizing low-light image enhancement for better visual quality, we propose a gap-closing framework. In particular, our gap-closing framework starts with the creation of a large-scale dataset for Subjective QUality Assessment of REconstructed LOw-Light Images (SQUARE-LOL). This database serves as the foundation for studying the quality of enhanced images and conducting a comprehensive subjective user study. Subsequently, we propose an objective quality assessment measure that plays a critical role in bridging the gap between visual quality and enhancement. Finally, we demonstrate that our proposed objective quality measure can be incorporated into the process of optimizing the learning of the enhancement model toward perceptual optimality. We validate the effectiveness of our proposed framework through both the accuracy of quality prediction and the perceptual quality of image enhancement.
C1 [Chen, Baoliang; Zhu, Lingyu; Zhu, Hanwei; Song, Linqi; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Yang, Wenhan] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Wang, Shiqi] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen, Peoples R China.
C3 City University of Hong Kong; Peng Cheng Laboratory; City University of
   Hong Kong; Shenzhen Research Institute, City University of Hong Kong
RP Wang, SQ (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
EM blchen6-c@my.cityu.edu.hk; lingyzhu-c@my.cityu.edu.hk;
   hanwei.zhu@my.cityu.edu.hk; yangwh@pcl.ac.cn; linqi.song@cityu.edu.hk;
   shiqwang@cityu.edu.hk
OI CHEN, Baoliang/0000-0003-4884-6956; Song, Linqi/0000-0003-2756-4984;
   ZHU, Lingyu/0000-0001-7608-7913; hanwei, zhu/0000-0002-0894-0561
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   B. Series, 2012, RECOMMENDATION ITU R, P13
   Banham MR, 1997, IEEE SIGNAL PROC MAG, V14, P24, DOI 10.1109/79.581363
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chen BL, 2022, Arxiv, DOI arXiv:2202.09738
   Chen BL, 2022, IEEE T CIRC SYST VID, V32, P1903, DOI 10.1109/TCSVT.2021.3088505
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   Coltuc D, 2006, IEEE T IMAGE PROCESS, V15, P1143, DOI 10.1109/TIP.2005.864170
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hussain MA, 2016, I C DEV ESYST ENG, P289, DOI 10.1109/DeSE.2016.45
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   LAND EH, 1964, AM SCI, V52, P247
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee CH, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P43, DOI 10.1109/SITIS.2013.19
   Li DQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P789, DOI 10.1145/3394171.3413804
   Li S. Z., 2009, MARKOV RANDOM FIELD
   Li Z, 2021, IEEE T MULTIMEDIA, V23, P306, DOI 10.1109/TMM.2020.2978640
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nakai K, 2013, I S INTELL SIG PROC, P445, DOI 10.1109/ISPACS.2013.6704591
   Ni ZK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1697, DOI 10.1145/3394171.3413839
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Sadou B., 2019, PROC 5 INT C SIGNAL, P13
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Wan RJ, 2023, IEEE T MULTIMEDIA, V25, P8006, DOI 10.1109/TMM.2022.3232206
   Wan WT, 2019, IEEE I CONF COMP VIS, P3404, DOI 10.1109/ICCV.2019.00350
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C., 2018, PROC BRIT MACH VIS C, P1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Yang Y, 2023, IEEE T CIRC SYST VID, V33, P4645, DOI 10.1109/TCSVT.2023.3245625
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Zamir SW, 2023, IEEE T PATTERN ANAL, V45, P1934, DOI 10.1109/TPAMI.2022.3167175
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang Y., 2020, arXiv
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 74
TC 1
Z9 1
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3430
EP 3443
DI 10.1109/TMM.2023.3312851
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200043
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Chen, WL
   Cai, BQ
   Zheng, SM
   Zhao, TS
   Gu, K
AF Chen, Weiling
   Cai, Boqin
   Zheng, Sumei
   Zhao, Tiesong
   Gu, Ke
TI Perception-and-Cognition-Inspired Quality Assessment for Sonar Image
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sonar image; super-resolution (SR); image quality assessment (IQA);
   task-oriented; hierarchical feature fusion
ID LOW-LEVEL; RECOGNITION; CONTOUR
AB Due to the light-independent imaging characteristics, sonar images play a crucial role in fields such as underwater detection and rescue. However, the resolution of sonar images is negatively correlated with the imaging distance. To overcome this limitation, Super-Resolution (SR) techniques have been introduced into sonar image processing. Nevertheless, it is not always guaranteed that SR maintains the utility of the image. Therefore, quantifying the utility of SR reconstructed Sonar Images (SRSIs) can facilitate their optimization and usage. Existing Image Quality Assessment (IQA) methods are inadequate for evaluating SRSIs as they fail to consider both the unique characteristics of sonar images and reconstruction artifacts while meeting task requirements. In this paper, we propose a Perception-and-Cognition-inspired quality Assessment method for Sonar image Super-resolution (PCASS). Our approach incorporates a hierarchical feature fusion-based framework inspired by the cognitive process in the human brain to comprehensively evaluate SRSIs' quality under object recognition tasks. Additionally, we select features at each level considering visual perception characteristics introduced by SR reconstruction artifacts such as texture abundance, contour details, and semantic information to measure image quality accurately. Importantly, our method does not require training data and is suitable for scenarios with limited available images. Experimental results validate its superior performance.
C1 [Chen, Weiling; Cai, Boqin; Zheng, Sumei; Zhao, Tiesong] Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transmi, Fuzhou 350116, Peoples R China.
   [Chen, Weiling; Zhao, Tiesong] Fuzhou Univ, Fujian Sci & Technol Innovat Lab Optoelect Informa, Fuzhou 350116, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Fac Informat Technol, Engn Res Ctr Intelligent Percept & Autonomous Cont, Beijing 100124, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Beijing Artificial Intelligence Inst, Beijing 100124, Peoples R China.
C3 Fuzhou University; Fujian Science & Technology Innovation Laboratory for
   Optoelectronic Information of China; Fuzhou University; Beijing
   University of Technology; Beijing University of Technology
RP Zhao, TS (corresponding author), Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transmi, Fuzhou 350116, Peoples R China.
EM weiling.chen@fzu.edu.cn; 221120099@fzu.edu.cn; 201127086@fzu.edu.cn;
   t.zhao@fzu.edu.cn; guke.doctor@gmail.com
OI Cai, Boqin/0009-0007-9216-1432
FU National Science Foundation of China
FX No Statement Available
CR Bare B, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1223, DOI 10.1109/ICASSP.2018.8461931
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Beron J, 2020, IEEE ACCESS, V8, P143201, DOI 10.1109/ACCESS.2020.3014497
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Chen Q, 2022, IEEE I C VI COM I PR, DOI 10.1109/VCIP56404.2022.10008885
   Chen WL, 2021, IEEE T MULTIMEDIA, V23, P1008, DOI 10.1109/TMM.2020.2991546
   Chen WL, 2020, IEEE T CIRC SYST VID, V30, P334, DOI 10.1109/TCSVT.2019.2890878
   Chen WL, 2019, IEEE T IMAGE PROCESS, V28, P5336, DOI 10.1109/TIP.2019.2910666
   Chen WL, 2017, IEEE IMAGE PROC, P176, DOI 10.1109/ICIP.2017.8296266
   Cui YL, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3205928
   Danaci EG, 2016, PATTERN RECOGN LETT, V84, P185, DOI 10.1016/j.patrec.2016.09.015
   Dijkstra N, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05888-8
   Fang YM, 2016, IEEE IMAGE PROC, P2057, DOI 10.1109/ICIP.2016.7532720
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   GORDON HR, 1989, LIMNOL OCEANOGR, V34, P1389, DOI 10.4319/lo.1989.34.8.1389
   Grigorescu C, 2004, IMAGE VISION COMPUT, V22, P609, DOI 10.1016/j.imavis.2003.12.004
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hong H, 2016, NAT NEUROSCI, V19, P613, DOI 10.1038/nn.4247
   HOU HS, 1978, IEEE T ACOUST SPEECH, V26, P508
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li YW, 2019, IEEE ACCESS, V7, P91912, DOI 10.1109/ACCESS.2019.2927032
   Ling SY, 2019, IEEE IMAGE PROC, P1735, DOI [10.1109/ICIP.2019.8803105, 10.1109/icip.2019.8803105]
   Ling SY, 2017, IEEE INT CON MULTI, P79, DOI 10.1109/ICME.2017.8019431
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu X., 2018, P 10 IAPR WORKSH PAT, P1
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Long H, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3248605
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Myers V, 2010, IEEE SIGNAL PROC LET, V17, P683, DOI 10.1109/LSP.2010.2051574
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Qu ZG, 2010, IEEE IMAGE PROC, P1937, DOI 10.1109/ICIP.2010.5651292
   Rouse DM, 2009, IEEE IMAGE PROC, P2217, DOI 10.1109/ICIP.2009.5413882
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Scott ET, 2016, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2016.7532327
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh D, 2021, IEEE INT CONF COMP V, P3734, DOI 10.1109/ICCVW54120.2021.00417
   Summerfield C, 2009, TRENDS COGN SCI, V13, P403, DOI 10.1016/j.tics.2009.06.003
   Thomas CT, 2022, OCEANS-IEEE, DOI 10.1109/OCEANSChennai45887.2022.9775508
   Ungerleider Leslie G., 1994, Current Opinion in Neurobiology, V4, P157, DOI 10.1016/0959-4388(94)90066-3
   Valdenegro-Toro M, 2019, 2019 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR), DOI 10.1109/ecmr.2019.8870959
   Vu CT, 2009, IEEE IMAGE PROC, P3101, DOI 10.1109/ICIP.2009.5414468
   Wan WF, 2018, IEEE INT CON MULTI
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang YM, 2004, COMPUT VIS IMAGE UND, V93, P328, DOI 10.1016/j.cviu.2003.10.006
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Xiaoli Zhou, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563128
   Xu S, 2010, IEEE GEOSCI REMOTE S, V7, P366, DOI 10.1109/LGRS.2009.2035644
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yang CZ, 2020, IEEE ACCESS, V8, P157587, DOI 10.1109/ACCESS.2020.3019800
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang KF, 2014, IEEE T IMAGE PROCESS, V23, P5020, DOI 10.1109/TIP.2014.2361210
   Yang LK, 2019, CHIN CONTR CONF, P3571, DOI [10.23919/chicc.2019.8865120, 10.23919/ChiCC.2019.8865120]
   Yang X, 2022, ARTIF INTELL REV, V55, P5263, DOI 10.1007/s10462-021-10130-z
   Zhang HQ, 2022, IET IMAGE PROCESS, V16, P992, DOI 10.1049/ipr2.12199
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao TS, 2021, IEEE T MULTIMEDIA, V24, P3570, DOI 10.1109/TMM.2021.3102401
   Zheng SM, 2022, OCEANS-IEEE, DOI 10.1109/OCEANS47191.2022.9977189
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou F, 2019, IEEE T IMAGE PROCESS, V28, P3528, DOI 10.1109/TIP.2019.2898638
   Zhou W, 2021, INT WORK QUAL MULTIM, P61, DOI 10.1109/QoMEX51781.2021.9465479
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
NR 68
TC 2
Z9 2
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6398
EP 6410
DI 10.1109/TMM.2024.3349929
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600032
DA 2024-08-05
ER

PT J
AU Gui, J
   Cong, XF
   He, L
   Tang, YY
   Kwok, JTY
AF Gui, Jie
   Cong, Xiaofeng
   He, Lei
   Tang, Yuan Yan
   Kwok, James Tin-Yau
TI Illumination Controllable Dehazing Network based on Unsupervised Retinex
   Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image dehazing; illumination controllable; retinex; transformer;
   unsupervised prior
AB On the one hand, the dehazing task is an ill-posedness problem, which means that no unique solution exists. On the other hand, the dehazing task should take into account the subjective factor, which is to give the user selectable dehazed images rather than a single result. Therefore, this paper proposes a multi-output dehazing network by introducing illumination controllable ability, called IC-Dehazing. The proposed IC-Dehazing can change the illumination intensity by adjusting the factor of the illumination controllable module, which is realized based on the interpretable Retinex model. Moreover, the backbone dehazing network of IC-Dehazing consists of a Transformer with double decoders for high-quality image restoration. Further, the prior-based loss function and unsupervised training strategy enable IC-Dehazing to complete the parameter learning process without the need for paired data. To demonstrate the effectiveness of the proposed IC-Dehazing, quantitative and qualitative experiments are conducted. Code is available at https://github.com/Xiaofeng-life/ICDehazing.
C1 [Gui, Jie; Cong, Xiaofeng] Southeast Univ, Sch Cyber Sci Engn, Nanjing 210000, Peoples R China.
   [Gui, Jie] Purple Mt Labs, Nanjing, Peoples R China.
   [Gui, Jie] Southeast Univ, Engn Res Ctr Blockchain Applicat Supervis & Manage, Minist Educ, Nanjing 210000, Peoples R China.
   [He, Lei] Purple Mt Labs, Nanjing 210000, Peoples R China.
   [Tang, Yuan Yan] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Kwok, James Tin-Yau] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong 999077, Peoples R China.
C3 Southeast University - China; Southeast University - China; University
   of Macau; Hong Kong University of Science & Technology
RP Cong, XF (corresponding author), Southeast Univ, Sch Cyber Sci Engn, Nanjing 210000, Peoples R China.
EM guijie@seu.edu.cn; cxf_svip@163.com; helei@pmlabs.com.cn;
   yytang@um.edu.mo; jamesk@cse.ust.hk
OI Cong, Xiaofeng/0000-0001-8850-3507
FU National Key Ramp;D Program of China
FX No Statement Available
CR Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Cong XF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1828, DOI 10.1145/3394171.3413876
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fan TH, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P410, DOI 10.1109/ICIVC.2017.7984588
   Galdran A, 2018, PROC CVPR IEEE, P8212, DOI 10.1109/CVPR.2018.00857
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Gui J, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3576918
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hongyu Li, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P2577, DOI 10.1145/3474085.3475432
   Dhara SK, 2021, IEEE T CIRC SYST VID, V31, P2076, DOI 10.1109/TCSVT.2020.3007850
   Kim G, 2021, IEEE T IMAGE PROCESS, V30, P5452, DOI 10.1109/TIP.2021.3084743
   Kulkarni A., 2023, P WINT C APPL COMP V, P6305
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li PY, 2021, IEEE T IMAGE PROCESS, V30, P1100, DOI 10.1109/TIP.2020.3040075
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li Y., 2022, P IEEE CVF C COMP VI, P5841
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liang Y., 2022, P INT JOINT C ART IN, P1
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu H, 2022, PROC CVPR IEEE, P5821, DOI 10.1109/CVPR52688.2022.00574
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shin J, 2022, IEEE T MULTIMEDIA, V24, P245, DOI 10.1109/TMM.2021.3050053
   Shyam P, 2021, AAAI CONF ARTIF INTE, V35, P9657
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Wang YZ, 2022, IEEE T INTELL TRANSP, V23, P20368, DOI 10.1109/TITS.2022.3170328
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Yang Y, 2022, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR52688.2022.00208
   Yi QS, 2022, IEEE T MULTIMEDIA, V24, P3114, DOI 10.1109/TMM.2021.3093724
   Yin SB, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108021
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang JA, 2022, IEEE T CYBERNETICS, V52, P11187, DOI 10.1109/TCYB.2021.3070310
   Zhang SD, 2023, IEEE T CYBERNETICS, V53, P454, DOI 10.1109/TCYB.2021.3124231
   Zhang XQ, 2022, IEEE T CIRC SYST VID, V32, P510, DOI 10.1109/TCSVT.2021.3067062
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zhu ZQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024335
NR 59
TC 1
Z9 1
U1 20
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4819
EP 4830
DI 10.1109/TMM.2023.3326881
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100050
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Hui, WJ
   Zhu, ZF
   Gu, GH
   Liu, MQ
   Zhao, Y
AF Hui, Wenjun
   Zhu, Zhenfeng
   Gu, Guanghua
   Liu, Meiqin
   Zhao, Yao
TI Implicit-Explicit Motion Learning for Video Camouflaged Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video camouflaged object detection; semantic segmentation; optical flow
   field; deep learning
AB Video camouflaged object detection aims to identify objects that are visually concealed within the surroundings in a video. Most of the existing methods fall into analyzing the implicit inter-frame motion to capture the camouflaged object. However, due to a lack of exploring the prior explicit motion of the camouflaged object, these works generally encounter difficulty in capturing the complete camouflaged object. To address this issue, we propose to integrate implicit and explicit motion learning into a unified framework, namely Implicit-Explicit Motion Learning network (IMEX), for video camouflaged object detection. Specifically, to promote the identifiability of the camouflaged object, a cross-scale representation fusion was proposed for global inter-frame alignment. By establishing cross-scale temporal-spatial association and aggregating the temporal-spatial attentive representations, it also achieves an elimination of the implicit motion of inter-frame to some extent. Moreover, to further improve the discriminability of boundary regions of the detected object, an explicit motion-induced consistency preserving of camouflaged objects is proposed, in which the prior boundary-aware explicit motion field is leveraged to supervise the consistency of camouflaged objects in consecutive frames. Extensive experiments show that our proposed IMEX achieves substantial performance improvements by a large margin.
C1 [Hui, Wenjun; Zhu, Zhenfeng; Liu, Meiqin; Zhao, Yao] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Hui, Wenjun; Zhu, Zhenfeng; Liu, Meiqin; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Gu, Guanghua] Yanshan Univ, Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao 066004, Peoples R China.
   [Gu, Guanghua] Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Yanshan
   University; Yanshan University
RP Liu, MQ (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.; Liu, MQ (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM wenjunhui@bjtu.edu.cn; zhfzhu@bjtu.edu.cn; guguanghua@ysu.edu.cn;
   mqliu@bjtu.edu.cn; yzhao@bjtu.edu.cn
OI Zhao, Yao/0000-0002-8581-9554
FU National Natural Science Foundation of China
FX No Statement Available
CR Bideau P., 2018, PROC EUR C COMPUT VI, P1
   Bideau P, 2016, LECT NOTES COMPUT SC, V9912, P433, DOI 10.1007/978-3-319-46484-8_26
   Cheng BW, 2021, PROC CVPR IEEE, P15329, DOI 10.1109/CVPR46437.2021.01508
   Cheng HK, 2021, ADV NEUR IN, V34
   Cheng XL, 2022, PROC CVPR IEEE, P13854, DOI 10.1109/CVPR52688.2022.01349
   Cuthill IC, 2005, NATURE, V434, P72, DOI 10.1038/nature03312
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Fan D. P., 2021, Scientia Sinica Informationis, V6, P1
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   He CM, 2023, PROC CVPR IEEE, P22046, DOI 10.1109/CVPR52729.2023.02111
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Z, 2023, PROC CVPR IEEE, P5557, DOI 10.1109/CVPR52729.2023.00538
   Ji GP, 2023, MACH INTELL RES, V20, P92, DOI 10.1007/s11633-022-1365-9
   Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14
   Kirillov A., 2023, P IEEE CVF INT C COM, P4015
   Lamdouar H., 2020, PROC ASIAN C COMPUT, P1
   Li MX, 2022, PROC CVPR IEEE, P1322, DOI 10.1109/CVPR52688.2022.00139
   Li XF, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P1116
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu T, 2019, IEEE T IND ELECTRON, V66, P9909, DOI 10.1109/TIE.2019.2893843
   Lyu Y, 2024, IEEE T MULTIMEDIA, V26, P4050, DOI 10.1109/TMM.2023.3295095
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Owens A, 2014, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2014.350
   Pang YW, 2022, PROC CVPR IEEE, P2150, DOI 10.1109/CVPR52688.2022.00220
   Pauschinger Dennis., 2020, Conflict and Society, V6, P108, DOI [10.3167/arcs.2020.060107, DOI 10.3167/ARCS.2020.060107]
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Sun Y., 2022, P INT JOINT C ART IN, P1335, DOI DOI 10.24963/IJCAI.2022/186
   Wise C., 2022, arXiv
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yang C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7157, DOI 10.1109/ICCV48922.2021.00709
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang Y, 2023, IEEE T IMAGE PROCESS, V32, P3580, DOI 10.1109/TIP.2023.3287137
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
NR 41
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7188
EP 7196
DI 10.1109/TMM.2024.3361170
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000055
DA 2024-08-05
ER

PT J
AU Li, JX
   Shi, DS
   Cui, Y
   Guo, DY
   Liu, QS
AF Li, Junxia
   Shi, Deshuo
   Cui, Ying
   Guo, Dongyan
   Liu, Qingshan
TI Adaptive Activation Network for Weakly Supervised Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Weakly supervised semantic segmentation; class activation maps;
   discriminative region; adaptive activation; scale adaptation
AB Class activation maps generated by image classifiers are widely used as priors for image-level weakly supervised semantic segmentation. However, these activation maps mainly focus on the sparse discriminative regions, which has been a bottleneck for the segmentation task. Based on our observations, the activation maps actually capture almost the entire target regions, and some regions with lower activation values are easily to be neglected. Thus, to solve the issue, we propose an adaptive activation network with two branches to recalibrate the low-confidence regions in the activation maps. Specifically, an activation enhancement branch is designed to redistribute the activation values by leveraging attention mechanism. Since multi-scale images can provide complementary information, a scale adaptation branch is paralleled to supervise the activation enhancement branch. The mutual supervision and fusion of the two branches can promote the less-discriminative parts, and deactivate the background regions. Based on them, a simple yet effective denoising module is proposed to further improve the quality of pseudo masks, which makes use of the large scale predictions of the trained segmentation network. Extensive experiments on the PASCAL VOC 2012 and MS COCO 2014 benchmarks show that our method achieves state-of-the-art performance, demonstrating the effectiveness of our algorithm. Code will be made publicly available.
C1 [Li, Junxia; Liu, Qingshan] Nanjing Univ Informat Sci & Technol, CICAEET, Sch Comp, Minist Educ, Nanjing 210044, Peoples R China.
   [Li, Junxia; Liu, Qingshan] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Sch Comp, Minist Educ, Nanjing 210044, Peoples R China.
   [Shi, Deshuo] Nanjing Univ Informat Sci & Technol, Sch Automat, Nanjing 210044, Peoples R China.
   [Cui, Ying; Guo, Dongyan] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; Nanjing University of
   Information Science & Technology; Zhejiang University of Technology
RP Liu, QS (corresponding author), Nanjing Univ Informat Sci & Technol, CICAEET, Sch Comp, Minist Educ, Nanjing 210044, Peoples R China.; Liu, QS (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Sch Comp, Minist Educ, Nanjing 210044, Peoples R China.
EM junxiali99@163.com; shideshuo10@163.com; cuiying@zjut.edu.cn;
   guodongyan@zjut.edu.cn; qsliu@nuist.edu.cn
FU National Major Project of China for New Generation of AI
FX No Statement Available
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Jiang PT, 2022, PROC CVPR IEEE, P16865, DOI 10.1109/CVPR52688.2022.01638
   Li Y., 2021, ICCV
   Zhou TF, 2022, PROC CVPR IEEE, P4289, DOI 10.1109/CVPR52688.2022.00426
NR 4
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6078
EP 6089
DI 10.1109/TMM.2023.3307941
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600019
DA 2024-08-05
ER

PT J
AU Li, Q
   Zu, G
   Xu, H
   Kong, J
   Zhang, YN
   Wang, JZ
AF Li, Qiang
   Zu, Guang
   Xu, Hui
   Kong, Jun
   Zhang, Yanni
   Wang, Jianzhong
TI An Adaptive Dual Selective Transformer for Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal action localization; action recognition; vision transformers;
   video understanding
AB Temporal action localization (TAL), which aims to identify and localize actions in long untrimmed videos, is a challenging task in video understanding. Recent studies have shown that the Transformer and its variants are effective at improving the performance of TAL. The success of the Transformer can be attributed to the use of multi-head self-attention (MHSA) as a token mixer to capture long-term temporal dependencies within the video sequence. However, in the existing Transformer architecture, the features obtained by multiple token mixing (i.e., self-attention) heads are treated equally, which neglects the distinct characteristics of different heads and hampers the exploitation of discriminative information. To this end, we present a new method called the adaptive dual selective Transformer (ADSFormer) for TAL in this paper. The key component in ADSFormer is the dual selective multi-head token mixer (DSMHTM), which integrates multiple feature representations from different token mixing heads by adaptively selecting important features across both the head and channel dimensions. Moreover, we also incorporate our ADSFormer into a pyramid structure so that the multi-scale features obtained can be effectively combined to improve TAL performance. Benefiting from the dual selective multi-head token mixer (DSMHTM) and pyramid feature combination, ADSFormer outperforms several state-of-the-art methods on four challenging benchmark datasets: THUMOS14, MultiTHUMOS, EPIC-KITCHENS-100 and ActivityNet-1.3.
C1 [Li, Qiang; Zu, Guang; Xu, Hui; Kong, Jun; Zhang, Yanni; Wang, Jianzhong] Northeast Normal Univ, Sch Informat Sci & Technol, Changchun 130117, Peoples R China.
   [Kong, Jun] Northeast Normal Univ, Key Lab Appl Stat MOE, Changchun 130117, Peoples R China.
C3 Northeast Normal University - China; Northeast Normal University - China
RP Kong, J; Wang, JZ (corresponding author), Northeast Normal Univ, Sch Informat Sci & Technol, Changchun 130117, Peoples R China.
EM liq782@nenu.edu.cn; zug727@nenu.edu.cn; xuh504@nenu.edu.cn;
   kongjun@nenu.edu.cn; zhangyn500@nenu.edu.cn; wangjz019@nenu.edu.cn
OI Li, Qiang/0009-0003-4955-8582; Wang, Jianzhong/0000-0002-6867-3282
FU NSFC
FX No Statement Available
CR Alwassel H, 2021, IEEE INT CONF COMP V, P3166, DOI 10.1109/ICCVW54120.2021.00356
   Alwassel H, 2018, LECT NOTES COMPUT SC, V11207, P264, DOI 10.1007/978-3-030-01219-9_16
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Buch S., 2017, BMVC
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang Shuning, 2022, HCMA '22: Proceedings of the 3rd International Workshop on Human-Centric Multimedia Analysis, P41, DOI 10.1145/3552458.3556443
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen TL, 2022, PROC CVPR IEEE, P12010, DOI 10.1109/CVPR52688.2022.01171
   Cheng F, 2022, LECT NOTES COMPUT SC, V13694, P503, DOI 10.1007/978-3-031-19830-4_29
   Choromanski K. M., 2021, PROC9TH INT C LEARN, P1
   Dai R, 2022, PROC CVPR IEEE, P20009, DOI 10.1109/CVPR52688.2022.01941
   Dai R, 2021, IEEE WINT CONF APPL, P2969, DOI 10.1109/WACV48630.2021.00301
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Damen D, 2022, INT J COMPUT VISION, V130, P33, DOI 10.1007/s11263-021-01531-2
   Dosovitskiy A., 2021, PROC ICLR
   Duda R.O., 2001, Pattern Classification
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Feng XL, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P717, DOI 10.1109/TDPVT.2002.1024148
   Gan MG, 2023, IEEE T MULTIMEDIA, V25, P3799, DOI 10.1109/TMM.2022.3166025
   Gong C., 2021, arXiv, DOI 10.48550/arXiv.2104.12753
   Gong GQ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102850
   Guibas J., 2021, PROC INT C LEARN REP, P1
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Lee-Thorp J, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P4296
   Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu Hong, 2021, ADV NEURAL INFORM PR, V34
   Liu QY, 2020, AAAI CONF ARTIF INTE, V34, P11612
   Liu XL, 2022, IEEE T IMAGE PROCESS, V31, P5427, DOI 10.1109/TIP.2022.3195321
   Liu XL, 2021, PROC CVPR IEEE, P12591, DOI 10.1109/CVPR46437.2021.01241
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Loshchilov I., 2017, INT C LEARNING REPRE
   Loshchilov I., 2018, INT C LEARN REPR
   Nag S, 2022, LECT NOTES COMPUT SC, V13663, P645, DOI 10.1007/978-3-031-20062-5_37
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Rao YM, 2023, IEEE T PATTERN ANAL, V45, P10960, DOI 10.1109/TPAMI.2023.3263824
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Shazeer N, 2020, Arxiv, DOI arXiv:2003.02436
   Shi DF, 2023, PROC CVPR IEEE, P18857, DOI 10.1109/CVPR52729.2023.01808
   Shi DF, 2022, LECT NOTES COMPUT SC, V13670, P105, DOI 10.1007/978-3-031-20080-9_7
   Sridhar D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13719, DOI 10.1109/ICCV48922.2021.01348
   Tae-Kyung Kang, 2022, 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), P365, DOI 10.1109/SMC53654.2022.9945289
   Tan J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13506, DOI 10.1109/ICCV48922.2021.01327
   Tan Jing, 2022, ADV NEURAL INFORM PR, V35, P15268
   Tang TN, 2023, Arxiv, DOI arXiv:2303.09055
   Tirupattur P, 2021, PROC CVPR IEEE, P1460, DOI 10.1109/CVPR46437.2021.00151
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang L., 2021, Temporal action proposal generation with transformers
   Wang LM, 2023, PROC CVPR IEEE, P14549, DOI 10.1109/CVPR52729.2023.01398
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wei DF, 2022, COMPUT VIS IMAGE UND, V222, DOI 10.1016/j.cviu.2022.103484
   Xia K, 2023, IEEE T MULTIMEDIA, V25, P9425, DOI 10.1109/TMM.2023.3252176
   Xu M., 2020, P IEEE CVF C COMP VI, P10156
   Yang Le, 2022, IEEE Trans Image Process, VPP, DOI 10.1109/TIP.2022.3180925
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Yueran Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P121, DOI 10.1007/978-3-030-58604-1_8
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang CL, 2022, LECT NOTES COMPUT SC, V13664, P492, DOI 10.1007/978-3-031-19772-7_29
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhao C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13638, DOI 10.1109/ICCV48922.2021.01340
   Zheng T., 2022, arXiv
   Zhu ZX, 2022, AAAI CONF ARTIF INTE, P3644
   Zhu ZX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13496, DOI 10.1109/ICCV48922.2021.01326
NR 75
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7398
EP 7412
DI 10.1109/TMM.2024.3367599
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000063
DA 2024-08-05
ER

PT J
AU Liu, L
   An, JF
   Yuan, SX
   Zhou, WG
   Li, HQ
   Wang, YF
   Tian, Q
AF Liu, Lin
   An, Junfeng
   Yuan, Shanxin
   Zhou, Wengang
   Li, Houqiang
   Wang, Yanfeng
   Tian, Qi
TI Video Demoireing With Deep Temporal Color Embedding and Video-Image
   Invertible Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video demoir & eacute;ing; temporal consistency; color distortion
ID MOIRE PATTERNS; NETWORK; SUPPRESSION; REMOVAL
AB Demoireing is the task of removing moire patterns, which are commonly caused by the interference between the screen and digital cameras. Although research on single image demoireing has made great progress, research on video demoireing has received less attention from the community. Video demoireing poses a new set of challenges. First, most existing video restoration algorithms rely on multi-resolution pixel-based alignment, which can cause damage to the details of the predicted results. Second, these algorithms are based on flow-based loss or relation-based loss, making it difficult to handle the large motions of adjacent frames while keeping temporal consistency intact. To address these challenges, we present a novel deep learning-based approach called the Deep Temporal Color Embedding network (DTCENet) that employs an invertible network to align distortion color patches in a patch-based embedding framework. DTCENet can well preserve details while eliminate color distortions. Furthermore, we introduce a video-image invertible loss function to effectively handle the color inconsistent problem of adjacent frames. Our approach shows promising results in demoir & eacute;ing videos, with improved performance over existing state-of-the-art algorithms. Our method gets about 10% improvements in terms of LPIPS and 10.3% improvements in terms of FID compared with the recent SOTA methods.
C1 [Liu, Lin; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [An, Junfeng] Harbin Inst Technol, CS, Shenzhen 518055, Peoples R China.
   [Yuan, Shanxin] Queen Mary Univ London, Sch Elect Engn & Comp Sci, Digital Environm, London E1 4NS, England.
   [Wang, Yanfeng] Shanghai AI Lab, Shanghai 202150, Peoples R China.
   [Tian, Qi] Huawei, Shenzhen 518129, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Harbin Institute of Technology; University of London; Queen
   Mary University London; Shanghai Artificial Intelligence Laboratory;
   Huawei Technologies
RP Zhou, WG (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.; Yuan, SX (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, Digital Environm, London E1 4NS, England.
EM ll0825@mail.ustc.edu.cn; junfengan1998a@gmail.com;
   shanxin.yuan@qmul.ac.uk; zhwg@ustc.edu.cn; lihq@ustc.edu.cn;
   tian.qi1@huawei.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahn N, 2022, IEEE T CIRC SYST VID, V32, P608, DOI 10.1109/TCSVT.2021.3068985
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Baniya AA, 2024, IEEE T MULTIMEDIA, V26, P540, DOI 10.1109/TMM.2023.3267294
   Buades A, 2020, IEEE T CIRC SYST VID, V30, P4143, DOI 10.1109/TCSVT.2019.2956691
   Chang YL, 2019, IEEE I CONF COMP VIS, P9065, DOI 10.1109/ICCV.2019.00916
   Chen G, 2023, IEEE T CIRC SYST VID, V33, P1787, DOI 10.1109/TCSVT.2022.3215979
   Chen HA, 2022, IEEE T MULTIMEDIA, V24, P2164, DOI 10.1109/TMM.2021.3077140
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen XY, 2021, IEEE T CIRC SYST VID, V31, P594, DOI 10.1109/TCSVT.2020.2980876
   Cui X, 2022, IEEE T CIRC SYST VID, V32, P8327, DOI 10.1109/TCSVT.2022.3190516
   Dai P, 2022, PROC CVPR IEEE, P17601, DOI 10.1109/CVPR52688.2022.01710
   Eilertsen G, 2019, PROC CVPR IEEE, P11168, DOI 10.1109/CVPR.2019.01143
   Fu Y, 2023, IEEE T MULTIMEDIA, V25, P8119, DOI 10.1109/TMM.2022.3233247
   He B., 2020, PROC INT C COMPUT VI, P86
   He B, 2019, IEEE I CONF COMP VIS, P2424, DOI 10.1109/ICCV.2019.00251
   Horvat C, 2021, ADV NEUR IN, V34
   Isobe Takashi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8005, DOI 10.1109/CVPR42600.2020.00803
   Jiang K, 2021, IEEE T IMAGE PROCESS, V30, P7404, DOI 10.1109/TIP.2021.3102504
   Jiang K, 2022, IEEE T NEUR NET LEAR, V33, P378, DOI 10.1109/TNNLS.2020.3027849
   Jo Y, 2021, IEEE COMPUT SOC CONF, P364, DOI 10.1109/CVPRW53098.2021.00046
   Junayed MS, 2023, IEEE T MULTIMEDIA, V25, P7494, DOI 10.1109/TMM.2022.3222932
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Lei CY, 2023, IEEE T PATTERN ANAL, V45, P356, DOI 10.1109/TPAMI.2022.3142071
   Lei CY, 2019, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2019.00387
   Lei Chenyang., 2020, ADV NEURAL INFORM PR, V33, P1083
   Li FY, 2024, IEEE T MULTIMEDIA, V26, P3137, DOI 10.1109/TMM.2023.3307970
   Lin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P86, DOI 10.1007/978-3-030-58601-0_6
   Liu BL, 2018, Arxiv, DOI arXiv:1804.03809
   Liu JL, 2022, IEEE T CIRC SYST VID, V32, P6599, DOI 10.1109/TCSVT.2022.3175171
   Liu JH, 2021, IEEE T CIRC SYST VID, V31, P3242, DOI 10.1109/TCSVT.2020.3037661
   Liu L., 2020, PROC INT C NEURAL IN, P22282
   Liu L, 2022, AAAI CONF ARTIF INTE, P1747
   Liu RX, 2022, IEEE T CIRC SYST VID, V32, P3539, DOI 10.1109/TCSVT.2021.3117964
   Liu S, 2020, IEEE COMPUT SOC CONF, P1751, DOI 10.1109/CVPRW50498.2020.00225
   Liu Y, 2021, PROC CVPR IEEE, P13360, DOI 10.1109/CVPR46437.2021.01316
   Liu YH, 2024, COMPUT VIS MEDIA, V10, P375, DOI 10.1007/s41095-023-0342-8
   Lugmayr Andreas, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P715, DOI 10.1007/978-3-030-58558-7_42
   Luo DY, 2023, IEEE T MULTIMEDIA, V25, P6808, DOI 10.1109/TMM.2022.3214775
   Luo XT, 2020, IEEE COMPUT SOC CONF, P1687, DOI 10.1109/CVPRW50498.2020.00218
   Mao AH, 2022, LECT NOTES COMPUT SC, V13663, P398, DOI 10.1007/978-3-031-20062-5_23
   Mishra D, 2021, IEEE T CIRC SYST VID, V31, P1452, DOI 10.1109/TCSVT.2020.3010627
   Niu YZ, 2023, IEEE T CIRC SYST VID, V33, P3608, DOI 10.1109/TCSVT.2023.3237810
   Niu YZ, 2012, IEEE T CIRC SYST VID, V22, P1037, DOI 10.1109/TCSVT.2012.2189689
   Oh G, 2023, Arxiv, DOI arXiv:2301.07330
   Puthussery D., 2022, PROC INT CONFCOMPUT, P532
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Sasada R, 2003, P SOC PHOTO-OPT INS, V5029, P688, DOI 10.1117/12.479595
   Siddiqui H, 2010, IEEE T IMAGE PROCESS, V19, P746, DOI 10.1109/TIP.2009.2035238
   Sidorov D., 2002, PROC EUR SIGNAL PROC, P1
   Sidorov DN, 2002, P SOC PHOTO-OPT INS, V4671, P895, DOI 10.1117/12.453134
   Sun B, 2014, IEEE T IMAGE PROCESS, V23, P3698, DOI 10.1109/TIP.2014.2332394
   Sun YJ, 2018, IEEE T IMAGE PROCESS, V27, P4160, DOI 10.1109/TIP.2018.2834737
   Tang ZS, 2023, IEEE T CIRC SYST VID, V33, P421, DOI 10.1109/TCSVT.2022.3199472
   Wang H., 2021, PROC IEEE INT C MULT, P1
   Wang H, 2023, IEEE T MULTIMEDIA, V25, P4742, DOI 10.1109/TMM.2022.3181458
   Wang RX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9680, DOI 10.1109/ICCV48922.2021.00956
   Wang WJ, 2020, AAAI CONF ARTIF INTE, V34, P12233
   Wang WJ, 2020, IEEE T IMAGE PROCESS, V29, P9125, DOI 10.1109/TIP.2020.3024018
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wang YF, 2020, IEEE T CIRC SYST VID, V30, P2590, DOI 10.1109/TCSVT.2019.2919482
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei ZP, 2012, MICRON, V43, P170, DOI 10.1016/j.micron.2011.07.009
   Xiao J, 2023, IEEE T MULTIMEDIA, V25, P8972, DOI 10.1109/TMM.2023.3243615
   Xiao Y, 2023, Arxiv, DOI [arXiv:2304.04421, DOI 10.1109/TCSVT.2023.3312321]
   Xiao Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3107352
   Xie Y., 2021, PROC SIGKDD INT C KN
   Xing YZ, 2021, PROC CVPR IEEE, P6283, DOI 10.1109/CVPR46437.2021.00622
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang JC, 1998, SIGNAL PROCESS, V70, P23, DOI 10.1016/S0165-1684(98)00111-X
   Yu X, 2022, LECT NOTES COMPUT SC, V13678, P646, DOI 10.1007/978-3-031-19797-0_37
   Yuan SX, 2020, IEEE COMPUT SOC CONF, P1882, DOI 10.1109/CVPRW50498.2020.00238
   Yue HJ, 2023, IEEE T MULTIMEDIA, V25, P5589, DOI 10.1109/TMM.2022.3198333
   Yue HJ, 2021, IEEE T CIRC SYST VID, V31, P49, DOI 10.1109/TCSVT.2020.2969984
   Zhang DY, 2021, IEEE T CIRC SYST VID, V31, P3954, DOI 10.1109/TCSVT.2020.3044451
   Zhang F, 2021, PROC CVPR IEEE, P4965, DOI 10.1109/CVPR46437.2021.00493
   Zhang Y., 2022, PROCINT C LEARN REPR
   Zheng BL, 2022, IEEE T PATTERN ANAL, V44, P7705, DOI 10.1109/TPAMI.2021.3115139
   Zheng BL, 2020, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR42600.2020.00369
   Zhou K, 2022, PROC CVPR IEEE, P6043, DOI 10.1109/CVPR52688.2022.00596
   Zhu J, 2023, IEEE T MULTIMEDIA, V25, P6821, DOI 10.1109/TMM.2022.3214776
   Zhu L, 2021, IEEE T CIRC SYST VID, V31, P2147, DOI 10.1109/TCSVT.2020.3022707
NR 82
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7386
EP 7397
DI 10.1109/TMM.2024.3366765
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000012
DA 2024-08-05
ER

PT J
AU Liu, MK
   Jiao, LC
   Liu, X
   Li, LL
   Liu, F
   Yang, SY
   Wang, S
   Hou, B
AF Liu, Mengkun
   Jiao, Licheng
   Liu, Xu
   Li, Lingling
   Liu, Fang
   Yang, Shuyuan
   Wang, Shuang
   Hou, Biao
TI Multi-Scale Contourlet Knowledge Guide Learning Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic segmentation; Shape; Image color analysis; Spectral analysis;
   Buildings; Knowledge engineering; Training; Multi-scales;
   multi-directions; pyramidal directional filter bank; polyp segmentation;
   building extraction
ID POLYP SEGMENTATION; ATTENTION NETWORK; ENDOSCOPY IMAGES; TRANSFORM;
   SELECTION
AB For accurate segmentation, effective feature extraction has always been a challenging problem, since the variability of appearance and the fuzziness of object boundaries. Convolutional neural networks have recently gained recognition in feature representation learning. However, it is only conducted in the spatial domain, and lacks effective representation of directionality, singularity and regularity in the spectral domain for anomaly detection of images. This is the key to feature learning representation of high-order singularity. To solve this problem, a multi-scale contourlet knowledge guide learning network is proposed in this paper. It is novel in this sense that, different from the CNNs in the spatial domain, the proposed method learns the multi-scale contourlet sparse representation to obtain more effective and sparse features in multi-scales and multi-directions. Furthermore, the contourlet knowledge guide learning can enhance the representation of spectral domain features. It is shown that the proposed network can learn the multi-level discriminative features and capture the more accurate object boundaries. The segmentation ability in theoretical analysis and experiments on five polyp segmentation datasets (CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS-LaribPolypDB, EndoSceneStill) and two building datasets (Massachusetts, WHU) are compared with developed methods. It must be emphasized that there is potential in effective feature learning representation and the generalization capability of the proposed method in deep learning, recognition and interpretation.
C1 [Liu, Mengkun; Jiao, Licheng; Liu, Xu; Li, Lingling; Liu, Fang; Yang, Shuyuan; Wang, Shuang; Hou, Biao] Xidian Univ, Int Res Ctr Intelligent Percept & Computat, Sch Artificial Intelligence, Minist Educ,Joint Int Res Lab Intelligent Percept, Xian 710071, Peoples R China.
C3 Xidian University
RP Jiao, LC (corresponding author), Xidian Univ, Int Res Ctr Intelligent Percept & Computat, Sch Artificial Intelligence, Minist Educ,Joint Int Res Lab Intelligent Percept, Xian 710071, Peoples R China.
EM mengkunliu31@163.com; lchjiao@mail.xidian.edu.cn; xuliu361@163.com;
   llli@xidian.edu.cn; f63liu@163.com; syyang@xidian.edu.cn;
   shwang@mail.xidian.edu.cn; avcodec@163.com
OI liu, mengkun/0000-0002-5696-5237
FU Key Scientific Technological Innovation Research Project
FX No Statement Available
CR Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197
   Anders U, 1999, NEURAL NETWORKS, V12, P309, DOI 10.1016/S0893-6080(98)00117-8
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Banik D, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3015607
   Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Chang Q., 2023, Medical Imaging 2023: Biomedical Applications in Molecular, Structural, and Functional Imaging, V12468
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Cheng B, 2021, ADV NEUR IN, V34
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Cheung YM, 2017, IEEE T NEUR NET LEAR, V28, P80, DOI 10.1109/TNNLS.2015.2501547
   David E, 2013, 2013 INTERNATIONAL SYMPOSIUM ON SIGNALS, CIRCUITS AND SYSTEMS (ISSCS)
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Ding BS, 2024, IEEE T MULTIMEDIA, V26, P202, DOI 10.1109/TMM.2023.3263078
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Dong B, 2024, Arxiv, DOI [arXiv:2108.06932, DOI 10.48550/ARXIV.2108.06932]
   Donoho DL, 1998, IEEE T INFORM THEORY, V44, P2435, DOI 10.1109/18.720544
   Fang YQ, 2021, IEEE SENS J, V21, P11799, DOI 10.1109/JSEN.2020.3015831
   Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34
   Ganz M, 2012, IEEE T BIO-MED ENG, V59, P2144, DOI 10.1109/TBME.2012.2195314
   Gautam A., 2022, P IEEE REG 10 S, P1
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   Hussain Ashfaq, 2020, Proceedings of Second International Conference on Inventive Research in Computing Applications (ICIRCA 2020), P38, DOI 10.1109/ICIRCA48905.2020.9183385
   Hwang S, 2007, IEEE IMAGE PROC, P1029
   Imtiaz T, 2023, IEEE ACCESS, V11, P129321, DOI 10.1109/ACCESS.2023.3321799
   Jerebko AK, 2002, RADIOLOGY, V225, P257
   Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Ji SP, 2019, IEEE T GEOSCI REMOTE, V57, P574, DOI 10.1109/TGRS.2018.2858817
   Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794
   Kim T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2167, DOI 10.1145/3474085.3475375
   Tomar NK, 2022, Arxiv, DOI arXiv:2206.08985
   Li BP, 2012, IEEE T INF TECHNOL B, V16, P323, DOI 10.1109/TITB.2012.2185807
   Li QL, 2017, 2017 10TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI)
   Li T., 2022, P IEEECVF C COMPUTER, P1246
   Liang Chen, 2022, P ADV NEUR INF PROC, V35, P31360
   Liu MK, 2021, IEEE T NEUR NET LEAR, V32, P2636, DOI 10.1109/TNNLS.2020.3007412
   Liu TT, 2023, EXPERT SYST APPL, V228, DOI 10.1016/j.eswa.2023.120434
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lou JX, 2024, IEEE T MULTIMEDIA, V26, P256, DOI 10.1109/TMM.2023.3263553
   Ma HC, 2020, IEEE T MULTIMEDIA, V22, P1667, DOI 10.1109/TMM.2019.2957990
   Meng D., 2023, IEEE Trans. Multimedia, DOI [10.1109/TMM.2023.3263078, DOI 10.1109/TMM.2023.3263078]
   Mnih V, 2013, Machine learning for aerial image labeling, DOI DOI 10.1109/ICCV.2015.178
   MORAN J, 1985, SCIENCE, V229, P782, DOI 10.1126/science.4023713
   Murugesan B, 2019, IEEE ENG MED BIO, P7223, DOI [10.1109/embc.2019.8857339, 10.1109/EMBC.2019.8857339]
   Nguyen NQ, 2020, IEEE ACCESS, V8, P99495, DOI 10.1109/ACCESS.2020.2995630
   Nguyen NQ, 2019, IEEE ACCESS, V7, P33795, DOI 10.1109/ACCESS.2019.2904094
   Nguyen NQ, 2018, 2018 IEEE FIRST INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND KNOWLEDGE ENGINEERING (AIKE), P208, DOI 10.1109/AIKE.2018.00048
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Park S., 2015, Seoul Nat. Univ., P1
   Patel K, 2021, 2021 18TH CONFERENCE ON ROBOTS AND VISION (CRV 2021), P181, DOI [10.1109/CRV52889.2021.00032, 10.1109/crv52889.2021.00032]
   Ribeiro E, 2016, COMP MED SY, P253, DOI 10.1109/CBMS.2016.39
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sanchez-Gonzalez A, 2018, IEEE INT SYMP SIGNAL, P579, DOI 10.1109/ISSPIT.2018.8642748
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Shen XB, 2022, IEEE T MULTIMEDIA, V24, P1116, DOI 10.1109/TMM.2021.3119868
   Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3
   Song JH, 2022, IEEE T MED IMAGING, V41, P2273, DOI 10.1109/TMI.2022.3162111
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tomar NK, 2022, LECT NOTES COMPUT SC, V13433, P151, DOI 10.1007/978-3-031-16437-8_15
   Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190
   VOORHEES H, 1988, NATURE, V333, P364, DOI 10.1038/333364a0
   Wang JF, 2022, LECT NOTES COMPUT SC, V13433, P110, DOI 10.1007/978-3-031-16437-8_11
   Wang S, 2021, IEEE J BIOMED HEALTH, V25, P514, DOI 10.1109/JBHI.2020.2997760
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66
   Wei X, 2018, IEEE I C NETW INFRAS, P140, DOI 10.1109/ICNIDC.2018.8525753
   Wen JY, 2024, IEEE T MULTIMEDIA, V26, P944, DOI 10.1109/TMM.2023.3273924
   Wen ZJ, 2021, IEEE J BIOMED HEALTH, V25, P1185, DOI 10.1109/JBHI.2020.3015844
   Wu HS, 2023, IEEE T CYBERNETICS, V53, P2610, DOI 10.1109/TCYB.2022.3162873
   Yao JH, 2004, IEEE T MED IMAGING, V23, P1344, DOI 10.1109/TMI.2004.826941
   Yin ZJ, 2022, I S BIOMED IMAGING, DOI 10.1109/ISBI52829.2022.9761402
   Yue GH, 2022, BIOMED SIGNAL PROCES, V78, DOI 10.1016/j.bspc.2022.103846
   Zhang KP, 2024, IEEE T MULTIMEDIA, V26, P737, DOI 10.1109/TMM.2023.3270637
   Zhang L, 2017, COMM COM INF SC, V723, P707, DOI 10.1007/978-3-319-60964-5_62
   Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao XQ, 2021, LECT NOTES COMPUT SC, V12901, P120, DOI 10.1007/978-3-030-87193-2_12
   Zhou TF, 2022, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR52688.2022.00261
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu JB, 2023, PROCESSES, V11, DOI 10.3390/pr11041035
   Zhu M, 2022, IET IMAGE PROCESS, V16, P3617, DOI 10.1049/ipr2.12580
NR 86
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4831
EP 4845
DI 10.1109/TMM.2023.3326949
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100010
DA 2024-08-05
ER

PT J
AU Liu, YX
   Ge, HW
   Wang, Z
   Hou, YQ
   Zhao, MD
AF Liu, Yuxuan
   Ge, Hongwei
   Wang, Zhen
   Hou, Yaqing
   Zhao, Mingde
TI Clothes-Changing Person Re-Identification via Universal Framework With
   Association and Forgetting Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Task analysis; Image color analysis; Germanium; Cameras;
   Robustness; Interference; Person re-identification (Re-ID);
   clothes-changing; association learning; forgetting learning
AB Clothes-changing person re-identification (Re-ID) aims at learning identity-relevant feature representations among clothing-changed persons. Currently, the state-of-the-art methods accomplish this task by using additional assistance (e.g., silhouettes, sketches, clothes labels, etc.) to explore identity-relevant information. However, humans do not require redundant assistance information to retrieve clothing-changed persons. It is commonly known that humans can recall targets they have seen before with a simple reminder. Inspired by human perception, we propose an association and forgetting learning (AFL) framework for clothes-changing person re-identification. Specifically, on the one hand, during the association learning process, the AFL framework constructs association factors for each identity to simulate the reminders found in human perception. Then, the original instances and the explored hardest positive instances are cross-correlated by the association factors to learn identity-relevant features. On the other hand, the model is forced to forget the identity-irrelevant features by the proposed forgetting learning module, which improves the intra-class compactness. Finally, we further propose a clustering relationship exploration (CRE) module to optimize the cluster distribution of clothes-changing instances, which enables AFL to also be effectively applied in unsupervised settings, improving the universal applicability of the model. Extensive experiment results obtained on clothes-changing person Re-ID datasets under supervised and unsupervised settings demonstrate the superiority of the proposed method over the existing state-of-the-art methods.
C1 [Liu, Yuxuan; Ge, Hongwei; Hou, Yaqing] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116023, Peoples R China.
   [Ge, Hongwei] Dalian Univ Technol, Key Lab Social Comp & Cognit Intelligence, Minist Educ, Dalian 116023, Peoples R China.
   [Wang, Zhen] Beihang Univ, Sch Math, Beijing 100091, Peoples R China.
   [Zhao, Mingde] McGill Univ, Sch Comp Sci, Montreal, PQ H3A 0E9, Canada.
   [Zhao, Mingde] Mila Quebec AI Inst, Montreal, PQ H2X 3P7, Canada.
C3 Dalian University of Technology; Dalian University of Technology;
   Beihang University; McGill University
RP Ge, HW (corresponding author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116023, Peoples R China.
EM lyx8880lzc@mail.dlut.edu.cn; hwge@dlut.edu.cn; wangzhen@dlut.edu.cn;
   houyq@dlut.edu.cn; mingde.zhao@mail.mcgill.ca
RI Liu, Yuxuan/IVH-1356-2023
OI Wang, Zhen/0000-0001-7487-3187; Liu, Yuxuan/0000-0003-1168-6645; Ge,
   Hongwei/0000-0002-8937-1515
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson MC, 2004, SCIENCE, V303, P232, DOI 10.1126/science.1089504
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P4285, DOI 10.1109/TMM.2021.3114539
   Chen JX, 2021, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR46437.2021.00805
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Dai Z., 2022, P ASIAN C COMPUTER V, P1142
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Ge YX, 2018, ADV NEUR IN, V31
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   Gu XQ, 2019, IEEE I CONF COMP VIS, P9646, DOI 10.1109/ICCV.2019.00974
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hong PX, 2021, PROC CVPR IEEE, P10508, DOI 10.1109/CVPR46437.2021.01037
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11875, DOI 10.1109/ICCV48922.2021.01168
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jin X, 2022, PROC CVPR IEEE, P14258, DOI 10.1109/CVPR52688.2022.01388
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11165
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li MK, 2022, IEEE T IMAGE PROCESS, V31, P3606, DOI 10.1109/TIP.2022.3173163
   Li S, 2018, IEEE T PATTERN ANAL, V40, P2963, DOI 10.1109/TPAMI.2017.2764893
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YJ, 2021, IEEE WINT CONF APPL, P2431, DOI 10.1109/WACV48630.2021.00248
   Liu XB, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P246, DOI 10.1109/MIPR.2019.00051
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   SAKAI K, 1991, NATURE, V354, P152, DOI 10.1038/354152a0
   Sekeres MJ, 2021, HIPPOCAMPUS, V31, P28, DOI 10.1002/hipo.23260
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Song WF, 2020, IEEE T IMAGE PROCESS, V29, P2860, DOI 10.1109/TIP.2019.2953587
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wan FB, 2020, IEEE COMPUT SOC CONF, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu DM, 2022, IEEE T INF FOREN SEC, V17, P115, DOI 10.1109/TIFS.2021.3075894
   Xuelin Qian, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12624), P71, DOI 10.1007/978-3-030-69535-4_5
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P924, DOI 10.1109/TPAMI.2020.3013379
   Ye M, 2022, IEEE T INF FOREN SEC, V17, P386, DOI 10.1109/TIFS.2021.3139224
   Ye M, 2022, IEEE T IMAGE PROCESS, V31, P379, DOI 10.1109/TIP.2021.3131937
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Zhang AG, 2021, PROC CVPR IEEE, P598, DOI 10.1109/CVPR46437.2021.00066
   Zhang DJ, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P5419
   Zhang D, 2021, MEM COGNITION, V49, P645, DOI 10.3758/s13421-020-01120-7
   Zhang X, 2023, IEEE T COGN DEV SYST, V15, P530, DOI 10.1109/TCDS.2022.3159557
   Zhang YY, 2023, IEEE T INF FOREN SEC, V18, P1554, DOI 10.1109/TIFS.2022.3224853
   Zhang ZY, 2022, IEEE SIGNAL PROC LET, V29, P304, DOI 10.1109/LSP.2021.3134195
   Zhao ZJ, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108120
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou QQ, 2020, IEEE T IMAGE PROCESS, V29, P7578, DOI 10.1109/TIP.2020.3004267
NR 61
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4294
EP 4307
DI 10.1109/TMM.2023.3321498
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100005
DA 2024-08-05
ER

PT J
AU Sun, JY
   Ji, LP
   Zhu, JW
AF Sun, Jiayuan
   Ji, Luping
   Zhu, Jiewen
TI Shared Coupling-Bridge Scheme for Weakly Supervised Local Feature
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Feature extraction; Pipelines; Representation learning;
   Decoding; Location awareness; Convolution; Weakly supervised local
   feature learning; cross normalization; epipolar constraint; fundamental
   matrix
ID ROBUST; RECONSTRUCTION; STEREO; SLAM
AB Local feature learning is believed to be of important significance in classic vision tasks such as visual localization, image matching and 3D reconstruction. Limited by training samples, weakly-supervised strategy has become one of widely-concerned effective schemes for local feature learning. Currently, it still has some weaknesses needing further improvement, mainly including the discrimination power of extracted local descriptors, the localization accuracy of detected keypoints, and the efficiency of weakly-supervised local feature learning. Focusing on promoting the performance of sparse local feature learning with camera pose supervision, this article pertinently proposes a Shared Coupling-bridge scheme with four light-weight yet effective improvements for weakly-supervised local feature (SCFeat) learning. It mainly contains: i) the Feature-Fusion-ResUNet Backbone (F2R-Backbone) for local descriptors learning, ii) a shared coupling-bridge normalization to improve the decoupling training of description network and detection network, iii) an improved detection network with peakiness measurement to detect keypoints and iv) a new reward factor of fundamental matrix error to further optimize feature detection training. Extensive experiments prove that our SCFeat scheme is effective and has wide task adaptability. It could often obtain a state-of-the-art performance on classic image matching and visual localization. Even in terms of 3D reconstruction, it could still achieve competitive results.
C1 [Sun, Jiayuan; Ji, Luping; Zhu, Jiewen] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Ji, LP (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM sunjiayuanro@std.uestc.edu.cn; jiluping@uestc.edu.cn;
   jiewenzhu@std.uestc.edu.cn
RI Sun, Jiayuan/KDN-9673-2024
OI Sun, Jiayuan/0009-0002-9569-131X; Ji, Luping/0000-0002-1200-5218
FU National Natural Science Foundation of China
FX No Statement Available
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Cavalli Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P770, DOI 10.1007/978-3-030-58529-7_45
   Chen HK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6281, DOI 10.1109/ICCV48922.2021.00624
   Deng Y, 2022, IEEE T MULTIMEDIA, V24, P2739, DOI 10.1109/TMM.2021.3087017
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Gao JN, 2023, IEEE T MULTIMEDIA, V25, P5248, DOI 10.1109/TMM.2022.3189247
   Gong XX, 2021, IEEE T MULTIMEDIA, V23, P2820, DOI 10.1109/TMM.2020.3017886
   Harris C, 1988, A combined corner and edge detector, P147, DOI [10.5244/C.2.23.23.1-23.6, DOI 10.5244/C.2.23]
   Hartley R, 2003, Multiple view geometry in computer vision, DOI [10.1016/S0143-8166(01)00145-2, DOI 10.1017/CBO9780511811685]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li K., 2022, P IEEECVF C COMPUTER, P15838
   Li X., 2020, NeurIPS, V33, P17346
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mishchuk A., 2017, ADV NEURAL INFORM PR, V30, P4826, DOI DOI 10.48550/ARXIV.1705.10872
   Mishkin D, 2018, LECT NOTES COMPUT SC, V11213, P287, DOI 10.1007/978-3-030-01240-3_18
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Ono Yuki, 2018, ADV NEURAL INFORM PR, V1, P2
   Pautrat Remi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P707, DOI 10.1007/978-3-030-58536-5_42
   Qianqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P757, DOI 10.1007/978-3-030-58452-8_44
   Revaud J., 2019, P NEURIPS, P12414
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300
   Savinov N, 2017, PROC CVPR IEEE, P3929, DOI 10.1109/CVPR.2017.418
   Schönberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Tian Y., 2020, NeurIPS, P7401
   Tyszkiewicz M., 2020, Advances in Neural Information Processing Systems, V33, P14254
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Vassileios D. P., 2016, P BRIT MACH VIS C
   Wang CW, 2022, AAAI CONF ARTIF INTE, P2388
   Wang CW, 2023, IEEE T MULTIMEDIA, V25, P3989, DOI 10.1109/TMM.2022.3169331
   Wiles O, 2021, PROC CVPR IEEE, P15915, DOI 10.1109/CVPR46437.2021.01566
   Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P3014, DOI 10.1109/TMM.2020.2967645
   Yang BH, 2022, IEEE T MULTIMEDIA, V24, P3947, DOI 10.1109/TMM.2021.3110667
   Yang X, 2021, IEEE T MULTIMEDIA, V23, P4208, DOI 10.1109/TMM.2020.3038323
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zhang HY, 2022, IEEE T MULTIMEDIA, V24, P3835, DOI 10.1109/TMM.2021.3108900
   Zhang ZC, 2021, INT J COMPUT VISION, V129, P821, DOI 10.1007/s11263-020-01399-8
   Zhou QJ, 2021, PROC CVPR IEEE, P4667, DOI 10.1109/CVPR46437.2021.00464
NR 56
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1200
EP 1212
DI 10.1109/TMM.2023.3278172
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700024
DA 2024-08-05
ER

PT J
AU Tang, H
   Zhao, GS
   Gao, J
   Qian, XM
AF Tang, Hao
   Zhao, Guoshuai
   Gao, Jing
   Qian, Xueming
TI Personalized Representation With Contrastive Loss for Recommendation
   Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Recommender systems; Training; Task analysis; Software
   engineering; Predictive models; Markov processes; Personalization;
   contrastive loss; sequential recommendation; uniformity
AB Sequential recommendation mines the user's interaction sequence or time information to get better recommendations and thus is gaining more and more attention. Existing sequential recommendations tend to build new models, and the study of the loss function is seriously neglected. Despite the increasing attention paid to contrastive learning recently, we believe that the key to contrastive learning is contrastive loss(CL), which also provides a new option for sequential recommendation. However, we find it works against the personalized representation of features. First, it is a relative constraint that keeps positive and negative samples away from each other but without an absolute constraint. Second, recent studies have shown that all embeddings should be uniformly distributed. However, CL only widens the distance of positive and negative samples within the training batch, rather than making a uniform distribution of all items. These two shortcomings make the embedding space too compact, which is harmful to personalized representation and recommendation. Therefore, this article proposes Personalized Contrastive Loss (PCL) to combine CL with absolute constraints of BCE/CE and employs regularization methods to make the representations uniformly distributed. State-of-the-art results are obtained in experiments on several commonly used datasets. The code and data will be available on GitHub.
C1 [Tang, Hao] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Tang, Hao] China Unicom Shaanxi Branch, Xian 710075, Peoples R China.
   [Zhao, Guoshuai; Gao, Jing] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Zhao, Guoshuai; Gao, Jing] Shaanxi Yulan Jiuzhou Intelligent Optoelect Techno, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University; Xi'an Jiaotong University
RP Zhao, GS (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
EM th1002@stu.xjtu.edu.cn; guoshuai.zhao@xjtu.edu.cn;
   gaojing0423@stu.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn
RI Tang, Hao/KMX-1377-2024
OI Tang, Hao/0000-0002-2200-6249
FU NSFC
FX No Statement Available
CR Chang JX, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P378, DOI 10.1145/3404835.3462968
   Chang SN, 2022, IEEE T MULTIMEDIA, V24, P4067, DOI 10.1109/TMM.2021.3112814
   Chen G., 2021, P 30 INT C INT JOINT, P1426, DOI 10.24963/ijcai.2021/197
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P27
   Chen R, 2018, IEEE ACCESS, V6, P64301, DOI 10.1109/ACCESS.2018.2877208
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen YJ, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P2172, DOI 10.1145/3485447.3512090
   Ding YJ, 2022, IEEE T MULTIMEDIA, V24, P2687, DOI 10.1109/TMM.2021.3088281
   Fan XY, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1733, DOI 10.1145/3404835.3462978
   Fan ZW, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P433, DOI 10.1145/3459637.3482242
   Hao JM, 2021, IEEE T MULTIMEDIA, V24, P3381, DOI 10.1109/TMM.2021.3097186
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   He RN, 2016, IEEE DATA MINING, P191, DOI [10.1109/ICDM.2016.88, 10.1109/ICDM.2016.0030]
   Hidasi B., 2016, PROC INT C LEARN REP
   Hidasi B, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P241, DOI 10.1145/2959100.2959167
   Huo XY, 2022, IEEE T MULTIMEDIA, V24, P4224, DOI 10.1109/TMM.2021.3115335
   Kang WC, 2018, IEEE DATA MINING, P197, DOI 10.1109/ICDM.2018.00035
   Khosla P., 2020, PROC INT ADV C NEURA
   Krichene W, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1748, DOI 10.1145/3394486.340226
   Li JC, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P322, DOI 10.1145/3336191.3371786
   Li J, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1419, DOI 10.1145/3132847.3132926
   Liu H., 2022, IEEE Trans. Knowl. Data Eng., V24, P2701
   Liu ZW, 2021, Arxiv, DOI arXiv:2108.06479
   Ma C, 2020, AAAI CONF ARTIF INTE, V34, P5045
   Qiu RH, 2022, WSDM'22: PROCEEDINGS OF THE FIFTEENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P813, DOI 10.1145/3488560.3498433
   Qiu RH, 2021, IEEE DATA MINING, P519, DOI 10.1109/ICDM51629.2021.00063
   Rendle S., 2009, UAI'09, P452
   Rendle S, 2010, P 19 INT C WORLD WID, P811
   Shi H., 2022, PROC INT C LEARN REP
   Sohn K, 2016, ADV NEUR IN, V29
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Sun F, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1441, DOI 10.1145/3357384.3357895
   Tang H, 2023, KNOWL-BASED SYST, V261, DOI 10.1016/j.knosys.2022.110180
   Tang H, 2023, IEEE T MULTIMEDIA, V25, P339, DOI 10.1109/TMM.2021.3126146
   Tang JX, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P565, DOI 10.1145/3159652.3159656
   Nguyen T, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1583
   Tong X., 2021, P 30 INT JOINT C ART, P1593
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DJ, 2022, IEEE T MULTIMEDIA, V24, P4170, DOI 10.1109/TMM.2021.3114545
   Wang SJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6332
   Wang T., 2020, International Conference on Machine Learning, P9929
   Wei YW, 2022, IEEE T MULTIMEDIA, V24, P2701, DOI 10.1109/TMM.2021.3088307
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wu JC, 2023, Arxiv, DOI [arXiv:2201.02327, DOI 10.48550/ARXIV.2201.02327]
   Wu YX, 2022, IEEE T KNOWL DATA EN, V34, P1944, DOI 10.1109/TKDE.2020.3002531
   Wu YX, 2023, IEEE T MULTIMEDIA, V25, P3113, DOI 10.1109/TMM.2022.3155900
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie X, 2022, PROC INT CONF DATA, P1259, DOI 10.1109/ICDE53745.2022.00099
   Xie Z, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P449, DOI 10.1145/3442381.3449873
   Yuan FJ, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P582, DOI 10.1145/3289600.3290975
   Yuan HH, 2023, Arxiv, DOI arXiv:2304.11383
   Zhou K, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P2388, DOI 10.1145/3485447.3512111
   Zhou K, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1893, DOI 10.1145/3340531.3411954
NR 54
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2419
EP 2429
DI 10.1109/TMM.2023.3295740
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100006
DA 2024-08-05
ER

PT J
AU Wang, XX
   Jiang, B
   Wang, X
   Tang, JH
   Luo, B
AF Wang, Xixi
   Jiang, Bo
   Wang, Xiao
   Tang, Jinhui
   Luo, Bin
TI Rethinking Batch Sample Relationships for Data Representation: A
   Batch-Graph Transformer Based Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; mini-batch; graph model; metric learning
ID CONVOLUTIONAL NETWORKS
AB Exploring sample relationships within each mini-batch has shown great potential for learning image representations. Existing works generally adopt the regular Transformer to model the visual content relationships, ignoring the cues of semantic/label correlations between samples. Also, they generally adopt the 'full' self-attention mechanism which are obviously redundant and also sensitive to the noisy samples. To overcome these issues, in this paper, we design a simple yet flexible Batch-Graph Transformer (BGFormer) for mini-batch sample representations by deeply capturing the relationships of image samples from both visual and semantic perspectives. BGFormer has three main aspects. (1) It employs a flexible graph model, termed Batch Graph to jointly encode both visual and semantic relationships of samples within each mini-batch. (2) It explores the neighborhood relationships of samples by borrowing the idea of sparse graph representation which thus performs robustly, w.r.t., noisy samples. (3) It devises a novel specific Transformer architecture that mainly adopts dual structure-constrained self-attention (SSA), together with graph normalization, FFN, etc, to carefully exploit the batch graph information for sample tokens (nodes) representations. As an application, we apply BGFormer to the metric learning tasks. Extensive experiments on four popular datasets demonstrate the effectiveness of the proposed model.
C1 [Wang, Xixi; Jiang, Bo; Wang, Xiao; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
   [Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Peoples R China.
C3 Anhui University; Nanjing University of Science & Technology
RP Jiang, B (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
EM sissiw0409@foxmail.com; jiangbo@ahu.edu.cn; wangxiaocvpr@foxmail.com;
   jinhuitang@njust.edu.cn; luobin@ahu.edu.cn
OI Wang, Sissi/0000-0001-8510-0964
FU National Natural Science Foundation of China
FX No Statement Available
CR Cakir F, 2019, PROC CVPR IEEE, P1861, DOI 10.1109/CVPR.2019.00196
   Carion N., 2020, EUR C COMP VIS, P213
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chen L. OBray, 2022, INT C MACHINE LEARNI, P3469
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZM, 2023, IEEE T PATTERN ANAL, V45, P6969, DOI 10.1109/TPAMI.2021.3063496
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   El-Nouby A, 2021, Arxiv, DOI [arXiv:2102.05644, DOI 10.48550/ARXIV.2102.05644]
   Ermolov A, 2022, PROC CVPR IEEE, P7399, DOI 10.1109/CVPR52688.2022.00726
   Fu ZR, 2023, IEEE T MULTIMEDIA, V25, P7758, DOI 10.1109/TMM.2022.3227414
   Fu Zhihong, 2022, P 31 INT JOINT C ART, P905, DOI DOI 10.24963/IJCAI.2022/127
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17
   Guo MH, 2023, IEEE T PATTERN ANAL, V45, P5436, DOI 10.1109/TPAMI.2022.3211006
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han K., 2022, Advances in Neural Information Processing Systems, P8291
   He J, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3511917
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hou Z., 2022, arXiv
   Hou Z, 2022, PROC CVPR IEEE, P7246, DOI 10.1109/CVPR52688.2022.00711
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jiang B, 2022, IEEE T PATTERN ANAL, V44, P4935, DOI 10.1109/TPAMI.2021.3070599
   Jiang B, 2021, IEEE T MULTIMEDIA, V24, P3218, DOI 10.1109/TMM.2021.3095789
   Jiang B, 2013, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2013.448
   Kim DH, 2021, IEEE T MULTIMEDIA, V24, P3533, DOI 10.1109/TMM.2021.3101944
   Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330
   Kim W, 2018, LECT NOTES COMPUT SC, V11205, P760, DOI 10.1007/978-3-030-01246-5_45
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Loshchilov I., 2018, INT C LEARN REPR
   Mondal A. K., 2021, BRIT MACH VIS C, P194
   Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Rao YM, 2021, 35 C NEURAL INFORM P, V34
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Roth K, 2019, IEEE I CONF COMP VIS, P7999, DOI 10.1109/ICCV.2019.00809
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Suh Y, 2019, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2019.00742
   Sun ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3591, DOI 10.1109/ICCV48922.2021.00359
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang SG, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5630, DOI 10.1145/3503161.3547830
   Tang SG, 2022, IEEE T MULTIMEDIA, V24, P4433, DOI 10.1109/TMM.2021.3117124
   Teh Eu Wern, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P448, DOI 10.1007/978-3-030-58586-0_27
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang J, 2023, IEEE T MULTIMEDIA, V25, P7726, DOI 10.1109/TMM.2022.3225738
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang XX, 2022, Arxiv, DOI arXiv:2208.12398
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Welinder Peter., 2010, Computation & Neural Systems Technical Report. 2010-001
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Xie GS, 2021, PROC CVPR IEEE, P5471, DOI 10.1109/CVPR46437.2021.00543
   Ying C., 2021, Advances in Neural Information Processing Systems, P28877
   Zhai H.-Y., 2018, BRIT MACH VIS C, V41
   Zhang L, 2023, IEEE T PATTERN ANAL, V45, P5712, DOI 10.1109/TPAMI.2022.3207500
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zhuang LS, 2012, PROC CVPR IEEE, P2328, DOI 10.1109/CVPR.2012.6247944
NR 67
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1578
EP 1588
DI 10.1109/TMM.2023.3283132
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800019
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yang, ZZ
   Huang, J
   Zhou, M
   Zheng, NS
   Zhao, F
AF Yang, Zizheng
   Huang, Jie
   Zhou, Man
   Zheng, Naishan
   Zhao, Feng
TI IRVR: A General Image Restoration Framework for Visual Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low-level vision; restoration; visual recognition; low for high
ID NEURAL-NETWORK
AB Images corrupted with degradations often result in a performance drop in downstream image recognition models trained on clean images. Previous image restoration (IR) methods either restore the images without delicately considering the semantic recovery, or the training objectives cannot meet unseen recognition models, leading to poor and non-generalizable performance for various downstream recognition tasks. In this paper, we propose a general Image Restoration framework for Visual Recognition (IRVR), which addresses generalized and effective semantic recovery in image restoration for a range of high-level tasks. Concretely, for better generalization, we train the IR models with semantic recovery as the primary objective, and image regression as a regularization term, respectively, where the primary objective gradient is calibrated with the regularization gradient to ensure the generalization of IR to unseen recognition models. For effectiveness, we introduce an intrinsic semantic consistency constraint to match the semantic statistical distribution between restored and clean image pairs. Our IRVR is recognition-agnostic and orthogonal to IR, making it a plug-and-play component that can be incorporated into existing IR methods without adding any computational cost during inference. Extensive experiments demonstrate the effectiveness and generalization of our IRVR for improving the performance of IR in diverse downstream high-level tasks. The IRVR's ability to accurately recover intrinsic semantics in images is instrumental in high-level machine analysis, which ensures the integrity and authenticity of multimedia content.
C1 [Yang, Zizheng; Huang, Jie; Zhou, Man; Zheng, Naishan; Zhao, Feng] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhao, F (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM yzz6000@mail.ustc.edu.cn; hj0117@mail.ustc.edu.cn;
   manman@mail.ustc.edu.cn; nszheng@mail.ustc.edu.cn; fzhao956@ustc.edu.cn
RI ; Zhao, Feng/C-8367-2009
OI Huang, Jie/0000-0002-3518-3404; Zhao, Feng/0000-0001-6767-8105
FU JKW Research Funds
FX No Statement Available
CR Abdelhamed A, 2019, IEEE I CONF COMP VIS, P3165, DOI 10.1109/ICCV.2019.00326
   Anwar S, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.115978
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Cong RM, 2023, IEEE T IMAGE PROCESS, V32, P4472, DOI 10.1109/TIP.2023.3286263
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang J, 2023, PROC CVPR IEEE, P9904, DOI 10.1109/CVPR52729.2023.00955
   Huang J, 2022, PROC CVPR IEEE, P6033, DOI 10.1109/CVPR52688.2022.00595
   Huang J, 2019, LECT NOTES COMPUT SC, V11133, P230, DOI 10.1007/978-3-030-11021-5_15
   Ke-Chi Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P343, DOI 10.1007/978-3-030-58586-0_21
   Kim I, 2021, PROC CVPR IEEE, P12252, DOI 10.1109/CVPR46437.2021.01208
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee S, 2022, PROC CVPR IEEE, P18889, DOI 10.1109/CVPR52688.2022.01834
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2022, PROC CVPR IEEE, P17431, DOI 10.1109/CVPR52688.2022.01693
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li CY, 2017, PATTERN RECOGN LETT, V94, P62, DOI 10.1016/j.patrec.2017.05.023
   Li CL, 2017, ADV NEUR IN, V30
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Li RT, 2020, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR42600.2020.00324
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lietal C., 2023, PROC INT C LEARN REP, P1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Z, 2023, IEEE T PATTERN ANAL, V45, P3032, DOI 10.1109/TPAMI.2022.3183243
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma JY, 2022, IEEE T MULTIMEDIA, V24, P3157, DOI 10.1109/TMM.2021.3094058
   Ma L, 2023, IEEE T MULTIMEDIA, V25, P3573, DOI 10.1109/TMM.2022.3162493
   Mei KF, 2022, LECT NOTES COMPUT SC, V13667, P384, DOI 10.1007/978-3-031-20071-7_23
   Pei YT, 2018, LECT NOTES COMPUT SC, V11214, P697, DOI 10.1007/978-3-030-01249-6_42
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Ren S., 2016, Faster r-cnn: Towards real-time object detection with region proposal networks
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Taeyoung Son, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P749, DOI 10.1007/978-3-030-58545-7_43
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Valanarasu JMJ, 2022, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR52688.2022.00239
   Vaswani A, 2017, ADV NEUR IN, V30
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wu RQ, 2023, PROC CVPR IEEE, P22282, DOI 10.1109/CVPR52729.2023.02134
   Wu SX, 2023, IEEE T MULTIMEDIA, V25, P1111, DOI 10.1109/TMM.2021.3139209
   Yang Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11046, DOI 10.1109/CVPR42600.2020.01106
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yang Z, 2022, LECT NOTES COMPUT SC, V13684, P552, DOI 10.1007/978-3-031-20053-3_32
   Yang ZZ, 2023, PROC CVPR IEEE, P14059, DOI 10.1109/CVPR52729.2023.01351
   Yu H, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6645, DOI 10.1145/3503161.3548410
   Yu H, 2022, LECT NOTES COMPUT SC, V13679, P181, DOI 10.1007/978-3-031-19800-7_11
   Yu L, 2023, IEEE T MULTIMEDIA, V25, P443, DOI 10.1109/TMM.2021.3127360
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang L, 2022, IEEE T MULTIMEDIA, V24, P1830, DOI 10.1109/TMM.2021.3073267
   Zhang L, 2022, IEEE T MULTIMEDIA, V24, P1415, DOI 10.1109/TMM.2021.3064408
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zheng NS, 2023, IEEE T MULTIMEDIA, V25, P5469, DOI 10.1109/TMM.2022.3193059
   zhou man, 2022, ADV NEURAL INFORM PR, V35, P22995
NR 68
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7012
EP 7026
DI 10.1109/TMM.2024.3358962
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000011
DA 2024-08-05
ER

PT J
AU Yao, Y
   Wang, K
   Chang, Q
   Weng, SW
AF Yao, Ye
   Wang, Ke
   Chang, Qi
   Weng, Shaowei
TI Reversible Data Hiding in Encrypted Images Using Global Compression of
   Zero-Valued High Bit-Planes and Block Rearrangement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Block rearrangement; encrypted image; privacy preservation; reversible
   data hiding; zero-valued high bit-planes compression
ID PREDICTION-ERROR EXPANSION; DIFFERENCE; WATERMARKING
AB Recently, reversible data hiding in encrypted images (RDHEI) has received widespread attention from researchers. To embed high payload into encrypted images while maintaining sufficient security, a novel RDHEI algorithm in combination with consecutive zero-valued high bit-planes compression, bit-plane swapping as well as block rearrangement is proposed in this article. The proposed method is the first work to compress global zero-valued high bit-planes in a block-wise manner and adaptively allocate different Huffman indicators based on the occurrence frequency of zero-valued bit-planes so that a higher embedded payload is greatly provided. Unlike existing RDHEI methods embedded with unencrypted auxiliary information, resulting in low security, the bit-plane swapping and block rearrangement are subtly designed to cluster together all embeddable bit-planes, which enables most auxiliary information to be encrypted, largely enhancing the security and facilitating data embedding and data extraction. The experiment results demonstrate that the proposed method outperforms some state-of-the-art RDHEI methods in terms of security and payload. The average payload of the proposed method for two publicly-used datasets including BOSSbase and BOWS-2, are 3.793 bpp and 3.705 bpp, respectively.
C1 [Yao, Ye; Wang, Ke; Chang, Qi] Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou 310018, Zhejiang, Peoples R China.
   [Weng, Shaowei] Fujian Univ Technol, Sch Elect Elect Engn & Phys, Fuzhou 350108, Fujian, Peoples R China.
C3 Hangzhou Dianzi University; Fujian University of Technology
RP Chang, Q (corresponding author), Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou 310018, Zhejiang, Peoples R China.
EM yaoye@hdu.edu.cn; wang_ke517@163.com; qichang@hdu.edu.cn;
   wswweiwei@126.com
OI Yao, Ye/0000-0002-7012-9307; chang, qi/0009-0003-8051-5521; Wang,
   Ke/0009-0008-5453-5197
FU National Natural Science Foundation of China
FX No Statement Available
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas P, 2017, Image database of BOWS-2
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Chang Q, 2022, IEEE T CIRC SYST VID, V32, P5725, DOI 10.1109/TCSVT.2022.3153796
   Chang Q, 2021, IEEE T CIRC SYST VID, V31, P4850, DOI 10.1109/TCSVT.2021.3055612
   Chen KM, 2019, J VIS COMMUN IMAGE R, V58, P334, DOI 10.1016/j.jvcir.2018.12.023
   Chunqiang Yu, 2022, IEEE Transactions on Circuits and Systems for Video Technology, V32, P451, DOI 10.1109/TCSVT.2021.3062947
   Coatrieux G, 2013, IEEE T INF FOREN SEC, V8, P111, DOI 10.1109/TIFS.2012.2224108
   Coatrieux G, 2009, IEEE T INF TECHNOL B, V13, P158, DOI 10.1109/TITB.2008.2007199
   Gao GY, 2023, DIGIT SIGNAL PROCESS, V133, DOI 10.1016/j.dsp.2022.103870
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Lee S, 2007, IEEE T INF FOREN SEC, V2, P321, DOI 10.1109/TIFS.2007.905146
   Li RYM, 2007, IEEE INT SYMP CIRC S, P1273, DOI 10.1109/ISCAS.2007.378403
   Liao X, 2015, J VIS COMMUN IMAGE R, V28, P21, DOI 10.1016/j.jvcir.2014.12.007
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Mohammadi A, 2020, IEEE T CIRC SYST VID, V30, P2366, DOI 10.1109/TCSVT.2020.2990952
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Puteaux P, 2018, IEEE INT WORKS INFOR
   Puyang Y, 2018, IEEE INT WORKS INFOR
   Qiu YQ, 2016, IEEE SIGNAL PROC LET, V23, P130, DOI 10.1109/LSP.2015.2504464
   Qu LF, 2022, IEEE T MULTIMEDIA, V24, P2924, DOI 10.1109/TMM.2021.3090588
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang YM, 2022, IEEE T MULTIMEDIA, V24, P1288, DOI 10.1109/TMM.2021.3062699
   Wang YM, 2021, IEEE T MULTIMEDIA, V23, P1466, DOI 10.1109/TMM.2020.2999187
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Weng S., 2022, IEEE Trans. Multimedia, carly access, DOI [10.1109/TMM.20223198877, DOI 10.1109/TMM.20223198877]
   Weng SW, 2023, IEEE T MULTIMEDIA, V25, P8738, DOI 10.1109/TMM.2023.3241541
   Wu YQ, 2020, IEEE T MULTIMEDIA, V22, P1929, DOI 10.1109/TMM.2019.2952979
   Xu S, 2023, IEEE T DEPEND SECURE, V20, P4199, DOI 10.1109/TDSC.2022.3219843
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yin ZX, 2022, IEEE T DEPEND SECURE, V19, P992, DOI 10.1109/TDSC.2020.3019490
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Yu CQ, 2022, INFORM SCIENCES, V584, P89, DOI 10.1016/j.ins.2021.10.050
   Zhang TC, 2022, IEEE T CIRC SYST VID, V32, P5041, DOI 10.1109/TCSVT.2022.3146159
   Zhang WM, 2014, SIGNAL PROCESS, V94, P118, DOI 10.1016/j.sigpro.2013.06.023
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
NR 39
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3701
EP 3714
DI 10.1109/TMM.2023.3314975
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200017
DA 2024-08-05
ER

PT J
AU Zhang, F
   Liu, N
   Duan, FQ
AF Zhang, Fan
   Liu, Na
   Duan, Fuqing
TI Coarse-to-Fine Depth Super-Resolution With Adaptive RGB-D Feature
   Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color; Image edge detection; Image color analysis; Feature extraction;
   Superresolution; Pipelines; Image restoration; Depth super-resolution;
   color guidance; attention mechanism
ID NETWORK
AB Depth maps suffer from multiple kinds of degradation such as noise and low resolution, due to the limitations of sensors. To improve the spatial resolution and quality of depth maps, RGB-D-based depth super-resolution (SR) methods utilize the corresponding color image to provide extra structure information. However, the inconsistency between the color texture and depth structure can lead to texture-copying artifacts if the two kinds of features are fused without selection. In this article, we propose a novel coarse-to-fine framework for RGB-D-based depth SR, which consists of two sub-networks, i.e., CONet for coarse SR and RFNet for refinement. Through the proposed coarse supervision strategy, CONet can alleviate multiple degradations in depth maps and assist with further SR in the refinement stage. Moreover, the branch attention module (BAM) is incorporated in the RFNet to adaptively select important information from RGB-D features and suppress the texture-copying artifact. Additionally, we propose an edge-aware spatial attention module (ESAM) to further locate and restore the depth discontinuity in the fused RGB-D features. Extensive experiments on multiple benchmarks demonstrate that compared to the state-of-the-art methods, the proposed method achieves improved results both quantitatively and qualitatively.
C1 [Zhang, Fan; Liu, Na; Duan, Fuqing] Beijing Normal Univ, Coll Artificial Intelligence, Beijing 100875, Peoples R China.
C3 Beijing Normal University
RP Duan, FQ (corresponding author), Beijing Normal Univ, Coll Artificial Intelligence, Beijing 100875, Peoples R China.
EM fzhang@mail.bnu.edu.cn; na_liu1994@126.com; fqduan@bnu.edu.cn
OI Duan, Fuqing/0000-0002-3849-8532; Liu, Na/0000-0001-6993-6331
FU National Key RD Program of China
FX No Statement Available
CR Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Cao X, 2021, NEUROCOMPUTING, V454, P350, DOI 10.1016/j.neucom.2021.04.096
   Diebel James, 2005, NIPS, P291
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fankhauser P, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P388, DOI 10.1109/ICAR.2015.7251485
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Gu SH, 2017, PROC CVPR IEEE, P712, DOI 10.1109/CVPR.2017.83
   Gu X, 2020, IEEE T IMAGE PROCESS, V29, P6343, DOI 10.1109/TIP.2020.2988574
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   He LZ, 2021, PROC CVPR IEEE, P9225, DOI 10.1109/CVPR46437.2021.00911
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Jiang ZY, 2018, IEEE T IMAGE PROCESS, V27, P2587, DOI 10.1109/TIP.2018.2806089
   Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195
   Kim B, 2019, Arxiv, DOI arXiv:1903.11286
   Kingma D. P., 2014, arXiv
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Liu P, 2022, NEUROCOMPUTING, V479, P75, DOI 10.1016/j.neucom.2022.01.050
   Liu W, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2612826
   Lu JJ, 2015, PROC CVPR IEEE, P2245, DOI 10.1109/CVPR.2015.7298837
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Scharstein D, 2007, PROC CVPR IEEE, P1688
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song XB, 2020, PROC CVPR IEEE, P5630, DOI 10.1109/CVPR42600.2020.00567
   Song XB, 2017, LECT NOTES COMPUT SC, V10114, P360, DOI 10.1007/978-3-319-54190-7_22
   Sun BL, 2021, PROC CVPR IEEE, P7788, DOI 10.1109/CVPR46437.2021.00770
   Tang JX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4390, DOI 10.1145/3474085.3475584
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Wang J, 2020, IEEE T MULTIMEDIA, V22, P1470, DOI 10.1109/TMM.2019.2946075
   Wang K, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109260
   Wang S, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2747-y
   Wang ZH, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107274
   Wen Y, 2019, IEEE T IMAGE PROCESS, V28, P994, DOI 10.1109/TIP.2018.2874285
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie J, 2016, IEEE T IMAGE PROCESS, V25, P428, DOI 10.1109/TIP.2015.2501749
   Yang BL, 2019, IEEE ACCESS, V7, P57616, DOI 10.1109/ACCESS.2019.2914065
   Yang J., 2017, P IEEE VIS COMM IM P, P1
   Yang J., 2019, RGB D IMAGE ANAL PRO, P51
   Yang Qingxiong, 1845, IEEE C COMPUTER VISI
   Yanjie Li, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P152, DOI 10.1109/ICME.2012.30
   Zhang F, 2020, IET IMAGE PROCESS, V14, P4708, DOI 10.1049/iet-ipr.2019.1623
   Zhang NAK, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3548254
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao ZX, 2022, PROC CVPR IEEE, P5687, DOI 10.1109/CVPR52688.2022.00561
   Zhong ZW, 2022, IEEE T IMAGE PROCESS, V31, P648, DOI 10.1109/TIP.2021.3131041
   Zhu HY, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10101187
   Zuo YF, 2019, INFORM SCIENCES, V495, P52, DOI 10.1016/j.ins.2019.05.003
NR 55
TC 2
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2621
EP 2633
DI 10.1109/TMM.2023.3301238
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400002
DA 2024-08-05
ER

PT J
AU Zhang, GJ
   Wei, SK
   Pang, HX
   Qiu, S
   Zhao, Y
AF Zhang, Gangjian
   Wei, Shikui
   Pang, Huaxin
   Qiu, Shuang
   Zhao, Yao
TI Enhance Composed Image Retrieval via Multi-Level Collaborative
   Localization and Semantic Activeness Perception
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Location awareness; Task analysis; Image retrieval; Training;
   Collaboration; Transformers; Composed image retrieval; multi-modal
   fusion and embedding; multi-modal representation learning; multi-modal
   retrieval; image retrieval
AB Composed image retrieval (CIR) is an emerging and challenging research task that combines two modalities, a reference image, and a modification text, into one query to retrieve the target image. In online shopping scenarios, the user would use the modification text as feedback to describe the difference between the reference and the desired image. In order to handle the task, there must be two main problems needed to be addressed. One is the localization problem: how to precisely find those spatial areas of the image mentioned by the text. The other is the modification problem: how to effectively modify the image semantics based on the text. However, existing methods merely fuse information coarsely from the two-modality, while the accurate spatial and semantic correspondence between these two heterogeneous features tends to be neglected. Therefore, image details cannot be precisely located and modified. To this end, we consider integrating information from the two modalities more accurately from spatial and semantic aspects. Thus, we propose an end-to-end framework for the CIR task, which contains three key components, i.e., Multi-level Collaborative Localization module (MCL), Differential Semantics Discrimination module (DSD), and Image Difference Enhancement constraints (IDE). Specifically, to solve the localization problem, MCL precisely locates the text to the image areas by collaboratively using text positioning information on multiple image layers. For the modification problem, DSD builds a distribution to evaluate the modification possibility of each image semantic dimension, and IDE effectively learns the modification patterns of text against image embedding based on the distribution. Extensive experiments on three datasets show that the proposed method achieves outstanding performance against the SOTA methods.
C1 [Zhang, Gangjian; Wei, Shikui; Pang, Huaxin; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Zhang, Gangjian; Wei, Shikui; Pang, Huaxin; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Qiu, Shuang] Taiyuan Univ Technol, Coll Data Sci, Taiyuan 030600, Peoples R China.
C3 Beijing Jiaotong University; Taiyuan University of Technology
RP Wei, SK (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.; Wei, SK (corresponding author), Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
EM 19120317@bjtu.edu.cn; shkwei@bjtu.edu.cn; 20112005@bjtu.edu.cn;
   14120332@bjtu.edu.cn; yzhao@bjtu.edu.cn
OI Zhao, Yao/0000-0002-8581-9554; Pang, Huaxin/0000-0003-4998-4576
FU National Key Ramp;D Program of China
FX No Statement Available
CR Ak KE, 2018, PROC CVPR IEEE, P7708, DOI 10.1109/CVPR.2018.00804
   Anwaar MU, 2021, IEEE WINT CONF APPL, P1139, DOI 10.1109/WACV48630.2021.00118
   Baldrati A., 2021, P ACM MULT AS, P1
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Can O. A, 2020, arXiv
   Chen Y.-W., 2019, P BRIT MACH VIS C
   Chen YB, 2020, PROC CVPR IEEE, P2998, DOI 10.1109/CVPR42600.2020.00307
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dodds E, 2020, Arxiv, DOI arXiv:2007.00145
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Faghri F, 2018, Arxiv, DOI arXiv:1707.05612
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Goenka S, 2022, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR52688.2022.01371
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Gu CB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4600, DOI 10.1145/3474085.3475619
   Guo XX, 2018, ADV NEUR IN, V31
   Guo YY, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3295822
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosseinzadeh M, 2020, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR42600.2020.00365
   Hu ZW, 2020, PROC CVPR IEEE, P4423, DOI 10.1109/CVPR42600.2020.00448
   Huang QB, 2022, IEEE T MULTIMEDIA, V24, P2004, DOI 10.1109/TMM.2021.3074803
   Jandial S, 2022, IEEE WINT CONF APPL, P597, DOI 10.1109/WACV51458.2022.00067
   Kim J, 2021, AAAI CONF ARTIF INTE, V35, P1771
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lee S, 2021, PROC CVPR IEEE, P802, DOI 10.1109/CVPR46437.2021.00086
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2105, DOI 10.1109/ICCV48922.2021.00213
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lu JS, 2019, ADV NEUR IN, V32
   Ma Jin, 2020, P IEEE CVF WINT C AP, P2503
   Mai L, 2017, PROC CVPR IEEE, P1121, DOI 10.1109/CVPR.2017.125
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Radenovic F, 2018, LECT NOTES COMPUT SC, V11209, P774, DOI 10.1007/978-3-030-01228-1_46
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma R., 2019, arXiv
   Sohn K, 2016, ADV NEUR IN, V29
   Souri Y, 2017, LECT NOTES COMPUT SC, V10115, P118, DOI 10.1007/978-3-319-54193-8_8
   Vaswani A, 2017, ADV NEUR IN, V30
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wen HK, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1369, DOI 10.1145/3404835.3462967
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yang YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3303, DOI 10.1145/3474085.3475483
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yelamarthi SK, 2018, LECT NOTES COMPUT SC, V11208, P316, DOI 10.1007/978-3-030-01225-0_19
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Zhang GJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5353, DOI 10.1145/3474085.3475659
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
NR 59
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 916
EP 928
DI 10.1109/TMM.2023.3273466
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700006
DA 2024-08-05
ER

PT J
AU Zhang, JJ
   Rao, YT
   Huang, XS
   Li, GY
   Zhou, X
   Zeng, D
AF Zhang, Junjie
   Rao, Yutao
   Huang, Xiaoshui
   Li, Guanyi
   Zhou, Xin
   Zeng, Dan
TI Frequency-Aware Multi-Modal Fine-Tuning for Few-Shot Open-Set Remote
   Sensing Scene Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Prototypes; Training; Visualization; Scene
   classification; Adaptation models; Semantics; RS scene classification;
   few-shot open-set recognition; multi-modal foundation model;
   parameter-efficient transfer learning
ID ZERO-SHOT; ALIGNMENT; NETWORK
AB Few-shot open-set recognition, as a new paradigm, leveraging a limited amount of supervised data to identify specific Remote Sensing (RS) scene categories and generalize to novel ones. However, the data bias induced by the small sample size not only causes severe overfitting within base classes, but also impairs the capacity for inference to identify RS scenes in hitherto unobserved categories. Furthermore, owing to environmental influences, RS images frequently manifest notable intra-class disparities and comparatively low inter-class distinctions, intensifying the challenge in obtaining suitable classifiers. To address above issues, we investigate the utilization of a Multi-modal Foundational Model (MFM) infused with essential domain knowledge to mitigate the generalization limitations encountered in few-shot scenarios. Recognizing that existing MFMs with a visual-text dual-branch structure are primarily tailored for natural scenes, we propose a custom Frequency Distribution-based Multi-modal Fine-Tuning strategy (FreqDiMFT) in a parameter-efficient manner. More specifically, within the vision branch, we address the high inter-class similarity and intra-class diversity in RS images by embedding the local-global frequency distribution information to facilitate the recognition of RS scenes. To further amplify the model's generalization ability post transfer, we introduce an adaptive feature refinement module designed for Transformers, proficient in filtering redundant features resulting from domain disparities. To mitigate the domain drift on the textual branch, we adopt an input format that combines basic templates with domain expertise from RS end to generate more discriminative class prototypes. To fully verify the effectiveness of our FreqDiMFT in a more practical setting, we collect a Large-Scale hybrid dataset (LSRS). Extensive experiments demonstrate that, even with a scant number of training samples, our strategy yields advanced performances compared to state-of-the-art models.
C1 [Zhang, Junjie; Rao, Yutao; Li, Guanyi; Zhou, Xin; Zeng, Dan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai 200444, Peoples R China.
   [Huang, Xiaoshui] Shanghai AI Lab, Shanghai 200433, Peoples R China.
C3 Shanghai University; Shanghai Artificial Intelligence Laboratory
RP Zeng, D (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai 200444, Peoples R China.
EM dzeng@shu.edu.cn
OI Li, Guanyi/0000-0003-2790-9962; Zhou, Xin/0009-0002-2828-3084; Huang,
   Xiaoshui/0000-0002-3579-538X
FU National Natural Science Foundation of China
FX No Statement Available
CR Bahng H, 2022, Arxiv, DOI arXiv:2203.17274
   Bai L, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3160492
   Bo Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8795, DOI 10.1109/CVPR42600.2020.00882
   Castelluccio M, 2015, Arxiv, DOI [arXiv:1508.00092, DOI 10.13229/J.CNKI.JDXBGXB20190885]
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998
   Cheng G, 2013, INT J REMOTE SENS, V34, P45, DOI 10.1080/01431161.2012.705443
   Cheng J, 2023, IEEE T MULTIMEDIA, V25, P191, DOI 10.1109/TMM.2021.3123813
   Dai DX, 2011, IEEE GEOSCI REMOTE S, V8, P173, DOI 10.1109/LGRS.2010.2055033
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Elhoseiny M, 2019, IEEE I CONF COMP VIS, P5783, DOI 10.1109/ICCV.2019.00588
   Fang J, 2019, IEEE T GEOSCI REMOTE, V57, P7492, DOI 10.1109/TGRS.2019.2913816
   Gao P, 2024, INT J COMPUT VISION, V132, P581, DOI 10.1007/s11263-023-01891-x
   Guibas J., 2021, arXiv
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   He J., 2022, PROC 10 INTCONF LEAR
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Huang HX, 2019, IEEE INT CON MULTI, P91, DOI 10.1109/ICME.2019.00024
   Huang SY, 2022, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR52688.2022.00703
   Jeong M, 2021, PROC CVPR IEEE, P12561, DOI 10.1109/CVPR46437.2021.01238
   Jia C, 2021, PR MACH LEARN RES, V139
   Jia ML, 2022, LECT NOTES COMPUT SC, V13693, P709, DOI 10.1007/978-3-031-19827-4_41
   Khattak MU, 2023, PROC CVPR IEEE, P19113, DOI 10.1109/CVPR52729.2023.01832
   Kim B, 2023, IEEE IMAGE PROC, P31, DOI 10.1109/ICIP49359.2023.10222412
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Li HF, 2020, Arxiv, DOI arXiv:1705.10450
   Li HF, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041226
   Li YN, 2017, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2017.553
   Li YS, 2021, IEEE T GEOSCI REMOTE, V59, P10590, DOI 10.1109/TGRS.2020.3047447
   Li YS, 2021, ISPRS J PHOTOGRAMM, V179, P145, DOI 10.1016/j.isprsjprs.2021.08.001
   Liu C, 2023, Arxiv, DOI arXiv:2306.06066
   Liu ZM, 2023, IEEE T MULTIMEDIA, V25, P7441, DOI 10.1109/TMM.2022.3222657
   Long Y, 2017, IEEE T GEOSCI REMOTE, V55, P2486, DOI 10.1109/TGRS.2016.2645610
   Ma SQ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14184533
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Ouyang Long, 2022, Advances in Neural Information Processing Systems, V35, P27730, DOI 10.1177/01454455830072006
   Patro BN, 2023, Arxiv, DOI [arXiv:2304.06446, 10.48550/arXiv.2304.06446]
   Pfeiffer J, 2021, 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2021), P487
   Qi K, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040569
   Qi XM, 2020, ISPRS J PHOTOGRAMM, V169, P337, DOI 10.1016/j.isprsjprs.2020.09.020
   Quan JC, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS AND COMMUNICATION ENGINEERING (ICECE 2018), P17, DOI 10.1109/ICECOME.2018.8645056
   Radford A, 2021, PR MACH LEARN RES, V139
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Singha Mainak, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P2024, DOI 10.1109/CVPRW59228.2023.00196
   Tu CH, 2023, PROC CVPR IEEE, P7725, DOI 10.1109/CVPR52729.2023.00746
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang C, 2021, IEEE J-STARS, V14, P12545, DOI 10.1109/JSTARS.2021.3132189
   Wang HY, 2023, PROC CVPR IEEE, P7507, DOI 10.1109/CVPR52729.2023.00725
   Wang Q, 2019, IEEE T GEOSCI REMOTE, V57, P1155, DOI 10.1109/TGRS.2018.2864987
   Wang Z., 2023, P ICCV, P3032
   Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xu WJ, 2023, ISPRS J PHOTOGRAMM, V198, P140, DOI 10.1016/j.isprsjprs.2023.02.012
   Yang Y., 2010, P 18 SIGSPATIAL INT, DOI [10.1145/1869790.1869829, DOI 10.1145/1869790.1869829]
   Yang YH, 2023, IEEE T MULTIMEDIA, V25, P280, DOI 10.1109/TMM.2021.3125134
   Zhang WH, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3192321
   Zhang XY, 2018, REMOTE SENS ENVIRON, V212, P231, DOI 10.1016/j.rse.2018.05.006
   Zhou KY, 2022, PROC CVPR IEEE, P16795, DOI 10.1109/CVPR52688.2022.01631
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
   Zhou WX, 2018, ISPRS J PHOTOGRAMM, V145, P197, DOI 10.1016/j.isprsjprs.2018.01.004
   Zhu B, 2023, IEEE I CONF COMP VIS, P15613, DOI 10.1109/ICCV51070.2023.01435
   Zhu QQ, 2016, IEEE GEOSCI REMOTE S, V13, P747, DOI 10.1109/LGRS.2015.2513443
   Zhu XY, 2023, IEEE I CONF COMP VIS, P2605, DOI 10.1109/ICCV51070.2023.00246
NR 64
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7823
EP 7837
DI 10.1109/TMM.2024.3372416
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000057
DA 2024-08-05
ER

PT J
AU Zhang, SJ
   Pan, JH
   Gao, JB
   Zheng, WS
AF Zhang, Shao-Jie
   Pan, Jia-Hui
   Gao, Jibin
   Zheng, Wei-Shi
TI Adaptive Stage-Aware Assessment Skill Transfer for Skill Determination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Feature extraction; Training; Videos; Adaptation models;
   Transfer learning; Hair; Adaptive source action search; assessment skill
   transfer; skill determination
ID VIDEO; NETWORK; MODEL
AB Skill determination aims to evaluate how well a participant performs a specific action. The task is rather challenging, due to the diversity of action types and the scarcity of samples. Many existing works train a skill determination model on limited samples of each action type separately. However, they neglect the skill similarities shared by different action types that can be exploited to enhance the skill determination process. How to exploit useful assessment skills from source actions to a related target action remains a challenge, and existing works have not ever found an effective way to accomplish this. In this work, we propose to achieve skill transfer for action assessment by an <bold>Ada</bold>ptive Stage-aware Assessment <bold>S</bold>kill <bold>T</bold>ransfer framework (<bold>AdaST</bold>) that transfers assessment skills from source actions to different stages of a target action adaptively. A source action search scheme is proposed to select relevant source actions for each target action. Furthermore, to encourage transferring effective and non-redundant assessment skills, a consistency loss and an orthogonality loss are introduced to ensure that the transferred assessment skills do not degrade the accurate determination and it provides complementary information. Extensive experiments on three public datasets demonstrate the effectiveness of the proposed method.
C1 [Zhang, Shao-Jie; Pan, Jia-Hui; Gao, Jibin; Zheng, Wei-Shi] Sun Yat sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
   [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen 518005, Peoples R China.
   [Zheng, Wei-Shi] Sun Yat sen Univ, Minist Educ, Key Lab Machine Intelligence & Adv Comp, Beijing 510006, Peoples R China.
C3 Sun Yat Sen University; Peng Cheng Laboratory; Sun Yat Sen University
RP Zheng, WS (corresponding author), Sun Yat sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.; Zheng, WS (corresponding author), Peng Cheng Lab, Shenzhen 518005, Peoples R China.; Zheng, WS (corresponding author), Sun Yat sen Univ, Minist Educ, Key Lab Machine Intelligence & Adv Comp, Beijing 510006, Peoples R China.
EM zhangshj56@mail2.sysu.edu.cn; panjh7@mail2.sysu.edu.cn;
   gaojb5@mail2.sysu.edu.cn; wszheng@ieee.org
FU NSFC
FX No Statement Available
CR Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Doughty H. R., 2021, Skill determination from long videos
   Doughty H, 2019, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2019.00805
   Doughty H, 2018, PROC CVPR IEEE, P6057, DOI 10.1109/CVPR.2018.00634
   Gao Y, 2014, MICCAI WORKSH M2CAI, V3, P3, DOI DOI 10.1109/MWSYM.2014.6848587
   Gattupalli S, 2017, IUI'17: PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P577, DOI 10.1145/3025171.3025213
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   He J., 2004, P 12 ANN ACM INT C M, P9, DOI DOI 10.1145/1027527.1027531
   Hu B, 2019, IEEE T MULTIMEDIA, V21, P2042, DOI 10.1109/TMM.2019.2894958
   Huang YC, 2010, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2010.5540012
   Jia CC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P87, DOI 10.1145/2647868.2654928
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Jibin Gao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P222, DOI 10.1007/978-3-030-58577-8_14
   Joachims T., 2006, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, P217, DOI [10.1145/1150402.1150429, DOI 10.1145/1150402.1150429]
   Kim H, 2018, IEEE T MULTIMEDIA, V20, P2415, DOI 10.1109/TMM.2018.2806224
   Lan T, 2013, PROC CVPR IEEE, P3103, DOI 10.1109/CVPR.2013.399
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li SW, 2017, IEEE T PATTERN ANAL, V39, P2423, DOI 10.1109/TPAMI.2017.2651818
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Liu J., 2011, CVPR 2011, P3209, DOI DOI 10.1109/CVPR.2011.5995729
   Malpani Anand, 2014, Information Processing in Computer-Assisted Interventions. 5th International Conference, IPCAI 2014. Proceedings: LNCS 8498, P138, DOI 10.1007/978-3-319-07521-1_15
   Mitchell M., 1998, INTRO GENETIC ALGORI, DOI DOI 10.1016/S0898-1221(96)90227-8
   Pan JH, 2022, IEEE T PATTERN ANAL, V44, P8779, DOI 10.1109/TPAMI.2021.3126534
   Pan JH, 2019, IEEE I CONF COMP VIS, P6340, DOI 10.1109/ICCV.2019.00643
   Parmar P, 2019, IEEE WINT CONF APPL, P1468, DOI 10.1109/WACV.2019.00161
   Parmar P, 2017, IEEE COMPUT SOC CONF, P76, DOI 10.1109/CVPRW.2017.16
   Pirsiavash H, 2014, LECT NOTES COMPUT SC, V8694, P556, DOI 10.1007/978-3-319-10599-4_36
   Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860
   Sánchez J, 2013, IMAGE PROCESS ON LIN, V3, P137, DOI 10.5201/ipol.2013.26
   Sargano AB, 2017, IEEE IJCNN, P463, DOI 10.1109/IJCNN.2017.7965890
   Su HS, 2021, IEEE T MULTIMEDIA, V23, P1503, DOI 10.1109/TMM.2020.2999184
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yansong Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9836, DOI 10.1109/CVPR42600.2020.00986
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7899, DOI 10.1109/ICCV48922.2021.00782
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhang SJ, 2022, IEEE T CIRC SYST VID, V32, P6017, DOI 10.1109/TCSVT.2022.3143549
   Zhang XY, 2019, AAAI CONF ARTIF INTE, P9227
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zia A, 2018, INT J COMPUT ASS RAD, V13, P443, DOI 10.1007/s11548-018-1704-z
   Zia A, 2015, LECT NOTES COMPUT SC, V9349, P430, DOI 10.1007/978-3-319-24553-9_53
NR 47
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4061
EP 4072
DI 10.1109/TMM.2023.3294800
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100017
DA 2024-08-05
ER

PT J
AU Zhang, SY
   Chen, YM
   Sun, YR
   Wang, F
   Shi, HB
   Wang, HR
AF Zhang, Siyu
   Chen, Yeming
   Sun, Yaoru
   Wang, Fang
   Shi, Haibo
   Wang, Haoran
TI LOIS: Looking Out of Instance Semantics for Visual Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual question answering (VQA); instance semantics; visual features;
   multimodal relation attention
ID ATTENTION; NETWORK
AB Visual question answering (VQA) has been intensively studied as a multimodal task, requiring efforts to bridge vision and language for correct answer inference. Recent attempts have developed various attention-based modules for solving VQA tasks. However, the performance of model inference is largely bottlenecked by visual semantic comprehension. Most existing detection methods rely on bounding boxes, remaining a serious challenge for VQA models to comprehend and correctly infer the causal nexus of contextual object semantics in images. To this end, we propose a finer model framework without bounding boxes in this work, termed Looking Out of Instance Semantics (LOIS) to address this crucial issue. LOIS can achieve more fine-grained feature descriptions to generate visual facts. Furthermore, to overcome the label ambiguity caused by instance masks, two types of relation attention modules: 1) intra-modality and 2) inter-modality, are devised to infer the correct answers from different visual features. Specifically, we implement a mutual relation attention module to model sophisticated and deeper visual semantic relations between instance objects and background information. In addition, our proposed attention model can further analyze salient image regions by focusing on important word-related questions. Experimental results on four benchmark VQA datasets prove that our proposed method has favorable performance in improving visual reasoning capability.
C1 [Zhang, Siyu; Chen, Yeming; Sun, Yaoru; Wang, Haoran] Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.
   [Wang, Fang] Brunel Univ, Dept Comp Sci, Uxbridge UB8 3PH, England.
   [Shi, Haibo] Shanghai Univ Finance & Econ, Sch Stat & Management, Shanghai 200433, Peoples R China.
C3 Tongji University; Brunel University; Shanghai University of Finance &
   Economics
RP Sun, YR (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.
EM zsyzsy@tongji.edu.cn; 2130769@tongji.edu.cn; yaoru@tongji.edu.cn;
   fang.wang@brunel.ac.uk; shihaibo@sufe.edu.cn;
   wanghaoran_tj@tongji.edu.cn
OI Sun, Yaoru/0000-0001-6052-2781; Wang, Haoran/0000-0002-4622-0119; wang,
   fang/0000-0003-1987-9150
FU National Natural Science Foundation of China
FX No Statement Available
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bai YL, 2018, LECT NOTES COMPUT SC, V11216, P21, DOI 10.1007/978-3-030-01258-8_2
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Chen H., 2020, CVPR, DOI DOI 10.1109/CVPR42600.2020.00860
   Cho J, 2021, PR MACH LEARN RES, V139
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guo WY, 2021, IEEE T IMAGE PROCESS, V30, P6730, DOI 10.1109/TIP.2021.3097180
   Guo YY, 2022, IEEE T IMAGE PROCESS, V31, P227, DOI 10.1109/TIP.2021.3128322
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   Huang ZC, 2020, Arxiv, DOI arXiv:2004.00849
   Jiang Huaizu, 2020, P IEEE CVF C COMP VI, P10267, DOI DOI 10.1109/CVPR42600.2020.01028
   Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005
   Kim J. H., 2017, PROC INT C LEARN REP, P1
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu Y, 2022, IEEE T CYBERNETICS, V52, P4520, DOI 10.1109/TCYB.2020.3029423
   Lu JS, 2019, ADV NEUR IN, V32
   Ma J, 2023, IEEE T NEUR NET LEAR, V34, P1380, DOI 10.1109/TNNLS.2021.3105284
   Ma Z., 2023, PROC AAAI C ARTIF IN, P13371
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Mishra Aakansha, 2023, IEEE Transactions on Artificial Intelligence, P81, DOI 10.1109/TAI.2022.3160418
   Osman A, 2019, COMPUT VIS IMAGE UND, V185, P24, DOI 10.1016/j.cviu.2019.05.001
   Ouyang NL, 2021, IEEE T MULTIMEDIA, V24, P3405, DOI 10.1109/TMM.2021.3097502
   Peng L, 2022, IEEE T KNOWL DATA EN, V34, P1644, DOI 10.1109/TKDE.2020.2998805
   Peng L, 2022, IEEE T PATTERN ANAL, V44, P318, DOI 10.1109/TPAMI.2020.3004830
   Peng L, 2019, MULTIMED TOOLS APPL, V78, P3843, DOI 10.1007/s11042-018-6389-3
   Qian SS, 2021, IEEE T MULTIMEDIA, V24, P3520, DOI 10.1109/TMM.2021.3101642
   Ren MY, 2015, ADV NEUR IN, V28
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Song JK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P906
   Su W., 2020, PROC INT C LEARN REP
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Teney D, 2018, PROC CVPR IEEE, P4223, DOI 10.1109/CVPR.2018.00444
   Wang T, 2020, IEEE COMPUT SOC CONF, P1547, DOI 10.1109/CVPRW50498.2020.00197
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Wu CF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P519, DOI 10.1145/3240508.3240513
   Wu CF, 2019, AAAI CONF ARTIF INTE, P8997
   Xiao JB, 2022, LECT NOTES COMPUT SC, V13696, P39, DOI 10.1007/978-3-031-20059-5_3
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zhang HY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P695, DOI 10.1145/3474085.3475234
   Zhang LY, 2021, IEEE T NEUR NET LEAR, V32, P4362, DOI 10.1109/TNNLS.2020.3017530
   Zhang X, 2023, IEEE T IMAGE PROCESS, V32, P4621, DOI 10.1109/TIP.2023.3302162
   Zhang ZJ, 2022, IEEE T MULTIMEDIA, V24, P3101, DOI 10.1109/TMM.2021.3093725
   Zhao J, 2022, NEURAL COMPUT APPL, V34, P9015, DOI 10.1007/s00521-022-06923-0
   Zheng XT, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3079918
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 55
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6202
EP 6214
DI 10.1109/TMM.2023.3347093
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600045
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, XY
   Xu, MR
   Tan, R
   Niyato, D
AF Zhang, XiuYu
   Xu, Minrui
   Tan, Rui
   Niyato, Dusit
TI Learning-Based Auction for Matching Demand and Supply of Holographic
   Digital Twin Over Immersive Communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Auction theory; digital twin; holographic-type communication; immersive
   communication; machine learning
AB Digital Twin (DT) technologies create digital models of physical entities frequently in multimedia forms, which are crucial for concurrent simulation and analysis of real-world systems. In displaying DTs, Holographic-Type Communication (HTC) provides immersive multimedia access for users to interact with Holographic DTs (HDTs) by transmitting holographic data such as Light Field (LF) and other multisensory information. HDT has applications in remote education, work, and social interactions. However, the effective matching of demand and supply between HDT users and providers remains a challenge. To address this issue, we propose a hierarchical architecture that integrates the DT and HTC paradigms. This architecture incorporates a marketplace for HDT services, leveraging a formulated Double Dutch Auction (DDA) mechanism to optimize matching and pricing based on user and provider valuation. Furthermore, We employ an actor-critic-based Deep Reinforcement Learning (DRL) algorithm to train a DDA auctioneer that dynamically adjusts auction clocks during the auction process. As an alternative to the Multi-layer Perceptron (MLP), we experiment with a Deep Simplistic Variational Quantum Circuit (DSVQC) to reduce the number of parameters and enhance performance stability. Our simulations reveal that the proposed learning-based auctioneer achieves 92% optimal social welfare at a 37% auction information exchange cost for an MLP-based actor and 99% optimal social welfare at a 77% auction information exchange cost for a DSVQC-based actor.
C1 [Zhang, XiuYu; Xu, Minrui; Tan, Rui; Niyato, Dusit] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Niyato, D (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM xiuyu.zhang@ntu.edu.sg; minrui001@e.ntu.edu.sg; tanrui@ntu.edu.sg;
   dniyato@ntu.edu.sg
RI XU, MINRUI/AFH-5904-2022
OI XU, MINRUI/0000-0002-8203-8146
FU National Research Foundation, Singapore
FX No Statement Available
CR Akyildiz I. F., 2022, ITU Journal on Future and Evolving Technologies, V3, P421
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Barricelli BR, 2019, IEEE ACCESS, V7, P167653, DOI 10.1109/ACCESS.2019.2953499
   Bergholm V, 2022, Arxiv, DOI arXiv:1811.04968
   Budhiraja I, 2021, IEEE INTERNET THINGS, V8, P3143, DOI 10.1109/JIOT.2020.3014926
   Cao ZW, 2022, IEEE COMMUN SURV TUT, V24, P895, DOI 10.1109/COMST.2022.3161275
   Chu C, 2022, I SYMPOS LOW POWER E, DOI 10.1145/3531437.3539719
   Clemm A, 2020, IEEE COMMUN MAG, V58, P93, DOI 10.1109/MCOM.001.1900272
   Cong I, 2019, NAT PHYS, V15, P1273, DOI 10.1038/s41567-019-0648-8
   El Saddik A, 2018, IEEE MULTIMEDIA, V25, P87, DOI 10.1109/MMUL.2018.023121167
   El-Sayed Nosayba, 2012, Performance Evaluation Review, V40, P163, DOI 10.1145/2318857.2254778
   Eyre J., 2018, Proc.Ind. Track EuroVR, P11
   Gershun A, 1939, J. Math. Phys, V18, P51, DOI [10.1002/sapm193918151, DOI 10.1002/SAPM193918151]
   He S, 2020, IEEE ICC, DOI 10.1109/icc40277.2020.9149429
   Hu XJ, 2023, IEEE T MULTIMEDIA, V25, P690, DOI 10.1109/TMM.2021.3129918
   Inagaki Y, 2018, LECT NOTES COMPUT SC, V11211, P431, DOI 10.1007/978-3-030-01234-2_26
   Ke SQ, 2019, PROC CIRP, V83, P753, DOI 10.1016/j.procir.2019.04.103
   Liu DY, 2023, IEEE T MULTIMEDIA, V25, P4400, DOI 10.1109/TMM.2022.3175023
   Liu SG, 2020, 2020 IEEE THE 3RD INTERNATIONAL CONFERENCE ON ELECTRONICS AND COMMUNICATION ENGINEERING (ICECE), P122, DOI 10.1109/ICECE51594.2020.9352884
   Liu Y., 2020, Global Energy Interconnection, V3, P272, DOI DOI 10.1016/J.GLOEI.2020.07.008
   Manvi SS, 2014, J NETW COMPUT APPL, V41, P424, DOI 10.1016/j.jnca.2013.10.004
   Peixeiro JP, 2018, IEEE T MULTIMEDIA, V20, P282, DOI 10.1109/TMM.2017.2742701
   Pérez-Salinas A, 2021, PHYS REV A, V104, DOI 10.1103/PhysRevA.104.012405
   Ruihang Wang, 2020, BuildSys '20: Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, P200, DOI 10.1145/3408308.3427982
   Schuld M, 2014, QUANTUM INF PROCESS, V13, P2567, DOI 10.1007/s11128-014-0809-8
   Schulman J, 2017, Arxiv, DOI [arXiv:1707.06347, DOI 10.48550/ARXIV.1707.06347]
   Strinati EC, 2019, IEEE VEH TECHNOL MAG, V14, P42, DOI 10.1109/MVT.2019.2921162
   Verhack R, 2020, IEEE T MULTIMEDIA, V22, P579, DOI 10.1109/TMM.2019.2932614
   Wang TY, 2017, ACTA OCEANOL SIN, V36, P1, DOI 10.1007/s13131-017-0987-1
   Wu XP, 2021, IEEE COMMUN SURV TUT, V23, P1398, DOI 10.1109/COMST.2021.3058296
   Zhang XY, 2022, Arxiv, DOI arXiv:2211.01016
   Zhang Y, 2013, IEEE COMMUN SURV TUT, V15, P1020, DOI 10.1109/SURV.2012.110112.00125
   Zhang ZT, 2022, IEEE IFIP NETW OPER, DOI 10.1109/NOMS54207.2022.9789853
NR 33
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5884
EP 5896
DI 10.1109/TMM.2023.3340548
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100004
DA 2024-08-05
ER

PT J
AU Zhang, Y
   Pan, YW
   Yao, T
   Huang, R
   Mei, T
   Chen, CW
AF Zhang, Yong
   Pan, Yingwei
   Yao, Ting
   Huang, Rui
   Mei, Tao
   Chen, Chang-Wen
TI End-to-End Video Scene Graph Generation With Temporal Propagation
   Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video scene graph generation; transformer; temporal propagation
AB Video scene graph generation has been an emerging research topic, which aims to interpret a video as a temporally-evolving graph structure by representing video objects as nodes and their relations as edges. Existing approaches predominantly follow a multi-step scheme, including frame-level object detection, relation recognition and temporal association. Although effective, these approaches neglect the mutual interactions between independent steps, resulting in a sub-optimal solution. We present a novel end-to-end framework for video scene graph generation, which naturally unifies object detection, object tracking, and relation recognition via a new Transformer structure, namely Temporal Propagation Transformer (TPT). Particularly, TPT extends the existing Transformer-based object detector (e.g., DETR) along the temporal dimension by involving a query propagation module, which can additionally associate the detected instances by identities across frames. A temporal dynamics encoder is then leveraged to dynamically enrich the features of the detected instances for relation recognition by attending to their historic states in previous frames. Meanwhile, the relation propagation strategy is devised to emphasize the temporal consistency of relation recognition results among adjacent frames. Extensive experiments conducted on VidHOI and Action Genome benchmarks demonstrate the superior performance of the proposed TPT over the state-of-the-art methods.
C1 [Zhang, Yong; Huang, Rui] Chinese Univ Hong Kong, Shenzhen 518172, Peoples R China.
   [Pan, Yingwei; Yao, Ting; Mei, Tao] HiDream Ai Inc, Beijing, Peoples R China.
   [Chen, Chang-Wen] Hong Kong Polytech Univ, Hong Kong, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Hong Kong Polytechnic
   University
RP Huang, R (corresponding author), Chinese Univ Hong Kong, Shenzhen 518172, Peoples R China.; Pan, YW (corresponding author), HiDream Ai Inc, Beijing, Peoples R China.
EM yongzhang@link.cuhk.edu.cn; panyw.ustc@gmail.com;
   tingyao.ustc@gmail.com; ruihuang@cuhk.edu.cn; tmei@live.com;
   changwen.chen@polyu.edu.hk
OI Chen, Chang Wen/0000-0002-6720-234X; Yao, Ting/0000-0001-7587-101X;
   Huang, Rui/0000-0002-7950-1662; Pan, Yingwei/0000-0002-4344-8898
FU Shenzhen Science and Technology
FX No Statement Available
CR Agarwal A, 2023, IEEE WINT CONF APPL, P5087, DOI 10.1109/WACV56688.2023.00507
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Cao QW, 2022, IEEE T MULTIMEDIA, V24, P3896, DOI 10.1109/TMM.2021.3109430
   Carion N., 2020, EUR C COMP VIS, P213
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chiou MJ, 2021, ICDAR '21: PROCEEDINGS OF THE 2021 WORKSHOP ON INTELLIGENT CROSS-DATA ANALYSIS AND RETRIEVAL, P9, DOI 10.1145/3463944.3469097
   Cong YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16352, DOI 10.1109/ICCV48922.2021.01606
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Feng SY, 2023, IEEE WINT CONF APPL, P5119, DOI 10.1109/WACV56688.2023.00510
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ji Jingwei, 2021, PROC IEEECVF INT C C, P8106
   Jingwei Ji, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10233, DOI 10.1109/CVPR42600.2020.01025
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kalman R.E., 1960, Trans ASME J Basic Eng, V82, P35, DOI 10.1115/1.3662552
   Kim B, 2021, PROC CVPR IEEE, P74, DOI 10.1109/CVPR46437.2021.00014
   Leal-Taixe Laura, 2015, ARXIV150401942
   Li RJ, 2022, PROC CVPR IEEE, P19464, DOI 10.1109/CVPR52688.2022.01888
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Li YM, 2022, PROC CVPR IEEE, P13864, DOI 10.1109/CVPR52688.2022.01350
   Li YJ, 2023, IEEE T MULTIMEDIA, V25, P1359, DOI 10.1109/TMM.2022.3141604
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin X, 2020, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR42600.2020.00380
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Luo JJ, 2023, PROC CVPR IEEE, P23359, DOI 10.1109/CVPR52729.2023.02237
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Pan YW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P7070, DOI 10.1145/3503161.3551581
   Pei JL, 2023, IEEE T MULTIMEDIA, V25, P1964, DOI 10.1109/TMM.2022.3141891
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Qian TW, 2023, IEEE T MULTIMEDIA, V25, P3950, DOI 10.1109/TMM.2022.3169065
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shang XD, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P279, DOI 10.1145/3323873.3325056
   Shang XD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1300, DOI 10.1145/3123266.3123380
   Shaw P., 2018, Self-attention with relative position representations, P464
   Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857
   Shit S, 2022, LECT NOTES COMPUT SC, V13697, P422, DOI 10.1007/978-3-031-19836-6_24
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Teng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13668, DOI 10.1109/ICCV48922.2021.01343
   Tu Danyang, 2022, P ADV NEUR INF PROC, V35, P23345
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan B, 2019, IEEE I CONF COMP VIS, P9468, DOI 10.1109/ICCV.2019.00956
   Wang N, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4985, DOI 10.1145/3474085.3475636
   Wu JF, 2022, LECT NOTES COMPUT SC, V13688, P553, DOI 10.1007/978-3-031-19815-1_32
   Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2023, IEEE T PATTERN ANAL, V45, P10870, DOI 10.1109/TPAMI.2023.3268446
   Yao T, 2022, LECT NOTES COMPUT SC, V13685, P328, DOI 10.1007/978-3-031-19806-9_19
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P606, DOI 10.1007/978-3-030-58592-1_36
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang J, 2019, PROC CVPR IEEE, P11527, DOI 10.1109/CVPR.2019.01180
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang Y, 2022, PROC CVPR IEEE, P19526, DOI 10.1109/CVPR52688.2022.01894
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhou XY, 2022, PROC CVPR IEEE, P8761, DOI 10.1109/CVPR52688.2022.00857
   Zhu X., 2020, PROC INT C LEARN REP, P1
   Zou C, 2021, PROC CVPR IEEE, P11820, DOI 10.1109/CVPR46437.2021.01165
NR 70
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1613
EP 1625
DI 10.1109/TMM.2023.3283879
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800003
DA 2024-08-05
ER

PT J
AU Zhao, ZX
   Yang, XT
   Liu, JT
   Zhou, C
   Zhao, CJ
AF Zhao, Zhenxi
   Yang, Xinting
   Liu, Jintao
   Zhou, Chao
   Zhao, Chunjiang
TI GCVC: Graph Convolution Vector Distribution Calibration for Fish Group
   Activity Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fish; Feature extraction; Activity recognition; Calibration; Adhesives;
   Training; Convolution; Graph convolution vector calibration; fish group
   activity; activity feature vector calibration; fish activity dataset
ID NEURAL-NETWORK; VISION
AB Only a few key fish individuals can play a dominant role in actual fish group, therefore, it is reasonable to infer group activities from the relationship between individual actions. However, the complex underwater environment, rapid and similar fish individual movements are likely to cause the indistinct action characteristics, as well as adhesion of data distribution, and it is difficult to infer the relationship between individual actions directly by using graph convolutional network (GCN). Therefore, this article proposes a graph convolution vector calibration (GCVC) network for fish group activity recognition through individual action relationship reasoning. By improving reasoning ability of GCN, an activity feature vector calibration module is designed to solve the data adhesion and mismatch between the estimated and true distribution. The idea is to first count the distribution of the original data, and make each dimension of its active feature vector follow the Gaussian distribution, so as to generate a better similar category distribution. In addition, we also produced a fish activity dataset to verify the performance of the proposed algorithm. The experimental results show that the GCVC achieves a group activity recognition accuracy of 93.33%, and the Macro-F1 is 93.25%, which is 19.21% and 24.2% higher than before, respectively. By using GCVC, the corrected activity feature vector distribution is more consistent, and the data adhesion is reduced, the model can achieve more fully supervised learning.
C1 [Zhao, Zhenxi; Zhao, Chunjiang] NorthWest A&F Univ, Coll Informat Engn, Yangling 712100, Peoples R China.
   [Zhao, Zhenxi; Yang, Xinting; Zhou, Chao; Zhao, Chunjiang] Natl Engn Res Ctr Informat Technol Agr, Beijing 100097, Peoples R China.
   [Zhao, Zhenxi; Yang, Xinting; Zhou, Chao; Zhao, Chunjiang] Beijing Acad Agr & Forestry Sci, Res Ctr Informat Technol, Beijing 100097, Peoples R China.
   [Zhao, Zhenxi; Yang, Xinting; Zhou, Chao; Zhao, Chunjiang] Natl Engn Lab Agriprod Qual Traceabil, Beijing 100097, Peoples R China.
   [Liu, Jintao] Univ Almeria, Sch Engn, Almeria 04120, Spain.
C3 Northwest A&F University - China; Beijing Academy of Agriculture &
   Forestry Sciences (BAAFS); Beijing Academy of Agriculture & Forestry
   Sciences (BAAFS); Universidad de Almeria
RP Zhao, CJ (corresponding author), NorthWest A&F Univ, Coll Informat Engn, Yangling 712100, Peoples R China.; Zhou, C; Zhao, CJ (corresponding author), Natl Engn Res Ctr Informat Technol Agr, Beijing 100097, Peoples R China.
EM 406138470@qq.com; nercitaznxt@163.com; jintaol@163.com;
   supperchao@hotmail.com; zzx0525_2018@163.com
OI Zhou, Chao/0000-0001-6528-3257; Zhao, zhenxi/0000-0001-8056-484X;
   Chunjiang, Zhao/0000-0001-8703-8420
FU Beijing Natural Science Foundation
FX No Statement Available
CR Azar SM, 2019, PROC CVPR IEEE, P7884, DOI 10.1109/CVPR.2019.00808
   Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365
   Biswas Sovan, 2018, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P1625, DOI 10.1109/WACV.2018.00180
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16
   Conrad JL, 2011, J FISH BIOL, V78, P395, DOI 10.1111/j.1095-8649.2010.02874.x
   Davis S, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.170312
   Duarte S, 2009, AQUACULT ENG, V41, P22, DOI 10.1016/j.aquaeng.2009.06.001
   Fu Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130805
   Fu XY, 2021, INT J COMPUT VISION, V129, P1691, DOI 10.1007/s11263-020-01428-6
   Gavrilyuk K, 2020, PROC CVPR IEEE, P836, DOI 10.1109/CVPR42600.2020.00092
   Han FF, 2020, IEEE ACCESS, V8, P126907, DOI 10.1109/ACCESS.2020.3008698
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hu GY, 2020, PROC CVPR IEEE, P977, DOI 10.1109/CVPR42600.2020.00106
   Hu XL, 2021, COMPUT ELECTRON AGR, V185, DOI 10.1016/j.compag.2021.106135
   Huang H, 2019, Arxiv, DOI arXiv:1812.05637
   Ibrahim MS, 2018, LECT NOTES COMPUT SC, V11207, P742, DOI 10.1007/978-3-030-01219-9_44
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Kearnes S, 2016, J COMPUT AID MOL DES, V30, P595, DOI 10.1007/s10822-016-9938-8
   Lee J.-V., 2013, RES J APPL SCI ENG T, V6, P3658, DOI 10.19026/rjaset.6.3573
   Li W., 2022, P INT JOINT C ARTIFI, P1102, DOI 10.24963/ijcai.2022/154
   Li ZG, 2017, Arxiv, DOI arXiv:1707.09835
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   Sánchez FL, 2020, INFORM FUSION, V64, P318, DOI 10.1016/j.inffus.2020.07.008
   Ma C, 2021, INT J COMPUT VISION, V129, P3255, DOI 10.1007/s11263-021-01527-y
   Papadakis VM, 2012, AQUACULT ENG, V46, P53, DOI 10.1016/j.aquaeng.2011.11.002
   Park S. -J., 2020, INT C LEARN REPRESEN, P7510, DOI DOI 10.5555/3524938.352563426
   Pedersen M, 2020, PROC CVPR IEEE, P2423, DOI 10.1109/CVPR42600.2020.00250
   Phamduy P, 2015, IEEE T MULTIMEDIA, V17, P2328, DOI 10.1109/TMM.2015.2480226
   Qi MS, 2018, LECT NOTES COMPUT SC, V11214, P104, DOI 10.1007/978-3-030-01249-6_7
   Qian ZM, 2016, BMC BIOINFORMATICS, V17, DOI 10.1186/s12859-016-1138-y
   Qin T., 2020, arXiv
   Ryoo MS, 2016, INT J COMPUT VISION, V119, P307, DOI 10.1007/s11263-015-0847-4
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Snell J, 2017, ADV NEUR IN, V30
   Tamura M., 2022, PROC 17 EURCONF COMP, P19
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tang YS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1283, DOI 10.1145/3240508.3240576
   Tang YS, 2019, IEEE T IMAGE PROCESS, V28, P4997, DOI 10.1109/TIP.2019.2914577
   Ubina N, 2021, AQUACULT ENG, V94, DOI 10.1016/j.aquaeng.2021.102178
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wang SJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P174, DOI 10.1145/3394171.3413871
   Wang SJ, 2021, AAAI CONF ARTIF INTE, V35, P2791
   Weihs D, 2017, THEOR APPL MECH LETT, V7, P276, DOI 10.1016/j.taml.2017.09.009
   Wongun Choi, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1282, DOI 10.1109/ICCVW.2009.5457461
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Wu LF, 2021, INT J AUTOM COMPUT, V18, P334, DOI 10.1007/s11633-020-1258-8
   Yan R, 2023, IEEE T PATTERN ANAL, V45, P6955, DOI 10.1109/TPAMI.2020.3034233
   Yan R, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1292, DOI 10.1145/3240508.3240572
   Yan SJ, 2018, Arxiv, DOI [arXiv:1801.07455, 10.48550/arXiv.1801.07455, 10.1609/aaai.v32i1.12328, 10.48550/ARXIV.1801.07455, DOI 10.1609/AAAI.V32I1.12328]
   Yang S., 2021, PROC INT C LEARN REP
   Ye HJ, 2021, INT J COMPUT VISION, V129, P1930, DOI 10.1007/s11263-020-01381-4
   Ye ZY, 2016, T ASABE, V59, P345
   Yu XN, 2021, COMPUT ELECTRON AGR, V185, DOI 10.1016/j.compag.2021.106169
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhou C, 2019, AQUACULTURE, V507, P457, DOI 10.1016/j.aquaculture.2019.04.056
   Zhuang PQ, 2021, IEEE T MULTIMEDIA, V23, P3603, DOI 10.1109/TMM.2020.3028482
NR 61
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1776
EP 1789
DI 10.1109/TMM.2023.3287339
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800020
DA 2024-08-05
ER

PT J
AU An, P
   Duan, YC
   Huang, YL
   Ma, J
   Chen, YF
   Wang, LH
   Yang, Y
   Liu, Q
AF An, Pei
   Duan, Yucong
   Huang, Yuliang
   Ma, Jie
   Chen, Yanfei
   Wang, Liheng
   Yang, You
   Liu, Qiong
TI SP-Det: Leveraging Saliency Prediction for Voxel-Based 3D Object
   Detection in Sparse Point Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Task analysis; Point cloud compression;
   Feature extraction; Laser radar; Detectors; Object detection; 3D object
   detection; 3D point cloud; autonomous driving; light detection and
   ranging; saliency prediction; voxel
ID R-CNN
AB Voxel is one of the common structural representation of 3D point cloud. Due to the sparsity of point cloud generated by light detection and ranging (LiDAR), there is the extreme imbalance in the foreground and background voxels. It decreases the accuracy of 3D object detection, has the negative effect on intelligent driving safety. To overcome this problem, we present a saliency prediction based 3D object detector SP-Det in this article. Although foreground voxels have the sufficient feature of object, it is difficult to localize the foreground region from voxel space with the larger background region. We design an auxiliary learning task, saliency prediction (SP). It benefits 3D detector in identifying the foreground region. SP task uses label diffusion to alleviate the label imbalance. It reduces the learning difficulty of saliency in voxel and bird's eye view (BEV) spaces. After that, to strengthen feature interaction from the sparse foreground region, we design saliency fusion (SF) module to fuse the learning result in SP task. It utilizes voxel and BEV saliency maps as progressive attention to resist the redundant feature from background region. To aggregate more foreground feature inside 3D and BEV region of interest (RoI), we design hybrid grid maps based RoI pooling (Hybrid-RoI pooling). Experiments are conducted in STF dataset. The adverse weather enlarges the sparsity of LiDAR point cloud, increasing the difficulty of object detection. SP-Det identifies and leverages foreground region, and achieves the performance better than the current methods. Hence, we believe that SP-Det benefits to LiDAR based 3D scene understanding in the adverse weather.
C1 [An, Pei; Huang, Yuliang; Chen, Yanfei; Wang, Liheng] Wuhan Inst Technol, Sch Elect & Informat Engn, Wuhan 430205, Peoples R China.
   [Duan, Yucong; Ma, Jie] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China.
   [Yang, You; Liu, Qiong] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Yang, You; Liu, Qiong] Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
C3 Wuhan Institute of Technology; Huazhong University of Science &
   Technology; Huazhong University of Science & Technology; Huazhong
   University of Science & Technology
RP Ma, J (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China.; Yang, Y (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM anpei@wit.edu.cn; yucongduan@hust.edu.cn; ylh@wit.edu.cn;
   majie@hust.edu.cn; 0509040@wit.edu.cn; wlhhust@wit.edu.cn;
   yangyou@hust.edu.cn; q.liu@hust.edu.cn
FU National Key Ramp;D Program of China
FX No Statement Available
CR An P, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15030627
   An P, 2022, COMPUT VIS IMAGE UND, V214, DOI 10.1016/j.cviu.2021.103295
   Bijelic Mario, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11679, DOI 10.1109/CVPR42600.2020.01170
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Deng JJ, 2021, IEEE T CIRC SYST VID, V31, P4722, DOI 10.1109/TCSVT.2021.3100848
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Deng SH, 2022, PROC CVPR IEEE, P8438, DOI 10.1109/CVPR52688.2022.00826
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Hahner M, 2022, PROC CVPR IEEE, P16343, DOI 10.1109/CVPR52688.2022.01588
   Hahner M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15263, DOI 10.1109/ICCV48922.2021.01500
   He CH, 2022, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR52688.2022.00823
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He QD, 2022, AAAI CONF ARTIF INTE, P870
   Hu JSK, 2022, PROC CVPR IEEE, P8459, DOI 10.1109/CVPR52688.2022.00828
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li B, 2022, IEEE T IMAGE PROCESS, V31, P6532, DOI 10.1109/TIP.2022.3212906
   Li L, 2022, IEEE T MULTIMEDIA, V24, P4504, DOI 10.1109/TMM.2021.3119872
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu S, 2022, NEUROCOMPUTING, V501, P555, DOI 10.1016/j.neucom.2022.06.054
   Liu Z, 2020, AAAI CONF ARTIF INTE, V34, P11677
   Lv CL, 2022, IEEE T MULTIMEDIA, V24, P1815, DOI 10.1109/TMM.2021.3073265
   Mahmoud A, 2023, IEEE WINT CONF APPL, P663, DOI 10.1109/WACV56688.2023.00073
   Mao JG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3144, DOI 10.1109/ICCV48922.2021.00315
   Meng QH, 2022, IEEE T PATTERN ANAL, V44, P4454, DOI 10.1109/TPAMI.2021.3063611
   Noh J, 2021, PROC CVPR IEEE, P14600, DOI 10.1109/CVPR46437.2021.01437
   Prabhakaran B, 2018, IEEE T MULTIMEDIA, V20, P1037, DOI 10.1109/TMM.2018.2821959
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Song N, 2022, AAAI CONF ARTIF INTE, P2271
   Wang S., 2023, IEEE Trans. Multimedia, early access, DOI DOI 10.1109/TMM.2023.3245359
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wu H., 2023, PROC AAAI C ARTIF IN, P1
   Xu C, 2020, IEEE T MULTIMEDIA, V22, P2234, DOI 10.1109/TMM.2019.2957933
   Xu QG, 2022, AAAI CONF ARTIF INTE, P2893
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang F, 2023, IEEE T IMAGE PROCESS, V32, P2889, DOI 10.1109/TIP.2022.3147983
   Yin JB, 2022, LECT NOTES COMPUT SC, V13698, P727, DOI 10.1007/978-3-031-19839-7_42
   Yin JB, 2022, LECT NOTES COMPUT SC, V13699, P17, DOI 10.1007/978-3-031-19842-7_2
   Yin JB, 2023, IEEE T PATTERN ANAL, V45, P9822, DOI 10.1109/TPAMI.2021.3125981
   Yoo JH, 2020, SELECTED PAPERS FROM THE NINETEENTH BIENNIAL IEEE CONFERENCE ON ELECTROMAGNETIC FIELD COMPUTATION (IEEE CEFC 2020), DOI [10.1007/978-3-030-58583-9_43, 10.1109/CEFC46938.2020.9451336]
   Yu J. Lei, 2022, IEEE Trans. Geosci. Remote Sens., V60
   Zhang YN, 2021, AAAI CONF ARTIF INTE, V35, P3430, DOI 10.1609/aaai.v35i4.16456
   Zhang YF, 2022, PROC CVPR IEEE, P18931, DOI 10.1109/CVPR52688.2022.01838
   Zhao ZJ, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108120
   Zhong Yuanxin, 2021, 32 BRIT MACH VIS C 2, P284
   Zhu HQ, 2023, IEEE T MULTIMEDIA, V25, P5291, DOI 10.1109/TMM.2022.3189778
NR 52
TC 2
Z9 2
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2795
EP 2808
DI 10.1109/TMM.2023.3304054
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400019
DA 2024-08-05
ER

PT J
AU Ankur
   Kumar, R
   Sharma, AK
AF Ankur
   Kumar, Rajeev
   Sharma, Ajay K.
TI Bit-Plane Based Reversible Data Hiding in Encrypted Images Using
   Multi-Level Blocking With Quad-Tree
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Correlation; Codes; Termination of employment; Encoding;
   Encryption; Image reconstruction; Reversible data hiding; encrypted
   image; bit-plane; quad-Tree; MBQ; RDHEI
ID DIGITAL-IMAGE
AB Reversible data hiding in encrypted images (RDHEI) has gained significant popularity among security and privacy researchers as well as users, because of its features such as reversibility, embedding capacity (EC), and security. To enlarge the EC while ensuring the complete reversibility and security, we propose a bit-plane based RDHEI method based on multi-level blocking with quad-tree. The proposed method uses median edge detector (MED) as well as difference predictor to transform the original input image into a low-magnitude difference matrix. The difference matrix is then encoded by first employing a novel quad-tree based bit-plane representation strategy to exploit the intra-bit plane correlation and subsequently by inter bit-plane redundancy mitigation strategy to exploit inter bit-plane level correlation, for significantly condensing their size. Thus, a bigger room is reserved inside the cover image for embedding, so that a large amount of secret data can be hidden while ensuring the complete reversibility of the image. Experimental results validate the superiority of the proposed method over the state-of-the-art methods.
C1 [Kumar, Rajeev] Delhi Technol Univ, Delhi 110042, India.
   [Sharma, Ajay K.] Natl Inst Technol, Delhi 110036, India.
C3 Delhi Technological University; National Institute of Technology (NIT
   System); National Institute of Technology Delhi
RP Kumar, R (corresponding author), Delhi Technol Univ, Delhi 110042, India.
EM ankur_2k20phdco507@dtu.ac.in; rajeevkumar@dtu.ac.in;
   director@nitdelhi.ac.in
RI Sharma, Ajay Kumar/AAD-3627-2022; Ankur, Ankur/JXX-5889-2024; Kumar,
   Rajeev/IUP-5006-2023
OI Sharma, Ajay Kumar/0000-0003-3384-1466; Ankur,
   Ankur/0000-0003-0509-9401; Kumar, Rajeev/0000-0002-5000-7644
CR Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas T., 2017, Image Database of BOWS-2
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Celik MU, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P157
   Chen F., 2023, IEEE Trans. Multimedia, early access, DOI [10.1109/TMM.2023.3238549.[34]Y., DOI 10.1109/TMM.2023.3238549.[34]Y]
   Chen F, 2021, IEEE T CIRC SYST VID, V31, P905, DOI 10.1109/TCSVT.2020.2992817
   Chen KM, 2019, J VIS COMMUN IMAGE R, V58, P334, DOI 10.1016/j.jvcir.2018.12.023
   CHU TC, 1985, EXP MECH, V25, P232, DOI 10.1007/BF02325092
   Chunqiang Yu, 2022, IEEE Transactions on Circuits and Systems for Video Technology, V32, P451, DOI 10.1109/TCSVT.2021.3062947
   Gao H, 2024, MULTIMED TOOLS APPL, V83, P8757, DOI 10.1007/s11042-023-15939-0
   Gao L. Zhang, 2023, Digit. Signal Process., V133
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Kumar N, 2021, IEEE SIGNAL PROC LET, V28, P1335, DOI 10.1109/LSP.2021.3090673
   Kumar R, 2019, MULTIMED TOOLS APPL, V78, P22977, DOI 10.1007/s11042-019-7640-2
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   Liu Y, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10060664
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   MARKAS T, 1992, INFORM PROCESS MANAG, V28, P707, DOI 10.1016/0306-4573(92)90063-6
   Mohammadi A, 2020, IEEE T CIRC SYST VID, V30, P2366, DOI 10.1109/TCSVT.2020.2990952
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103085
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Puyang Y, 2018, IEEE INT WORKS INFOR
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Ren F, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11112537
   Sahu M, 2022, CAAI T INTELL TECHNO, V7, P695, DOI 10.1049/cit2.12130
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weber A.G., 2006, The USC-SIPI Image Database: Version 5
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Wu M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P437, DOI 10.1109/ICIP.1998.723413
   Wu Y., 2011, Journal of Selected Areas in Telecommunications (JSAT), V1, P31
   Wu YQ, 2020, IEEE T MULTIMEDIA, V22, P1929, DOI 10.1109/TMM.2019.2952979
   Xu S, 2023, IEEE T DEPEND SECURE, V20, P4199, DOI 10.1109/TDSC.2022.3219843
   Yao K., 2023, IEEE Trans. Multimedia, early access, DOI [10.1109/ACCESS.2020.296925, DOI 10.1109/ACCESS.2020.296925]
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yi S, 2017, SIGNAL PROCESS, V133, P40, DOI 10.1016/j.sigpro.2016.10.017
   Yin ZX, 2022, IEEE T DEPEND SECURE, V19, P992, DOI 10.1109/TDSC.2020.3019490
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Zhang XP, 2016, IEEE T CIRC SYST VID, V26, P1622, DOI 10.1109/TCSVT.2015.2433194
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zhou JT, 2016, IEEE T CIRC SYST VID, V26, P441, DOI 10.1109/TCSVT.2015.2416591
NR 45
TC 2
Z9 2
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4722
EP 4735
DI 10.1109/TMM.2023.3325993
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100021
DA 2024-08-05
ER

PT J
AU Chen, C
   Jin, A
   Wang, ZY
   Zheng, YW
   Yang, BS
   Zhou, J
   Xu, YH
   Tu, ZG
AF Chen, Chi
   Jin, Ang
   Wang, Zhiye
   Zheng, Yongwei
   Yang, Bisheng
   Zhou, Jian
   Xu, Yuhang
   Tu, Zhigang
TI SGSR-Net: Structure Semantics Guided LiDAR Super-Resolution Network for
   Indoor LiDAR SLAM
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Laser radar; Superresolution; Sensors;
   Simultaneous localization and mapping; Three-dimensional displays; Task
   analysis; Point cloud; Super-Resolution; SLAM; LiDAR
AB Multi-Beam LiDAR (MBL) sensors sample the real-world with discrete 3D point clouds (PC) and have become a major and essential 3D sensing capability for autonomous robots. To ensure an accurate point sampling on surfaces, high-resolution MBL sensors (e.g., Ouster OS0-128) are commonly used to collect dense point clouds for robot tasks, including object detection and tracking, simultaneous localization and mapping (SLAM), in applications such as autonomous driving vehicles (ADVs). However, the high cost and large volume/weight/energy consumption of such sensors limit their usage in broader applications such as UAV/UGV swarms with small-scale agents with limited payload. Existing studies on Super-Resolution (SR) upsampling of the PC from low-resolution MBL have not considered the geometry semantics of the scenes, thus resulting in less optimal SR points for downstream subtasks (e.g., SLAM). Thus, this article proposes SGSR-Net, a structure semantics-guided MBL Super-Resolution network. SGSR-Net takes the low-resolution range images of the MBL sensors as input and produces dense and structure-aware Super-Resolution point cloud from those sparse measurements through a vertical spatial and channel attention-enhanced CNN model coupling with guided Monte Carlo filtering, for indoor LiDAR-SLAM applications. The SGSR-Net is validated using datasets collected by a UGV equipped with multiple MBL sensors. The results demonstrate that the proposed CG-LSR (CASE Attention Guided Encoder-Decoder LiDAR Super-Resolution Network) reduces the MAE of the SR points by 12.4% down to 0.177 m when compared with the state-of-the-art (SOTA) method Shan et al. (2020), Ren et al. (2021), Kwon et al. (2022), Long and Wang (2022). The indoor SLAM results with SR-points produced by SGSR-Net show that the mean and RMSE of the absolute pose error (APE) are decreased by 27% and 30%, down to 0.849 m and 0.902 m, respectively, which significantly improve the indoor-SLAM performance and stability of SOTA LiDAR-SLAM systems (i.e. LeGO-LOAM Shan and Englot (2018), Dellenbach et al. (2022), Vizzo et al. (2023), Zhang and Singh (2014)).
C1 [Chen, Chi; Jin, Ang; Wang, Zhiye; Zheng, Yongwei; Yang, Bisheng; Zhou, Jian; Xu, Yuhang; Tu, Zhigang] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & Re, Wuhan 430079, Peoples R China.
C3 Wuhan University
RP Chen, C (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & Re, Wuhan 430079, Peoples R China.
EM chichen@whu.edu.cn; angjin@whu.edu.cn; zhiye.wang@whu.edu.cn;
   2016301610307@whu.edu.cn; bshyang@whu.edu.cn; jianzhou@whu.edu.cn;
   2021206190006@whu.edu.cn; tuzhigang@whu.edu.cn
RI Al-obaidi, Abdullah Thair/P-8487-2017; Yang, Bisheng/A-4642-2013
OI Al-obaidi, Abdullah Thair/0000-0002-9971-5895; Zhou,
   Jian/0000-0001-6707-6542; Yang, Bisheng/0000-0001-7736-0803
FU National Natural Science Foundation of China
FX No Statement Available
CR Akhtar A, 2022, IEEE T IMAGE PROCESS, V31, P4133, DOI 10.1109/TIP.2022.3180904
   Bansal P, 2017, TRANSPORT RES A-POL, V95, P49, DOI 10.1016/j.tra.2016.10.013
   Beltrán J, 2018, IEEE INT C INTELL TR, P3517, DOI 10.1109/ITSC.2018.8569311
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chen C, 2022, INT J APPL EARTH OBS, V112, DOI 10.1016/j.jag.2022.102960
   Cheng XJ, 2020, AAAI CONF ARTIF INTE, V34, P10615
   Chenglin Pang, 2019, 2019 IEEE International Conference on Real-time Computing and Robotics (RCAR). Proceedings, P868, DOI 10.1109/RCAR47638.2019.9044147
   Cong YZ, 2022, ISPRS J PHOTOGRAMM, V186, P232, DOI 10.1016/j.isprsjprs.2022.02.005
   Dellenbach P, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022), P5580, DOI 10.1109/ICRA46639.2022.9811849
   Ding WD, 2020, IEEE INT CONF ROBOT, P4322, DOI 10.1109/icra40945.2020.9196698
   Ebadi K, 2020, IEEE INT CONF ROBOT, P80, DOI [10.1109/icra40945.2020.9197082, 10.1109/ICRA40945.2020.9197082]
   Eldesokey A, 2020, IEEE T PATTERN ANAL, V42, P2423, DOI 10.1109/TPAMI.2019.2929170
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P3022, DOI 10.1109/TMM.2021.3068606
   Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470
   Gao YB, 2023, IEEE T INTELL TRANSP, V24, P2158, DOI 10.1109/TITS.2022.3140355
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Grupp M., 2017, EVO: Python package for the evaluation of odometry andSLAM
   Han XG, 2019, PROC CVPR IEEE, P234, DOI 10.1109/CVPR.2019.00032
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu JJ, 2023, IEEE T PATTERN ANAL, V45, P8244, DOI 10.1109/TPAMI.2022.3229090
   Huang TX, 2023, IEEE T MULTIMEDIA, V25, P5903, DOI 10.1109/TMM.2022.3200851
   Huang ZX, 2020, IEEE T IMAGE PROCESS, V29, P3429, DOI 10.1109/TIP.2019.2960589
   Jaritz M, 2018, INT CONF 3D VISION, P52, DOI 10.1109/3DV.2018.00017
   Karam S, 2021, ISPRS J PHOTOGRAMM, V181, P413, DOI 10.1016/j.isprsjprs.2021.09.020
   Ku J, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P16, DOI 10.1109/CRV.2018.00013
   Kwon YS, 2022, IEEE INT CONF ROBOT, P8424, DOI 10.1109/ICRA46639.2022.9811992
   Lee BU, 2021, PROC CVPR IEEE, P13911, DOI 10.1109/CVPR46437.2021.01370
   Li L, 2023, IEEE T MULTIMEDIA, V25, P3855, DOI 10.1109/TMM.2022.3167810
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li ZQ, 2022, LECT NOTES COMPUT SC, V13669, P1, DOI 10.1007/978-3-031-20077-9_1
   Li ZZ, 2023, IEEE T MULTIMEDIA, V25, P3432, DOI 10.1109/TMM.2022.3160604
   Lin YK, 2022, AAAI CONF ARTIF INTE, P1638
   Long C., 2022, P 30 ACM INT C MULT, P10
   Ma FC, 2018, IEEE INT CONF ROBOT, P4796
   Ma RQ, 2022, ISPRS J PHOTOGRAMM, V191, P33, DOI 10.1016/j.isprsjprs.2022.07.006
   Park J., 2017, P EUR C COMP VIS, P120
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qiu JX, 2019, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR.2019.00343
   Ren L., 2021, P COLL COMP NETW APP, P697
   Shan TX, 2020, ROBOT AUTON SYST, V134, DOI 10.1016/j.robot.2020.103647
   Shan TX, 2018, IEEE INT C INT ROBOT, P4758, DOI 10.1109/IROS.2018.8594299
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang J, 2021, IEEE T IMAGE PROCESS, V30, P1116, DOI 10.1109/TIP.2020.3040528
   Tian D, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3204097
   Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012
   Vizzo I, 2023, IEEE ROBOT AUTOM LET, V8, P1029, DOI 10.1109/LRA.2023.3236571
   Wang H, 2021, NEUROCOMPUTING, V443, P247, DOI 10.1016/j.neucom.2021.03.010
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Z, 2023, IEEE T MULTIMEDIA, V25, P3905, DOI 10.1109/TMM.2022.3168423
   Weiss U, 2011, ROBOT AUTON SYST, V59, P265, DOI 10.1016/j.robot.2011.02.011
   Wen W, 2019, ELECTRON LETT, V55, P348, DOI 10.1049/el.2018.8075
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu W, 2021, IEEE ROBOT AUTOM LET, V6, P3317, DOI 10.1109/LRA.2021.3064227
   Yan ZQ, 2022, LECT NOTES COMPUT SC, V13687, P214, DOI 10.1007/978-3-031-19812-0_13
   Yang X, 2019, J COMPUT SCI TECH-CH, V34, P1123, DOI 10.1007/s11390-019-1964-2
   Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yue J, 2020, Arxiv, DOI [arXiv:2008.03694, DOI arXiv:2008.03694.null]
   Zhang J., 2014, Robotics: Science and systems, P9, DOI 10.15607/RSS.2014.X.007
   Zhang J., 2022, arXiv
   Zhang YN, 2021, AAAI CONF ARTIF INTE, V35, P3430, DOI 10.1609/aaai.v35i4.16456
   Zhang YD, 2018, PROC CVPR IEEE, P175, DOI 10.1109/CVPR.2018.00026
   Zhao WB, 2022, PROC CVPR IEEE, P1998, DOI 10.1109/CVPR52688.2022.00204
   Zhu YW, 2021, IEEE INT CONF ROBOT, P5049, DOI 10.1109/ICRA48506.2021.9561149
NR 66
TC 4
Z9 4
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1842
EP 1854
DI 10.1109/TMM.2023.3289752
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800013
DA 2024-08-05
ER

PT J
AU Deng, JH
   Zhang, XY
   Li, W
   Duan, LX
   Xu, D
AF Deng, Jinhong
   Zhang, Xiaoyue
   Li, Wen
   Duan, Lixin
   Xu, Dong
TI Cross-Domain Detection Transformer Based on Spatial-Aware and
   Semantic-Aware Token Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Training; Object detection; Feature extraction; Task
   analysis; Semantics; Decoding; Detection transformer; domain adaptation;
   object detection
AB Detection transformers such as DETR (Carion et al., 2020) have recently exhibited promising performance for many object detection tasks, but the generalization ability of those methods is still quite limited for cross-domain adaptation scenarios. To address the cross-domain issue, a straightforward method is to perform token alignment with adversarial training in transformers. However, its performance is often unsatisfactory because the tokens in detection transformers are quite diverse and represent different spatial and semantic information. In this paper, we propose a new method for cross-domain detection transformers called spatial-aware and semantic-aware token alignment (SSTA). Specifically, we take advantage of the characteristics of cross-attention as used in the detection transformer and propose spatial-aware token alignment (SpaTA) and semantic-aware token alignment (SemTA) strategies to guide the token alignment across domains. For spatial-aware token alignment, we extract the information from the cross-attention map (CAM) to align the distribution of tokens according to their attention to object queries. For semantic-aware token alignment, we inject the category information into the cross-attention map and construct domain embedding to guide the learning of a multi-class discriminator to model the category relationship and achieve category-level token alignment during the entire adaptation process. We conduct extensive experiments on several widely-used benchmarks, and the results clearly show the effectiveness of our proposed approach over existing state-of-the-art methods.
C1 [Deng, Jinhong; Zhang, Xiaoyue; Duan, Lixin; Xu, Dong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Li, Wen] Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Shenzhen 518110, Peoples R China.
   [Xu, Dong] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Shenzhen Institute for
   Advanced Study, UESTC; University of Hong Kong
RP Li, W (corresponding author), Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Shenzhen 518110, Peoples R China.
EM jhdengvision@gmail.com; xzhangeo@connect.ust.hk; liwen@uestc.edu.cn;
   lxduan@uestc.edu.cn; dongxu@hku.hk
OI Duan, Lixin/0000-0002-0723-4016; Li, Wen/0000-0002-5559-8594
FU National Natural Science Foundation of China
FX No Statement Available
CR Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chang-Dong Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11721, DOI 10.1109/CVPR42600.2020.01174
   Chen C., 2020, P IEEECVF C COMPUTER, P8869, DOI DOI 10.1109/CVPR42600.2020.00889
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Chen SY, 2022, LECT NOTES COMPUT SC, V13141, P272, DOI 10.1007/978-3-030-98358-1_22
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Cheng-Chun Hsu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P733, DOI 10.1007/978-3-030-58545-7_42
   Congcong Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P481, DOI 10.1007/978-3-030-58601-0_29
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JJ, 2021, IEEE T MULTIMEDIA, V23, P846, DOI 10.1109/TMM.2020.2990070
   Deng JH, 2021, PROC CVPR IEEE, P4089, DOI 10.1109/CVPR46437.2021.00408
   Ganin Y, 2016, J MACH LEARN RES, V17
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong KX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1543, DOI 10.1145/3503161.3548246
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P642, DOI 10.1007/978-3-030-58568-6_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZW, 2019, IEEE I CONF COMP VIS, P6667, DOI 10.1109/ICCV.2019.00677
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/wacv45572.2020.9093358, 10.1109/WACV45572.2020.9093358]
   Jelodar AB, 2019, IEEE T MULTIMEDIA, V21, P1813, DOI 10.1109/TMM.2018.2885228
   Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092
   Kim T, 2019, PROC CVPR IEEE, P12448, DOI 10.1109/CVPR.2019.01274
   Kingma D.P., 2014, Proc. of ICLR
   Kouw WM, 2021, IEEE T PATTERN ANAL, V43, P766, DOI 10.1109/TPAMI.2019.2945942
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JL, 2023, IEEE WINT CONF APPL, P612, DOI 10.1109/WACV56688.2023.00068
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li WY, 2023, IEEE T MULTIMEDIA, V25, P7051, DOI 10.1109/TMM.2022.3217388
   Li WY, 2022, AAAI CONF ARTIF INTE, P1421
   Li WY, 2022, PROC CVPR IEEE, P5281, DOI 10.1109/CVPR52688.2022.00522
   Li YJ, 2022, PROC CVPR IEEE, P7571, DOI 10.1109/CVPR52688.2022.00743
   Lin C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8751, DOI 10.1109/ICCV48922.2021.00865
   Liu DN, 2023, IEEE T MULTIMEDIA, V25, P1333, DOI 10.1109/TMM.2022.3141614
   Meng DP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3631, DOI 10.1109/ICCV48922.2021.00363
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Munir MA, 2021, ADV NEUR IN
   Pan YS, 2020, IEEE WINT CONF APPL, P1313, DOI [10.1109/wacv45572.2020.9093287, 10.1109/WACV45572.2020.9093287]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezaeianaran F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9184, DOI 10.1109/ICCV48922.2021.00907
   Roh B., 2022, P INT C LEARN REPR
   RoyChowdhury A, 2019, PROC CVPR IEEE, P780, DOI 10.1109/CVPR.2019.00087
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun KY, 2023, IEEE ROBOT AUTOM LET, V8, P3645, DOI 10.1109/LRA.2023.3267692
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Tian K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9113, DOI 10.1109/ICCV48922.2021.00900
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   VS Vibashan, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P4514, DOI 10.1109/CVPR46437.2021.00449
   Wang W, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1730, DOI 10.1145/3474085.3475317
   Wang YM, 2022, AAAI CONF ARTIF INTE, P2567
   Wu AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9322, DOI 10.1109/ICCV48922.2021.00921
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yu JZ, 2022, LECT NOTES COMPUT SC, V13669, P629, DOI 10.1007/978-3-031-20077-9_37
   Zhang Hang, 2022, P INT C LEARN REPR
   Zhang JY, 2021, Arxiv, DOI arXiv:2103.17084
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhao L, 2022, PROC CVPR IEEE, P14197, DOI 10.1109/CVPR52688.2022.01382
   Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503
   Zheng Yangtao, 2020, P IEEE CVF C COMP VI, P13766
   Zhou WZ, 2022, PROC CVPR IEEE, P9571, DOI 10.1109/CVPR52688.2022.00936
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
   Zhu XG, 2019, PROC CVPR IEEE, P687, DOI 10.1109/CVPR.2019.00078
NR 69
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5234
EP 5245
DI 10.1109/TMM.2023.3330524
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600047
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Du, JF
   Zhou, SL
   Yu, J
   Han, P
   Shang, S
AF Du, Jiangfeng
   Zhou, Silin
   Yu, Jie
   Han, Peng
   Shang, Shuo
TI Cross-Task Multimodal Reinforcement for Long Tail Next POI
   Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Next POI recommendation; multimodal; long tail; cross-task
ID GRAPH
AB Next Point-of-Interest (POI) recommendation seeks to recommend locations that users are most likely to visit next based on their historical trajectories, providing both users and service providers with substantial benefits. However, most next POI recommendation methods calculate the distances between POIs when mining spatial information and adjust their weights accordingly, ignoring the characteristics and multimedia content features of the regions in which POIs are located. In addition, the next POI recommendations suffer from the long tail effect, in which only a small portion of POIs appear frequently in users' recommendation lists due to their high popularity, while remainders maintain a low presence. To this end, we propose the cross-task multimodal reinforcement method which enriches the representations of regions by incorporating information from auxiliary domains. Moreover, we devise a cross-task reinforcement module to effectively integrate the local representations with pre-trained encoders from auxiliary domains. Actually, the enhanced region representations contain constructive district properties which are helpful to find proper POIs that suit users' tastes and thus alleviate the long tail effect. Experiments conducted on two real-world datasets indicate that our proposed method outperforms the state-of-the-art models in terms of both general performance and that of niche POIs.
C1 [Du, Jiangfeng; Zhou, Silin; Yu, Jie; Han, Peng; Shang, Shuo] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Du, Jiangfeng; Zhou, Silin; Yu, Jie; Han, Peng; Shang, Shuo] Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Shenzhen Institute for
   Advanced Study, UESTC
RP Han, P; Shang, S (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.; Han, P; Shang, S (corresponding author), Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Chengdu 611731, Peoples R China.
EM dujiangfeng250@gmail.com; zhousilinxy@gmail.com; JieYu_AI@hotmail.com;
   pengh@cs.aau.dk; jedi.shang@gmail.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen LS, 2020, AAAI CONF ARTIF INTE, V34, P582
   Chen XS, 2021, IEEE T MULTIMEDIA, V23, P484, DOI 10.1109/TMM.2020.2978618
   Chen YD, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2692, DOI 10.1145/3447548.3467132
   Cheng C., 2013, Proceedings of the Twenty- Third international joint conference on Artificial Intelligence, P2605
   Cheng C, 2012, P 26 AAAI C ART INT, P17
   Feng J, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1459, DOI 10.1145/3178876.3186058
   Feng SS, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2069
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   Han P, 2022, AAAI CONF ARTIF INTE, P4014
   Han P, 2022, IEEE INT WORKSH MULT, DOI 10.1109/MMSP55362.2022.9949116
   Han P, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P556, DOI 10.1145/3447548.3467337
   Han P, 2022, IEEE T KNOWL DATA EN, V34, P5484, DOI 10.1109/TKDE.2021.3059744
   Han P, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2484
   Hu XJ, 2021, NEUROCOMPUTING, V428, P376, DOI 10.1016/j.neucom.2020.01.118
   Jiang RH, 2022, NEUROCOMPUTING, V472, P338, DOI 10.1016/j.neucom.2020.10.001
   Li JC, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P322, DOI 10.1145/3336191.3371786
   Li J, 2023, IEEE T KNOWL DATA EN, V35, P3072, DOI 10.1109/TKDE.2021.3118469
   Li J, 2022, IEEE T KNOWL DATA EN, V34, P828, DOI 10.1109/TKDE.2020.2983360
   Li K., 2021, IJCAI, P3656
   Lian DF, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2009, DOI 10.1145/3394486.3403252
   Lim N, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P1133, DOI 10.1145/3477495.3531989
   Liu Dachuan, 2022, KDD '22: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, P1042, DOI 10.1145/3534678.3539397
   Liu Q, 2016, AAAI CONF ARTIF INTE, P194
   Luo YT, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2177, DOI 10.1145/3442381.3449998
   Mathew W, 2012, UBICOMP'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P911
   Mnih V, 2014, ADV NEUR IN, V27
   Pang X., 2023, P ACM WEB C, P1754
   Qin Yifang, 2023, WSDM '23: Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, P508, DOI 10.1145/3539597.3570408
   Quanjun Chen, 2020, SIGSPATIAL '20: Proceedings of the 28th International Conference on Advances in Geographic Information Systems, P283, DOI 10.1145/3397536.3422221
   Quintanilla E, 2021, IEEE T MULTIMEDIA, V23, P1083, DOI 10.1109/TMM.2020.2992941
   Rao X., 2022, P INT JOINT C ART IN, P2022
   Rao X, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P1463, DOI 10.1145/3534678.3539383
   Rendle S, 2010, P 19 INT C WORLD WID, P811
   Sang L, 2021, IEEE T MULTIMEDIA, V23, P2019, DOI 10.1109/TMM.2020.3007330
   Shang S, 2020, GEOINFORMATICA, V24, P1, DOI 10.1007/s10707-020-00397-9
   Sun HM, 2021, WORLD WIDE WEB, V24, P1749, DOI 10.1007/s11280-021-00895-2
   Sun Huimin, 2021, IJCAI, P3017, DOI DOI 10.24963/IJCAI.2021/415
   Sun K, 2020, AAAI CONF ARTIF INTE, V34, P214
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3877
   Wu S., 2022, IJCAI, P2312
   Yang DQ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2184
   Yang DQ, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2147, DOI 10.1145/3308558.3313635
   Yang DQ, 2015, IEEE T SYST MAN CY-S, V45, P129, DOI 10.1109/TSMC.2014.2327053
   Yang S, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P1144, DOI 10.1145/3477495.3531983
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Ye J., 2013, P 2013 SIAM INT C DA, P171, DOI 10.1137/1.9781611972832.19
   Yu FQ, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1264, DOI 10.1145/3366423.3380202
   Zhang HW, 2021, IEEE T MULTIMEDIA, V23, P4441, DOI 10.1109/TMM.2020.3042055
   Zhang J., 2014, SIGSPATIAL, P103, DOI 10.1145/2666310.2666400
   Zhao PP, 2019, AAAI CONF ARTIF INTE, P5877
   Zhao ZX, 2019, 2019 IEEE SMARTWORLD, UBIQUITOUS INTELLIGENCE & COMPUTING, ADVANCED & TRUSTED COMPUTING, SCALABLE COMPUTING & COMMUNICATIONS, CLOUD & BIG DATA COMPUTING, INTERNET OF PEOPLE AND SMART CITY INNOVATION (SMARTWORLD/SCALCOM/UIC/ATC/CBDCOM/IOP/SCI 2019), P1117, DOI 10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00211
   Zhou P. Han, 2022, World Wide Web, V10
   Zhou SL, 2022, IEEE INT WORKSH MULT, DOI 10.1109/MMSP55362.2022.9949145
NR 54
TC 0
Z9 0
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1996
EP 2005
DI 10.1109/TMM.2023.3290723
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800011
DA 2024-08-05
ER

PT J
AU Gao, Y
   Li, X
   Hui, Y
AF Gao, Yuan
   Li, Xin
   Hui, Yan
TI Rethinking Graph Contrastive Learning: An Efficient Single-View Approach
   via Instance Discrimination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Single-view graph contrastive learning; instance discrimination;
   alignment; uniformity
AB With the successful integration of contrastive learning and graph neural networks, graph contrastive learning (GCL) has demonstrated superior performance in graph representation learning. Majority of earlier works tend to utilize dual-view frameworks. However, they require high computational costs; additionally, we observed that they can hardly obtain robust result as the training processes swing between two important metrics: alignment and uniformity. We address these problems by designing a novel single-view paradigm called Light Single-view Graph Contrastive Learning (LSGCL). To reduce time consumption, we use a single-view framework. Specifically, the input graph is fed directly into a message-passing pattern encoder by concatenating the row-wise normalized hidden representations. Next, a novel single-view instance discrimination is applied, redefining the anchor, positive, and negative samples. The anchor is the positive sample, and the other nodes are negative. We also discuss why LSGCL can successfully achieve the trade-off between alignment and uniformity. In particular, the obtained representation is perfectly aligned, and visualizations show that the representation can provide cutting-edge value under uniformity. Albeit simple, our LSGCL can produce comparable performance or better than state-of-the-art methods while only incurring about 20% time cost compared to the state-of-the-art baselines.
C1 [Gao, Yuan; Hui, Yan] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Li, Xin] Nanjing Univ Posts & Telecommun, Sch Internet Things, Nanjing 210094, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of Posts
   & Telecommunications
RP Hui, Y (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM maxgaoyuan@163.com; lixin@njupt.edu.cn; yanhui@njust.edu.cn
OI Gao, Yuan/0000-0002-3635-9635
FU National Natural Science Foundation of China
FX No Statement Available
CR Abu-El-Haifa S, 2019, PR MACH LEARN RES, V97
   Adhikari B, 2018, LECT NOTES ARTIF INT, V10938, P170, DOI 10.1007/978-3-319-93037-4_14
   Armandpour M, 2019, AAAI CONF ARTIF INTE, P3191
   Bachman P, 2019, ADV NEUR IN, V32
   Borgwardt KM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P74, DOI 10.1109/ICDM.2005.132
   Cai X., 2023, P INT C LEARN REPR
   Chen Ming, 2020, P MACHINE LEARNING R, V119
   Chen T, 2020, PR MACH LEARN RES, V119
   Chien Eli, 2020, P INT C LEARN REPR
   Glorot X., Understanding the difficulty of training deep feedforward neural networks, V9, P249
   Gong XM, 2023, AAAI CONF ARTIF INTE, P4284
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Guo X., 2023, P INT C LEAM REPR
   Gutmann A., 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hassani K, 2020, PR MACH LEARN RES, V119
   Kingma D.P., 2015, PROC INT C LEARN RE
   Kipf T., 2016, ARXIV
   Kriege N. M, 2012, P 29 INT COFERENCE I, P291
   Kumar S, 2022, INFORM SCIENCES, V607, P1617, DOI 10.1016/j.ins.2022.06.075
   Li B., 2023, P INT C WEB SEARCH D
   Li HF, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3248871
   LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36
   Liu K, 2023, IEEE T MULTIMEDIA, V25, P9343, DOI 10.1109/TMM.2023.3251108
   Liu M, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P338, DOI 10.1145/3394486.3403076
   Liu YJ, 2023, IEEE T MULTIMEDIA, V25, P126, DOI 10.1109/TMM.2021.3121564
   Liu Y, 2020, IEEE ACM T COMPUT BI, V17, P435, DOI 10.1109/TCBB.2018.2848904
   Mo YJ, 2022, AAAI CONF ARTIF INTE, P7797
   Narayanan A., 2017, P INT C LEARN REPR
   Noekhah S, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102140
   Peng H, 2023, IEEE T PATTERN ANAL, V45, P980, DOI 10.1109/TPAMI.2022.3144993
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Poole B, 2019, PR MACH LEARN RES, V97
   Shao P., 2021, P INT C LEARN REPR
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Sun F.-Y., 2020, P INT C LEARN REPR
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Tschannen M., 2019, INT C LEARN REPR
   Velickovic P., 2019, ICLR POSTER, V2, P4
   Vishwanathan S.V.N., 2009, AISTATS, V5, P488
   Wang Haonan, 2022, arXiv, DOI DOI 10.48550/ARXIV.2204.04874
   Wang R., 2022, Advances in Neural Information Processing Systems, V35, P32465
   Wang T., 2020, International Conference on Machine Learning, P9929
   Wang Y, 2022, INFORM SCIENCES, V607, P869, DOI 10.1016/j.ins.2022.05.127
   Wu F, 2021, INFORM SCIENCES, V578, P683, DOI 10.1016/j.ins.2021.07.041
   Wu F, 2019, PR MACH LEARN RES, V97
   Wu ZH, 2023, IEEE T MULTIMEDIA, V25, P267, DOI 10.1109/TMM.2021.3125130
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xu C, 2022, INFORM SCIENCES, V607, P783, DOI 10.1016/j.ins.2022.06.010
   Xu J., 2020, P ACMIEEE JOINT C DI, P77
   Xu K, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225060
   Xu KYL, 2018, PR MACH LEARN RES, V80
   Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417
   Yang HR, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P1238, DOI 10.1145/3485447.3512211
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Yin YH, 2022, AAAI CONF ARTIF INTE, P8892
   You Yuning, 2020, Adv Neural Inf Process Syst
   Zheng Y., 2022, P ADV NEUR INF PROC
   Zhu H, 2021, Advances in Neural Information Processing Systems, V34
   Zhu Y., 2020, P INT C LEARN REPR
NR 60
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3616
EP 3625
DI 10.1109/TMM.2023.3313267
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200033
DA 2024-08-05
ER

PT J
AU Li, ZX
   Ye, WN
   Jiang, TT
   Huang, TJ
AF Li, Zhixuan
   Ye, Weining
   Jiang, Tingting
   Huang, Tiejun
TI GIN: Generative INvariant Shape Prior for Amodal Instance Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Task analysis; Artificial intelligence; Three-dimensional
   displays; Dictionaries; Transformers; Knowledge engineering; Amodal
   perception; instance segmentation; shape prior
AB Amodal instance segmentation (AIS) predicts the complete shape of the occluded object, including both visible and occluded regions. Because visual clues are lacking, the occluded region is difficult to segment accurately. In human amodal perception, shape-prior knowledge is helpful for AIS. The previous method uses a 2D shape prior by rote memorizing, establishing a shape dictionary and retrieving the closest mask to the segmentation result. However, this approach cannot obtain the shape prior, which is not prestored in the shape dictionary. In this article, to improve generalization ability, we propose a generative invariant shape-prior network (GIN), simulating the human perception process that learns the basic shape, which is invariant to transformations, including translation, rotation, and scaling. We design a novel framework that decouples the learning of shape priors from transformation. GIN is end-to-end trainable and needs no dictionary establishment, making the whole pipeline efficient. GIN outperforms state-of-the-art methods on three public datasets (D2SA, COCOA-cls, and KINS) with large margins.
C1 [Li, Zhixuan; Ye, Weining; Jiang, Tingting; Huang, Tiejun] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
   [Huang, Tiejun] Beijing Acad Artificial Intelligence, Beijing 100084, Peoples R China.
C3 Peking University
RP Jiang, TT (corresponding author), Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
EM zhixuanli@pku.edu.cn; ywning@pku.edu.cn; ttjiang@pku.edu.cn;
   tjhuang@pku.edu.cn
RI Li, Zhixuan/JXM-7018-2024
OI Li, Zhixuan/0000-0002-2558-508X; Jiang, Tingting/0000-0002-5372-0656
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2019, IEEECVF C COMPUT VIS
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Carion N., 2020, EUR C COMP VIS, P213
   Cerrone L, 2019, PROC CVPR IEEE, P12551, DOI 10.1109/CVPR.2019.01284
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Cheng D, 2023, IEEE T CIRC SYST VID, V33, P4244, DOI 10.1109/TCSVT.2023.3243205
   Cheng D, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109270
   de Wit TCJ, 2008, INFANCY, V13, P660, DOI 10.1080/15250000802458864
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Ehsani K, 2018, PROC CVPR IEEE, P6144, DOI 10.1109/CVPR.2018.00643
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Ferrari C, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P509, DOI 10.1109/3DV.2015.63
   Follmann P, 2019, IEEE WINT CONF APPL, P1328, DOI 10.1109/WACV.2019.00146
   Follmann P, 2018, LECT NOTES COMPUT SC, V11214, P581, DOI 10.1007/978-3-030-01249-6_35
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gonzalez R.C., 2008, DIGITAL IMAGE PROCES
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang P., 2022, P IEEE CVF C COMP VI, P7622
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Jaderberg M, 2015, ADV NEUR IN, V28
   Ke L, 2021, PROC CVPR IEEE, P4018, DOI 10.1109/CVPR46437.2021.00401
   Lee Y., 2020, CVPR, P13906
   Li JS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P45, DOI 10.1145/3240508.3240515
   Li Jianshu., 2017, Multiple-human parsing in the wild
   Li K, 2016, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2016.398
   Li K, 2016, LECT NOTES COMPUT SC, V9906, P677, DOI 10.1007/978-3-319-46475-6_42
   Li TP, 2022, IEEE T MULTIMEDIA, V24, P492, DOI 10.1109/TMM.2021.3054526
   Li Z., 2023, PROC IEEE INT C ACOU, P1
   Li Z., 2023, P IEEECVF INT C COMP, P23504
   Li ZX, 2022, LECT NOTES COMPUT SC, V13689, P165, DOI 10.1007/978-3-031-19818-2_10
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ling Huan, 2020, ADV NEURAL INFORM PR, V33, P16246
   Liu ST, 2019, PROC CVPR IEEE, P6452, DOI 10.1109/CVPR.2019.00662
   Liu YF, 2019, LECT NOTES COMPUT SC, V11662, P102, DOI 10.1007/978-3-030-27202-9_9
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Nagasaka Y, 2007, PERCEPT PSYCHOPHYS, V69, P596, DOI 10.3758/BF03193917
   Patel VM, 2012, IEEE T INF FOREN SEC, V7, P954, DOI 10.1109/TIFS.2012.2189205
   Qiu Q, 2012, LECT NOTES COMPUT SC, V7575, P631, DOI 10.1007/978-3-642-33765-9_45
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao Shuai, 2018, ARXIV180500123
   Song X., 2020, P EUR C COMP VIS AUG, P32
   Sugita Y, 1999, NATURE, V401, P269, DOI 10.1038/45785
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tran M, 2022, PROC BRIT MACH VIS C
   Wang T, 2021, IEEE T MULTIMEDIA, V23, P3239, DOI 10.1109/TMM.2020.3021979
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Xiao YT, 2021, AAAI CONF ARTIF INTE, V35, P2995
   Xie J, 2021, IEEE T IMAGE PROCESS, V30, P3872, DOI 10.1109/TIP.2020.3040854
   Xu K, 2021, IEEE T MULTIMEDIA, V23, P3530, DOI 10.1109/TMM.2020.3026913
   Xu Y, 2017, IEEE T BIO-MED ENG, V64, P2901, DOI 10.1109/TBME.2017.2686418
   Xuangeng Chu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12211, DOI 10.1109/CVPR42600.2020.01223
   Yan XS, 2019, IEEE I CONF COMP VIS, P7617, DOI 10.1109/ICCV.2019.00771
   Yao S., 2018, PROC INT C ADV NEURA, P1891
   Yao Z., 2021, Efficient DETR: Improving end-to-end object detector with dense prior
   Zhan XH, 2020, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR42600.2020.00384
   Zhang K., 2013, IEEE INT WORKSHOPMAC, P1
   Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39
   Zhang ZH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2124, DOI 10.1145/3343031.3350911
   Zhang ZY, 2016, PROC CVPR IEEE, P669, DOI 10.1109/CVPR.2016.79
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhu X., 2020, PROC INT C LEARN REP, P1
   Zhu Y, 2017, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2017.320
NR 64
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3924
EP 3936
DI 10.1109/TMM.2023.3318075
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300007
DA 2024-08-05
ER

PT J
AU Qin, C
   Li, XM
   Zhang, ZY
   Li, FY
   Zhang, XP
   Feng, GR
AF Qin, Chuan
   Li, Xiaomeng
   Zhang, Zhenyi
   Li, Fengyong
   Zhang, Xinpeng
   Feng, Guorui
TI Print-Camera Resistant Image Watermarking With Deep Noise Simulation and
   Constrained Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Robust watermarking; print-camera resistance; image quality; noise
   simulation network; just noticeable difference
AB In this article, an effective print-camera (P-C) resistant image watermarking scheme is proposed. To achieve watermark robustness, most of existing works try to simulate P-C noise by a sophisticated math model. However, the diversity of P-C noises in the real world is ignored, and the watermarked image may not attain a good balance between high robustness and low distortion. To address the problem, we construct an efficient end-to-end network architecture for watermark embedding and extraction. To be specific, a deep noise simulation network (NSN) is designed to simulate the fusion process of real P-C noises, which can help to generate high-robust watermarked image. Also, a multitask loss function based on just-noticeable-difference (JND) is proposed to conduct constrained learning for residual image containing watermark information, thus, the distortion of generated watermarked image can be significantly reduced. Experimental results show that our scheme can achieve high robustness against P-C process while maintaining a satisfactory watermark capacity and visual quality of watermarked image.
C1 [Qin, Chuan; Li, Xiaomeng; Zhang, Zhenyi] Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
   [Li, Fengyong] Shanghai Univ Elect Power, Coll Comp Sci & Technol, Shanghai 200090, Peoples R China.
   [Zhang, Xinpeng; Feng, Guorui] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 University of Shanghai for Science & Technology; Shanghai University of
   Electric Power; Shanghai University
RP Feng, GR (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM qin@usst.edu.cn; 202330353@st.usst.edu.cn; justicefy@163.com;
   fyli@shiep.edu.cn; xzhang@shu.edu.cn; grfeng@shu.edu.cn
RI Qin, Chuan/C-1106-2017
OI Qin, Chuan/0000-0002-0370-4623; Li, Fengyong/0000-0002-3385-8164
FU National Natural Science Foundation of China
FX No Statement Available
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Boracchi G, 2012, IEEE T IMAGE PROCESS, V21, P3502, DOI 10.1109/TIP.2012.2192126
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Chu WC, 2003, IEEE T MULTIMEDIA, V5, P34, DOI 10.1109/TMM.2003.808816
   Cui H, 2019, IEEE INFOCOM SER, P1315, DOI [10.1109/INFOCOM.2019.8737627, 10.1109/infocom.2019.8737627]
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gourrame K, 2019, MULTIMED TOOLS APPL, V78, P2621, DOI 10.1007/s11042-018-6302-0
   Gourrame K, 2016, LECT NOTES COMPUT SC, V9680, P356, DOI 10.1007/978-3-319-33618-3_36
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jia J, 2022, IEEE T CYBERNETICS, V52, P7094, DOI 10.1109/TCYB.2020.3037208
   Kang XG, 2010, IEEE T INF FOREN SEC, V5, P1, DOI 10.1109/TIFS.2009.2039604
   Katayama A., 2004, P 3 INT C MOB UB MUL, P109
   Kim W, 2006, LECT NOTES COMPUT SC, V4261, P106
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lin SS, 2015, IEEE T MULTIMEDIA, V17, P1515, DOI 10.1109/TMM.2015.2437711
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Makbol NM, 2014, I S INTELL SIG PROC, P48, DOI 10.1109/ISPACS.2014.7024423
   Meng Mu, 2021, Advances in Graphic Communication, Printing and Packaging Technology and Materials. Proceedings of 2020 11th China Academic Conference on Printing and Packaging. Lecture Notes in Electrical Engineering (LNEE 754), P199, DOI 10.1007/978-981-16-0503-1_30
   Nakamura T., 2004, P 3 INT C MOB UB MUL, P101
   Pramila A, 2012, SIGNAL IMAGE VIDEO P, V6, P211, DOI 10.1007/s11760-011-0211-2
   Qian YL, 2015, PROC SPIE, V9409, DOI 10.1117/12.2083479
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi HC, 2018, LECT NOTES COMPUT SC, V10735, P534, DOI 10.1007/978-3-319-77380-3_51
   Shin R., 2017, P WORKSH MACH LEARN, V1
   Shuang Liang, 2019, 2019 IEEE 19th International Conference on Communication Technology (ICCT), P1626, DOI 10.1109/ICCT46805.2019.8947254
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Tang WX, 2019, IEEE T INF FOREN SEC, V14, P2074, DOI 10.1109/TIFS.2019.2891237
   Tang WX, 2017, IEEE SIGNAL PROC LET, V24, P1547, DOI 10.1109/LSP.2017.2745572
   Volkhonskiy D, 2020, PROC SPIE, V11433, DOI 10.1117/12.2559429
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu P, 2018, FUTURE INTERNET, V10, DOI 10.3390/fi10060054
   Wu ST, 2020, IEEE T MULTIMEDIA, V22, P256, DOI 10.1109/TMM.2019.2920605
   Xu ML, 2019, IEEE T MULTIMEDIA, V21, P1960, DOI 10.1109/TMM.2019.2891420
   Yamana T., 2013, Proceedings of the Ninth Symposium of the International Working Group on Plant Viruses with Fungal Vectors, Obihiro, Hokkaido, Japan, 19-22 August 2013, P49
   Yang JH, 2020, IEEE T INF FOREN SEC, V15, P839, DOI 10.1109/TIFS.2019.2922229
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 39
TC 3
Z9 3
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2164
EP 2177
DI 10.1109/TMM.2023.3293272
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100017
DA 2024-08-05
ER

PT J
AU Shi, JL
   Shao, F
   Tian, CZ
   Chen, HW
   Xu, L
   Ho, YS
AF Shi, Jiangli
   Shao, Feng
   Tian, Chongzhen
   Chen, Hangwei
   Xu, Long
   Ho, Yo-Sung
TI Progressive Bidirectional Feature Extraction and Enhancement Network for
   Quality Evaluation of Night-Time Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Feature extraction; Degradation; Image resolution;
   Reflectivity; Artificial neural networks; Visualization; Bidirectional
   enhancement; blind image quality assessment; image decomposition;
   night-time image
AB Blind image quality assessment (BIQA) has received increasing attention in the past decades. However, it still remains inadequately researched on BIQA for night-time images suffering from the diverse authentic degradations. Since the intrinsic content degradations of night-time images are highly related to the illumination, how to use the connection between content and illumination to enhance the feature representation ability is the key issue in designing BIQA methods for night-time images. In this article, we first construct an ultra-high-definition night-time image dataset (UHD-NID) with high image resolution and abundant parameter settings. UHD-NID contains 1600 images with a high resolution of 5616 x 3744, and each group of images contains ten exposure levels. Then, we conduct subjective assessment and analyze the subjective data to obtain a mean opinion score to each image in UHD-NID. To enhance the feature representation ability in content and illumination, we propose a Progressive Bidirectional Feature Extraction and Enhancement Network (PBFEE-Net). In addition, we use a decomposition network to decompose the input image into the reflectance and illumination, which can facilitate the ability of feature extraction to some extent. The experimental results show that our proposed method achieves superior performance in evaluating the quality of night-time images.
C1 [Shi, Jiangli; Shao, Feng; Tian, Chongzhen; Chen, Hangwei] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Xu, Long] Natl Astron Observ CAS, Key Lab Solar Act, Beijing 100101, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Chinese Academy of Sciences; National Astronomical
   Observatory, CAS; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 1260230517@qq.com; shaofeng@nbu.edu.cn; 178879166@qq.com;
   1010075746@qq.com; lxu@nao.cas.cn; hoyo@gist.ac.kr
RI Xu, Long/AAH-9908-2019
OI Xu, Long/0000-0002-9286-2876; Chen, Hangwei/0000-0002-3756-2029; HO,
   YO-SUNG/0000-0002-7220-1034
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2002, METHODOLOGY SUBJECTI
   [Anonymous], 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Gao F, 2015, IEEE T NEUR NET LEAR, V26, P2275, DOI 10.1109/TNNLS.2014.2377181
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guan XD, 2022, IEEE T CIRC SYST VID, V32, P6627, DOI 10.1109/TCSVT.2022.3177518
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Hu Y., 2021, DISPLAYS, V69
   Jiang Q., 2022, Deep decomposition and bilinear pooling network for blind night-time image quality evaluation
   Jiang QP, 2021, IEEE T IND INFORM, V17, P6062, DOI 10.1109/TII.2020.3035448
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Leveque L, 2020, IEEE IMAGE PROC, P116, DOI 10.1109/ICIP40778.2020.9190737
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li X, 2020, IEEE ACCESS, V8, P37632, DOI 10.1109/ACCESS.2020.2975310
   Liang LH, 2010, SIGNAL PROCESS-IMAGE, V25, P502, DOI 10.1016/j.image.2010.01.007
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Lin YH, 2022, IEEE T IMAGE PROCESS, V31, P4897, DOI 10.1109/TIP.2022.3189805
   Liu W, 2021, IEEE T IMAGE PROCESS, V30, P176, DOI 10.1109/TIP.2020.3033402
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Lv F., 2018, INPROC BRIT MACH VIS
   McCann J., 2016, ENCY COLOR SCI TECHN, P1118, DOI [10.1007/978-1-4419-8071-7_260, DOI 10.1007/978-1-4419-8071-7_260]
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nafchi HZ, 2018, IEEE T BROADCAST, V64, P518, DOI 10.1109/TBC.2018.2818402
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Paszke A, 2019, ADV NEUR IN, V32
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Saad MA, 2010, IEEE SIGNAL PROC LET, V17, P583, DOI 10.1109/LSP.2010.2045550
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Song CY, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455956
   Tan X, 2021, IEEE T IMAGE PROCESS, V30, P9085, DOI 10.1109/TIP.2021.3122004
   Wang XJ, 2021, IEEE T MULTIMEDIA, V23, P692, DOI 10.1109/TMM.2020.2986583
   Wang Y., 2021, IEEE INT CONFMULTIME, P1
   Wei W., 2018, BRIT MACH VIS C
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xiang T, 2020, IEEE T MULTIMEDIA, V22, P1259, DOI 10.1109/TMM.2019.2938612
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Yang Y., 2023, IEEE Trans. Circuits Syst. Video Technol.
   Yin G., 2022, Content-variant reference image quality assessment via knowledge distillation
   Yu XX, 2022, IEEE WINT CONF APPL, P74, DOI 10.1109/WACVW54805.2022.00013
   Zhan YB, 2017, IEEE SIGNAL PROC LET, V24, P760, DOI 10.1109/LSP.2017.2688371
   Zhang L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1623, DOI 10.1145/3343031.3351069
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhou F, 2019, IEEE T IMAGE PROCESS, V28, P3528, DOI 10.1109/TIP.2019.2898638
   Zhou YD, 2020, IEEE I C VI COM I PR, P507, DOI 10.1109/vcip49819.2020.9301758
   Zhu AQ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102962
NR 59
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1690
EP 1705
DI 10.1109/TMM.2023.3284988
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800010
DA 2024-08-05
ER

PT J
AU Tang, KX
   Kan, NW
   Jiang, YK
   Li, CL
   Dai, WR
   Zou, JN
   Xiong, HK
AF Tang, Kexin
   Kan, Nuowen
   Jiang, Yuankun
   Li, Chenglin
   Dai, Wenrui
   Zou, Junni
   Xiong, Hongkai
TI Successor Feature-Based Transfer Reinforcement Learning for Video Rate
   Adaptation With Heterogeneous QoE Preferences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; Task analysis; Streaming media; Measurement; Bit
   rate; Heuristic algorithms; Video recording; Adaptive video streaming;
   transfer reinforcement learning; successor features; heterogeneous
   preferences
ID QUALITY; EXPERIENCE; DASH
AB In adaptive video streaming, the design of an adaptive bitrate (ABR) strategy is critical for the quality-of-experience (QoE) perceived by users. Though current learning-based ABR algorithms achieve state-of-the-art performance for users with a given QoE metric setting for training, they may unfortunately suffer the poor generalization issue for other users with different QoE preferences. Besides, how to quantitatively characterize the distinct QoE preference for a user has also not been extensively studied yet. In this paper, we propose STEER, a successor feature-based transfer reinforcement learning framework for fast learning the ABR strategies on heterogeneous QoE preferences. Specifically, we first develop a QoE preference analysis scheme to infer the personal QoE preference of a single user based on the user's actual viewing history. We then formulate the personalized QoE maximization problem as a reinforcement learning (RL) task, which optimizes the ABR strategy to maximize the overall QoE perceived by the user. Further, we model the QoE maximization problem for multiple users with heterogeneous QoE preferences as a multi-task RL problem, with each task distinguished by the user-distinct QoE preference. To efficiently address this problem, the proposed STEER solves for each RL-based ABR task by learning its optimal successor feature (SF) function, which can be exploited as shared knowledge across tasks to facilitate the transfer between tasks. With SF functions, STEER can quickly evaluate the optimal policies of previously learned tasks on a new task, and further use the generalized policy improvement operation to obtain a jumpstart policy. Both theoretically and empirically, we show that this jumpstart policy is a good initial policy with a performance guarantee for better generalization in the new task, and can also lead to a faster convergence to the optimal policy of the new task.
C1 [Tang, Kexin; Kan, Nuowen; Jiang, Yuankun; Li, Chenglin; Dai, Wenrui; Zou, Junni; Xiong, Hongkai] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Li, CL (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM tkx-china@163.com; kannw_1230@sjtu.edu.cn; yuankunjiang@sjtu.edu.cn;
   lcl1985@sjtu.edu.cn; daiwenrui@sjtu.edu.cn; zoujunni@sjtu.edu.cn;
   xionghongkai@sjtu.edu.cn
OI Xiong, Hongkai/0000-0003-4552-0029
FU National Natural Science Foundation of China
FX No Statement Available
CR Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   [Anonymous], 2016, Dash industry form
   [Anonymous], 2016, Raw data-Measuring broadband America
   [Anonymous], 2020, Puffer
   [Anonymous], 2020, VMAF development kit (VDK) open source package
   [Anonymous], 2020, Comyco video description dataset
   Barreto A, 2017, ADV NEUR IN, V30
   Bentaleb M., 2023, P IEEE INFO COM C CO, P1
   Brunnstrom K., 2013, Qualinet white paper on definitions of quality of experience
   Chiariotti F, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P77, DOI 10.1145/2910017.2910603
   Cui LZ, 2021, IEEE T MULTIMEDIA, V23, P651, DOI 10.1109/TMM.2020.2985631
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Duanmu Z., 2020, arXiv
   Duanmu Z, 2019, Arxiv, DOI arXiv:1911.07944
   Duanmu ZF, 2018, IEEE T BROADCAST, V64, P474, DOI 10.1109/TBC.2018.2822870
   El Essaili A, 2015, IEEE T CIRC SYST VID, V25, P988, DOI 10.1109/TCSVT.2014.2367355
   Finn C, 2017, PR MACH LEARN RES, V70
   Frans J., 2018, P INT C LEARN REPR, V52
   Gadaleta M, 2017, IEEE T COGN COMMUN, V3, P703, DOI 10.1109/TCCN.2017.2755007
   Gao G., 2019, IEEE ICC, P1, DOI DOI 10.1109/icc.2019.8761156
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2022, IEEE J SEL AREA COMM, V40, P2485, DOI 10.1109/JSAC.2022.3180804
   Huang TC, 2020, IEEE J SEL AREA COMM, V38, P2324, DOI 10.1109/JSAC.2020.3000363
   Huo LY, 2020, IEEE T CIRC SYST VID, V30, P3210, DOI 10.1109/TCSVT.2019.2939282
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kan NW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3006, DOI 10.1145/3503161.3548331
   Kingma D. P., 2014, arXiv
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Li Z, 2016, SCI REP-UK, V6, DOI 10.1038/srep30338
   Liu Y, 2015, IEEE T BROADCAST, V61, P651, DOI 10.1109/TBC.2015.2460611
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Rakelly K, 2019, PR MACH LEARN RES, V97
   Rassool R, 2017, IEEE INT SYM BROADB, P351
   Rehman A, 2015, PROC SPIE, V9394, DOI 10.1117/12.2077917
   Riiser H., 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schaul T, 2015, PR MACH LEARN RES, V37, P1312
   Stockhammer T., 2011, Proceedings of the second annual ACM conference on Multimedia systems, P133
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Sutton A., 1998, Introduction to Reinforcement Learning
   Tang KX, 2021, IEEE T CIRC SYST VID, V31, P798, DOI 10.1109/TCSVT.2020.2980587
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan FY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P495
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang HZ, 2020, IEEE T MULTIMEDIA, V22, P3210, DOI 10.1109/TMM.2020.2973828
   Zhang X., 2022, arXiv
   Zhang X, 2021, PROCEEDINGS OF THE 18TH USENIX SYMPOSIUM ON NETWORKED SYSTEM DESIGN AND IMPLEMENTATION, P303
   Zhao TS, 2017, IEEE COMMUN SURV TUT, V19, P285, DOI 10.1109/COMST.2016.2619982
   Zhu ZD, 2023, IEEE T PATTERN ANAL, V45, P13344, DOI 10.1109/TPAMI.2023.3292075
NR 56
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5340
EP 5357
DI 10.1109/TMM.2023.3331487
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600036
DA 2024-08-05
ER

PT J
AU Wang, XZ
   Chen, KQ
   Wang, ZX
   Huang, WH
AF Wang, Xingzheng
   Chen, Kaiqiang
   Wang, Zixuan
   Huang, Wenhao
TI PMSNet: Parallel Multi-Scale Network for Accurate Low-Light Light-Field
   Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Feature extraction; Convolution; Spatial resolution; Image
   reconstruction; Three-dimensional displays; Lighting; Low-light light
   field; image enhancement; Parallel multi-scale; 3D convolution
ID DEPTH
AB Current low-light light-field (LF) image enhancement algorithms tend to produce blurry results, for (1) loss of spatial details during enhancement and (2) inefficient exploitation of angular correlations, which helps to recover spatial details. Therefore, in this article, we propose a parallel multi-scale network (PMSNet), which attempts to (1) process features of different scales in parallel to aggregate the different contributions of multi-scale features at each layer, thus fully preserve spatial details, and (2) integrate multi-resolution 3D convolution streams to efficiently utilize angular correlations. Specifically, PMSNet consists of three stages: Stage-I employs multi-scale modules (MSMs) to generate local understanding with the aid of adjacent views. Notably, MSM retains high-resolution feature extraction to minimize loss of spatial details. Stage-II processes all views to encode global information. Based on the above extracted local and global information, Stage-III utilizes 3D multi-scale modules (3D-MSMs) to efficiently exploit angular correlations. To validate our idea, we comprehensively evaluate the performance of PMSNet on three publicly available datasets. Experimental results show that our method is superior to the current state-of-the-art methods, achieving an average PSNR of 24.76 dB.
C1 [Wang, Xingzheng; Chen, Kaiqiang; Wang, Zixuan; Huang, Wenhao] Shenzhen Univ, Coll Mechatron & Control Engn, Shenzhen 518060, Peoples R China.
C3 Shenzhen University
RP Wang, XZ (corresponding author), Shenzhen Univ, Coll Mechatron & Control Engn, Shenzhen 518060, Peoples R China.
EM xingzheng.wang@szu.edu.cn; 2110296002@email.szu.edu.cn;
   2110296004@email.szu.edu.cn; 2110296026@email.szu.edu.cn
RI Huang, Wenhao/GWU-9337-2022
FU National Natural Science Foundation of China
FX No Statement Available
CR Chen J, 2018, IEEE T IMAGE PROCESS, V27, P4889, DOI 10.1109/TIP.2018.2839524
   Cui HS, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.837208
   Dansereau D.G., 2013, SPIE, V8657, P176
   Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137
   Guo MT, 2022, IEEE T PATTERN ANAL, V44, P6094, DOI 10.1109/TPAMI.2021.3087485
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jin J, 2020, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR42600.2020.00233
   Kingma D., 2014, P INT C LEARN REPR, P1
   Kuanar S, 2022, SIGNAL PROCESS-IMAGE, V109, DOI 10.1016/j.image.2022.116839
   Kuanar S, 2021, SIGNAL PROCESS-IMAGE, V99, DOI 10.1016/j.image.2021.116409
   Kuanar S, 2022, VISUAL COMPUT, V38, P1121, DOI 10.1007/s00371-021-02071-z
   Lamba M, 2022, IEEE WINT CONF APPL, P3152, DOI 10.1109/WACV51458.2022.00321
   Lamba M, 2021, IEEE T IMAGE PROCESS, V30, P1501, DOI 10.1109/TIP.2020.3045617
   Levoy M, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.270
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li YJ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2480, DOI 10.1109/ICASSP39728.2021.9413449
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu DY, 2021, SIGNAL PROCESS-IMAGE, V97, DOI 10.1016/j.image.2021.116353
   Liu GS, 2023, IEEE T MULTIMEDIA, V25, P256, DOI 10.1109/TMM.2021.3124385
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu Nian, 2021, P IEEE CVF INT C COM, P4712
   Liu YJ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1840, DOI 10.1109/ICASSP39728.2021.9413433
   Lumsdaine A., 2009, P IEEE INT C COMP PH, P1, DOI DOI 10.1109/ICCPHOT.2009.5559008
   Lv F., 2018, Proc. BMVC, V220, P4
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Mantang Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P278, DOI 10.1007/978-3-030-58536-5_17
   Mittal S, 2021, J SYST ARCHITECT, V115, DOI 10.1016/j.sysarc.2021.102041
   Ng R., 2005, [31] R. Ng, M. Levoy, M. Bredif, G. Duval, M. Horowitz, and P. Hanrahan, "Light field photography with a hand-held plenoptic camera," Stanford University Com- puter Science Tech Report CSTR 2005-02, 2005.
   Piao YR, 2021, Arxiv, DOI arXiv:2104.05971
   Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang TT, 2019, IEEE I CONF COMP VIS, P8837, DOI 10.1109/ICCV.2019.00893
   Wang TC, 2016, IEEE T PATTERN ANAL, V38, P2170, DOI 10.1109/TPAMI.2016.2515615
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang X, 2016, APPL OPTICS, V55, P2580, DOI 10.1364/AO.55.002580
   Wang YA, 2020, IEEE WINT CONF APPL, P118, DOI 10.1109/WACV45572.2020.9093448
   Wang YQ, 2023, IEEE T PATTERN ANAL, V45, P425, DOI 10.1109/TPAMI.2022.3152488
   Wang YQ, 2019, IEEE SIGNAL PROC LET, V26, P204, DOI 10.1109/LSP.2018.2885213
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Xuan Q, 2019, IEEE ACCESS, V7, P92528, DOI 10.1109/ACCESS.2019.2923022
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Yingqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P290, DOI 10.1007/978-3-030-58592-1_18
   Yu F., 2016, ICLR, P1
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zamir SW, 2023, IEEE T PATTERN ANAL, V45, P1934, DOI 10.1109/TPAMI.2022.3167175
   Zhang C, 2018, PATTERN RECOGN, V81, P176, DOI 10.1016/j.patcog.2018.03.020
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P4421, DOI 10.1109/TIP.2020.2970529
   Zhang K, 2020, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR42600.2020.00176
   Zhang R, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SS, 2021, SIGNAL PROCESS, V189, DOI 10.1016/j.sigpro.2021.108279
   Zhang SS, 2021, NEUROCOMPUTING, V456, P76, DOI 10.1016/j.neucom.2021.05.074
   Zhang SF, 2019, PROC CVPR IEEE, P919, DOI 10.1109/CVPR.2019.00101
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao ZJ, 2022, IEEE T CIRC SYST VID, V32, P1076, DOI 10.1109/TCSVT.2021.3073371
   Zheng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4419, DOI 10.1109/ICCV48922.2021.00440
NR 63
TC 3
Z9 3
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2041
EP 2055
DI 10.1109/TMM.2023.3291498
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800021
DA 2024-08-05
ER

PT J
AU Wang, YJ
   Chen, ML
   Li, XL
AF Wang, Yajie
   Chen, Mulin
   Li, Xuelong
TI Continuous Emotion-Based Image-to-Music Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-to-music generation; valence-arousal space; multi-modal cognitive
   computing; vicinagearth security
AB Image-to-music generation aims to generate realistic pure music according to a given image. Although many previous works are conducted on bridging image and music, they mainly focus on the content-based cross-modal matching. For example, matching the Christmas song to an image that contains a Christmas tree. By comparison, image-to-music generation is a more challenging task due to its ambiguity and subjectivity. Specifically, there is no explicit correlation between the image content and music melody, without any lyric and human sound. Meanwhile, the perception of generated music varies from person to person. Inspired by the synesthesia phenomenon, we think that if an image tends to elicit a certain emotion on human, the generated music should also leave a similar impression. Therefore, in this paper, we propose a continuous emotion-based image-to-music generation framework, which uses emotion as the key for cross-modal generation. Specifically, a new image-music dataset is established, which uses valence-arousal (VA) space to capture the complex and nuanced nature of emotions. After that, a plug and play model is proposed to translate an image into a piece of music with similar emotion, which projects the emotions into continuous-valued labels, and explores both the intra-modal and inter-modal emotional consistency with contrastive learning. To our best knowledge, this is the first end-to-end framework towards the task of pure music generation from natural images. Extensive experiments show that the generated music achieves satisfactory emotional consistency with the input images, as well as impressive quality.
C1 [Wang, Yajie; Chen, Mulin; Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence, OPt & Elect iOPEN, Xian 710072, Peoples R China.
   [Wang, Yajie; Chen, Mulin; Li, Xuelong] Northwestern Polytech Univ, Key Lab Intelligent Interact & Applicat, Minist Ind & Informat Technol, Xian 710072, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Chen, ML; Li, XL (corresponding author), Northwestern Polytech Univ, Sch Artificial Intelligence, OPt & Elect iOPEN, Xian 710072, Peoples R China.
EM yajie_wang@mail.nwpu.edu.cn; chenmulin@nwpu.edu.cn; li@nwpu.edu.cn
RI Li, Xuelong/Z-3785-2019
FU National Key Research and Development Program of China
FX No Statement Available
CR Aljanaki A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173392
   [Anonymous], 2017, PROC IEEE INT C COMP
   Bonny H., 2002, MUSIC CONSCIOUSNESS
   Cífka O, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P96, DOI 10.1109/ICASSP39728.2021.9414235
   Di SZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2037, DOI 10.1145/3474085.3475195
   Dong HW, 2018, AAAI CONF ARTIF INTE, P34
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Fan J Y, 2017, INT SOC MUSIC INF RE, P368
   Fan JY, 2020, INT CONF ACOUST SPEE, P521, DOI [10.1109/ICASSP40776.2020.9052994, 10.1109/icassp40776.2020.9052994]
   Fan JY, 2017, INT CONF AFFECT, P196, DOI 10.1109/ACII.2017.8273600
   Ferreira L N, 2019, Proceedings of the 20th International Society for Music Information Retrieval Conference (Delft, The Netherlands), P384, DOI [DOI 10.5281/ZENODO.3527824, 10 . 5281 / zenodo.3527824]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Higgins I., 2017, INT C LEARN REPR
   Hu ZJ, 2023, IEEE T MULTIMEDIA, V25, P2296, DOI 10.1109/TMM.2022.3146002
   Huang YSA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1180, DOI 10.1145/3394171.3413671
   Hung H., 2021, ISMIR, P318
   Hung YN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4697
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kim HR, 2018, IEEE T MULTIMEDIA, V20, P2980, DOI 10.1109/TMM.2018.2827782
   Lang PJ., 1997, International Affective Picture System (IAPS)
   Li XL, 2008, NEUROCOMPUTING, V71, P2023, DOI 10.1016/j.neucom.2008.01.025
   Lin JC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P519, DOI 10.1145/3123266.3123399
   Lin JC, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P899, DOI 10.1145/2733373.2806359
   Lin Jen-Chun, 2016, P ACM INT C MULT, P372
   Livingstone R., Comput
   Lu CY, 2019, AAAI CONF ARTIF INTE, P1061
   Marchewka A, 2014, BEHAV RES METHODS, V46, P596, DOI 10.3758/s13428-013-0379-1
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Oord A.v.d., 2016, arXiv, DOI DOI 10.48550/ARXIV.1609.03499
   Pati A., 2019, INT C MACH LEARN MAC
   Pidhorskyi S., 2020, P IEEE CVF C COMP VI, P14092
   Ping Wei, 2020, INT C MACHINE LEARNI, P7706
   Roberts A., 2018, P INT C MACH LEARN, V80, P4361
   Rtte L., INT C LEARN REPRESEN
   Saito Y., 2021, PROC IEEE 10 GLOB C, P268
   Santos A., 2021, PROC INT C COMPUT CR, P103
   Tan D., 2020, inProc. Int. Soc. Music Inf., P109
   Tanaka K, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P111, DOI 10.1109/ICASSP39728.2021.9414059
   van den Oord A, 2017, ADV NEUR IN, V30
   Verma G, 2019, INT CONF ACOUST SPEE, P3975, DOI 10.1109/ICASSP.2019.8683133
   Wang J.-C., 2012, ACM INT C MULTIMEDIA, P89
   Wang YJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1162, DOI 10.1145/3394171.3413894
   Wu XX, 2016, IEEE T MULTIMEDIA, V18, P1305, DOI 10.1109/TMM.2016.2557722
   Wu Y., 2020, PROC INT SOC MUSIC I, P142
   Xiong Zinan, 2022, 2022 IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR), P228, DOI 10.1109/MIPR54900.2022.00048
   Xue T, 2023, IEEE T MULTIMEDIA, V25, P243, DOI 10.1109/TMM.2021.3124080
   Yang L.-C., 2017, ISMIR, P324
   Yang XT, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12105050
   Zhang XY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1204, DOI 10.1145/3503161.3548084
   Zhao SC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2945, DOI 10.1145/3394171.3413776
   Zhuo L., 2023, P IEEECVF INT C COMP, P15637
NR 51
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5670
EP 5679
DI 10.1109/TMM.2023.3338089
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100003
DA 2024-08-05
ER

PT J
AU Xian, K
   Peng, JW
   Cao, ZG
   Zhang, JM
   Lin, GS
AF Xian, Ke
   Peng, Juewen
   Cao, Zhiguo
   Zhang, Jianming
   Lin, Guosheng
TI ViTA: Video Transformer Adaptor for Robust Video Depth Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Robust video depth estimation; video transformer adaptor;
   spatio-temporal consistency loss
AB Depth information plays a pivotal role in numerous computer vision applications, including autonomous driving, 3D reconstruction, and 3D content generation. When deploying depth estimation models in practical applications, it is essential to ensure that the models have strong generalization capabilities. However, existing depth estimation methods primarily concentrate on robust single-image depth estimation, leading to the occurrence of flickering artifacts when applied to video inputs. On the other hand, video depth estimation methods either consume excessive computational resources or lack robustness. To address the above issues, we propose ViTA, a video transformer adaptor, to estimate temporally consistent video depth in the wild. In particular, we leverage a pre-trained image transformer (i.e., DPT) and introduce additional temporal embeddings in the transformer blocks. Such designs enable our ViTA to output reliable results given an unconstrained video. Besides, we present a spatio-temporal consistency loss for supervision. The spatial loss computes the per-pixel discrepancy between the prediction and the ground truth in space, while the temporal loss regularizes the inconsistent outputs of the same point in consecutive frames. To find the correspondences between consecutive frames, we design a bi-directional warping strategy based on the forward and backward optical flow. During inference, our ViTA no longer requires optical flow estimation, which enables it to estimate spatially accurate and temporally consistent video depth maps with fine-grained details in real time. We conduct a detailed ablation study to verify the effectiveness of the proposed components. Extensive experiments on the zero-shot cross-dataset evaluation demonstrate that the proposed method is superior to previous methods.
C1 [Xian, Ke; Lin, Guosheng] Nanyang Technol Univ NTU, S Lab, Singapore 639798, Singapore.
   [Peng, Juewen; Cao, Zhiguo] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Image Proc & Intelligent Control, Minist Educ, Wuhan 430074, Peoples R China.
   [Zhang, Jianming] Adobe Res, San Francisco, CA 94107 USA.
C3 Nanyang Technological University; Huazhong University of Science &
   Technology; Adobe Systems Inc.
RP Lin, GS (corresponding author), Nanyang Technol Univ NTU, S Lab, Singapore 639798, Singapore.
EM ke.xian@ntu.edu.sg; juewenpeng@hust.edu.cn; zgcao@hust.edu.cn;
   jianmzha@adobe.com; gslin@ntu.edu.sg
RI Xian, Ke/HCH-2921-2022; Zhang, Jianming/B-1665-2017
OI Xian, Ke/0000-0002-0884-5126; Zhang, Jianming/0000-0002-9954-6294; Peng,
   Juewen/0000-0001-5740-2682
FU RIE2020 Industry Alignment Fund - Industry Collaboration Projects
FX No Statement Available
CR [Anonymous], 2008, ADV NEURAL INFORM PR
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bian JW, 2022, IEEE T PATTERN ANAL, V44, P9802, DOI 10.1109/TPAMI.2021.3136220
   Bian JW, 2021, INT J COMPUT VISION, V129, P2548, DOI 10.1007/s11263-021-01484-6
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Calagari K, 2018, IEEE T MULTIMEDIA, V20, P605, DOI 10.1109/TMM.2017.2748458
   Cao YZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P190, DOI 10.1145/3474085.3475564
   Cao YZH, 2020, IEEE T CIRC SYST VID, V30, P2674, DOI 10.1109/TCSVT.2019.2929202
   Cao YZH, 2018, IEEE T CIRC SYST VID, V28, P3174, DOI 10.1109/TCSVT.2017.2740321
   Carion N., 2020, EUR C COMP VIS, P213
   Chen Weifeng, 2016, ADV NEURAL INFORM PR, V29, P730, DOI DOI 10.5555/3157096.3157178
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eigen D, 2014, ADV NEUR IN, V27
   Gao DF, 2023, PROC CVPR IEEE, P14773, DOI 10.1109/CVPR52729.2023.01419
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Karim R, 2023, PROC CVPR IEEE, P6323, DOI 10.1109/CVPR52729.2023.00612
   Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835
   Khan N, 2023, PROC CVPR IEEE, P9119, DOI 10.1109/CVPR52729.2023.00880
   Koch T, 2019, LECT NOTES COMPUT SC, V11131, P331, DOI 10.1007/978-3-030-11015-4_25
   Kopf J, 2021, PROC CVPR IEEE, P1611, DOI 10.1109/CVPR46437.2021.00166
   Lee M, 2013, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR.2013.169
   Li RB, 2019, LECT NOTES COMPUT SC, V11364, P663, DOI 10.1007/978-3-030-20870-7_41
   Li SY, 2021, IEEE INT CONF COMP V, P1145, DOI 10.1109/ICCVW54120.2021.00134
   Li YH, 2022, PROC CVPR IEEE, P4794, DOI 10.1109/CVPR52688.2022.00476
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Li ZQ, 2019, PROC CVPR IEEE, P4516, DOI 10.1109/CVPR.2019.00465
   Ling CW, 2022, IEEE T MULTIMEDIA, V24, P2938, DOI 10.1109/TMM.2021.3091308
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo X, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392377
   Peng JW, 2022, PROC CVPR IEEE, P16262, DOI 10.1109/CVPR52688.2022.01580
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Teed Z., 2019, P INT C LEARN REPR, P1
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012
   Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596
   Wang CY, 2019, INT CONF 3D VISION, P348, DOI 10.1109/3DV.2019.00046
   Wang Q., 2021, P INT C MULT EXP, P1
   Wang WS, 2020, IEEE INT C INT ROBOT, P4909, DOI 10.1109/IROS45743.2020.9341801
   Wang YR, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6347, DOI 10.1145/3503161.3547978
   Wang YR, 2021, IEEE COMPUT SOC CONF, P2457, DOI 10.1109/CVPRW53098.2021.00278
   Xian K, 2020, PROC CVPR IEEE, P608, DOI 10.1109/CVPR42600.2020.00069
   Xian K, 2018, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2018.00040
   Xu HF, 2022, PROC CVPR IEEE, P8111, DOI 10.1109/CVPR52688.2022.00795
   Yang GL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16249, DOI 10.1109/ICCV48922.2021.01596
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Yin W, 2021, PROC CVPR IEEE, P204, DOI 10.1109/CVPR46437.2021.00027
   Yin W, 2022, IEEE T PATTERN ANAL, V44, P7282, DOI 10.1109/TPAMI.2021.3097396
   Yoon JS, 2020, PROC CVPR IEEE, P5335, DOI 10.1109/CVPR42600.2020.00538
   Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52
   Zhang HK, 2019, IEEE I CONF COMP VIS, P1725, DOI 10.1109/ICCV.2019.00181
   Zhang N, 2023, PROC CVPR IEEE, P18537, DOI 10.1109/CVPR52729.2023.01778
   Zhang XT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322988
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/ICCV48922.2021.01332, 10.1109/iccv48922.2021.01332]
   Zhang ZT, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459871
   Zheng MX, 2023, PROC CVPR IEEE, P4025, DOI 10.1109/CVPR52729.2023.00392
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
NR 67
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3302
EP 3316
DI 10.1109/TMM.2023.3309559
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200035
DA 2024-08-05
ER

PT J
AU Xu, CH
   Yan, JX
   Yang, YH
   Deng, C
AF Xu, Chenghao
   Yan, Jiexi
   Yang, Yanhua
   Deng, Cheng
TI Implicit Compositional Generative Network for Length-Variable Co-Speech
   Gesture Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-speech gesture synthesis; implicit neural representations;
   compositional generation
ID FEATURES
AB Co-speech gesture synthesis is a practical yet challenging task that aims to generate body motion sequences in line with speech audio. Most of the existing methods can only generate the gesture sequence with a fixed number of frames, which does not satisfy the high-quality requirement of the virtual speech video in real-world applications. In this paper, we propose a novel Implicit Compositional Generative Network (ICGN) for length-variable co-speech gesture synthesis. In ICGN, the implicit neural representation is captured and optimized for a whole gesture sequence of arbitrary length with temporal embeddings. Moreover, to enforce the synthesized gestures more realistic and consistent, we compositionally generate the gesture sequence through a well-designed asymmetric two-stream network that effectively captures and utilizes the rich correlations between speech audio and human body motions. In this way, the coarse and fine-grained gestures are synthesized, respectively, according to the corresponding content-aware and emotion-aware audio components. Extensive experiments on four widely-used benchmarks demonstrate that the proposed method renders realistic human gestures and achieves the superior performance against several state-of-the-art methods.
C1 [Xu, Chenghao; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Yan, Jiexi; Yang, Yanhua] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University
RP Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM chx@stu.xidian.edu.cn; jxyan1995@gmail.com; yanhyang@xidian.edu.cn;
   chdeng.xd@gmail.com
FU National Key Ramp;D Program of China
FX No Statement Available
CR Ahuja C, 2019, INT CONF 3D VISION, P719, DOI 10.1109/3DV.2019.00084
   Alexanderson S, 2020, COMPUT GRAPH FORUM, V39, P487, DOI 10.1111/cgf.13946
   [Anonymous], 2015, P 14 PYTHON SCI C, P18, DOI DOI 10.25080/MAJORA-7B98E3ED-003
   Cassell J., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P413, DOI 10.1145/192161.192272
   Cassell J., 1999, PRAGMAT COGN, V7, P1, DOI [10.1075/pc.7.1.03cas, DOI 10.1075/PC.7.1.03CAS]
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chiu CC, 2015, LECT NOTES ARTIF INT, V9238, P152, DOI 10.1007/978-3-319-21996-7_17
   Choutas Vasileios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P20, DOI 10.1007/978-3-030-58607-2_2
   DeVries T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14284, DOI 10.1109/ICCV48922.2021.01404
   Ellis DPW, 2007, J NEW MUSIC RES, V36, P51, DOI 10.1080/09298210701653344
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Eskimez SE, 2021, IEEE T MULTIMEDIA, V24, P3480, DOI 10.1109/TMM.2021.3099900
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Ginosar S, 2019, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2019.00361
   Guo CA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2021, DOI 10.1145/3394171.3413635
   Hasegawa D, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P79, DOI 10.1145/3267851.3267878
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Kasten Y., 2020, PROC NEURIPS, V33, P2492, DOI 10.48550/arXiv.2003.09852
   Kingma D. P., 2021, P INT C LEARN REPR
   Kopp S, 2006, LECT NOTES ARTIF INT, V4133, P205
   Kucherenko T, 2019, PROCEEDINGS OF THE 19TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA' 19), P97, DOI 10.1145/3308532.3329472
   Levine S., 2009, P ACM SIGGRAPH AS, P1
   Levine S., 2010, P 21 ANN C COMP GRAP, P1
   Li B., 2018, P BRIT MACH VIS C
   Li BY, 2022, AAAI CONF ARTIF INTE, P1272
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li RL, 2021, Arxiv, DOI arXiv:2101.08779
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Liang YZ, 2022, PROC CVPR IEEE, P10463, DOI 10.1109/CVPR52688.2022.01022
   Liu X, 2022, PROC CVPR IEEE, P10452, DOI 10.1109/CVPR52688.2022.01021
   Mai L, 2022, PROC CVPR IEEE, P10728, DOI 10.1109/CVPR52688.2022.01047
   McNeill D., 1992, Hand and Mind: What Gestures Reveal about Thought
   Miao Liao, 2021, Computer Vision - ACCV 2020 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12626), P308, DOI 10.1007/978-3-030-69541-5_19
   Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24
   Niemeyer M, 2021, PROC CVPR IEEE, P11448, DOI 10.1109/CVPR46437.2021.01129
   Niemeyer M, 2019, IEEE I CONF COMP VIS, P5378, DOI 10.1109/ICCV.2019.00548
   Park N., 2021, P INT C LEARN REPR, P6000
   Paszke A, 2019, ADV NEUR IN, V32
   Qian Kaizhi, 2020, INT C MACH LEARN, P7836
   Qian SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11057, DOI 10.1109/ICCV48922.2021.01089
   Sahidullah M, 2012, SPEECH COMMUN, V54, P543, DOI 10.1016/j.specom.2011.11.004
   Salem M., 2011, 2011 RO-MAN: The 20th IEEE International Symposium on Robot and Human Interactive Communication, P247, DOI 10.1109/ROMAN.2011.6005285
   Salem M, 2012, INT J SOC ROBOT, V4, P201, DOI 10.1007/s12369-011-0124-9
   Schwarz K., 2020, ADV NEURAL INFORM PR, V33, P20154, DOI DOI 10.48550/ARXIV.2007.02442
   Shirian A., 2020, IEEE Trans. Multimedia, V24, P780
   Shlizerman E, 2018, PROC CVPR IEEE, P7574, DOI 10.1109/CVPR.2018.00790
   Sitzmann V., 2020, Advances in neural information processing systems, V33, P7462, DOI DOI 10.48550/ARXIV.2006.09661
   Skorokhodov I, 2021, PROC CVPR IEEE, P10748, DOI 10.1109/CVPR46437.2021.01061
   Vaswani A, 2017, ADV NEUR IN, V30
   Wagner P, 2014, SPEECH COMMUN, V57, P209, DOI 10.1016/j.specom.2013.09.008
   Wang P., 2022, P INT C LEARN REPR, P25
   Xiu Y., 2018, BMVC
   Yoon Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417838
   Yoon Y, 2019, IEEE INT CONF ROBOT, P4303, DOI [10.1109/icra.2019.8793720, 10.1109/ICRA.2019.8793720]
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhao XT, 2022, INT CONF ACOUST SPEE, P7022, DOI 10.1109/ICASSP43922.2022.9747625
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
NR 57
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6325
EP 6335
DI 10.1109/TMM.2023.3348331
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600056
DA 2024-08-05
ER

PT J
AU Yuan, ZQ
   Zhang, BZ
   Xu, H
   Gao, K
AF Yuan, Ziqi
   Zhang, Baozheng
   Xu, Hua
   Gao, Kai
TI Meta Noise Adaption Framework for Multimodal Sentiment Analysis With
   Feature Noise
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Task analysis; Training; Metalearning; Sentiment
   analysis; Adaptation models; Visualization; Feature noise; late fusion
   based architecture; meta learning; robust multimodal sentiment analysis
AB Improving the robustness of models against feature noise has emerged as one of the most crucial research topics in the field of multimodal sentiment analysis. Recent studies assume that the training instances are free of noise and develop either translation or reconstruction based method under the guidance of perfect training data for robust testing time performance. However, such an ideal assumption neglects the potential presence of the feature noise in training instances and inevitably results in degradation for the scenario where high-quality training instances are unavailable. In order to achieve robust training with noisy instances, we propose the Meta Noise Adaption (Meta-NA) learning strategy, a meta learning method accumulating the experience of dealing with various types of feature noise. Specifically, we first formulate the tasks distribution where each task is corresponding to one specific pattern of noise, and propose the feature adaption module adding on the unimodal encoder in late fusion based architecture. Through an nested online optimization between the auxiliary feature adaption module and the late fusion backbone modules, the proposed method can leverage shared knowledge across different noisy source tasks and learn how to learn from the noisy instances for robust testing performances. Extensive experiments are conducted on two benchmark multimodal sentiment analysis datasets, namely MOSI and CH-SIMS v2. The results demonstrate that our proposed method can rapidly adapt to various unseen types of feature noise and outperforms all baseline methods, particularly when the training instances are limited.
C1 [Yuan, Ziqi; Zhang, Baozheng; Xu, Hua] Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
   [Zhang, Baozheng; Gao, Kai] Hebei Univ Sci & Technol, Sch Informat Sci & Engn, Shijiazhuang 050018, Peoples R China.
   [Zhang, Baozheng; Xu, Hua] Samton Jiangxi Technol Dev Co Ltd, Nanchang 330036, Peoples R China.
C3 Tsinghua University; Hebei University of Science & Technology
RP Xu, H (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
EM xuhua@tsinghua.edu.cn
OI Yuan, Ziqi/0000-0003-2397-2163; Zhang, BaoZheng/0009-0007-4816-5153
FU National Natural Science Foundation of China
FX No Statement Available
CR Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Binghua Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P431, DOI 10.1007/978-3-030-58586-0_26
   Chi H., 2022, P 2 C AS PAC CHAPT A, P121
   Chuah SHW, 2021, J RETAIL CONSUM SERV, V61, DOI 10.1016/j.jretconser.2021.102551
   Devlin J, 2018, ARXIV
   Finn C, 2017, PR MACH LEARN RES, V70
   Gandhi A, 2023, INFORM FUSION, V91, P424, DOI 10.1016/j.inffus.2022.09.025
   Guo XB, 2023, IEEE T MULTIMEDIA, V25, P9437, DOI 10.1109/TMM.2023.3252270
   Pham H, 2018, FIRST GRAND CHALLENGE AND WORKSHOP ON HUMAN MULTIMODAL LANGUAGE (CHALLENGE-HML), P53
   Han W., 2022, PRAC C EMP METHODS N, V7, P498, DOI [10.18653/v1/2022.emnlp-main.717, DOI 10.18653/V1/2022.EMNLP-MAIN.717]
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Hazarika D, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P685
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hospedales T, 2022, IEEE T PATTERN ANAL, V44, P5149, DOI 10.1109/TPAMI.2021.3079209
   Huisman M, 2021, ARTIF INTELL REV, V54, P4483, DOI 10.1007/s10462-021-10004-4
   Kaur R., 2022, Research anthology on implementing sentiment analysis across multiple disciplines (2022), P1846, DOI DOI 10.4018/978-1-6684-6303-1.CH098
   Li Z., 2017, arXiv
   Liang P. P., 2021, PROC NEURAL INF PROC
   Liang PP, 2022, Arxiv, DOI arXiv:2209.03430
   Liang PP, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1569
   Liu YH, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, ICMI 2022, P247, DOI 10.1145/3536221.3556630
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Ma MM, 2022, PROC CVPR IEEE, P18156, DOI 10.1109/CVPR52688.2022.01764
   Ma MM, 2021, AAAI CONF ARTIF INTE, V35, P2302
   Mai Sijie, 2023, IEEE Transactions on Multimedia, P4121, DOI 10.1109/TMM.2022.3171679
   Mao H., 2022, PROC AAAI C ARTIF IN, P16458
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI [DOI 10.1145/2070481.2070509, 10.1145/2070481.2070509]
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Rincon JA, 2019, KNOWL INF SYST, V60, P363, DOI 10.1007/s10115-018-1231-9
   Shu J, 2019, ADV NEUR IN, V32
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Song H, 2023, IEEE T NEUR NET LEAR, V34, P8135, DOI 10.1109/TNNLS.2022.3152527
   Sun LC, 2024, IEEE T AFFECT COMPUT, V15, P309, DOI 10.1109/TAFFC.2023.3274829
   Sun Y, 2023, IEEE T AFFECT COMPUT, V14, P2209, DOI 10.1109/TAFFC.2022.3178231
   Tang J., 2021, LONG PAPERS, V1, P5301, DOI DOI 10.18653/V1/2021.ACL-LONG.412
   Tsai Y.-H. H., 2019, PME INT C LEARN REPR
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vilalta R, 2002, ARTIF INTELL REV, V18, P77, DOI 10.1023/A:1019956318069
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P4909, DOI 10.1109/TMM.2022.3183830
   Wang D, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109259
   Wang Z, 2020, PROC CVPR IEEE, P4523, DOI 10.1109/CVPR42600.2020.00458
   Wang ZL, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2514, DOI 10.1145/3366423.3380000
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yu WM, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3718
   Yuan ZQ, 2024, IEEE T MULTIMEDIA, V26, P529, DOI 10.1109/TMM.2023.3267882
   Yuan ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4400, DOI 10.1145/3474085.3475585
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zeng JD, 2023, IEEE T MULTIMEDIA, V25, P6301, DOI 10.1109/TMM.2022.3207572
   Zhang C, 2020, IEEE J-STSP, V14, P478, DOI 10.1109/JSTSP.2020.2987728
   Zhao JM, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2608
   Zhou H, 2018, AAAI CONF ARTIF INTE, P730
NR 56
TC 0
Z9 0
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7265
EP 7277
DI 10.1109/TMM.2024.3362600
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000020
DA 2024-08-05
ER

PT J
AU Zheng, LM
   Luo, Y
   Zhou, ZH
   Ling, J
   Yue, GH
AF Zheng, Limin
   Luo, Yu
   Zhou, Zihan
   Ling, Jie
   Yue, Guanghui
TI CDINet: Content Distortion Interaction Network for Blind Image Quality
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Image quality; Image restoration; Feature extraction;
   Decoding; Dams; Visualization; Blind image quality assessment; content
   distortion interaction; image restoration; deep neural networks
ID FRAMEWORK; METRICS; BLUR
AB Perceptual image quality is related to content and distortion. Distortion classification is a common way to learn distortion information. How to extract distortion information consistent with human perception is a problem to be solved. Besides, the joint effect on image quality caused by the interplay of content and distortion has not been fully studied. In this paper, a novel Content Distortion Interaction Network (CDINet) is proposed for blind image quality assessment. Distortion representation are guided by content representation to learn quality-aware representation. CDINet consists of four components: a Distortion-Aware Module (DAM), a Content-Aware Module (CAM), an Asymmetric Content-Distortion Interaction (ACDI) module, and a quality regression module. The content representation and distortion representation are extracted respectively and fused interactively in CDINet. Specifically, with the assistance of image restoration, distortion representation consistent with human perception is learned. To further improve the ability in distortion representation, the DAM is used to construct the differences between the distorted image and its reference image. The proposed ACDI module enables the interaction of content and distortion representations to occur at different levels with less computational cost. Since the proposed CDINet considers the joint impact on image quality caused by the interplay of content and distortion, the predicted image qualities highly align with human perception. Comprehensive experiments on 8 benchmark datasets demonstrate that the proposed CDINet effectively extracts quality-aware representation, achieving state-of-the-art performance in evaluating both synthetically and authentically distorted images.
C1 [Zheng, Limin; Luo, Yu; Ling, Jie] Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510006, Peoples R China.
   [Zhou, Zihan] South China Agr Univ, Coll Math & Informat, Guangzhou 510642, Peoples R China.
   [Yue, Guanghui] Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasound, Sch Biomed Engn,Med Sch, Shenzhen 518054, Peoples R China.
C3 Guangdong University of Technology; South China Agricultural University;
   Shenzhen University
RP Yue, GH (corresponding author), Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasound, Sch Biomed Engn,Med Sch, Shenzhen 518054, Peoples R China.
EM 2112205075@mail2.gdut.edu.cn; yuluo@gdut.edu.cn; zhouzihan@scau.edu.cn;
   jling@gdut.edu.cn; yueguanghui@szu.edu.cn
RI Luo, Yu/KVC-0220-2024
OI Ling, Jie/0000-0001-7736-1566; luo, yu/0000-0003-3968-9725
FU National Natural Science Foundation of China
FX No Statement Available
CR Babu NC, 2023, IEEE WINT CONF APPL, P2458, DOI 10.1109/WACV56688.2023.00249
   Chen CF, 2021, PROC CVPR IEEE, P11891, DOI 10.1109/CVPR46437.2021.01172
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Farias MCQ, 2012, ELECTRON LETT, V48, P631, DOI 10.1049/el.2012.0642
   Freitas PG, 2017, SIBGRAPI, P330, DOI 10.1109/SIBGRAPI.2017.50
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Gu Jinjin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P633, DOI 10.1007/978-3-030-58621-8_37
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Huang YP, 2023, IEEE T MULTIMEDIA, V25, P7672, DOI 10.1109/TMM.2022.3225728
   Jenadeleh M, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.4.043018
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kang S.B., 2006, P IEEE COMP SOC C CV, V1, P901, DOI [DOI 10.1109/CVPR.2006.207, 10.1109/cvpr.2006.207]
   Konuk B, 2015, SIG PROCESS COMMUN, P966, DOI 10.1109/SIU.2015.7129992
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin WS, 2022, IEEE T MULTIMEDIA, V24, P3706, DOI 10.1109/TMM.2021.3106503
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma JP, 2021, IEEE T IMAGE PROCESS, V30, P3650, DOI 10.1109/TIP.2021.3064195
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma Q, 2008, LECT NOTES COMPUT SC, V5226, P1124
   Macknik SL, 1998, NAT NEUROSCI, V1, P144, DOI 10.1038/393
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Pan Zhaoqing, 2023, IEEE Transactions on Artificial Intelligence, P148, DOI 10.1109/TAI.2022.3146804
   Pan ZQ, 2022, IEEE T IMAGE PROCESS, V31, P1613, DOI 10.1109/TIP.2022.3144892
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Prabhakaran V, 2023, IEEE WINT CONF APPL, P538, DOI 10.1109/WACVW58289.2023.00060
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Siahaan E, 2018, SIGNAL PROCESS-IMAGE, V60, P237, DOI 10.1016/j.image.2017.10.009
   Siahaan E, 2016, IEEE INT SYM MULTIM, P307, DOI [10.1109/ISM.2016.0067, 10.1109/ISM.2016.54]
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tong YB, 2010, J IMAGING SCI TECHN, V54, DOI 10.2352/J.ImagingSci.Technol.2010.54.3.030503
   Wang Q, 2016, NEUROCOMPUTING, V173, P1798, DOI 10.1016/j.neucom.2015.09.057
   Wang X., 2023, PROC IEEE INT C ACOU, P1, DOI [10.1109/ACASSP49357.2023.10096087, DOI 10.1109/ACASSP49357.2023.10096087]
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P981, DOI 10.1109/ICIP.2000.899622
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xie JM, 2023, IEEE INT CON MULTI, P1301, DOI 10.1109/ICME55011.2023.00226
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yue GH, 2023, IEEE T MULTIMEDIA, V25, P6499, DOI 10.1109/TMM.2022.3209889
   Yue GH, 2022, IEEE IMAGE PROC, P2182, DOI 10.1109/ICIP46576.2022.9897784
   Zeng H., 2017, PROC IEEE 25 INT C I, DOI DOI 10.1109/ICIP.2018.8451285
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
NR 55
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7089
EP 7100
DI 10.1109/TMM.2024.3360697
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000014
DA 2024-08-05
ER

PT J
AU Zhu, HL
   Yuan, JL
   Zhong, X
   Liao, L
   Wang, Z
AF Zhu, Huilin
   Yuan, Jingling
   Zhong, Xian
   Liao, Liang
   Wang, Zheng
TI Find Gold in Sand: Fine-Grained Similarity Mining for Domain-Adaptive
   Crowd Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data mining; Adaptation models; Evidence theory; Data models; Task
   analysis; Computational modeling; Synthetic data; Crowd counting; domain
   adaptive; evidence theory; multi-scale similarity
ID ADAPTATION
AB The domain shift of crowd scenes significantly hinders the application of crowd counting models in open scenarios. Although domain adaptation methods for crowd counting have bridged this gap to some extent, they ignore one of the significant causes of domain shift, which is the inter-domain data distribution bias. We discover that there exists a connection between the known and unknown distribution, which can be utilized by similarity mining to address the domain shift. However, there are still challenges related to insufficient and inaccurate similarity mining. In this article, a novel Fine-grained Inter-domain Similarity Mining (FSIM) framework is proposed. To comprehensively explore the similar distributions between source and target domains, we propose a Multi-scale Distribution Alignment (MDA) module based on diffusion retrieval. To enhance the reliability of inter- domain similarity mining, we propose a Multi-retrieval Refinement (MR) module based on evidence theory, which serves as an uncertainty measurement method. Eventually, to eliminate the data distribution bias, we perform model retraining using a similar distribution. Extensive experiments conducted on five standard crowd counting benchmarks, SHA, SHB, QNRF, NWPU, and JHU-CROWD++, show that the proposed FSIM has strong generalizability.
C1 [Zhu, Huilin; Yuan, Jingling; Zhong, Xian] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Hubei Key Lab Transportat Internet Things, Wuhan 430070, Peoples R China.
   [Liao, Liang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Wang, Zheng] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 Wuhan University of Technology; Nanyang Technological University; Wuhan
   University
RP Yuan, JL; Zhong, X (corresponding author), Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Hubei Key Lab Transportat Internet Things, Wuhan 430070, Peoples R China.
EM jsj_zhl@whut.edu.cn; yjl@whut.edu.cn; zhongx@whut.edu.cn;
   liang.liao@ntu.edu.sg; wangzwhu@whu.edu.cn
OI Liao, Liang/0000-0002-2238-2420; Zhu, Huilin/0000-0003-1607-7283
FU National Natural Science Foundation of China
FX No Statement Available
CR Arteta C, 2014, LECT NOTES COMPUT SC, V8691, P504, DOI 10.1007/978-3-319-10578-9_33
   Bai S, 2019, IEEE T IMAGE PROCESS, V28, P88, DOI 10.1109/TIP.2018.2863028
   Bernardo J.M., 2001, BAYESIAN THEORY, V12, P221, DOI 10.1088/0957-0233/12/2/702
   Cheng J, 2021, IEEE T IMAGE PROCESS, V30, P2862, DOI 10.1109/TIP.2021.3055631
   Cheng ZQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1897, DOI 10.1145/3343031.3350898
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dang ZY, 2021, PROC CVPR IEEE, P13688, DOI 10.1109/CVPR46437.2021.01348
   Dempster AP, 2008, STUD FUZZ SOFT COMP, V219, P57
   Du Zhipeng, 2023, P AAAI C ART INT, V37, P561
   Gao JY, 2023, IEEE T NEUR NET LEAR, V34, P4803, DOI 10.1109/TNNLS.2021.3124272
   Gong SJ, 2022, PROC CVPR IEEE, P7532, DOI 10.1109/CVPR52688.2022.00739
   Han ZB, 2023, IEEE T PATTERN ANAL, V45, P2551, DOI 10.1109/TPAMI.2022.3171983
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Iscen A, 2017, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2017.105
   Josang A., 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P1225
   Kingma D. P., 2014, arXiv
   Li M., 2008, IEEE 19 INT C PATTER, P1
   Liang D., 2021, IEEE Trans. Multimedia
   Liang DK, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3445-y
   Liao L, 2023, AAAI CONF ARTIF INTE, P1558
   Liao L, 2023, ISPRS J PHOTOGRAMM, V198, P45, DOI 10.1016/j.isprsjprs.2023.02.006
   Liao L, 2022, IEEE T IMAGE PROCESS, V31, P3525, DOI 10.1109/TIP.2022.3172208
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Liu WZ, 2022, PROC CVPR IEEE, P5331, DOI 10.1109/CVPR52688.2022.00527
   Liu YT, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P129, DOI 10.1145/3394171.3413825
   Ma XZ, 2022, PROC CVPR IEEE, P18900, DOI 10.1109/CVPR52688.2022.01835
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Qu SQ, 2023, PROC CVPR IEEE, P20019, DOI 10.1109/CVPR52729.2023.01917
   Rahutomo T., 2012, INT C ADV SCI TECHNO, P1
   Reddy MKK, 2022, IEEE T MULTIMEDIA, V24, P1008, DOI 10.1109/TMM.2021.3062481
   Rouse DM, 2008, IEEE IMAGE PROC, P1188, DOI 10.1109/ICIP.2008.4711973
   Shafer G, 2016, INT J APPROX REASON, V79, P7, DOI 10.1016/j.ijar.2016.07.009
   Shi ZL, 2018, PROC CVPR IEEE, P5382, DOI 10.1109/CVPR.2018.00564
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Tang GY, 2021, NEUROCOMPUTING, V442, P337, DOI 10.1016/j.neucom.2020.12.008
   Tian Y, 2021, Arxiv, DOI arXiv:2109.14483
   Unnikrishnan R, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P394
   Wang L, 2019, IEEE INT CON MULTI, P193, DOI 10.1109/ICME.2019.00041
   Wang MJ, 2023, IEEE T MULTIMEDIA, V25, P2074, DOI 10.1109/TMM.2022.3142398
   Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wei K, 2022, IEEE T CYBERNETICS, V52, P13788, DOI 10.1109/TCYB.2021.3110369
   Wu QQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P658, DOI 10.1145/3474085.3475230
   Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847
   Yan ZY, 2022, IEEE T MULTIMEDIA, V24, P2633, DOI 10.1109/TMM.2021.3086709
   Yang F, 2020, AAAI CONF ARTIF INTE, V34, P12589
   Yang F, 2019, AAAI CONF ARTIF INTE, P9087
   Yang YH, 2024, IEEE T MULTIMEDIA, V26, P1909, DOI 10.1109/TMM.2023.3243674
   Ye YL, 2022, IEEE T MULTIMEDIA, V24, P1325, DOI 10.1109/TMM.2021.3063616
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhao Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3174815
   Zhong X, 2023, PROC IEEE INT C ACOU, P1
   Zhong X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5316, DOI 10.1145/3474085.3475654
   Zhong Xian, 2022, P INT JOINT C ART IN, P1743, DOI [10.24963/ijcai.2022/243, DOI 10.24963/IJCAI.2022/243]
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   Zhu HL, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5659, DOI 10.1145/3503161.3548298
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou ZK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2185, DOI 10.1145/3474085.3475377
NR 61
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3842
EP 3855
DI 10.1109/TMM.2023.3316437
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300020
DA 2024-08-05
ER

PT J
AU Chen, TB
   Wang, WM
   Jiang, Z
   Li, RC
   Wang, BS
AF Chen, Tongbao
   Wang, Wenmin
   Jiang, Zhe
   Li, Ruochen
   Wang, Bingshu
TI Cross-Modality Knowledge Calibration Network for Video Corpus Moment
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Database languages; Semantics; Pipelines;
   Calibration; Transformers; Cross-modality; calibration; transformer;
   video corpus moment retrieval
AB Video corpus moment retrieval has become a hot topic recently, which aims to localize a consequent video moments highly relevant to the given query language description from video corpus. Existing methods towards this challenging task are suffering from the cases when the visual information and textual information in the video are very different from each other or from the cases where the redundant video content is semantically irrelevant with the query language description, which make the model confused of figuring out the truly useful within- and cross-modality information. In this article, we propose a novel Cross-Modality Knowledge Calibration Network (CKCN) to solve the issue mentioned above. Specifically, a dual calibration transformer module with improved multi-head attention is proposed to simultaneously capture the within- and cross-modality features between the visual and textual modality of the video automatically compressing the redundant information, and then a query-dependent fusion module is designed to guide feature fusion of the video's multi-modal information using the prior knowledge of query which further refine more important modality features. At last, a query-guided calibration transformer module with a well-designed learnable cell is utilized to align the query and video, forming a single joint representation for moment localization. Meanwhile, we introduce transfer learning into the task of video corpus moment retrieval (VCMR) for the first time to solve the defect of insufficient labeled data. Extensive experiments have been conducted on both the widely used TVR dataset and DiDeMo dataset which have achieved new state-of-the-art, thus verifying the effectiveness of our proposed CKCN.
C1 [Chen, Tongbao; Wang, Wenmin; Jiang, Zhe; Li, Ruochen] Macau Univ Sci & Technol, Sch Comp Sci & Engn, Taipa 999078, Macau, Peoples R China.
   [Chen, Tongbao] Guangdong Univ Technol, Sch Adv Mfg, Jieyang 515200, Peoples R China.
   [Jiang, Zhe] Guilin Coll Aerosp Technol, Sch Comp Sci & Engn, Guilin 541004, Peoples R China.
   [Wang, Bingshu] Northwestern Polytech Univ, Sch Software, Xian 710072, Peoples R China.
C3 Macau University of Science & Technology; Guangdong University of
   Technology; Guilin University of Aerospace Technology; Northwestern
   Polytechnical University
RP Wang, WM (corresponding author), Macau Univ Sci & Technol, Sch Comp Sci & Engn, Taipa 999078, Macau, Peoples R China.
EM tongbaochenchn@163.com; wmwang@must.edu.mo;
   2109853xia30002@student.must.edu.mo; lircsszz@outlook.com;
   wangbingshu@nwpu.edu.cn
OI chen, tongbao/0000-0002-7719-8364; Li, Ruochen/0000-0002-4341-6474;
   Wang, Bingshu/0000-0002-2603-8328
FU Science and Technology Development Fund
FX No Statement Available
CR Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Chen YT, 2022, PROC CVPR IEEE, P5110, DOI 10.1109/CVPR52688.2022.00506
   Chen Z., 2021, INT C LEARN REPRESEN
   Cheng ZQ, 2017, PROC CVPR IEEE, P4169, DOI 10.1109/CVPR.2017.444
   Cheng ZQ, 2017, IEEE T MULTIMEDIA, V19, P1170, DOI 10.1109/TMM.2016.2647386
   Cheng Zhi-Qi, 2017, P 2017ACM INT C MULT, P287
   Clark C, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P845
   Escorcia V, 2022, Arxiv, DOI arXiv:1907.12763
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Ghosh S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1984
   Gong GQ, 2023, IEEE T MULTIMEDIA, V25, P7402, DOI 10.1109/TMM.2022.3222664
   Hao JC, 2022, NEUROCOMPUTING, V483, P72, DOI 10.1016/j.neucom.2022.01.085
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hou ZJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3900, DOI 10.1145/3474085.3475281
   Jie Lei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P447, DOI 10.1007/978-3-030-58589-1_27
   Kim D, 2022, INT CONF ACOUST SPEE, P1720, DOI 10.1109/ICASSP43922.2022.9747523
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1665
   Liu DZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9302
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Luong M-T, 2015, ARXIV
   Nguyen P. A., 2017, TREC Video Re-trieval Eval.
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/wacv45572.2020.9093328, 10.1109/WACV45572.2020.9093328]
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Sung YL, 2022, PROC CVPR IEEE, P5217, DOI 10.1109/CVPR52688.2022.00516
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Teng JY, 2022, IEEE T MULTIMEDIA, V24, P1141, DOI 10.1109/TMM.2021.3120545
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GM, 2022, IEEE T MULTIMEDIA, V24, P1221, DOI 10.1109/TMM.2022.3142420
   Wang H, 2021, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR46437.2021.00695
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wang YX, 2023, IEEE T MULTIMEDIA, V25, P3921, DOI 10.1109/TMM.2022.3168424
   Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Yi K., 2020, PROC INT C LEARN REP
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, P IEEECVF C COMPUTER, P10287
   Zhang BW, 2020, Arxiv, DOI arXiv:2011.09046
   Zhang H., 2020, P 58 ANN M ASS COMPU, P6543
   Zhang H, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P685, DOI 10.1145/3404835.3462874
   Zhang ZJ, 2021, IEEE T MULTIMEDIA, V23, P3306, DOI 10.1109/TMM.2020.3023339
   Zhen LL, 2022, IEEE T NEUR NET LEAR, V33, P798, DOI 10.1109/TNNLS.2020.3029181
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
NR 56
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3799
EP 3813
DI 10.1109/TMM.2023.3316025
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300001
DA 2024-08-05
ER

PT J
AU Chen, ZY
   Wang, HL
   Chen, CW
AF Chen, Ziyu
   Wang, Hanli
   Chen, Chang Wen
TI Self-Supervised Video Representation Learning by Serial Restoration With
   Elastic Complexity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Self-supervised learning; pretext task; video representation learning;
   curriculum learning; video captioning; action recognition; nearest
   neighbor retrieval
ID ORDER
AB Self-supervised video representation learning leaves out heavy manual annotation by automatically excavating supervisory signals. Although contrastive learning based approaches exhibit superior performances, pretext task based approaches still deserve further study. This is because the pretext tasks exploit the nature of data and encourage feature extractors to learn spatiotemporal logic by discovering dependencies among video clips or cubes, without manual engineering on data augmentations or manual construction of contrastive pairs. To utilize chronological property more effectively and efficiently, this work proposes a novel pretext task, named serial restoration of shuffled clips (SRSC), disentangled by an elaborately designed task network composed of an order-aware encoder and a serial restoration decoder. In contrast to other order based pretext tasks that formulate clip order recognition as a one-step classification problem, the proposed SRSC task restores shuffled clips into the right order in multiple steps. Owing to the excellent elasticity of SRSC, a novel taxonomy of curriculum learning is further proposed to equip SRSC with different pre-training strategies. According to the factors that affect the complexity of solving the SRSC task, the proposed curriculum learning strategies can be categorized into task based, model based and data based. Extensive experiments are conducted on the subdivided strategies to explore their effectiveness and noteworthy laws. Compared with existing approaches, this work demonstrates that the proposed approach achieves state-of-the-art performances in pretext task based self-supervised video representation learning and a majority of the proposed strategies further boost the performance of downstream tasks. For the first time, the features pre-trained by the pretext tasks are applied to video captioning by feature-level early fusion, and enhance the input of existing approaches as a lightweight plugin.
C1 [Chen, Ziyu; Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
   [Chen, Ziyu; Wang, Hanli] Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.
   [Chen, Chang Wen] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 Tongji University; Tongji University; Hong Kong Polytechnic University
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.; Wang, HL (corresponding author), Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.
EM ziyuchen1997@tongji.edu.cn; hanliwang@tongji.edu.cn;
   changwen.chen@polyu.edu.hk
RI Wang, Hanli/G-5111-2014
OI Wang, Hanli/0000-0002-9999-4871; Chen, Ziyu/0009-0000-7045-3919; Chen,
   Chang Wen/0000-0002-6720-234X
FU National Natural Science Foundation of China
FX No Statement Available
CR Assefa M, 2023, IEEE T MULTIMEDIA, V25, P5500, DOI 10.1109/TMM.2022.3193559
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Büchler U, 2018, LECT NOTES COMPUT SC, V11219, P797, DOI 10.1007/978-3-030-01267-0_47
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen David, 2011, ACL
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diba A, 2019, IEEE I CONF COMP VIS, P6191, DOI 10.1109/ICCV.2019.00629
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan Haodong, 2022, PROC IEEECVF C COMPU, P3000
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Gidaris S, 2018, ICLR, DOI DOI 10.1109/ICCV.2019.00156
   Guo S, 2022, PROC CVPR IEEE, P19248, DOI 10.1109/CVPR52688.2022.01867
   Han T., 2020, Advances in Neural Information Processing Systems, V33, P5679
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang J, 2022, IEEE T CIRC SYST VID, V32, P3475, DOI 10.1109/TCSVT.2021.3114209
   Huang LH, 2021, PROC CVPR IEEE, P13881, DOI 10.1109/CVPR46437.2021.01367
   Jabri A., 2020, Advances in neural information processing systems
   Jenni Simon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P425, DOI 10.1007/978-3-030-58604-1_26
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Kim D, 2018, IEEE WINT CONF APPL, P793, DOI 10.1109/WACV.2018.00092
   Kong Quan, 2020, ARXIV201014810, V33, P8089
   Korbar B, 2018, ADV NEUR IN, V31
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Liang HW, 2022, AAAI CONF ARTIF INTE, P1564
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1978, DOI 10.1109/TIP.2022.3147032
   Luo DZ, 2020, AAAI CONF ARTIF INTE, V34, P11701
   Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751
   Mahendran A, 2019, LECT NOTES COMPUT SC, V11365, P99, DOI 10.1007/978-3-030-20873-8_7
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qian R, 2022, LECT NOTES COMPUT SC, V13686, P145, DOI 10.1007/978-3-031-19809-0_9
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Somandepalli K, 2021, IEEE T MULTIMEDIA, V24, P3355, DOI 10.1109/TMM.2021.3096155
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tan GC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P745
   Tao L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2193, DOI 10.1145/3394171.3413694
   Tengda Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P312, DOI 10.1007/978-3-030-58580-8_19
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang J., 2020, ECCV, P504
   Wang JL, 2022, IEEE T PATTERN ANAL, V44, P3791, DOI 10.1109/TPAMI.2021.3057833
   Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413
   Wang JP, 2021, AAAI CONF ARTIF INTE, V35, P10129
   Wu CE, 2022, IEEE COMPUT SOC CONF, P4079, DOI 10.1109/CVPRW56347.2022.00452
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xiao J, 2021, IEEE T MULTIMEDIA, V23, P3454, DOI 10.1109/TMM.2020.3025661
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2021, IEEE T MULTIMEDIA, V23, P3530, DOI 10.1109/TMM.2020.3026913
   Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658
   Zheng Qi, 2020, P CVPR
NR 62
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2235
EP 2248
DI 10.1109/TMM.2023.3293727
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100019
DA 2024-08-05
ER

PT J
AU Cheng, Y
   Fan, HH
   Lin, DY
   Sun, Y
   Kankanhalli, M
   Lim, JH
AF Cheng, Yi
   Fan, Hehe
   Lin, Dongyun
   Sun, Ying
   Kankanhalli, Mohan
   Lim, Joo-Hwee
TI Keyword-Aware Relative Spatio-Temporal Graph Networks for Video Question
   Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognition; Dogs; Feature extraction; Visualization; Semantics; Question
   answering (information retrieval); Task analysis; Video question
   answering; relative relation reasoning; spatial-temporal graph
AB The main challenge in video question answering (VideoQA) is to capture and understand the complex spatial and temporal relations between objects based on given questions. Existing graph-based methods for VideoQA usually ignore keywords in questions and employ a simple graph to aggregate features without considering relative relations between objects, which may lead to inferior performance. In this paper, we propose a Keyword-aware Relative Spatio-Temporal (KRST) graph network for VideoQA. First, to make question features aware of keywords, we employ an attention mechanism to assign high weights to keywords during question encoding. The keyword-aware question features are then used to guide video graph construction. Second, because relations are relative, we integrate the relative relation modeling to better capture the spatio-temporal dynamics among object nodes. Moreover, we disentangle the spatio-temporal reasoning into an object-level spatial graph and a frame-level temporal graph, which reduces the impact of spatial and temporal relation reasoning on each other. Extensive experiments on the TGIF-QA, MSVD-QA and MSRVTT-QA datasets demonstrate the superiority of our KRST over multiple state-of-the-art methods.
C1 [Cheng, Yi; Lin, Dongyun; Sun, Ying; Lim, Joo-Hwee] ASTAR, Inst Infocomm Res I2R, Singapore 138632, Singapore.
   [Cheng, Yi; Kankanhalli, Mohan] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Fan, Hehe] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Lim, Joo-Hwee] Nanyang Technol Univ, SCSE, Singapore 639798, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); National University of Singapore; Zhejiang
   University; Nanyang Technological University
RP Cheng, Y (corresponding author), ASTAR, Inst Infocomm Res I2R, Singapore 138632, Singapore.
EM cheng_yi@i2r.a-star.edu.sg; hehefan@zju.edu.cn;
   lin_dongyun@i2r.a-star.edu.sg; suny@i2r.a-star.edu.sg;
   mohan@comp.nus.edu.sg; joohwee@i2r.a-star.edu.sg
OI LIM, Joo Hwee/0000-0002-4103-3824; Kankanhalli,
   Mohan/0000-0002-4846-2015; Fan, Hehe/0000-0001-9572-2345
FU Agency for Science, Technology and Research
FX No Statement Available
CR Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Dang Long Hoang, 2021, P 30 INT JOINT C ART, P636, DOI DOI 10.24963/IJCAI.2021/88
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding SF, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109436
   Fan HH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3390891
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Gu M, 2021, IEEE T IMAGE PROCESS, V30, P2758, DOI 10.1109/TIP.2021.3051756
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang D, 2020, AAAI CONF ARTIF INTE, V34, P11021
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Jiang JW, 2020, AAAI CONF ARTIF INTE, V34, P11101
   Jiang P, 2020, AAAI CONF ARTIF INTE, V34, P11109
   Jin WK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1193, DOI 10.1145/3343031.3351065
   Kim JH, 2018, ADV NEUR IN, V31
   Kim KM, 2018, LECT NOTES COMPUT SC, V11219, P698, DOI 10.1007/978-3-030-01267-0_41
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lei J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1369
   Li D., 2022, P IEEECVF C COMPUTER, P4953
   LI JL, 2022, P IEEE CVF C COMP VI, P3032
   Li Juncheng, 2021, P IEEE CVF INT C COM, P1867
   Li Juncheng, 2022, ADV NEURAL INFORM PR, V35, P7290
   Li JN, 2020, IEEE T MULTIMEDIA, V22, P554, DOI 10.1109/TMM.2019.2930041
   Li XP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1166, DOI 10.1145/3343031.3350971
   Li XP, 2019, AAAI CONF ARTIF INTE, P8658
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1684, DOI 10.1109/TIP.2022.3142526
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Park J, 2021, PROC CVPR IEEE, P15521, DOI 10.1109/CVPR46437.2021.01527
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Seo A., 2021, P 59 ANN M ASS COMP, V1, P6167, DOI 10.18653/v1/2021.acllong.481
   Simonyan K, 2014, ADV NEUR IN, V27
   Thao Minh Le, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9969, DOI 10.1109/CVPR42600.2020.00999
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Xiao JB, 2022, AAAI CONF ARTIF INTE, P2804
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1666, DOI 10.1109/ICCV48922.2021.00171
   Yang XF, 2023, IEEE T MULTIMEDIA, V25, P8408, DOI 10.1109/TMM.2023.3237166
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yu WJ, 2021, ADV NEUR IN, V34
   Zellers R, 2021, Advances in Neural Information Processing Systems, V34
   Zeng PP, 2022, IEEE T IMAGE PROCESS, V31, P5936, DOI 10.1109/TIP.2022.3205212
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang X, 2022, IEEE T MULTIMEDIA, V24, P2986, DOI 10.1109/TMM.2021.3091882
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhao S, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P970, DOI 10.1145/3477495.3531950
   Zhao Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3690
   Zhong Y., 2022, P C EMP METH NAT LAN, P6439
   Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7
   Zhu Linchao, 2020, P IEEECVF C COMPUTER, DOI 10.1109/CVPR42600.2020.00877
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 53
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6131
EP 6141
DI 10.1109/TMM.2023.3345172
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600037
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Cong, W
   Cong, Y
   Dong, JH
   Sun, G
   Ding, HH
AF Cong, Wei
   Cong, Yang
   Dong, Jiahua
   Sun, Gan
   Ding, Henghui
TI Gradient-Semantic Compensation for Incremental Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gradient compensation; incremental learning; relations distillation;
   semantic segmentation
AB Incremental semantic segmentation focuses on continually learning the segmentation of new coming classes without obtaining the training data from previously seen classes. However, most current methods fail to tackle catastrophic forgetting and background shift since they 1) treat all previous classes equally without considering different forgetting paces caused by imbalanced gradient back-propagation; 2) lack strong semantic guidance between classes. In this paper, to solve the aforementioned challenges, we propose a Gradient-Semantic Compensation (GSC) model, which surmounts incremental semantic segmentation from both gradient and semantic perspectives. Specifically, to handle catastrophic forgetting from the gradient aspect, we develop a step-aware gradient compensation that can balance forgetting paces of previously seen classes by re-weighting gradient back-propagation. Meanwhile, we propose a soft-sharp semantic relation distillation to distill consistent inter-class semantic relations via soft labels for alleviating catastrophic forgetting from the semantic aspect. In addition, we design a prototypical pseudo re-labeling which provides strong semantic guidance to mitigate background shift. It produces high-quality pseudo labels for background pixels belonging to previous classes by assessing distances of pixels relative to class-wise prototypes. Experiments on three public segmentation datasets provide strong evidence for the effectiveness of our proposed GSC model.
C1 [Cong, Wei; Dong, Jiahua; Sun, Gan] Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang 110016, Peoples R China.
   [Cong, Wei; Dong, Jiahua; Sun, Gan] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110169, Peoples R China.
   [Cong, Wei; Dong, Jiahua] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Cong, Yang] South China Univ Technol, Coll Automat Sci & Engn, Guangzhou 510640, Peoples R China.
   [Ding, Henghui] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; South China University of Technology;
   Nanyang Technological University
RP Cong, Y (corresponding author), South China Univ Technol, Coll Automat Sci & Engn, Guangzhou 510640, Peoples R China.
EM congwei45@gmail.com; congyang81@gmail.com; dongjiahua1995@gmail.com;
   sungan1412@gmail.com; henghui.ding@gmail.com
RI Sun, Gan/ABD-6793-2021
OI Sun, Gan/0000-0003-1111-6909; Cong, Wei/0000-0002-9531-7179
FU National Key Ramp;D Program of China
FX No Statement Available
CR Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Baek D., 2022, Advances in Neural Information Processing Systems, P10380
   Cermelli Fabio, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9230, DOI 10.1109/CVPR42600.2020.00925
   Cha S, 2021, ADV NEUR IN
   Cha S, 2023, PROC CVPR IEEE, P20127, DOI 10.1109/CVPR52729.2023.01927
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen Zhiyuan, 2018, Lifelong machine learning, V12, P1, DOI DOI 10.2200/S00737ED1V01Y201610AIM033
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Cui YW, 2023, IEEE T MULTIMEDIA, V25, P6422, DOI 10.1109/TMM.2022.3208743
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Dong JH, 2023, PROC CVPR IEEE, P3934, DOI 10.1109/CVPR52729.2023.00383
   Dong JH, 2022, PROC CVPR IEEE, P10154, DOI 10.1109/CVPR52688.2022.00992
   Douillard Arthur, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P86, DOI 10.1007/978-3-030-58565-5_6
   Douillard A, 2021, PROC CVPR IEEE, P4039, DOI 10.1109/CVPR46437.2021.00403
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Garland-Thomson Rosemarie., 2009, STARING WE LOOK
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Kundu Jogendra Nath, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P53, DOI 10.1007/978-3-030-58601-0_4
   Li YX, 2024, IEEE T MULTIMEDIA, V26, P1346, DOI 10.1109/TMM.2023.3280011
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin D, 2018, LECT NOTES COMPUT SC, V11207, P622, DOI 10.1007/978-3-030-01219-9_37
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Ma LF, 2023, IEEE T MULTIMEDIA, V25, P2774, DOI 10.1109/TMM.2022.3151145
   Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810
   McCloskey M., 1989, Psychology of learning and motivation, V24, P165
   Michieli U, 2021, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR46437.2021.00117
   Michieli U, 2019, IEEE INT CONF COMP V, P3205, DOI 10.1109/ICCVW.2019.00400
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Oh Y., 2022, Adv. Neural Inform. Process. Syst., V35, P14516
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Rosenfeld A, 2020, IEEE T PATTERN ANAL, V42, P651, DOI 10.1109/TPAMI.2018.2884462
   Schwarz J, 2018, PR MACH LEARN RES, V80
   Sun G, 2022, IEEE T PATTERN ANAL, V44, P3895, DOI 10.1109/TPAMI.2021.3058852
   Sun G, 2022, IEEE T NEUR NET LEAR, V33, P1467, DOI 10.1109/TNNLS.2020.3042500
   Thuseethan S, 2022, IEEE T MULTIMEDIA, V24, P4367, DOI 10.1109/TMM.2021.3116434
   Wang LX, 2019, Arxiv, DOI arXiv:1910.06044
   Wang LX, 2021, AAAI CONF ARTIF INTE, V35, P10165
   Xiao JW, 2023, PROC CVPR IEEE, P7204, DOI 10.1109/CVPR52729.2023.00696
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xie JW, 2022, PROC CVPR IEEE, P14331, DOI 10.1109/CVPR52688.2022.01395
   Xu RT, 2024, IEEE T MULTIMEDIA, V26, P581, DOI 10.1109/TMM.2023.3267891
   Xu RT, 2023, IEEE T IMAGE PROCESS, V32, P1052, DOI 10.1109/TIP.2023.3238648
   Yang GL, 2023, IEEE T MULTIMEDIA, V25, P3841, DOI 10.1109/TMM.2022.3167555
   Yang GL, 2023, IEEE T PATTERN ANAL, V45, P2567, DOI 10.1109/TPAMI.2022.3163806
   Yaoyao Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12242, DOI 10.1109/CVPR42600.2020.01226
   Yoon J., 2018, PROC INT C LEARN REP
   Yu L, 2023, IEEE T NEUR NET LEAR, V34, P9116, DOI 10.1109/TNNLS.2022.3155746
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhang CB, 2022, PROC CVPR IEEE, P7043, DOI 10.1109/CVPR52688.2022.00692
   Zhang D., 2023, PROC C EMPIR METHODS
   Zhang DZ, 2023, PROCEEDINGS OF THE 32ND ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2023, P3319, DOI 10.1145/3583780.3615075
   Zhang Z., 2022, NeurIPS, V35, P24340
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhu LY, 2023, PROC CVPR IEEE, P3082, DOI 10.1109/CVPR52729.2023.00301
NR 71
TC 1
Z9 1
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5561
EP 5574
DI 10.1109/TMM.2023.3336243
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800013
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Guo, XY
   Wei, X
   Zhang, SL
   Lu, W
   Xing, WW
AF Guo, Xiaoyu
   Wei, Xiang
   Zhang, Shunli
   Lu, Wei
   Xing, Weiwei
TI DCRP: Class-Aware Feature Diffusion Constraint and Reliable
   Pseudo-Labeling for Imbalanced Semi-Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Feature extraction; Semisupervised learning; Reliability; Data
   models; Data augmentation; Tail; Class-imbalanced learning; feature
   diffusion; reliable pseudo-labeling; semi-supervised learning
AB Despite the astounding progress made in semi-supervised learning (SSL) and imbalanced supervised learning (ISL), there has been little attention devoted to the research of imbalanced semi-supervised learning (ISSL). The "Matthew effect", a phenomenon where a disparity in data representation becomes more severe in a class-imbalanced dataset during training, could be amplified in a semi-supervised setting. In this study, we addressed two key challenges in ISSL: maintaining the reliability of pseudo-labels and ensuring a balanced representation of features. Specifically, we propose a class-aware feature-diffusion constraint and reliable pseudo-labeling (DCRP) framework to address these issues. In the DCRP, we counteract the overconfidence problem of softmax by adding an extra class to the typical $K$ class problem without the need for additional parameters. Moreover, we introduced a flexible class-aware feature diffusion constraint in the feature extractor, promoting a more balanced feature diversity. Experimental validations on various datasets, such as CIFAR10-LT, CIFAR100-LT, SVHN-LT, and Small ImageNet-127, demonstrated consistent improvements in accuracy with our DCRP method. In particular, we achieved a steady improvement in accuracy of approximately 1% under the newly published ACR prototype across most settings.
C1 [Guo, Xiaoyu; Wei, Xiang; Zhang, Shunli; Lu, Wei; Xing, Weiwei] Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Wei, X (corresponding author), Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.
EM guoxiaoyu@bjtu.edu.cn; xiangwei@bjtu.edu.cn; slzhang@bjtu.edu.cn;
   luwei@bjtu.edu.cn; wwxing@bjtu.edu.cn
OI Wei, Xiang/0000-0002-8967-6423
FU National Natural Science Foundation of China
FX No Statement Available
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   Berthelot D, 2020, 8 INT C LEARNING REP
   Chen C, 2022, IEEE T MULTIMEDIA, V24, P3679, DOI 10.1109/TMM.2021.3105807
   Chen H., 2023, PROC 11 INT C LEARN
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Fan Y, 2022, PROC CVPR IEEE, P14554, DOI 10.1109/CVPR52688.2022.01417
   Guo CA, 2017, PR MACH LEARN RES, V70
   Guo LZ, 2022, PR MACH LEARN RES
   Hein M, 2019, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2019.00013
   Jiang YBY, 2023, IEEE T PATTERN ANAL, V45, P5970, DOI 10.1109/TPAMI.2022.3208419
   Kim J., 2020, ADV NEUR IN, V33, P14567
   Kong XY, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109793
   Kong XY, 2022, KNOWL-BASED SYST, V253, DOI 10.1016/j.knosys.2022.109561
   Laine S, 2016, INT C LEARN REPR
   Lee H, 2021, 35 C NEURAL INFORM P, V34
   Li G, 2022, LECT NOTES COMPUT SC, V13669, P457, DOI 10.1007/978-3-031-20077-9_27
   Li Y, 2019, PROC CVPR IEEE, P9564, DOI 10.1109/CVPR.2019.00980
   Maldonado S, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108511
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Oh Y, 2022, PROC CVPR IEEE, P9776, DOI 10.1109/CVPR52688.2022.00956
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Tarvainen A, 2017, ADV NEUR IN, V30
   Verma V, 2022, NEURAL NETWORKS, V145, P90, DOI 10.1016/j.neunet.2021.10.008
   Wang JX, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3102026
   Wang QY, 2022, EXPERT SYST APPL, V189, DOI 10.1016/j.eswa.2021.115999
   Wang R., 2023, PROC 11 INT C LEARN, P1
   Wang SS, 2023, NEUROCOMPUTING, V523, P213, DOI 10.1016/j.neucom.2022.12.048
   Wang XD, 2022, PROC CVPR IEEE, P14627, DOI 10.1109/CVPR52688.2022.01424
   Wang Y., 2023, PROC 11 INT C LEARN, P1
   Wei C, 2021, PROC CVPR IEEE, P10852, DOI 10.1109/CVPR46437.2021.01071
   Wei T, 2023, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR52729.2023.00338
   Wei T, 2024, MACH LEARN, V113, P1725, DOI 10.1007/s10994-022-06247-z
   Wei X., 2018, PROC INT C LEARN REP
   Wei X, 2021, NEURAL NETWORKS, V133, P166, DOI 10.1016/j.neunet.2020.10.018
   Wu ZH, 2023, IEEE T MULTIMEDIA, V25, P8593, DOI 10.1109/TMM.2023.3260649
   Xu Y, 2021, INT C MACHINE LEARNI, V139
   Yu SH, 2022, PROC CVPR IEEE, P70, DOI 10.1109/CVPR52688.2022.00017
   Yu Z., 2023, PROC 11 INT C LEARN
   Zhang B., 2021, Advances in Neural Information Processing Systems, V34, P18408, DOI DOI 10.48550/ARXIV.2110.08263
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang YH, 2023, IEEE T MULTIMEDIA, V25, P1749, DOI 10.1109/TMM.2022.3158069
NR 41
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7146
EP 7159
DI 10.1109/TMM.2024.3360704
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000027
DA 2024-08-05
ER

PT J
AU Jin, YL
   Ji, ZY
   Zeng, D
   Zhang, XP
AF Jin, Yan-Liang
   Ji, Ze-Yu
   Zeng, Dan
   Zhang, Xiao-Ping
TI VWP:An Efficient DRL-Based Autonomous Driving Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Autonomous driving; deep deterministic policy gradient; deep
   reinforcement learning; proximal policy optimization; variational auto
   encoder; wasserstein generative adversarial network
AB In this paper, a novel DRL-based model (VWP, VAE-WGAN-PPOE) is proposed to solve the problem of long training time and unsatisfactory training effect in the end-to-end autonomous driving. The model is optimized from feature extraction and algorithm decision. In feature extraction, we encode the input video by combining variational auto encoder (VAE) with wasserstein generative adversarial network (WGAN). The state dimension is reduced and the problem of mode collapse and gradient disappearance caused by generative adversarial network (GAN) training is solved. In decision algorithm, we formulate a new reward function by analyzing the factors affecting driving performance. Furthermore, we propose an enhanced algorithm PPOE based on the proximal policy optimization (PPO). In the CARLA simulator, compared with CNN and ResNet34, the convergence speed of the DRL model based on VAE-WGAN increases by 26.1% and 20.3%, the navigation task completion rate increases by 18.5% and 9.2%, and the collision rate decreases by 13.6% and 9.4%. Compared with deep deterministic policy gradient (DDPG) decision algorithm, the convergence speed of the DRL model based on PPOE increases by 23.3%, the navigation task completion rate increases by 5.0% in sunny days and 8.4% in severe weather, the collision rate decreases by 3.5% in sunny days and 6.6% in severe weather. Extensive experiments show that the proposed model enables the agent to drive safely along the navigational route in the complex environment with pedestrian and vehicle interaction, even in severe weather.
C1 [Jin, Yan-Liang; Ji, Ze-Yu] Shanghai Univ, Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Zeng, Dan] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Elect Engn, Toronto, ON M5B 2K3, Canada.
C3 Shanghai University; Shanghai University; Toronto Metropolitan
   University
RP Jin, YL (corresponding author), Shanghai Univ, Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM wuhaide@shu.edu.cn; jizeyushuedu@163.com; dzeng@shu.edu.cn;
   xzhang@ee.ryerson.ca
RI Zhang, Xiaoping/AFW-5367-2022; Zhang, Xiaoping/AAX-7947-2021
OI Zhang, Xiaoping/0000-0002-8891-0978; Zhang,
   Xiaoping/0000-0002-8891-0978; Jin, Yanliang/0000-0001-9836-8249; Ji,
   Zeyu/0000-0002-7815-171X
FU Innovation Program of Shanghai Municipal Science and Technology
   Commission
FX No Statement Available
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bojarski M, 2016, Arxiv, DOI arXiv:1604.07316
   Chae H, 2017, IEEE INT C INTELL TR
   Chen L, 2019, IEEE-CAA J AUTOMATIC, V6, P236, DOI 10.1109/JAS.2018.7511186
   Chen SY, 2020, IEEE T VEH TECHNOL, V69, P4740, DOI 10.1109/TVT.2020.2979493
   Chen X, 2017, IEEE INT VEH SYM, P379, DOI 10.1109/IVS.2017.7995748
   Chen YL, 2019, IEEE COMPUT SOC CONF, P1326, DOI 10.1109/CVPRW.2019.00172
   Codevilla F, 2019, IEEE I CONF COMP VIS, P9328, DOI 10.1109/ICCV.2019.00942
   Codevilla F, 2018, IEEE INT CONF ROBOT, P4693
   Denton E, 2015, ADV NEUR IN, V28
   Fujimoto S, 2018, PR MACH LEARN RES, V80
   Furuta R, 2020, IEEE T MULTIMEDIA, V22, P1704, DOI 10.1109/TMM.2019.2960636
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Haarnoja T, 2019, Arxiv, DOI [arXiv:1812.05905, DOI 10.48550/ARXIV.1812.05905]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jung A., 2017, Self-driving truck
   Kendall A, 2019, IEEE INT CONF ROBOT, P8248, DOI [10.1109/ICRA.2019.8793742, 10.1109/icra.2019.8793742]
   King DB, 2015, ACS SYM SER, V1214, P1
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Lau B., 2016, Using keras and deep deterministic policy gradient to play torcs
   Li X, 2015, 2015 SIXTH INTERNATIONAL CONFERENCE ON INTELLIGENT CONTROL AND INFORMATION PROCESSING (ICICIP), P336, DOI 10.1109/ICICIP.2015.7388193
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39
   Liang XD, 2018, LECT NOTES COMPUT SC, V11211, P604, DOI 10.1007/978-3-030-01234-2_36
   Lillicrap T. P., 2015, P INT C LEARN REPR S, P1
   Michels J., 2005, Proceedings of the 22nd international conference on Machine learning, P593, DOI DOI 10.1145/1102351.1102426
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Parmar N, 2018, PR MACH LEARN RES, V80
   Porav H, 2018, IEEE INT C INTELL TR, P958, DOI 10.1109/ITSC.2018.8569222
   Prakash A, 2021, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR46437.2021.00700
   Pu YC, 2016, ADV NEUR IN, V29
   Santana E, 2016, IEEE IJCNN, P3296, DOI 10.1109/IJCNN.2016.7727620
   Schulman J, 2017, Arxiv, DOI [arXiv:1707.06347, DOI 10.48550/ARXIV.1707.06347]
   Schulman J, 2015, PR MACH LEARN RES, V37, P1889
   Sobh, 2018, P ADV NEUR INF PROC, P1
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Thrun S.B., 1992, Tech. Rep. CMU-CS-92-102
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Vasquez R, 2019, IEEE INT C INTELL TR, P4348, DOI 10.1109/ITSC.2019.8916912
   Wang JX, 2019, IEEE INT VAC ELECT C, DOI 10.1109/ivec.2019.8745021
   Wang P, 2017, IEEE INT C INTELL TR
   Wang P, 2019, IEEE INT VEH SYM, P1454, DOI [10.1109/IVS.2019.8813903, 10.1109/ivs.2019.8813903]
   Wang P, 2018, IEEE INT VEH SYM, P1379, DOI 10.1109/IVS.2018.8500556
   Wang ZY, 2016, PR MACH LEARN RES, V48
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Wu YQ, 2023, IEEE T NEUR NET LEAR, V34, P3680, DOI 10.1109/TNNLS.2021.3116063
   Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376
   Xu JX, 2018, 2018 INTERNATIONAL CONFERENCE ON CONTROL AND ROBOTS (ICCR), P57, DOI 10.1109/ICCR.2018.8534494
   Yu A., 2016, Course Project Reports, V2016, P1
   Yu Y, 2017, LECT NOTES COMPUT SC, V10667, P97, DOI 10.1007/978-3-319-71589-6_9
   Zaremba Wojciech, 2015, P 3 INT C LEARN REPR
   Zhang JK, 2017, AAAI CONF ARTIF INTE, P2891
   Zhang XY, 2021, IEEE T MULTIMEDIA, V24, P3240, DOI 10.1109/TMM.2021.3096009
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P2545, DOI 10.1109/TMM.2020.3013350
   Zhu Z., 2021, arXiv
NR 58
TC 1
Z9 1
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2096
EP 2108
DI 10.1109/TMM.2022.3177942
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100003
DA 2024-08-05
ER

PT J
AU Lin, JX
   Zhao, W
   Wang, YJ
AF Lin, Jianxin
   Zhao, Wei
   Wang, Yijun
TI Visual Correspondence Learning and Spatially Attentive Synthesis via
   Transformer for Exemplar-Based Anime Line Art Colorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Line art colorization; transformer; correspondence learning
ID IMAGE
AB Exemplar-based anime line art colorization of the same character has been a challenging problem in digital art production because of the sparse representation of line images and the significantly different anime appearance between line and color images. Therefore, it is a fundamental problem to find semantic correspondence between two kinds of images. In this paper, we propose a correspondence learning Transformer network for exemplar-based line art colorization, called ArtFormer, which utilizes a Transformer-based architecture to learn both spatial and visual relationships between line art and color images. ArtFormer mainly consists of two parts: correspondence learning and high-quality image generation. In particular, the correspondence learning module is composed of several Transformer blocks, each of which formulates the deep line image features and color images features as queries and keys, and learns the dense correspondence between two image domains. Then, the network synthesizes high-quality images with a newly proposed Spatial Attention Adaptive Normalization (SAAN) that uses warped deep exemplar features to modulate the shallow features for better adaptive normalization parameters generation. Both qualitative and quantitative experiments show that our method achieves the best performance on exemplar-based line art colorization compared with state-of-the-art methods and other baselines.
C1 [Lin, Jianxin; Zhao, Wei; Wang, Yijun] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
C3 Hunan University
RP Wang, YJ (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
EM linjianxin@hnu.edu.cn; zhaoweiheap@hnu.edu.cn; wyjun@hnu.edu.cn
OI wang, yijun/0000-0002-3372-8167; Lin, Jianxin/0000-0003-0359-8821
FU National Natural Science Foundation of China
FX No Statement Available
CR Carion N., 2020, EUR C COMP VIS, P213
   Carrillo H., 2022, P AS C COMP VIS, P4548
   Casey Evan, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Hensel M, 2017, ADV NEUR IN, V30
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P4002, DOI 10.1109/TMM.2021.3111501
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jampani V, 2017, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2017.336
   Jiang E., 2021, P IEEE CVF INT C COM, P6207
   Jiang Y., 2021, P 35 C NEUR INF PROC, P14745
   Kim Taebum, 2018, Anime sketch colorization pair
   Kingma D. P., 2014, arXiv
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Lee Kwonjoon, 2022, P INT C LEARN REPR
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liu SH, 2022, LECT NOTES COMPUT SC, V13676, P72, DOI 10.1007/978-3-031-19787-1_5
   lllyasviel, 2017, An U-Net with some algorithm to take the sketch from apainting
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Min JH, 2019, Arxiv, DOI arXiv:1908.10543
   Orzan A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360691
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Ren X., 2022, P INT C LEARN REPR
   Rocco I, 2018, ADV NEUR IN, V31
   Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sauvaget C., 2010, Proceedings of the Sixth International Conference on Signal-Image Technology & Internet-Based Systems (SITIS 2010), P153, DOI 10.1109/SITIS.2010.35
   Shi M, 2023, IEEE T VIS COMPUT GR, V29, P2965, DOI 10.1109/TVCG.2022.3146000
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Sykora D, 2009, COMPUT GRAPH FORUM, V28, P599, DOI 10.1111/j.1467-8659.2009.01400.x
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao H Y, 2024, arXiv
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou XR, 2021, PROC CVPR IEEE, P11460, DOI 10.1109/CVPR46437.2021.01130
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu HC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925872
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
NR 58
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6880
EP 6890
DI 10.1109/TMM.2024.3358027
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600044
DA 2024-08-05
ER

EF